{
    "q": [
        {
            "docid": "3717_69",
            "document": "Brain . Theorists have worked to understand these response patterns by constructing mathematical models of neurons and neural networks, which can be simulated using computers. Some useful models are abstract, focusing on the conceptual structure of neural algorithms rather than the details of how they are implemented in the brain; other models attempt to incorporate data about the biophysical properties of real neurons. No model on any level is yet considered to be a fully valid description of brain function, though. The essential difficulty is that sophisticated computation by neural networks requires distributed processing in which hundreds or thousands of neurons work cooperatively\u2014current methods of brain activity recording are only capable of isolating action potentials from a few dozen neurons at a time.",
            "score": 113.68995022773743
        },
        {
            "docid": "33826069_3",
            "document": "Viral neuronal tracing . Most neuroanatomists would agree that understanding how the brain is connected to itself and the body is of paramount importance. As such, it is of equal importance to have a way to visualize and study the connections among neurons. Neuronal tracing methods offer an unprecedented view into the morphology and connectivity of neural networks. Depending on the tracer used, this can be limited to a single neuron or can progress trans-synaptically to adjacent neurons. After the tracer has spread sufficiently, the extent may be measured either by fluorescence (for dyes) or by immunohistochemistry (for biological tracers). An important innovation in this field is the use of neurotropic viruses as tracers. These not only spread throughout the initial site of infection, but can jump across synapses. The use of a virus provides a self-replicating tracer. This can allow for the elucidation of neural microcircuitry to an extent that was previously unobtainable.  This has significant implications for the real world. If we can better understand what parts of the brain are intimately connected, we can predict the effect of localized brain injury. For example, if a patient has a stroke in the amygdala, primarily responsible for emotion, the patient might also have trouble learning to perform certain tasks because the amygdala is highly interconnected with the orbitofrontal cortex, responsible for reward learning. As always, the first step to solving a problem is fully understanding it, so if we are to have any hope of fixing brain injury, we must first understand its extent and complexity.",
            "score": 103.49663019180298
        },
        {
            "docid": "9617564_2",
            "document": "Oja's rule . Oja's learning rule, or simply Oja's rule, named after Finnish computer scientist Erkki Oja, is a model of how neurons in the brain or in artificial neural networks change connection strength, or learn, over time. It is a modification of the standard Hebb's Rule (see Hebbian learning) that, through multiplicative normalization, solves all stability problems and generates an algorithm for principal components analysis. This is a computational form of an effect which is believed to happen in biological neurons.",
            "score": 111.56762981414795
        },
        {
            "docid": "1726672_4",
            "document": "Neural circuit . The connections between neurons in the brain are much more complex than those of the artificial neurons used in the connectionist neural computing models of artificial neural networks. The basic kinds of connections between neurons are synapses, chemical and electrical synapses. One principle by which neurons work is neural summation \u2013 potentials at the postsynaptic membrane will sum up in the cell body. If the depolarization of the neuron at the axon goes above threshold an action potential will occur that travels down the axon to the terminal endings to transmit a signal to other neurons. Excitatory and inhibitory synaptic transmission is realized mostly by inhibitory postsynaptic potentials (IPSPs) and excitatory postsynaptic potentials (EPSPs).",
            "score": 67.50838255882263
        },
        {
            "docid": "33818014_8",
            "document": "Nervous system network models . On a high level representation, the neurons can be viewed as connected to other neurons to form a neural network in one of three ways. A specific network can be represented as a physiologically (or anatomically) connected network and modeled that way. There are several approaches to this (see Ascoli, G.A. (2002) Sporns, O. (2007), Connectionism, Rumelhart, J. L., McClelland, J. L., and PDP Research Group (1986), Arbib, M. A. (2007)). Or, it can form a functional network that serves a certain function and modeled accordingly (Honey, C. J., Kotter, R., Breakspear, R., & Sporns, O. (2007), Arbib, M. A. (2007)). A third way is to hypothesize a theory of the functioning of the biological components of the neural system by a mathematical model, in the form of a set of mathematical equations. The variables of the equation are some or all of the neurobiological properties of the entity being modeled, such as the dimensions of the dendrite or the stimulation rate of action potential along the axon in a neuron. The mathematical equations are solved using computational techniques and the results are validated with either simulation or experimental processes. This approach to modeling is called computational neuroscience. This methodology is used to model components from the ionic level to system level of the brain. This method is applicable for modeling integrated system of biological components that carry information signal from one neuron to another via intermediate active neurons that can pass the signal through or create new or additional signals. The computational neuroscience approach is extensively used and is based on two generic models, one of cell membrane potential Goldman (1943) and Hodgkin and Katz (1949), and the other based on Hodgkin-Huxley model of action potential (information signal).",
            "score": 102.04338955879211
        },
        {
            "docid": "39199253_2",
            "document": "Percolation (cognitive psychology) . Percolation (from the Latin word \"percolatio\", meaning filtration) is a theoretical model used to understand the way activation and diffusion of neural activity occur within neural networks. Percolation is a model used to explain how neural activity is transmitted across the various connections within the brain. Often it is easiest to understand percolation theory by explaining its use in epidemiology. Individuals that are infected with a disease can spread the disease through contact with others in their social network. Those who are more social and come into contact with more people will help to propagate the disease quicker than those who are less social. Therefore factors such as occupation and sociability influence the rate of infection. Now, if one were to think of \"neurons\" as the \"individuals\" and \"synaptic connections\" as the \"social bonds\" between people, then one can determine how easily messages between neurons will spread. When a neuron fires, the message is transmitted along all synaptic connections to other neurons until it can no longer continue. Synaptic connections are considered either open or closed (like a social or unsocial person) and messages will flow along any and all open connections until they can go no further. Just like occupation and sociability play a key role in the spread of disease, so too do the number of neurons, synaptic plasticity and long-term potentiation when talking about neural percolation.",
            "score": 85.97847175598145
        },
        {
            "docid": "21855574_5",
            "document": "Brain simulation . The connectivity of the neural circuit for touch sensitivity of the simple C. elegans nematode (roundworm) was mapped in 1985 and partly simulated in 1993. Since 2004, many software simulations of the complete neural and muscular system have been developed, including simulation of the worm's physical environment. Some of these models have been made available for download. However, there is still a lack of understanding of how the neurons and the connections between them generate the surprisingly complex range of behaviors that are observed in the relatively simple organism. This contrast between the apparent simplicity of how the mapped neurons interact with their neighbours, and exceeding complexity of the overall brain function, is an example of an emergent property. Interestingly, this kind of emergent property is paralleled within artificial neural networks, the neurons of which are exceedingly simple compared to their often complex, abstract outputs.",
            "score": 72.30791139602661
        },
        {
            "docid": "33818014_7",
            "document": "Nervous system network models . The basic structural unit of the neural network is connectivity of one neuron to another via an active junction, called synapse. Neurons of widely divergent characteristics are connected to each other via synapses, whose characteristics are also of diverse chemical and electrical properties. In presenting a comprehensive view of all possible modeling of the brain and neural network, an approach is to organize the material based on the characteristics of the networks and the goals that need to be accomplished. The latter could be either for understanding the brain and the nervous system better or to apply the knowledge gained from the total or partial nervous system to real world applications such as artificial intelligence, Neuroethics or improvements in medical science for society.",
            "score": 73.77465176582336
        },
        {
            "docid": "21523_3",
            "document": "Artificial neural network . An ANN is based on a collection of connected units or nodes called artificial neurons which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it.",
            "score": 71.0894079208374
        },
        {
            "docid": "8402086_18",
            "document": "Computational neurogenetic modeling . Because the amount of data on the interplay of genes and neurons and their effects is not enough to construct a rigorous model,  evolutionary computation is used to optimize artificial neural networks and gene regulatory networks, a common technique being the genetic algorithm. A genetic algorithm is a process that can be used to refine models by mimicking the process of natural selection observed in biological ecosystems. The primary advantages are that, due to not requiring derivative information, it can be applied to black box problems and multimodal optimization. The typical process for using genetic algorithms to refine a gene  regulatory network is: first, create a population; next, to create offspring via a crossover operation and  evaluate their fitness; then, on a group chosen for high fitness, simulate mutation via a mutation operator;  finally, taking the now mutated group, repeat this process until a desired level of fitness is demonstrated. Methods by which artificial neural networks may alter their structure without simulated mutation and fitness selection have been developed. A dynamically evolving neural network is one approach, as the creation of new connections and new neurons can  be modeled as the system adapts to new data. This enables the network to evolve in modeling accuracy without simulated natural selection. One method by which dynamically evolving networks may be optimized, called evolving layer neuron aggregation, combines neurons with sufficiently similar input weights into one neuron. This can take place during the training of the network, referred to as online aggregation, or between periods of training, referred to as offline aggregation. Experiments have suggested that offline aggregation is more efficient.",
            "score": 133.63068199157715
        },
        {
            "docid": "33818014_2",
            "document": "Nervous system network models . Network of human nervous system comprises nodes (for example, neurons) that are connected by links (for example, synapses). The connectivity may be viewed anatomically, functionally, or electrophysiologically. These are presented in several Wikipedia articles that include Connectionism (a.k.a. Parallel Distributed Processing (PDP)), Biological neural network, Artificial neural network (a.k.a. Neural network), Computational neuroscience, as well as in several books by Ascoli, G. A. (2002), Sterratt, D., Graham, B., Gillies, A., & Willshaw, D. (2011), Gerstner, W., & Kistler, W. (2002), and Rumelhart, J. L., McClelland, J. L., and PDP Research Group (1986) among others. The focus of this article is a comprehensive view of modeling a neural network (technically neuronal network based on neuron model). Once an approach based on the perspective and connectivity is chosen, the models are developed at microscopic (ion and neuron), mesoscopic (functional or population), or macroscopic (system) levels. Computational modeling refers to models that are developed using computing tools.",
            "score": 87.07371068000793
        },
        {
            "docid": "3737445_11",
            "document": "Quantum neural network . Most learning algorithms follow the classical model of training an artificial neural network to learn the input-output function of a given training set and use classical feedback loops to update parameters of the quantum system until they converge to an optimal configuration. Learning as a parameter optimisation problem has also been approached by adiabatic models of quantum computing. Recently there has been proposed a new post-learning strategy to allow the search for improved set of weights based on analogy with quantum effects occurring in nature. The technique, proposed in is based on the analogy of modeling a biological neuron as a semiconductor heterostructure consisting of one energetic barrier sandwiched between two energetically lower areas. The activation function of the neuron is therefore considered as a particle entering the heterostructure and interacting with the barrier. In this way auxiliary reinforcement to the classical learning process of neural networks is achieved with minimal additional computational costs.",
            "score": 120.33038377761841
        },
        {
            "docid": "22000_10",
            "document": "Neural Darwinism . Criticism of Neural \"Darwinism\" was made by Francis Crick on the basis that neuronal groups are instructed by the environment rather than undergoing blind variation. A recent review by Fernando, Szathmary and Husbands explains why Edelman's Neural Darwinism is not Darwinian because it does not contain units of evolution as defined by John Maynard Smith. It is selectionist in that it satisfies the Price equation, but there is no mechanism in Edelman's theory that explains how information can be transferred between neuronal groups. A recent theory called Evolutionary Neurodynamics being developed by Eors Szathmary and Chrisantha Fernando has proposed several means by which true replication may take place in the brain. These neuronal models have been extended by Fernando in a later paper . In the most recent model, three plasticity mechanisms i) multiplicative STDP, ii) LTD, and iii) Heterosynaptic competition, are responsible for copying of connectivity patterns from one part of the brain to another. Exactly the same plasticity rules can explain experimental data for how infants do causal learning in the experiments conducted by Alison Gopnik. It has also been shown that by adding Hebbian learning to neuronal replicators the power of neuronal evolutionary computation may actually be greater than natural selection in organisms.",
            "score": 124.01244330406189
        },
        {
            "docid": "3717_60",
            "document": "Brain . Computational neuroscience encompasses two approaches: first, the use of computers to study the brain; second, the study of how brains perform computation. On one hand, it is possible to write a computer program to simulate the operation of a group of neurons by making use of systems of equations that describe their electrochemical activity; such simulations are known as \"biologically realistic neural networks\". On the other hand, it is possible to study algorithms for neural computation by simulating, or mathematically analyzing, the operations of simplified \"units\" that have some of the properties of neurons but abstract out much of their biological complexity. The computational functions of the brain are studied both by computer scientists and neuroscientists.",
            "score": 120.23565292358398
        },
        {
            "docid": "8402086_6",
            "document": "Computational neurogenetic modeling . Modeling of genes and proteins allows individual responses of neurons in an artificial neural network that mimic responses in biological nervous systems, such as division (adding new neurons to the artificial neural network), creation of proteins to expand their cell membrane and foster neurite outgrowth (and thus stronger connections with other neurons), up-regulate or down-regulate receptors at synapses (increasing or decreasing the weight (strength) of synaptic inputs), uptake more neurotransmitters, change into different types of neurons, or die due to necrosis or apoptosis. The creation and analysis of these networks can be divided into two sub-areas of research: the  gene up-regulation that is involved in the normal functions of a neuron, such as growth, metabolism, and synapsing; and the effects of mutated genes on neurons and cognitive functions.",
            "score": 75.00534987449646
        },
        {
            "docid": "2367309_8",
            "document": "State-dependent memory . At its most basic, state-dependent memory is the product of the strengthening of a particular synaptic pathway in the brain. A neural synapse is the space between brain cells, or neurons, that allows chemical signals to be passed from one neuron to another. Chemicals called neurotransmitters leave one cell, travel across the synapse, and are taken in by the next neuron through a neurotransmitter receptor. This creates a connection between the two neurons called a neural pathway. Memory relies on the strengthening of these neural pathways, associating one neuron with another. When we learn something, new pathways are made between neurons in the brain which then communicate through chemical signals. If these cells have a history of sending out certain signals under specific chemical conditions within the brain, they are then primed to work most effectively under similar circumstances. State-dependent memory happens when a new neural connection is made while the brain is in a specific chemical state - for instance, a child with ADHD learns their multiplication tables while on stimulant medication. Because their brain created these new connections related to multiplication tables while the brain was chemically affected by the stimulant medication, their neurons will be primed in the future to remember these facts best when the same levels of medication are present in the brain.",
            "score": 73.21003377437592
        },
        {
            "docid": "21523_5",
            "document": "Artificial neural network . The original goal of the ANN approach was to solve problems in the same way that a human brain would. However, over time, attention moved to performing specific tasks, leading to deviations from biology. ANNs have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis. Warren McCulloch and Walter Pitts (1943) created a computational model for neural networks based on mathematics and algorithms called threshold logic. This model paved the way for neural network research to split into two approaches. One approach focused on biological processes in the brain while the other focused on the application of neural networks to artificial intelligence. This work led to work on nerve networks and their link to finite automata.",
            "score": 128.26086497306824
        },
        {
            "docid": "233488_23",
            "document": "Machine learning . An artificial neural network (ANN) learning algorithm, usually called \"neural network\" (NN), is a learning algorithm that is vaguely inspired by biological neural networks. Computations are structured in terms of an interconnected group of artificial neurons, processing information using a connectionist approach to computation. Modern neural networks are non-linear statistical data modeling tools. They are usually used to model complex relationships between inputs and outputs, to find patterns in data, or to capture the statistical structure in an unknown joint probability distribution between observed variables.",
            "score": 104.56779456138611
        },
        {
            "docid": "10974486_12",
            "document": "Storage (memory) . The neural network model assumes that neurons in a neural network form a complex network with other neurons, forming a highly interconnected network; each neuron is characterized by the activation value, and the connection between two neurons is characterized by the weight value. Interaction between each neuron is characterized by the McCullough\u2013Pitts dynamical rule, and change of weight and connections between neurons resulting from learning is represented by the Hebbian learning rule.",
            "score": 46.23832559585571
        },
        {
            "docid": "33818014_18",
            "document": "Nervous system network models . There are three types of brain connectivity models of a network (Sporns, O. (2007)). \u201cAnatomical (or structural) connectivity\u201d describes a network with anatomical links having specified relationship between connected \u201cunits.\u201d If the dependent properties are stochastic, it is defined as \u201cfunctional connectivity.\u201d \u201cEffective connectivity\u201d has causal interactions between distinct units in the system. As stated earlier, brain connectivity can be described at three levels. At microlevel, it connects neurons through electrical or chemical synapses. A column of neurons can be considered as a unit in the mesolevel and regions of the brain comprising a large number of neurons and neuron populations as units in the macrolevel. The links in the latter case are the inter-regional pathways, forming large-scale connectivity. Figure 2 shows the three types of connectivity. The analysis is done using the directed graphs (see Sporns, O. (2007) and Hilgetag, C. C. (2002)). In the structural brain connectivity type, the connectivity is a sparse and directed graph. The functional brain connectivity has bidirectional graphs. The effective brain connectivity is bidirectional with interactive cause and effect relationships. Another representation of the connectivity is by matrix representation (See Sporns, O. (2007)). Hilgetag, C. C. (2002) describes the computational analysis of brain connectivity.",
            "score": 69.2275562286377
        },
        {
            "docid": "8402086_7",
            "document": "Computational neurogenetic modeling . An artificial neural network generally refers to any computational model that mimics the central nervous system, with capabilities such as learning and pattern recognition. With regards to computational neurogenetic modeling, however, it is often used to refer to those specifically designed for biological accuracy rather than computational efficiency. Individual neurons are the basic unit of an artificial neural network, with each neuron acting as a node. Each node receives weighted signals from other nodes that are either excitatory or inhibitory. To determine the output, a transfer function (or activation function) evaluates the sum of the weighted signals and, in some artificial neural networks, their input rate. Signal weights are strengthened (long-term potentiation) or weakened (long-term depression) depending on how synchronous the presynaptic and postsynaptic activation rates are (Hebbian theory).",
            "score": 87.36461591720581
        },
        {
            "docid": "33818014_21",
            "document": "Nervous system network models . As mentioned in Section 2.4, development of artificial neural network (ANN), or neural network as it is now called, started as simulation of biological neuron network and ended up using artificial neurons. Major development work has gone into industrial applications with learning process. Complex problems were addressed by simplifying the assumptions. Algorithms were developed to achieve a neurological related performance, such as learning from experience. Since the background and overview have been covered in the other internal references, the discussion here is limited to the types of models. The models are at the system or network level.",
            "score": 78.37347936630249
        },
        {
            "docid": "941909_26",
            "document": "Receptive field . The term receptive field is also used in the context of artificial neural networks, most often in relation to convolutional neural networks (CNNs). When used in this sense, the term adopts a meaning reminiscent of receptive fields in actual biological nervous systems. CNNs have a distinct architecture, designed to mimic the way in which real animal brains are understood to function; instead of having every neuron in each layer connect to all neurons in the next layer (Multilayer perceptron), the neurons are arranged in a 3-dimensional structure in such a way as to take into account the spatial relationships between different neurons with respect to the original data. Since CNNs are used primarily in the field of computer vision, the data that the neurons represent is typically an image; each input neuron represents one pixel from the original image. The first layer of neurons is composed of all the input neurons; neurons in the next layer will receive connections from some of the input neurons (pixels), but not all, as would be the case in a MLP and in other traditional neural networks. Hence, instead of having each neuron receive connections from all neurons in the previous layer, CNNs use a receptive field-like layout in which each neuron receives connections only from a subset of neurons in the previous (lower) layer. The receptive field of a neuron in one of the lower layers encompasses only a small area of the image, while the receptive field of a neuron in subsequent (higher) layers involves a combination of receptive fields from several (but not all) neurons in the layer before (i. e. a neuron in a higher layer \"looks\" at a larger portion of the image than does a neuron in a lower layer). In this way, each successive layer is capable of learning increasingly abstract features of the original image. The use of receptive fields in this fashion is thought to give CNNs an advantage in recognizing visual patterns when compared to other types of neural networks.",
            "score": 78.3189606666565
        },
        {
            "docid": "29648114_8",
            "document": "Andrea Brand . Brand has provided this \u201cplain English\u201d explanation of her work: \u201cOne of the goals of research in neurobiology is to repair or regenerate neurons after damage to the brain or spinal cord. Before we can understand how to repair the nervous system, however, we must first learn how the nervous system is put together. Of all the tissues and organs in the human body the nervous system is the most intricate and complex, consisting of more than one trillion neurons. These neurons make precise connections with each other to form functional networks that can transmit information at amazing speed over considerable distances.",
            "score": 81.21360874176025
        },
        {
            "docid": "2567511_17",
            "document": "Neural engineering . Scientists can use experimental observations of neuronal systems and theoretical and computational models of these systems to create Neural networks with the hopes of modeling neural systems in as realistic a manner as possible. Neural networks can be used for analyses to help design further neurotechnological devices. Specifically, researchers handle analytical or finite element modeling to determine nervous system control of movements and apply these techniques to help patients with brain injuries or disorders. Artificial neural networks can be built from theoretical and computational models and implemented on computers from theoretically devices equations or experimental results of observed behavior of neuronal systems. Models might represent ion concentration dynamics, channel kinetics, synaptic transmission, single neuron computation, oxygen metabolism, or application of dynamic system theory (LaPlaca et al. 2005). Liquid-based template assembly was used to engineer 3D neural networks from neuron-seeded microcarrier beads.",
            "score": 94.47332739830017
        },
        {
            "docid": "40158142_9",
            "document": "Nonlinear system identification . Artificial neural networks try loosely to imitate the network of neurons in the brain where computation takes place through a large number of simple processing elements. A typical neural network consists of a number of simple processing units interconnected to form a complex network. Layers of such units are arranged so that data is entered at the input layer and passes through either one or several intermediate layers before reaching the output layer. In supervised learning the network is trained by operating on the difference between the actual output and the desired output of the network, the prediction error, to change the connection strengths between the nodes. By iterating the weights are modified until the output error reaches an acceptable level. This process is called machine learning because the network adjusts the weights so that the output pattern is reproduced. Neural networks have been extensively studied and there are many excellent textbooks devoted to this topic in general, and more focussed textbooks which emphasise control and systems applications. There are two main problem types that can be studied using neural networks: static problems, and dynamic problems. Static problems include pattern recognition, classification, and approximation. Dynamic problems involve lagged variables and are more appropriate for system identification and related applications. Depending on the architecture of the network the training problem can be either nonlinear-in-the-parameters which involves optimisation or linear-in-the-parameters which can be solved using classical approaches. The training algorithms can be categorised into supervised, unsupervised, or reinforcement learning. Neural networks have excellent approximation properties but these are usually based on standard function approximation results using for example the Weierstrass Theorem that applies equally well to polynomials, rational functions, and other well-known models.  Neural networks have been applied extensively to system identification problems which involve nonlinear and dynamic relationships. However, classical neural networks are purely gross static approximating machines. There is no dynamics within the network. Hence when fitting dynamic models all the dynamics arise by allocating lagged inputs and outputs to the input layer of the network. The training procedure then produces the best static approximation that relates the lagged variables assigned to the input nodes to the output. There are more complex network architectures, including recurrent networks, that produce dynamics by introducing increasing orders of lagged variables to the input nodes. But in these cases it is very easy to over specify the lags and this can lead to over fitting and poor generalisation properties.  Neural networks have several advantages; they are conceptually simple, easy to train and to use, have excellent approximation properties, the concept of local and parallel processing is important and this provides integrity and fault tolerant behaviour. The biggest criticism of the classical neural network models is that the models produced are completely opaque and usually cannot be written down or analysed. It is therefore very difficult to know what is causing what, to analyse the model, or to compute dynamic characteristics from the model. Some of these points will not be relevant to all applications but they are for dynamic modelling.",
            "score": 111.62217140197754
        },
        {
            "docid": "21523_125",
            "document": "Artificial neural network . Many types of models are used, defined at different levels of abstraction and modeling different aspects of neural systems. They range from models of the short-term behavior of individual neurons, models of how the dynamics of neural circuitry arise from interactions between individual neurons and finally to models of how behavior can arise from abstract neural modules that represent complete subsystems. These include models of the long-term, and short-term plasticity, of neural systems and their relations to learning and memory from the individual neuron to the system level.",
            "score": 70.52314949035645
        },
        {
            "docid": "288292_16",
            "document": "Neuropsychology . Connectionism is the use of artificial neural networks to model specific cognitive processes using what are considered to be simplified but plausible models of how neurons operate. Once trained to perform a specific cognitive task these networks are often damaged or 'lesioned' to simulate brain injury or impairment in an attempt to understand and compare the results to the effects of brain injury in humans.",
            "score": 71.76278829574585
        },
        {
            "docid": "652038_16",
            "document": "Language of thought hypothesis . Connectionism is an approach to artificial intelligence that often accepts a lot of the same theoretical framework that LOTH accepts, namely that mental states are computational and causally efficacious and very often that they are representational. However, connectionism stresses the possibility of thinking machines, most often realized as artificial neural networks, an inter-connectional set of nodes, and describes mental states as able to create memory by modifying the strength of these connections over time. Some popular types of neural networks are interpretations of units, and learning algorithm. \"Units\" can be interpreted as neurons or groups of neurons. A learning algorithm is such that, over time, a change in connection weight is possible, allowing networks to modify their connections. Connectionist neural networks are able to change over time via their activation. An activation is a numerical value that represents any aspect of a unit that a neural network has at any time. Activation spreading is the spreading or taking over of other over time of the activation to all other units connected to the activated unit.",
            "score": 86.21392512321472
        },
        {
            "docid": "27622452_4",
            "document": "Julian Jack . Jack studies how nerve cells, or neurons, communicate with one another in the nervous system. He is also interested in understanding how chemical and electrical signals move through neural networks, such as the spinal cord or cerebral cortex. Although neurons form large networks, these cells do not actually touch each other. Instead, when the end of a nerve is activated it releases ions or chemicals known as neurotransmitters. Subsequently, these move across the gap, or synapse, between the neuron and the adjacent cell in the network, activating its receptors and perpetuating the signal. Jack applies theoretical and experimental approaches to research this process of synaptic transmission. This includes the use of neurophysiology methods to record bioelectrical activity and mathematical models to analyse the central and peripheral nervous systems. His work on neurotransmission is offering insight into disorders of the nervous system, such as Alzheimer\u2019s disease and multiple sclerosis, and has the potential to improve their diagnosis.",
            "score": 66.02153265476227
        },
        {
            "docid": "1729542_18",
            "document": "Neural network . A \"neural network\" (NN), in the case of artificial neurons called \"artificial neural network\" (ANN) or \"simulated neural network\" (SNN), is an interconnected group of natural or artificial neurons that uses a mathematical or computational model for information processing based on a connectionistic approach to computation. In most cases an ANN is an adaptive system that changes its structure based on external or internal information that flows through the network.",
            "score": 80.86648273468018
        },
        {
            "docid": "33818014_15",
            "document": "Nervous system network models . The concept of artificial neural network (ANN) was introduced by McColloch, W. S. & Pitts, W. (1943) for models based on behavior of biological neurons. Norbert Wiener (1961) gave this new field the popular name of cybernetics, whose principle is the interdisciplinary relationship among engineering, biology, control systems, brain functions, and computer science. With the computer science field advancing, the von Neumann-type computer was introduced early in the neuroscience study. But it was not suitable for symbolic processing, nondeterministic computations, dynamic executions, parallel distributed processing, and management of extensive knowledge bases, which are needed for biological neural network applications; and the direction of mind-like machine development changed to a learning machine. Computing technology has since advanced extensively and computational neuroscience is now able to handle mathematical models developed for biological neural network. Research and development are progressing in both artificial and biological neural networks including efforts to merge the two.",
            "score": 122.53153252601624
        }
    ],
    "r": [
        {
            "docid": "330102_24",
            "document": "Social learning theory . In modern field of computational intelligence, the social learning theory is adopted to develop a new computer optimization algorithm, the social learning algorithm. Emulating the observational learning and reinforcement behaviors, a virtual society deployed in the algorithm seeks the strongest behavioral patterns with the best outcome. This corresponds to searching for the best solution in solving optimization problems. Compared with other bio-inspired global optimization algorithms that mimic natural evolution or animal behaviors, the social learning algorithm has its prominent advantages. First, since the self-improvement through learning is more direct and rapid than the evolution process, the social learning algorithm can improve the efficiency of the algorithms mimicking natural evolution. Second, compared with the interaction and learning behaviors in animal groups, the social learning process of human beings exhibits a higher level of intelligence. By emulating human learning behaviors, it is possible to arrive at more effective optimizers than existing swarm intelligence algorithms. Experimental results have demonstrated the effectiveness and efficiency of the social learning algorithm, which has in turn also verified through computer simulations the outcomes of the social learning behavior in human society.",
            "score": 152.80831909179688
        },
        {
            "docid": "156423_14",
            "document": "Training . Researchers have developed training methods for artificial-intelligence devices as well. Evolutionary algorithms, including genetic programming and other methods of machine learning, use a system of feedback based on \"fitness functions\" to allow computer programs to determine how well an entity performs a task. The methods construct a series of programs, known as a \u201cpopulation\u201d of programs, and then automatically test them for \"fitness\", observing how well they perform the intended task. The system automatically generates new programs based on members of the population that perform the best. These new members replace programs that perform the worst. The procedure repeats until the achievement of optimum performance. In robotics, such a system can continue to run in real-time after initial training, allowing robots to adapt to new situations and to changes in themselves, for example, due to wear or damage. Researchers have also developed robots that can appear to mimic simple human behavior as a starting point for training.",
            "score": 146.2487335205078
        },
        {
            "docid": "149353_4",
            "document": "Computational biology . Computational Biology, which includes many aspects of bioinformatics, is the science of using biological data to develop algorithms or models to understand biological systems and relationships.  Until recently, biologists did not have access to very large amounts of data. This data has now become commonplace, particularly in molecular biology and genomics. Researchers were able to develop analytical methods for interpreting biological information, but were unable to share them quickly among colleagues.  Bioinformatics began to develop in the early 1970s. It was considered the science of analyzing informatics processes of various biological systems. At this time, research in artificial intelligence was using network models of the human brain in order to generate new algorithms. This use of biological data to develop other fields pushed biological researchers to revisit the idea of using computers to evaluate and compare large data sets. By 1982, information was being shared amongst researchers through the use of punch cards. The amount of data being shared began to grow exponentially by the end of the 1980s. This required the development of new computational methods in order to quickly analyze and interpret relevant information. Since the late 1990s, computational biology has become an important part of developing emerging technologies for the field of biology. The terms computational biology and evolutionary computation have a similar name, but are not to be confused. Unlike computational biology, evolutionary computation is not concerned with modeling and analyzing biological data. It instead creates algorithms based on the ideas of evolution across species. Sometimes referred to as genetic algorithms, the research of this field can be applied to computational biology. While evolutionary computation is not inherently a part of computational biology, Computational evolutionary biology is a subfield of it.",
            "score": 138.3655242919922
        },
        {
            "docid": "1164_9",
            "document": "Artificial intelligence . The earliest (and easiest to understand) approach to AI was symbolism (such as formal logic): \"If an otherwise healthy adult has a fever, then they may have influenza\". A second, more general, approach is Bayesian inference: \"If the current patient has a fever, adjust the probability they have influenza in such-and-such way\". The third major approach, extremely popular in routine business AI applications, are analogizers such as SVM and nearest-neighbor: \"After examining the records of known past patients whose temperature, symptoms, age, and other factors mostly match the current patient, X% of those patients turned out to have influenza\". A fourth approach is harder to intuitively understand, but is inspired by how the brain's machinery works: the artificial neural network approach uses artificial \"neurons\" that can learn by comparing itself to the desired output and altering the strengths of the connections between its internal neurons to \"reinforce\" connections that seemed to be useful. These four main approaches can overlap with each other and with evolutionary systems; for example, neural nets can learn to make inferences, to generalize, and to make analogies. Some systems implicitly or explicitly use multiple of these approaches, alongside many other AI and non-AI algorithms; the best approach is often different depending on the problem. Learning algorithms work on the basis that strategies, algorithms, and inferences that worked well in the past are likely to continue working well in the future. These inferences can be obvious, such as \"since the sun rose every morning for the last 10,000 days, it will probably rise tomorrow morning as well\". They can be nuanced, such as \"X% of families have geographically separate species with color variants, so there is an Y% chance that undiscovered black swans exist\". Learners also work on the basis of \"Occam's razor\": The simplest theory that explains the data is the likeliest. Therefore, to be successful, a learner must be designed such that it prefers simpler theories to complex theories, except in cases where the complex theory is proven substantially better. Settling on a bad, overly complex theory gerrymandered to fit all the past training data is known as overfitting. Many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data, but penalizing the theory in accordance with how complex the theory is. Besides classic overfitting, learners can also disappoint by \"learning the wrong lesson\". A toy example is that an image classifier trained only on pictures of brown horses and black cats might conclude that all brown patches are likely to be horses. A real-world example is that, unlike humans, current image classifiers don't determine the spatial relationship between components of the picture; instead, they learn abstract patterns of pixels that humans are oblivious to, but that linearly correlate with images of certain types of real objects. Faintly superimposing such a pattern on a legitimate image results in an \"adversarial\" image that the system misclassifies. Compared with humans, existing AI lacks several features of human \"commonsense reasoning\"; most notably, humans have powerful mechanisms for reasoning about \"na\u00efve physics\" such as space, time, and physical interactions. This enables even young children to easily make inferences like \"If I roll this pen off a table, it will fall on the floor\". Humans also have a powerful mechanism of \"folk psychology\" that helps them to interpret natural-language sentences such as \"The city councilmen refused the demonstrators a permit because they advocated violence\". (A generic AI has difficulty inferring whether the councilmen or the demonstrators are the ones alleged to be advocating violence.) This lack of \"common knowledge\" means that AI often makes different mistakes than humans make, in ways that can seem incomprehensible. For example, existing self-driving cars cannot reason about the location nor the intentions of pedestrians in the exact way that humans do, and instead must use non-human modes of reasoning to avoid accidents.",
            "score": 137.048583984375
        },
        {
            "docid": "55962927_3",
            "document": "AI aftermath scenarios . Most scientists believe that AI research will at some point lead to the creation of machines that are as intelligent, or more intelligent, than human beings in every domain of interest. There is no physical law precluding particles from being organised in ways that perform even more advanced computations than the arrangements of particles in human brains; therefore superintelligence is physically possible. In addition to potential algorithmic improvements over human brains, a digital brain can be many orders of magnitude larger and faster than a human brain, which was constrained in size by evolution to be small enough to fit through a birth canal. While there is no consensus on \"when\" artificial intelligence will outperform humans, many scholars argue that whenever it does happen, the introduction of a second species of intelligent life onto the planet will have far-reaching implications. Scholars often disagree with one another both about what types of post-AI scenarios are \"most likely\", and about what types of post-AI scenarios would be \"most desirable\". Finally, some dissenters argue that AI will never become as intelligent as humans, for example because the human race will already likely have destroyed itself before research has time to advance sufficiently to create artificial general intelligence.",
            "score": 134.59336853027344
        },
        {
            "docid": "8402086_18",
            "document": "Computational neurogenetic modeling . Because the amount of data on the interplay of genes and neurons and their effects is not enough to construct a rigorous model,  evolutionary computation is used to optimize artificial neural networks and gene regulatory networks, a common technique being the genetic algorithm. A genetic algorithm is a process that can be used to refine models by mimicking the process of natural selection observed in biological ecosystems. The primary advantages are that, due to not requiring derivative information, it can be applied to black box problems and multimodal optimization. The typical process for using genetic algorithms to refine a gene  regulatory network is: first, create a population; next, to create offspring via a crossover operation and  evaluate their fitness; then, on a group chosen for high fitness, simulate mutation via a mutation operator;  finally, taking the now mutated group, repeat this process until a desired level of fitness is demonstrated. Methods by which artificial neural networks may alter their structure without simulated mutation and fitness selection have been developed. A dynamically evolving neural network is one approach, as the creation of new connections and new neurons can  be modeled as the system adapts to new data. This enables the network to evolve in modeling accuracy without simulated natural selection. One method by which dynamically evolving networks may be optimized, called evolving layer neuron aggregation, combines neurons with sufficiently similar input weights into one neuron. This can take place during the training of the network, referred to as online aggregation, or between periods of training, referred to as offline aggregation. Experiments have suggested that offline aggregation is more efficient.",
            "score": 133.6306915283203
        },
        {
            "docid": "1422176_7",
            "document": "Developmental robotics . Developmental robotics emerged at the crossroads of several research communities including embodied artificial intelligence, enactive and dynamical systems cognitive science, connectionism. Starting from the essential idea that learning and development happen as the self-organized result of the dynamical interactions among brains, bodies and their physical and social environment, and trying to understand how this self- organization can be harnessed to provide task-independent lifelong learning of skills of increasing complexity, developmental robotics strongly interacts with fields such as developmental psychology, developmental and cognitive neuroscience, developmental biology (embryology), evolutionary biology, and cognitive linguistics. As many of the theories coming from these sciences are verbal and/or descriptive, this implies a crucial formalization and computational modeling activity in developmental robotics. These computational models are then not only used as ways to explore how to build more versatile and adaptive machines, but also as a way to evaluate their coherence and possibly explore alternative explanations for understanding biological development.",
            "score": 133.45359802246094
        },
        {
            "docid": "18689983_14",
            "document": "Human-based computation game . \"Foldit\", while also a GWAP, has a different type of method for tapping the collective human brain. This game challenges players to use their human intuition of 3-dimensional space to help with protein folding algorithms. Unlike the ESP game, which focuses on the results that humans are able to provide, Foldit is trying to understand how humans approach complicated 3 dimensional objects. By 'watching' how humans play the game, researchers hope to be able to improve their own computer programs. Instead of simply performing tasks that computers cannot do, this GWAP is asking humans to help make current machine algorithms better.",
            "score": 131.6366729736328
        },
        {
            "docid": "17667553_6",
            "document": "Hyper-heuristic . Despite the significant progress in building search methodologies for a wide variety of application areas so far, such approaches still require specialists to integrate their expertise in a given problem domain. Many researchers from computer science, artificial intelligence and operational research have already acknowledged the need for developing automated systems to replace the role of a human expert in such situations. One of the main ideas for automating the design of heuristics requires the incorporation of machine learning mechanisms into algorithms to adaptively guide the search. Both learning and adaptation processes can be realised on-line or off-line, and be based on constructive or perturbative heuristics.",
            "score": 130.80087280273438
        },
        {
            "docid": "34590724_24",
            "document": "Intelligent design and science . As a means of criticism, certain skeptics have pointed to a challenge of intelligent design derived from the study of artificial intelligence. The criticism is a counter to intelligent design claims about what makes a design intelligent, specifically that \"no preprogrammed device can be truly intelligent, that intelligence is irreducible to natural processes\". This claim is similar in type to an assumption of Cartesian dualism that posits a strict separation between \"mind\" and the material Universe. However, in studies of artificial intelligence, while there is an implicit assumption that supposed \"intelligence\" or creativity of a computer program is determined by the capabilities given to it by the computer programmer, artificial intelligence need not be bound to an inflexible system of rules. Rather, if a computer program can access randomness as a function, this effectively allows for a flexible, creative, and adaptive intelligence. Evolutionary algorithms, a subfield of machine learning (itself a subfield of artificial intelligence), have been used to mathematically demonstrate that randomness and selection can be used to \"evolve\" complex, highly adapted structures that are not explicitly designed by a programmer. Evolutionary algorithms use the Darwinian metaphor of random mutation, selection and the survival of the fittest to solve diverse mathematical and scientific problems that are usually not solvable using conventional methods. Intelligence derived from randomness is essentially indistinguishable from the \"innate\" intelligence associated with biological organisms, and poses a challenge to the intelligent design conception that intelligence itself necessarily requires a designer. Cognitive science continues to investigate the nature of intelligence along these lines of inquiry. The intelligent design community, for the most part, relies on the assumption that intelligence is readily apparent as a fundamental and basic property of complex systems.",
            "score": 130.666259765625
        },
        {
            "docid": "11270885_3",
            "document": "Fuzzy cognitive map . Fuzzy cognitive maps are signed fuzzy digraphs. They may look at first blush like Hasse diagrams but they are not. Spreadsheets or tables are used to map FCMs into matrices for further computation. FCM is a technique used for causal knowledge acquisition and representation, it supports causal knowledge reasoning process and belong to the neuro-fuzzy system that aim at solving decision making problems, modeling and simulate complex systems .  Learning algorithms have been proposed for training and updating FCMs weights mostly based on ideas coming from the field of Artificial Neural Networks . Adaptation and learning methodologies used to adapt the FCM model and adjust its weights. Kosko and Dickerson (Dickerson & Kosko, 1994) suggested the Differential Hebbian Learning (DHL) to train FCM. There have been proposed algorithms based on the initial Hebbian algorithm; others algorithms come from the field of genetic algorithms, swarm intelligence and evolutionary computation. Learning algorithms are used to overcome the shortcomings that the traditional FCM present i.e. decreasing the human intervention by suggested automated FCM candidates; or by activating only the most relevant concepts every execution time; or by making models more transparent and dynamic.",
            "score": 130.1291961669922
        },
        {
            "docid": "25345530_44",
            "document": "Models of neural computation . Genetic algorithms are used to evolve neural (and sometimes body) properties in a model brain-body-environment system so as to exhibit some desired behavioral performance. The evolved agents can then be subjected to a detailed analysis to uncover their principles of operation. Evolutionary approaches are particularly useful for exploring spaces of possible solutions to a given behavioral task because these approaches minimize a priori assumptions about how a given behavior ought to be instantiated. They can also be useful for exploring different ways to complete a computational neuroethology model when only partial neural circuitry is available for a biological system of interest.",
            "score": 129.613525390625
        },
        {
            "docid": "2958015_19",
            "document": "Philosophy of artificial intelligence . Russell and Norvig point out that, in the years since Dreyfus published his critique, progress has been made towards discovering the \"rules\" that govern unconscious reasoning. The situated movement in robotics research attempts to capture our unconscious skills at perception and attention. Computational intelligence paradigms, such as neural nets, evolutionary algorithms and so on are mostly directed at simulated unconscious reasoning and learning. Statistical approaches to AI can make predictions which approach the accuracy of human intuitive guesses. Research into commonsense knowledge has focused on reproducing the \"background\" or context of knowledge. In fact, AI research in general has moved away from high level symbol manipulation or \"GOFAI\", towards new models that are intended to capture more of our \"unconscious\" reasoning . Historian and AI researcher Daniel Crevier wrote that \"time has proven the accuracy and perceptiveness of some of Dreyfus's comments. Had he formulated them less aggressively, constructive actions they suggested might have been taken much earlier.\"",
            "score": 128.7599639892578
        },
        {
            "docid": "21523_5",
            "document": "Artificial neural network . The original goal of the ANN approach was to solve problems in the same way that a human brain would. However, over time, attention moved to performing specific tasks, leading to deviations from biology. ANNs have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis. Warren McCulloch and Walter Pitts (1943) created a computational model for neural networks based on mathematics and algorithms called threshold logic. This model paved the way for neural network research to split into two approaches. One approach focused on biological processes in the brain while the other focused on the application of neural networks to artificial intelligence. This work led to work on nerve networks and their link to finite automata.",
            "score": 128.2608642578125
        },
        {
            "docid": "4253446_4",
            "document": "Human-based computation . Human-based computation (apart from the historical meaning of \"computer\") research has its origins in the early work on interactive evolutionary computation (EC). The idea behind interactive evolutionary algorithms is due to Richard Dawkins. In the Biomorphs software accompanying his book \"The Blind Watchmaker\" (Dawkins, 1986) the preference of a human experimenter is used to guide the evolution of two-dimensional sets of line segments. In essence, this program asks a human to be the fitness function of an evolutionary algorithm, so that the algorithm can use human visual perception and aesthetic judgment to do something that a normal evolutionary algorithm cannot do. However, it is difficult to get enough evaluations from a single human if we want to evolve more complex shapes. Victor Johnston and Karl Sims extended this concept by harnessing the power of many people for fitness evaluation (Caldwell and Johnston, 1991; Sims, 1991). As a result, their programs could evolve beautiful faces and pieces of art appealing to public. These programs effectively reversed the common interaction between computers and humans. In these programs, the computer is no longer an agent of its user, but instead, a coordinator aggregating efforts of many human evaluators. These and other similar research efforts became the topic of research in aesthetic selection or interactive evolutionary computation (Takagi, 2001), however the scope of this research was limited to outsourcing evaluation and, as a result, it was not fully exploring the full potential of the outsourcing.",
            "score": 127.28568267822266
        },
        {
            "docid": "31058472_8",
            "document": "Adaptive memory . One major finding is that survival processing has been shown to yield better retention than imagery, self-reference and pleasantness, which are all considered to be among the best conditions for remembering learned information. Otgaar, Smeets and van Bergen hypothesized that since visual processing developed earlier than language, there ought to be a survival advantage for images as well as words, and they found such an advantage. Grasslands survival scenarios showed higher retention than near-identical scenarios in which the word 'grasslands' was replaced with 'city' and the word 'predators' replaced with 'attackers'. It is suspected that this result is due to the human mind being scenarios relevant to our species' ancestral past, even though threats present in a modern urban society are far more relevant today. There was a greater memory recall in both the ancestral and modern survival conditions when compared to pleasantness control conditions, but only the ancestral condition presented significantly greater word recall. Both conditions are fitness-relevant, but there was no memory enhancement for survival processing in the modern context. Additionally, as females typically performed gathering tasks over the span of human evolution, and males performed hunting tasks, research into this gender dichotomy was conducted. No significant data were found. The implications of this research lie in helping to understand how the mind evolved and how it works. The idea that we are able to retain information most relevant to our own survival provides a foundation of research for empirical studying of memory through an evolutionary lens. Understanding the circumstances when memory is at its best can help study the functions of memory as a whole, and help understand what memory is capable of.",
            "score": 127.15739440917969
        },
        {
            "docid": "25839999_11",
            "document": "Computer-automated design . To reduce the search time, the biologically-inspired evolutionary algorithm (EA) can be used instead, which is a (non-deterministic) polynomial algorithm. The EA based multi-objective \"search team\" can be interfaced with an existing CAD simulation package in a batch mode. The EA encodes the design parameters (encoding being necessary if some parameters are non-numerical) to refine multiple candidates through parallel and interactive search. In the search process, 'selection' is performed using 'survival of the fittest' \"a posteriori\" learning. To obtain the next 'generation' of possible solutions, some parameter values are exchanged between two candidates (by an operation called 'crossover') and new values introduced (by an operation called 'mutation'). This way, the evolutionary technique makes use of past trial information in a similarly intelligent manner to the human designer.",
            "score": 126.99723052978516
        },
        {
            "docid": "4253446_6",
            "document": "Human-based computation . Finally, Human-based genetic algorithm (HBGA) encourages human participation in multiple different roles. Humans are not limited to the role of evaluator or some other predefined role, but can choose to perform a more diverse set of tasks. In particular, they can contribute their innovative solutions into the evolutionary process, make incremental changes to existing solutions, and perform intelligent recombination. In short, HBGA allows humans to participate in all operations of a typical genetic algorithm. As a result of this, HBGA can process solutions for which there are no computational innovation operators available, for example, natural languages. Thus, HBGA obviated the need for a fixed representational scheme that was a limiting factor of both standard and interactive EC. These algorithms can also be viewed as novel forms of social organization coordinated by a computer (Kosorukoff and Goldberg, 2002).",
            "score": 126.45974731445312
        },
        {
            "docid": "37255573_4",
            "document": "IBM Research \u2013 Ireland . IBM Research \u2013 Ireland\u2019s strategic mission has now evolved to focus on partnering with industry clients to make better decisions using cognitive computing, Internet of Things (IoT), natural language processing and machine learning. The lab also has specific expertise in cognitive integrated healthcare. The lab's research projects include developing cognitive Internet of Things (IoT) systems such as self-aware buildings which learn and adapt to human behavior and connected cars that have context-aware in-car reasoning systems. Researchers are also using cognitive computing and Internet of Things (IoT) to digitally integrate healthcare for patients with chronic diseases. Teams dedicated to interactive reasoning and cognitive analytics are developing algorithms that can assist humans in making decisions. Our data-centric computing experts are building computing infrastructure for edge devices.",
            "score": 126.33821105957031
        },
        {
            "docid": "22000_10",
            "document": "Neural Darwinism . Criticism of Neural \"Darwinism\" was made by Francis Crick on the basis that neuronal groups are instructed by the environment rather than undergoing blind variation. A recent review by Fernando, Szathmary and Husbands explains why Edelman's Neural Darwinism is not Darwinian because it does not contain units of evolution as defined by John Maynard Smith. It is selectionist in that it satisfies the Price equation, but there is no mechanism in Edelman's theory that explains how information can be transferred between neuronal groups. A recent theory called Evolutionary Neurodynamics being developed by Eors Szathmary and Chrisantha Fernando has proposed several means by which true replication may take place in the brain. These neuronal models have been extended by Fernando in a later paper . In the most recent model, three plasticity mechanisms i) multiplicative STDP, ii) LTD, and iii) Heterosynaptic competition, are responsible for copying of connectivity patterns from one part of the brain to another. Exactly the same plasticity rules can explain experimental data for how infants do causal learning in the experiments conducted by Alison Gopnik. It has also been shown that by adding Hebbian learning to neuronal replicators the power of neuronal evolutionary computation may actually be greater than natural selection in organisms.",
            "score": 124.01244354248047
        },
        {
            "docid": "1164_4",
            "document": "Artificial intelligence . Thought-capable artificial beings appeared as storytelling devices in antiquity, and have been common in fiction, as in Mary Shelley's \"Frankenstein\" or Karel \u010capek's \"R.U.R. (Rossum's Universal Robots)\". These characters and their fates raised many of the same issues now discussed in the ethics of artificial intelligence. The study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of mathematical logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate any conceivable act of mathematical deduction. This insight, that digital computers can simulate any process of formal reasoning, is known as the Church\u2013Turing thesis. Along with concurrent discoveries in neurobiology, information theory and cybernetics, this led researchers to consider the possibility of building an electronic brain. Turing proposed that \"if a human could not distinguish between responses from a machine and a human, the machine could be considered \u201cintelligent\". The first work that is now generally recognized as AI was McCullouch and Pitts' 1943 formal design for Turing-complete \"artificial neurons\". The field of AI research was born at a workshop at Dartmouth College in 1956. Attendees Allen Newell (CMU), Herbert Simon (CMU), John McCarthy (MIT), Marvin Minsky (MIT) and Arthur Samuel (IBM) became the founders and leaders of AI research. They and their students produced programs that the press described as \"astonishing\": computers were learning checkers strategies (c. 1954) (and by 1959 were reportedly playing better than the average human), solving word problems in algebra, proving logical theorems (Logic Theorist, first run c. 1956) and speaking English. By the middle of the 1960s, research in the U.S. was heavily funded by the Department of Defense and laboratories had been established around the world. AI's founders were optimistic about the future: Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\". Marvin Minsky agreed, writing, \"within a generation\u00a0... the problem of creating 'artificial intelligence' will substantially be solved\". They failed to recognize the difficulty of some of the remaining tasks. Progress slowed and in 1974, in response to the criticism of Sir James Lighthill and ongoing pressure from the US Congress to fund more productive projects, both the U.S. and British governments cut off exploratory research in AI. The next few years would later be called an \"AI winter\", a period when obtaining funding for AI projects was difficult. In the early 1980s, AI research was revived by the commercial success of expert systems, a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985 the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S and British governments to restore funding for academic research. However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting hiatus began. In the late 1990s and early 21st century, AI began to be used for logistics, data mining, medical diagnosis and other areas. The success was due to increasing computational power (see Moore's law), greater emphasis on solving specific problems, new ties between AI and other fields (such as statistics, economics and mathematics), and a commitment by researchers to mathematical methods and scientific standards. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov on 11 May 1997. In 2011, a \"Jeopardy!\" quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy champions, Brad Rutter and Ken Jennings, by a significant margin. Faster computers, algorithmic improvements, and access to large amounts of data enabled advances in machine learning and perception; data-hungry deep learning methods started to dominate accuracy benchmarks around 2012. The Kinect, which provides a 3D body\u2013motion interface for the Xbox 360 and the Xbox One use algorithms that emerged from lengthy AI research as do intelligent personal assistants in smartphones. In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. In the 2017 Future of Go Summit, AlphaGo won a three-game match with Ke Jie, who at the time continuously held the world No. 1 ranking for two years. This marked the completion of a significant milestone in the development of Artificial Intelligence as Go is an extremely complex game, more so than Chess.",
            "score": 123.83797454833984
        },
        {
            "docid": "27075922_12",
            "document": "Natural computing . The study of evolutionary systems has historically evolved along three main branches:  Evolution strategies provide a solution to parameter optimization problems for real-valued as well as discrete and mixed types of parameters.  Evolutionary programming originally aimed at creating optimal \"intelligent agents\" modelled, e.g., as finite state machines. Genetic algorithms applied the idea of evolutionary computation to the problem of finding a (nearly-)optimal solution to a given problem. Genetic algorithms initially consisted of an input population of individuals encoded as fixed-length bit strings, the genetic operators mutation (bit flips) and recombination (combination of a prefix of a parent with the suffix of the other), and a problem-dependent fitness function. Genetic algorithms have been used to optimize computer programs, called genetic programming, and today they are also applied to real-valued parameter optimization problems as well as to many types of combinatorial tasks.",
            "score": 123.46011352539062
        },
        {
            "docid": "1050195_9",
            "document": "Evolutionary robotics . Many of the commonly used machine learning algorithms require a set of training examples consisting of both a hypothetical input and a desired answer. In many robot learning applications the desired answer is an action for the robot to take. These actions are usually not known explicitly a priori, instead the robot can, at best, receive a value indicating the success or failure of a given action taken. Evolutionary algorithms are natural solutions to this sort of problem framework, as the fitness function need only encode the success or failure of a given controller, rather than the precise actions the controller should have taken. An alternative to the use of evolutionary computation in robot learning is the use of other forms of reinforcement learning, such as q-learning, to learn the fitness of any particular action, and then use predicted fitness values indirectly to create a controller.",
            "score": 122.68306732177734
        },
        {
            "docid": "33818014_15",
            "document": "Nervous system network models . The concept of artificial neural network (ANN) was introduced by McColloch, W. S. & Pitts, W. (1943) for models based on behavior of biological neurons. Norbert Wiener (1961) gave this new field the popular name of cybernetics, whose principle is the interdisciplinary relationship among engineering, biology, control systems, brain functions, and computer science. With the computer science field advancing, the von Neumann-type computer was introduced early in the neuroscience study. But it was not suitable for symbolic processing, nondeterministic computations, dynamic executions, parallel distributed processing, and management of extensive knowledge bases, which are needed for biological neural network applications; and the direction of mind-like machine development changed to a learning machine. Computing technology has since advanced extensively and computational neuroscience is now able to handle mathematical models developed for biological neural network. Research and development are progressing in both artificial and biological neural networks including efforts to merge the two.",
            "score": 122.53153228759766
        },
        {
            "docid": "43278016_12",
            "document": "Joe Z. Tsien . In 2015, Tsien developed the \"Theory of Connectivity\" to explain the design principle upon which evolution and development may construct the brain to be capable of generating intelligence. This theory has made six predictions which have received supportive evidence by a recent set of experiments on both the mouse brain and hamster brain. At its core, the \"Theory of Connectivity\" predicts that the cell assemblies in the brain are not random, rather they should conform to the power-of-two-based equation, N = 2 - 1, to form the pre-configured building block termed as the functional connectivity motif (FCM). Instead of using a single neuron as the computational unit in some extremely simple brains, the theory denotes that in most brains, a group of neurons exhibiting similar tuning properties, termed as a neural clique, should serve as the basic computing processing unit (CPU). Defined by the power-of-two-based equation, N = 2 - 1, each FCM consists of principal-projection neuron cliques (N), ranging from those specific cliques receiving specific information inputs (i) to those general and sub-general cliques receiving various combinatorial convergent inputs. As the evolutionarily conserved logic, its validation requires experimental demonstrations of the following three major properties: 1) Anatomical prevalence - FCMs are prevalent across neural circuits, regardless of gross anatomical shapes; 2) Species conservancy - FCMs are conserved across different animal species; and 3) Cognitive universality - FCMs serve as a universal computational logic at the cell-assembly level for processing a variety of cognitive experiences and flexible behaviors. More importantly, this \"Theory of Connectivity\" further predicts that the specific-to-general combinatorial connectivity pattern within FCMs should be pre-configured by evolution, and emerge innately from development as the brain's computational primitives. This proposed design principle can also explain the general purpose and computational algorithm of the neocortex. This proposed design principle of intelligence can be examined via various experiments and also be modeled by neuromorphic engineers and computer scientists. However, Dr. Joe Tsien cautions that artificial general intelligence based on the brain's principles can come with great benefits and, potentially, even greater risks.",
            "score": 121.90058135986328
        },
        {
            "docid": "2384297_6",
            "document": "The Lives of a Cell: Notes of a Biology Watcher . This essay focuses on how connected humanity is to nature and how we must make strides to understand our role. Thomas argues that even our own bodies are not solely ours since the mitochondria and other organelles are descended from other organisms. He creates a metaphor of the Earth as a giant cell itself with humans just as one part of a vast system. Astronauts must be decontaminated before they are allowed to interact on Earth. Thomas states that this is an act of \u201chuman chauvinism.\u201d Most organisms on Earth are symbiotic or, if harmful, have both adapted to warn the other. All organisms on Earth are interdependent and a stray virus or bacteria from the moon will not be adapted to harm us since it is not part of this connection. Bacteria are interconnected to the point where some cannot survive without others and some even live within others. We must recognize how interconnected even the smallest organisms are on Earth; especially if we must interact with life outside our planet. Thomas introduces one of his key metaphors of humans behaving like ants. He suggests that this metaphor is not used because humans do not like to be compared to insects that, as a society, can function as an organism. There are many examples of animals acting as a large organism when in large groups from termites and slime molds to birds and fish. Thomas argues that the communication of results in science puts humans in the same model as these other species. As all scientists communicate and build on each other\u2019s work in order to explore that which we do not know. Humans fear pheromones because we believe we have gone above the basic secretion of chemicals in our communication. However, there are signs that point to humans relying on pheromones as well as our most technological forms of communication. Thomas shows pheromones in the animal world with examples of moths and fish. He then goes on to explain what impact pheromones in humans could have on the future such as in the perfume industry and finding histocompatible donors. Music is the only form of communication that saves us from an overwhelming amount of small talk. This is not only a human phenomenon, but happens throughout the animal world. Thomas makes examples of animals from termites and earthworms to gorillas and alligators that perform some sort of rhythmic noise making that can be interpreted as music if we had full range of hearing. From the vast number of animals that participate in music it is clear that the need to make music is a fundamental characteristic of biology. Thomas proposes that the animal world is continuing a musical memory that has been going since the beginning of time. Thomas argues that even though we have the technological advancements to destroy the Earth that we do not know near enough about the world in which we live. To solve this problem he suggests that we should not be able to fire nuclear weapons without being able to explain one living thing fully. The organism that Thomas proposes is the protozoan Myxotricha paradoxa. There is information known about this protozoan that lives in the digestive tract of Australian termites but with more study it could be a model for how our cells developed. It is seen throughout nature that organisms cooperate and progress into more complex forms. We cannot destroy vast amounts of Earth with nuclear weapons until we understand how interconnected we all are. Thomas presents the three levels of technology in medicine: \u201cnontechnology\u201d that helps patients with diseases that are not well understood but does not help solve the underlying mechanisms of the disease, \u201chalfway technology\u201d that makes up for disease or postpones death for diseases whose courses we cannot do much about, and \u201chigh technology\u201d that from understanding the mechanism of the disease we are now able to cure. When looking at the costs of the three different technologies they are all needed, but once a \u201chigh technology\u201d is found for a disease the benefits outweigh the costs of studying the mechanism of the disease so thoroughly. Thomas suggests that in order to save money in health care, the highest priority in funding should be given to basic research. Humans leave a trace of chemicals in every place they go and on everything they touch. Other animals use signaling mechanisms to leave trails or identify each other. The sense of smell is an important sense in using these mechanisms, but it is still not well understood. Humans, compared to the rest of the animal world, do not have a good olfactory sense though we may be better than we first assume. Johannes Kepler once argued that the Earth is an immense organism itself, with chemical signals spreading across the globe through various organisms in order to keep the world functioning and well informed. Tau Ceti is a nearby sun-like star that we are on the verge of being able to begin making contact with, as well as other celestial bodies, to search for life. We have been attracted to the vast regions of space outside our Earth bubble and what they could hold. If extraterrestrial life is found, it scientifically would make sense, but the social impact of no longer being unique would give humans a new sense of community. The question of what information to send out is answered by Thomas by sending music, specifically Bach. It is timeless and the best language we have to express who we are. If possible Thomas also suggests sending art. However, the questions of what to send will not stop once we receive a reply. As humans we always evade death, despite how it is a natural part of our lives. Unless it is far removed, as in war or on television, then we can discuss it without a problem. It is a subconscious effort that by not thinking about death we may continue to live. Nevertheless, even if we cured all diseases we still would die one day. We must not fear death and research the dying process just as we would any other biological process. Most people who have a near death experience do not recall any pain or fear. It is perhaps the loss of consciousness that people fear more than death itself. Thomas returns to his pondering of the social behaviors of insects in this essay. He discusses the change in behavior of insects in groups and singular insects. We have used insects and their behavior to convey lessons, rules, and virtues and now they have been used in art. Thomas describes an art exhibit with living ants, surrounded by humans who act in a similar manner to the ants themselves. Thomas praises the Marine Biological Laboratory as \u201ca paradigm, a human institution possessed of a life of its own, self-regenerating, touched all around by human meddle but consistently improved, embellished by it.\u201d It attracts the brightest minds and makes great strides in science autonomously. Thomas paints pictures with his description of scientists covering the beach with their diagrams and making \u201cmusic\u201d of discussion after a lecture at the MBL. Humans have to learn how to walk, skip, and ride a bicycle but inside our bodies perform specific manipulations from birth that we do not need to learn. There is new research that suggests humans may be able to change these inner processes with teaching. Thomas reasons that his body has been functioning fine without him trying to control every little process so he will let it continue to do so. He suggests to try the exact opposite and try to disconnect from your body altogether. The biologic revolution is filling in the gaps in understanding about how our cells function. As we begin to understand more about organelles it is clear that they are not originally created from our cells. Mitochondria and chloroplasts most likely have a bacterial ancestry and flagellae and cilia most likely were once spirochetes. It is not necessarily a master-slave relationship that we have with our organelles, but one where their ancestors found an easy way to stay protected and secure. We have brought them along with us as we evolved and yet we do not understand them completely. Organelles and eukaryotic cells are one of the most established symbiotic relationships. We treat bacteria as an ever present enemy even though there are only a small number that actually cause disease, and by accident in most cases. Bacteria normally do not gain anything by causing illness or death in their hosts. Our illness is mostly caused by our immune system doing too great of a job in response to bacteria in our system. The strength of our response is not necessary for most cases, but remains from a primitive time. Health care has become the new name for medicine though this is a misnomer since illness and death cannot be totally eradicated. Thomas argues that to understand how medicine should be used we should look to those internists that are involved in the system. Most things get better in a short while by themselves, so we should no longer be instilling in the public a constant fear of failed health. This will be the best way to solve the problem of funding health care since people will only use it when it is necessary. There are different degrees of social behavior in animals. However, it is not clear where humans fit on the scale. Most signs point that we are above the social behavior of ants and bees that go about a singular task as a whole community. Language is the one trait that brings us to the level of such animals. All humans engage in language and are born with the understanding of language. Language, and perhaps along with art and music, is the core of our social behavior. The human mind comes with the understanding of how to deal with and use language. We store up information as a cell stores energy, though with language, this information can be put to further use. Another main difference between language and other communication systems in biology is the ambiguity that is a necessity in language which would cause the other communication systems to fail. Death is not supposed to happen in the open, along highways and in sight of others. Everything is in the process of dying all around us, though we keep it hidden from our sight and minds. Death is part of the cycle and we need to understand we are part of a larger process. The process of dying is necessary for the birth of the new and we will all experience it together. Thomas explains science as a wild manifestation of human behavior. He explains that science and discovery is a compulsion that scientists seem to have written in their very genes. Science cannot be organized and forced; it must be free to go where the next question leads. It is similar to a bee hive in some sense, but also to animals on a hunt. The activity is never ending and the conglomeration of minds always yearning for the next discovery cannot be kept under control. How humans approach nature has been changing throughout recent years. We used to view nature as ours to control and use to better mankind. Now we have moved away from this view and seen that we are part of the larger system and not the ruler of it. However Thomas argues that we must see ourselves as \u201cindispensable elements of nature\u201d and work for the betterment of the Earth but also be able to protect ourselves. This essay focuses on the tribe of Iks in northern Uganda. Thomas comments on an anthropologist\u2019s report on the Iks that argues that they represent the basic elements of mankind. Thomas instead thinks that each Ik acts as a group and that by observing the whole tribe of Iks you can see how we behave in groups ranging from committees to nations. In order to improve upon our group interactions, we must stay human even when in masses. Computers are approaching humanity, but they will never be able to fully replace us for they will not be able to replicate our collective behavior because we do not understand it ourselves. We are involved in a never ending transfer of information and collective thinking. This is the cause of the unpredictability in our future. The one problem with our information transfer is that we are much better at gaining information than giving output back. Thomas explains in this essay his view on scientific funding and planning. He believes that research should be focused in basic science. Unlike basic science, disease problems do not have the right type of questions to allow for great discoveries. The distinguishing factor of basic science is that there can be an element of surprise that allows for even more discoveries to be made. It is difficult to organize plans for this type of surprise in research even though it may seem a better business model to do so. It is the improbability and maze of puzzles that occur in basic research that Thomas believes will lead us to the most knowledge. Mythical creatures were created by our ancestors but even though we presently have no need for these beasts we continue to use them. The hybridization of animals in mythology is present from multiple ancient people such as the Ganesha, Griffon, Centaur, and Sphinx. Thomas suggests that perhaps we look to replace these mythological creatures which are more biological. He suggests the Myxotricha paradoxa, blepharisma, bacteria, and plant-animal combinations that are either made up of different organisms or set up joint endeavors with more than one organism to survive. From Thomas\u2019s metaphor on how humans behave like ants, he again argues that language is the quality that best resembles social insects. Without any outside direction, humans continually change language. We build language like ants build their hill, without ever knowing what the final result is and how our minuscule changes affect any other part. Thomas explains how some words have changed and developed different meanings. Two words, gene and bheu, are two words that we have derived a great number of current words from. Their descended words: kind, nature, physics are related in the present but also in its ancestry. Thomas compares language to the social behavior of termites in this essay. He thinks of language as an organism that is alive and changing. The genes of language are how words originated when you look into each of their histories. He traces multiple words to their origins to prove his point. He comments that it would be near impossible to keep track of all roots of words back to Indo-European that you use. We should be in awe that we exist and are unique among all the humans on Earth according to probability. Though we are indeed individual organisms, Thomas argues that one\u2019s own self is a myth. He believes we are part of a larger organization of information sharing. Through this system we are adapting and creating. By being more open with communication and less restrictive we will be able to uncover even more surprising discoveries. Thomas compares the Earth to a living cell, one with its own membrane that allows it to keep out disorder. He shows how the evolution of cells was closely tied to the \u201cbreath\u201d of the Earth, the cycling of oxygen concentration in the atmosphere. The atmosphere is \u201cfor sheer size and perfection of function, it is far and away the grandest product of collaboration in all of nature.\u201d It gives us the oxygen we need, protection from UV light, and protection from the millions of meteorites.",
            "score": 121.75225067138672
        },
        {
            "docid": "283329_9",
            "document": "Personal information management . With the increasing availability of computers in the 1950s came an interest in the computer as a source of metaphors and a test bed for efforts to understand the human ability to process information and to solve problems. Newell and Simon pioneered the computer's use as a tool to model human thought. They produced \"The Logic Theorist,\" generally thought to be the first running artificial intelligence (AI) program. The computer of the 1950s was also an inspiration for the development of an information processing approach to human behavior and performance. After the 1950s research showed that the computer, as a symbol processor, could \"think\" (to varying degrees of fidelity) like people do, the 1960s saw an increasing interest in the use of the computer to help people to think better and to process information more effectively. Working with Andries van Dam and others, Ted Nelson, who coined the word \"hypertext\", developed one of the first hypertext systems, The Hypertext Editing System, in 1968. That same year, Douglas Engelbart also completed work on a hypertext system called NLS (oN-Line System). Engelbart advanced the notion that the computer could be used to augment the human intellect. As heralded by the publication of Ulric Neisser's book Cognitive Psychology, the 1960s also saw the emergence of cognitive psychology as a discipline that focused primarily on a better understanding of the human ability to think, learn, and remember.",
            "score": 120.89685821533203
        },
        {
            "docid": "42985608_11",
            "document": "Cognitive biology . The words \u2018cognitive\u2019 and \u2018biology\u2019 are also used together as the name of a category. The category of \"cognitive biology\" has no fixed content but, rather, the content varies with the user. If the content can only be recruited from \"cognitive science\", then cognitive biology would seem limited to a selection of items in the main set of sciences included by the interdisciplinary concept\u2014cognitive psychology, artificial intelligence, linguistics, philosophy, neuroscience, and cognitive anthropology. These six separate sciences were allied \u201cto bridge the gap between brain and mind\u201d with an interdisciplinary approach in the mid-1970s. Participating scientists were concerned only with human cognition. As it gained momentum, the growth of cognitive science in subsequent decades seemed to offer a big tent to a variety of researchers. Some, for example, considered evolutionary epistemology a fellow-traveler. Others appropriated the keyword, as for example Donald Griffin in 1978, when he advocated the establishment of cognitive ethology. Meanwhile, breakthroughs in molecular, cell, evolutionary, and developmental biology generated a cornucopia of data-based theory relevant to cognition. Categorical assignments were problematic. For example, the decision to append \"cognitive\" to a body of biological research on neurons, e.g. the cognitive biology of neuroscience, is separate from the decision to put such body of research in a category named cognitive sciences. No less difficult a decision needs be made\u2014between the computational and constructivist approach to cognition, and the concomitant issue of simulated v. embodied cognitive models\u2014before appending biology to a body of cognitive research, e.g. the cognitive science of artificial life. One solution is to consider \"cognitive biology\" only as a subset of \"cognitive science\". For example, a major publisher\u2019s website displays links to material in a dozen domains of major scientific endeavor. One of which is described thus: \u201cCognitive science is the study of how the mind works, addressing cognitive functions such as perception and action, memory and learning, reasoning and problem solving, decision-making and consciousness.\u201d Upon its selection from the display, the \"Cognitive Science\" page offers in nearly alphabetical order these topics: \"Cognitive Biology\", Computer Science, Economics, Linguistics, Psychology, Philosophy, and Neuroscience. Linked through that list of topics, upon its selection the \"Cognitive Biology\" page offers a selection of reviews and articles with biological content ranging from cognitive ethology through evolutionary epistemology; cognition and art; evo-devo and cognitive science; animal learning; genes and cognition; cognition and animal welfare; etc.",
            "score": 120.63626098632812
        },
        {
            "docid": "190837_3",
            "document": "Evolutionary algorithm . Evolutionary algorithms often perform well approximating solutions to all types of problems because they ideally do not make any assumption about the underlying fitness landscape. Techniques from evolutionary algorithms applied to the modeling of biological evolution are generally limited to explorations of microevolutionary processes and planning models based upon cellular processes. In most real applications of EAs, computational complexity is a prohibiting factor. In fact, this computational complexity is due to fitness function evaluation. Fitness approximation is one of the solutions to overcome this difficulty. However, seemingly simple EA can solve often complex problems; therefore, there may be no direct link between algorithm complexity and problem complexity.",
            "score": 120.43074798583984
        },
        {
            "docid": "3737445_11",
            "document": "Quantum neural network . Most learning algorithms follow the classical model of training an artificial neural network to learn the input-output function of a given training set and use classical feedback loops to update parameters of the quantum system until they converge to an optimal configuration. Learning as a parameter optimisation problem has also been approached by adiabatic models of quantum computing. Recently there has been proposed a new post-learning strategy to allow the search for improved set of weights based on analogy with quantum effects occurring in nature. The technique, proposed in is based on the analogy of modeling a biological neuron as a semiconductor heterostructure consisting of one energetic barrier sandwiched between two energetically lower areas. The activation function of the neuron is therefore considered as a particle entering the heterostructure and interacting with the barrier. In this way auxiliary reinforcement to the classical learning process of neural networks is achieved with minimal additional computational costs.",
            "score": 120.33038330078125
        },
        {
            "docid": "3717_60",
            "document": "Brain . Computational neuroscience encompasses two approaches: first, the use of computers to study the brain; second, the study of how brains perform computation. On one hand, it is possible to write a computer program to simulate the operation of a group of neurons by making use of systems of equations that describe their electrochemical activity; such simulations are known as \"biologically realistic neural networks\". On the other hand, it is possible to study algorithms for neural computation by simulating, or mathematically analyzing, the operations of simplified \"units\" that have some of the properties of neurons but abstract out much of their biological complexity. The computational functions of the brain are studied both by computer scientists and neuroscientists.",
            "score": 120.23564910888672
        },
        {
            "docid": "47152350_29",
            "document": "Human performance modeling . The paradigm shift in psychology from behaviorism to the study of cognition had a huge impact on the field of Human Performance Modeling. Regarding memory and cognition, the research of Newell and Simon regarding artificial intelligence and the General Problem Solver (GPS; Newell & Simon, 1963), demonstrated that computational models could effectively capture fundamental human cognitive behavior. Newell and Simon were not simply concerned with the amount of information - say, counting the number of bits the human cognitive system had to receive from the perceptual system - but rather the actual computations being performed. They were critically involved with the early success of comparing cognition to computation, and the ability of computation to simulate critical aspects of cognition - thus leading to the creation of the sub-discipline of artificial intelligence within computer science, and changing how cognition was viewed in the psychological community. Although cognitive processes do not literally flip bits in the same way that discrete electronic circuits do, pioneers were able to show that any universal computational machine could simulate the processes used in another, without a physical equivalence (Phylyshyn, 1989; Turing, 1936).The cognitive revolution allowed all of cognition to be approached by modeling, and these models now span a vast array of cognitive domains - from simple list memory, to comprehension of communication, to problem solving and decision making, to imagery, and beyond.",
            "score": 119.89104461669922
        }
    ]
}