{
    "q": [
        {
            "docid": "25345530_45",
            "document": "Models of neural computation . The NEURON software, developed at Duke University, is a simulation environment for modeling individual neurons and networks of neurons. The NEURON environment is a self-contained environment allowing interface through its GUI or via scripting with hoc or python. The NEURON simulation engine is based on a Hodgkin\u2013Huxley type model using a Borg\u2013Graham formulation. Several examples of models written in NEURON are available from the online database ModelDB.",
            "score": 186.74415469169617
        },
        {
            "docid": "22549833_2",
            "document": "Modeling and simulation . Modeling and simulation (M&S) in simple terms is a substitute for physical experimentation, in which computers are used to compute the results of some physical phenomenon. As it is apparent from its name \"Modeling and simulation\" firstly computer is used to build a mathematical model which contains all the parameters of physical model and represent physical model in virtual form then conditions are applied which we want to experiment on physical model, then simulation starts i.e, we leave on computer to compute/calculate the results of those conditions on mathematical model. In this way actual experimentation can be avoided which is costly and time consuming instead of using mathematical knowledge and computer's computation power to solve real world problems cheaply and in time efficient manner. As such, M&S can facilitate understanding a system's behavior without actually testing the system in the real world. For instance, to determine which type of spoiler would improve traction the most while designing a race car, a computer simulation of the car could be used to estimate the effect of different spoiler shapes on the coefficient of friction in a turn. Useful insights about different decisions in the design could be gleaned without actually building the car. In addition, simulation can support experimentation that occurs totally in software, or in human-in-the-loop environments where simulation represents systems or generates data needed to meet experiment objectives. Furthermore, simulation can be used to train persons using a virtual environment that would otherwise be difficult or expensive to produce.",
            "score": 221.1430298089981
        },
        {
            "docid": "43966823_24",
            "document": "Multi-state modeling of biomolecules . The Stochastic Simulator Compiler (SSC) allows for rule-based, modular specification of interacting biomolecules in regions of arbitrarily complex geometries. Again, the system is represented using graphs, with chemical interactions or diffusion events formalised as graph-rewriting rules. The compiler then generates the entire reaction network before launching a stochastic reaction-diffusion algorithm.  A different approach is taken by PySB, where model specification is embedded in the programming language Python. A model (or part of a model) is represented as a Python programme. This allows users to store higher-order biochemical processes such as catalysis or polymerisation as macros and re-use them as needed. The models can be simulated and analysed using Python libraries, but PySB models can also be exported into BNGL, kappa, and SBML.",
            "score": 187.2658851146698
        },
        {
            "docid": "8402086_18",
            "document": "Computational neurogenetic modeling . Because the amount of data on the interplay of genes and neurons and their effects is not enough to construct a rigorous model,  evolutionary computation is used to optimize artificial neural networks and gene regulatory networks, a common technique being the genetic algorithm. A genetic algorithm is a process that can be used to refine models by mimicking the process of natural selection observed in biological ecosystems. The primary advantages are that, due to not requiring derivative information, it can be applied to black box problems and multimodal optimization. The typical process for using genetic algorithms to refine a gene  regulatory network is: first, create a population; next, to create offspring via a crossover operation and  evaluate their fitness; then, on a group chosen for high fitness, simulate mutation via a mutation operator;  finally, taking the now mutated group, repeat this process until a desired level of fitness is demonstrated. Methods by which artificial neural networks may alter their structure without simulated mutation and fitness selection have been developed. A dynamically evolving neural network is one approach, as the creation of new connections and new neurons can  be modeled as the system adapts to new data. This enables the network to evolve in modeling accuracy without simulated natural selection. One method by which dynamically evolving networks may be optimized, called evolving layer neuron aggregation, combines neurons with sufficiently similar input weights into one neuron. This can take place during the training of the network, referred to as online aggregation, or between periods of training, referred to as offline aggregation. Experiments have suggested that offline aggregation is more efficient.",
            "score": 209.26481115818024
        },
        {
            "docid": "26100025_4",
            "document": "UrbanSim . The initial implementation of UrbanSim was implemented in Java. The software architecture was modularized and reimplemented in Python beginning in 2005, making extensive use of the Numpy numerical library. The software has been generalized and abstracted from the UrbanSim model system, and is now referred to as the Open Platform for Urban Simulation (OPUS), in order to facilitate a plug-in architecture for models such as activity-based travel, dynamic traffic assignment, emissions, and land cover change. OPUS includes a Graphical User Interface, and a concise expression language to facilitate access to complex internal operations by non-programmers. Beginning in 2012, UrbanSim was re-implemented using current Scientific Python libraries such as Pandas. UrbanSim Inc. has developed the UrbanSim Cloud Platform that deploys simulations on the cloud for scalability, enabling hundreds or even thousands of simulations to be run simultaneously, and a web browser based User Interface that features a 3D web map view of inputs and outputs from the simulation. UrbanSim models have been pre-built for 400 metropolitan areas within the United States at a census block level of detail. Users anywhere in the world can also build UrbanSim models using zone and parcel templates, by uploading local data and using the cloud resources to auto-specify and calibrate the models using local data. Details are available at www.urbansim.com.",
            "score": 173.02614212036133
        },
        {
            "docid": "1038989_2",
            "document": "SystemC . SystemC is a set of C++ classes and macros which provide an event-driven simulation interface (see also discrete event simulation). These facilities enable a designer to \"simulate\" concurrent processes, each described using plain C++ syntax. SystemC processes can communicate in a \"simulated\" real-time environment, using signals of all the datatypes offered by C++, some additional ones offered by the SystemC library, as well as user defined. In certain respects, SystemC deliberately mimics the hardware description languages VHDL and Verilog, but is more aptly described as a \"system-level modeling language\".",
            "score": 182.33138871192932
        },
        {
            "docid": "37907506_3",
            "document": "Multi-Use Simulation Models . The development of multi-use simulation models has been addressed by several studies during the last two decades. The first significant trial to make general-purpose simulation was presented by Hajjar and AbouRizk, 1997. They developed a general purpose simulation model to be used in estimating and planning of earth moving operation depending on reusable templates. Other research was to use reusable templates in preparing draft scheduling for the construction of similar projects. The templates were made to encode much of the knowledge dealing with activity scoping and sequencing and the use of special activity in planning structures. Similarly, an interactive simulation system to be adapted by a beginner level user has been developed in 1999. This system was designed to provide easy access and easy manipulating environment for studying, analyzing and simulating construction processes. He used a simple and attractive graphical user interface to overcome the potential resistance by the user to simulation as analytical tool.",
            "score": 178.7593388557434
        },
        {
            "docid": "43444_100",
            "document": "Simulation . Most engineering simulations entail mathematical modeling and computer assisted investigation. There are many cases, however, where mathematical modeling is not reliable. Simulation of fluid dynamics problems often require both mathematical and physical simulations. In these cases the physical models require dynamic similitude. Physical and chemical simulations have also direct realistic uses, rather than research uses; in chemical engineering, for example, process simulations are used to give the process parameters immediately used for operating chemical plants, such as oil refineries. Simulators are also used for plant operator training. It is called Operator Training Simulator (OTS) and has been widely adopted by many industries from chemical to oil&gas and to power industry. This created a safe and realistic virtual environment to train board operators and engineers. Mimic is capable of providing high fidelity dynamic models of nearly all chemical plants for operator training and control system testing.",
            "score": 185.26836144924164
        },
        {
            "docid": "8035060_5",
            "document": "Ecosystem model . There are two major types of ecological models, which are generally applied to different types of problems: (1) \"analytic\" models and (2) \"simulation\" / \"computational\" models. Analytic models are typically relatively simple (often linear) systems, that can be accurately described by a set of mathematical equations whose behavior is well-known. Simulation models on the other hand, use numerical techniques to solve problems for which analytic solutions are impractical or impossible. Simulation models tend to be more widely used, and are generally considered more ecologically realistic, while analytic models are valued for their mathematical elegance and explanatory power. Ecopath is a powerful software system which uses simulation and computational methods to model marine ecosystems. It is widely used by marine and fisheries scientists as a tool for modelling and visualising the complex relationships that exist in real world marine ecosystems.",
            "score": 217.53837370872498
        },
        {
            "docid": "3717_60",
            "document": "Brain . Computational neuroscience encompasses two approaches: first, the use of computers to study the brain; second, the study of how brains perform computation. On one hand, it is possible to write a computer program to simulate the operation of a group of neurons by making use of systems of equations that describe their electrochemical activity; such simulations are known as \"biologically realistic neural networks\". On the other hand, it is possible to study algorithms for neural computation by simulating, or mathematically analyzing, the operations of simplified \"units\" that have some of the properties of neurons but abstract out much of their biological complexity. The computational functions of the brain are studied both by computer scientists and neuroscientists.",
            "score": 218.9105761051178
        },
        {
            "docid": "50399682_11",
            "document": "Predictive engineering analytics . 1D system simulation, also referred to as 1D CAE or mechatronics system simulation, allows scalable modeling of multi-domain systems. The full system is presented in a schematic way, by connecting validated analytical modeling blocks of electrical, hydraulic, pneumatic and mechanical subsystems (including control systems). It helps engineers predict the behavior of concept designs of complex mechatronics, either transient or steady-state.  Manufacturers often have validated libraries available that contain predefined components for different physical domains. Or if not, specialized software suppliers can provide them. Using those, the engineers can do concept predictions very early, even before any Computer-aided Design (CAD) geometry is available. During later stages, parameters can then be adapted. 1D system simulation calculations are very efficient. The components are analytically defined, and have input and output ports. Causality is created by connecting inputs of a components to outputs of another one (and vice versa). Models can have various degrees of complexity, and can reach very high accuracy as they evolve. Some model versions may allow real-time simulation, which is particularly useful during control systems development or as part of built-in predictive functionality.",
            "score": 181.68937301635742
        },
        {
            "docid": "30797416_2",
            "document": "Simcad Pro . Simcad Pro simulation software is a product of CreateASoft, Inc which is used for simulating process based environments such as manufacturing, supply lines, logistics, healthcare, and many others. The software is a tool for planning, organizing, optimizing, and engineering real process based systems. Simcad Pro allows the creation of a computer model, which can be manipulated by the user and represents a real environment. Using the model, it is possible to test for efficiency in any scenario possible, as well as locate points of improvement among the process flow. Simcad Pro's dynamic computer model also allows for changes to occur while the model is running for a fully realistic simulation.",
            "score": 193.26749086380005
        },
        {
            "docid": "5868379_10",
            "document": "Ns (simulator) . ns is built using C++ and Python with scripting capability. The ns library is wrapped by Python thanks to the pybindgen library which delegates the parsing of the ns C++ headers to castxml and pygccxml to automatically generate the corresponding C++ binding glue. These automatically-generated C++ files are finally compiled into the ns Python module to allow users to interact with the C++ ns models and core through Python scripts. The ns simulator features an integrated attribute-based system to manage default and per-instance values for simulation parameters.",
            "score": 136.47175192832947
        },
        {
            "docid": "11042797_2",
            "document": "Dynamic simulation . Dynamic simulation (or dynamic system simulation) is the use of a computer program to model the time varying behavior of a system. The systems are typically described by ordinary differential equations or partial differential equations. As mathematical models incorporate real-world constraints, like gear backlash and rebound from a hard stop, equations become nonlinear. This requires numerical methods to solve the equations. A numerical simulation is done by stepping through a time interval and calculating the integral of the derivatives by approximating the area under the derivative curves. Some methods use a fixed step through the interval, and others use an adaptive step that can shrink or grow automatically to maintain an acceptable error tolerance. Some methods can use different time steps in different parts of the simulation model. Industrial uses of dynamic simulation are many and range from nuclear power, steam turbines, 6 degree of freedom vehicle modeling, electric motors, econometric models, biological systems, robot arms, mass spring dampers, hydraulic systems, and drug dose migration through the human body to name a few. These models can often be run in real time to give a virtual response close to the actual system. This is useful in process control and mechatronic systems for tuning the automatic control systems before they are connected to the real system, or for human training before they control the real system. Simulation is also used in computer games and animation and can be accelerated by using a physics engine, the technology used in many powerful computer graphics software programs, like 3ds Max, Maya, Lightwave, and many others to simulate physical characteristics. In computer animation, things like hair, cloth, liquid, fire, and particles can be easily modeled, while the human animator animates simpler objects. Computer-based dynamic animation was first used at a very simple level in the 1989 Pixar short film \"Knick Knack\" to move the fake snow in the snowglobe and pebbles in a fish tank.",
            "score": 197.03293228149414
        },
        {
            "docid": "42908722_4",
            "document": "Model order reduction . Reduced order models are useful in settings where it is often unfeasible to perform numerical simulations using the complete full order model. This can be due to limitations in computational resources or the requirements of the simulations setting, for instance real-time simulation settings or many-query settings in which a large number of simulations needs to be performed. Examples of Real-time simulation settings include control systems in electronics and visualization of model results while examples for a many-query setting can include optimisation problems and design exploration. In order to be applicable to real-world problems, often the requirements of a reduced order model are:",
            "score": 212.1826832294464
        },
        {
            "docid": "43966823_3",
            "document": "Multi-state modeling of biomolecules . Biological signaling systems often rely on complexes of biological macromolecules that can undergo several functionally significant modifications that are mutually compatible. Thus, they can exist in a very large number of functionally different states. Modeling such multi-state systems poses two problems: The problem of how to describe and specify a multi-state system (the \"specification problem\") and the problem of how to use a computer to simulate the progress of the system over time (the \"computation problem\"). To address the specification problem, modelers have in recent years moved away from explicit specification of all possible states, and towards rule-based formalisms that allow for implicit model specification, including the \u03ba-calculus, BioNetGen, the Allosteric Network Compiler and others. To tackle the computation problem, they have turned to particle-based methods that have in many cases proved more computationally efficient than population-based methods based on ordinary differential equations, partial differential equations, or the Gillespie stochastic simulation algorithm. Given current computing technology, particle-based methods are sometimes the only possible option. Particle-based simulators further fall into two categories: Non-spatial simulators such as StochSim, DYNSTOC, RuleMonkey, and NFSim and spatial simulators, including Meredys, SRSim and MCell. Modelers can thus choose from a variety of tools; the best choice depending on the particular problem. Development of faster and more powerful methods is ongoing, promising the ability to simulate ever more complex signaling processes in the future.",
            "score": 217.17552518844604
        },
        {
            "docid": "33818014_4",
            "document": "Nervous system network models . Why does one want to model the brain and neural network? Although highly sophisticated computer systems have been developed and used in all walks of life, they are nowhere close to the human system in hardware and software capabilities. So, scientists have been at work to understand the human operation system and try to simulate its functionalities. In order to accomplish this, one needs to model its components and functions and validate its performance with real life. Computational models of a well simulated nervous system enable learning the nervous system and apply it to real life problem solutions.",
            "score": 218.35949230194092
        },
        {
            "docid": "20468959_4",
            "document": "Simulation Open Framework Architecture . A key aspect of SOFA is the use of a scene graph to organize and process the elements of a simulation while clearly separating the computation tasks from their possibly parallel scheduling. The description of a SOFA simulation can easily be done in an XML file. For even more flexibility, a Python plugin allows scripting simulations using the Python language.  Basically, a SOFA scene-graph is composed with:",
            "score": 143.4578183889389
        },
        {
            "docid": "31541804_3",
            "document": "ICME cyberinfrastructure . The ICME cyberinfrastructure provides storage, access, and computational capabilities for an extensive network of manufacturing, design, and life-cycle simulation software. Within this software framework, data is archived, searchable and interactive, offering engineers and scientists a vast database of materials-related information for use in research, multiscale modeling, simulation implementation, and an array of other activities in support of more efficient, less costly product development. Furthermore, the ICME cyberinfrastructure is expected to provide the capability to access and link application codes, including the development of protocols necessary to integrate hierarchical modeling approaches. With an emphasis on computational efficiency, experimental validation of models, and protecting intellectual property, the cyberinfrastructure assimilates 1) process-microstructure-property relations, 2) development of constitutive materials models that accurately predict multiscale material behaviors admitting microstructure/inclusions and history effects, 3) access to shared databases of analytical and experimental data, and 4) material models. As such, it is also crucial to identifying gaps in materials knowledge, which, in turn, guides the development of new materials theories, models, and simulation tools. Such a community-based knowledge foundation ultimately enables materials informatics systems that fuse high fidelity experimental databases with models of physical processes.",
            "score": 190.70718896389008
        },
        {
            "docid": "43045000_14",
            "document": "Building performance simulation . Given the complexity of building energy and mass flows, it is generally not possible to find an analytical solution, so the simulation software employs other techniques, such as response function methods, or numerical methods in finite differences or finite volume, as an approximation. Most of today's whole building simulation programs formulate models using imperative programming languages. These languages assign values to variables, declare the sequence of execution of these assignments and change the state of the program, as is done for example in C/C++, Fortran or MATLAB/Simulink. In such programs, model equations are tightly connected to the solution methods, often by making the solution procedure part of the actual model equations. The use of imperative programming languages limits the applicability and extensibility of models. More flexibility offer simulation engines using symbolic Differential Algebraic Equations (DAEs) with general purpose solvers that increase model reuse, transparency and accuracy. Since some of these engines have been developed for more than 20 years (e.g. IDA ICE) and due to the key advantages of equation-based modeling, these simulation engines can be considered as state of the art technology.",
            "score": 189.5377004146576
        },
        {
            "docid": "33818014_20",
            "document": "Nervous system network models . There are three views of modules for modeling. They are (1) modules for brain structures, (2) modules as schemas, and (3) modules as interfaces. Figure 3 presents the modular design of a model for reflex control of saccades (Arbib, M. A. (2007)). It involves two main modules, one for superior colliculus (SC), and one for brainstem. Each of these is decomposed into submodules, with each submodule defining an array of physiologically defined neurons. In Figure 3(b) the model of Figure 3(a) is embedded into a far larger model which embraces various regions of cerebral cortex (represented by the modules Pre-LIP Vis, Ctx., LIP, PFC, and FEF), thalamus, and basal ganglia. While the model may indeed be analyzed at this top level of modular decomposition, we need to further decompose basal ganglia, BG, as shown in Figure 3(c) if we are to tease apart the role of dopamine in differentially modulating (the 2 arrows shown arising from SNc) the direct and indirect pathways within the basal ganglia (Crowley, M. (1997)). Neural Simulation Language (NSL) has been developed to provide a simulation system for large-scale general neural networks. It provides an environment to develop an object-oriented approach to brain modeling. NSL supports neural models having as basic data structure neural layers with similar properties and similar connection patterns. Models developed using NSL are documented in Brain Operation Database (BODB) as hierarchically organized modules that can be decomposed into lower levels.",
            "score": 210.55372631549835
        },
        {
            "docid": "375416_3",
            "document": "Computer simulation . Computer simulations are realized by running computer programs that can be either small, running almost instantly on small devices, or large-scale programs that run for hours or days on network-based groups of computers. The scale of events being simulated by computer simulations has far exceeded anything possible (or perhaps even imaginable) using traditional paper-and-pencil mathematical modeling. Over 10 years ago, a desert-battle simulation of one force invading another involved the modeling of 66,239 tanks, trucks and other vehicles on simulated terrain around Kuwait, using multiple supercomputers in the DoD High Performance Computer Modernization Program. Other examples include a 1-billion-atom model of material deformation; a 2.64-million-atom model of the complex protein-producing organelle of all living organisms, the ribosome, in 2005; a complete simulation of the life cycle of Mycoplasma genitalium in 2012; and the Blue Brain project at EPFL (Switzerland), begun in May 2005 to create the first computer simulation of the entire human brain, right down to the molecular level.",
            "score": 207.87776362895966
        },
        {
            "docid": "43444_3",
            "document": "Simulation . Simulation is used in many contexts, such as simulation of technology for performance optimization, safety engineering, testing, training, education, and video games. Often, computer experiments are used to study simulation models. Simulation is also used with scientific modelling of natural systems or human systems to gain insight into their functioning, as in economics. Simulation can be used to show the eventual real effects of alternative conditions and courses of action. Simulation is also used when the real system cannot be engaged, because it may not be accessible, or it may be dangerous or unacceptable to engage, or it is being designed but not yet built, or it may simply not exist.",
            "score": 183.74012851715088
        },
        {
            "docid": "39619438_3",
            "document": "AnimatLab . Neuromechanical simulation enables investigators to explore the dynamical relationships between the brain, the body, and the world in ways that are difficult or impossible through experiment alone. This is done by producing biologically realistic models of the neural networks that control behavior, while also simulating the physics that controls the environment in which an animal is situated. Interactions with the simulated world can then be fed back to the virtual nervous system using models of sensory systems. This provides feedback similar to what the real animal would encounter, and makes it possible to close the sensory-motor feedback loop to study the dynamic relationship between nervous function and behavior. This relationship is crucial to understanding how nervous systems work.",
            "score": 209.1887502670288
        },
        {
            "docid": "3717_69",
            "document": "Brain . Theorists have worked to understand these response patterns by constructing mathematical models of neurons and neural networks, which can be simulated using computers. Some useful models are abstract, focusing on the conceptual structure of neural algorithms rather than the details of how they are implemented in the brain; other models attempt to incorporate data about the biophysical properties of real neurons. No model on any level is yet considered to be a fully valid description of brain function, though. The essential difficulty is that sophisticated computation by neural networks requires distributed processing in which hundreds or thousands of neurons work cooperatively\u2014current methods of brain activity recording are only capable of isolating action potentials from a few dozen neurons at a time.",
            "score": 235.93708181381226
        },
        {
            "docid": "20856654_21",
            "document": "Instructional simulation . In education, virtual learning environments are simulated experiences which utilize the pedagogical strategies of instructional modeling and role playing for the teaching of new concepts. The environment in which the experiences are presented is a virtual one often accessed via a computer or other video projection interface. Immersive virtual environment headsets have been used with younger children and students with special needs. The advantages of using instructional simulators via VLEs include: students are motivated when they are able to use computers and other technology; VLEs allow for interaction, exploration, and experimentation with locations, objects, and environments that would otherwise be unavailable in the absence of the VLE; instructors can adapt programs and parameters of the virtual learning experience to meet individual learner needs; when multi-user virtual environments are used collaborative and cooperative learning is encouraged; VLEs relate to students the real-world relevance of their learning by extending concepts and skills to application in the simulated environment; and learning can occur in an emotionally and physically safe environment without detrimental consequence.",
            "score": 170.95237600803375
        },
        {
            "docid": "18943937_22",
            "document": "Emulator . In 1963, when microcode was first used to speed up this simulation process, IBM engineers coined the term \"emulator\" to describe the concept. In the 2000s, it has become common to use the word \"emulate\" in the context of software. However, before 1980, \"emulation\" referred only to emulation with a hardware or microcode assist, while \"simulation\" referred to pure software emulation. For example, a computer specially built for running programs designed for another architecture is an emulator. In contrast, a simulator could be a program which runs on a PC, so that old Atari games can be simulated on it. Purists continue to insist on this distinction, but currently the term \"emulation\" often means the complete imitation of a machine executing binary code while \"simulation\" often refers to computer simulation, where a computer program is used to simulate an abstract model. Computer simulation is used in virtually every scientific and engineering domain and Computer Science is no exception, with several projects simulating abstract models of computer systems, such as network simulation, which both practically and semantically differs from network emulation.",
            "score": 203.7733746767044
        },
        {
            "docid": "34458189_3",
            "document": "Rule-based modeling . Early efforts to use rule-based modeling in simulation of biochemical systems include the stochastic simulation systems StochSim A widely used tool for rule-based modeling of biochemical networks is BioNetGen It is released under the GNU GPL, version 3. BioNetGen includes a language to describe chemical substances, including the states they can assume and the bindings they can undergo. These rules can be used to create a reaction network model or to perform computer simulations directly on the rule set. The biochemical modeling framework Virtual Cell includes a BioNetGen interpreter.",
            "score": 209.40667963027954
        },
        {
            "docid": "37641842_3",
            "document": "MS4 Modeling Environment . MS4 Me supports the development and simulation of DEVS models via a natural language or Java. Finite Deterministic DEVS (FDDEVS) models can also be quickly developed and analyzed. DEVS models can be composed into more complex systems via the use of System Entity Structures, and System Entity Structures can be composed into complex systems of systems for simulation. Many different configurations of these systems can be stored and simulated via the use of pruning.",
            "score": 179.7807114124298
        },
        {
            "docid": "5575498_2",
            "document": "Symbolic simulation . In computer science, a simulation is a computation of the execution of some appropriately modelled state-transition system. Typically this process models the complete state of the system at individual points in a discrete linear time frame, computing each state sequentially from its predecessor. Models for computer programs or VLSI logic designs can be very easily simulated, as they often have an operational semantics which can be used directly for simulation.",
            "score": 189.31866121292114
        },
        {
            "docid": "36648035_5",
            "document": "Pedro Pedrosa Mendes . Mendes research is concerned with computational systems biology, which aims to better understand biological systems through the use of computer models. He is the author of the biochemical simulator GEPASI (General Pathway Simulator) and leader of the new COPASI (COmplex PAthway SImulator) simulator. He has also been actively involved in the development of the Systems Biology Markup Language (SBML), and MIRIAM (Minimum Information Required in the Annotation of Models). His research group work on problems in the following areas:",
            "score": 175.80624175071716
        },
        {
            "docid": "21855574_5",
            "document": "Brain simulation . The connectivity of the neural circuit for touch sensitivity of the simple C. elegans nematode (roundworm) was mapped in 1985 and partly simulated in 1993. Since 2004, many software simulations of the complete neural and muscular system have been developed, including simulation of the worm's physical environment. Some of these models have been made available for download. However, there is still a lack of understanding of how the neurons and the connections between them generate the surprisingly complex range of behaviors that are observed in the relatively simple organism. This contrast between the apparent simplicity of how the mapped neurons interact with their neighbours, and exceeding complexity of the overall brain function, is an example of an emergent property. Interestingly, this kind of emergent property is paralleled within artificial neural networks, the neurons of which are exceedingly simple compared to their often complex, abstract outputs.",
            "score": 215.499325633049
        }
    ],
    "r": [
        {
            "docid": "3717_69",
            "document": "Brain . Theorists have worked to understand these response patterns by constructing mathematical models of neurons and neural networks, which can be simulated using computers. Some useful models are abstract, focusing on the conceptual structure of neural algorithms rather than the details of how they are implemented in the brain; other models attempt to incorporate data about the biophysical properties of real neurons. No model on any level is yet considered to be a fully valid description of brain function, though. The essential difficulty is that sophisticated computation by neural networks requires distributed processing in which hundreds or thousands of neurons work cooperatively\u2014current methods of brain activity recording are only capable of isolating action potentials from a few dozen neurons at a time.",
            "score": 235.9370880126953
        },
        {
            "docid": "53757504_6",
            "document": "Viktor K. Jirsa . The Virtual Brain is a free open source neuroinformatics tool designed to aid in the exploration of network mechanisms of brain function and associated pathologies. TVB provides the possibility to feed computational neuronal network models with information about structural and functional imaging data including population (sEEG/EEG/MEG) activity, spatially highly resolved whole brain metabolic/vascular signals (fMRI) and global measures of neuronal connections (DTI) \u2013 for intact as well as pathologically altered connectivity. TVB is model agnostic and offers a wide range of neural population models to be used as network nodes. The software infrastructure of the Virtual Brain is composed of a functional core running the large-scale brain simulations independently or in batch mode, a web based interface to access the simulator, as well as a command line interface to develop more extensive applications. All simulations may be performed on workstations and labtops, as well as on high-performance clusters (HPCs). Manipulations of network parameters within the Virtual Brain allow researchers and clinicians to test the effects of experimental paradigms, interventions (such as stimulation and surgery) and therapeutic strategies (such as pharmaceutical interventions targeting local areas). The computational environment allows the user to visualise the simulated data in 2D and 3D and perform data analyses in the same way as commonly performed with empirical data.",
            "score": 231.12142944335938
        },
        {
            "docid": "22549833_2",
            "document": "Modeling and simulation . Modeling and simulation (M&S) in simple terms is a substitute for physical experimentation, in which computers are used to compute the results of some physical phenomenon. As it is apparent from its name \"Modeling and simulation\" firstly computer is used to build a mathematical model which contains all the parameters of physical model and represent physical model in virtual form then conditions are applied which we want to experiment on physical model, then simulation starts i.e, we leave on computer to compute/calculate the results of those conditions on mathematical model. In this way actual experimentation can be avoided which is costly and time consuming instead of using mathematical knowledge and computer's computation power to solve real world problems cheaply and in time efficient manner. As such, M&S can facilitate understanding a system's behavior without actually testing the system in the real world. For instance, to determine which type of spoiler would improve traction the most while designing a race car, a computer simulation of the car could be used to estimate the effect of different spoiler shapes on the coefficient of friction in a turn. Useful insights about different decisions in the design could be gleaned without actually building the car. In addition, simulation can support experimentation that occurs totally in software, or in human-in-the-loop environments where simulation represents systems or generates data needed to meet experiment objectives. Furthermore, simulation can be used to train persons using a virtual environment that would otherwise be difficult or expensive to produce.",
            "score": 221.14303588867188
        },
        {
            "docid": "3717_60",
            "document": "Brain . Computational neuroscience encompasses two approaches: first, the use of computers to study the brain; second, the study of how brains perform computation. On one hand, it is possible to write a computer program to simulate the operation of a group of neurons by making use of systems of equations that describe their electrochemical activity; such simulations are known as \"biologically realistic neural networks\". On the other hand, it is possible to study algorithms for neural computation by simulating, or mathematically analyzing, the operations of simplified \"units\" that have some of the properties of neurons but abstract out much of their biological complexity. The computational functions of the brain are studied both by computer scientists and neuroscientists.",
            "score": 218.9105682373047
        },
        {
            "docid": "413102_6",
            "document": "Folding@home . Due to the complexity of proteins' conformation or configuration space (the set of possible shapes a protein can take), and limits in computing power, all-atom molecular dynamics simulations have been severely limited in the timescales which they can study. While most proteins typically fold in the order of milliseconds, before 2010 simulations could only reach nanosecond to microsecond timescales. General-purpose supercomputers have been used to simulate protein folding, but such systems are intrinsically costly and typically shared among many research groups. Further, because the computations in kinetic models occur serially, strong scaling of traditional molecular simulations to these architectures is exceptionally difficult. Moreover, as protein folding is a stochastic process and can statistically vary over time, it is challenging computationally to use long simulations for comprehensive views of the folding process. Protein folding does not occur in one step. Instead, proteins spend most of their folding time, nearly 96% in some cases, \"waiting\" in various intermediate conformational states, each a local thermodynamic free energy minimum in the protein's energy landscape. Through a process known as adaptive sampling, these conformations are used by Folding@home as starting points for a set of simulation trajectories. As the simulations discover more conformations, the trajectories are restarted from them, and a Markov state model (MSM) is gradually created from this cyclic process. MSMs are discrete-time master equation models which describe a biomolecule's conformational and energy landscape as a set of distinct structures and the short transitions between them. The adaptive sampling Markov state model method significantly increases the efficiency of simulation as it avoids computation inside the local energy minimum itself, and is amenable to distributed computing (including on GPUGRID) as it allows for the statistical aggregation of short, independent simulation trajectories. The amount of time it takes to construct a Markov state model is inversely proportional to the number of parallel simulations run, i.e., the number of processors available. In other words, it achieves linear parallelization, leading to an approximately four orders of magnitude reduction in overall serial calculation time. A completed MSM may contain tens of thousands of sample states from the protein's phase space (all the conformations a protein can take on) and the transitions between them. The model illustrates folding events and pathways (i.e., routes) and researchers can later use kinetic clustering to view a coarse-grained representation of the otherwise highly detailed model. They can use these MSMs to reveal how proteins misfold and to quantitatively compare simulations with experiments.",
            "score": 218.40426635742188
        },
        {
            "docid": "33818014_4",
            "document": "Nervous system network models . Why does one want to model the brain and neural network? Although highly sophisticated computer systems have been developed and used in all walks of life, they are nowhere close to the human system in hardware and software capabilities. So, scientists have been at work to understand the human operation system and try to simulate its functionalities. In order to accomplish this, one needs to model its components and functions and validate its performance with real life. Computational models of a well simulated nervous system enable learning the nervous system and apply it to real life problem solutions.",
            "score": 218.35948181152344
        },
        {
            "docid": "8035060_5",
            "document": "Ecosystem model . There are two major types of ecological models, which are generally applied to different types of problems: (1) \"analytic\" models and (2) \"simulation\" / \"computational\" models. Analytic models are typically relatively simple (often linear) systems, that can be accurately described by a set of mathematical equations whose behavior is well-known. Simulation models on the other hand, use numerical techniques to solve problems for which analytic solutions are impractical or impossible. Simulation models tend to be more widely used, and are generally considered more ecologically realistic, while analytic models are valued for their mathematical elegance and explanatory power. Ecopath is a powerful software system which uses simulation and computational methods to model marine ecosystems. It is widely used by marine and fisheries scientists as a tool for modelling and visualising the complex relationships that exist in real world marine ecosystems.",
            "score": 217.5383758544922
        },
        {
            "docid": "43966823_3",
            "document": "Multi-state modeling of biomolecules . Biological signaling systems often rely on complexes of biological macromolecules that can undergo several functionally significant modifications that are mutually compatible. Thus, they can exist in a very large number of functionally different states. Modeling such multi-state systems poses two problems: The problem of how to describe and specify a multi-state system (the \"specification problem\") and the problem of how to use a computer to simulate the progress of the system over time (the \"computation problem\"). To address the specification problem, modelers have in recent years moved away from explicit specification of all possible states, and towards rule-based formalisms that allow for implicit model specification, including the \u03ba-calculus, BioNetGen, the Allosteric Network Compiler and others. To tackle the computation problem, they have turned to particle-based methods that have in many cases proved more computationally efficient than population-based methods based on ordinary differential equations, partial differential equations, or the Gillespie stochastic simulation algorithm. Given current computing technology, particle-based methods are sometimes the only possible option. Particle-based simulators further fall into two categories: Non-spatial simulators such as StochSim, DYNSTOC, RuleMonkey, and NFSim and spatial simulators, including Meredys, SRSim and MCell. Modelers can thus choose from a variety of tools; the best choice depending on the particular problem. Development of faster and more powerful methods is ongoing, promising the ability to simulate ever more complex signaling processes in the future.",
            "score": 217.17552185058594
        },
        {
            "docid": "355240_6",
            "document": "Cognitive model . A computational model is a mathematical model in computational science that requires extensive computational resources to study the behavior of a complex system by computer simulation. The system under study is often a complex nonlinear system for which simple, intuitive analytical solutions are not readily available. Rather than deriving a mathematical analytical solution to the problem, experimentation with the model is done by changing the parameters of the system in the computer, and studying the differences in the outcome of the experiments. Theories of operation of the model can be derived/deduced from these computational experiments. Examples of common computational models are weather forecasting models, earth simulator models, flight simulator models, molecular protein folding models, and neural network models. . expressed in characters, usually nonnumeric, that require translation before they can be used \"subsymbolic\" if it is made by constituent entities that are not representations in their turn, e.g., pixels, sound images as perceived by the ear, signal samples; subsymbolic units in neural networks can be considered particular cases of this category Hybrid computers are computers that exhibit features of analog computers and digital computers. The digital component normally serves as the controller and provides logical operations, while the analog component normally serves as a solver of differential equations. See more details at hybrid intelligent system. In the traditional computational approach, representations are viewed as static structures of discrete symbols. Cognition takes place by transforming static symbol structures in discrete, sequential steps. Sensory information is transformed into symbolic inputs, which produce symbolic outputs that get transformed into motor outputs. The entire system operates in an ongoing cycle.",
            "score": 216.31533813476562
        },
        {
            "docid": "15855253_13",
            "document": "Quantification of margins and uncertainties . QMU has the potential to support improved decision-making for programs that must rely heavily on modeling and simulation. Modeling and simulation results are being used more often during the acquisition, development, design, and testing of complex engineering systems. One of the major challenges of developing simulations is to know how much fidelity should be built into each element of the model. The pursuit of higher fidelity can significantly increase development time and total cost of the simulation development effort. QMU provides a formal method for describing the required fidelity relative to the design threshold margins for key performance variables. This information can also be used to prioritize areas of future investment for the simulation. Analysis of the various M/U ratios for the key performance variables can help identify model components that are in need of fidelity upgrades to order to increase simulation effectiveness.",
            "score": 215.9429473876953
        },
        {
            "docid": "21855574_5",
            "document": "Brain simulation . The connectivity of the neural circuit for touch sensitivity of the simple C. elegans nematode (roundworm) was mapped in 1985 and partly simulated in 1993. Since 2004, many software simulations of the complete neural and muscular system have been developed, including simulation of the worm's physical environment. Some of these models have been made available for download. However, there is still a lack of understanding of how the neurons and the connections between them generate the surprisingly complex range of behaviors that are observed in the relatively simple organism. This contrast between the apparent simplicity of how the mapped neurons interact with their neighbours, and exceeding complexity of the overall brain function, is an example of an emergent property. Interestingly, this kind of emergent property is paralleled within artificial neural networks, the neurons of which are exceedingly simple compared to their often complex, abstract outputs.",
            "score": 215.49932861328125
        },
        {
            "docid": "42908722_4",
            "document": "Model order reduction . Reduced order models are useful in settings where it is often unfeasible to perform numerical simulations using the complete full order model. This can be due to limitations in computational resources or the requirements of the simulations setting, for instance real-time simulation settings or many-query settings in which a large number of simulations needs to be performed. Examples of Real-time simulation settings include control systems in electronics and visualization of model results while examples for a many-query setting can include optimisation problems and design exploration. In order to be applicable to real-world problems, often the requirements of a reduced order model are:",
            "score": 212.18267822265625
        },
        {
            "docid": "33818014_20",
            "document": "Nervous system network models . There are three views of modules for modeling. They are (1) modules for brain structures, (2) modules as schemas, and (3) modules as interfaces. Figure 3 presents the modular design of a model for reflex control of saccades (Arbib, M. A. (2007)). It involves two main modules, one for superior colliculus (SC), and one for brainstem. Each of these is decomposed into submodules, with each submodule defining an array of physiologically defined neurons. In Figure 3(b) the model of Figure 3(a) is embedded into a far larger model which embraces various regions of cerebral cortex (represented by the modules Pre-LIP Vis, Ctx., LIP, PFC, and FEF), thalamus, and basal ganglia. While the model may indeed be analyzed at this top level of modular decomposition, we need to further decompose basal ganglia, BG, as shown in Figure 3(c) if we are to tease apart the role of dopamine in differentially modulating (the 2 arrows shown arising from SNc) the direct and indirect pathways within the basal ganglia (Crowley, M. (1997)). Neural Simulation Language (NSL) has been developed to provide a simulation system for large-scale general neural networks. It provides an environment to develop an object-oriented approach to brain modeling. NSL supports neural models having as basic data structure neural layers with similar properties and similar connection patterns. Models developed using NSL are documented in Brain Operation Database (BODB) as hierarchically organized modules that can be decomposed into lower levels.",
            "score": 210.55372619628906
        },
        {
            "docid": "34458189_3",
            "document": "Rule-based modeling . Early efforts to use rule-based modeling in simulation of biochemical systems include the stochastic simulation systems StochSim A widely used tool for rule-based modeling of biochemical networks is BioNetGen It is released under the GNU GPL, version 3. BioNetGen includes a language to describe chemical substances, including the states they can assume and the bindings they can undergo. These rules can be used to create a reaction network model or to perform computer simulations directly on the rule set. The biochemical modeling framework Virtual Cell includes a BioNetGen interpreter.",
            "score": 209.40667724609375
        },
        {
            "docid": "8402086_18",
            "document": "Computational neurogenetic modeling . Because the amount of data on the interplay of genes and neurons and their effects is not enough to construct a rigorous model,  evolutionary computation is used to optimize artificial neural networks and gene regulatory networks, a common technique being the genetic algorithm. A genetic algorithm is a process that can be used to refine models by mimicking the process of natural selection observed in biological ecosystems. The primary advantages are that, due to not requiring derivative information, it can be applied to black box problems and multimodal optimization. The typical process for using genetic algorithms to refine a gene  regulatory network is: first, create a population; next, to create offspring via a crossover operation and  evaluate their fitness; then, on a group chosen for high fitness, simulate mutation via a mutation operator;  finally, taking the now mutated group, repeat this process until a desired level of fitness is demonstrated. Methods by which artificial neural networks may alter their structure without simulated mutation and fitness selection have been developed. A dynamically evolving neural network is one approach, as the creation of new connections and new neurons can  be modeled as the system adapts to new data. This enables the network to evolve in modeling accuracy without simulated natural selection. One method by which dynamically evolving networks may be optimized, called evolving layer neuron aggregation, combines neurons with sufficiently similar input weights into one neuron. This can take place during the training of the network, referred to as online aggregation, or between periods of training, referred to as offline aggregation. Experiments have suggested that offline aggregation is more efficient.",
            "score": 209.2648162841797
        },
        {
            "docid": "39619438_3",
            "document": "AnimatLab . Neuromechanical simulation enables investigators to explore the dynamical relationships between the brain, the body, and the world in ways that are difficult or impossible through experiment alone. This is done by producing biologically realistic models of the neural networks that control behavior, while also simulating the physics that controls the environment in which an animal is situated. Interactions with the simulated world can then be fed back to the virtual nervous system using models of sensory systems. This provides feedback similar to what the real animal would encounter, and makes it possible to close the sensory-motor feedback loop to study the dynamic relationship between nervous function and behavior. This relationship is crucial to understanding how nervous systems work.",
            "score": 209.18875122070312
        },
        {
            "docid": "19378_43",
            "document": "Mind . Neuroscience studies the nervous system, the physical basis of the mind. At the systems level, neuroscientists investigate how biological neural networks form and physiologically interact to produce mental functions and content such as reflexes, multisensory integration, motor coordination, circadian rhythms, emotional responses, learning, and memory. At a larger scale, efforts in computational neuroscience have developed large-scale models that simulate simple, functioning brains. As of 2012, such models include the thalamus, basal ganglia, prefrontal cortex, motor cortex, and occipital cortex, and consequentially simulated brains can learn, respond to visual stimuli, coordinate motor responses, form short-term memories, and learn to respond to patterns. Currently, researchers aim to program the hippocampus and limbic system, hypothetically imbuing the simulated mind with long-term memory and crude emotions.",
            "score": 209.0120849609375
        },
        {
            "docid": "288292_16",
            "document": "Neuropsychology . Connectionism is the use of artificial neural networks to model specific cognitive processes using what are considered to be simplified but plausible models of how neurons operate. Once trained to perform a specific cognitive task these networks are often damaged or 'lesioned' to simulate brain injury or impairment in an attempt to understand and compare the results to the effects of brain injury in humans.",
            "score": 208.38534545898438
        },
        {
            "docid": "74554_25",
            "document": "Hardware description language . To simulate an HDL model, an engineer writes a top-level simulation environment (called a test bench). At minimum, a testbench contains an instantiation of the model (called the device under test or DUT), pin/signal declarations for the model's I/O, and a clock waveform. The testbench code is event driven: the engineer writes HDL statements to implement the (testbench-generated) reset-signal, to model interface transactions (such as a host\u2013bus read/write), and to monitor the DUT's output. An HDL simulator \u2014 the program that executes the testbench \u2014 maintains the simulator clock, which is the master reference for all events in the testbench simulation. Events occur only at the instants dictated by the testbench HDL (such as a reset-toggle coded into the testbench), or in reaction (by the model) to stimulus and triggering events. Modern HDL simulators have full-featured graphical user interfaces, complete with a suite of debug tools. These allow the user to stop and restart the simulation at any time, insert simulator breakpoints (independent of the HDL code), and monitor or modify any element in the HDL model hierarchy. Modern simulators can also link the HDL environment to user-compiled libraries, through a defined PLI/VHPI interface. Linking is system-dependent (Win32/Linux/SPARC), as the HDL simulator and user libraries are compiled and linked outside the HDL environment.",
            "score": 208.2642364501953
        },
        {
            "docid": "375416_3",
            "document": "Computer simulation . Computer simulations are realized by running computer programs that can be either small, running almost instantly on small devices, or large-scale programs that run for hours or days on network-based groups of computers. The scale of events being simulated by computer simulations has far exceeded anything possible (or perhaps even imaginable) using traditional paper-and-pencil mathematical modeling. Over 10 years ago, a desert-battle simulation of one force invading another involved the modeling of 66,239 tanks, trucks and other vehicles on simulated terrain around Kuwait, using multiple supercomputers in the DoD High Performance Computer Modernization Program. Other examples include a 1-billion-atom model of material deformation; a 2.64-million-atom model of the complex protein-producing organelle of all living organisms, the ribosome, in 2005; a complete simulation of the life cycle of Mycoplasma genitalium in 2012; and the Blue Brain project at EPFL (Switzerland), begun in May 2005 to create the first computer simulation of the entire human brain, right down to the molecular level.",
            "score": 207.8777618408203
        },
        {
            "docid": "10159567_7",
            "document": "Spiking neural network . In practice, there is a major difference between the theoretical power of spiking neural networks and what has been demonstrated. They have proved useful in neuroscience, but not (yet) in engineering. Some large scale neural network models have been designed that take advantage of the pulse coding found in spiking neural networks, these networks mostly rely on the principles of reservoir computing. However, the real world application of large scale spiking neural networks has been limited because the increased computational costs associated with simulating realistic neural models have not been justified by commensurate benefits in computational power. As a result, there has been little application of large scale spiking neural networks to solve computational tasks of the order and complexity that are commonly addressed using rate coded (second generation) neural networks. In addition it can be difficult to adapt second generation neural network models into real time, spiking neural networks (especially if these network algorithms are defined in discrete time). It is relatively easy to construct a spiking neural network model and observe its dynamics. It is much harder to develop a model with stable behavior that computes a specific function.",
            "score": 206.37551879882812
        },
        {
            "docid": "22921_96",
            "document": "Psychology . Computational modeling is a tool used in mathematical psychology and cognitive psychology to simulate behavior. This method has several advantages. Since modern computers process information quickly, simulations can be run in a short time, allowing for high statistical power. Modeling also allows psychologists to visualize hypotheses about the functional organization of mental events that couldn't be directly observed in a human. Computational neuroscience uses mathematical models to simulate the brain. Another method is symbolic modeling, which represents many mental objects using variables and rules. Other types of modeling include dynamic systems and stochastic modeling.",
            "score": 204.97865295410156
        },
        {
            "docid": "2509524_5",
            "document": "Game physics . A common aspect of computer games that model some type of conflict is the explosion. Early computer games used the simple expedient of repeating the same explosion in each circumstance. However, in the real world an explosion can vary depending on the terrain, altitude of the explosion, and the type of solid bodies being impacted. Depending on the processing power available, the effects of the explosion can be modeled as the split and shattered components propelled by the expanding gas. This is modelled by means of a particle system simulation. A particle system model allows a variety of other physical phenomena to be simulated, including smoke, moving water, precipitation, and so forth. The individual particles within the system are modelled using the other elements of the physics simulation rules, with the limitation that the number of particles that can be simulated is restricted by the computing power of the hardware. Thus explosions may need to be modelled as a small set of large particles, rather than the more accurate huge number of fine particles.",
            "score": 204.93516540527344
        },
        {
            "docid": "387746_20",
            "document": "Social simulation . A model is a representation of a specific thing ranging from objects and people to structures and products created through mathematical equations and are designed, using computers, in such a way that they are able to stand-in as the aforementioned things in a study. Models can be either simplistic or complex, depending on the need for either; however, models are intended to be simpler than what they are representing while remaining realistically similar in order to be used accurately. They are built using a collection of data that is translated into computing languages that allow them to represent the system in question. These models, much like simulations, are used to help us better understand specific roles and actions of different things so as to predict behavior and the like.",
            "score": 204.75091552734375
        },
        {
            "docid": "18943937_22",
            "document": "Emulator . In 1963, when microcode was first used to speed up this simulation process, IBM engineers coined the term \"emulator\" to describe the concept. In the 2000s, it has become common to use the word \"emulate\" in the context of software. However, before 1980, \"emulation\" referred only to emulation with a hardware or microcode assist, while \"simulation\" referred to pure software emulation. For example, a computer specially built for running programs designed for another architecture is an emulator. In contrast, a simulator could be a program which runs on a PC, so that old Atari games can be simulated on it. Purists continue to insist on this distinction, but currently the term \"emulation\" often means the complete imitation of a machine executing binary code while \"simulation\" often refers to computer simulation, where a computer program is used to simulate an abstract model. Computer simulation is used in virtually every scientific and engineering domain and Computer Science is no exception, with several projects simulating abstract models of computer systems, such as network simulation, which both practically and semantically differs from network emulation.",
            "score": 203.77337646484375
        },
        {
            "docid": "586357_19",
            "document": "Artificial general intelligence . There are some research projects that are investigating brain simulation using more sophisticated neural models, implemented on conventional computing architectures. The Artificial Intelligence System project implemented non-real time simulations of a \"brain\" (with 10 neurons) in 2005. It took 50 days on a cluster of 27 processors to simulate 1 second of a model. The Blue Brain project used one of the fastest supercomputer architectures in the world, IBM's Blue Gene platform, to create a real time simulation of a single rat neocortical column consisting of approximately 10,000 neurons and 10 synapses in 2006. A longer term goal is to build a detailed, functional simulation of the physiological processes in the human brain: \"It is not impossible to build a human brain and we can do it in 10 years,\" Henry Markram, director of the Blue Brain Project said in 2009 at the TED conference in Oxford. There have also been controversial claims to have simulated a cat brain. Neuro-silicon interfaces have been proposed as an alternative implementation strategy that may scale better.",
            "score": 202.25196838378906
        },
        {
            "docid": "3740698_10",
            "document": "Direct numerical simulation . Therefore, the computational cost of DNS is very high, even at low Reynolds numbers. For the Reynolds numbers encountered in most industrial applications, the computational resources required by a DNS would exceed the capacity of the most powerful computers currently available. However, direct numerical simulation is a useful tool in fundamental research in turbulence. Using DNS it is possible to perform \"numerical experiments\", and extract from them information difficult or impossible to obtain in the laboratory, allowing a better understanding of the physics of turbulence. Also, direct numerical simulations are useful in the development of turbulence models for practical applications, such as sub-grid scale models for large eddy simulation (LES) and models for methods that solve the Reynolds-averaged Navier\u2013Stokes equations (RANS). This is done by means of \"a priori\" tests, in which the input data for the model is taken from a DNS simulation, or by \"a posteriori\" tests, in which the results produced by the model are compared with those obtained by DNS.",
            "score": 202.24090576171875
        },
        {
            "docid": "37907506_2",
            "document": "Multi-Use Simulation Models . The concept of multi-use simulation models relates to the notion of pre-designed templates that are developed for using in simulation projects which simulate repetitive activities. These models can be perceived as \u201cBuilding Blocks\u201d which are designed for a specific purpose. The chief objective of this concept is to facilitate the conceptualisation and understanding of the simulation model by non-specialist. In practice, the concept can be implemented in different contexts, mainly in the construction industry; as those who work in the development of a simulation project are interested in the efficiency and effectiveness of their simulation model rather than its underpinning mathematical complexity.",
            "score": 200.8583221435547
        },
        {
            "docid": "47152350_43",
            "document": "Human performance modeling . The network approach to modeling using these programs is popular due to its technical accessibility to individual with general knowledge of computer simulation techniques and human performance analysis. The flowcharts that result from task analysis lead naturally to formal network models. The models can be developed to serve specific purposes - from simulation of an individual using a human-computer interface to analyzing potential traffic flow in a hospital emergency center. Their weakness is the great difficulty required to derive performance times and success probabilities from previous data or from theory or first principles. These data provide the model's principle content.",
            "score": 200.41519165039062
        },
        {
            "docid": "43439629_4",
            "document": "Software Process simulation . Software process simulation starts with identifying a question that we want to answer. The question could be, for example, related to assessment of an alternative, incorporating a new practice in the software development process. Introducing such changes in the actual development process will be expensive and if the consequences of change are not positive the implications can be dire for the organization. Thus, through the use of simulation we attempt to get an initial assessment of such changes on the model instead of an active development project.  Based on this problem description an appropriate scope of the process is chosen. A simulation approach is chosen to model the development process. Such a model is then calibrated using empirical data and then used to conduct simulation based investigations. A detailed description of each step in general can be found in Balci's work, and in particular for software process simulation a comprehensive overview can be found in Ali et al.",
            "score": 200.035400390625
        },
        {
            "docid": "5626_29",
            "document": "Cognitive science . Computational models require a mathematically and logically formal representation of a problem. Computer models are used in the simulation and experimental verification of different specific and general properties of intelligence. Computational modeling can help us understand the functional organization of a particular cognitive phenomenon. There are two basic approaches to cognitive modeling. The first is focused on abstract mental functions of an intelligent mind and operates using symbols, and the second, which follows the neural and associative properties of the human brain, is called subsymbolic.",
            "score": 199.9461669921875
        },
        {
            "docid": "33818014_8",
            "document": "Nervous system network models . On a high level representation, the neurons can be viewed as connected to other neurons to form a neural network in one of three ways. A specific network can be represented as a physiologically (or anatomically) connected network and modeled that way. There are several approaches to this (see Ascoli, G.A. (2002) Sporns, O. (2007), Connectionism, Rumelhart, J. L., McClelland, J. L., and PDP Research Group (1986), Arbib, M. A. (2007)). Or, it can form a functional network that serves a certain function and modeled accordingly (Honey, C. J., Kotter, R., Breakspear, R., & Sporns, O. (2007), Arbib, M. A. (2007)). A third way is to hypothesize a theory of the functioning of the biological components of the neural system by a mathematical model, in the form of a set of mathematical equations. The variables of the equation are some or all of the neurobiological properties of the entity being modeled, such as the dimensions of the dendrite or the stimulation rate of action potential along the axon in a neuron. The mathematical equations are solved using computational techniques and the results are validated with either simulation or experimental processes. This approach to modeling is called computational neuroscience. This methodology is used to model components from the ionic level to system level of the brain. This method is applicable for modeling integrated system of biological components that carry information signal from one neuron to another via intermediate active neurons that can pass the signal through or create new or additional signals. The computational neuroscience approach is extensively used and is based on two generic models, one of cell membrane potential Goldman (1943) and Hodgkin and Katz (1949), and the other based on Hodgkin-Huxley model of action potential (information signal).",
            "score": 199.9035186767578
        }
    ]
}