{
    "q": [
        {
            "docid": "525667_10",
            "document": "Human echolocation . In a 2014 study by Thaler and colleagues, the researchers first made recordings of the clicks and their very faint echoes using tiny microphones placed in the ears of the blind echolocators as they stood outside and tried to identify different objects such as a car, a flag pole, and a tree. The researchers then played the recorded sounds back to the echolocators while their brain activity was being measured using functional magnetic resonance imaging. Remarkably, when the echolocation recordings were played back to the blind experts, not only did they perceive the objects based on the echoes, but they also showed activity in those areas of their brain that normally process visual information in sighted people, primarily primary visual cortex or V1. This result is surprising, as visual areas, as their names suggest, are only active during visual tasks. The brain areas that process auditory information were no more activated by sound recordings of outdoor scenes containing echoes than they were by sound recordings of outdoor scenes with the echoes removed. Importantly, when the same experiment was carried out with sighted people who did not echolocate, these individuals could not perceive the objects and there was no echo-related activity anywhere in the brain. This suggests that the cortex of blind echolocators is plastic and reorganizes such that primary visual cortex, rather than any auditory area, becomes involved in the computation of echolocation tasks.",
            "score": 182.90005576610565
        },
        {
            "docid": "41087200_3",
            "document": "Perceptual-based 3D sound localization . Human listeners combine information from two ears to localize and separate sound sources originating in different locations in a process called binaural hearing. The powerful signal processing methods found in the neural systems and brains of humans and other animals are flexible, environmentally adaptable, and take place rapidly and seemingly without effort. Emulating the mechanisms of binaural hearing can improve recognition accuracy and signal separation in DSP algorithms, especially in noisy environments. Furthermore, by understanding and exploiting biological mechanisms of sound localization, virtual sound scenes may be rendered with more perceptually relevant methods, allowing listeners to accurately perceive the locations of auditory events.",
            "score": 189.92213487625122
        },
        {
            "docid": "620396_41",
            "document": "Origin of language . Proponents of the motor theory of language evolution have primarily focused on the visual domain and communication through observation of movements. The \"Tool-use sound hypothesis\" suggests that the production and perception of sound, also contributed substantially, particularly \"incidental sound of locomotion\" (\"ISOL\") and \"tool-use sound\" (\"TUS\"). Human bipedalism resulted in rhythmic and more predictable \"ISOL\". That may have stimulated the evolution of musical abilities, auditory working memory, and abilities to produce complex vocalizations, and to mimic natural sounds. Since the human brain proficiently extracts information about objects and events from the sounds they produce, \"TUS\", and mimicry of \"TUS\", might have achieved an iconic function. The prevalence of sound symbolism in many extant languages supports this idea. Self-produced TUS activates multimodal brain processing (motor neurons, hearing, proprioception, touch, vision), and \"TUS\" stimulates primate audiovisual mirror neurons, which is likely to stimulate the development of association chains. Tool use and auditory gestures involve motor-processing of the forelimbs, which is associated with the evolution of vertebrate vocal communication. The production, perception, and mimicry of \"TUS\" may have resulted in a limited number of vocalizations or protowords that were associated with tool use. A new way to communicate about tools, especially when out of sight, would have had selective advantage. A gradual change in acoustic properties and/or meaning could have resulted in arbitrariness and an expanded repertoire of words. Humans have been increasingly exposed to \"TUS\" over millions of years, coinciding with the period during which spoken language evolved.",
            "score": 158.22443664073944
        },
        {
            "docid": "56533741_11",
            "document": "American Epic (documentary) . Extensive tracking shots were filmed of the landscape in each state and used as a device to demonstrate how much the geography influenced the music of the musicians in the 1920s. MacMahon explained, \u201cAs a filmmaker, I\u2019m fascinated by how the eye informs the heart. Driving through these remote locations with the film crew, we would play the music from that area in the van and it was extraordinary how closely the melodies and rhythms reflected the terrain from which they sprung. I see music visually and I think it mirrors its environment perfectly. The music of the Hopi sounded otherworldly when I first heard it, but after traveling to the Hopi reservation and having the honor of being allowed to film there, I started humming their songs. 'Chant of the Eagle Dance' now sounds like a pop single to me.\u201d He added, \u201cThings sound like the place they\u2019re from, the music makes sense. You\u2019re never going to hear Miles Davis make more sense than listening to him in a New York cab, and the Carter Family will never make more sense than if you listen to the music, watching the farm scenes from the 1920s from right where they lived.\u201d MacMahon coined the term \u201cgeographonics\u201d for this phenomenon. MacMahon went to great lengths to find the locations of old historic photographs related to the stories and made frequent use of dissolves between these old photographs and contemporary footage he shot to show the passage of time. The interviews were all filmed using an Arri Alexa on a slider or a camera dolly. All the interviews and the occasional musical performances were storyboarded by MacMahon prior to filming. \u201cWe set out to explore why particular recordings gave us particular feelings and touched particular emotions and found that an important part of that was the way they reflected particular communities and the particular geography of the places where those people lived. The more we traveled, the more we became convinced that sounds and styles arise from specific environments, and you can only truly understand them when you go where they came from. Of course, you can enjoy music without hearing it in its native setting, but we kept finding that we had never fully experienced a recording or felt it to the depth of our souls until we listened to it in its home.\u201d",
            "score": 126.61092758178711
        },
        {
            "docid": "3883287_8",
            "document": "Tranquillity . Within tranquillity studies, much of the emphasis has been placed on understanding the role of vision in the perception of natural environments, which is probably not surprising, considering that upon first viewing a scene its configurational coherence can be established with incredible speed. Indeed, scene information can be captured in a single glance and the gist of a scene determined in as little as 100ms. The speed of processing of complex natural images was tested by Thorpe \"et al.\" using colour photographs of a wide range of animals (mammals, birds, reptiles and fish), in their natural environments, mixed with distracters that included pictures of forests, mountains, lakes, buildings and fruit. During this experiment, subjects were shown an image for 20ms and asked to determine whether it contained an animal or not. The electrophysiological brain responses obtained in this study showed that a decision could be made within 150ms of the image being seen, indicating the speed at which cognitive visual processing occurs. However, audition, and in particular the individual components that collectively comprise the soundscape, a term coined by Schafer to describe the ever-present array of sounds that constitute the sonic environment, also significantly inform the various schemata used to characterise differing landscape types. This interpretation is supported by the auditory reaction times, which are 50 to 60ms faster than that of the visual modality. It is also known that sound can alter visual perception and that under certain conditions areas of the brain involved in processing auditory information can be activated in response to visual stimuli.  Research conducted by Pheasant \"et al.\" has shown that when individuals make tranquillity assessments based on a uni-modal auditory or visual sensory input, they characterise the environment by drawing upon a number of key landscape and soundscape characteristics. For example, when making assessments in response to visual-only stimuli the percentage of water, flora and geological features present within a scene, positively influence how tranquil a location is perceived to be. Likewise when responding to uni-modal auditory stimuli, the perceived loudness of biological sounds positively influences the perception of tranquillity, whilst the perceived loudness of mechanical sounds have a negative effect. However, when presented with bi-modal auditory-visual stimuli the individual soundscape and landscape components alone no longer influenced the perception of tranquillity. Rather configurational coherence was provided by the percentage of natural and contextual features present within the scene and the equivalent continuous sound pressure level (LAeq).",
            "score": 198.2120715379715
        },
        {
            "docid": "31075772_15",
            "document": "Thought identification . On 31 January 2012 Brian Pasley and colleagues of University of California Berkeley published their paper in PLoS Biology wherein subjects' internal neural processing of auditory information was decoded and reconstructed as sound on computer by gathering and analyzing electrical signals directly from subjects' brains. The research team conducted their studies on the superior temporal gyrus, a region of the brain that is involved in higher order neural processing to make semantic sense from auditory information. The research team used a computer model to analyze various parts of the brain that might be involved in neural firing while processing auditory signals. Using the computational model, scientists were able to identify the brain activity involved in processing auditory information when subjects were presented with recording of individual words. Later, the computer model of auditory information processing was used to reconstruct some of the words back into sound based on the neural processing of the subjects. However the reconstructed sounds were not of good quality and could be recognized only when the audio wave patterns of the reconstructed sound were visually matched with the audio wave patterns of the original sound that was presented to the subjects. However this research marks a direction towards more precise identification of neural activity in cognition.",
            "score": 175.17330050468445
        },
        {
            "docid": "14339999_6",
            "document": "Virtual pitch . Terhardt rejected the idea of periodicity pitch, because it was not consistent with empirical data on pitch perception, e.g. measurements of the gradual shift of the virtual pitch of a complex tone with a missing fundamental when the partials were gradually shifted. Terhardt instead broke pitch perception into two steps: auditory frequency analysis in the inner ear, and harmonic pitch pattern recognition in the brain. The inner ear effectively performs a running frequency analysis of incoming sounds - otherwise we would not be able to hear out spectral pitches within a complex tone. Physiologically, each spectral pitch depends on both temporal and spectral aspects (i.e. periodicity of the waveform and position of excitation on the basilar membrane), but in Terhardt's approach the spectral pitch itself is a purely experiential parameter, not a physical parameter: it is the outcome of a psychoacoustical experiment in which the conscious listener plays an active role. Psychoacoustic measurements and models can predict which partials are \"perceptually relevant\" in a given complex tone; they are perceptually relevant if you can hear a difference in the whole sound if the frequency or amplitude of a partial is changed). The ear has evolved to separate spectral frequencies, because due to reflection and superposition in everyday environments spectral frequencies are more reliably carriers of environmental information than spectral amplitudies, which in turn are more reliable carriers of environmentally relevant information than phase relationships between partials (when perceived monoaurally). On this basis, Terhardt proposed that spectral pitches - which are what the listener experiences when hearing out partials (as opposed to the physical partials themselves) - are the only information available to the brain for the purpose of extracting virtual pitches. The \"pitch extraction\" process then involves the recognition of incomplete harmonic patterns and happens in neural networks.",
            "score": 161.4014674425125
        },
        {
            "docid": "35988494_3",
            "document": "Selective auditory attention . The cocktail party problem was first brought up in 1953 by Colin Cherry. This common problem is how our minds solves the issue of knowing what in the auditory scene is important and combining those in a coherent whole, such as the problem of how we can perceive our friend talking in the midst of a crowded cocktail party. He suggested that the auditory system can filter sounds being heard. Physical characteristics of the auditory information such as speaker's voice or location can improve a person's ability to focus on certain stimuli even if there is other auditory stimuli present. Cherry also did work with shadowing which involves different information being played into both ears and only one ear's information can be processed and remembered (Eysneck, 2012, p.\u00a084). Another psychologist, Albert Bregman, came up with the auditory scene analysis model. The model has three main characteristics: segmentation, integration, and segregation. Segmentation involves the division of auditory messages into segments of importance. The process of combining parts of an auditory message to form a whole is associated with integration. Segregation is the separation of important auditory messages and the unwanted information in the brain. It is important to note that Bregman also makes a link back to the idea of perception. He states that it is essential for one to make a useful representation of the world from sensory inputs around us. Without perception, an individual will not recognize or have the knowledge of what is going on around them. While Begman's seminal work is critical to understanding selective auditory attention, his studies did not focus on the way in which an auditory message is selected, if and when it was correctly segregated from other sounds in a mixture, which is a critical stage of selective auditory attention. Inspired in part by Bregman's work, a number of researchers then set out to link directly work on auditory scene analysis to the processes governing attention, including Maria Chait, Mounya Elhilali, Shihab Shamma, and Barbara Shinn-Cunningham.",
            "score": 176.76003336906433
        },
        {
            "docid": "1021754_13",
            "document": "Sound localization . Sound localization is the process of determining the location of a sound source. Objectively speaking, the major goal of sound localization is to simulate a specific sound field, including the acoustic sources, the listener, the media and environments of sound propagation. The brain utilizes subtle differences in intensity, spectral, and timing cues to allow us to localize sound sources. In this section, to more deeply understand the human auditory mechanism, we will briefly discuss about human ear localization theory.",
            "score": 174.86925673484802
        },
        {
            "docid": "52705707_6",
            "document": "Near to the Wild Heart of Life . The album features a more polished aesthetic in comparison to its predecessors \"Post-Nothing\" (2009) and \"Celebration Rock\" (2012). Regarding this change in sound, Brian King noted, \"When we started the band ten years ago, we were really into a lot of raw and live sounding records by garage rock bands where the record sounded like you were going to a show. That\u2019s the kind of band we wanted to start and that's the kind of records we wanted to make. I think when we finished \"Celebration Rock\", we felt like we\u2019d achieved that.\" The band implemented the use of studio overdubs more so than in the past, and introduced new instruments including bass guitar, acoustic guitar and synthesizers: \"We didn't worry about it sounding live or only performing the songs in a way exactly like we would on stage; we worried about the whole live performance elements afterward. And that\u2019s uncharted territory for us.\" Drummer David Prowse attributes the band's change in direction to the amount of time it took to record the album: \"We wanted to make a more sonically diverse record as well. Exploring different sounds and not having it just be, like, okay Brian sets up his pedals in exactly this way, plugs in and the guitar sound is static and all those things that we\u2019ve done in the past. We were very interested in doing something different, but at the same time we didn\u2019t have a clear idea of how we were going to do that. And that\u2019s part of what took time.\"",
            "score": 100.90814566612244
        },
        {
            "docid": "146002_14",
            "document": "Pet Sounds . Author Michael Zager wrote that \"Pet Sounds\" resembles Spector's productions more than it does \"Rubber Soul\", and that it recycles many of Spector's Wall of Sound production watermarks. Wilson talked of Spector's influence on his work, having learned how to produce records through attending his sessions, and in a 1988 interview, Wilson asserted that the album aspired to \"extend\" Spector's music, that \"in one sense of the word, we [the Beach Boys] were his messengers.\" In \"Pet Sounds\", Wilson sought to emulate Spector's Wall of Sound, driven by his fascination with \"combining one [sound] to make another\u00a0... It's amazing\". Shortly before the album's release, Wilson spoke of recent popular music trends, saying \"[They] helped the Beach Boys evolve. We listen to what's happening and it affects what we do too. The trends have influenced my work, but so has my own scene.\" In 1996, Wilson said of his partnership with Asher: \"none of us looked back\u00a0... We went forward, kind of like on our own little wavelength. It wasn't like we were thinking, 'Okay, let's beat Spector,' let's out-do Motown.' It was more what I would call exclusive collaboration not to specifically try to kick somebody's butt, but just to do it the way you really want it to be. That's what I thought we did.\"",
            "score": 124.17181503772736
        },
        {
            "docid": "32116125_4",
            "document": "Amblyaudia . Amblyaudia is a deficit in binaural integration of environmental information entering the auditory system. It is a disorder related to brain organization and function rather than what is typically considered a \u201chearing loss\u201d (damage to the cochlea). It may be genetic or developmentally acquired or both. When animals are temporarily deprived of hearing from an early age, profound changes occur in the brain. Specifically, cell sizes in brainstem nuclei are reduced, the configuration of brainstem dendrites are altered and neurons respond in different ways to sounds presented to both the deprived and non-deprived ears (in cases of asymmetric deprivation). This last point is particularly important for listening tasks that require inputs from two ears to perform well. There are multiple auditory functions that rely on the computation of well calibrated inputs from the two ears. Chief among these is the ability to localize sound sources and separate what we want to hear from a background of noise. In the brainstem, the auditory system compares the timing and levels of sounds between the two ears to encode the location of sound sources (sounds that originate from our right as opposed to left side are louder and arrive earlier in our right ear). This ability to separate sound sources not only helps us locate the trajectories of moving objects, but also to separate different sound sources in noisy environments.",
            "score": 169.99323391914368
        },
        {
            "docid": "29354346_7",
            "document": "Change deafness . One study used fMRI data to distinguish neural correlates of physical changes in auditory input (independent of conscious change detection), from those of conscious perception of change (independent of an actual physical change). The study made use of a change deafness paradigm in which participants were exposed to complex auditory scenes consisting of six individual auditory streams differing in pitch, rhythm, and sound source location, and received a cue indicating which stream to attend to. Each participant listened to two consecutively presented auditory scenes after which they were prompted to indicate whether both scenes were identical or not. Functional MRI results revealed that physical change in stimulus was correlated with increased BOLD responses in the right auditory cortex, near the lateral portion of Heschl's gyrus, the first cortical structure to process incoming auditory information, but not in hierarchically higher brain regions. Conscious change detection was correlated with increased coupled responses in the ACC and the right insula, consistent with additional evidence that the anterior insula functions to mediate dynamic interactions between other brain networks involved in attention to external stimuli, forming a salience network with the ACC that identifies salient stimulus events and initiates additional processing. In absence of change detection, this salience network was not activated; however increased activity in other cortical areas suggests that undetected changes are still perceived on some level, but fail to trigger conscious change detection, thus producing the change deafness phenomenon.",
            "score": 192.10690295696259
        },
        {
            "docid": "5366050_62",
            "document": "Speech perception . The fuzzy logical theory of speech perception developed by Dominic Massaro proposes that people remember speech sounds in a probabilistic, or graded, way. It suggests that people remember descriptions of the perceptual units of language, called prototypes. Within each prototype various features may combine. However, features are not just binary (true or false), there is a fuzzy value corresponding to how likely it is that a sound belongs to a particular speech category. Thus, when perceiving a speech signal our decision about what we actually hear is based on the relative goodness of the match between the stimulus information and values of particular prototypes. The final decision is based on multiple features or sources of information, even visual information (this explains the McGurk effect). Computer models of the fuzzy logical theory have been used to demonstrate that the theory's predictions of how speech sounds are categorized correspond to the behavior of human listeners.",
            "score": 122.45877146720886
        },
        {
            "docid": "1307428_6",
            "document": "Dosage (album) . \"Dosage\" marked a change in recording and style for Collective Soul. Unhappy with the production and sound of the previous album \"Disciplined Breakdown\", the band focused more on production and technique for the recording of \"Dosage\". The result led to an arduous six-month recording period where according to guitarist Dean Roland: \"The way we recorded \"Dosage\", we were really meticulous about everything that we did for that record.\" More than before the band progressed to a high production pop-rock sound. This status was obvious with the extensive use of loops, Pro Tools effects, and synth-pop sounds, especially in comparison with the band's previous three albums which are more organic and raw in sound. \"Anthony brought an unbelievable spirit and attitude\" Ed continues, \"He has more gadgets than we do so we all had a good time just plugging stuff in and seeing what sounds would work\".  Dean Roland \" We started working with Anthony J. Resta when we recorded Dosage and we just met him through another friend of ours. And initially, he just came in to help us with some programming ideas and drum arrangements, he\u2019s an amazing drummer and he\u2019s got that natural rhythmic thing going on. So he came in there and helped us with some of those things and that relationship just grew into other things you know like ultimately co-producing and co-writing with us. He\u2019s just a great soul, a great spirit to have around when you\u2019re working and creating.",
            "score": 98.54557740688324
        },
        {
            "docid": "8953842_2",
            "document": "Computational auditory scene analysis . Computational auditory scene analysis (CASA) is the study of auditory scene analysis by computational means. In essence, CASA systems are \"machine listening\" systems that aim to separate mixtures of sound sources in the same way that human listeners do. CASA differs from the field of blind signal separation in that it is (at least to some extent) based on the mechanisms of the human auditory system, and thus uses no more than two microphone recordings of an acoustic environment. It is related to the cocktail party problem.",
            "score": 188.84894585609436
        },
        {
            "docid": "5366050_50",
            "document": "Speech perception . Neurophysiological methods rely on utilizing information stemming from more direct and not necessarily conscious (pre-attentative) processes. Subjects are presented with speech stimuli in different types of tasks and the responses of the brain are measured. The brain itself can be more sensitive than it appears to be through behavioral responses. For example, the subject may not show sensitivity to the difference between two speech sounds in a discrimination test, but brain responses may reveal sensitivity to these differences. Methods used to measure neural responses to speech include event-related potentials, magnetoencephalography, and near infrared spectroscopy. One important response used with event-related potentials is the mismatch negativity, which occurs when speech stimuli are acoustically different from a stimulus that the subject heard previously.",
            "score": 125.00418436527252
        },
        {
            "docid": "39364726_36",
            "document": "Birdman (film) . Sound design for the film was handled by Mart\u00edn Hern\u00e1ndez, who has worked on all the director's feature films. The production sound his team was given was \"surprisingly perfect and clean\", so they didn't need to spend much time tidying it. The lack of visual cuts in the film meant a departure from the usual way Hern\u00e1ndez edited sound though. \"Normally Alejandro likes, as he calls it, 'the clashing of the sounds.' The camera changes angle within the scene and there is a change of everything backgrounds, texture, dialog. Obviously, that would not work on \"Birdman\" because here, everything is flowing.\" Additionally, the sound of objects in the film needed to be coordinated with their position onscreen, a non-trivial task since the camera was nearly always moving. Hern\u00e1ndez also helped to cut S\u00e1nchez's tracks, selecting and editing moments from the New York recordings against the film. When the tracks were re-recording in Los Angeles, in order \"to have distance, and closeness and resonance\" the team used thirty two microphones for each take. Hern\u00e1ndez described the job as \"crazy\". A large challenge of the post-production soundwork was designing the sound of the theatre audience, particularly in the scene when Mike is drunk and comes out of character. I\u00f1\u00e1rritu knew how he wanted the audience to respond, but figuring out the sound of their reaction took four months of work from five sound designers. Hern\u00e1ndez said he thought, for a film in general, that the narrative created by sound \"provokes more 'explosions'\" than that created by images, and that in \"Birdman\" \"Alejandro has sound exploding all throughout the film.\"",
            "score": 124.80099952220917
        },
        {
            "docid": "37494768_4",
            "document": "Notes from the Underground (Hollywood Undead album) . In an interview with Keven Skinner of \"The Daily Blam\", Charlie Scene revealed more information about the album's details. He revealed that there may be collaborations with guest artists on the album. \"[Collaborations] would be awesome. I think it's bad to do [them] on your first couple records, to ask people on, but I think that the third record is kind of the point where maybe you can have somebody featured. I think it would be really cool to have someone else sing a chorus on one of our songs or do a verse.\" When asked about the masks, he replied that they will be upgrading their masks for the next album as well, as they did with the previous two albums. Charlie Scene also explained that the third album will be released much sooner than \"American Tragedy\" was, and predicts it will be released by the summer of 2012. \"We have a bunch of songs written and we're writing more on the road. We brought some studio equipment with us so we've been able to work on stuff while we're touring. After this tour and we're back home, we'll be able to work with producers that we want to work with and dial all the stuff in that we have \u2013 skeletons of songs. We have some really good stuff that we're excited to work on for sure. One thing that we definitely don't want to do is take as long as it took for us to do 'American Tragedy' so there will definitely be a third record out next year and we're hoping for at the latest \u2013 summer.\" He also explained that the album will sound more like \"Swan Songs\" did, in the lines of it having more party tracks than \"American Tragedy\" did.",
            "score": 85.18861603736877
        },
        {
            "docid": "4281315_7",
            "document": "Storm the Studio . A sample from children's series \"Rainbow\", in which Zippy says \"You're supposed to listen to the rhythm George, the rhythm of the music,\" was explained by Adams: \"There's not much else you can do when you're on the dole except watch \"Rainbow\".\" The band taped every episode of the show for three years looking for the particular scene so they could sample it, with Adams explaining: \"You never know what's going to sound good on the finished product.\" He had recorded the squeak of his sneaker on the ground, then sampled it and slowed it down, but he felt it \"sounded really stupid\" so did not use it on the album. One of the album's television-sourced samples, a news broadcaster saying \"a spokesman at the Health Ministry said that to talk repeatedly about AIDS would cause the public to panic, tourism will certainly be affected,\" was the result of what the band called a 'random edit,' though the band kept it in the album because they felt it showed how the news \"sounded like they were more concerned with tourism than peoples' lives. It was something that would make you think, rather than the 'DJ get on down' stuff.\"",
            "score": 123.16728472709656
        },
        {
            "docid": "5292823_7",
            "document": "Coming Home (New Found Glory album) . The band entered Jackson Browne's private recording studio named Groovemasters in January 2006, after Panunzio had suggested it would be a suitable recording location. New Found Glory strove to achieve a \"clean kind of classic guitar sound\" when recording, using a Vox AC30 amp on almost the entire record. The amp, known for its \"jangly\" high-end sound, was used with several classic guitars in the studio including a Fender Tele, Les Paul, Gibson 335-S and a Rickenbacker. Gilbert enthused that, \"It sounds huge. When you put our old records on and our new record, there's actually less guitars on our new album, but it sounds bigger.\" Jordan Pundik likewise accounted; \"he [Panunzio] brought this classic vibe to it, especially with the tones he got. We learned we don't have to double-up 15 Mesa cabinets and make it all distorted to make it sound big.\" Pundik also spoke of the band's desire to challenge themselves musically; \"Usually with every record we think, 'We\u2019ve got to put the fast punk song on it or people won't like it', but this wasn't anything like that.\" He did admit that around thirty songs were written, including some fast-paced songs, but were excluded as, \"(they) didn't really fit.\" Steve Klein, the band's principal lyricist and rhythm guitarist, also praised Panunzio for helping the band bring new elements to their sound. Describing the sessions as \"best recording experience ever\", he added, \"It's this empty mansion where we were able to set up all our equipment, we just woke up and wrote songs. We were really relaxed and able to set our own pace. Everything about the record is way more classic rock sounding because Thom has done a bunch of classic rock records like Tom Petty and Bruce Springsteen and Ozzy, and the list goes on and on. He kind of brought this different element to our band. This disc is less guitar driven and more melody driven, more than any other of our records.\"",
            "score": 96.81123435497284
        },
        {
            "docid": "23158496_4",
            "document": "Superior temporal sulcus . In individuals without autism, the superior temporal sulcus also activates when hearing human voices. It is thought to be a source of sensory encoding linked to motor output through the superior parietal-temporal areas of the brain inferred from the time course of activation. The conclusion of pertinence to vocal processing can be drawn from data showing that the regions of the STS (superior temporal sulcus) are more active when people are listening to vocal sounds rather than non-vocal environmentally based sounds and corresponding control sounds, which can be scrambled or modulated voices. These experimental results indicate the involvement of the STS in the areas of speech and language recognition.",
            "score": 154.67312037944794
        },
        {
            "docid": "8953842_15",
            "document": "Computational auditory scene analysis . While many models consider the audio signal as a complex combination of different frequencies, modeling the auditory system can also require consideration for the neural components. By taking a holistic process, where a stream (of feature-based sounds) correspond to neuronal activity distributed in many brain areas, the perception of the sound could be mapped and modeled. Two different solutions have been proposed to the binding of the audio perception and the area in the brain. Hierarchical coding models many cells to encode all possible combinations of features and objects in the auditory scene. Temporal or oscillatory correlation addressing the binding problem by focusing on the synchrony and desynchrony between neural oscillations to encode the state of binding among the auditory features. These two solutions are very similar to the debacle between place coding and temporal coding. While drawing from modeling neural components, another phenomenon of ASA comes into play with CASA systems: the extent of modeling neural mechanisms. The studies of CASA systems have involved modeling some known mechanisms, such as the bandpass nature of cochlear filtering and random auditory nerve firing patterns, however, these models may not lead to finding new mechanisms, but rather give an understanding of purpose to the known mechanisms.",
            "score": 177.80664563179016
        },
        {
            "docid": "4220231_8",
            "document": "Evolutionary musicology . The evolutionary switch to bipedalism may have influenced the origins of music. The background is that noise of locomotion and ventilation may mask critical auditory information. Human locomotion is likely to produce more predictable sounds than those of non-human primates. Predictable locomotion sounds may have improved our capacity of entrainment to external rhythms and to feel the beat in music. A sense of rhythm could aid the brain in distinguishing among sounds arising from discrete sources and also help individuals to synchronize their movements with one another. Synchronization of group movement may improve perception by providing periods of relative silence and by facilitating auditory processing. The adaptive value of such skills to early human ancestors may have been keener detection of prey or stalkers and enhanced communication. Thus, bipedal walking may have influenced the development of entrainment in humans and thereby the evolution of rhythmic abilities. Primitive hominids lived and moved around in small groups. The noise generated by the locomotion of two or more individuals can result in a complicated mix of footsteps, breathing, movements against vegetation, echoes, etc. The ability to perceive differences in pitch, rhythm, and harmonies, i.e. \u201cmusicality,\u201d could help the brain to distinguish among sounds arising from discrete sources, and also help the individual to synchronize movements with the group. Endurance and an interest in listening might, for the same reasons, have been associated with survival advantages eventually resulting in adaptive selection for rhythmic and musical abilities and reinforcement of such abilities. Listening to music seems to stimulate release of dopamine. Rhythmic group locomotion combined with attentive listening in nature may have resulted in reinforcement through dopamine release. A primarily survival-based behavior may eventually have attained similarities to dance and music, due to such reinforcement mechanisms . Since music may facilitate social cohesion, improve group effort, reduce conflict, facilitate perceptual and motor skill development, and improve trans-generational communication, music-like behavior may at some stage have become incorporated into human culture.",
            "score": 156.48262476921082
        },
        {
            "docid": "27300499_3",
            "document": "Alphabet f\u00fcr Li\u00e8ge . The fundamental idea underlying \"Alphabet\" is the notion that sound vibrations can affect both living beings and inanimate matter . There are thirteen \"scenes\", or \"musical images\", each illustrating the physical effects of sound, ranging from making acoustic vibrations visible to a demonstration of Asian mantra techniques. These ideas were developed in conversations with the British biophysicist and lecturer on mystical aspects of sound vibration Jill Purce, who also called Stockhausen's attention to the work of Hans Jenny (; ). In a radio interview three months before the premiere, Stockhausen explained his purpose was to show \"how sound waves always change the molecules, even the atoms of a being who listens to music, making them vibrate. And that is what we want to make visible, because most people only believe what they see\" .",
            "score": 127.3842521905899
        },
        {
            "docid": "45204814_27",
            "document": "Development of The Last of Us . The sound design team began working on the game early in development, in order to achieve the best results; they immediately realised that it would be challenging. Early in development, Druckmann told the sound team to \"make it subtle\", and underplay ideas. Audio lead Phillip Kovats was excited to completely create all sounds; no sounds were carried across from previous games. The team looked at ways to create sounds from a naturalistic point-of-view, and how to introduce minimalism into a game. By doing so, they found that it added feelings of tension, loss and hope, and that the game appeared to be a typical \"action game\" without the minimalism approach. They used a high dynamic range, allowing them the opportunity to inform players on tactical information, and locations to explore. The game's sound design was created to reflect a more \"grounded\" and subtle mood than \"Uncharted\", particularly focusing on the lack of sound. Taking inspiration from \"No Country for Old Men\", the team attempted to \"do more with less\"; Kovats said that the team was trying to tell a story by \"going for a reductive quality\". Straley stated that the audio is vital to some scenes in the game; \"It's more about the psychology of what's happening on the audioscape than what you're seeing,\" he stated. He felt that this decision allowed a more impactful and meaningful effect with sound occurred. The sound team also attempted to portray the game's dark themes through sound. The team felt that it was important to let sounds play for as long as possible in the game, drawing tension. The team used a propagation technique to help players determine the exact locations of enemies, using this as a tactical advantage. This system, created by the team at Naughty Dog, is processed at random in the game engine. For the game's audio, the engine throws out 1500\u20132500 ray casts per frame; though most games avoid this, the game's engine allowed it to work. The team spent a lot of time recording sounds for the game, namely doors, and rusty metal. Sound designer Neil Uchitel traveled to Rio de Janeiro, discovering locations to record sounds; he recorded chickens, which were used in the game as the voices of rats. The team continued to add and change the game's sounds until the end of development. To create the sound of the Clickers, the third stage of the Infected, the team found inspiration from a report on the journalism show \"20/20\" (1978\u2013present) about blind children using clicking noises to communicate. \"We liked the idea of taking this benign sound and attributing it to something really scary,\" said Druckmann. In addition, the team wanted to reflect the creature's feeling of pain and suffering, attempting to balance it with a sense of creepiness. The sound team created the sound of the Clicker first, realising early that it was the most challenging. To create the sound, they hired voice actors to perform their renditions. When voice actress Misty Lee provided her own rendition, a noise that Kovats described as originating in the \"back of the throat\", Kovats and senior sound designer Derrick Espino agreed that it was what they wanted. Kovats then emulated the sound to feature in the game.",
            "score": 121.32453191280365
        },
        {
            "docid": "29354346_5",
            "document": "Change deafness . Another study examined the effect of selective attention on the perception of changes to auditory scenes consisting of multiple naturalistic sounds, and found that auditory perception is limited by attention. In the task, listeners heard two versions of any auditory scene, with one object missing from the second version. Participants were either instructed to attend to a specific object, and report whether that object was missing in the second version of the scene, or to attend to all objects, and report whether any object was missing in the second scene; these are called the directed- and non-directed attention conditions respectively. Results showed that in the absence of an attentional cue, change-detection in auditory scenes consisting of more than about four objects is unreliable, where changes consist of either the disappearance of an object or a change in its location. It is important to note the ambiguity concerning the mechanism that produces the effect of attention on change-deafness, and this study suggests two possibilities. The first is that segregation of the distinct streams composing an auditory scene requires directed attention, meaning that the change-deafness effects observed in the study would reflect a difficulty in perceiving separate auditory scenes in the absence of attentional cues. A second alternative is that complex auditory scenes are initially perceived as consisting of separate streams, and thus change-deafness effects are the result of limits in encoding and storing multiple sets of auditory information for comparison with a subsequent scene.",
            "score": 154.23295783996582
        },
        {
            "docid": "213270_22",
            "document": "Back to the Future Part II . According to Zemeckis, the 2015 depicted in the film was not meant to be an accurate depiction of the future. \"For me, filming the future scenes of the movie were the least enjoyable of making the whole trilogy, because I don't really like films that try and predict the future. The only ones I've actually enjoyed were the ones done by Stanley Kubrick, and not even he predicted the PC when he made \"A Clockwork Orange\". So, rather than trying to make a scientifically sound prediction that we were probably going to get wrong anyway, we figured, let's just make it funny.\" Despite this, the filmmakers did do some research into what scientists thought may occur in the year 2015. Bob Gale said, \"We knew we weren't going to have flying cars by the year 2015, but God we had to have those in our movie.\"",
            "score": 78.29209566116333
        },
        {
            "docid": "38523090_16",
            "document": "Statistical learning in language acquisition . Since the discovery of infants\u2019 statistical learning abilities in word learning, the same general mechanism has also been studied in other facets of language learning. For example, it is well-established that infants can discriminate between phonemes of many different languages but eventually become unable to discriminate between phonemes that do not appear in their native language; however, it was not clear how this decrease in discriminatory ability came about. Maye et al. suggested that the mechanism responsible might be a statistical learning mechanism in which infants track the distributional regularities of the sounds in their native language. To test this idea, Maye et al. exposed 6- and 8-month-old infants to a continuum of speech sounds that varied on the degree to which they were voiced. The distribution that the infants heard was either bimodal, with sounds from both ends of the voicing continuum heard most often, or unimodal, with sounds from the middle of the distribution heard most often. The results indicated that infants from both age groups were sensitive to the distribution of phonemes. At test, infants heard either non-alternating (repeated exemplars of tokens 3 or 6 from an 8-token continuum) or alternating (exemplars of tokens 1 and 8) exposures to specific phonemes on the continuum. Infants exposed to the bimodal distribution listened longer to the alternating trials than the non-alternating trials while there was no difference in listening times for infants exposed to the unimodal distribution. This finding indicates that infants exposed the bimodal distribution were better able to discriminate sounds from the two ends of the distribution than were infants in the unimodal condition, regardless of age. This type of statistical learning differs from that used in lexical acquisition, as it requires infants to track frequencies rather than transitional probabilities, and has been named \u201cdistributional learning.\u201d",
            "score": 136.55897665023804
        },
        {
            "docid": "20544661_3",
            "document": "The Happiness Project . The project sprang from casual interviews with people in Spearin's neighbourhood on the subject of happiness. After each interview I would listen back to the recording for moments that were interesting in both meaning and melody. By meaning I mean the thoughts expressed, by melody I mean the cadence and inflection that give the voice a sing-song quality. It has always been interesting to me how we use sounds to convey concepts. Normally, we don\u2019t pay any attention to the movement of our lips and tongue, and the rising and falling of our voices as we toss our thoughts back and forth to each other. We just talk and listen. The only time we pay attention to these qualities is in song. (Just as when we read we don\u2019t pay attention to the curl and swing of the letters as though they were little drawings.)",
            "score": 78.89475572109222
        },
        {
            "docid": "1677048_66",
            "document": "Inattentional blindness . William James addressed the benefits of attention by saying, \"Only those items which I notice shape my mind \u2013 without selective interest, experience is utter chaos\". Humans have a limited mental capacity that is incapable of attending to all the sights, sounds and other inputs that rush the senses every moment. Inattentional blindness is beneficial in the sense that it is a mechanism that has evolved with attention to help filter out irrelevant input, allowing only important information to reach consciousness. Several researchers, notably James J. Gibson, have argued that, even before the retina, perception begins in the ecology, which has turned perceptual processes into informational relationships in the environment through evolution. This allows humans to focus our limited mental resources more efficiently in our environment. For example, New et al. maintain that survival required monitoring animals, both human and non-human, to become part of the evolutionary adaptiveness of the human species. They found that when participants were shown an image with a rapidly altering scene where the scene change included an animate or inanimate object that the participants were significantly better at identifying humans and animals. New et al. argue that better performance in detecting animals and humans is not a factor of acquired expertise, rather it is an evolved survival mechanism in human perception.",
            "score": 109.72342562675476
        },
        {
            "docid": "35988494_6",
            "document": "Selective auditory attention . The prevalence of selective hearing has not been clearly researched yet. However, there are some that have argued that the proportion of selective hearing is particularly higher in males than females. Ida Z\u00fcndorf, Hans-Otto Karnath and J\u00f6rg Lewald carried out a study in 2010 which investigated the advantages and abilities males have in the localization of auditory information. A sound localization task centered on the cocktail party effect was utilized in their study. The male and female participants had to try to pick out sounds from a specific source, on top of other competing sounds from other sources. The results showed that the males had a better performance overall. Female participants found it more difficult to locate target sounds in a multiple-source environment. Z\u00fcndorf et al. suggested that there may be sex differences in the attention processes that helped locate the target sound from a multiple-source auditory field. While men and women do have some differences when it comes to selective auditory hearing, they both struggle when presented with the challenge of multitasking, especially when tasks that are to be attempted concurrently are very similar in nature (Dittrich, and Stahl, 2012, p.\u00a0626).",
            "score": 128.37188255786896
        }
    ],
    "r": [
        {
            "docid": "3883287_8",
            "document": "Tranquillity . Within tranquillity studies, much of the emphasis has been placed on understanding the role of vision in the perception of natural environments, which is probably not surprising, considering that upon first viewing a scene its configurational coherence can be established with incredible speed. Indeed, scene information can be captured in a single glance and the gist of a scene determined in as little as 100ms. The speed of processing of complex natural images was tested by Thorpe \"et al.\" using colour photographs of a wide range of animals (mammals, birds, reptiles and fish), in their natural environments, mixed with distracters that included pictures of forests, mountains, lakes, buildings and fruit. During this experiment, subjects were shown an image for 20ms and asked to determine whether it contained an animal or not. The electrophysiological brain responses obtained in this study showed that a decision could be made within 150ms of the image being seen, indicating the speed at which cognitive visual processing occurs. However, audition, and in particular the individual components that collectively comprise the soundscape, a term coined by Schafer to describe the ever-present array of sounds that constitute the sonic environment, also significantly inform the various schemata used to characterise differing landscape types. This interpretation is supported by the auditory reaction times, which are 50 to 60ms faster than that of the visual modality. It is also known that sound can alter visual perception and that under certain conditions areas of the brain involved in processing auditory information can be activated in response to visual stimuli.  Research conducted by Pheasant \"et al.\" has shown that when individuals make tranquillity assessments based on a uni-modal auditory or visual sensory input, they characterise the environment by drawing upon a number of key landscape and soundscape characteristics. For example, when making assessments in response to visual-only stimuli the percentage of water, flora and geological features present within a scene, positively influence how tranquil a location is perceived to be. Likewise when responding to uni-modal auditory stimuli, the perceived loudness of biological sounds positively influences the perception of tranquillity, whilst the perceived loudness of mechanical sounds have a negative effect. However, when presented with bi-modal auditory-visual stimuli the individual soundscape and landscape components alone no longer influenced the perception of tranquillity. Rather configurational coherence was provided by the percentage of natural and contextual features present within the scene and the equivalent continuous sound pressure level (LAeq).",
            "score": 198.21206665039062
        },
        {
            "docid": "29354346_7",
            "document": "Change deafness . One study used fMRI data to distinguish neural correlates of physical changes in auditory input (independent of conscious change detection), from those of conscious perception of change (independent of an actual physical change). The study made use of a change deafness paradigm in which participants were exposed to complex auditory scenes consisting of six individual auditory streams differing in pitch, rhythm, and sound source location, and received a cue indicating which stream to attend to. Each participant listened to two consecutively presented auditory scenes after which they were prompted to indicate whether both scenes were identical or not. Functional MRI results revealed that physical change in stimulus was correlated with increased BOLD responses in the right auditory cortex, near the lateral portion of Heschl's gyrus, the first cortical structure to process incoming auditory information, but not in hierarchically higher brain regions. Conscious change detection was correlated with increased coupled responses in the ACC and the right insula, consistent with additional evidence that the anterior insula functions to mediate dynamic interactions between other brain networks involved in attention to external stimuli, forming a salience network with the ACC that identifies salient stimulus events and initiates additional processing. In absence of change detection, this salience network was not activated; however increased activity in other cortical areas suggests that undetected changes are still perceived on some level, but fail to trigger conscious change detection, thus producing the change deafness phenomenon.",
            "score": 192.10690307617188
        },
        {
            "docid": "8953380_4",
            "document": "Auditory scene analysis . Sound reaches the ear and the eardrum vibrates as a whole. This signal has to be analyzed (in some way). Bregman's ASA model proposes that sounds will either be heard as \"integrated\" (heard as a whole \u2013 much like harmony in music), or \"segregated\" into individual components (which leads to counterpoint). For example, a bell can be heard as a 'single' sound (integrated), or some people are able to hear the individual components \u2013 they are able to segregate the sound. This can be done with chords where it can be heard as a 'color', or as the individual notes. Natural sounds, such as the human voice, musical instruments, or cars passing in the street, are made up of many frequencies, which contribute to the perceived quality (like timbre) of the sounds. When two or more natural sounds occur at once, all the components of the simultaneously active sounds are received at the same time, or overlapped in time, by the ears of listeners. This presents their auditory systems with a problem: which parts of the sound should be grouped together and treated as parts of the same source or object? Grouping them incorrectly can cause the listener to hear non-existent sounds built from the wrong combinations of the original components.",
            "score": 189.99342346191406
        },
        {
            "docid": "41087200_3",
            "document": "Perceptual-based 3D sound localization . Human listeners combine information from two ears to localize and separate sound sources originating in different locations in a process called binaural hearing. The powerful signal processing methods found in the neural systems and brains of humans and other animals are flexible, environmentally adaptable, and take place rapidly and seemingly without effort. Emulating the mechanisms of binaural hearing can improve recognition accuracy and signal separation in DSP algorithms, especially in noisy environments. Furthermore, by understanding and exploiting biological mechanisms of sound localization, virtual sound scenes may be rendered with more perceptually relevant methods, allowing listeners to accurately perceive the locations of auditory events.",
            "score": 189.92213439941406
        },
        {
            "docid": "8953842_2",
            "document": "Computational auditory scene analysis . Computational auditory scene analysis (CASA) is the study of auditory scene analysis by computational means. In essence, CASA systems are \"machine listening\" systems that aim to separate mixtures of sound sources in the same way that human listeners do. CASA differs from the field of blind signal separation in that it is (at least to some extent) based on the mechanisms of the human auditory system, and thus uses no more than two microphone recordings of an acoustic environment. It is related to the cocktail party problem.",
            "score": 188.84893798828125
        },
        {
            "docid": "11920671_6",
            "document": "Machine perception . Machine hearing, also known as machine listening or computer audition, is the ability of a computer or machine to take in and process sound data such as music or speech. This area has a wide range of application including music recording and compression, speech synthesis, and speech recognition. Moreover, this technology allows the machine to replicate the human brain\u2019s ability to selectively focus in a specific sound against many other competing sounds and background noise. This particular ability is called \u201cauditory scene analysis\u201d. The technology enables the machine to segment several streams occurring at the same time. Many commonly used devices such as a smartphones, voice translators, and cars make use of some form of machine hearing.",
            "score": 184.96958923339844
        },
        {
            "docid": "525667_10",
            "document": "Human echolocation . In a 2014 study by Thaler and colleagues, the researchers first made recordings of the clicks and their very faint echoes using tiny microphones placed in the ears of the blind echolocators as they stood outside and tried to identify different objects such as a car, a flag pole, and a tree. The researchers then played the recorded sounds back to the echolocators while their brain activity was being measured using functional magnetic resonance imaging. Remarkably, when the echolocation recordings were played back to the blind experts, not only did they perceive the objects based on the echoes, but they also showed activity in those areas of their brain that normally process visual information in sighted people, primarily primary visual cortex or V1. This result is surprising, as visual areas, as their names suggest, are only active during visual tasks. The brain areas that process auditory information were no more activated by sound recordings of outdoor scenes containing echoes than they were by sound recordings of outdoor scenes with the echoes removed. Importantly, when the same experiment was carried out with sighted people who did not echolocate, these individuals could not perceive the objects and there was no echo-related activity anywhere in the brain. This suggests that the cortex of blind echolocators is plastic and reorganizes such that primary visual cortex, rather than any auditory area, becomes involved in the computation of echolocation tasks.",
            "score": 182.90005493164062
        },
        {
            "docid": "20242841_13",
            "document": "Albert Bregman . He developed the concept of auditory stream segregation (also called \"streaming\") to describe how a single sequence of sounds could be interpreted by the auditory system as two or more concurrent streams of sound. Extensive research by Bregman and his students and postdoctoral fellows exposed many of the acoustic variables that controlled this process. Eventually he came to think of streaming as a part of a larger auditory process, which he called \"auditory scene analysis\" (ASA), a process responsible for analyzing the complex mixture of sound that reaches the listener's ears and for building distinct perceptual representations of the individual acoustic sources that were buried in the mixture.",
            "score": 180.3551788330078
        },
        {
            "docid": "8953380_5",
            "document": "Auditory scene analysis . In many circumstances the segregated elements can be linked together in time, producing an auditory stream. This ability of auditory streaming can be demonstrated by the so-called cocktail party effect. Up to a point, with a number of voices speaking at the same time or with background sounds, one is able to follow a particular voice even though other voices and background sounds are present. In this example, the ear is segregating this voice from other sounds (which are integrated), and the mind \"streams\" these segregated sounds into an auditory stream. This is a skill which is highly developed by musicians, notably conductors who are able to listen to one, two, three or more instruments at the same time (segregating them), and following each as an independent line through auditory streaming.",
            "score": 180.04299926757812
        },
        {
            "docid": "8953842_15",
            "document": "Computational auditory scene analysis . While many models consider the audio signal as a complex combination of different frequencies, modeling the auditory system can also require consideration for the neural components. By taking a holistic process, where a stream (of feature-based sounds) correspond to neuronal activity distributed in many brain areas, the perception of the sound could be mapped and modeled. Two different solutions have been proposed to the binding of the audio perception and the area in the brain. Hierarchical coding models many cells to encode all possible combinations of features and objects in the auditory scene. Temporal or oscillatory correlation addressing the binding problem by focusing on the synchrony and desynchrony between neural oscillations to encode the state of binding among the auditory features. These two solutions are very similar to the debacle between place coding and temporal coding. While drawing from modeling neural components, another phenomenon of ASA comes into play with CASA systems: the extent of modeling neural mechanisms. The studies of CASA systems have involved modeling some known mechanisms, such as the bandpass nature of cochlear filtering and random auditory nerve firing patterns, however, these models may not lead to finding new mechanisms, but rather give an understanding of purpose to the known mechanisms.",
            "score": 177.806640625
        },
        {
            "docid": "35988494_3",
            "document": "Selective auditory attention . The cocktail party problem was first brought up in 1953 by Colin Cherry. This common problem is how our minds solves the issue of knowing what in the auditory scene is important and combining those in a coherent whole, such as the problem of how we can perceive our friend talking in the midst of a crowded cocktail party. He suggested that the auditory system can filter sounds being heard. Physical characteristics of the auditory information such as speaker's voice or location can improve a person's ability to focus on certain stimuli even if there is other auditory stimuli present. Cherry also did work with shadowing which involves different information being played into both ears and only one ear's information can be processed and remembered (Eysneck, 2012, p.\u00a084). Another psychologist, Albert Bregman, came up with the auditory scene analysis model. The model has three main characteristics: segmentation, integration, and segregation. Segmentation involves the division of auditory messages into segments of importance. The process of combining parts of an auditory message to form a whole is associated with integration. Segregation is the separation of important auditory messages and the unwanted information in the brain. It is important to note that Bregman also makes a link back to the idea of perception. He states that it is essential for one to make a useful representation of the world from sensory inputs around us. Without perception, an individual will not recognize or have the knowledge of what is going on around them. While Begman's seminal work is critical to understanding selective auditory attention, his studies did not focus on the way in which an auditory message is selected, if and when it was correctly segregated from other sounds in a mixture, which is a critical stage of selective auditory attention. Inspired in part by Bregman's work, a number of researchers then set out to link directly work on auditory scene analysis to the processes governing attention, including Maria Chait, Mounya Elhilali, Shihab Shamma, and Barbara Shinn-Cunningham.",
            "score": 176.76002502441406
        },
        {
            "docid": "31075772_15",
            "document": "Thought identification . On 31 January 2012 Brian Pasley and colleagues of University of California Berkeley published their paper in PLoS Biology wherein subjects' internal neural processing of auditory information was decoded and reconstructed as sound on computer by gathering and analyzing electrical signals directly from subjects' brains. The research team conducted their studies on the superior temporal gyrus, a region of the brain that is involved in higher order neural processing to make semantic sense from auditory information. The research team used a computer model to analyze various parts of the brain that might be involved in neural firing while processing auditory signals. Using the computational model, scientists were able to identify the brain activity involved in processing auditory information when subjects were presented with recording of individual words. Later, the computer model of auditory information processing was used to reconstruct some of the words back into sound based on the neural processing of the subjects. However the reconstructed sounds were not of good quality and could be recognized only when the audio wave patterns of the reconstructed sound were visually matched with the audio wave patterns of the original sound that was presented to the subjects. However this research marks a direction towards more precise identification of neural activity in cognition.",
            "score": 175.1732940673828
        },
        {
            "docid": "1021754_13",
            "document": "Sound localization . Sound localization is the process of determining the location of a sound source. Objectively speaking, the major goal of sound localization is to simulate a specific sound field, including the acoustic sources, the listener, the media and environments of sound propagation. The brain utilizes subtle differences in intensity, spectral, and timing cues to allow us to localize sound sources. In this section, to more deeply understand the human auditory mechanism, we will briefly discuss about human ear localization theory.",
            "score": 174.8692626953125
        },
        {
            "docid": "41087200_13",
            "document": "Perceptual-based 3D sound localization . The body of a human listener obstructs incoming sound waves, causing linear filtering of the sound signal due to interference from the head, ears, and body. Humans use dynamic cues to reinforce localization. These arise from active, sometimes unconscious, motions of the listener, which change the relative position of the source. It is reported that front/back confusions which are common in static listening tests disappear when listeners are allowed to slightly turn their heads to help them in localization. However, if the sound scene is presented through headphones without compensation for head motion, the scene does not change with the user\u2019s motion, and dynamic cues are absent.",
            "score": 171.85015869140625
        },
        {
            "docid": "21642043_9",
            "document": "Audience (company) . The company\u2019s technology was built around the foundation of Computational Auditory Scene Analysis (CASA) -- a field of study that builds on the concept of Auditory Scene Analysis (ASA), a term first coined by psychologist Albert Bregman. ASA enables humans to accurately group sounds\u2014even when composed of multiple frequencies, as in music, or when heard simultaneously \u2013- and avoid blending \"sources.\" As a result, ASA allows the listener to correctly distinguish and identify a sound of interest, like a voice, from other noise sources.",
            "score": 171.78489685058594
        },
        {
            "docid": "31251362_5",
            "document": "Beat deafness . When sound waves reach the ears, the energy they contain is converted into electrical signals, which are sent via the auditory nerves to the brain. Sound processing begins when these electrical signals reach the primary auditory receiving area in the core part of the temporal lobe. Signals then travel to the area surrounding the core, known as the belt area, and are then transmitted to the parabelt area, which is located next to the belt. Simple sounds such as pure tones are able to activate the core area of the brain, but both the belt and parabelt areas are activated by only complex sounds, such as those found in speech and music. The auditory cortex in the left hemisphere of the brain is responsible for processing beat and rhythm in music. The right auditory cortex is primarily used in distinguishing between different harmonics, which are simple pure tones that combine to create complex tones.",
            "score": 171.5865936279297
        },
        {
            "docid": "32116125_4",
            "document": "Amblyaudia . Amblyaudia is a deficit in binaural integration of environmental information entering the auditory system. It is a disorder related to brain organization and function rather than what is typically considered a \u201chearing loss\u201d (damage to the cochlea). It may be genetic or developmentally acquired or both. When animals are temporarily deprived of hearing from an early age, profound changes occur in the brain. Specifically, cell sizes in brainstem nuclei are reduced, the configuration of brainstem dendrites are altered and neurons respond in different ways to sounds presented to both the deprived and non-deprived ears (in cases of asymmetric deprivation). This last point is particularly important for listening tasks that require inputs from two ears to perform well. There are multiple auditory functions that rely on the computation of well calibrated inputs from the two ears. Chief among these is the ability to localize sound sources and separate what we want to hear from a background of noise. In the brainstem, the auditory system compares the timing and levels of sounds between the two ears to encode the location of sound sources (sounds that originate from our right as opposed to left side are louder and arrive earlier in our right ear). This ability to separate sound sources not only helps us locate the trajectories of moving objects, but also to separate different sound sources in noisy environments.",
            "score": 169.99322509765625
        },
        {
            "docid": "8953380_8",
            "document": "Auditory scene analysis . The job of ASA is to group incoming sensory information to form an accurate mental representation of the individual sounds. When sounds are grouped by the auditory system into a perceived sequence, distinct from other co-occurring sequences, each of these perceived sequences is called an \"auditory stream\". In the real world, if the ASA is successful, a stream corresponds to a distinct environmental sound source producing a pattern that persists over time, such as a person talking, a piano playing, or a dog barking. However, in the lab, by manipulating the acoustic parameters of the sounds, it is possible to induce the perception of one or more auditory streams.",
            "score": 169.11294555664062
        },
        {
            "docid": "32116125_2",
            "document": "Amblyaudia . Amblyaudia (amblyos- blunt; audia-hearing) is a term coined by Dr. Deborah Moncrieff from the University of Pittsburgh to characterize a specific pattern of performance from dichotic listening tests. Dichotic listening tests are widely used to assess individuals for binaural integration, a type of auditory processing skill. During the tests, individuals are asked to identify different words presented simultaneously to the two ears. Normal listeners can identify the words fairly well and show a small difference between the two ears with one ear slightly dominant over the other. For the majority of listeners, this small difference is referred to as a \"right-ear advantage\" because their right ear performs slightly better than their left ear. But some normal individuals produce a \"left-ear advantage\" during dichotic tests and others perform at equal levels in the two ears. Amblyaudia is diagnosed when the scores from the two ears are significantly different with the individual's dominant ear score much higher than the score in the non-dominant ear  Researchers interested in understanding the neurophysiological underpinnings of amblyaudia consider it to be a brain based hearing disorder that may be inherited or that may result from auditory deprivation during critical periods of brain development. Individuals with amblyaudia have normal hearing sensitivity (in other words they hear soft sounds) but have difficulty hearing in noisy environments like restaurants or classrooms. Even in quiet environments, individuals with amblyaudia may fail to understand what they are hearing, especially if the information is new or complicated. Amblyaudia can be conceptualized as the auditory analog of the better known central visual disorder amblyopia. The term \u201clazy ear\u201d has been used to describe amblyaudia although it is currently not known whether it stems from deficits in the auditory periphery (middle ear or cochlea) or from other parts of the auditory system in the brain, or both. A characteristic of amblyaudia is suppression of activity in the non-dominant auditory pathway by activity in the dominant pathway which may be genetically determined and which could also be exacerbated by conditions throughout early development.",
            "score": 164.05015563964844
        },
        {
            "docid": "4301708_10",
            "document": "Cochlear nucleus . The cochlear nuclear complex is the first integrative, or processing, stage in the auditory system. Information is brought to the nuclei from the ipsilateral cochlea via the cochlear nerve. Several tasks are performed in the cochlear nuclei. By distributing acoustic input to multiple types of principal cells, the auditory pathway is subdivided into parallel ascending pathways, which can simultaneously extract different types of information. The cells of the ventral cochlear nucleus extract information that is carried by the auditory nerve in the timing of firing and in the pattern of activation of the population of auditory nerve fibers. The cells of the dorsal cochlear nucleus perform a non-linear spectral analysis and place that spectral analysis into the context of the location of the head, ears and shoulders and that separate expected, self-generated spectral cues from more interesting, unexpected spectral cues using input from the auditory cortex, pontine nuclei, trigeminal ganglion and nucleus, dorsal column nuclei and the second dorsal root ganglion. It is likely that these neurons help mammals to use spectral cues for orienting toward those sounds. The information is used by higher brainstem regions to achieve further computational objectives (such as sound source location or improvement in signal to noise ratio). The inputs from these other areas of the brain probably play a role in sound localization.",
            "score": 161.7166748046875
        },
        {
            "docid": "14339999_6",
            "document": "Virtual pitch . Terhardt rejected the idea of periodicity pitch, because it was not consistent with empirical data on pitch perception, e.g. measurements of the gradual shift of the virtual pitch of a complex tone with a missing fundamental when the partials were gradually shifted. Terhardt instead broke pitch perception into two steps: auditory frequency analysis in the inner ear, and harmonic pitch pattern recognition in the brain. The inner ear effectively performs a running frequency analysis of incoming sounds - otherwise we would not be able to hear out spectral pitches within a complex tone. Physiologically, each spectral pitch depends on both temporal and spectral aspects (i.e. periodicity of the waveform and position of excitation on the basilar membrane), but in Terhardt's approach the spectral pitch itself is a purely experiential parameter, not a physical parameter: it is the outcome of a psychoacoustical experiment in which the conscious listener plays an active role. Psychoacoustic measurements and models can predict which partials are \"perceptually relevant\" in a given complex tone; they are perceptually relevant if you can hear a difference in the whole sound if the frequency or amplitude of a partial is changed). The ear has evolved to separate spectral frequencies, because due to reflection and superposition in everyday environments spectral frequencies are more reliably carriers of environmental information than spectral amplitudies, which in turn are more reliable carriers of environmentally relevant information than phase relationships between partials (when perceived monoaurally). On this basis, Terhardt proposed that spectral pitches - which are what the listener experiences when hearing out partials (as opposed to the physical partials themselves) - are the only information available to the brain for the purpose of extracting virtual pitches. The \"pitch extraction\" process then involves the recognition of incomplete harmonic patterns and happens in neural networks.",
            "score": 161.40145874023438
        },
        {
            "docid": "45871_60",
            "document": "Loudspeaker . The interaction of a loudspeaker system with its environment is complex and is largely out of the loudspeaker designer's control. Most listening rooms present a more or less reflective environment, depending on size, shape, volume, and furnishings. This means the sound reaching a listener's ears consists not only of sound directly from the speaker system, but also the same sound delayed by traveling to and from (and being modified by) one or more surfaces. These reflected sound waves, when added to the direct sound, cause cancellation and addition at assorted frequencies (e.g., from resonant room modes), thus changing the timbre and character of the sound at the listener's ears. The human brain is very sensitive to small variations, including some of these, and this is part of the reason why a loudspeaker system sounds different at different listening positions or in different rooms.",
            "score": 160.3050994873047
        },
        {
            "docid": "35988494_2",
            "document": "Selective auditory attention . Selective auditory attention or selective hearing is a type of selective attention and involves the auditory system of the nervous system. Selective hearing is characterized as the action in which people focus their attention on a specific source of a sound or spoken words. The sounds and noise in the surrounding environment is heard by the auditory system but only certain parts of the auditory information are processed in the brain. Most often, auditory attention is directed at things people are most interested in hearing. In an article by Krans, Isbell, Giuliano, and Neville (2013), selective auditory attention is defined as the ability to acknowledge some stimuli while ignoring other stimuli that is occurring at the same time. An example of this is a student focusing on a teacher giving a lesson and ignoring the sounds of classmates in a rowdy classroom (p.\u00a053). This is an example of bottlenecking which means that information cannot be processed simultaneously so only some sensory information gets through the \"bottleneck\" and is processed. A brain simply cannot process all sensory information that is occurring in an environment so only that which is most important is thoroughly processed. Selective hearing is not a physiological disorder but rather it is the capability of humans to block out sounds and noise. It is the notion of ignoring certain things in the surrounding environment. Over the years, there has been increased research in the selectivity of auditory attention, namely selective hearing.",
            "score": 159.96334838867188
        },
        {
            "docid": "8953842_14",
            "document": "Computational auditory scene analysis . Typical approaches in CASA systems starts with segmenting sound-sources into individual constituents, in its attempts to mimic the physical auditory system. However, there is evidence that the brain does not necessarily process audio input separately, but rather as a mixture. Instead of breaking the audio signal down to individual constituents, the input is broken down of by higher level descriptors, such as chords, bass and melody, beat structure, and chorus and phrase repetitions. These descriptors run into difficulties in real-world scenarios, with monaural and binaural signals. Also, the estimation of these descriptors is highly dependent on the cultural influence of the musical input. For example, within Western music, the melody and bass influences the identity of the piece, with the core formed by the melody. By distinguishing the frequency responses of melody and bass, a fundamental frequency can be estimated and filtered for distinction. Chord detection can be implemented through pattern recognition, by extracting low-level features describing harmonic content.  The techniques utilized in music scene analysis can also be applied to speech recognition, and other environmental sounds. Future bodies of work include a top-down integration of audio signal processing, such as a real-time beat-tracking system and expanding out of the signal processing realm with the incorporation of auditory psychology and physiology.",
            "score": 159.8931884765625
        },
        {
            "docid": "33107185_27",
            "document": "Music and emotion . \"Brain Stem Reflex\": 'This refers to a process whereby an emotion is induced by music because one or more fundamental acoustical characteristics of the music are taken by the brain stem to signal a potentially important and urgent event. All other things being equal, sounds that are sudden, loud, dissonant, or feature fast temporal patterns induce arousal or feelings of unpleasantness in listeners...Such responses reflect the impact of auditory sensations \u2013 music as sound in the most basic sense.'",
            "score": 159.57891845703125
        },
        {
            "docid": "8953380_9",
            "document": "Auditory scene analysis . One example of this is the phenomenon of streaming, also called \"stream segregation.\" If two sounds, A and B, are rapidly alternated in time, after a few seconds the perception may seem to \"split\" so that the listener hears two rather than one stream of sound, each stream corresponding to the repetitions of one of the two sounds, for example, A-A-A-A-, etc. accompanied by B-B-B-B-, etc. The tendency towards segregation into separate streams is favored by differences in the acoustical properties of sounds A and B. Among the differences classically shown to promote segregation are those of frequency (for pure tones), fundamental frequency (for complex tones), frequency composition, source location. But it has been suggested that about any systematic perceptual difference between two sequences can elicit streaming, provided the speed of the sequence is sufficient.",
            "score": 158.9861602783203
        },
        {
            "docid": "3864383_7",
            "document": "Virtual surround . Perception of direction is greatly affected by the relative time that a sound arrives at each ear and any difference in the amplitude of a sound at each ear. It is possible to create a sound source having an output characteristic which is rapidly varying with direction and frequency of signal. These kinds of sources create sound fields which are rapidly variable around the listeners room. These are often referred to as diffuse sources, this is because their output resembles a diffuse sound field \u2014 a sound field where soundwaves are traveling in all directions with equal probability. In a diffuse field the sound at each of a listeners' ears is so completely different that it is impossible for the brain to work out where the sound has come from. A diffuse source located in front of the listener will be hard to localize and can be used to carry the surround signals.",
            "score": 158.5337677001953
        },
        {
            "docid": "620396_41",
            "document": "Origin of language . Proponents of the motor theory of language evolution have primarily focused on the visual domain and communication through observation of movements. The \"Tool-use sound hypothesis\" suggests that the production and perception of sound, also contributed substantially, particularly \"incidental sound of locomotion\" (\"ISOL\") and \"tool-use sound\" (\"TUS\"). Human bipedalism resulted in rhythmic and more predictable \"ISOL\". That may have stimulated the evolution of musical abilities, auditory working memory, and abilities to produce complex vocalizations, and to mimic natural sounds. Since the human brain proficiently extracts information about objects and events from the sounds they produce, \"TUS\", and mimicry of \"TUS\", might have achieved an iconic function. The prevalence of sound symbolism in many extant languages supports this idea. Self-produced TUS activates multimodal brain processing (motor neurons, hearing, proprioception, touch, vision), and \"TUS\" stimulates primate audiovisual mirror neurons, which is likely to stimulate the development of association chains. Tool use and auditory gestures involve motor-processing of the forelimbs, which is associated with the evolution of vertebrate vocal communication. The production, perception, and mimicry of \"TUS\" may have resulted in a limited number of vocalizations or protowords that were associated with tool use. A new way to communicate about tools, especially when out of sight, would have had selective advantage. A gradual change in acoustic properties and/or meaning could have resulted in arbitrariness and an expanded repertoire of words. Humans have been increasingly exposed to \"TUS\" over millions of years, coinciding with the period during which spoken language evolved.",
            "score": 158.22442626953125
        },
        {
            "docid": "404084_27",
            "document": "Hebbian theory . Evidence for that perspective comes from many experiments that show that motor programs can be triggered by novel auditory or visual stimuli after repeated pairing of the stimulus with the execution of the motor program (for a review of the evidence, see Giudice et al., 2009). For instance, people who have never played the piano do not activate brain regions involved in playing the piano when listening to piano music. Five hours of piano lessons, in which the participant is exposed to the sound of the piano each time he presses a key, suffices to later trigger activity in motor regions of the brain upon listening to piano music. Consistent with the fact that spike-timing-dependent plasticity occurs only if the presynaptic neuron's firing predicts the post-synaptic neuron's firing, the link between sensory stimuli and motor programs also only seem to be potentiated if the stimulus is contingent on the motor program.",
            "score": 157.8580322265625
        },
        {
            "docid": "4220231_8",
            "document": "Evolutionary musicology . The evolutionary switch to bipedalism may have influenced the origins of music. The background is that noise of locomotion and ventilation may mask critical auditory information. Human locomotion is likely to produce more predictable sounds than those of non-human primates. Predictable locomotion sounds may have improved our capacity of entrainment to external rhythms and to feel the beat in music. A sense of rhythm could aid the brain in distinguishing among sounds arising from discrete sources and also help individuals to synchronize their movements with one another. Synchronization of group movement may improve perception by providing periods of relative silence and by facilitating auditory processing. The adaptive value of such skills to early human ancestors may have been keener detection of prey or stalkers and enhanced communication. Thus, bipedal walking may have influenced the development of entrainment in humans and thereby the evolution of rhythmic abilities. Primitive hominids lived and moved around in small groups. The noise generated by the locomotion of two or more individuals can result in a complicated mix of footsteps, breathing, movements against vegetation, echoes, etc. The ability to perceive differences in pitch, rhythm, and harmonies, i.e. \u201cmusicality,\u201d could help the brain to distinguish among sounds arising from discrete sources, and also help the individual to synchronize movements with the group. Endurance and an interest in listening might, for the same reasons, have been associated with survival advantages eventually resulting in adaptive selection for rhythmic and musical abilities and reinforcement of such abilities. Listening to music seems to stimulate release of dopamine. Rhythmic group locomotion combined with attentive listening in nature may have resulted in reinforcement through dopamine release. A primarily survival-based behavior may eventually have attained similarities to dance and music, due to such reinforcement mechanisms . Since music may facilitate social cohesion, improve group effort, reduce conflict, facilitate perceptual and motor skill development, and improve trans-generational communication, music-like behavior may at some stage have become incorporated into human culture.",
            "score": 156.4826202392578
        },
        {
            "docid": "9536113_7",
            "document": "Computer audition . Since audio signals are interpreted by the human ear-brain system, that complex perceptual mechanism should be simulated somehow in software for \"machine listening\". In other words, to perform on par with humans, the computer should hear and understand audio content much as humans do. Analyzing audio accurately involves several fields: electrical engineering (spectrum analysis, filtering, and audio transforms); artificial intelligence (machine learning and sound classification); psychoacoustics (sound perception); cognitive sciences (neuroscience and artificial intelligence); acoustics (physics of sound production); and music (harmony, rhythm, and timbre). Furthermore, audio transformations such as pitch shifting, time stretching, and sound object filtering, should be perceptually and musically meaningful. For best results, these transformations require perceptual understanding of spectral models, high-level feature extraction, and sound analysis/synthesis. Finally, structuring and coding the content of an audio file (sound and metadata) could benefit from efficient compression schemes, which discard inaudible information in the sound. Computational models of music and sound perception and cognition can lead to a more meaningful representation, a more intuitive digital manipulation and generation of sound and music in musical human-machine interfaces.",
            "score": 156.36056518554688
        },
        {
            "docid": "38443366_4",
            "document": "Melodic fission . In psychophysics, auditory scene analysis is the process by which the brain separates and organizes sounds into perceptually distinct groups, known as auditory streams.",
            "score": 156.18052673339844
        }
    ]
}