{
    "q": [
        {
            "docid": "33932515_22",
            "document": "Social cue . Benjamin Straube, Antonia Green, Andreas Jansen, Anjan Chatterjee, and Tilo Kircher found that social cues influence the neural processing of speech-gesture utterances. Past studies have focused on mentalizing as being a part of perception of social cues and it is believed that this process relies on the neural system, which consists of: When people focus on things in a social context, the medial prefrontal cortex and precuneus areas of the brain are activated, however when people focus on a non-social context there is no activation of these areas. Straube et al. hypothesized that the areas of the brain involved in mental processes were mainly responsible for social cue processing. It is believed that when iconic gestures are involved, the left temporal and occipital regions would be activated and when emblematic gestures were involved the temporal poles would be activated. When it came to abstract speech and gestures, the left frontal gyrus would be activated according to Straube et al. After conducting an experiment on how body position, speech and gestures affected activation in different areas of the brain Straube et al. came to the following conclusions:",
            "score": 138.51265597343445
        },
        {
            "docid": "25146378_20",
            "document": "Functional specialization (brain) . Other researchers who provide evidence to support the theory of distributive processing include Anthony McIntosh and William Uttal, who question and debate localization and modality specialization within the brain. McIntosh's research suggests that human cognition involves interactions between the brain regions responsible for processes sensory information, such as vision, audition, and other mediating areas like the prefrontal cortex. McIntosh explains that modularity is mainly observed in sensory and motor systems, however, beyond these very receptors, modularity becomes \"fuzzier\" and you see the cross connections between systems increase. He also illustrates that there is an overlapping of functional characteristics between the sensory and motor systems, where these regions are close to one another. These different neural interactions influence each other, where activity changes in one area influence other connected areas. With this, McIntosh suggest that if you only focus on activity in one area, you may miss the changes in other integrative areas. Neural interactions can be measured using analysis of covariance in neuroimaging. McIntosh used this analysis to convey a clear example of the interaction theory of distributive processing. In this study, subjects learned that an auditory stimulus signalled a visual event. McIntosh found activation (an increase blood flow), in an area of the occipital cortex, a region of the brain involved in visual processing, when the auditory stimulus was presented alone. Correlations between the occipital cortex and different areas of the brain such as the prefrontal cortex, premotor cortex and superior temporal cortex showed a pattern of co-variation and functional connectivity.",
            "score": 155.44887971878052
        },
        {
            "docid": "18008_3",
            "document": "Language center . New medical imaging techniques such as PET and fMRI have allowed researchers to generate pictures showing which areas of a living brain are active at a given time. In the past, research was primarily based on observations of loss of ability resulting from damage to the cerebral cortex. Indeed, medical imaging has represented a radical step forward for research on speech processing. Since then, a whole series of relatively large areas of the brain have been found to be involved in speech processing. In more recent research, subcortical regions (those lying below the cerebral cortex such as the putamen and the caudate nucleus) as well as the pre-motor areas (BA 6) have received increased attention. It is now generally assumed that the following structures of the cerebral cortex near the primary and secondary auditory cortices play a fundamental role in speech processing:",
            "score": 111.8317403793335
        },
        {
            "docid": "315084_25",
            "document": "Lip reading . Following the discovery that auditory brain regions, including Heschl's gyrus, were activated by seen speech, the neural circuitry for speechreading was shown to include supra-modal processing regions, especially superior temporal sulcus (all parts) as well as posterior inferior occipital-temporal regions including regions specialised for the processing of faces and biological motion. In some but not all studies, activation of Broca's area is reported for speechreading, suggesting that articulatory mechanisms can be activated in speechreading. Studies of the time course of audiovisual speech processing showed that sight of speech can prime auditory processing regions in advance of the acoustic signal. Better lipreading skill is associated with greater activation in (left) superior temporal sulcus and adjacent inferior temporal (visual) regions in hearing people. In deaf people, the circuitry devoted to speechreading appears to be very similar to that in hearing people, with similar associations of (left) superior temporal activation and lipreading skill.",
            "score": 149.53847229480743
        },
        {
            "docid": "525667_10",
            "document": "Human echolocation . In a 2014 study by Thaler and colleagues, the researchers first made recordings of the clicks and their very faint echoes using tiny microphones placed in the ears of the blind echolocators as they stood outside and tried to identify different objects such as a car, a flag pole, and a tree. The researchers then played the recorded sounds back to the echolocators while their brain activity was being measured using functional magnetic resonance imaging. Remarkably, when the echolocation recordings were played back to the blind experts, not only did they perceive the objects based on the echoes, but they also showed activity in those areas of their brain that normally process visual information in sighted people, primarily primary visual cortex or V1. This result is surprising, as visual areas, as their names suggest, are only active during visual tasks. The brain areas that process auditory information were no more activated by sound recordings of outdoor scenes containing echoes than they were by sound recordings of outdoor scenes with the echoes removed. Importantly, when the same experiment was carried out with sighted people who did not echolocate, these individuals could not perceive the objects and there was no echo-related activity anywhere in the brain. This suggests that the cortex of blind echolocators is plastic and reorganizes such that primary visual cortex, rather than any auditory area, becomes involved in the computation of echolocation tasks.",
            "score": 209.0714054107666
        },
        {
            "docid": "5366050_42",
            "document": "Speech perception . Research into the relationship between music and cognition is an emerging field related to the study of speech perception. Originally it was theorized that the neural signals for music were processed in a specialized \"module\" in the right hemisphere of the brain. Conversely, the neural signals for language were to be processed by a similar \"module\" in the left hemisphere. However, utilizing technologies such as fMRI machines, research has shown that two regions of the brain traditionally considered exclusively to process speech, Broca's and Wernicke's areas, also become active during musical activities such as listening to a sequence of musical chords. Other studies, such as one performed by Marques et al. in 2006 showed that 8-year-olds who were given six months of musical training showed an increase in both their pitch detection performance and their electrophysiological measures when made to listen to an unknown foreign language.",
            "score": 137.4172501564026
        },
        {
            "docid": "32018467_7",
            "document": "Christian Keysers . After finishing his master, Christian Keysers decided to concentrate on a subfield of cognitive neuroscience called social neuroscience that uses neuroscience methods to understand how we process the social world. He therefore performed his doctoral studies at the University of St Andrews with David Ian Perrett, one of the founding father of the field, to understand how the brain processes faces and facial expressions. This thesis work led to new insights into how quickly the brain can process the faces of others. During this period, Keysers became fascinated with the question of how the brain can attach meaning to the faces of others. How is it for instance, that we understand that a certain grimace would signal that another person is happy? How do we understand that a certain bodily movement towards a glass indicates that the other person aims to grasp a glass? In 1999, Keysers was exposed to a visit of Vittorio Gallese, who presented his recent discovery of mirror neurons in the Psychology department lecture series. This deeply influenced Keysers who decided to move to the lab of Giacomo Rizzolatti to undertake further studies on how these fascinating neurons could contribute to social perception. In 2000, after finishing his doctorate, Christian Keysers moved to the University of Parma to study mirror neurons. In early work there demonstrated that mirror neurons in the premotor cortex not only respond to the sight of actions, but also when actions can only be deduced or heard, leading to a publication in the journal \"Science\". This work had tremendous impact on the field, as it suggested that the premotor cortex could play a central, modality independent role in perception and may lay the origin for the evolution of speech in humans.  Together this work indicated that brain regions involved in our own actions play a role in how we process the actions of others. Keysers wondered whether a similar principle may underlie how we process the tactile sensations and emotions of others, and became increasingly independent of the research focus on the motor system in Parma. At the time, Keysers had also met his to be wife, Valeria Gazzola, a biologist in the final phases of her studies, and together they decided to explore if the somatosensory system might be involved in perceiving the sensations of others. Via a fruitful collaboration with the French neuroimaging specialist Bruno Wicker, they used functional magnetic resonance imaging, and showed for the first time, that the secondary somatosensory cortex, previously thought only to represent a persons own experiences of touch, is also activated when seeing someone or something else be touched. They also showed that the insula, thought only to respond to the experience of first-hand emotions, was also activated when we see another individual experience similar emotions. Together this indicated a much more general principle than the original mirror neuron theory, in which people process the actions, sensations and emotions of others by vicariously activating owns own actions, sensations and emotions. Jointly, this work laid the foundation of the neuroscientific investigation of empathy.",
            "score": 152.91658782958984
        },
        {
            "docid": "162707_53",
            "document": "Singing . Much research has been done recently on the link between music and language, especially singing. It is becoming increasingly clear that these two processes are very much alike, and yet also different. Levitin describes how, beginning with the eardrum, sound waves are translated into pitch, or a tonotopic map, and then shortly thereafter \"speech and music probably diverge into separate processing circuits\" (130). There is evidence that neural circuits used for music and language may start out in infants undifferentiated. There are several areas of the brain that are used for both language and music. For example, Brodmann Area 47. Levitin recounts how in certain studies, \"listening to music and attending its syntactic features,\" similar to the syntactic processes in language, activated this part of the brain. In addition, \"musical syntax . . . has been localized to . . . areas adjacent to and overlapping with those regions that process speech syntax, such as Broca's area\" and \"the regions involved in musical semantics . . . appear to be [localized] near Wernicke's area.\" Both Broca's area and Wernicke's area are important steps in language processing and production.",
            "score": 146.01313471794128
        },
        {
            "docid": "485309_16",
            "document": "Face perception . There are several parts of the brain that play a role in face perception. Rossion, Hanseeuw, and Dricot used BOLD fMRI mapping to identify activation in the brain when subjects viewed both cars and faces. The majority of BOLD fMRI studies use blood oxygen level dependent (BOLD) contrast to determine which areas of the brain are activated by various cognitive functions. They found that the occipital face area, located in the occipital lobe, the fusiform face area, the superior temporal sulcus, the amygdala, and the anterior/inferior cortex of the temporal lobe, all played roles in contrasting the faces from the cars, with the initial face perception beginning in the area and occipital face areas. This entire region links to form a network that acts to distinguish faces. The processing of faces in the brain is known as a \"sum of parts\" perception. However, the individual parts of the face must be processed first in order to put all of the pieces together. In early processing, the occipital face area contributes to face perception by recognizing the eyes, nose, and mouth as individual pieces. Furthermore, Arcurio, Gold, and James used BOLD fMRI mapping to determine the patterns of activation in the brain when parts of the face were presented in combination and when they were presented singly. The occipital face area is activated by the visual perception of single features of the face, for example, the nose and mouth, and preferred combination of two-eyes over other combinations. This research supports that the occipital face area recognizes the parts of the face at the early stages of recognition. On the contrary, the fusiform face area shows no preference for single features, because the fusiform face area is responsible for \"holistic/configural\" information, meaning that it puts all of the processed pieces of the face together in later processing. This theory is supported by the work of Gold et al. who found that regardless of the orientation of a face, subjects were impacted by the configuration of the individual facial features. Subjects were also impacted by the coding of the relationships between those features. This shows that processing is done by a summation of the parts in the later stages of recognition.",
            "score": 168.0808982849121
        },
        {
            "docid": "5366050_49",
            "document": "Speech perception . Computational modeling has also been used to simulate how speech may be processed by the brain to produce behaviors that are observed. Computer models have been used to address several questions in speech perception, including how the sound signal itself is processed to extract the acoustic cues used in speech, and how speech information is used for higher-level processes, such as word recognition.",
            "score": 111.97318124771118
        },
        {
            "docid": "540571_8",
            "document": "Wernicke's area . Support for a broad range of speech processing areas was furthered by a recent study done at University of Rochester in which American Sign Language native speakers were subject to MRIs while interpreting sentences that identified a relationship using either syntax (relationship is determined by the word order) or inflection (relationship is determined by physical motion of \"moving hands through space or signing on one side of the body\"). Distinct areas of the brain were activated with the frontal cortex (associated with ability to put information into sequences) being more active in the syntax condition and the temporal lobes (associated with dividing information into its constituent parts) being more active in the inflection condition. However, these areas are not mutually exclusive and show a large amount of overlap. These findings imply that while speech processing is a very complex process, the brain may be using fairly basic, preexisting computational methods.",
            "score": 150.29274797439575
        },
        {
            "docid": "36058569_5",
            "document": "Frank H. Guenther . Frank Guenther\u2019s research is aimed at uncovering the neural computations underlying the processing of speech by the human brain. He is the originator of the Directions Into Velocities of Articulators (DIVA) model, which is currently the leading model of the neural computations underlying speech production. This model mathematically characterizes the computations performed by each brain region involved in speech production as well as the function of the interconnections between these regions. The model has been supported by a wide range of experimental tests of model predictions, including electromagnetic articulometry studies investigating speech movements, auditory perturbation studies involving modification of a speaker\u2019s feedback of his/her own speech in real time, and functional magnetic resonance imaging studies of brain activity during speech, though some parts of the model remain to be experimentally verified. The DIVA model has been used to investigate the neural underpinnings of a number of communication disorders, including stuttering apraxia of speech, and hearing-impaired speech.",
            "score": 116.94069480895996
        },
        {
            "docid": "484650_7",
            "document": "Functional neuroimaging . Traditional \"activation studies\" focus on determining distributed patterns of brain activity associated with specific tasks. However, scientists are able to more thoroughly understand brain function by studying the interaction of distinct brain regions, as a great deal of neural processing is performed by an integrated network of several regions of the brain. An active area of neuroimaging research involves examining the functional connectivity of spatially remote brain regions. Functional connectivity analyses allow the characterization of interregional neural interactions during particular cognitive or motor tasks or merely from spontaneous activity during rest. FMRI and PET enable creation of functional connectivity maps of distinct spatial distributions of temporally correlated brain regions called functional networks. Several studies using neuroimaging techniques have also established that posterior visual areas in blind individuals may be active during the performance of nonvisual tasks such as Braille reading, memory retrieval, and auditory localization as well as other auditory functions.",
            "score": 168.102303981781
        },
        {
            "docid": "5366050_50",
            "document": "Speech perception . Neurophysiological methods rely on utilizing information stemming from more direct and not necessarily conscious (pre-attentative) processes. Subjects are presented with speech stimuli in different types of tasks and the responses of the brain are measured. The brain itself can be more sensitive than it appears to be through behavioral responses. For example, the subject may not show sensitivity to the difference between two speech sounds in a discrimination test, but brain responses may reveal sensitivity to these differences. Methods used to measure neural responses to speech include event-related potentials, magnetoencephalography, and near infrared spectroscopy. One important response used with event-related potentials is the mismatch negativity, which occurs when speech stimuli are acoustically different from a stimulus that the subject heard previously.",
            "score": 136.5897319316864
        },
        {
            "docid": "403676_28",
            "document": "Gesture . Gestures are processed in the same areas of the brain as speech and sign language such as the left inferior frontal gyrus (Broca's area) and the posterior middle temporal gyrus, posterior superior temporal sulcus and superior temporal gyrus (Wernicke's area). It has been suggested that these parts of the brain originally supported the pairing of gesture and meaning and then were adapted in human evolution \"for the comparable pairing of sound and meaning as voluntary control over the vocal apparatus was established and spoken language evolved\". As a result, it underlies both symbolic gesture and spoken language in the present human brain. Their common neurological basis also supports the idea that symbolic gesture and spoken language are two parts of a single fundamental semiotic system that underlies human discourse. The linkage of hand and body gestures in conjunction with speech is further revealed by the nature of gesture use in blind individuals during conversation. This phenomenon uncovers a function of gesture that goes beyond portraying communicative content of language and extends David McNeill's view of the gesture-speech system. This suggests that gesture and speech work tightly together, and a disruption of one (speech or gesture) will cause a problem in the other. Studies have found strong evidence that speech and gesture are innately linked in the brain and work in an efficiently wired and choreographed system. McNeill's view of this linkage in the brain is just one of three currently up for debate; the others declaring gesture to be a \"support system\" of spoken language or a physical mechanism for lexical retrieval.",
            "score": 153.01316225528717
        },
        {
            "docid": "34776464_4",
            "document": "Bilingual memory . One of the processes involved in analyzing which neural regions of the brain are involved in bilingual memory is a subtraction method. Researchers compare what has been impaired with what is functioning regularly. This contrast between the destroyed and intact regions of the brain, aids researchers in discovering the components of language processing. It has been found that under typical circumstances, when multiple languages are lost at the same time, they are usually regained in the same fashion. It is therefore presumed that areas of the brain, which are responsible for processing language, are potentially the same. There have been examples of cases where languages have been restored prior to one another and to a greater degree, but this is fairly uncommon. The techniques allowing researchers to observe brain activity in multilingual patients are conducted whilst the subject is simultaneously performing and processing a language. Research has proposed that the entire production and comprehension of language is most likely regulated and managed by neural pools, whose stations of communication are in the cortical and subcortical regions. It has been shown that there are no grounds on which to assume the existence of distinct cerebral organization of separate languages in the bilingual brain. That is to say, the cerebral regions that are engaged for both languages are the same. Although neurologists have a basic understanding of the underlying neural components and mechanisms of bilingual language, further research is necessary in order to fully understand or conclude any other findings.  Neuroimaging techniques such as fMRIs have shown that at least four brain areas are involved in bilingual switching: dorsolateral prefrontal cortex, inferior frontal cortex, anterior cingulate, and supramarginal gyrus. It is expected that switching from one language to another should involve different functional processes when compared to the brain of an individual who only speaks one language. However further studies on brain activation during this switching of languages needs to be done.",
            "score": 133.8290388584137
        },
        {
            "docid": "179092_10",
            "document": "Neurolinguistics . Much work in neurolinguistics has, like Broca's and Wernicke's early studies, investigated the locations of specific language \"modules\" within the brain. Research questions include what course language information follows through the brain as it is processed, whether or not particular areas specialize in processing particular sorts of information, how different brain regions interact with one another in language processing, and how the locations of brain activation differs when a subject is producing or perceiving a language other than his or her first language.",
            "score": 138.08792114257812
        },
        {
            "docid": "23158496_4",
            "document": "Superior temporal sulcus . In individuals without autism, the superior temporal sulcus also activates when hearing human voices. It is thought to be a source of sensory encoding linked to motor output through the superior parietal-temporal areas of the brain inferred from the time course of activation. The conclusion of pertinence to vocal processing can be drawn from data showing that the regions of the STS (superior temporal sulcus) are more active when people are listening to vocal sounds rather than non-vocal environmentally based sounds and corresponding control sounds, which can be scrambled or modulated voices. These experimental results indicate the involvement of the STS in the areas of speech and language recognition.",
            "score": 111.0018150806427
        },
        {
            "docid": "43124500_5",
            "document": "Sophie Scott . Scott is head of the Speech Communication Group at UCL's Institute of Cognitive Neuroscience. Her research investigates the neural basis of vocal communication \u2013 how our brains process the information in speech and voices, and how our brains control the production of our voice. Within this, her research covers the roles of streams of processing in auditory cortex, hemispheric asymmetries, and the interaction of speech processing with attentional and working memory factors. Other interests include individual differences in speech perception and plasticity in speech perception, since these are important factors for people with cochlear implants. She is also interested in the expression of emotion in the voice. In particular, research in recent years has focused on the neuroscience of laughter.",
            "score": 123.46308994293213
        },
        {
            "docid": "524233_7",
            "document": "Brodmann area 45 . A strong correlation has been found between speech-language and the anatomically asymmetric pars triangularis. Foundas, et al. showed that language function can be localized to one region of the brain, as Paul Broca had done before them, but they also supported the idea that one side of the brain is more involved with language than the other. The human brain has two hemispheres, and each one looks similar to the other; that is, it looks like one hemisphere is a mirror image of the other. However, Foundas, et al. found that the pars triangularis in Broca's area is actually larger than the same region in the right side of the brain. This \"leftward asymmetry\" corresponded both in form and function, which means that the part of the brain that is active during language processing is larger. In almost all the test subjects, this was the left side. In fact, the only subject tested that had right-hemispheric language dominance was found to have a rightward asymmetry of the pars triangularis.",
            "score": 127.91457939147949
        },
        {
            "docid": "3704475_50",
            "document": "Executive functions . Despite the growing currency of the 'biasing' model of executive functions, direct evidence for functional connectivity between the PFC and sensory regions when executive functions are used, is to date rather sparse. Indeed, the only direct evidence comes from studies in which a portion of frontal cortex is damaged, and a corresponding effect is observed far from the lesion site, in the responses of sensory neurons. However, few studies have explored whether this effect is specific to situations where executive functions are required. Other methods for measuring connectivity between distant brain regions, such as correlation in the fMRI response, have yielded indirect evidence that the frontal cortex and sensory regions communicate during a variety of processes thought to engage executive functions, such as working memory, but more research is required to establish how information flows between the PFC and the rest of the brain when executive functions are used. As an early step in this direction, an fMRI study on the flow of information processing during visuospatial reasoning has provided evidence for causal associations (inferred from the temporal order of activity) between sensory-related activity in occipital and parietal cortices and activity in posterior and anterior PFC. Such approaches can further elucidate the distribution of processing between executive functions in PFC and the rest of the brain.",
            "score": 117.68350958824158
        },
        {
            "docid": "226722_59",
            "document": "Functional magnetic resonance imaging . Neuroimaging methods such as fMRI offer a measure of the activation of certain brain areas in response to cognitive tasks engaged in during the scanning process. Data obtained during this time allow cognitive neuroscientists to gain information regarding the role of particular brain regions in cognitive function. However, an issue arises when certain brain regions are alleged by researchers to identify the activation of previously labeled cognitive processes. Poldrack clearly describes this issue: Reverse inference demonstrates the logical fallacy of affirming what you just found, although this logic could be supported by instances where a certain outcome is generated solely by a specific occurrence. With regard to the brain and brain function it is seldom that a particular brain region is activated solely by one cognitive process. Some suggestions to improve the legitimacy of reverse inference have included both increasing the selectivity of response in the brain region of interest and increasing the prior probability of the cognitive process in question. However, Poldrack suggests that reverse inference should be used merely as a guide to direct further inquiry rather than a direct means to interpret results.",
            "score": 124.1719286441803
        },
        {
            "docid": "33993614_28",
            "document": "Neurocomputational speech processing . On the other hand the speech sound map, if activated for a specific speech unit (single neuron activation; punctual activation), activates sensory information by synaptic projections between speech sound map and auditory target region map and between speech sound map and somatosensory target region map. Auditory and somatosensory target regions are assumed to be located in higher-order auditory cortical regions and in higher-order somatosensory cortical regions respectively. These target region sensory activation patterns - which exist for each speech unit - are learned during speech acquisition (by imitation training; see below: learning).",
            "score": 78.66002535820007
        },
        {
            "docid": "11037189_10",
            "document": "Chromesthesia . The cross-activation theory of synesthesia was formulated by V.S. Ramachandran and E.M. Hubard, based on converging evidence over studies of the various forms of synesthesia that sensory areas for processing real and synesthetic information tend to be neighboring brain regions. This is most apparent in grapheme-color synesthesia, because the brain regions for color processing and visual word form processing are adjacent. Individuals with sound-color synesthesia show activation of brain areas involved in visual processing, such as V4, immediately after the auditory perception, indicating an automatic linking of sounds and colors. The reason for this cross-activation is still unclear, but one hypothesis is that the increased connectivity between adjacent brain regions is due to a reduction in the pruning back of neuronal networks during childhood.",
            "score": 144.87660479545593
        },
        {
            "docid": "4231622_9",
            "document": "Inferior temporal gyrus . These areas must all work together, as well as with the hippocampus, in order to create an array of understanding of the physical world. The hippocampus is key for storing the memory of what an object is/what it looks like for future use so that it can be compared and contrasted with other objects. Correctly being able to recognize an object is highly dependent on this organized network of brain areas that process, share, and store information. In a study by Denys et al., functional magnetic resonance imaging (FMRI) was used to compare the processing of visual shape between humans and macaques. They found, amongst other things, that there was a degree of overlap between shape and motion sensitive regions of the cortex, but that the overlap was more distinct in humans. This would suggest that the human brain is better evolved for a high level of functioning in a distinct, three-dimensional, visual world.",
            "score": 141.50969767570496
        },
        {
            "docid": "2363287_6",
            "document": "Visual learning . Various areas of the brain work together in a multitude of ways in order to produce the images that we see with our eyes and that are encoded by our brains. The basis of this work takes place in the visual cortex of the brain. The visual cortex is located in the occipital lobe of the brain and harbors many other structures that aid in visual recognition, categorization, and learning. One of the first things the brain must do when acquiring new visual information is recognize the incoming material. Brain areas involved in recognition are the inferior temporal cortex, the superior parietal cortex, and the cerebellum. During tasks of recognition, there is increased activation in the left inferior temporal cortex and decreased activation in the right superior parietal cortex. Recognition is aided by neural plasticity, or the brain's ability to reshape itself based on new information. Next the brain must categorize the material. The three main areas that are used when categorizing new visual information are the orbitofrontal cortex and two dorsolateral prefrontal regions which begin the process of sorting new information into groups and further assimilating that information into things that you might already know. After recognizing and categorizing new material entered into the visual field, the brain is ready to begin the encoding process \u2013 the process which leads to learning. Multiple brain areas are involved in this process such as the frontal lobe, the right extrastriate cortex, the neocortex, and again, the neostriatum. One area in particular, the limbic-diencephalic region, is essential for transforming perceptions into memories. With the coming together of tasks of recognition, categorization and learning; schemas help make the process of encoding new information and relating it to things you already know much easier. One can remember visual images much better when they can apply it to an already known schema. Schemas actually provide enhancement of visual memory and learning.",
            "score": 159.72827053070068
        },
        {
            "docid": "14241792_2",
            "document": "TRACE (psycholinguistics) . TRACE is a connectionist model of speech perception, proposed by James McClelland and Jeffrey Elman in 1986. It is based on a structure called \"the Trace,\" a dynamic processing structure made up of a network of units, which performs as the system's working memory as well as the perceptual processing mechanism. TRACE was made into a working computer program for running perceptual simulations. These simulations are predictions about how a human mind/brain processes speech sounds and words as they are heard in real time.",
            "score": 82.54223918914795
        },
        {
            "docid": "40166_28",
            "document": "Broca's area . Examination of the brains of Broca's two historic patients with high-resolution MRI has produced several interesting findings. First, the MRI findings suggest that other areas besides Broca's area may also have contributed to the patients' reduced productive speech. This finding is significant because it has been found that, though lesions to Broca's area alone can possibly cause temporary speech disruption, they do not result in severe speech arrest. Therefore, there is a possibility that the aphasia denoted by Broca as an absence of productive speech also could have been influenced by the lesions in the other region. Another finding is that the region, which was once considered to be critical for speech by Broca, is not precisely the same region as what is now known as Broca's area. This study provides further evidence to support the claim that language and cognition are far more complicated than once thought and involve various networks of brain regions.",
            "score": 93.2261233329773
        },
        {
            "docid": "21312318_27",
            "document": "Recognition memory . Recognition memory is critically dependent on a hierarchically organized network of brain areas including the visual ventral stream, medial temporal lobe structures, frontal lobe and parietal cortices along with the hippocampus. As mentioned previously, the processes of recollection and familiarity are represented differently in the brain. As such, each of the regions listed above can be further subdivided according to which part is primarily involved in recollection or in familiarity. In the temporal cortex, for instance, the medial region is related to recollection whereas the anterior region is related to familiarity. Similarly, in the parietal cortex, the lateral region is related to recollection whereas the superior region is related to familiarity. An even more specific account divides the medial parietal region, relating the posterior cingulate to recollection and the precuneus to familiarity. The hippocampus plays a prominent role in recollection whereas familiarity depends heavily on the surrounding medial-temporal regions, especially the perirhinal cortex. Finally, it is not yet clear what specific regions of the prefrontal lobes are associated with recollection versus familiarity, although there is evidence that the left prefrontal cortex is correlated more strongly with recollection whereas the right prefrontal cortex is involved more in familiarity. Though left-side activation involved in recollection was originally hypothesized to result from semantic processing of words (many of these earlier studies used written words for stimuli) subsequent studies using nonverbal stimuli produced the same finding\u2014suggesting that prefrontal activation in the left hemisphere results from any kind of detailed remembering.  As previously mentioned, recognition memory is not a stand-alone concept; rather it is a highly interconnected and integrated sub-system of memory. Perhaps misleadingly, the regions of the brain listed above correspond to an abstract and highly generalized understanding of recognition memory, in which the stimuli or items-to-be-recognized are not specified. In reality, however, the location of brain activation involved in recognition is highly dependent on the nature of the stimulus itself. Consider the conceptual differences in recognizing written words compared to recognizing human faces. These are two qualitatively different tasks and as such it is not surprising that they involve additional, distinct regions of the brain. Recognizing words, for example, involves the visual word form area, a region in the left fusiform gyrus, which is believed to specialized in recognizing written words. Similarly, the fusiform face area, located in the right hemisphere, is linked specifically to the recognition of faces.",
            "score": 120.32085871696472
        },
        {
            "docid": "179092_3",
            "document": "Neurolinguistics . Neurolinguistics is historically rooted in the development in the 19th century of aphasiology, the study of linguistic deficits (aphasias) occurring as the result of brain damage. Aphasiology attempts to correlate structure to function by analyzing the effect of brain injuries on language processing. One of the first people to draw a connection between a particular brain area and language processing was Paul Broca, a French surgeon who conducted autopsies on numerous individuals who had speaking deficiencies, and found that most of them had brain damage (or \"lesions\") on the left frontal lobe, in an area now known as Broca's area. Phrenologists had made the claim in the early 19th century that different brain regions carried out different functions and that language was mostly controlled by the frontal regions of the brain, but Broca's research was possibly the first to offer empirical evidence for such a relationship, and has been described as \"epoch-making\" and \"pivotal\" to the fields of neurolinguistics and cognitive science. Later, Carl Wernicke, after whom Wernicke's area is named, proposed that different areas of the brain were specialized for different linguistic tasks, with Broca's area handling the motor production of speech, and Wernicke's area handling auditory speech comprehension. The work of Broca and Wernicke established the field of aphasiology and the idea that language can be studied through examining physical characteristics of the brain. Early work in aphasiology also benefited from the early twentieth-century work of Korbinian Brodmann, who \"mapped\" the surface of the brain, dividing it up into numbered areas based on each area's cytoarchitecture (cell structure) and function; these areas, known as Brodmann areas, are still widely used in neuroscience today.",
            "score": 148.59485256671906
        },
        {
            "docid": "2640086_28",
            "document": "Affective neuroscience . Instead of investigating specific emotions, Kober, et al. 2008 reviewed 162 neuroimaging studies published between 1990-2005 to determine if groups of brain regions show consistent patterns of activation during emotional experience (that is, actively experiencing an emotion first-hand) and during emotion perception (that is, perceiving a given emotion as experienced by another). This meta-analysis used multilevel kernal density analysis (MKDA) to examine fMRI and PET studies, a technique that prevents single studies from dominating the results (particularly if they report multiple nearby peaks) and that enables studies with large sample sizes (those involving more participants) to exert more influence upon the results. MKDA was used to establish a neural reference space that includes the set of regions showing consistent increases across all studies (for further discussion of MDKA see Wager et al. 2007). Next, this neural reference space was partitioned into functional groups of brain regions showing similar activation patterns across studies by first using multivariate techniques to determine co-activation patterns and then using data-reduction techniques to define the functional groupings (resulting in six groups). Consistent with a psychological construction approach to emotion, the authors discuss each functional group in terms more basic psychological operations. The first \u201cCore Limbic\u201d group included the left amygdala, hypothalamus, periaqueductal gray/thalamus regions, and amygdala/ventral striatum/ventral globus pallidus/thalamus regions, which the authors discuss as an integrative emotional center that plays a general role in evaluating affective significance. The second \u201cLateral Paralimbic\u201d group included the ventral anterior insula/frontal operculum/right temporal pole/ posterior orbitofrontal cortex, the anterior insula/ posterior orbitofrontal cortex, the ventral anterior insula/ temporal cortex/ orbitofrontal cortex junction, the midinsula/ dorsal putamen, and the ventral striatum /mid insula/ left hippocampus, which the authors suggest plays a role in motivation, contributing to the general valuation of stimuli and particularly in reward. The third \u201cMedial Prefrontal Cortex\u201d group included the dorsal medial prefrontal cortex, pregenual anterior cingulate cortex, and rostral dorsal anterior cingulate cortex, which the authors discuss as playing a role in both the generation and regulation of emotion. The fourth \u201cCognitive/ Motor Network\u201d group included right frontal operculum, the right interior frontal gyrus, and the pre-supplementray motor area/ left interior frontal gyrus, regions that are not specific to emotion, but instead appear to play a more general role in information processing and cognitive control. The fifth \u201cOccipital/ Visual Association\u201d group included areas V8 and V4 of the primary visual cortex, the medial temporal lobe, and the lateral occipital cortex, and the sixth \u201cMedial Posterior\u201d group included posterior cingulate cortex and area V1 of the primary visual cortex. The authors suggest that these regions play a joint role in visual processing and attention to emotional stimuli.",
            "score": 122.10217142105103
        },
        {
            "docid": "40621603_5",
            "document": "Linguistic intelligence . Speech production is process by which a thought in the brain is converted into an understandable auditory form. This is a multistage mechanism that involves many different areas of the brain. The first stage is planning, where the brain constructs words and sentences that turn the thought into an understandable form. This occurs primarily in the inferior frontal cortex, specifically in an area known as Broca's area. Next, the brain must plan how to physically create the sounds necessary for speech by linking the planned speech with known sounds, or phonemes. While the location of these associations is not known, it is known that the supplementary motor area plays a key role in this step. Finally, the brain must signal for the words to actually be spoken. This is carried out by the premotor cortex and the motor cortex. In most cases, speech production is controlled by the left hemisphere. In a series of studies, Wilder Penfield, among others, probed the brains of both right-handed (generally left-hemisphere dominant) and left-handed (generally right-hemisphere dominant) patients. They discovered that, regardless of handedness, the left hemisphere was almost always the speech controlling side. However, it has been discovered that in cases of neural stress (hemorrhage, stroke, etc.) the right hemisphere has the ability to take control of speech functions.",
            "score": 127.54270052909851
        }
    ],
    "r": [
        {
            "docid": "525667_10",
            "document": "Human echolocation . In a 2014 study by Thaler and colleagues, the researchers first made recordings of the clicks and their very faint echoes using tiny microphones placed in the ears of the blind echolocators as they stood outside and tried to identify different objects such as a car, a flag pole, and a tree. The researchers then played the recorded sounds back to the echolocators while their brain activity was being measured using functional magnetic resonance imaging. Remarkably, when the echolocation recordings were played back to the blind experts, not only did they perceive the objects based on the echoes, but they also showed activity in those areas of their brain that normally process visual information in sighted people, primarily primary visual cortex or V1. This result is surprising, as visual areas, as their names suggest, are only active during visual tasks. The brain areas that process auditory information were no more activated by sound recordings of outdoor scenes containing echoes than they were by sound recordings of outdoor scenes with the echoes removed. Importantly, when the same experiment was carried out with sighted people who did not echolocate, these individuals could not perceive the objects and there was no echo-related activity anywhere in the brain. This suggests that the cortex of blind echolocators is plastic and reorganizes such that primary visual cortex, rather than any auditory area, becomes involved in the computation of echolocation tasks.",
            "score": 209.0714111328125
        },
        {
            "docid": "15559385_11",
            "document": "Tactile discrimination . When a person has become blind, in order to \u201csee\u201d the world, their other senses become heightened. An important sense for the blind is their sense of touch, which becomes more frequently used to help them perceive the world. People that are blind have displayed that their visual cortices become more responsive to auditory and tactile stimulation. Braille allows the blind to be able to use their sense of touch to feel the roughness, and distance of various patterns to be used as a form of language. Within the brain, the activation of the occipital cortex is functionally relevant for tactile braille reading, as well as the somatosensory cortex. These various parts of the brain function in their own way, in which they each contribute to the effectiveness of how braille is read by the blind. People that are blind also rely heavily on Tactile Gnosis, Spatial discrimination, Graphesthesia, and Two-point discrimination. Essentially, the occipital cortex allows one to effectively make judgements on the distance of braille patterns, which is related to spatial discrimination. Meanwhile, the somatosensory cortex allows one to effectively make judgements on the roughness of braille patterns, which is related to two-point discrimination. The various visual areas in the brain are very essential for a blind person to read braille, just as much as it is for a person that has sight. Essentially, whether one is blind or not, the perception of objects that involves tactile discrimination is not impaired if one cannot see. When comparing people that are blind to people that have sight, the amount of activity within the their somatosensory and visual areas of the brain do differ. The activity in the somatosensory and visual areas are not as high in tactile gnosis for people that are not blind, and are more-so active for more visual related stimuli that does not involve touch. Nonetheless, there is a difference in these various areas within the brain when comparing the blind to the sighted, which is that shape discrimination causes a difference in brain activity, as well as tactile gnosis. The visual cortices of blind individuals are active during various vision related tasks including tactile discrimination, and the function of the cortices resemble the activity of adults with sight.",
            "score": 203.91322326660156
        },
        {
            "docid": "7913402_16",
            "document": "Paul Baltes . Neuronal plasticity, or the capability of the brain to adapt to new requirements, is a prime example of plasticity stressing that the individual\u2019s ability to change is a lifelong process. Recently, researchers have been analyzing how the spared senses compensate for the loss of vision. Without visual input, blind humans have demonstrated that tactile and auditory functions still fully develop. A superiority of the blind has even been observed when they are presented with tactile and auditory tasks. This superiority may suggest that the specific sensory experiences of the blind may influence the development of certain sensory functions, namely tactile and auditory. One experiment was designed by R\u00f6der and colleagues to clarify the auditory localization skills of the blind in comparison to the sighted. They examined both blind human adults\u2019 and sighted human adults\u2019 abilities to locate sounds presented either central or peripheral (lateral) to them. Both congenitally blind adults and sighted adults could locate a sound presented in front of them with precision but the blind were clearly superior in locating sounds presented laterally. Currently, brain-imaging studies have revealed that the sensory cortices in the brain are reorganized after visual deprivation. These findings suggest that when vision is absent in development, the auditory cortices in the brain recruit areas that are normally devoted to vision, thus becoming further refined.",
            "score": 186.58724975585938
        },
        {
            "docid": "433584_4",
            "document": "McGurk effect . Vision is the primary sense for humans, but speech perception is multimodal, which means that it involves information from more than one sensory modality, in particular, audition and vision. The McGurk effect arises during phonetic processing because the integration of audio and visual information happens early in speech perception. The McGurk effect is very robust; that is, knowledge about it seems to have little effect on one's perception of it. This is different from certain optical illusions, which break down once one 'sees through' them. Some people, including those that have been researching the phenomenon for more than twenty years, experience the effect even when they are aware that it is taking place. With the exception of people who can identify most of what is being said from speech-reading alone, most people are quite limited in their ability to identify speech from visual-only signals. A more extensive phenomenon is the ability of visual speech to increase the intelligibility of heard speech in a noisy environment. Visible speech can also alter the perception of perfectly audible speech sounds when the visual speech stimuli are mismatched with the auditory speech. Normally, speech perception is thought to be an auditory process; however, our use of information is immediate, automatic, and, to a large degree, unconscious and therefore, despite what is widely accepted as true, speech is not only something we hear. Speech is perceived by all of the senses working together (seeing, touching, and listening to a face move). The brain is often unaware of the separate sensory contributions of what it perceives. Therefore, when it comes to recognizing speech the brain cannot differentiate whether it is seeing or hearing the incoming information.",
            "score": 183.8829345703125
        },
        {
            "docid": "33106906_52",
            "document": "Eyewitness memory . It has been suggested that blind individuals have an enhanced ability to hear and recall auditory information in order to compensate for a lack of vision. However, whilst blind adults' neural systems demonstrate heightened excitability and activity compared to sighted adults, it is still not exactly clear to what extent this compensatory hypothesis is accurate. Nevertheless, many studies have found that there appears to be a high activation of certain visual brain areas in blind individuals when they perform non-visual tasks. This suggests that in blind individuals' brains, a reorganization of what are normally visual areas has occurred in order for them to process non-visual input. This supports a compensatory hypothesis in the blind.",
            "score": 179.2935333251953
        },
        {
            "docid": "10567836_7",
            "document": "Ordinal numerical competence . Both behavioral research and brain-imaging research show distinct differences in the way \"exact\" arithmetic and \"approximate\" arithmetic are processed. Exact arithmetic is information that is precise and follows specific rules and patterns such as multiplication tables or geometric formulas, and approximate arithmetic is a general comparison between numbers such as the comparisons of greater than or less than. Research shows that exact arithmetic is language-based and processed in the left inferior frontal lobe. Approximate arithmetic is processed much differently in a different part of the brain. Approximate arithmetic is processed in the bilateral areas of the parietal lobes. This part of the brain processes visual information to understand how objects are spatially related to each other, for example, understanding that 10 of something is more than two of something. This difference in brain function can create a difference in how we experience certain types of arithmetic. Approximate arithmetic can be experienced as intuitive and exact arithmetic experienced as recalled knowledge.",
            "score": 173.6009521484375
        },
        {
            "docid": "433584_6",
            "document": "McGurk effect . Both hemispheres of the brain make a contribution to the McGurk effect. They work together to integrate speech information that is received through the auditory and visual senses. A McGurk response is more likely to occur in right-handed individuals for whom the face has privileged access to the right hemisphere and words to the left hemisphere. In people that have had callosotomies done, the McGurk effect is still present but significantly slower. In people with lesions to the left hemisphere of the brain, visual features often play a critical role in speech and language therapy. People with lesions in the left hemisphere of the brain show a greater McGurk effect than normal controls. Visual information strongly influences speech perception in these people. There is a lack of susceptibility to the McGurk illusion if left hemisphere damage resulted in a deficit to visual segmental speech perception. In people with right hemisphere damage, impairment on both visual-only and audio-visual integration tasks is exhibited, although they are still able to integrate the information to produce a McGurk effect. Integration only appears if visual stimuli is used to improve performance when the auditory signal is impoverished but audible. Therefore, there is a McGurk effect exhibited in people with damage to the right hemisphere of the brain but the effect is not as strong as a normal group.",
            "score": 172.5944061279297
        },
        {
            "docid": "31075772_15",
            "document": "Thought identification . On 31 January 2012 Brian Pasley and colleagues of University of California Berkeley published their paper in PLoS Biology wherein subjects' internal neural processing of auditory information was decoded and reconstructed as sound on computer by gathering and analyzing electrical signals directly from subjects' brains. The research team conducted their studies on the superior temporal gyrus, a region of the brain that is involved in higher order neural processing to make semantic sense from auditory information. The research team used a computer model to analyze various parts of the brain that might be involved in neural firing while processing auditory signals. Using the computational model, scientists were able to identify the brain activity involved in processing auditory information when subjects were presented with recording of individual words. Later, the computer model of auditory information processing was used to reconstruct some of the words back into sound based on the neural processing of the subjects. However the reconstructed sounds were not of good quality and could be recognized only when the audio wave patterns of the reconstructed sound were visually matched with the audio wave patterns of the original sound that was presented to the subjects. However this research marks a direction towards more precise identification of neural activity in cognition.",
            "score": 171.35350036621094
        },
        {
            "docid": "33702464_5",
            "document": "Extrastriate body area . The experiment had subjects view images of different objects, including faces (as a control group), body parts, animals, parts of the face and intimate objects. While viewing the images, the subjects were scanned with an fMRI to see what area of the brain was activated. Through the trials a compilation of the fMRI\u2019s was made. From this compilation image a specific region was determined to have increased activity when shown visual stimuli of body parts and even more activity when viewing whole bodies. There have been no studies involving brain damage to the EBA. Thus far, only scans of brain activity, as well as transcranial magnetic stimulation, have been used to study the EBA. To find the specific functions of the EBA, Comimo Urgesi, Giovanni Berlucchi and Salvatore M. Aglioti used repetitive transcranial magnetic stimulation (rTMS) to disrupt part of the brain, making the brain less responsive in the target area. The study used event-related rTMS to disrupt the EBA, resulting in inactivation of cortical areas. This inactivation caused a slower response time in discriminating body parts. The study used facial features and motorcycle parts as non human parts for control groups. The facial features and motorcycle body parts did not display any change in response time. The neural activity data shows the EBA handles some of the visual processing of human body and parts but is not related to the processing of the face or other objects.",
            "score": 169.8907928466797
        },
        {
            "docid": "26945761_5",
            "document": "Cross modal plasticity . The somatosensory cortex is also able to recruit the visual cortex to assist with tactile sensation. Cross modal plasticity reworks the network structure of the brain, leading to increased connections between the somatosensory and visual cortices. Furthermore, the somatosensory cortex acts as a hub region of nerve connections in the brain for the early blind but not for the sighted. With this cross-modal networking the early blind are able to react to tactile stimuli with greater speed and accuracy, as they have more neural pathways to work with. One element of the visual system that the somatosensory cortex is able to recruit is the dorsal-visual stream. The dorsal stream is used by the sighted to identify spatial information visually, but the early blind use it during tactile sensation of 3D objects. However, both sighted and blind participants used the dorsal stream to process spatial information, suggesting that cross modal plasticity in the blind re-routed the dorsal visual stream to work with the sense of touch rather than changing the overall function of the stream.",
            "score": 168.4107208251953
        },
        {
            "docid": "484650_7",
            "document": "Functional neuroimaging . Traditional \"activation studies\" focus on determining distributed patterns of brain activity associated with specific tasks. However, scientists are able to more thoroughly understand brain function by studying the interaction of distinct brain regions, as a great deal of neural processing is performed by an integrated network of several regions of the brain. An active area of neuroimaging research involves examining the functional connectivity of spatially remote brain regions. Functional connectivity analyses allow the characterization of interregional neural interactions during particular cognitive or motor tasks or merely from spontaneous activity during rest. FMRI and PET enable creation of functional connectivity maps of distinct spatial distributions of temporally correlated brain regions called functional networks. Several studies using neuroimaging techniques have also established that posterior visual areas in blind individuals may be active during the performance of nonvisual tasks such as Braille reading, memory retrieval, and auditory localization as well as other auditory functions.",
            "score": 168.10231018066406
        },
        {
            "docid": "485309_16",
            "document": "Face perception . There are several parts of the brain that play a role in face perception. Rossion, Hanseeuw, and Dricot used BOLD fMRI mapping to identify activation in the brain when subjects viewed both cars and faces. The majority of BOLD fMRI studies use blood oxygen level dependent (BOLD) contrast to determine which areas of the brain are activated by various cognitive functions. They found that the occipital face area, located in the occipital lobe, the fusiform face area, the superior temporal sulcus, the amygdala, and the anterior/inferior cortex of the temporal lobe, all played roles in contrasting the faces from the cars, with the initial face perception beginning in the area and occipital face areas. This entire region links to form a network that acts to distinguish faces. The processing of faces in the brain is known as a \"sum of parts\" perception. However, the individual parts of the face must be processed first in order to put all of the pieces together. In early processing, the occipital face area contributes to face perception by recognizing the eyes, nose, and mouth as individual pieces. Furthermore, Arcurio, Gold, and James used BOLD fMRI mapping to determine the patterns of activation in the brain when parts of the face were presented in combination and when they were presented singly. The occipital face area is activated by the visual perception of single features of the face, for example, the nose and mouth, and preferred combination of two-eyes over other combinations. This research supports that the occipital face area recognizes the parts of the face at the early stages of recognition. On the contrary, the fusiform face area shows no preference for single features, because the fusiform face area is responsible for \"holistic/configural\" information, meaning that it puts all of the processed pieces of the face together in later processing. This theory is supported by the work of Gold et al. who found that regardless of the orientation of a face, subjects were impacted by the configuration of the individual facial features. Subjects were also impacted by the coding of the relationships between those features. This shows that processing is done by a summation of the parts in the later stages of recognition.",
            "score": 168.08090209960938
        },
        {
            "docid": "1948637_27",
            "document": "Neuroplasticity . Human echolocation is a learned ability for humans to sense their environment from echoes. This ability is used by some blind people to navigate their environment and sense their surroundings in detail. Studies in 2010 and 2011 using functional magnetic resonance imaging techniques have shown that parts of the brain associated with visual processing are adapted for the new skill of echolocation. Studies with blind patients, for example, suggest that the click-echoes heard by these patients were processed by brain regions devoted to vision rather than audition.",
            "score": 167.76846313476562
        },
        {
            "docid": "43527201_5",
            "document": "Usha Goswami . Dyslexia is a disorder in which the person affected has difficulty reading due to the reversal of letters in the brain that isn't linked to intelligence. In people with dyslexia, the brain processes certain signals in a specific way making it a very specific learning difficulty. Dr. Goswami's research is concerned with focusing on dyslexia as a language disorder rather than a visual disorder as she has found that the way that children with dyslexia hear language is slightly different than others. When sound waves approach the brain, they vary in pressure depending on the syllables within the words being spoken creating a rhythm. When these signals reach the brain they are lined up with speech rhythms and this process doesn't work properly in those with dyslexia. Goswami is currently researching whether or not reading poetry, nursery rhymes, and singing can be used to help children with dyslexia. The rhythm of the words could allow the child to match the syllable patterns to language before they begin reading as to catch them up to where children without the disability might be.",
            "score": 166.97622680664062
        },
        {
            "docid": "13643290_3",
            "document": "Daniel Kish . Kish's work has inspired a number of scientific studies related to human echolocation. In a 2009 study at the University of Alcal\u00e1 in Madrid, Spain, ten sighted subjects were taught basic navigation skills within a few days. The study aimed to analyze various sounds which can be used to echolocate and evaluate which were most effective. In another study, MRI brain scans were taken of Kish and another echolocation expert to identify the parts of the brain involved in echolocation, with readings suggesting \"that brain structures that process visual information in sighted people process echo information in blind echolocation experts.\"",
            "score": 166.2664031982422
        },
        {
            "docid": "37691878_17",
            "document": "Phonemic restoration effect . Much like the McGurk Effect, when listeners were also able to see the words being spoken, they were much more likely to correctly identify the missing phonemes. Like every sense, the brain will use every piece of information it deems important to make a judgement about what it is perceiving. Using the visual cues of mouth movements, the brain will you both in top-down processing to make a decision about what phoneme is supposed to be heard. Vision is the primary sense for humans and for the most part assists in speech perception the most.",
            "score": 164.13238525390625
        },
        {
            "docid": "17524_35",
            "document": "Language . Early work in neurolinguistics involved the study of language in people with brain lesions, to see how lesions in specific areas affect language and speech. In this way, neuroscientists in the 19th century discovered that two areas in the brain are crucially implicated in language processing. The first area is Wernicke's area, which is in the posterior section of the superior temporal gyrus in the dominant cerebral hemisphere. People with a lesion in this area of the brain develop receptive aphasia, a condition in which there is a major impairment of language comprehension, while speech retains a natural-sounding rhythm and a relatively normal sentence structure. The second area is Broca's area, in the posterior inferior frontal gyrus of the dominant hemisphere. People with a lesion to this area develop expressive aphasia, meaning that they know what they want to say, they just cannot get it out. They are typically able to understand what is being said to them, but unable to speak fluently. Other symptoms that may be present in expressive aphasia include problems with fluency, articulation, word-finding, word repetition, and producing and comprehending complex grammatical sentences, both orally and in writing. Those with this aphasia also exhibit ungrammatical speech and show inability to use syntactic information to determine the meaning of sentences. Both expressive and receptive aphasia also affect the use of sign language, in analogous ways to how they affect speech, with expressive aphasia causing signers to sign slowly and with incorrect grammar, whereas a signer with receptive aphasia will sign fluently, but make little sense to others and have difficulties comprehending others' signs. This shows that the impairment is specific to the ability to use language, not to the physiology used for speech production.",
            "score": 163.835205078125
        },
        {
            "docid": "25146378_15",
            "document": "Functional specialization (brain) . During the 1960s, Roger Sperry conducted a natural experiment on epileptic patients who had previously had their corpora callosa cut. The corpus callosum is the area of the brain dedicated to linking both the right and left hemisphere together. Sperry et al.'s experiment was based on flashing images in the right and left visual fields of his participants. Because the participant's corpus callosum was cut, the information processed by each visual field could not be transmitted to the other hemisphere. In one experiment, Sperry flashed images in the right visual field (RVF), which would subsequently be transmitted to the left hemisphere (LH) of the brain. When asked to repeat what they had previously seen, participants were fully capable of remembering the image flashed. However, when the participants were then asked to draw what they had seen, they were unable to. When Sperry et al. flashed images in the left visual field (LVF), the information processed would be sent to the right hemisphere (RH) of the brain. When asked to repeat what they had previously seen, participants were unable to recall the image flashed, but were very successful in drawing the image. Therefore, Sperry concluded that the left hemisphere of the brain was dedicated to language as the participants could clearly speak the image flashed. On the other hand, Sperry concluded that the right hemisphere of the brain was involved in more creative activities such as drawing.",
            "score": 162.59042358398438
        },
        {
            "docid": "29176704_13",
            "document": "Human Connectome Project . The Human Connectome Project Young Adult study made data on the brain connections of 1100 healthy young adults available to the scientific community. Scientists have used data from the study to support theories about which areas of the brain communicate with one another. For example, one study used data from the project to show that the amygdala, a part of the brain essential for emotional processing, is connected to the parts of the brain that receive information from the senses and plan movement. Another study showed that healthy individuals who had a high tendency to experience anxious or depressed mood had fewer connections between the amygdala and a number of brain areas related to attention.",
            "score": 162.0992889404297
        },
        {
            "docid": "176997_5",
            "document": "Blindsight . Patients with blindsight have damage to the visual system that allows perception (the visual cortex of the brain and some of the nerve fibers that bring information to it from the eyes) rather than the system that controls eye movements. This phenomenon shows how, after the more complex visual system is damaged, people can use the latter visual system of their brains to guide hand movements towards an object even though they cannot see what they are reaching for. Hence, visual information can control behavior without producing a conscious sensation. This ability of those with blindsight to \"see\" objects that they are unconscious of suggests that consciousness is not a general property of all parts of the brain; yet it suggests that only certain parts of the brain play a special role in consciousness.",
            "score": 161.61549377441406
        },
        {
            "docid": "11630765_5",
            "document": "Pure alexia . Pure alexia almost always involves an infarct to the left posterior cerebral artery (which perfuses the splenium of the corpus callosum and left visual cortex, among other things). The resulting deficit will be pure alexia \u2013 i.e., the patient can write but cannot read (even what they have just written). However, because pure alexia affects visual input, not auditory input, patients with pure alexia can recognize words that are spelled out loud to them. This is because the left visual cortex has been damaged, leaving only the right visual cortex (occipital lobe) able to process visual information, but it is unable to send this information to the language areas (Broca's area, Wernicke's area, etc.) in the left brain because of the damage to the splenium of the corpus callosum. Patients with this deficit mostly do have a stroke to the posterior cerebral artery. But they may be susceptible to pure alexia as a consequence of other traumatic brain injuries (TBIs) as well. Anything that stops proper blood flow to the area necessary for normal reading abilities will cause a form of alexia. The posterior cerebral artery is a main local for the cause of this deficit because this artery is not just responsible for itself. It also supplies the anterior temporal branches, the posterior temporal branches, the calcarrine branch, and the parieto-occipital branch. What is important about these arteries is their location. All of them supply blood to the back outer parts of the brain. This part of the brain is also referred to as the posterior lateral part of the brain. In cases of pure alexia, locations are found in the section of the brain, specifically the temporo-occipital area. This is the area that is activated when people without any sort of alexia receive activation when undergoing orthographic processing. This area is known as the visual word form area due to this pattern of activation.",
            "score": 160.1692352294922
        },
        {
            "docid": "2458955_14",
            "document": "Colin Blakemore . Although initially controversial, the idea that the mammalian brain is 'plastic' and adaptive is now a dominant theme in neuroscience. The plasticity of connections between nerve cells is thought to underlie many different types of learning and memory, as well as sensory development. The changes in organisation can be remarkably rapid, even in adults. Blakemore has shown that the visual parts of the human cortex become responsive to input from the other senses, especially touch, in people who have been blind since shortly after birth. After stroke or other forms of brain injury, reorganisation of this sort can help the process of recovery, as other parts of the brain take over the function of the damaged part.",
            "score": 159.97068786621094
        },
        {
            "docid": "2363287_6",
            "document": "Visual learning . Various areas of the brain work together in a multitude of ways in order to produce the images that we see with our eyes and that are encoded by our brains. The basis of this work takes place in the visual cortex of the brain. The visual cortex is located in the occipital lobe of the brain and harbors many other structures that aid in visual recognition, categorization, and learning. One of the first things the brain must do when acquiring new visual information is recognize the incoming material. Brain areas involved in recognition are the inferior temporal cortex, the superior parietal cortex, and the cerebellum. During tasks of recognition, there is increased activation in the left inferior temporal cortex and decreased activation in the right superior parietal cortex. Recognition is aided by neural plasticity, or the brain's ability to reshape itself based on new information. Next the brain must categorize the material. The three main areas that are used when categorizing new visual information are the orbitofrontal cortex and two dorsolateral prefrontal regions which begin the process of sorting new information into groups and further assimilating that information into things that you might already know. After recognizing and categorizing new material entered into the visual field, the brain is ready to begin the encoding process \u2013 the process which leads to learning. Multiple brain areas are involved in this process such as the frontal lobe, the right extrastriate cortex, the neocortex, and again, the neostriatum. One area in particular, the limbic-diencephalic region, is essential for transforming perceptions into memories. With the coming together of tasks of recognition, categorization and learning; schemas help make the process of encoding new information and relating it to things you already know much easier. One can remember visual images much better when they can apply it to an already known schema. Schemas actually provide enhancement of visual memory and learning.",
            "score": 159.728271484375
        },
        {
            "docid": "102958_11",
            "document": "Roger Wolcott Sperry . Working with his graduate student Michael Gazzaniga, Sperry invited several of the \"split-brain\" patients to volunteer to take part in his study to determine if the surgery affected their functioning. These tests were designed to test the patients' language, vision, and motor skills. When a person views something in the left visual field (that is on the left side of their body), the information travels to the right hemisphere of the brain and vice versa. In the first series of tests, Sperry would present a word to either the left or right visual field for a short period of time. If the word was shown to the right visual field, meaning the left hemisphere would process it, then the patient could report seeing the word. If the word was shown to the left visual field, meaning the right hemisphere would process it, then the patient could not report seeing the word. This led Sperry to believe that only the left side of the brain could articulate speech. However, in a follow-up experiment, Sperry discovered that the right hemisphere does have some language abilities. In this experiment, he had the patients place their left hands in a tray full of objects located under a partition so the patient would not be able to see the objects. Then a word was shown to the patient's left visual field, which was processed by the right side of the brain. This word described one of the objects in the tray, so the patient's left hand picked up the object corresponding to the word. When participants were asked about the word and the object in their hand, they claimed they had not seen the word and had no idea why they were holding the object. The right side of the brain had recognized the word and told the left hand to pick it up, but because the right side of the brain cannot speak and the left side of the brain had not seen the word, the patient could not articulate what they had seen.",
            "score": 158.98609924316406
        },
        {
            "docid": "1215674_37",
            "document": "Visual memory . These parts are the sustained and transient visual processing systems. The sustained system is responsible for fine detail such as word and letter recognition and is very important in encoding words in their correct order. The transient system is responsible for controlling eye movements, and processing the larger visual environment around us. When these two processes do not work in synchronization this can cause reading disabilities. This has been tested by having children with and without reading disabilities perform on tasks related to the transient systems, where the children with reading disabilities did very poorly. It has also been found in postmortem examinations of the brains of people with reading disabilities that they have fewer neurons and connections in the areas representing the transient visual systems. However there is debate over whether this is the only reason for reading disabilities, scotopic sensitivity syndrome, deficits in verbal memory and orthographic knowledge are other proposed factors.  Deficits in visual memory can also be caused by disease and/or trauma to the brain. These can lead to the patient losing their spatial memory, and/or their visual memory for specific things. For example a patient \u201cL.E.\u201d suffered brain damage and her ability to draw from memory was severely diminished, whilst her spatial memory remained normal. Other patients represent the opposite, where memory for colors and shapes is unaffected but spatial memory for previously known places is greatly impaired. These case studies show that these two types of visual memory are located in different parts of the brain and are somewhat unrelated in terms of functioning in daily life.",
            "score": 158.31472778320312
        },
        {
            "docid": "1732213_24",
            "document": "Language processing in the brain . Research on bilingual speakers shows that information about both languages is activated in the brain even when a speaker is only using one language. Some research shows that, because bilingual speakers access linguistic information in their brain differently from monolingual speakers, they have an advantage in language processing, and they outperform monolingual speakers in reaction times for language processing and then producing relevant language in certain tasks. However, other studies have found that this may not be applicable to all bilinguals.",
            "score": 158.17417907714844
        },
        {
            "docid": "1903855_7",
            "document": "Sensory substitution . In a regular visual system, the data collected by the retina is converted into an electrical stimulus in the optic nerve and relayed to the brain, which re-creates the image and perceives it. Because it is the brain that is responsible for the final perception, sensory substitution is possible. During sensory substitution an intact sensory modality relays information to the visual perception areas of the brain so that the person can perceive to see. With sensory substitution, information gained from one sensory modality can reach brain structures physiologically related to other sensory modalities. Touch-to-visual sensory substitution transfers information from touch receptors to the visual cortex for interpretation and perception. For example, through fMRI, we can determine which parts of the brain are activated during sensory perception. In blind persons, we can see that while they are only receiving tactile information, their visual cortex is also activated as they perceive to \"see\" objects. We can also have touch to touch sensory substitution where information from touch receptors of one region can be used to perceive touch in another region. For example, in one experiment by Bach-y-Rita, he was able to restore the touch perception in a patient who lost peripheral sensation from leprosy.",
            "score": 157.15988159179688
        },
        {
            "docid": "12209564_6",
            "document": "Aprosodia . Research into the perisylvan region of the right hemisphere has shown that there are similarly mapped analogues to the speech center in the left hemisphere. This is especially evident in those areas resembling Broca's area and Wernicke's area. The similarity of these regions has led scientists to view aprosodias in a similar manner to how some aphasias are viewed. Because the presence of an aphasia is often more pronounced in an individual than an aprosodia might be, aphasias have traditionally been more heavily studied. Because aphasias are rooted in deficiencies in language modalities rather than affective aspects of language, it has been easier to characterize the underlying impairment caused by brain damage (e.g. inability to choose the right word or inability to speak due to motor control). Combining aphasic research with right-left analogue mapping has allowed for researchers to produce hypotheses on the underlying process behind various aprosodias.  Additionally, in studying the brain regions associated with aprosodia, brain imaging tests were performed to determine if aprosodia is both a lateralized and dominant function of the right hemisphere areas of language production. Aprosodia can be considered a dominant function of the right hemisphere because strong correlation was found between deficits in affective prosody and distribution of lesions in the cortices of those with right brain damage. No correlation was found between the distribution of cortical lesions in patients with left brain damage and the types of aphasic deficits pronounced in those patients. Aprosodia can be considered a lateralized function of the right hemisphere because of the differences in the ability of a patient to respond to affective prosodic information in those with left brain damage when compared to those with right brain damage. Patients with affective-prosodic deficits in the left hemisphere (dysprosodic patients) showed improvement in understanding and repeating prosodic information when other conveyed linguistic information was simplified, i.e. requiring the patient to mainly determine prosodic information contained in an interaction. This improvement in processing affective prosodic information under reduced linguistic processing demands did not occur for patients with right brain damage.",
            "score": 155.60523986816406
        },
        {
            "docid": "25146378_20",
            "document": "Functional specialization (brain) . Other researchers who provide evidence to support the theory of distributive processing include Anthony McIntosh and William Uttal, who question and debate localization and modality specialization within the brain. McIntosh's research suggests that human cognition involves interactions between the brain regions responsible for processes sensory information, such as vision, audition, and other mediating areas like the prefrontal cortex. McIntosh explains that modularity is mainly observed in sensory and motor systems, however, beyond these very receptors, modularity becomes \"fuzzier\" and you see the cross connections between systems increase. He also illustrates that there is an overlapping of functional characteristics between the sensory and motor systems, where these regions are close to one another. These different neural interactions influence each other, where activity changes in one area influence other connected areas. With this, McIntosh suggest that if you only focus on activity in one area, you may miss the changes in other integrative areas. Neural interactions can be measured using analysis of covariance in neuroimaging. McIntosh used this analysis to convey a clear example of the interaction theory of distributive processing. In this study, subjects learned that an auditory stimulus signalled a visual event. McIntosh found activation (an increase blood flow), in an area of the occipital cortex, a region of the brain involved in visual processing, when the auditory stimulus was presented alone. Correlations between the occipital cortex and different areas of the brain such as the prefrontal cortex, premotor cortex and superior temporal cortex showed a pattern of co-variation and functional connectivity.",
            "score": 155.44888305664062
        },
        {
            "docid": "599917_31",
            "document": "Mental image . As cognitive neuroscience approaches to mental imagery continued, research expanded beyond questions of serial versus parallel or topographic processing to questions of the relationship between mental images and perceptual representations. Both brain imaging (fMRI and ERP) and studies of neuropsychological patients have been used to test the hypothesis that a mental image is the reactivation, from memory, of brain representations normally activated during the perception of an external stimulus. In other words, if perceiving an apple activates contour and location and shape and color representations in the brain\u2019s visual system, then imagining an apple activates some or all of these same representations using information stored in memory. Early evidence for this idea came from neuropsychology. Patients with brain damage that impairs perception in specific ways, for example by damaging shape or color representations, seem to generally to have impaired mental imagery in similar ways. Studies of brain function in normal human brains support this same conclusion, showing activity in the brain\u2019s visual areas while subjects imagined visual objects and scenes.",
            "score": 155.4014129638672
        },
        {
            "docid": "9325864_16",
            "document": "Imagination . Memory and mental imagery, often seen as a part of the process of imagination, have been shown to be affected by one another. \"Images made by functional magnetic resonance imaging technology show that remembering and imagining sends blood to identical parts of the brain.\" Various psychological factors can influence the mental processing of and can heighten the chance of the brain to retain information as either long-term memories or short-term memories. John Sweller indicated that experiences stored as long-term memories are easier to recall, as they are ingrained deeper in the mind. Each of these forms require information to be taught in a specific manner so as to use various regions of the brain when being processed. This information can potentially help develop programs for young students to cultivate or further enhance their creative abilities from a young age. The neocortex and thalamus are responsible for controlling the brain's imagination, along with many of the brain's other functions such as consciousness and abstract thought. Since imagination involves many different brain functions, such as emotions, memory, thoughts, etc., portions of the brain where multiple functions occur\u2014such as the thalamus and neocortex\u2014are the main regions where imaginative processing has been documented. The understanding of \"how\" memory and imagination are linked in the brain, paves the way to better understand one's ability to link significant past experiences with their imagination.",
            "score": 155.01541137695312
        },
        {
            "docid": "1764639_17",
            "document": "Levels-of-processing effect . Several brain imaging studies using positron emission tomography and functional magnetic resonance imaging techniques have shown that higher levels of processing correlate with more brain activity and activity in different parts of the brain than lower levels. For example, in a lexical analysis task, subjects showed activity in the left inferior prefrontal cortex only when identifying whether the word represented a living or nonliving object, and not when identifying whether or not the word contained an \"a\". Similarly, an auditory analysis task showed increased activation in the left inferior prefrontal cortex when subjects performed increasingly semantic word manipulations. Synaptic aspects of word recognition have been correlated with the left frontal operculum and the cortex lining the junction of the inferior frontal and inferior precentral sulcus. The self-reference effect also has neural correlates with a region of the medial prefrontal cortex, which was activated in an experiment where subjects analyzed the relevance of data to themselves. Specificity of processing is explained on a neurological basis by studies that show brain activity in the same location when a visual memory is encoded and retrieved, and lexical memory in a different location. Visual memory areas were mostly located within the bilateral extrastriate visual cortex.",
            "score": 154.87437438964844
        }
    ]
}