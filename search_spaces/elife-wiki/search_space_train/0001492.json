{
    "q": [
        {
            "docid": "9738540_12",
            "document": "Phylogenetic comparative methods . Martins and Garland proposed in 1991 that one way to account for phylogenetic relations when conducting statistical analyses was to use computer simulations to create many data sets that are consistent with the null hypothesis under test (e.g., no correlation between two traits, no difference between two ecologically defined groups of species) but that mimic evolution along the relevant phylogenetic tree. If such data sets (typically 1,000 or more) are analyzed with the same statistical procedure that is used to analyze a real data set, then results for the simulated data sets can be used to create phylogenetically correct (or \"PC\") null distributions of the test statistic (e.g., a correlation coefficient, t, F). Such simulation approaches can also be combined with such methods as phylogenetically independent contrasts or PGLS (see above).",
            "score": 142.01716685295105
        },
        {
            "docid": "45329906_21",
            "document": "Solvent models . Quantitative Structure\u2013Activity Relationships (QSAR)/Quantitative Structure\u2013Property Relationships (QSPR), whilst unable to directly model the physical process occurring in a condensed solvent phase, can provide useful predictions of solvent and solvation properties and activities; such as the solubility of a solute. These methods come in a varied way from simple regression models to sophisticated machine learning methods. Generally, QSAR/QSPR methods require descriptors; these come in many different forms and are used to represent physical features and properties of a system of interest. Descriptors are generally single numerical values which hold some information about a physical property. A regression model or statistical learning model is then applied to find a correlation between the descriptor(s) and the property of interest. Once trained on some known data these model can be applied to similar unknown data to make predictions. Typically the known data comes from experimental measurement, although there is no reason why similar methods can not be used to correlate descriptor(s) with theoretical or predicted values. It is currently debated whether if more accurate experimental data was used to train these models whether the prediction from such models would be more accurate.",
            "score": 138.3961842060089
        },
        {
            "docid": "1414677_7",
            "document": "Comparative politics . The comparative method is \u2013 together with the experimental method, the statistical method and the case study approach \u2013 one of the four fundamental scientific methods which can be used to test the validity of theoretical propositions, often with the use of empirical data i.e. to establish relationships among two or more empirical variables or concepts while all other variables are held constant. In particular, the comparative method is generally used when neither the experimental nor the statistical method can be employed: on the one hand, experiments can only rarely be conducted in political science; on the other hand the statistical method implies the mathematical manipulation of quantitative data about a large number of cases, while sometimes political research must be conducted by analyzing the behavior of qualitative variables in a small number of cases.",
            "score": 130.11267066001892
        },
        {
            "docid": "9738540_2",
            "document": "Phylogenetic comparative methods . Phylogenetic comparative methods (PCMs) use information on the historical relationships of lineages (phylogenies) to test evolutionary hypotheses. The comparative method has a long history in evolutionary biology; indeed, Charles Darwin used differences and similarities between species as a major source of evidence in \"The Origin of Species\". However, the fact that closely related lineages share many traits and trait combinations as a result of the process of descent with modification means that lineages are not independent. This realization inspired the development of explicitly phylogenetic comparative methods. Initially, these methods were primarily developed to control for phylogenetic history when testing for adaptation however in recent years the use of the term has broadened to include any use of phylogenies in statistical tests. Although most studies that employ PCMs focus on extant organisms, many methods can also be applied to extinct taxa and can incorporate information from the fossil record.",
            "score": 124.76583313941956
        },
        {
            "docid": "9014_54",
            "document": "Developmental psychology . Developmental psychologists have a number of methods to study changes in individuals over time. Common research methods include systematic observation, including naturalistic observation or structured observation; self-reports, which could be clinical interviews or structured interviews; clinical or case study method; and ethnography or participant observation. These methods differ in the extent of control researchers impose on study conditions, and how they construct ideas about which variables to study. Every developmental investigation can be characterized in terms of whether its underlying strategy involves the \"experimental\", \"correlational\", or \"case study\" approach. The experimental method involves \"actual manipulation of various treatments, circumstances, or events to which the participant or subject is exposed; the \"experimental design\" points to cause-and-effect relationships. This method allows for strong inferences to be made of causal relationships between the manipulation of one or more independent variables and subsequent behavior, as measured by the dependent variable. The advantage of using this research method is that it permits determination of cause-and-effect relationships among variables. On the other hand, the limitation is that data obtained in an artificial environment may lack generalizability. The correlational method explores the relationship between two or more events by gathering information about these variables without researcher intervention. The advantage of using a correlational design is that it estimates the strength and direction of relationships among variables in the natural environment; however, the limitation is that it does not permit determination of cause-and-effect relationships among variables. The case study approach allows investigations to obtain an in-depth understanding of an individual participant by collecting data based on interviews, structured questionnaires, observations, and test scores. Each of these methods have its strengths and weaknesses but the experimental method when appropriate is the preferred method of developmental scientists because it provides a controlled situation and conclusions to be drawn about cause-and-effect relationships.",
            "score": 155.6716833114624
        },
        {
            "docid": "1522933_17",
            "document": "Telecommunications forecasting . It is difficult to determine the accuracy of any forecast, as it represents an attempt to predict future events, which is always challenging. To help improve and test forecast accuracy researchers use many different checking methods. A simple checking method involves the use of several different forecasting methods and comparing the results to see if they are more or less equal. Another method can involve statistically calculating the errors in the forecasting calculation and expressing them in terms of the root mean squared error, thereby providing an indication of the overall error in the method. A sensitivity analysis can also be useful, as it determines what will happen if some of the original data upon which the forecast was developed turned out to be wrong. Determining forecast accuracy, like forecasting itself, can never be performed with certainty and so it is advisable to ensure that input data is measured and obtained as accurately as possible, the most appropriate forecasting methods are selected, and the forecasting process is conducted as rigorously as possible.",
            "score": 112.94582796096802
        },
        {
            "docid": "21312140_5",
            "document": "Quantitative comparative linguistics . The standard method for assessing language relationships has been the comparative method. However this has a number of limitations. Not all linguistic material is suitable as input and there are issues of the linguistic levels on which the method operates. The reconstructed languages are idealized and different scholars can produce different results. Language family trees are often used in conjunction with the method and \"borrowings\" must be excluded from the data, which is difficult when borrowing is within a family. It is often claimed that the method is limited in the time depth over which it can operate. The method is difficult to apply and there is no independent test. Thus alternative methods have been sought that have a formalised method, quantify the relationships and can be tested.",
            "score": 134.61887669563293
        },
        {
            "docid": "634_2",
            "document": "Analysis of variance . Analysis of variance (ANOVA) is a collection of statistical models and their associated estimation procedures (such as the \"variation\" among and between groups) used to analyze the differences among group means in a sample. ANOVA was developed by statistician and evolutionary biologist Ronald Fisher. In the ANOVA setting, the observed variance in a particular variable is partitioned into components attributable to different sources of variation. In its simplest form, ANOVA provides a statistical test of whether the population means of several groups are equal, and therefore generalizes the \"t\"-test to more than two groups. ANOVA is useful for comparing (testing) three or more group means for statistical significance. It is conceptually similar to multiple two-sample t-tests, but is more conservative (results in less type I error) and is therefore suited to a wide range of practical problems. While the analysis of variance reached fruition in the 20th century, antecedents extend centuries into the past according to Stigler. These include hypothesis testing, the partitioning of sums of squares, experimental techniques and the additive model. Laplace was performing hypothesis testing in the 1770s. The development of least-squares methods by Laplace and Gauss circa 1800 provided an improved method of combining observations (over the existing practices then used in astronomy and geodesy). It also initiated much study of the contributions to sums of squares. Laplace knew how to estimate a variance from a residual (rather than a total) sum of squares. By 1827 Laplace was using least squares methods to address ANOVA problems regarding measurements of atmospheric tides. Before 1800 astronomers had isolated observational errors resulting  from reaction times (the \"personal equation\") and had developed methods of reducing the errors. The experimental methods used in the study of the personal equation were later accepted by the emerging field of psychology which developed strong (full factorial) experimental methods to which randomization and blinding were soon added. An eloquent non-mathematical explanation of the additive effects model was available in 1885.",
            "score": 111.20214796066284
        },
        {
            "docid": "2893954_5",
            "document": "Bulgarian Turks . A Y-DNA genetic study on Slavic peoples and some of their neighbours published two statistical distributions of distance because of the volume of details studied, based on pairwise F values, the Turks from Bulgaria are most related to Anatolian Turks, thereafter to Italians, Bulgarians and others]; while according to the R values, the Turks from Bulgaria are most related to Bulgarians, thereafter to Macedonians, Anatolian Turks, Serbs and the rest, while Balts and North Slavs remain most unrelated according to them both. The study claims that the F genetic distances reflect interpopulation relationships between the compared populations much better than their stepwise-based analogues, but that at the same time the genetic variation was more profoundly calculated by R. F and R calculate allele (haplotype or microsatellite) frequencies among populations and the distribution of evolutionary distances among alleles. R is based on the number of repeat differences between alleles at each microsatellite locus and is proposed to be better for most typical sample sizes, when data consist of variation at microsatellite loci or of nucleotide sequence (haplotype) information, the method may be unreliable unless a large number of loci are used. A nonsignificant test suggests that F should be preferred or when there is high gene flow within populations, F calculations are based on allele identity, it is likely to perform better than counterparts based on allele size information, the method depends on mutation rate, sometimes can likely provide biased estimate, but R will not perform necessarily better. A Bulgarian and other population studies observed concluded that when there is not much differiation, both statistical means show similar results, otherwise R is often superior to the F. However, no procedure has been developed to date for testing whether single-locus R and F estimates are significantly different.",
            "score": 119.06316208839417
        },
        {
            "docid": "14986442_46",
            "document": "History of statistics . He also founded the statistical hypothesis testing theory, Pearson's chi-squared test and principal component analysis. In 1911 he founded the world's first university statistics department at University College London. The second wave of mathematical statistics was pioneered by Ronald Fisher who wrote two textbooks, \"Statistical Methods for Research Workers\", published in 1925 and \"The Design of Experiments\" in 1935, that were to define the academic discipline in universities around the world. He also systematized previous results, putting them on a firm mathematical footing. In his 1918 seminal paper \"The Correlation between Relatives on the Supposition of Mendelian Inheritance\", the first use to use the statistical term, variance. In 1919, at Rothamsted Experimental Station he started a major study of the extensive collections of data recorded over many years. This resulted in a series of reports under the general title \"Studies in Crop Variation.\" In 1930 he published \"The Genetical Theory of Natural Selection\" where he applied statistics to evolution. Over the next seven years, he pioneered the principles of the design of experiments (see below) and elaborated his studies of analysis of variance. He furthered his studies of the statistics of small samples. Perhaps even more important, he began his systematic approach of the analysis of real data as the springboard for the development of new statistical methods. He developed computational algorithms for analyzing data from his balanced experimental designs. In 1925, this work resulted in the publication of his first book, \"Statistical Methods for Research Workers\". This book went through many editions and translations in later years, and it became the standard reference work for scientists in many disciplines. In 1935, this book was followed by \"The Design of Experiments\", which was also widely used.",
            "score": 108.31928789615631
        },
        {
            "docid": "5457540_12",
            "document": "Ka/Ks ratio . Of course, it is necessary to perform a statistical analysis to determine whether a result is significantly different from 1, or whether any apparent difference may occur as a result of a limited data set. The appropriate statistical test for an approximate method involves approximating dN \u2212 dS with a normal approximation, and determining whether 0 falls within the central region of the approximation. More sophisticated likelihood techniques can be used to analyse the results of a Maximum Likelihood analysis, by performing a chi-squared test to distinguish between a null model (K/K = 1) and the observed results.",
            "score": 94.12846755981445
        },
        {
            "docid": "931814_9",
            "document": "Comparative linguistics . Recently, computerised statistical hypothesis testing methods have been developed which are related to both the comparative method and lexicostatistics. Character based methods are similar to the former and distanced based methods are similar to the latter (see Quantitative comparative linguistics). The characters used can be morphological or grammatical as well as lexical. Since the mid-1990s these more sophisticated tree- and network-based phylogenetic methods have been used to investigate the relationships between languages and to determine approximate dates for proto-languages. These are considered by many to show promise but are not wholly accepted by traditionalists. However, they are not intended to replace older methods but to supplement them. Such statistical methods cannot be used to derive the features of a proto-language, apart from the fact of the existence of shared items of the compared vocabulary. These approaches have been challenged for their methodological problems, since without a reconstruction or at least a detailed list of phonological correspondences there can be no demonstration that two words in different languages are cognate.",
            "score": 105.23767411708832
        },
        {
            "docid": "23047_10",
            "document": "Pseudoscience . A number of basic principles are accepted by scientists as standards for determining whether a body of knowledge, method, or practice is scientific. Experimental results should be reproducible and verified by other researchers. These principles are intended to ensure experiments can be reproduced measurably given the same conditions, allowing further investigation to determine whether a hypothesis or theory related to given phenomena is valid and reliable. Standards require the scientific method to be applied throughout, and bias to be controlled for or eliminated through randomization, fair sampling procedures, blinding of studies, and other methods. All gathered data, including the experimental or environmental conditions, are expected to be documented for scrutiny and made available for peer review, allowing further experiments or studies to be conducted to confirm or falsify results. Statistical quantification of significance, confidence, and error are also important tools for the scientific method.",
            "score": 107.54378581047058
        },
        {
            "docid": "35256679_4",
            "document": "Community fingerprinting . The advantages of community fingerprinting are that it can be performed quickly and relatively cheaply, and the analyses can accommodate a large number of samples simultaneously. These properties make community fingerprinting especially useful for monitoring changes in microbial communities over time. Also, fingerprinting techniques do not require one to have \"a priori\" sequence data for organisms in a sample. A disadvantage of community fingerprinting is that it results in largely qualitative, not quantitative data. When using qualitative data, it can be difficult to compare patterns observed in different studies or between different investigators. Also, community fingerprinting does not directly identify taxa in an environmental sample, though the data output from certain techniques (e.g. DGGE) can be analyzed further if one desires identification. Some authors point to poor reproducibility of results for certain fingerprinting methods, while other authors have criticized the inaccuracy of abundance estimates and the inability of some techniques to capture the presence of rare taxa. For example, it is difficult for the DGGE method to detect microbes that comprise less than 0.5%-1% of a bacterial community.",
            "score": 122.27656984329224
        },
        {
            "docid": "42253_18",
            "document": "Data mining . Data mining can unintentionally be misused, and can then produce results which appear to be significant; but which do not actually predict future behaviour and cannot be reproduced on a new sample of data and bear little use. Often this results from investigating too many hypotheses and not performing proper statistical hypothesis testing. A simple version of this problem in machine learning is known as overfitting, but the same problem can arise at different phases of the process and thus a train/test split - when applicable at all - may not be sufficient to prevent this from happening. The final step of knowledge discovery from data is to verify that the patterns produced by the data mining algorithms occur in the wider data set. Not all patterns found by the data mining algorithms are necessarily valid. It is common for the data mining algorithms to find patterns in the training set which are not present in the general data set. This is called overfitting. To overcome this, the evaluation uses a test set of data on which the data mining algorithm was not trained. The learned patterns are applied to this test set, and the resulting output is compared to the desired output. For example, a data mining algorithm trying to distinguish \"spam\" from \"legitimate\" emails would be trained on a training set of sample e-mails. Once trained, the learned patterns would be applied to the test set of e-mails on which it had \"not\" been trained. The accuracy of the patterns can then be measured from how many e-mails they correctly classify. A number of statistical methods may be used to evaluate the algorithm, such as ROC curves.",
            "score": 131.6094753742218
        },
        {
            "docid": "12398_59",
            "document": "Geographic information system . Interpolation is the process by which a surface is created, usually a raster dataset, through the input of data collected at a number of sample points. There are several forms of interpolation, each which treats the data differently, depending on the properties of the data set. In comparing interpolation methods, the first consideration should be whether or not the source data will change (exact or approximate). Next is whether the method is subjective, a human interpretation, or objective. Then there is the nature of transitions between points: are they abrupt or gradual. Finally, there is whether a method is global (it uses the entire data set to form the model), or local where an algorithm is repeated for a small section of terrain.",
            "score": 119.28595185279846
        },
        {
            "docid": "62329_14",
            "document": "Meta-analysis . A Tandem Method for analyzing publication bias has been suggested for cutting down false positive error problems. This Tandem method consists of three stages. Firstly, one calculates Orwin's fail-safe N, to check how many studies should be added in order to reduce the test statistic to a trivial size. If this number of studies is larger than the number of studies used in the meta-analysis, it is a sign that there is no publication bias, as in that case, one needs a lot of studies to reduce the effect size. Secondly, one can do an Egger's regression test, which tests whether the funnel plot is symmetrical. As mentioned before: a symmetrical funnel plot is a sign that there is no publication bias, as the effect size and sample size are not dependent. Thirdly, one can do the trim-and-fill method, which imputes data if the funnel plot is asymmetrical.",
            "score": 107.79542589187622
        },
        {
            "docid": "1311951_18",
            "document": "Data dredging . Ultimately, the statistical significance of a test and the statistical confidence of a finding are joint properties of data and the method used to examine the data. Thus, if someone says that a certain event has probability of 20% \u00b1 2% 19 times out of 20, this means that if the probability of the event is estimated \"by the same method\" used to obtain the 20% estimate, the result is between 18% and 22% with probability 0.95. No claim of statistical significance can be made by only looking, without due regard to the method used to assess the data.",
            "score": 110.82321977615356
        },
        {
            "docid": "1367908_14",
            "document": "Morphometrics . Multivariate statistical methods can be used to test statistical hypotheses about factors that affect shape and to visualize their effects. To visualize the patterns of variation in the data, the data need to be reduced to a comprehensible (low-dimensional) form. Principal component analysis (PCA) is a commonly employed tool to do summarize the variation. Simply put, the technique projects as much of the overall variation as possible into a few dimensions. See the figure at the right for an example. Each axis on a PCA plot is an eigenvector of the covariance matrix of shape variables. The first axis accounts for maximum variation in the sample, with further axes representing further ways in which the samples vary. The pattern of clustering of samples in this morphospace represents similarities and differences in shapes, which can reflect phylogenetic relationships. As well as exploring patterns of variation, Multivariate statistical methods can be used to test statistical hypotheses about factors that affect shape and to visualize their effects, although PCA is not needed for this purpose unless the method requires inverting the variance-covariance matrix.",
            "score": 114.07364177703857
        },
        {
            "docid": "1041204_13",
            "document": "Granular computing . A different class of variable granulation methods derive more from data clustering methodologies than from the linear systems theory informing the above methods. It was noted fairly early that one may consider \"clustering\" related variables in just the same way that one considers clustering related data. In data clustering, one identifies a group of similar entities (using a \"measure of similarity\" suitable to the domain), and then in some sense \"replaces\" those entities with a prototype of some kind. The prototype may be the simple average of the data in the identified cluster, or some other representative measure. But the key idea is that in subsequent operations, we may be able to use the single prototype for the data cluster (along with perhaps a statistical model describing how exemplars are derived from the prototype) to \"stand in\" for the much larger set of exemplars. These prototypes are generally such as to capture most of the information of interest concerning the entities. Similarly, it is reasonable to ask whether a large set of variables might be aggregated into a smaller set of \"prototype\" variables that capture the most salient relationships between the variables. Although variable clustering methods based on linear correlation have been proposed (;), more powerful methods of variable clustering are based on the mutual information between variables. Watanabe has shown (;) that for any set of variables one can construct a \"polytomic\" (i.e., n-ary) tree representing a series of variable agglomerations in which the ultimate \"total\" correlation among the complete variable set is the sum of the \"partial\" correlations exhibited by each agglomerating subset (see figure). Watanabe suggests that an observer might seek to thus partition a system in such a way as to minimize the interdependence between the parts \"... as if they were looking for a natural division or a hidden crack.\"",
            "score": 115.00288593769073
        },
        {
            "docid": "326471_27",
            "document": "Mathematics education . As with other educational research (and the social sciences in general), mathematics education research depends on both quantitative and qualitative studies. Quantitative research includes studies that use inferential statistics to answer specific questions, such as whether a certain teaching method gives significantly better results than the status quo. The best quantitative studies involve randomized trials where students or classes are randomly assigned different methods in order to test their effects. They depend on large samples to obtain statistically significant results.",
            "score": 89.7371654510498
        },
        {
            "docid": "37837121_2",
            "document": "Informal Methods (Validation and Verification) . Informal methods of validation and verification are some of the more frequently used in modeling and simulation. They are called informal because they are more qualitative than quantitative. Whereas many methods of validation or verification rely on numerical results, informal methods tend to rely on the opinions of experts to draw a conclusion. While numerical results are not the primary focus, this does not mean that the numerical results are completely ignored. There are several reasons why an informal method might be chosen. In some cases, informal methods offer the convenience of quick testing to see if a model can be validated. In other instances, informal methods are the best available option. In all cases though it is important to note that informal does not mean it is any less of a true testing method. These methods should be performed with the same discipline and structure that one would expect in \"formal\" methods. When executed in such a way, solid conclusions can be made. In modeling and simulation, verification techniques are used to analyze the state of the model. Verification is completed by different methods with the focus of comparing different aspects of the executable model with the conceptual model. On the other hand, validation methods are the methods by which a model, either conceptual or executable is compared with the situation it is trying to model. Both are methods by which the model can be analyzed to help find defects in the modeling methods being used, or potential misrepresentations of the real-life situation.",
            "score": 83.9443359375
        },
        {
            "docid": "4466421_4",
            "document": "Clinical data repository . The use of Clinical Data Repositories could provide a wealth of knowledge about patients, their medical conditions, and their outcome. The database could serve as a way to study the relationship and potential patterns between disease progression and management. The term \"Medical Data Mining\" has been coined for this method of research. Past epidemiological studies may not have had as complete of information as that which is contained in a CDR, which could lead to inconclusive data/results. The use of medical data mining and correlative studies using the CDR could serve as a valuable resource helping the future of healthcare in all facets of medicine. The idea of data mining a CDW was used for screening variables that were associated with diabetes and poor glycemic control. It allowed for novel correlations that may have not been discovered without this method.",
            "score": 105.85711073875427
        },
        {
            "docid": "5630419_3",
            "document": "Preimplantation genetic haplotyping . PGH differs from common PGD methods such as fluorescence \"in situ\" hybridization (FISH) and polymerase chain reaction (PCR) for two primary reasons. First, rather than focusing on the genetic makeup of an embryo PGH compares the genome of affected and unaffected members of previous generations. This examination of generational variation then allows for a haplotype of genetic markers statistically associated with the target disease to be identified, rather than searching merely for a mutation. PGH is often used to reinforce other methods of genetic testing, and is considered more accurate than certain more common PGD methods because it has been found to reduce risk of misdiagnoses. Studies have found that misdiagnoses due to allele dropout (ADO), one of the most common causes of interpretation error, can be almost entirely eliminated through use of PGH. Further, in the case of mutation due to translocation, PGH is able to detect chromosome abnormality to its full extent by differentiating between embryos carrying balanced forms of a translocation versus those carrying the homologous normal chromosomes. This is an advantage because PGD methods such as FISH are able to reveal whether an embryo will express the phenotypic difference, but not whether an embryo may be a carrier. In 2015, PGH was used in conjunction with a whole-genome amplification (WGA) process to not only diagnose disease but also distinguish meiotic segregation errors from mitotic ones.",
            "score": 86.3342479467392
        },
        {
            "docid": "27579_5",
            "document": "Statistical theory . Statistical models describe the sources of data and can have different types of formulation corresponding to these sources and to the problem being studied. Such problems can be of various kinds: Statistical models, once specified, can be tested to see whether they provide useful inferences for new data sets. Testing a hypothesis using the data that was used to specify the model is a fallacy, according to the natural science of Bacon and the scientific method of Peirce.",
            "score": 108.52696895599365
        },
        {
            "docid": "322317_8",
            "document": "Economic data . Many methods can be used to analyse the data. These include, \"e.g.,\" time-series analysis using multiple regression, Box\u2013Jenkins analysis, and seasonality analysis. Analysis may be univariate (modeling one series) or multivariate (from several series). Econometricians, economic statisticians, and financial analysts formulate models, whether for past relationships or for economic forecasting. These models may include partial equilibrium microeconomics aimed at examining particular parts of an economy or economies, or they may cover a whole economic system, as in general equilibrium theory or in macroeconomics. Economists use these models to understand past events and to forecast future events, \"e.g.,\" demand, prices and employment. Methods have also been developed for analyzing or correcting results from use of incomplete data and errors in variables.",
            "score": 107.63130521774292
        },
        {
            "docid": "149353_4",
            "document": "Computational biology . Computational Biology, which includes many aspects of bioinformatics, is the science of using biological data to develop algorithms or models to understand biological systems and relationships.  Until recently, biologists did not have access to very large amounts of data. This data has now become commonplace, particularly in molecular biology and genomics. Researchers were able to develop analytical methods for interpreting biological information, but were unable to share them quickly among colleagues.  Bioinformatics began to develop in the early 1970s. It was considered the science of analyzing informatics processes of various biological systems. At this time, research in artificial intelligence was using network models of the human brain in order to generate new algorithms. This use of biological data to develop other fields pushed biological researchers to revisit the idea of using computers to evaluate and compare large data sets. By 1982, information was being shared amongst researchers through the use of punch cards. The amount of data being shared began to grow exponentially by the end of the 1980s. This required the development of new computational methods in order to quickly analyze and interpret relevant information. Since the late 1990s, computational biology has become an important part of developing emerging technologies for the field of biology. The terms computational biology and evolutionary computation have a similar name, but are not to be confused. Unlike computational biology, evolutionary computation is not concerned with modeling and analyzing biological data. It instead creates algorithms based on the ideas of evolution across species. Sometimes referred to as genetic algorithms, the research of this field can be applied to computational biology. While evolutionary computation is not inherently a part of computational biology, Computational evolutionary biology is a subfield of it.",
            "score": 129.1236242055893
        },
        {
            "docid": "6642351_15",
            "document": "BS 5930 . These are used when laboratory testing is not enough to determine the required properties of the ground. Laboratory samples are at times not considered to be representative and of insufficient quality, stress, pore pressure, and degree of saturation. Discontinuities in the ground can also warrant filed tests. Sample sizes depend on ground nature and test type. Boreholes are commonly used. The SPT is a simple and inexpensive test which can furnish a piling contractor with useful information. The van test is used to determine the shear strength of a soil - material with coarse silt or sand can mean unreliable results.  Permeability is found by determining whether the relevant aquifer is confined on unconfined with cognizance of the normal fluctuations in the aquifer. Installation of the borehole itself may influence stresses. For a reliable test this should be followed by a pumping test. Packer tests are also used to measure the impermeability of grouted ground and permeability of dam foundations, strength and deformation data can also be taken. There are many types, mechanical, hydraulic and pneumatic, the latter being the most popular. A clean borehole with a properly seated packer is essential (cement mortar is sometime used). Geophysical logs from the boreholes used can be taken at the same time enhancing the value of the results.  Pressuremeter tests are used to stress, stiffness and strength of the ground to be investigated. It can be used in most ground types. There are three main types \u2013 pre-bored, self bored and pushed in. Boring should do minimal damage to the ground as much as possible. an unload reload method of around three times is used to give an accurate value of stiffness.  Probing from the surface is done using a steel rod. Mainly used at a preliminary stage, it is also useful to check surrounding ground but is unsuitable in soils with boulders and cobbles. Static probing is mostly conducted with the use of electric sensors. It is quick and cheap. Pumping allows the determination of groundwater conditions using pumping and observation wells. Data interpretation can be complicated and is classed in steady and non-steady states. Density testing is conducted using the average of three results to obtain a significant result. The use of the sand replacement and core cutter test are common and the use of water replacement, rubber balloon, and nuclear methods are also used. In-situ testing data is important to design of works. Stress measurement in rocks and soils can be determined. Bearing tests are used to determine the shear strength and deformation characteristics of a soil. In-situ shear testing is done using a system similar to the laboratory shear box test. Large scale testing should be assessed on a case by case basis. Slope failure or settlement of a structure after field tests have been conducted are examples of phenomena which may be considered as back analysis, this be conducted successfully when accompanied by a full investigation to determine the ground and ground water conditions.  Geophysical surveying can be useful in site investigation for determining layers of rock other geological features, locating aquifers, mineral deposits, voids \u2013 natural or man-made and engineering properties of the ground. Electrical resistivity and seismic methods amongst others are used. This is a specialized field. The geophysical adviser should be involved at all stages. Experience has shown care should be taken when writing the specification for this type of work amongst other factors.",
            "score": 110.44982469081879
        },
        {
            "docid": "42253_7",
            "document": "Data mining . The related terms \"data dredging\", \"data fishing\", and \"data snooping\" refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations.",
            "score": 100.3465564250946
        },
        {
            "docid": "35269346_6",
            "document": "Statistical data type . The issue of whether or not it is appropriate to apply different kinds of statistical methods to data obtained from different kinds of measurement procedures is complicated by issues concerning the transformation of variables and the precise interpretation of research questions. \"The relationship between the data and what they describe merely reflects the fact that certain kinds of statistical statements may have truth values which are not invariant under some transformations. Whether or not a transformation is sensible to contemplate depends on the question one is trying to answer\" (Hand, 2004, p.\u00a082).",
            "score": 108.81497406959534
        },
        {
            "docid": "52016755_10",
            "document": "Doping at the 1998 Tour de France . At the time of the race there was no official test for EPO. In 2004, 60 remaining antidoping samples given by riders during the 1998 Tour, were tested retrospectively for recombinant EPO by using three recently developed detection methods. More precisely the laboratory compared the result of test method A: \"Autoradiography\u00a0\u2014 visual inspection of light emitted from a strip displaying the isoelectric profile for EPO\" (published in the Nature journal as the first EPO detection method in June 2000), with the result of test method B: \"Percentage of basic isoforms\u00a0\u2014 using an ultra-sensitive camera that by percentage quantify the light intensity emitted from each of the isoelectric bands\" (pioneered at the Olympics in September 2000, with values above 80% classified as failed, but the laboratory applying an 85% threshold for retrospective samples - to be absolutely certain that no false-positives (incorrectly failed tests) can occur when analyzing on samples stored for multiple years). For those samples with enough urine left, these results of test method A+B were finally also compared with the best and latest test method C: \"Statistical discriminant analysis\u00a0\u2014 taking account all the band profiles by statistical distinguish calculations for each band\" (which feature both higher sensitivity and accuracy compared to test method B).",
            "score": 94.98535215854645
        },
        {
            "docid": "41224221_2",
            "document": "Weighted correlation network analysis . Weighted correlation network analysis, also known as weighted gene co-expression network analysis (WGCNA), is a widely used data mining method especially for studying biological networks based on pairwise correlations between variables. While it can be applied to most high-dimensional data sets, it has been most widely used in genomic applications. It allows one to define modules (clusters), intramodular hubs, and network nodes with regard to module membership, to study the relationships between co-expression modules, and to compare the network topology of different networks (differential network analysis). WGCNA can be used as a data reduction technique (related to oblique factor analysis ), as a clustering method (fuzzy clustering), as a feature selection method (e.g. as gene screening method), as a framework for integrating complementary (genomic) data (based on weighted correlations between quantitative variables), and as a data exploratory technique. Although WGCNA incorporates traditional data exploratory techniques, its intuitive network language and analysis framework transcend any standard analysis technique. Since it uses network methodology and is well suited for integrating complementary genomic data sets, it can be interpreted as systems biologic or systems genetic data analysis method. By selecting intramodular hubs in consensus modules, WGCNA also gives rise to network based meta analysis techniques.",
            "score": 118.8388341665268
        }
    ],
    "r": [
        {
            "docid": "31839460_23",
            "document": "Fatigue detection software . To overcome this problem, scientists developed the Universal Fatigue Algorithm based on a data-driven approach. Drowsiness is a state determined by independent non-EEG measures. The Oxford Sleep Resistance Test (OSLER test) and the Psychomotor Vigilance Test (PVT) are the most commonly used measures in sleep research. Both tests were used to establish the sample dataset for development of the Universal Fatigue Algorithm. The algorithm was developed from real EEG of a large number of individuals. Artificial intelligence techniques were then used to map the multitude of individual relationships. The implication is that the result gets progressively universal and significant as more data from a wider range of individuals are included in the algorithm. In addition to an unseen-blinded experiment approach, testing of the algorithm is also subject to independent external parties.",
            "score": 160.7086944580078
        },
        {
            "docid": "416612_2",
            "document": "Cross-validation (statistics) . Cross-validation, sometimes called rotation estimation, or out-of-sample testing is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice. In a prediction problem, a model is usually given a dataset of \"known data\" on which training is run (\"training dataset\"), and a dataset of \"unknown data\" (or \"first seen\" data) against which the model is tested (called the validation dataset or \"testing set\"). The goal of cross-validation is to test the model\u2019s ability to predict new data that were not used in estimating it, in order to flag problems like overfitting and to give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem).",
            "score": 155.7864532470703
        },
        {
            "docid": "9014_54",
            "document": "Developmental psychology . Developmental psychologists have a number of methods to study changes in individuals over time. Common research methods include systematic observation, including naturalistic observation or structured observation; self-reports, which could be clinical interviews or structured interviews; clinical or case study method; and ethnography or participant observation. These methods differ in the extent of control researchers impose on study conditions, and how they construct ideas about which variables to study. Every developmental investigation can be characterized in terms of whether its underlying strategy involves the \"experimental\", \"correlational\", or \"case study\" approach. The experimental method involves \"actual manipulation of various treatments, circumstances, or events to which the participant or subject is exposed; the \"experimental design\" points to cause-and-effect relationships. This method allows for strong inferences to be made of causal relationships between the manipulation of one or more independent variables and subsequent behavior, as measured by the dependent variable. The advantage of using this research method is that it permits determination of cause-and-effect relationships among variables. On the other hand, the limitation is that data obtained in an artificial environment may lack generalizability. The correlational method explores the relationship between two or more events by gathering information about these variables without researcher intervention. The advantage of using a correlational design is that it estimates the strength and direction of relationships among variables in the natural environment; however, the limitation is that it does not permit determination of cause-and-effect relationships among variables. The case study approach allows investigations to obtain an in-depth understanding of an individual participant by collecting data based on interviews, structured questionnaires, observations, and test scores. Each of these methods have its strengths and weaknesses but the experimental method when appropriate is the preferred method of developmental scientists because it provides a controlled situation and conclusions to be drawn about cause-and-effect relationships.",
            "score": 155.67169189453125
        },
        {
            "docid": "3259720_7",
            "document": "Multifactor dimensionality reduction . As illustrated above, the basic constructive induction algorithm in MDR is very simple. However, its implementation for mining patterns from real data can be computationally complex. As with any machine learning algorithm there is always concern about overfitting. That is, machine learning algorithms are good at finding patterns in completely random data. It is often difficult to determine whether a reported pattern is an important signal or just chance. One approach is to estimate the generalizability of a model to independent datasets using methods such as cross-validation. Models that describe random data typically don't generalize. Another approach is to generate many random permutations of the data to see what the data mining algorithm finds when given the chance to overfit. Permutation testing makes it possible to generate an empirical p-value for the result. Replication in independent data may also provide evidence for an MDR model but can be sensitive to difference in the data sets. These approaches have all been shown to be useful for choosing and evaluating MDR models. An important step in an machine learning exercise is interpretation. Several approaches have been used with MDR including entropy analysis and pathway analysis. Tips and approaches for using MDR to model gene-gene interactions have been reviewed.",
            "score": 154.51499938964844
        },
        {
            "docid": "236382_5",
            "document": "FishBase . The origins of FishBase go back to the 1970s, when the fisheries scientist Daniel Pauly found himself struggling to test a hypothesis on how the growing ability of fish was affected by the size of their gills. Hypotheses, such as this one, could be tested only if large amounts of empirical data were available. At the time, fisheries management used analytical models which required estimates for fish growth and mortality. It can be difficult for fishery scientists and managers to get the information they need on the species that concern them, because the relevant facts can be scattered across and buried in numerous journal articles, reports, newsletters and other sources. It can be particularly difficult for people in developing countries who need such information. Pauly believed that the only practical way fisheries managers could access the volume of data they needed was to assemble and consolidate all the data available in the published literature into some central and easily accessed repository. Such a database would be particularly useful if the data has also been standardised and validated. This would mean that when scientists or managers need to test a new hypothesis, the available data will already be there in a validated and accessible form, and there will be no need to create a new dataset and then have to validate it.",
            "score": 146.6010284423828
        },
        {
            "docid": "10022970_2",
            "document": "Research data archiving . Research data archiving is the long-term storage of scholarly research data, including the natural sciences, social sciences, and life sciences. The various academic journals have differing policies regarding how much of their data and methods researchers are required to store in a public archive, and what is actually archived varies widely between different disciplines. Similarly, the major grant-giving institutions have varying attitudes towards public archival of data. In general, the tradition of science has been for publications to contain sufficient information to allow fellow researchers to replicate and therefore test the research. In recent years this approach has become increasingly strained as research in some areas depends on large datasets which cannot easily be replicated independently.",
            "score": 145.88209533691406
        },
        {
            "docid": "1217358_49",
            "document": "Misuse of statistics . Data dredging is an abuse of data mining. In data dredging, large compilations of data are examined in order to find a correlation, without any pre-defined choice of a hypothesis to be tested. Since the required confidence interval to establish a relationship between two parameters is usually chosen to be 95% (meaning that there is a 95% chance that the relationship observed is not due to random chance), there is a thus a 5% chance of finding a correlation between any two sets of completely random variables. Given that data dredging efforts typically examine large datasets with many variables, and hence even larger numbers of pairs of variables, spurious but apparently statistically significant results are almost certain to be found by any such study.",
            "score": 144.3251495361328
        },
        {
            "docid": "9738540_12",
            "document": "Phylogenetic comparative methods . Martins and Garland proposed in 1991 that one way to account for phylogenetic relations when conducting statistical analyses was to use computer simulations to create many data sets that are consistent with the null hypothesis under test (e.g., no correlation between two traits, no difference between two ecologically defined groups of species) but that mimic evolution along the relevant phylogenetic tree. If such data sets (typically 1,000 or more) are analyzed with the same statistical procedure that is used to analyze a real data set, then results for the simulated data sets can be used to create phylogenetically correct (or \"PC\") null distributions of the test statistic (e.g., a correlation coefficient, t, F). Such simulation approaches can also be combined with such methods as phylogenetically independent contrasts or PGLS (see above).",
            "score": 142.0171661376953
        },
        {
            "docid": "2208074_11",
            "document": "Neurophilosophy . The brain regions of interest are somewhat constrained by the size of the voxels. Rs-fcMRI uses voxels that are few millimeters cubed so the brain regions will have to be defined on a larger scale. Two of the statistical methods that are commonly applied to network analysis can work on the single voxel spatial scale, but graph theory methods are extremely sensitive to the way nodes are defined. Brains regions can be divided according to their cellular architectural, according to their connectivity, or according to physiological measures. Alternatively, you could take a theory neutral approach and randomly divide the cortex into partitions of the size of your choosing. As mentioned earlier, there are several approaches to network analysis once the your brain regions have been defined. Seed based analysis begins with an a priori defined seed region and finds all of the regions that are functionally connected to that region. Wig et al. caution that the resulting network structure will not give any information concerning the inter-connectivity of the identified regions or the relations of those regions to regions other than the seed region. Another approach is to use independent component analysis to create spatio-temporal component maps and the components are sorted by components that carry information of interest and those that are caused by noise. Wigs et al. once again warns that inference of functional brain region communities is difficult under ICA. ICA also has the issue of imposing orthogonality on the data. Graph theory uses a matrix to characterize covariance between regions which is then transformed into a network map. The problem with graph theory analysis is that network mapping is heavily influenced by a priori brain region and connectivity (nodes and edges), thus the researcher is at risk for cherry picking regions and connections according to their own theories. However, graph theory analysis is extremely valuable since it is the only method that gives pair-wise relationships between nodes. ICA has the added advantage of being a fairly principled method. It seems that using both methods will be important in uncovering the network connectivity of the brain. Mumford et al. hoped to avoid these issues and use a principled approach that could determine pair-wise relationships using a statistical technique adopted from analysis of gene co-expression networks.",
            "score": 139.59535217285156
        },
        {
            "docid": "36054440_5",
            "document": "Effects range low and effects range median . NOAA originally calculated ERL/ERMs using existing toxicity data compiled from completed toxicity assays with varying endpoints, including effects on commonly tested organisms, particularly at sensitive life stages. The process is considered a \"weight of evidence approach\", in which results are based on a large database of previously conducted studies. The studies used included synoptically collected sediment chemical analyses and toxicity effects data. Using data already collected (\"data mining\") has the advantage of being able to quickly and inexpensively make an assessment with a large dataset that would otherwise require much more time-consuming and costly specific toxicity assays. Compiled data sets include a variety of endpoints including mortality, reproduction, growth rate, and juvenile survival in sediment toxicity data sets for all organisms for which tests have been conducted. Studies are screened, and only those assays using standardized methods and resulting in significant effects are used for the determination of ERL/ERM guidelines.",
            "score": 139.40020751953125
        },
        {
            "docid": "45329906_21",
            "document": "Solvent models . Quantitative Structure\u2013Activity Relationships (QSAR)/Quantitative Structure\u2013Property Relationships (QSPR), whilst unable to directly model the physical process occurring in a condensed solvent phase, can provide useful predictions of solvent and solvation properties and activities; such as the solubility of a solute. These methods come in a varied way from simple regression models to sophisticated machine learning methods. Generally, QSAR/QSPR methods require descriptors; these come in many different forms and are used to represent physical features and properties of a system of interest. Descriptors are generally single numerical values which hold some information about a physical property. A regression model or statistical learning model is then applied to find a correlation between the descriptor(s) and the property of interest. Once trained on some known data these model can be applied to similar unknown data to make predictions. Typically the known data comes from experimental measurement, although there is no reason why similar methods can not be used to correlate descriptor(s) with theoretical or predicted values. It is currently debated whether if more accurate experimental data was used to train these models whether the prediction from such models would be more accurate.",
            "score": 138.39617919921875
        },
        {
            "docid": "28353714_7",
            "document": "Alzheimer's Disease Neuroimaging Initiative . ADNI contributes data to a number of consortia and big data projects which have the potential to unlock many of the mysteries of neurological diseases. It shares imaging and genetic data with the Enhancing Neuro Imaging Genetics through Meta-Analysis (ENIGMA) consortium which uses imaging genetics to study 12 major brain diseases including schizophrenia, bipolar disease and depression. The ADNI dataset was also used as the \"test\" dataset in the Dialogue on Reverse Engineering Assessment and Methods (DREAM) Alzheimer's disease Big Data Challenge #1 for the discovery of novel predictive AD biomarkers. One measure of the success of this open data sharing approach is the number of scientific publications arising from ADNI data: currently over 1000 and a wide variety of fields including areas outside of Alzheimer's disease.",
            "score": 137.45501708984375
        },
        {
            "docid": "5354105_67",
            "document": "Hockey stick graph . In a paper published by PNAS on 9 September 2008, Mann and colleagues produced updated reconstructions of Earth surface temperature for the past two millennia. This reconstruction used a more diverse dataset that was significantly larger than the original tree-ring study, at more than 1,200 proxy records. They used two complementary methods, both of which showed a similar \"hockey stick\" graph with recent increases in northern hemisphere surface temperature are anomalous relative to at least the past 1300 years. Mann said, \"Ten years ago, the availability of data became quite sparse by the time you got back to 1,000 AD, and what we had then was weighted towards tree-ring data; but now you can go back 1,300 years without using tree-ring data at all and still get a verifiable conclusion.\" In a PNAS response, McIntyre and McKitrick said that they perceived a number of problems, including that Mann \"et al\" used some data with the axes upside down. Mann et al. replied that McIntyre and McKitrick \"raise no valid issues regarding our paper\" and the \"claim that 'upside down' data were used is bizarre\", as the methods \"are insensitive to the sign of predictors.\" They also said that excluding the contentious datasets has little effect on the result.",
            "score": 135.29541015625
        },
        {
            "docid": "371317_3",
            "document": "Quantitative psychological research . Statistics is widely used in quantitative psychological research. Typically a project begins with the collection of data based on a theory or hypothesis, followed by the application of descriptive or inferential statistical methods. Often it is necessary to collect a very large volume of data, which require validating, verifying and recording. Software packages such as SPSS and R are typically used for this purpose, and for subsequent analysis. Causal relationships are studied by manipulating factors thought to influence the phenomena of interest while controlling other variables relevant to the experimental outcomes. Researchers might measure and study the relationship between education and measurable psychological effects, whilst controlling for other key variables. Quantitatively based surveys are widely used by psychologists, and statistics such as the proportion of respondents who display one or more psychological traits reported. In such surveys, respondents are asked a set of structured questions and their responses are tabulated. The software can then perform correlation analysis or other procedures on the data. Surveys are a common example of how statistics and quantitative research are utilized to gather data.",
            "score": 135.0812530517578
        },
        {
            "docid": "21312140_5",
            "document": "Quantitative comparative linguistics . The standard method for assessing language relationships has been the comparative method. However this has a number of limitations. Not all linguistic material is suitable as input and there are issues of the linguistic levels on which the method operates. The reconstructed languages are idealized and different scholars can produce different results. Language family trees are often used in conjunction with the method and \"borrowings\" must be excluded from the data, which is difficult when borrowing is within a family. It is often claimed that the method is limited in the time depth over which it can operate. The method is difficult to apply and there is no independent test. Thus alternative methods have been sought that have a formalised method, quantify the relationships and can be tested.",
            "score": 134.61888122558594
        },
        {
            "docid": "8838284_105",
            "document": "Hockey stick controversy . In a paper published by PNAS on 9 September 2008, Mann and colleagues produced updated reconstructions of Earth surface temperature for the past two millennia. This reconstruction used a more diverse dataset that was significantly larger than the original tree-ring study, at more than 1,200 proxy records. They used two complementary methods, both of which showed a similar \"hockey stick\" graph with recent increases in northern hemisphere surface temperature are anomalous relative to at least the past 1300 years. Mann said, \"Ten years ago, the availability of data became quite sparse by the time you got back to 1,000 AD, and what we had then was weighted towards tree-ring data; but now you can go back 1,300 years without using tree-ring data at all and still get a verifiable conclusion.\" In a PNAS response, McIntyre and McKitrick said that they perceived a number of problems, including that Mann \"et al\" used some data with the axes upside down. Mann et al. replied that McIntyre and McKitrick \"raise no valid issues regarding our paper\" and the \"claim that 'upside down' data were used is bizarre\", as the methods \"are insensitive to the sign of predictors.\" They also said that excluding the contentious datasets has little effect on the result.",
            "score": 134.022705078125
        },
        {
            "docid": "968834_17",
            "document": "VO2 max . The method relies on an analysis of the linear relationship between oxygen consumption and running speed, meaning that the oxygen cost of running increases when running speed increases. To facilitate analysis and enhance accuracy, timed segments of recorded activity data are identified on the basis of heart rate ranges and reliability; and only the most reliable segments are utilized. This allows the method to be applied to freely performed running, walking and cycling activities and diminishes the need for dedicated fitness testing protocols. The calculation requires user basic anthropometric data (age, gender, height, weight, etc.), heartbeat data (internal workload), and a measure of external workload. VO max estimates provided by the Firstbeat method are most accurate during running activities that utilize GPS to capture external workload data. This combination has been validated to be 95% accurate compared to laboratory testing. Because the Firstbeat estimation method is sub-maximal in nature, accuracy of the estimate is strongly tied to validity of the HRmax value used in the calculation.",
            "score": 133.98255920410156
        },
        {
            "docid": "21017316_14",
            "document": "Fault detection and isolation . In fault detection and diagnosis, mathematical classification models which in fact belong to supervised learning methods, are trained on the training set of a labeled dataset to accurately identify the redundancies, faults and anomalous samples. During the past decades, there are different classification and preprocessing models that have been developed and proposed in this research area. \"K\"-nearest-neighbors algorithm (\"k\"NN) is one of the oldest techniques which has been used to solve fault detection and diagnosis problems. Despite the simple logic that this instance-based algorithm has, there are some problems with large dimensionality and processing time when it is used on large datasets. Since \"k\"NN is not able to automatically extract the features to overcome the curse of dimensionality, so often some data preprocessing techniques like Principal component analysis(PCA), Linear discriminant analysis(LDA) or Canonical correlation analysis(CCA) accompany it to reach a better performance. In many industrial cases, the effectiveness of \"k\"NN has been compared with other methods, specially with more complex classification models such as Support Vector Machines (SVMs), which is widely used in this field. Thanks to their appropriate nonlinear mapping using kernel methods, SVMs have an impressive performance in generalization, even with small training data. However, general SVMs do not have automatic feature extraction themselves and just like \"k\"NN, are often coupled with a data pre-processing technique. Another drawback of SVMs is that their performance is highly sensitive to the initial parameters, particularly to the kernel methods, so in each signal dataset, a parameter tuning process is required to be conducted first. Therefore, the low speed of the training phase is a limitation of SVMs when it comes to its usage in fault detection and diagnosis cases.",
            "score": 133.98040771484375
        },
        {
            "docid": "389564_11",
            "document": "Quantitative research . Statistics is the most widely used branch of mathematics in quantitative research outside of the physical sciences, and also finds applications within the physical sciences, such as in statistical mechanics. Statistical methods are used extensively within fields such as economics, social sciences and biology. Quantitative research using statistical methods starts with the collection of data, based on the hypothesis or theory. Usually a big sample of data is collected \u2013 this would require verification, validation and recording before the analysis can take place. Software packages such as SPSS and R are typically used for this purpose. Causal relationships are studied by manipulating factors thought to influence the phenomena of interest while controlling other variables relevant to the experimental outcomes. In the field of health, for example, researchers might measure and study the relationship between dietary intake and measurable physiological effects such as weight loss, controlling for other key variables such as exercise. Quantitatively based opinion surveys are widely used in the media, with statistics such as the proportion of respondents in favor of a position commonly reported. In opinion surveys, respondents are asked a set of structured questions and their responses are tabulated. In the field of climate science, researchers compile and compare statistics such as temperature or atmospheric concentrations of carbon dioxide.",
            "score": 132.3342742919922
        },
        {
            "docid": "53970843_3",
            "document": "Machine learning in bioinformatics . Prior to the emergence of machine learning algorithms, bioinformatics algorithms had to be explicitly programmed by hand which, for problems such as protein structure prediction, proves extremely difficult. Machine learning techniques such as deep learning enable the algorithm to make use of automatic feature learning which means that based on the dataset alone, the algorithm can learn how to combine multiple features of the input data into a more abstract set of features from which to conduct further learning. This multi-layered approach to learning patterns in the input data allows such systems to make quite complex predictions when trained on large datasets. In recent years, the size and number of available biological datasets have skyrocketed, enabling bioinformatics researchers to make use of these machine learning systems. Machine learning has been applied to six main subfields of bioinformatics: genomics, proteomics, microarrays, systems biology, evolution, and text mining.",
            "score": 132.31768798828125
        },
        {
            "docid": "24688832_22",
            "document": "Exploratory factor analysis . In 2012 Ruscio and Roche introduced the comparative data (CD) procedure in an attempt improve upon the PA method. The authors state that \"rather than generating random datasets, which only take into account sampling error, multiple datasets with known factorial structures are analyzed to determine which best reproduces the profile of eigenvalues for the actual data\" (p.\u00a0258). The strength of the procedure is its ability to not only incorporate sampling error, but also the factorial structure and multivariate distribution of the items. Ruscio and Roche's (2012) simulation study determined that the CD procedure outperformed many other methods aimed at determining the correct number of factors to retain. In that study, the CD technique, making use of Pearson correlations accurately predicted the correct number of factors 87.14% of the time. Although, it should be noted that the simulated study did not involve more than five factors. Therefore, the applicability of the CD procedure to estimate factorial structures beyond five factors is yet to be tested. Courtney includes this procedure in his recommended list and gives guidelines showing how it can be easily carried out from within SPSS's user interface.",
            "score": 132.1240234375
        },
        {
            "docid": "42253_18",
            "document": "Data mining . Data mining can unintentionally be misused, and can then produce results which appear to be significant; but which do not actually predict future behaviour and cannot be reproduced on a new sample of data and bear little use. Often this results from investigating too many hypotheses and not performing proper statistical hypothesis testing. A simple version of this problem in machine learning is known as overfitting, but the same problem can arise at different phases of the process and thus a train/test split - when applicable at all - may not be sufficient to prevent this from happening. The final step of knowledge discovery from data is to verify that the patterns produced by the data mining algorithms occur in the wider data set. Not all patterns found by the data mining algorithms are necessarily valid. It is common for the data mining algorithms to find patterns in the training set which are not present in the general data set. This is called overfitting. To overcome this, the evaluation uses a test set of data on which the data mining algorithm was not trained. The learned patterns are applied to this test set, and the resulting output is compared to the desired output. For example, a data mining algorithm trying to distinguish \"spam\" from \"legitimate\" emails would be trained on a training set of sample e-mails. Once trained, the learned patterns would be applied to the test set of e-mails on which it had \"not\" been trained. The accuracy of the patterns can then be measured from how many e-mails they correctly classify. A number of statistical methods may be used to evaluate the algorithm, such as ROC curves.",
            "score": 131.60946655273438
        },
        {
            "docid": "2908018_5",
            "document": "Behrens\u2013Fisher problem . Solutions to the Behrens\u2013Fisher problem have been presented that make use of either a classical or a Bayesian inference point of view and either solution would be notionally invalid judged from the other point of view. If consideration is restricted to classical statistical inference only, it is possible to seek solutions to the inference problem that are simple to apply in a practical sense, giving preference to this simplicity over any inaccuracy in the corresponding probability statements. Where exactness of the significance levels of statistical tests is required, there may be an additional requirement that the procedure should make maximum use of the statistical information in the dataset. It is well known that an exact test can be gained by randomly discarding data from the larger dataset until the sample sizes are equal, assembling data in pairs and taking differences, and then using an ordinary t-test to test for the mean-difference being zero: clearly this would not be \"optimal\" in any sense.",
            "score": 131.24386596679688
        },
        {
            "docid": "5354105_32",
            "document": "Hockey stick graph . In considering the 1998 Jones et al. reconstruction which went back a thousand years, Mann, Bradley and Hughes reviewed their own research and reexamined 24 proxy records which extended back before 1400. Mann carried out a series of statistical sensitivity tests, removing each proxy in turn to see the effect its removal had on the result. He found that certain proxies were critical to the reliability of the reconstruction, particularly one tree ring dataset collected by Gordon Jacoby and Rosanne D'Arrigo in a part of North America Bradley's earlier research had identified as a key region. This dataset only extended back to 1400, and though another proxy dataset from the same region (in the International Tree-Ring Data Bank) went further back and should have given reliable proxies for earlier periods, validation tests only supported their reconstruction after 1400. To find out why, Mann compared the two datasets and found that they tracked each other closely from 1400 to 1800, then diverged until around 1900 when they again tracked each other. He found a likely reason in the \"fertilisation effect\" affecting tree rings as identified by Graybill and Idso, with the effect ending once levels had increased to the point where warmth again became the key factor controlling tree growth at high altitude. Mann used comparisons with other tree ring data from the region to produce a corrected version of this dataset. Their reconstruction using this corrected dataset passed the validation tests for the extended period, but they were cautious about the increased uncertainties.",
            "score": 131.217529296875
        },
        {
            "docid": "36190617_6",
            "document": "Kai Li . Li also led the Scalable Display Wall project, which explores how to build and use a high-resolution, wall-size display system to visualize massive datasets. Recently, he has been working with colleagues at Stanford on a large well-labelled image dataset called ImageNet to help computer vision community develop object recognition and classification methods for large image data. More recently, he has been working with colleagues at Princeton on developing methods to efficiently manage and analyze the vast amount of data generated by increasingly sophisticated research in fields ranging from genomics to neuroscience.",
            "score": 130.94656372070312
        },
        {
            "docid": "1001976_22",
            "document": "E-Science . Traditional science is representative of two distinct philosophical traditions within the history of science, but e-Science, it is being argued, requires a paradigm shift, and the addition of a third branch of the sciences. \"The idea of open data is not a new one; indeed, when studying the history and philosophy of science, Robert Boyle is credited with stressing the concepts of skepticism, transparency, and reproducibility for independent verification in scholarly publishing in the 1660s. The scientific method later was divided into two major branches, deductive and empirical approaches. Today, a theoretical revision in the scientific method should include a new branch, Victoria Stodden advocate[s], that of the computational approach, where like the other two methods, all of the computational steps by which scientists draw conclusions are revealed. This is because within the last 20 years, people have been grappling with how to handle changes in high performance computing and simulation.\" As such, e-science aims at combining both empirical and theoretical traditions, while computer simulations can create artificial data, and real-time big data can be used to calibrate theoretical simulation models. Conceptually, e-Science revolves around developing new methods to support scientists in conducting scientific research with the aim of making new scientific discoveries by analyzing vast amounts of data accessible over the internet using vast amounts of computational resources. However, discoveries of value cannot be made simply by providing computational tools, a cyberinfrastructure or by performing a pre-defined set of steps to produce a result. Rather, there needs to be an original, creative aspect to the activity that by its nature cannot be automated. This has led to various research that attempts to define the properties that e-Science platforms should provide in order to support a new paradigm of doing science, and new rules to fulfill the requirements of preserving and making computational data results available in a manner such that they are reproducible in traceable, logical steps, as an intrinsic requirement for the maintenance of modern scientific integrity that allows an extenuation of \"Boyle's tradition in the computational age\".",
            "score": 130.60743713378906
        },
        {
            "docid": "22101888_7",
            "document": "Oversampling and undersampling in data analysis . There are a number of methods available to oversample a dataset used in a typical classification problem (using a classification algorithm to classify a set of images, given a labelled training set of images). The most common technique is known as SMOTE: Synthetic Minority Over-sampling Technique. To illustrate how this technique works consider some training data which has \"s\" samples, and \"f\" features in the feature space of the data. Note that these features, for simplicity, are continuous. As an example, consider a dataset of birds for clarification. The feature space for the minority class for which we want to oversample could be beak length, wingspan, and weight (all continuous). To then oversample, take a sample from the dataset, and consider its \"k\" nearest neighbors (in feature space). To create a synthetic data point, take the vector between one of those \"k\" neighbors, and the current data point. Multiply this vector by a random number \"x\" which lies between 0, and 1. Add this to the current data point to create the new, synthetic data point.",
            "score": 130.49301147460938
        },
        {
            "docid": "1414677_7",
            "document": "Comparative politics . The comparative method is \u2013 together with the experimental method, the statistical method and the case study approach \u2013 one of the four fundamental scientific methods which can be used to test the validity of theoretical propositions, often with the use of empirical data i.e. to establish relationships among two or more empirical variables or concepts while all other variables are held constant. In particular, the comparative method is generally used when neither the experimental nor the statistical method can be employed: on the one hand, experiments can only rarely be conducted in political science; on the other hand the statistical method implies the mathematical manipulation of quantitative data about a large number of cases, while sometimes political research must be conducted by analyzing the behavior of qualitative variables in a small number of cases.",
            "score": 130.1126708984375
        },
        {
            "docid": "226722_50",
            "document": "Functional magnetic resonance imaging . The GLM model does not take into account the contribution of relationships between multiple voxels. Whereas GLM analysis methods assess whether a voxel or region's signal amplitude is higher or lower for one condition than another, newer statistical models such as multi-voxel pattern analysis (MVPA), utilize the unique contributions of multiple voxels within a voxel-population. In a typical implementation, a classifier or more basic algorithm is trained to distinguish trials for different conditions within a subset of the data. The trained model is then tested by predicting the conditions of the remaining (independent) data. This approach is most typically achieved by training and testing on different scanner sessions or runs. If the classifier is linear, then the training model is a set of weights used to scale the value in each voxel before summing them to generate a single number that determines the condition for each testing set trial. More information on training and testing classifiers is at statistical classification.",
            "score": 130.04550170898438
        },
        {
            "docid": "596646_54",
            "document": "Recommender system . To measure the effectiveness of recommender systems, and compare different approaches, three types of evaluations are available: user studies, online evaluations (A/B tests), and offline evaluations. User studies are rather small scale. A few dozens or hundreds of users are presented recommendations created by different recommendation approaches, and then the users judge, which recommendations are best. In A/B tests, recommendations are shown to typically thousands of users of a real product, and the recommender system randomly picks at least two different recommendation approaches to generate recommendations. The effectiveness is measured with implicit measures of effectiveness such as conversion rate or click-through rate. Offline evaluations are based on historic data, e.g. a dataset that contains information about how users previously rated movies. The effectiveness of recommendation approaches is then measured based on how well a recommendation approach can predict the users' ratings in the dataset. While a rating is an explicit expression of whether a user liked a movie, such information is not available in all domains. For instance, in the domain of citation recommender systems, users typically do not rate a citation or recommended article. In such cases, offline evaluations may use implicit measures of effectiveness. For instance, it may be assumed that a recommender system is effective that is able to recommend as many articles as possible that are contained in a research article's reference list. However, this kind of offline evaluations is seen critical by many researchers. For instance, it has been shown that results of offline evaluations have low correlation with results from user studies or A/B tests. A dataset popular for offline evaluation has been shown to contain duplicate data and thus to lead to wrong conclusions in the evaluation of algorithms.",
            "score": 129.34188842773438
        },
        {
            "docid": "5936786_12",
            "document": "Familiar stranger . There have been a number of studies that have further characterized the relationship between familiar strangers using automatically generated sets of data from urban systems. Using bus usage data, it was found that a person's set of familiar strangers is highly based on routine and daily behavior. Familiar strangers come into contact typically during a particular time each day and in a particular location. Unlike other social networks that have densely connected neighborhoods, the network of familiar strangers is more diffuse and evenly distributed. This indicates that person's familiar stranger network can quickly stretch an entire metropolitan area. Wi-Fi usage data for university campuses have provided additional datasets for analyzing familiar strangers. These datasets have yielded similar results as the bus usage data, but the researchers divided relationships based on regularity of interaction and closeness of relationship.",
            "score": 129.26312255859375
        },
        {
            "docid": "49399383_3",
            "document": "SAMPL Challenge . The SAMPL challenge seeks to accelerate progress in developing quantitative, accurate drug discovery tools by providing prospective validation and rigorous comparisons for computational methodologies and force fields. Computer-aided drug design methods have been considerably improved over time, along with the rapid growth of high-performance computing capabilities. However, their applicability in the pharmaceutical industry are still highly limited, due to the insufficient accuracy. Lacking large-scale prospective validations, methods tend to suffer from over-fitting the pre-existing experimental data. To overcome this, SAMPL challenges have been organized as blind tests: each time new datasets are carefully designed and collected from academic or industrial research laboratories, and measurements are released shortly after the deadline of prediction submission. Researchers then can compare those high-quality, prospective experimental data with the submitted estimates.",
            "score": 129.2556915283203
        }
    ]
}