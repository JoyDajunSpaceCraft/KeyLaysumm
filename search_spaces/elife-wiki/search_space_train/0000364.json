{
    "q": [
        {
            "docid": "9736652_25",
            "document": "Auditory masking . When a sinusoidal signal and a sinusoidal masker (tone) are presented simultaneously the envelope of the combined stimulus fluctuates in a regular pattern described as beats. The fluctuations occur at a rate defined by the difference between the frequencies of the two sounds. If the frequency difference is small then the sound is perceived as a periodic change in the loudness of a single tone. If the beats are fast then this can be described as a sensation of roughness. When there is a large frequency separation, the two components are heard as separate tones without roughness or beats. Beats can be a cue to the presence of a signal even when the signal itself is not audible. The influence of beats can be reduced by using a narrowband noise rather than a sinusoidal tone for either signal or masker.",
            "score": 129.72570943832397
        },
        {
            "docid": "56439577_40",
            "document": "Temporal envelope and fine structure . Psychophysical studies have suggested that degraded TFS processing due to age and hearing loss may underlie some suprathreshold deficits, such as speech perception; however, debate remains about the underlying neural correlates. The strength of phase locking to the temporal fine structure of signals (TFS) in quiet listening conditions remains normal in peripheral single-neuron responses following cochlear hearing loss. Although these data suggest that the fundamental ability of auditory-nerve fibers to follow the rapid fluctuations of sound remains intact following cochlear hearing loss, deficits in phase locking strength do emerge in background noise. This finding, which is consistent with the common observation that listeners with cochlear hearing loss have more difficulty in noisy conditions, results from reduced cochlear frequency selectivity associated with outer-hair-cell dysfunction. \u00a0Although only limited effects of age and hearing loss have been observed in terms of TFS coding strength of narrowband sounds, more dramatic deficits have been observed in TFS coding quality in response to broadband sounds, which are more relevant for everyday listening. \u00a0A dramatic loss of tonotopicity can occur following noise induced hearing loss, where auditory-nerve fibers that should be responding to mid frequencies (e.g., 2\u20134\u00a0kHz) have dominant TFS responses to lower frequencies (e.g., 700\u00a0Hz). \u00a0Notably, the loss of tonotopicity generally occurs only for TFS coding but not for ENV coding, which is consistent with greater perceptual deficits in TFS processing. This tonotopic degradation is likely to have important implications for speech perception, and can account for degraded coding of vowels following noise-induced hearing loss in which most of the cochlea responds to only the first formant, eliminating the normal tonotopic representation of the second and third formants.",
            "score": 112.52410626411438
        },
        {
            "docid": "18994087_28",
            "document": "Sound . Pitch is perceived as how \"low\" or \"high\" a sound is and represents the cyclic, repetitive nature of the vibrations that make up sound. For simple sounds, pitch relates to the frequency of the slowest vibration in the sound (called the fundamental harmonic). In the case of complex sounds, pitch perception can vary. Sometimes individuals identify different pitches for the same sound, based on their personal experience of particular sound patterns. Selection of a particular pitch is determined by pre-conscious examination of vibrations, including their frequencies and the balance between them. Specific attention is given to recognising potential harmonics. Every sound is placed on a pitch continuum from low to high. For example: white noise (random noise spread evenly across all frequencies) sounds higher in pitch than pink noise (random noise spread evenly across octaves) as white noise has more high frequency content. Figure 1 shows an example of pitch recognition. During the listening process, each sound is analysed for a repeating pattern (See Figure 1: orange arrows) and the results forwarded to the auditory cortex as a single pitch of a certain height (octave) and chroma (note name).",
            "score": 133.08394408226013
        },
        {
            "docid": "14339999_6",
            "document": "Virtual pitch . Terhardt rejected the idea of periodicity pitch, because it was not consistent with empirical data on pitch perception, e.g. measurements of the gradual shift of the virtual pitch of a complex tone with a missing fundamental when the partials were gradually shifted. Terhardt instead broke pitch perception into two steps: auditory frequency analysis in the inner ear, and harmonic pitch pattern recognition in the brain. The inner ear effectively performs a running frequency analysis of incoming sounds - otherwise we would not be able to hear out spectral pitches within a complex tone. Physiologically, each spectral pitch depends on both temporal and spectral aspects (i.e. periodicity of the waveform and position of excitation on the basilar membrane), but in Terhardt's approach the spectral pitch itself is a purely experiential parameter, not a physical parameter: it is the outcome of a psychoacoustical experiment in which the conscious listener plays an active role. Psychoacoustic measurements and models can predict which partials are \"perceptually relevant\" in a given complex tone; they are perceptually relevant if you can hear a difference in the whole sound if the frequency or amplitude of a partial is changed). The ear has evolved to separate spectral frequencies, because due to reflection and superposition in everyday environments spectral frequencies are more reliably carriers of environmental information than spectral amplitudies, which in turn are more reliable carriers of environmentally relevant information than phase relationships between partials (when perceived monoaurally). On this basis, Terhardt proposed that spectral pitches - which are what the listener experiences when hearing out partials (as opposed to the physical partials themselves) - are the only information available to the brain for the purpose of extracting virtual pitches. The \"pitch extraction\" process then involves the recognition of incomplete harmonic patterns and happens in neural networks.",
            "score": 146.6441489458084
        },
        {
            "docid": "14713486_9",
            "document": "Frog hearing and communication . Dr. Feng\u2019s work applies the neuroethology of frog communication to medicine. A recent project on hearing aids is based on how female frogs find their mates. Females must recognize the male they choose by his call. By localizing where his call is coming from she can find him. An additional challenge is that she is localizing his call while listening to the many other frogs in the chorus, and to the noise of the stream and insects. The breeding pond is a very noisy place, and females must distinguish a male\u2019s calls from the other noise. How they recognize the sound pattern of the male they are pursuing from the surrounding noise is similar to how intelligent hearing aids help people hear certain sounds and cancel out others. The underlying neural mechanisms are fast neural oscillations, and synaptic inhibition to cancel out noise. The timing and frequency of the sound also play a part in frog communication and may be used in Feng\u2019s work. He also studies bat echolocation to create intelligent hearing aids. He is also working on cochlear implants.",
            "score": 105.22345781326294
        },
        {
            "docid": "8146465_5",
            "document": "Alvin Liberman . Liberman was one of the first to conduct research and experimental studies in the field of speech development and linguistics. Through his research he aimed to gain a thorough understanding of the importance and purpose of speech in the act of reading and the process of learning to read. Some of his profound investigations were made during his time at Haskins Laboratories where he worked as a research scientist trying to investigate the relationships between speech and acoustics. From his research he came up with the idea that we hear spoken words much differently than sounds. It was evident to Liberman that speech, the speed at which someone says something in particular, is connected to the word's amount of syllables, or in other terms its \"acoustic complexity\" (Whalen, 2000).  The difference in the difficulty of speech and reading exists even as alphabetic writing systems provide discrete and invariant signals and nods to vowels, sounds and consonants. When it comes to speech and conversation, it \"does not come to us as a series of individual words; we must extract the words from a stream of speech.\" Liberman and his colleagues were training the blind to read using a reading machine that would replace each letter of the alphabet with a specific sound. However, he and his colleagues found that the replacement of the sounds for each distinct letter of the alphabet did not help with the blind to learn to read or pronounce the letters fluently. After long investigations of why this was, Liberman established that speech was not as simple as an acoustic alphabet. Therefore, speech signals are very distinct from acoustic alphabet (Fowler, 2001). These investigations showed that speech perception is different from perception of other acoustic signals, and convinced Liberman that speech perception is the result of the human biological adaptations to language. Human listeners are able to decode the repetitive variable signal of running speech and to translate it into phonemic components. This is also known as the \"motor theory of speech perception\". Liberman ascribed this to the human biological disposition towards speech as opposed to reading which is not ingrained genetically.",
            "score": 100.57003021240234
        },
        {
            "docid": "2263473_17",
            "document": "Volley theory . A fundamental frequency is the lowest frequency of a harmonic. In some cases, sound can have all the frequencies of a harmonic but be missing the fundamental frequency, this is known as missing fundamental. When listening to a sound with a missing fundamental, the human brain still receives information for all frequencies, including the fundamental frequency which does not exist in the sound. This implies that sound is encoded by neurons firing at all frequencies of a harmonic, therefore, the neurons must be locked in some way to result in the hearing of one sound. Congenital deafness or sensorineural hearing loss is an often used model for the study of the inner ear regarding pitch perception and theories of hearing in general. Frequency analysis of these individuals\u2019 hearing has given insight on common deviations from normal tuning curves, excitation patterns, and frequency discrimination ranges. By applying pure or complex tones, information on pitch perception can be obtained. In 1983, it was shown that subjects with low frequency sensorineural hearing loss demonstrated abnormal psychophysical tuning curves. Changes in the spatial responses in these subjects showed similar pitch judgment abilities when compared to subjects with normal spatial responses. This was especially true regarding low frequency stimuli. These results suggest that the place theory of hearing does not explain pitch perception at low frequencies, but that the temporal (frequency) theory is more likely. This conclusion is due to the finding that when deprived of basilar membrane place information, these patients still demonstrated normal pitch perception. Computer models for pitch perception and loudness perception are often used during hearing studies on acoustically impaired subjects. The combination of this modeling and knowledge of natural hearing allows for better development of hearing aids.",
            "score": 161.76185774803162
        },
        {
            "docid": "25663206_14",
            "document": "Psychoacoustics . Suppose a listener cannot hear a given acoustical signal under silent condition. When a signal is playing while another sound is being played (a masker) the signal has to be stronger for the listener to hear it. The masker does not need to have the frequency components of the original signal for masking to happen. A masked signal can be heard even though it is weaker than the masker. Masking happens when a signal and a masker are played together. It also happens when a masker starts after a signal stops playing. The effects of backward masking is weaker than forward masking. The masking effect has been widely used in psychoacoustical research. With masking you can change the levels of the masker and measure the threshold, then create a diagram of a psychophysical tuning curve that will reveal similar features. Masking effects are also used for audio encoding. The masking effect is used in lossy encoders. It can eliminate some of the weaker sounds, so the listener can not hear the difference. This technique has been used in MP3's.",
            "score": 81.13592064380646
        },
        {
            "docid": "8953380_9",
            "document": "Auditory scene analysis . One example of this is the phenomenon of streaming, also called \"stream segregation.\" If two sounds, A and B, are rapidly alternated in time, after a few seconds the perception may seem to \"split\" so that the listener hears two rather than one stream of sound, each stream corresponding to the repetitions of one of the two sounds, for example, A-A-A-A-, etc. accompanied by B-B-B-B-, etc. The tendency towards segregation into separate streams is favored by differences in the acoustical properties of sounds A and B. Among the differences classically shown to promote segregation are those of frequency (for pure tones), fundamental frequency (for complex tones), frequency composition, source location. But it has been suggested that about any systematic perceptual difference between two sequences can elicit streaming, provided the speed of the sequence is sufficient.",
            "score": 131.4896023273468
        },
        {
            "docid": "41087200_3",
            "document": "Perceptual-based 3D sound localization . Human listeners combine information from two ears to localize and separate sound sources originating in different locations in a process called binaural hearing. The powerful signal processing methods found in the neural systems and brains of humans and other animals are flexible, environmentally adaptable, and take place rapidly and seemingly without effort. Emulating the mechanisms of binaural hearing can improve recognition accuracy and signal separation in DSP algorithms, especially in noisy environments. Furthermore, by understanding and exploiting biological mechanisms of sound localization, virtual sound scenes may be rendered with more perceptually relevant methods, allowing listeners to accurately perceive the locations of auditory events.",
            "score": 70.51076889038086
        },
        {
            "docid": "31352483_4",
            "document": "Soundscape ecology . Soundscape ecologists seek to investigate the structure of soundscapes, explain how they are generated, and study how organisms interrelate acoustically. A number of hypotheses have been proposed to explain the structure of soundscapes, particularly elements of biophony. For instance, an ecological theory known as the acoustic adaptation hypothesis predicts that acoustic signals of animals are altered in different physical environments in order to maximize their propagation through the habitat. In addition, acoustic signals from organisms may be under selective pressure to minimize their frequency (pitch) overlap with other auditory features of the environment. This acoustic niche hypothesis is analogous to the classical ecological concept of niche partitioning. It suggests that acoustic signals in the environment should display frequency partitioning as a result of selection acting to maximize the effectiveness of intraspecific communication for different species. Observations of frequency differentiation among insects, birds, and anurans support the acoustic niche hypothesis. Organisms may also partition their vocalization frequencies to avoid overlap with pervasive geophonic sounds. For example, territorial communication in some frog species takes place partially in the high frequency ultrasonic spectrum. This communication method represents an evolutionary adaptation to the frogs' riparian habitat where running water produces constant low frequency sound. Invasive species that introduce new sounds into soundscapes can disrupt acoustic niche partitioning in native communities, a process known as biophonic invasion. Although adaptation to acoustic niches may explain the frequency structure of soundscapes, spatial variation in sound is likely to be generated by environmental gradients in altitude, latitude, or habitat disturbance. These gradients may alter the relative contributions of biophony, geophony, and anthrophony to the soundscape. For example, when compared with unaltered habitats, regions with high levels of urban land-use are likely to have increased levels of anthrophony and decreased physical and organismal sound sources. Soundscapes typically exhibit temporal patterns, with daily and seasonal cycles being particularly prominent. These patterns are often generated by the communities of organisms that contribute to biophony. For example, birds chorus heavily at dawn and dusk while anurans call primarily at night; the timing of these vocalization events may have evolved to minimize temporal overlap with other elements of the soundscape.",
            "score": 113.9967713356018
        },
        {
            "docid": "29468_72",
            "document": "Speech recognition . As mentioned earlier in this article, accuracy of speech recognition may vary depending on the following factors: e.g. the 10 digits \"zero\" to \"nine\" can be recognized essentially perfectly, but vocabulary sizes of 200, 5000 or 100000 may have error rates of 3%, 7% or 45% respectively. e.g. the 26 letters of the English alphabet are difficult to discriminate because they are confusable words (most notoriously, the E-set: \"B, C, D, E, G, P, T, V, Z\"); an 8% error rate is considered good for this vocabulary. A speaker-dependent system is intended for use by a single speaker. A speaker-independent system is intended for use by any speaker (more difficult). With isolated speech, single words are used, therefore it becomes easier to recognize the speech. With discontinuous speech full sentences separated by silence are used, therefore it becomes easier to recognize the speech as well as with isolated speech.  With continuous speech naturally spoken sentences are used, therefore it becomes harder to recognize the speech, different from both isolated and discontinuous speech. e.g. Querying application may dismiss the hypothesis \"The apple is red.\"  e.g. Constraints may be semantic; rejecting \"The apple is angry.\"  e.g. Syntactic; rejecting \"Red is apple the.\"  Constraints are often represented by a grammar.  When a person reads it's usually in a context that has been previously prepared, but when a person uses spontaneous speech, it is difficult to recognize the speech because of the disfluencies (like \"uh\" and \"um\", false starts, incomplete sentences, stuttering, coughing, and laughter) and limited vocabulary.  Environmental noise (e.g. Noise in a car or a factory)  Acoustical distortions (e.g. echoes, room acoustics) Speech recognition is a multi-levelled pattern recognition task. e.g. Phonemes, Words, Phrases, and Sentences; e.g. Known word pronunciations or legal word sequences, which can compensate for errors or uncertainties at lower level; By combining decisions probabilistically at all lower levels, and making more deterministic decisions only at the highest level, speech recognition by a machine is a process broken into several phases. Computationally, it is a problem in which a sound pattern has to be recognized or classified into a category that represents a meaning to a human. Every acoustic signal can be broken in smaller more basic sub-signals. As the more complex sound signal is broken into the smaller sub-sounds, different levels are created, where at the top level we have complex sounds, which are made of simpler sounds on lower level, and going to lower levels even more, we create more basic and shorter and simpler sounds. The lowest level, where the sounds are the most fundamental, a machine would check for simple and more probabilistic rules of what sound should represent. Once these sounds are put together into more complex sound on upper level, a new set of more deterministic rules should predict what new complex sound should represent. The most upper level of a deterministic rule should figure out the meaning of complex expressions. In order to expand our knowledge about speech recognition we need to take into a consideration neural networks. There are four steps of neural network approaches:  For telephone speech the sampling rate is 8000 samples per second;  computed every 10\u00a0ms, with one 10\u00a0ms section called a frame;",
            "score": 105.37307262420654
        },
        {
            "docid": "5366050_41",
            "document": "Speech perception . Cochlear implantation restores access to the acoustic signal in individuals with sensorineural hearing loss. The acoustic information conveyed by an implant is usually sufficient for implant users to properly recognize speech of people they know even without visual clues. For cochlear implant users, it is more difficult to understand unknown speakers and sounds. The perceptual abilities of children that received an implant after the age of two are significantly better than of those who were implanted in adulthood. A number of factors have been shown to influence perceptual performance, specifically: duration of deafness prior to implantation, age of onset of deafness, age at implantation (such age effects may be related to the Critical period hypothesis) and the duration of using an implant. There are differences between children with congenital and acquired deafness. Postlingually deaf children have better results than the prelingually deaf and adapt to a cochlear implant faster. In both children with cochlear implants and normal hearing, vowels and voice onset time becomes prevalent in development before the ability to discriminate the place of articulation. Several months following implantation, children with cochlear implants can normalize speech perception. One of the basic problems in the study of speech is how to deal with the noise in the speech signal. This is shown by the difficulty that computer speech recognition systems have with recognizing human speech. These programs can do well at recognizing speech when they have been trained on a specific speaker's voice, and under quiet conditions. However, these systems often do poorly in more realistic listening situations where humans can understand speech without difficulty.",
            "score": 83.67924582958221
        },
        {
            "docid": "24034128_2",
            "document": "Assistive listening device . An Assistive listening device (ALD) is used to improve hearing ability for people in a variety of situations where they are unable to distinguish speech in noise. Often in a noisy or crowded room it is almost impossible for an individual who is hard of hearing to distinguish one voice among many. The hard of hearing listener has to distinguish between background noise, noise between them and the speaker and then there will be the effect of room acoustics on the quality of sound reaching their ears. Hearing aids are able to amplify and process these sounds and improve the speech to noise ratio but if the sound is too distorted by the time it reaches the listener even the best hearing aids will struggle to unscramble the signal.",
            "score": 111.89575576782227
        },
        {
            "docid": "4296904_5",
            "document": "Audio noise measurement . Attempts to measure noise in audio equipment as RMS voltage, using a simple level meter or voltmeter, do not produce useful results; a special noise-measuring instrument is required. This is because noise contains energy spread over a wide range of frequencies and levels, and different sources of noise have different spectral content. For measurements to allow fair comparison of different systems they must be made using a measuring instrument that responds in a way that corresponds to how we hear sounds. From this, three requirements follow. Firstly, it is important that frequencies above or below those that can be heard by even the best ears are filtered out and ignored by bandwidth limiting (usually 22\u00a0Hz to 22\u00a0kHz). Secondly, the measuring instrument should give varying emphasis to different frequency components of the noise in the same way that our ears do, a process referred to as \u2018weighting\u2019. Thirdly, the rectifier or detector that is used to convert the varying alternating noise signal into a steady positive representation of level should take time to respond fully to brief peaks to the same extent that our ears do; it should have the correct \u2018dynamics\u2019.",
            "score": 118.45069324970245
        },
        {
            "docid": "1021754_40",
            "document": "Sound localization . This kind of sound localization technique provides us the real virtual stereo system. It utilizes \"smart\" manikins, such as KEMAR, to glean signals or use DSP methods to simulate the transmission process from sources to ears. After amplifying, recording and transmitting, the two channels of received signals will be reproduced through earphones or speakers. This localization approach uses electroacoustic methods to obtain the spatial information of the original sound field by transferring the listener's auditory apparatus to the original sound field. The most considerable advantages of it would be that its acoustic images are lively and natural. Also, it only needs two independent transmitted signal to reproduce the acoustic image of a 3D system. The representatives of this kind of system are SRS Audio Sandbox, Spatializer Audio Lab and Qsound Qxpander. They use HRTF to simulate the received acoustic signals at the ears from different directions with common binary-channel stereo reproduction. Therefore, they can simulate reflected sound waves and improve subjective sense of space and envelopment. Since they are para-virtualization stereo systems, the major goal of them is to simulate stereo sound information. Traditional stereo systems use sensors that are quite different from human ears. Although those sensors can receive the acoustic information from different directions, they do not have the same frequency response of human auditory system. Therefore, when binary-channel mode is applied, human auditory systems still cannot feel the 3D sound effect field. However, the 3D para-virtualization stereo system overcome such disadvantages. It uses HRTF principles to glean acoustic information from the original sound field then produce a lively 3D sound field through common earphones or speakers.",
            "score": 94.5595029592514
        },
        {
            "docid": "8953842_15",
            "document": "Computational auditory scene analysis . While many models consider the audio signal as a complex combination of different frequencies, modeling the auditory system can also require consideration for the neural components. By taking a holistic process, where a stream (of feature-based sounds) correspond to neuronal activity distributed in many brain areas, the perception of the sound could be mapped and modeled. Two different solutions have been proposed to the binding of the audio perception and the area in the brain. Hierarchical coding models many cells to encode all possible combinations of features and objects in the auditory scene. Temporal or oscillatory correlation addressing the binding problem by focusing on the synchrony and desynchrony between neural oscillations to encode the state of binding among the auditory features. These two solutions are very similar to the debacle between place coding and temporal coding. While drawing from modeling neural components, another phenomenon of ASA comes into play with CASA systems: the extent of modeling neural mechanisms. The studies of CASA systems have involved modeling some known mechanisms, such as the bandpass nature of cochlear filtering and random auditory nerve firing patterns, however, these models may not lead to finding new mechanisms, but rather give an understanding of purpose to the known mechanisms.",
            "score": 112.00847685337067
        },
        {
            "docid": "2218_5",
            "document": "Additive synthesis . The sounds that are heard in everyday life are not characterized by a single frequency. Instead, they consist of a sum of pure sine frequencies, each one at a different amplitude. When humans hear these frequencies simultaneously, we can recognize the sound. This is true for both \"non-musical\" sounds (e.g. water splashing, leaves rustling, etc.) and for \"musical sounds\" (e.g. a piano note, a bird's tweet, etc.). This set of parameters (frequencies, their relative amplitudes, and how the relative amplitudes change over time) are encapsulated by the \"timbre\" of the sound. Fourier analysis is the technique that is used to determine these exact timbre parameters from an overall sound signal; conversely, the resulting set of frequencies and amplitudes is called the Fourier series of the original sound signal.",
            "score": 114.85228705406189
        },
        {
            "docid": "4778800_27",
            "document": "Gunfire locator . Another method of classifying gunfire uses \"temporal pattern recognition,\" as referred by its developer, that employs artificial neural networks that are trained and then listen for a sound signature in acoustic events. Like other acoustic sensing systems they are fundamentally based on the physics of acoustics, but they analyze the physical acoustic data using a neural network. Information in the network is coded in terms of variation in the sequence of all-or-none (spike) events, or temporal patterns, transmitted between artificial \"neurons\". Identifying the nonlinear input/output properties of neurons involved in forming memories for new patterns and developing mathematical models of those nonlinear properties enable the identification of specific types of sounds. These neural networks can then be trained as \"recognizers\" of a target sound, like a gunshot, even in the presence of high noise.",
            "score": 75.333904504776
        },
        {
            "docid": "27424107_2",
            "document": "Sensear . The Sensear earmuffs and earplugs developed by Sensear allow people to talk face to face and use mobile phone two-way radio communication in high noise (above 85\u00a0dB(A)) environments while providing hearing protection. Sensear uses a patented software system called SENS that actively \u2018listens\u2019 and seeks out human voices and actively suppresses background noise. Each Sensear unit is the size of a mobile phone and uses a series of directional microphones that process sound from all directions. A complex frequency algorithm determines which direction speech is coming from. Where speech is detected, ambient noise from other directions is suppressed while the speaker's voice is enhanced. The processed sound is delivered in stereo to the wearer. The units have a Bluetooth capability so users can talk on the phone without removing their protection.",
            "score": 116.67275369167328
        },
        {
            "docid": "6894544_2",
            "document": "Noise-induced hearing loss . Noise-induced hearing loss (NIHL) is hearing impairment resulting from exposure to loud sound. People may have a loss of perception of a narrow range of frequencies, impaired cognitive perception of sound including sensitivity to sound or ringing in the ears. When exposure to hazards such as noise occur at work and is associated with hearing loss, it is referred to as occupational hearing loss.  Hearing may deteriorate gradually from chronic and repeated noise exposure, such as to loud music or background noise, or suddenly, from exposure to impulse noise (a short high intensity noise), such as a gunshot or airhorn. In both types, loud sound overstimulates delicate hearing cells, leading to the permanent injury or death of the cells. Once lost this way, hearing cannot be restored in humans.  There are a variety of prevention strategies available to avoid or reduce hearing loss. Lowering the volume of sound at its source, limiting the time of exposure and physical protection can reduce the impact of excessive noise. If not prevented, hearing loss can be managed through assistive devices and cognitive therapies. The largest burden of NIHL has been through occupational exposures; however, noise-induced hearing loss can also be due to unsafe recreational, residential, social and military service-related noise exposures. It is estimated that 15% of young people are exposed to sufficient leisure noises (i.e. concerts, sporting events, daily activities, personal listening devices, etc.) to cause NIHL. There is not a limited list of noise sources that can cause hearing loss; rather, it is important to understand that exposure to excessively high decibel (dB) levels from any sound source over time, can cause hearing loss. The first symptom of NIHL may be difficulty hearing a conversation against a noisy background. The effect of hearing loss on speech perception has two components. The first component is the loss of audibility, which may be perceived as an overall decrease in volume. Modern hearing aids compensate this loss with amplification. The second component is known as \u201cdistortion\" or \u201cclarity loss\u201d due to selective frequency loss.\u201d Consonants, due to their higher frequency, are typically affected first. For example, the sounds \u201cs\u201d and \u201ct\u201d are often difficult to hear for those with hearing loss, affecting clarity of speech. NIHL can affect either one or both ears. Monaural hearing loss causes problems with directional hearing, affecting the ability to localize sound.",
            "score": 120.71511816978455
        },
        {
            "docid": "18839_27",
            "document": "Music . Pitch is an aspect of a sound that we can hear, reflecting whether one musical sound, note or tone is \"higher\" or \"lower\" than another musical sound, note or tone. We can talk about the highness or lowness of pitch in the more general sense, such as the way a listener hears a piercingly high piccolo note or whistling tone as higher in pitch than a deep thump of a bass drum. We also talk about pitch in the precise sense associated with musical melodies, basslines and chords. Precise pitch can only be determined in sounds that have a frequency that is clear and stable enough to distinguish from noise. For example, it is much easier for listeners to discern the pitch of a single note played on a piano than to try to discern the pitch of a crash cymbal that is struck. A melody (also called a \"tune\") is a series of pitches (notes) sounding in succession (one after the other), often in a rising and falling pattern. The notes of a melody are typically created using pitch systems such as scales or modes. Melodies also often contain notes from the chords used in the song. The melodies in simple folk songs and traditional songs may use only the notes of a single scale, the scale associated with the tonic note or key of a given song. For example, a folk song in the key of C (also referred to as C major) may have a melody that uses only the notes of the C major scale (the individual notes C, D, E, F, G, A, B and C; these are the \"white notes\" on a piano keyboard. On the other hand, Bebop-era jazz from the 1940s and contemporary music from the 20th and 21st centuries may use melodies with many chromatic notes (i.e., notes in addition to the notes of the major scale; on a piano, a chromatic scale would include all the notes on the keyboard, including the \"white notes\" and \"black notes\" and unusual scales, such as the whole tone scale (a whole tone scale in the key of C would contain the notes C, D, E, F, G and A). A low, deep musical line played by bass instruments such as double bass, electric bass or tuba is called a bassline.",
            "score": 104.5793467760086
        },
        {
            "docid": "44673348_4",
            "document": "Ernst Terhardt . According to Terhardt\u2019s theory of pitch perception, pitch perception can be divided into two separate stages: auditory spectral analysis and harmonic pitch pattern recognition. In the first stage, the inner ear (cochlea and basilar membrane) performs a running spectral analysis of the incoming signal. The parameters of this analysis (e.g. the effective length and shape of the analysis window) depend directly on physiology and indirectly on the co-evolution of ear and voice as our human and prehuman ancestors interacted with their social and physical environments. The output of this first stage is called a spectral pitch pattern, when it is determined by psychoacoustic experiments in which listeners make subjective judgments, matching the perceived pitch of a pure reference tone to that of a successively presented complex tone. The spectral pitches differ in perceptual salience since their sound pressure levels differ physically, they lie at different distances above the threshold of hearing, they mask each other (and therefore lie at different distances above the masked threshold), and may or may not lie in a region to which the ear is particularly sensitive (a dominance region of pitch perception). A cornerstone of Terhardt\u2019s is approach is the idea that because spectral pitches are subjective, we must not jump to conclusions about the relationship between them and their physiological (physical) foundations in the ear and brain.",
            "score": 92.20175421237946
        },
        {
            "docid": "569650_21",
            "document": "Stimulus modality . Aside from pitch and loudness, another quality that distinguishes sound stimuli is timbre. Timbre allows us to hear the difference between two instruments that are playing at the same frequency and loudness, for example. When two simple tones are put together they create a complex tone. The simple tones of an instrument are called harmonics or overtones. Timbre is created by putting the harmonics together with the fundamental frequency (a sound's basic pitch). When a complex sound is heard, it causes different parts in the basilar membrane to become simultaneously stimulated and flex. In this way, we are able to distinguish different timbres.",
            "score": 160.2028946876526
        },
        {
            "docid": "26063832_3",
            "document": "Partial concurrent thinking aloud . In general, in the usability evaluation both retrospective and concurrent TAP could be used according to the aims and goals of the study. Nevertheless, when a usability evaluation is carried out with blind people several studies propose to use the retrospective TAP: indeed, using a screen reader and talking about the way of interacting with the computer implies a structural interference between action and verbalization. Undoubtedly, cognitive studies provided a lot of evidence supporting the idea that individuals can listen, verbalize, or manipulate, and rescue information in multiple task condition. As Colin Cherry showed, subjects, when listening to two different messages from a single loudspeaker, can separate sounds from background noise, recognize the gender of the speaker, the direction, and the pitch (cocktail party effect). At the same time, subjects that must verbalize the content of a message (attended message) listening to two different message simultaneously (attended and unattended message) have a reduced ability to report the content of the attended massage, while they are unable to report the content of the unattended message. Moreover, K. Anders Ericsson and Walter Kintsch showed that, in a multiple task condition, subjects' ability of rescuing information is not compromised by an interruption of the action flow (as it happens in the concurrent thinking aloud technique), thanks to the \u201cLong Term Working Memory mechanism\u201d of information retrieval (Working Memory section Ericsson and Kintsch).  Even if users can listen, recognize, and verbalize multiple messages in a multiple task condition and they can stop and restart actions without losing any information, other cognitive studies underlined that the overlap of activities in a multiple task condition have an effect on the goal achievement: Kemper, Herman and Lian, analysing the users' abilities to verbalize actions in a multiple task condition, showed that the fluency of a user's conversation is influenced by the overlap of actions. Adults are likely to continue to talk as they navigate in a complex physical environment. However, the fluency of their conversation is likely to change: Older adults are likely to speak more slowly than they would if resting; Young adults continue to speak just as rapidly while walking as while resting, but they adopt a further set of speech accommodations, reducing sentence length, grammatical complexity, and propositional density. Just by reducing length, complexity, and propositional density adults free up working memory resources. We do not know how and how much the content of verbalizations could be influenced by the strategy of verbalization (i.e. the modification of fluency and the complexity in a multiple task condition). Anyway, we well know that users in the concurrent thinking aloud verbalize the problems in a more accurate and pertinent way (i.e. more focused on the problems directly perceived during the interaction) then in the retrospective one. The pertinence is granted to the user by the proximity of action-verbalization-next action; this multiple task proximity compels the subject to apply a strategy of verbalization that reduce the overload of the working memory. However, for blind users this time proximity between action and verbalization is lost: the use of the screen reader, in fact, increase the time for verbalization (i.e. in order to verbalize, blind users must first stop the [screen reader] and then restart it).",
            "score": 114.02716732025146
        },
        {
            "docid": "7330954_28",
            "document": "Pattern recognition (psychology) . Music provides deep and emotional experiences for the listener. These experiences become contents in long-term memory, and every time we hear the same tunes, those contents are activated. Recognizing the content by the pattern of the music affects our emotion. The mechanism that forms the pattern recognition of music and the experience has been studied by multiple researchers. The sensation felt when listening to our favorite music is evident by the dilation of the pupils, the increase in pulse and blood pressure, the streaming of blood to the leg muscles, and the activation of the cerebellum, the brain region associated with physical movement.  While retrieving the memory of a tune demonstrates general recognition of musical pattern, pattern recognition also occurs while listening to a tune for the first time. The recurring nature of the metre allows the listener to follow a tune, recognize the metre, expect its upcoming occurrence, and figure the rhythm. The excitement of following a familiar music pattern happens when the pattern breaks and becomes unpredictable. This following and breaking of a pattern creates a problem-solving opportunity for the mind that form the experience. Psychologist Daniel Levitin argues that the repetitions, melodic nature and organization of this music create meaning for the brain. The brain stores information in an arrangement of neurons which retrieve the same information when activated by the environment. By constantly referencing information and additional stimulation from the environment, the brain constructs musical features into a perceptual whole.",
            "score": 72.42125034332275
        },
        {
            "docid": "32454456_7",
            "document": "Frequency following response . Currently, there is renewed interest in using the FFR to evaluate: the role of neural phase-locking in encoding of complex sounds in normally hearing and hearing impaired subjects, encoding of voice pitch, binaural hearing, and evaluating the characteristics of the neural version of cochlear nonlinearity.. Furthermore, it is demonstrated that the temporal pattern of phase-locked brainstem neural activity generating the FFR may contain information relevant to the binaural processes underlying spatial release from masking (SRM) in challenging listening environments.",
            "score": 85.46133589744568
        },
        {
            "docid": "2485942_5",
            "document": "Audiogram . Hearing thresholds of humans and other mammals can be found by using behavioural hearing tests or physiological tests. An audiogram can be obtained using a behavioural hearing test called Audiometry. For humans the test involves different tones being presented at a specific frequency (pitch) and intensity (loudness). When the person hears the sound they raise their hand or press a button so that the tester knows that they have heard it. The lowest intensity sound they can hear is recorded. The test varies for children, their response to the sound can be a head turn or using a toy. The child learns what they can do when they hear the sound, for example they are taught that when they heard the sound they can put a toy man in a boat. This is referred to as conditioned play audiometry. Visual reinforcement audiometry is also used with children. When the child hears the sound, they look in the direction the sound came from and are reinforced with a light and/or animated toy. A similar technique can be used when testing some animals but instead of a toy, food can be used as a reward for responding to the sound.  Physiological tests do not need the patient to respond (Katz 2002). For example when performing the brainstem auditory evoked potentials the patient\u2019s brainstem responses are being measured when a sound is played into their ear, or otoacoustic emissions (OAEs) which are generated by a healthy inner ear either spontaneously or evoked by an outside stimulus.  In the US, the NIOSH recommends that people who are regularly exposed to hazardous noise have their hearing tested once a year, or every three years otherwise.",
            "score": 98.68298876285553
        },
        {
            "docid": "8953842_2",
            "document": "Computational auditory scene analysis . Computational auditory scene analysis (CASA) is the study of auditory scene analysis by computational means. In essence, CASA systems are \"machine listening\" systems that aim to separate mixtures of sound sources in the same way that human listeners do. CASA differs from the field of blind signal separation in that it is (at least to some extent) based on the mechanisms of the human auditory system, and thus uses no more than two microphone recordings of an acoustic environment. It is related to the cocktail party problem.",
            "score": 56.08164858818054
        },
        {
            "docid": "156859_72",
            "document": "Comparison of analog and digital recording . It is also worth noting two issues that affect perception of sound playback. The first is human ear dynamic range which for practical and hearing safety reasons might be regarded as 120 decibels, from barely audible sound received by the ear situated within an otherwise silent environment, to the threshold of pain or onset of damage to the ear's delicate mechanism. The other critical issue is manifestly more complex; the presence and nature of background noise in any listening environment. Background noise subtracts useful hearing dynamic range, in any number of ways that depend on the nature of the noise from the listening environment: noise spectral content, noise coherence or periodicity, angular aspects such as localization of noise sources with respect to localization of playback system sources and so on.",
            "score": 87.24490487575531
        },
        {
            "docid": "292200_5",
            "document": "Missing fundamental . It was once thought that this effect was because the missing fundamental was replaced by distortions introduced by the physics of the ear. However, experiments subsequently showed that when a noise was added that would have masked these distortions had they been present, listeners still heard a pitch corresponding to the missing fundamental, as reported by J. C. R. Licklider in 1954. It is now widely accepted that the brain processes the information present in the overtones to calculate the fundamental frequency. The precise way in which it does so is still a matter of debate, but the processing seems to be based on an autocorrelation involving the timing of neural impulses in the auditory nerve. However, it has long been noted that any neural mechanisms which may accomplish a delay (a necessary operation of a true autocorrelation) have not been found. At least one model shows a temporal delay to be unnecessary to produce an autocorrelation model of pitch perception, appealing to phase shifts between cochlear filters; however, earlier work has shown that certain sounds with a prominent peak in their autocorrelation function do not elicit a corresponding pitch percept, and that certain sounds without a peak in their autocorrelation function nevertheless elicit a pitch. Autocorrelation can thus be considered, at best, an incomplete model.",
            "score": 92.61912870407104
        },
        {
            "docid": "54542174_3",
            "document": "Rain (The Script song) . The Script described the track as a \"feel-good summer tune\". \"After a very long process of making 'Album 5', the song 'Rain' came right at the end. It's a summer song so we thought, only The Script can make it 'Rain in Summer'\" the band stated. In an interview with \"Metro\", the band regarded the single as a progression. \"I think we have afforded ourselves a little bit of leeway. The past four records have been not the same sound but we've been progressing at a slow rate. It's been two years since we had something out so there's two years worth of progression in our music. I'm sure to a lot of people it might sound quite drastic at first but if they heard the 60 songs we've put out you'd hear a slower progression.\" When asked if their change in sound was deliberate, they said:\"When we started this record we wanted to do something where people would hear a song and go 'oh I love that song!' and then look it up and find it's us. We wanted to change our sound a push our sound a bit. It's either adapt, change or die in this industry. It's very difficult in this industry. We wanted to revamp and reboot the sound and at the end of the day we're still the same songwriters and that's never going to go away.\"The band said that they always \"want something different in our lives\" and \"hoping our songs penetrate different markets and gain new fans\", They referred themselves as an ambitious band. \"With Rain we just decided we needed something a bit lighter, you can't just walk into a party of people and start with a heavy topic. It's nice to have a bit of escapism that people can bob their head to and not be so serious about but when people peel the layers of the song they realise that lyrically we've gone deep but on the surface we wanted people to have a bit of fun.\"",
            "score": 68.3171489238739
        }
    ],
    "r": [
        {
            "docid": "2263473_17",
            "document": "Volley theory . A fundamental frequency is the lowest frequency of a harmonic. In some cases, sound can have all the frequencies of a harmonic but be missing the fundamental frequency, this is known as missing fundamental. When listening to a sound with a missing fundamental, the human brain still receives information for all frequencies, including the fundamental frequency which does not exist in the sound. This implies that sound is encoded by neurons firing at all frequencies of a harmonic, therefore, the neurons must be locked in some way to result in the hearing of one sound. Congenital deafness or sensorineural hearing loss is an often used model for the study of the inner ear regarding pitch perception and theories of hearing in general. Frequency analysis of these individuals\u2019 hearing has given insight on common deviations from normal tuning curves, excitation patterns, and frequency discrimination ranges. By applying pure or complex tones, information on pitch perception can be obtained. In 1983, it was shown that subjects with low frequency sensorineural hearing loss demonstrated abnormal psychophysical tuning curves. Changes in the spatial responses in these subjects showed similar pitch judgment abilities when compared to subjects with normal spatial responses. This was especially true regarding low frequency stimuli. These results suggest that the place theory of hearing does not explain pitch perception at low frequencies, but that the temporal (frequency) theory is more likely. This conclusion is due to the finding that when deprived of basilar membrane place information, these patients still demonstrated normal pitch perception. Computer models for pitch perception and loudness perception are often used during hearing studies on acoustically impaired subjects. The combination of this modeling and knowledge of natural hearing allows for better development of hearing aids.",
            "score": 161.76185607910156
        },
        {
            "docid": "569650_21",
            "document": "Stimulus modality . Aside from pitch and loudness, another quality that distinguishes sound stimuli is timbre. Timbre allows us to hear the difference between two instruments that are playing at the same frequency and loudness, for example. When two simple tones are put together they create a complex tone. The simple tones of an instrument are called harmonics or overtones. Timbre is created by putting the harmonics together with the fundamental frequency (a sound's basic pitch). When a complex sound is heard, it causes different parts in the basilar membrane to become simultaneously stimulated and flex. In this way, we are able to distinguish different timbres.",
            "score": 160.20289611816406
        },
        {
            "docid": "34118956_14",
            "document": "Perception of infrasound . In experiments using heart-rate conditioning, Pigeons have been found to be able to detect sounds in the infrasonic range at frequencies as low as 0.5\u00a0Hz. For frequencies below 10\u00a0Hz, the pigeon threshold is at about 55\u00a0dB which is at least 50\u00a0dB more sensitive than humans. Pigeons are able to discriminate small frequency differences in sounds at between 1\u00a0Hz and 20\u00a0Hz, with sensitivity ranging from a 1% shift at 20\u00a0Hz to a 7% shift at 1\u00a0Hz. Sensitivities are measured through a heart-rate conditioning test. In this test, an anesthetized bird is presented with a single sound or a sequence of sounds, followed by an electric shock. The bird\u2019s heart-rate will increase in anticipation of a shock. Therefore, a measure of the heart-rate can determine whether the bird is able to distinguish between stimuli that would be followed by a shock from stimuli that would not. Similar methods have also been used to determine the pigeon\u2019s sensitivity to barometric pressure changes, polarized light, and UV light. These experiments were conducted in sound isolation chambers to avoid the influence of ambient noise. Infrasonic stimuli are hard to produce and are often transmitted through a filter that attenuates higher frequency components. Also, the tone burst stimuli used in these experiments were presented with stimulus onset and offsets ramped on and off gradually in order to prevent initial turn-on and turn-off transients.",
            "score": 149.6197967529297
        },
        {
            "docid": "17523336_22",
            "document": "Olivocochlear system . Although Scharf et al.\u2019s (1993, 1994, 1997) experiments failed to produce any clear differences in the basic psychophysical characteristics of hearing (other than the detection of unexpected sounds), many other studies using both animals and humans have implicated the OCB in listening-in-noise tasks using more complex stimuli. In constant BGN, rhesus monkeys with intact OCBs have been observed to perform better in vowel discrimination tasks than those without (Dewson, 1968). In cats, an intact OCB is associated with better vowel identification (Heinz et al., 1998), sound localisation (May et al., 2004), and intensity discrimination (May and McQuone, 1995). All of these studies were performed in constant BGN. In humans, speech-in-noise discrimination measurements have been performed on individuals who had undergone unilateral vestibular neurectomy (resulting in OCB sectioning). Giraud et al. (1997) observed a small advantage in the healthy ear over the operated ear for phoneme recognition and speech intelligibility in BGN. Scharf et al. (1988) had previously investigated the role of auditory attention during speech perception, and suggested that speech-in-noise discrimination is assisted by attentional focus on frequency regions. In 2000, Zeng et al., reported that vestibular neurectomy did not directly affect pure-tone thresholds or intensity discrimination, confirming earlier findings of Scharf et al. 1994; 1997. For the listening-in-noise tasks, they observed a number of discrepancies between the healthy and operated ear. Consistent with the earlier findings of May and McQuone (1995), intensity discrimination in noise was observed to be slightly worse in the ear without olivocochlear bundle (OCB) input. However, Zeng et al.\u2019s main finding related to the \u201covershoot\u201d effect, which was found to be significantly reduced (~50%) in the operated ears. This effect was first observed by Zwicker (1965), and was characterised as an increased detection threshold of a tone when it is presented at the onset of the noise compared to when it is presented in constant, steady-state noise. Zeng et al. proposed that this finding is consistent with MOCS-evoked antimasking; that is, MOCS-evoked antimasking being absent at the onset of noise however becoming active during steady-state noise. This theory was supported by the time course of MOC activation; being similar to the time course of the overshoot effect (Zwicker, 1965), as well as the overshoot effect being disrupted in subjects with sensorineural hearing loss, for whom the MOCS would be most likely ineffectual (Bacon and Takahashi, 1992).",
            "score": 148.06201171875
        },
        {
            "docid": "14339999_6",
            "document": "Virtual pitch . Terhardt rejected the idea of periodicity pitch, because it was not consistent with empirical data on pitch perception, e.g. measurements of the gradual shift of the virtual pitch of a complex tone with a missing fundamental when the partials were gradually shifted. Terhardt instead broke pitch perception into two steps: auditory frequency analysis in the inner ear, and harmonic pitch pattern recognition in the brain. The inner ear effectively performs a running frequency analysis of incoming sounds - otherwise we would not be able to hear out spectral pitches within a complex tone. Physiologically, each spectral pitch depends on both temporal and spectral aspects (i.e. periodicity of the waveform and position of excitation on the basilar membrane), but in Terhardt's approach the spectral pitch itself is a purely experiential parameter, not a physical parameter: it is the outcome of a psychoacoustical experiment in which the conscious listener plays an active role. Psychoacoustic measurements and models can predict which partials are \"perceptually relevant\" in a given complex tone; they are perceptually relevant if you can hear a difference in the whole sound if the frequency or amplitude of a partial is changed). The ear has evolved to separate spectral frequencies, because due to reflection and superposition in everyday environments spectral frequencies are more reliably carriers of environmental information than spectral amplitudies, which in turn are more reliable carriers of environmentally relevant information than phase relationships between partials (when perceived monoaurally). On this basis, Terhardt proposed that spectral pitches - which are what the listener experiences when hearing out partials (as opposed to the physical partials themselves) - are the only information available to the brain for the purpose of extracting virtual pitches. The \"pitch extraction\" process then involves the recognition of incomplete harmonic patterns and happens in neural networks.",
            "score": 146.64414978027344
        },
        {
            "docid": "17523336_20",
            "document": "Olivocochlear system . In humans, psychophysical experiments conducted in constant BGN have also implicated the olivocochlear bundle (OCB) in selective listening. The research perhaps most relevant to this thesis has been performed by Scharf and his colleagues. In 1993, Scharf et al. presented data from eight patients who had undergone unilateral vestibular neurectomy to treat M\u00e9ni\u00e8re\u2019s disease, a procedure which severs the OCB (presumably both the MOCS and the LOCS). Scharf et al. (1993) did not find any clear differences in subjects\u2019 thresholds to tones in noise before and after surgery. Shortly after this finding, Scharf et al. (1994, 1997) performed a comprehensive set of psychophysical experiments from a total of sixteen patients who had undergone unilateral vestibular neurectomy (including the original eight subjects). They measured performance in the psychophysical listening tasks before and after surgery, and found no significant difference in performance for (i) detection of tones, (ii) intensity discrimination of tones, (iii) frequency discrimination of tones, (iv) loudness adaptation, and (v) detection of tones in notched-noise. Their only positive finding was that most patients detected unexpected sounds in the operated ear better than in the healthy ear, or the same ear before surgery. This result was obtained using a truncated probe-signal procedure which led the patient to expect a certain frequency on each trial. Twelve subjects completed this experiment. Their procedure was similar to that of Greenberg and Larkin (1968), except only 50% of trials (not 77%) contained a target whose frequency matched that of the auditory cue. The other 50% of trials containing a probe whose frequency differed from that of the cue. Also, only two probe frequencies were used, one whose frequency was higher than the target, and one whose frequency was lower than the target. All trials contained an auditory cue (at the target frequency) prior to the first observation interval. The results were used to construct a basic attentional filter, which displayed detection level of the expected (and cued) target frequency and the two unexpected probe frequencies. From the two published reports (Scharf et al., 1994, 1997), ears for which the OCB has been lesioned showed an attentional filter with an average depth of about 15%-correct less than those ears for which the OCB was intact. Although there is no way to empirically convert this value to dB, a rough estimate based on psychometric functions presented by Green and Swets (1966) yields a value of 2-3\u00a0dB. Their results have been summarised in the inset figure.",
            "score": 146.28599548339844
        },
        {
            "docid": "4548229_5",
            "document": "Interaural time difference . The duplex theory states that ITDs are used to localise low frequency sounds, in particular, while ILDs are used in the localisation of high frequency sound inputs. However, the frequency ranges for which the auditory system can use ITDs and ILDs significantly overlap, and most natural sounds will have both high and low frequency components, so that the auditory system will in most cases have to combine information from both ITDs and ILDs to judge the location of a sound source.  A consequence of this duplex system is that it is also possible to generate so-called \"cue trading\" or \"time\u2013intensity trading\" stimuli on headphones, where ITDs pointing to the left are offset by ILDs pointing to the right, so the sound is perceived as coming from the midline. A limitation of the duplex theory is that the theory does not completely explain directional hearing, as no explanation is given for the ability to distinguish between a sound source directly in front and behind. Also the theory only relates to localising sounds in the horizontal plane around the head. The theory also does not take into account the use of the pinna in localisation.(Gelfand, 2004)",
            "score": 145.52789306640625
        },
        {
            "docid": "9736652_7",
            "document": "Auditory masking . If two sounds of two different frequencies are played at the same time, two separate sounds can often be heard rather than a combination tone. The ability to hear frequencies separately is known as \"frequency resolution\" or \"frequency selectivity\". When signals are perceived as a combination tone, they are said to reside in the same \"critical bandwidth\". This effect is thought to occur due to filtering within the cochlea, the hearing organ in the inner ear. A complex sound is split into different frequency components and these components cause a peak in the pattern of vibration at a specific place on the cilia inside the basilar membrane within the cochlea. These components are then coded independently on the auditory nerve which transmits sound information to the brain. This individual coding only occurs if the frequency components are different enough in frequency, otherwise they are in the same critical band and are coded at the same place and are perceived as one sound instead of two.",
            "score": 143.6493682861328
        },
        {
            "docid": "56439577_19",
            "document": "Temporal envelope and fine structure . TFS information in the auditory nerve may be used to encode the (audio) frequency of low-frequency sounds, including single tones and more complex stimuli such as frequency-modulated tones or steady-state vowels (see role and applications to speech and music).",
            "score": 142.57965087890625
        },
        {
            "docid": "56439577_41",
            "document": "Temporal envelope and fine structure . Several psychophysical studies have shown that older people with normal hearing and people with sensorineural hearing loss often show impaired performance for auditory tasks that are assumed to rely on the ability of the monaural and binaural auditory system to encode and use TFS cues, such as: discrimination of sound frequency, discrimination of the fundamental frequency of harmonic sounds, detection of FM at rates below 5\u00a0Hz, melody recognition for sequences of pure tones and complex sounds, lateralization and localization of pure tones and complex tones, and segregation of concurrent harmonic sounds (such as speech sounds). However, it remains unclear to which extent deficits associated with hearing loss reflect poorer TFS processing or reduced cochlear frequency selectivity.",
            "score": 141.5835723876953
        },
        {
            "docid": "35332840_28",
            "document": "Noise in music . When pure-frequency sine tones were first synthesised into complex timbres, starting in 1953, combinations using inharmonic relationships (noises) were used far more often than harmonic ones (tones). Tones were seen as analogous to vowels, and noises to consonants in human speech, and because traditional music had emphasised tones almost exclusively, composers of electronic music saw scope for exploration along the continuum stretching from single, pure (sine) tones to white noise (the densest superimposition of all audible frequencies)\u2014that is, from entirely periodic to entirely aperiodic sound phenomena. In a process opposite to the building up of sine tones into complexes, white noise could be filtered to produce sounds with different bandwidths, called \"coloured noises\", such as the speech sounds represented in English by \"sh\", \"f\", \"s\", or \"ch\". An early example of an electronic composition composed entirely by filtering white noise in this way is Henri Pousseur's \"Scambi\" (Exchanges), realised at the Studio di Fonologia in Milan in 1957.",
            "score": 135.76036071777344
        },
        {
            "docid": "994097_9",
            "document": "Auditory cortex . Neurons in the auditory cortex are organized according to the frequency of sound to which they respond best. Neurons at one end of the auditory cortex respond best to low frequencies; neurons at the other respond best to high frequencies. There are multiple auditory areas (much like the multiple areas in the visual cortex), which can be distinguished anatomically and on the basis that they contain a complete \"frequency map.\" The purpose of this frequency map (known as a tonotopic map) is unknown, and is likely to reflect the fact that the cochlea is arranged according to sound frequency. The auditory cortex is involved in tasks such as identifying and segregating \"auditory objects\" and identifying the location of a sound in space. For example, it has been shown that A1 encodes complex and abstract aspects of auditory stimuli without encoding their \"raw\" aspects like frequency content, presence of a distinct sound or its echoes.",
            "score": 135.51470947265625
        },
        {
            "docid": "18994087_28",
            "document": "Sound . Pitch is perceived as how \"low\" or \"high\" a sound is and represents the cyclic, repetitive nature of the vibrations that make up sound. For simple sounds, pitch relates to the frequency of the slowest vibration in the sound (called the fundamental harmonic). In the case of complex sounds, pitch perception can vary. Sometimes individuals identify different pitches for the same sound, based on their personal experience of particular sound patterns. Selection of a particular pitch is determined by pre-conscious examination of vibrations, including their frequencies and the balance between them. Specific attention is given to recognising potential harmonics. Every sound is placed on a pitch continuum from low to high. For example: white noise (random noise spread evenly across all frequencies) sounds higher in pitch than pink noise (random noise spread evenly across octaves) as white noise has more high frequency content. Figure 1 shows an example of pitch recognition. During the listening process, each sound is analysed for a repeating pattern (See Figure 1: orange arrows) and the results forwarded to the auditory cortex as a single pitch of a certain height (octave) and chroma (note name).",
            "score": 133.0839385986328
        },
        {
            "docid": "14339999_2",
            "document": "Virtual pitch . Virtual pitch is a pitch at the missing fundamental of a harmonic complex tone. It corresponds to the phenomenon whereby one's brain extracts tones from everyday signals (including speech) and music, even if parts of the signal are masked by other sounds. Virtual pitch is contrasted to spectral pitch, which is the pitch of a pure tone or spectral component. Virtual pitch is called \"virtual\" because there is no acoustical correlate at the frequency corresponding to the pitch: even when a virtual pitch corresponds to a physically present fundamental (or first harmonic), as it often does in everyday harmonic complex tones, the exact virtual pitch depends on the exact frequencies of higher harmonics and is almost independent of the exact frequency of the fundamental.",
            "score": 132.31979370117188
        },
        {
            "docid": "35988494_3",
            "document": "Selective auditory attention . The cocktail party problem was first brought up in 1953 by Colin Cherry. This common problem is how our minds solves the issue of knowing what in the auditory scene is important and combining those in a coherent whole, such as the problem of how we can perceive our friend talking in the midst of a crowded cocktail party. He suggested that the auditory system can filter sounds being heard. Physical characteristics of the auditory information such as speaker's voice or location can improve a person's ability to focus on certain stimuli even if there is other auditory stimuli present. Cherry also did work with shadowing which involves different information being played into both ears and only one ear's information can be processed and remembered (Eysneck, 2012, p.\u00a084). Another psychologist, Albert Bregman, came up with the auditory scene analysis model. The model has three main characteristics: segmentation, integration, and segregation. Segmentation involves the division of auditory messages into segments of importance. The process of combining parts of an auditory message to form a whole is associated with integration. Segregation is the separation of important auditory messages and the unwanted information in the brain. It is important to note that Bregman also makes a link back to the idea of perception. He states that it is essential for one to make a useful representation of the world from sensory inputs around us. Without perception, an individual will not recognize or have the knowledge of what is going on around them. While Begman's seminal work is critical to understanding selective auditory attention, his studies did not focus on the way in which an auditory message is selected, if and when it was correctly segregated from other sounds in a mixture, which is a critical stage of selective auditory attention. Inspired in part by Bregman's work, a number of researchers then set out to link directly work on auditory scene analysis to the processes governing attention, including Maria Chait, Mounya Elhilali, Shihab Shamma, and Barbara Shinn-Cunningham.",
            "score": 131.7499237060547
        },
        {
            "docid": "8953380_9",
            "document": "Auditory scene analysis . One example of this is the phenomenon of streaming, also called \"stream segregation.\" If two sounds, A and B, are rapidly alternated in time, after a few seconds the perception may seem to \"split\" so that the listener hears two rather than one stream of sound, each stream corresponding to the repetitions of one of the two sounds, for example, A-A-A-A-, etc. accompanied by B-B-B-B-, etc. The tendency towards segregation into separate streams is favored by differences in the acoustical properties of sounds A and B. Among the differences classically shown to promote segregation are those of frequency (for pure tones), fundamental frequency (for complex tones), frequency composition, source location. But it has been suggested that about any systematic perceptual difference between two sequences can elicit streaming, provided the speed of the sequence is sufficient.",
            "score": 131.48960876464844
        },
        {
            "docid": "25260196_25",
            "document": "Neuronal encoding of sound . Primary auditory neurons carry action potentials from the cochlea into the transmission pathway shown in the adjacent image. Multiple relay stations act as integration and processing centers. The signals reach the first level of cortical processing at the primary auditory cortex (A1), in the superior temporal gyrus of the temporal lobe. Most areas up to and including A1 are tonotopically mapped (that is, frequencies are kept in an ordered arrangement). However, A1 participates in coding more complex and abstract aspects of auditory stimuli without coding well the frequency content, including the presence of a distinct sound or its echoes.  Like lower regions, this region of the brain has combination-sensitive neurons that have nonlinear responses to stimuli.",
            "score": 131.34068298339844
        },
        {
            "docid": "1781678_9",
            "document": "Cocktail party effect . Selective attention shows up across all ages. Starting with infancy, babies begin to turn their heads toward a sound that is familiar to them, such as their parents' voices. This shows that infants selectively attend to specific stimuli in their environment. Furthermore, reviews of selective attention indicate that infants favor \"baby\" talk over speech with an adult tone. This preference indicates that infants can recognize physical changes in the tone of speech. However, the accuracy in noticing these physical differences, like tone, amid background noise improves over time. Infants may simply ignore stimuli because something like their name, while familiar, holds no higher meaning to them at such a young age. However, research suggests that the more likely scenario is that infants do not understand that the noise being presented to them amidst distracting noise is their own name, and thus do not respond. The ability to filter out unattended stimuli reaches its prime in young adulthood. In reference to the cocktail party phenomenon, older adults have a harder time than younger adults focusing in on one conversation if competing stimuli, like \"subjectively\" important messages, make up the background noise.",
            "score": 130.8503875732422
        },
        {
            "docid": "292200_2",
            "document": "Missing fundamental . A harmonic sound is said to have a missing fundamental, suppressed fundamental, or phantom fundamental when its overtones suggest a fundamental frequency but the sound lacks a component at the fundamental frequency itself. The brain perceives the pitch of a tone not only by its fundamental frequency, but also by the periodicity implied by the relationship between the higher harmonics; we may perceive the same pitch (perhaps with a different timbre) even if the fundamental frequency is missing from a tone.",
            "score": 130.63565063476562
        },
        {
            "docid": "9736652_25",
            "document": "Auditory masking . When a sinusoidal signal and a sinusoidal masker (tone) are presented simultaneously the envelope of the combined stimulus fluctuates in a regular pattern described as beats. The fluctuations occur at a rate defined by the difference between the frequencies of the two sounds. If the frequency difference is small then the sound is perceived as a periodic change in the loudness of a single tone. If the beats are fast then this can be described as a sensation of roughness. When there is a large frequency separation, the two components are heard as separate tones without roughness or beats. Beats can be a cue to the presence of a signal even when the signal itself is not audible. The influence of beats can be reduced by using a narrowband noise rather than a sinusoidal tone for either signal or masker.",
            "score": 129.7257080078125
        },
        {
            "docid": "31251362_5",
            "document": "Beat deafness . When sound waves reach the ears, the energy they contain is converted into electrical signals, which are sent via the auditory nerves to the brain. Sound processing begins when these electrical signals reach the primary auditory receiving area in the core part of the temporal lobe. Signals then travel to the area surrounding the core, known as the belt area, and are then transmitted to the parabelt area, which is located next to the belt. Simple sounds such as pure tones are able to activate the core area of the brain, but both the belt and parabelt areas are activated by only complex sounds, such as those found in speech and music. The auditory cortex in the left hemisphere of the brain is responsible for processing beat and rhythm in music. The right auditory cortex is primarily used in distinguishing between different harmonics, which are simple pure tones that combine to create complex tones.",
            "score": 129.45230102539062
        },
        {
            "docid": "56439577_22",
            "document": "Temporal envelope and fine structure . It is often assumed that many perceptual capacities rely on the ability of the monaural and binaural auditory system to encode and use TFS cues evoked by components in sounds with frequencies below about 1\u20134\u00a0kHz. These capacities include discrimination of frequency, discrimination of the fundamental frequency of harmonic sounds, detection of FM at rates below 5\u00a0Hz, melody recognition for sequences of pure tones and complex tones, lateralization and localization of pure tones and complex tones, and segregation of concurrent harmonic sounds (such as speech sounds). It appears that TFS cues require correct tonotopic (place) representation to be processed optimally by the auditory system. Moreover, musical pitch perception has been demonstrated for complex tones with all harmonics above 6\u00a0kHz, demonstrating that it is not entirely dependent on neural phase locking to TFS (i.e., TFS) cues.",
            "score": 127.56085968017578
        },
        {
            "docid": "1379269_3",
            "document": "Musical acoustics . Whenever two different pitches are played at the same time, their sound waves interact with each other \u2013 the highs and lows in the air pressure reinforce each other to produce a different sound wave. Any repeating sound wave that is not a sine wave can be modeled by many different sine waves of the appropriate frequencies and amplitudes (a frequency spectrum). In humans the hearing apparatus (composed of the ears and brain) can usually isolate these tones and hear them distinctly. When two or more tones are played at once, a variation of air pressure at the ear \"contains\" the pitches of each, and the ear and/or brain isolate and decode them into distinct tones.",
            "score": 125.6402816772461
        },
        {
            "docid": "29329_18",
            "document": "Spectrum . A source of sound can have many different frequencies mixed. A Musical tone's timbre is characterized by its harmonic spectrum. Sound in our environment that we refer to as \"noise\" includes many different frequencies. When a sound signal contains a mixture of all audible frequencies, distributed equally over the audio spectrum, it is called white noise.",
            "score": 124.50717163085938
        },
        {
            "docid": "3883287_8",
            "document": "Tranquillity . Within tranquillity studies, much of the emphasis has been placed on understanding the role of vision in the perception of natural environments, which is probably not surprising, considering that upon first viewing a scene its configurational coherence can be established with incredible speed. Indeed, scene information can be captured in a single glance and the gist of a scene determined in as little as 100ms. The speed of processing of complex natural images was tested by Thorpe \"et al.\" using colour photographs of a wide range of animals (mammals, birds, reptiles and fish), in their natural environments, mixed with distracters that included pictures of forests, mountains, lakes, buildings and fruit. During this experiment, subjects were shown an image for 20ms and asked to determine whether it contained an animal or not. The electrophysiological brain responses obtained in this study showed that a decision could be made within 150ms of the image being seen, indicating the speed at which cognitive visual processing occurs. However, audition, and in particular the individual components that collectively comprise the soundscape, a term coined by Schafer to describe the ever-present array of sounds that constitute the sonic environment, also significantly inform the various schemata used to characterise differing landscape types. This interpretation is supported by the auditory reaction times, which are 50 to 60ms faster than that of the visual modality. It is also known that sound can alter visual perception and that under certain conditions areas of the brain involved in processing auditory information can be activated in response to visual stimuli.  Research conducted by Pheasant \"et al.\" has shown that when individuals make tranquillity assessments based on a uni-modal auditory or visual sensory input, they characterise the environment by drawing upon a number of key landscape and soundscape characteristics. For example, when making assessments in response to visual-only stimuli the percentage of water, flora and geological features present within a scene, positively influence how tranquil a location is perceived to be. Likewise when responding to uni-modal auditory stimuli, the perceived loudness of biological sounds positively influences the perception of tranquillity, whilst the perceived loudness of mechanical sounds have a negative effect. However, when presented with bi-modal auditory-visual stimuli the individual soundscape and landscape components alone no longer influenced the perception of tranquillity. Rather configurational coherence was provided by the percentage of natural and contextual features present within the scene and the equivalent continuous sound pressure level (LAeq).",
            "score": 123.86723327636719
        },
        {
            "docid": "31352483_10",
            "document": "Soundscape ecology . Even among birds that are able to alter their songs to be better heard in environments inundated with anthropophony, these behavioral changes may have important fitness consequences. In the great tit, for example, there is a tradeoff between signal strength and signal detection that depends on song frequency. Male birds that include more low frequency sounds in their song repertoire experience better sexual fidelity from their mates which results in increased reproductive success. However, low frequency sounds tend to be masked when anthropogenic noise is present, and high frequency songs are more effective at eliciting female responses under these conditions. Birds may therefore experience competing selective pressures in habitats with high levels of anthropogenic noise: pressure to call more at lower frequencies in order to improve signal strength and secure good mates versus opposing pressure to sing at higher frequencies in order to ensure that calls are detected against a background of anthrophony. In addition, use of certain vocalizations, including high amplitude sounds that reduce masking in noisy environments, may impose energetic costs that reduce fitness. Because of the reproductive trade-offs and other stresses they impose on some birds, noisy habitats may represent ecological traps, habitats in which individuals have reduced fitness yet are colonized at rates greater than or equal to other habitats.",
            "score": 123.68500518798828
        },
        {
            "docid": "2430604_3",
            "document": "Intelligibility (communication) . Intelligibility is negatively impacted by background noise and too much reverberation. The relationship between sound and noise levels is generally described in terms of a signal-to-noise ratio. With a background noise level between 35 and 100\u00a0dB, the threshold for 100% intelligibility is usually a signal-to-noise ratio of 12\u00a0dB. 12\u00a0dB means that the signal should be roughly 4 times louder that the background noise. The speech signal ranges from about 200\u20138000\u00a0Hz, while human hearing ranges from about 20-20,000\u00a0Hz, so the effects of masking depend on the frequency range of the masking noise. Additionally, different speech sounds make use of different parts of the speech frequency spectrum, so a continuous background noise such as white or pink noise will have a different effect on intelligibility than a variable or modulated background noise such as competing speech, multi-talker or \"cocktail party\" babble, or industrial machinery.",
            "score": 123.5118179321289
        },
        {
            "docid": "2956315_21",
            "document": "Two-streams hypothesis . Conduction aphasia affects a subject's ability to reproduce speech (typically by repetition), though it has no influence on the subject's ability to comprehend spoken language. This shows that conduction aphasia must reflect not an impairment of the ventral pathway but instead of the dorsal pathway. Hickok and Poeppel found that conduction aphasia can be the result of damage, particularly lesions, to the Spt (Sylvian parietal temporal). This is shown by the Spt's involvement in acquiring new vocabulary, for while experiments have shown that most conduction aphasiacs can repeat high-frequency, simple words, their ability to repeat low-frequency, complex words is impaired. The Spt is responsible for connecting the motor and auditory systems by making auditory code accessible to the motor cortex. It appears that the motor cortex recreates high-frequency, simple words (like \"cup\") in order to more quickly and efficiently access them, while low-frequency, complex words (like \"Sylvian parietal temporal\") require more active, online regulation by the Spt. This explains why conduction aphasiacs have particular difficulty with low-frequency words which requires a more hands-on process for speech production. \"Functionally, conduction aphasia has been characterized as a deficit in the ability to encode phonological information for production,\" namely because of a disruption in the motor-auditory interface. Conduction aphasia has been more specifically related to damage of the arcuate fasciculus, which is vital for both speech and language comprehension, as the arcuate fasiculus makes up the connection between Broca and Wernicke's areas.",
            "score": 123.3646240234375
        },
        {
            "docid": "6147487_29",
            "document": "Neural coding . For very brief stimuli, a neuron's maximum firing rate may not be fast enough to produce more than a single spike. Due to the density of information about the abbreviated stimulus contained in this single spike, it would seem that the timing of the spike itself would have to convey more information than simply the average frequency of action potentials over a given period of time. This model is especially important for sound localization, which occurs within the brain on the order of milliseconds. The brain must obtain a large quantity of information based on a relatively short neural response. Additionally, if low firing rates on the order of ten spikes per second must be distinguished from arbitrarily close rate coding for different stimuli, then a neuron trying to discriminate these two stimuli may need to wait for a second or more to accumulate enough information. This is not consistent with numerous organisms which are able to discriminate between stimuli in the time frame of milliseconds, suggesting that a rate code is not the only model at work.",
            "score": 123.2153091430664
        },
        {
            "docid": "156859_33",
            "document": "Comparison of analog and digital recording . A perceptual study by Nishiguchi et al. (2004) concluded that \"no significant difference was found between sounds with and without very high frequency components among the sound stimuli and the subjects... however, [Nishiguchi et al] can still neither confirm nor deny the possibility that some subjects could discriminate between musical sounds with and without very high frequency components.\"",
            "score": 122.92521667480469
        },
        {
            "docid": "1560437_45",
            "document": "Portable media player . Natural mode is characterised by subjective effect of balance of different frequency sounds, regardless of level of distortion, appearing in the reproduction device. It is also regardless of personal user's ability to perceive specific sound frequencies (excluding obvious hearing loss). The natural effect is obtained due to special sound processing algorithm (i.e. \"formula of subjective equalisation of frequency-response function\"). Its principle is to assess frequency response function (FRF) of mediaplayer or any other sound reproduction device, in accordance with audibility threshold in silence (subjective for each person), and to apply gain modifying factor. The factor is determined with the help of integrated function to test audibility threshold: the program generates tone signals (with divergent oscillations \u2013 from minimum volume 30\u201345\u00a0Hz to maximum volume appr. 16\u00a0kHz), and user assess their subjective audibility. The principle is similar to in situ audiometry, used in medicine to prescribe a hearing aid. However, the results of test may be used to a limited extent as far as FRF of sound devices depends on reproduction volume. It means correction coefficient should be determined several times \u2013 for various signal strengths, which is not a particular problem from a practical standpoint.",
            "score": 121.27813720703125
        },
        {
            "docid": "6147487_24",
            "document": "Neural coding . Neurons exhibit high-frequency fluctuations of firing-rates which could be noise or could carry information. Rate coding models suggest that these irregularities are noise, while temporal coding models suggest that they encode information. If the nervous system only used rate codes to convey information, a more consistent, regular firing rate would have been evolutionarily advantageous, and neurons would have utilized this code over other less robust options. Temporal coding supplies an alternate explanation for the \u201cnoise,\" suggesting that it actually encodes information and affects neural processing. To model this idea, binary symbols can be used to mark the spikes: 1 for a spike, 0 for no spike. Temporal coding allows the sequence 000111000111 to mean something different from 001100110011, even though the mean firing rate is the same for both sequences, at 6 spikes/10\u00a0ms. Until recently, scientists had put the most emphasis on rate encoding as an explanation for post-synaptic potential patterns. However, functions of the brain are more temporally precise than the use of only rate encoding seems to allow. In other words, essential information could be lost due to the inability of the rate code to capture all the available information of the spike train. In addition, responses are different enough between similar (but not identical) stimuli to suggest that the distinct patterns of spikes contain a higher volume of information than is possible to include in a rate code.",
            "score": 121.05469512939453
        }
    ]
}