{
    "q": [
        {
            "docid": "12781902_6",
            "document": "User modeling . Though the first method is a good way to quickly collect main data it lacks the ability to automatically adapt to shifts in users' interests. It depends on the users' readiness to give information and it is unlikely that they are going to edit their answers once the registration process is finished. Therefore, there is a high likelihood that the user models are not up to date. However, this first method allows the users to have full control over the collected data about them. It is in their decision which information they are willing to provide. This possibility is missing in the second method. Adaptive changes in a system that learns users' preferences and needs only by interpreting their behavior might appear a bit opaque to the users, because they cannot fully understand and reconstruct why the system behaves the way it does. Moreover, the system is forced to collect a certain amount of data before it is able to predict the users' needs with the required accuracy. Therefore, it takes a certain learning time before a user can benefit from adaptive changes. However, afterwards these automatically adjusted user models allow a quite accurate adaptivity of the system. The hybrid approach tries to combine the advantages of both methods. Through collecting data by directly asking its users it gathers a first stock of information which can be used for adaptive changes. By learning from the users' interactions it can adjust the user models and reach more accuracy. Yet, the designer of the system has to decide, which of these information should have which amount of influence and what to do with learned data that contradicts some of the information given by a user.",
            "score": 135.71055471897125
        },
        {
            "docid": "56345675_2",
            "document": "Predation risk allocation hypothesis . The predation risk allocation hypothesis attempts to explain how and why animals' behaviour and foraging strategies differ in various predatory situations, depending on their risk of endangerment. The hypothesis suggests that an animal's alertness and attention, along with its willingness to hunt for food, will change depending on the risk factors within that animal's environment and the presence of predators that could attack. The model assumes there are different levels of risk factors within various environments and prey animals will behave more cautiously when they are found in high-risk environments. The overall effectiveness of the model for predicting animal behaviour varies, therefore, its results are dependent on the prey species used in the model and how their behaviour changes. There are several reasons the predation risk allocation hypothesis was developed to observe how animal behaviour varies depending on its risk factors. Mixed results have been found for the model's effectiveness in predicting predator defensive behaviour for various species.",
            "score": 90.29216623306274
        },
        {
            "docid": "14246162_76",
            "document": "Memristor . Memristor patents include applications in programmable logic, signal processing, neural networks, control systems, reconfigurable computing, brain-computer interfaces and RFID. Memristive devices are potentially used for stateful logic implication, allowing a replacement for CMOS-based logic computation. Several early works have been reported in this direction. In 2009, a simple electronic circuit consisting of an LC network and a memristor was used to model experiments on adaptive behavior of unicellular organisms. It was shown that subjected to a train of periodic pulses, the circuit learns and anticipates the next pulse similar to the behavior of slime molds \"Physarum polycephalum\" where the viscosity of channels in the cytoplasm responds to periodic environment changes. Applications of such circuits may include, e.g., pattern recognition. The DARPA SyNAPSE project funded HP Labs, in collaboration with the Boston University Neuromorphics Lab, has been developing neuromorphic architectures which may be based on memristive systems. In 2010, Versace and Chandler described the MoNETA (Modular Neural Exploring Traveling Agent) model. MoNETA is the first large-scale neural network model to implement whole-brain circuits to power a virtual and robotic agent using memristive hardware. Application of the memristor crossbar structure in the construction of an analog soft computing system was demonstrated by Merrikh-Bayat and Shouraki. In 2011, they showed how memristor crossbars can be combined with fuzzy logic to create an analog memristive neuro-fuzzy computing system with fuzzy input and output terminals. Learning is based on the creation of fuzzy relations inspired from Hebbian learning rule.",
            "score": 108.22550523281097
        },
        {
            "docid": "39619438_3",
            "document": "AnimatLab . Neuromechanical simulation enables investigators to explore the dynamical relationships between the brain, the body, and the world in ways that are difficult or impossible through experiment alone. This is done by producing biologically realistic models of the neural networks that control behavior, while also simulating the physics that controls the environment in which an animal is situated. Interactions with the simulated world can then be fed back to the virtual nervous system using models of sensory systems. This provides feedback similar to what the real animal would encounter, and makes it possible to close the sensory-motor feedback loop to study the dynamic relationship between nervous function and behavior. This relationship is crucial to understanding how nervous systems work.",
            "score": 70.7400678396225
        },
        {
            "docid": "5162898_4",
            "document": "Dynamic network analysis . DNA statistical tools tend to provide more measures to the user, because they have measures that use data drawn from multiple networks simultaneously. Latent space models (Sarkar and Moore, 2005) and agent-based simulation are often used to examine dynamic social networks (Carley et al., 2009). From a computer simulation perspective, nodes in DNA are like atoms in quantum theory, nodes can be, though need not be, treated as probabilistic. Whereas nodes in a traditional SNA model are static, nodes in a DNA model have the ability to learn. Properties change over time; nodes can adapt: A company's employees can learn new skills and increase their value to the network; or, capture one terrorist and three more are forced to improvise. Change propagates from one node to the next and so on. DNA adds the element of a network's evolution and considers the circumstances under which change is likely to occur. There are three main features to dynamic network analysis that distinguish it from standard social network analysis. First, rather than just using social networks, DNA looks at meta-networks. Second, agent-based modeling and other forms of simulations are often used to explore how networks evolve and adapt as well as the impact of interventions on those networks. Third, the links in the network are not binary; in fact, in many cases they represent the probability that there is a link.",
            "score": 111.570277094841
        },
        {
            "docid": "55680252_6",
            "document": "Amyotrophic lateral sclerosis research . Many animals have been used over the years to study ALS and to search for a potential therapy. The animal models can be C. elegans which has only 959 cells with simple structure, and known gene code.. Also, some studied have introduced the transgenic strain of C. elegans, which has a mutation in a gene related to ALS for example, and crossed them with the transgenic nlp-29 GFP reporter strain, resulting in fluorescent markers to the cells that are expressing these mutated genes, which can be used to monitor the disease development and effects . Similar, but more complex nervous system from the C. elegans is the Drosophila. Fruit fly ALS models can be used to study the locomotion and eye changes that can be related to human symptoms. Thus, drugs can be tested on these transgenic fruit flies to discovery new target molecules . On the other hand, zebrafish models have been used widely due to their similarity in the development and anatomy characteristics as a vertebrate to the human body . A study introduced the SOD1/GFP transgenic zebra-fish to study that specific gene on the development and occurrence of ALS in the fish, and how can that be used in testing potential therapeutic molecules . All the previous models are considered simple, and saves time and money due to their short lifespan and small and simple body structure .",
            "score": 83.75090801715851
        },
        {
            "docid": "21468960_26",
            "document": "Domain-specific multimodeling . Existing editors do not take inter-language consistency relations into account when providing guidance. In the previous example, an ideal editor should for instance be able to suggest the createSurvey service as a valid value when the developer edits the target attribute in the Form definition. An environment which could reason about artifacts from different languages would also be able to help the developer identify program states where there was local but not global consistency. Such a situation can arise when a model is well-formed and hence locally consistent but at the same time violates an inter-language constraint. Guidance or intelligent assistance in the form of proposals on how to complete a model would be useful for setups with multiple languages and complex consistency constraints. Tool-suggested editing operations could make it easier for the developer to get started on the process of learning how to use the languages.",
            "score": 87.2651938199997
        },
        {
            "docid": "4698768_6",
            "document": "Naturalistic decision-making . The recognition-primed decision (RPD) model is the main protocol derived from the NDM framework. RPD describes how people use their experience in the form of patterns. These patterns highlight the relevant cues, provide expected outcomes, identify plausible goals, and suggest typical types of reactions in that type of situation. When people need to make a decision, they can quickly match the situation to the patterns they have learned and experienced in the past. Doing this, people can successfully make rapid decisions. The RPD model explains how people can make good decisions without comparing options. However, there is more to the RPD model than pattern matching. How can a person evaluate an option without comparing it with others? It has been found that fireground commanders evaluate a course of action by using mental simulation to imagine how a situation would play out within the context of the current situation. If it would work, then the commanders could initiate the action. If it almost worked, they could try to adapt it or else consider other actions that were somewhat less typical, continuing until they found an option that felt comfortable. This process exemplifies Herbert Simon's (1957) notion of satisficing \u2013 looking for the first workable option rather than trying to find the best possible option. Because fires grow exponentially, the faster the commanders could react, the easier their job. Therefore, the RPD model is a blend of intuition and analysis. The pattern matching is the intuitive part, and the mental simulation is the conscious, deliberate, and analytical part. Intuitive strategy relying only on pattern matching would be too risky because sometimes the pattern matching generates flawed options. Also, a completely deliberative and analytical strategy would be too slow; the fires would be out of control by the time the commanders finished deliberating. In-depth interviews with fireground commanders who recently experienced challenging incidents show that the percentage of RPD strategies used in those situations generally ranged from 80% to 90% (Klein, 1989). Other researchers have replicated these findings (see Klein, 1998). The first moves that occurred to them were much better than would be expected by chance. These findings support the RPD hypothesis that the first option considered is usually satisfactory. These results were later replicated by Johnson and Raab (2003).",
            "score": 93.46244084835052
        },
        {
            "docid": "4316253_8",
            "document": "Robert A. Rescorla . In 1972, Robert A. Rescorla and his colleague, Allan R. Wagner, created the Rescorla\u2013Wagner model. This models classical conditioning. It is unique because it explains how the unexpected can influence learning. The model shows how the element of surprise can progress learning in an animal. Learning is subjective to how surprising the unconditioned stimulus (US) is. The surprise (US) that follows the conditioned stimulus (CS) in the initial trial was learned because it is unexpected, or very surprising. However, in the following trials, the subject learns less because the US is predictable - or less surprising",
            "score": 97.33910369873047
        },
        {
            "docid": "5824073_12",
            "document": "High-content screening . This technology allows a (very) large number of experiments to be performed, allowing explorative screening. Cell-based systems are mainly used in chemical genetics where large, diverse small molecule collections are systematically tested for their effect on cellular model systems. Novel drugs can be found using screens of tens of thousands of molecules, and these have promise for the future of drug development.  Beyond drug discovery, chemical genetics is aimed at functionalizing the genome by identifying small molecules that acts on most of the 21,000 gene products in a cell. High-content technology will be part of this effort which could provide useful tools for learning where and when proteins act by knocking them out chemically. This would be most useful for gene where knock out mice (missing one or several genes) can not be made because the protein is required for development, growth or otherwise lethal when it is not there. Chemical knock out could address how and where these genes work. Further the technology is used in combination with RNAi to identify sets of genes involved in specific mechanisms, for example cell division. Here, libraries of RNAis, covering a whole set of predicted genes inside the target organism's genome can be used to identify relevant subsets, facilitating the annotation of genes for which no clear role has been established beforehand. The large datasets produced by automated cell biology contain spatially resolved, quantitative data which can be used for building for systems level models and simulations of how cells and organisms function. Systems biology models of cell function would permit prediction of why, where and how the cell responds to external changes, growth and disease.",
            "score": 94.91833126544952
        },
        {
            "docid": "588615_8",
            "document": "Ant colony optimization algorithms . Nature has given us several examples of how minuscule organisms, if they all follow the same basic rule, can create a form of collective intelligence on the macroscopic level. Colonies of social insects perfectly illustrate this model which greatly differs from human societies. This model is based on the co-operation of independent units with simple and unpredictable behavior. They move through their surrounding area to carry out certain tasks and only possess a very limited amount of information to do so. A colony of ants, for example, represents numerous qualities that can also be applied to a network of ambient objects. Colonies of ants have a very high capacity to adapt themselves to changes in the environment as well as an enormous strength in dealing with situations where one individual fails to carry out a given task. This kind of flexibility would also be very useful for mobile networks of objects which are perpetually developing. Parcels of information that move from a computer to a digital object behave in the same way as ants would do. They move through the network and pass from one knot to the next with the objective of arriving at their final destination as quickly as possible.",
            "score": 90.43014073371887
        },
        {
            "docid": "22549833_2",
            "document": "Modeling and simulation . Modeling and simulation (M&S) in simple terms is a substitute for physical experimentation, in which computers are used to compute the results of some physical phenomenon. As it is apparent from its name \"Modeling and simulation\" firstly computer is used to build a mathematical model which contains all the parameters of physical model and represent physical model in virtual form then conditions are applied which we want to experiment on physical model, then simulation starts i.e, we leave on computer to compute/calculate the results of those conditions on mathematical model. In this way actual experimentation can be avoided which is costly and time consuming instead of using mathematical knowledge and computer's computation power to solve real world problems cheaply and in time efficient manner. As such, M&S can facilitate understanding a system's behavior without actually testing the system in the real world. For instance, to determine which type of spoiler would improve traction the most while designing a race car, a computer simulation of the car could be used to estimate the effect of different spoiler shapes on the coefficient of friction in a turn. Useful insights about different decisions in the design could be gleaned without actually building the car. In addition, simulation can support experimentation that occurs totally in software, or in human-in-the-loop environments where simulation represents systems or generates data needed to meet experiment objectives. Furthermore, simulation can be used to train persons using a virtual environment that would otherwise be difficult or expensive to produce.",
            "score": 89.41475081443787
        },
        {
            "docid": "52588865_9",
            "document": "William T. Greenough . This view of brain structure, neural activity, and learning was completely overturned by Greenough's research. Greenough initially worked with mice and rat models, later studying primates and humans. His studies demonstrated that fundamental physical changes occurred in neurons in the brain in response to stimulating environments. At the most basic cellular level, the brains of rats that lived in stimulating environments developed more synapses than those that did not. He went on to demonstrate that new synapses were formed as a result of activities that involved learning, not just increased activity. Moreover, changes occurred in areas of the brain that were associated with the performance of specific learned tasks. Observed changes in learning, memory, and synapse formation persisted after training. Learning and memory formation were therefore fundamentally related to ongoing synapse formation. The result of Greenough's work has been a new model of brain 'plasticity' in which long-term memories are formed at a structural level in the brain as part of lifelong processes of learning.",
            "score": 171.41796147823334
        },
        {
            "docid": "1126216_7",
            "document": "Anticipation (artificial intelligence) . Humans can make decisions based on explicit beliefs about the future. More broadly, animals can act in appropriate ways that take future events into account, although they may not necessarily have an explicit cognitive model of the future; evolution may have shaped simpler systemic features that result in adaptive anticipatory behavior in a narrow domain. For example, hibernation is anticipatory behavior, but does not appear to be driven by a cognitive model of the future.",
            "score": 55.750157833099365
        },
        {
            "docid": "183403_49",
            "document": "Learning . However, in environments where change occurs within an animal's lifetime but is not constant, learning is more likely to evolve. Learning is beneficial in these scenarios because an animal can adapt to the new situation, but can still apply the knowledge that it learns for a somewhat extended period of time. Therefore, learning increases the chances of success as opposed to guessing. An example of this is seen in aquatic environments with landscapes subject to change. In these environments, learning is favored because the fish are predisposed to learn the specific spatial cues where they live.",
            "score": 89.60696601867676
        },
        {
            "docid": "53913187_22",
            "document": "Land change modeling . As there is continuous reinvention of modeling environments, frameworks, and platforms, land change modeling can improve from better research infrastructure support. For example, model and software infrastructure development can help avoid duplication of initiatives by land change modeling community members, co-learn about land change modeling, and integrate models to evaluate impacts of land change. Better data infrastructure can provide more data resources to support compilation, curation, and comparison of heterogeneous data sources. Better community modeling and governance can advance decision-making and modeling capabilities within a community with specific and achievable goals. Community modeling and governance would provide a step towards reaching community agreement on specific goals to move modeling and data capabilities forward.",
            "score": 97.1312940120697
        },
        {
            "docid": "188540_48",
            "document": "Classical conditioning . An organism's need to predict future events is central to modern theories of conditioning. Most theories use associations between stimuli to take care of these predictions. For example: In the R\u2013W model, the associative strength of a CS tells us how strongly that CS predicts a US. A different approach to prediction is suggested by models such as that proposed by Gallistel & Gibbon (2000, 2002). Here the response is not determined by associative strengths. Instead, the organism records the times of onset and offset of CSs and USs and uses these to calculate the probability that the US will follow the CS. A number of experiments have shown that humans and animals can learn to time events (see Animal cognition), and the Gallistel & Gibbon model yields very good quantitative fits to a variety of experimental data. However, recent studies have suggested that duration-based models cannot account for some empirical findings as well as associative models.",
            "score": 91.7526786327362
        },
        {
            "docid": "36086848_5",
            "document": "Fear processing in the brain . It has been observed that fear can contribute to behavioral changes. One way this phenomenon has been studied is on the basis of the repeated stress model done by Camp RM et al.(among others). In this particular study, it was examined that the contribution fear conditioning may play a huge role in altering an animal's (Fischer rat's) behavior in a repeated stress paradigm. Behavioral changes that are commonly referred to as depressive-like behaviors resulted from this model of testing. After setting a control and a valid experimental design, Fischer rats were exposed daily to different stressors in a complex environment. After four days of stressor exposure, both exploratory behavior and social interaction were tested on day 5 in either the same environment or a new environment. The rats showed much decreased exploration and social interaction when tested in different contexts compared to control rats. To further make a correlation to the biochemistry (as mentioned below), chronic infusion of propranolol (beta-adrenergic receptor antagonist) prevented the behavioral changes following repeated stressor exposure thus halting long term potentiation. Some physiological changes also occurred including the decrease in body weight gain and adrenal hypertrophy observed in animals exposed to stress. Overall, the conditioned fear responses can contribute to behavioral changes in a repeated stress paradigm. This can be extended to correlate to other animals as well but with varying degrees of responses.",
            "score": 102.32958829402924
        },
        {
            "docid": "6282756_3",
            "document": "Interactive skeleton-driven simulation . Methods for simulating deformation, such as changes of shapes, of dynamic bodies involve intensive calculations, and several models have been developed. Some of these are known as \"free-form deformation\", \"skeleton-driven deformation\", \"dynamic deformation\" and \"anatomical modelling\". Skeletal animation is well known in computer animation and 3D character simulation. Because of the calculation intensitivity of the simulation, few interactive systems are available which realistically can simulate dynamic bodies in real-time. Being able to \"interact\" with such a realistic 3D model would mean that calculations would have to be performed within the constraints of a frame rate which would be acceptable via a user interface.",
            "score": 63.83036661148071
        },
        {
            "docid": "27179535_6",
            "document": "Leah Krubitzer . Krubitzer and her team have decided to focus part of their research on studying the cortical evolution of the neocortex. It is known that the neocortex has the ability to adapt and change over time. This is an important feature that allows the brain function and connectivity to coordinate movements vital for an organism's survival in a specific environment. Krubitzer and her team used a variety of different rodents and squirrels to test their hypothesis. They hypothesized that the level of cortical activity organization would be directly correlated with the specific connections in the brain based on the environment that the particular animal was accustomed. Differences were found between the lab animals and wild caught animals as would be expected. Krubitzer varied the amounts of sensory stimuli that each test subject was exposed to early on in development. By doing this, she was able to track the organization of the cortex from early development on. One example of how she did this was altering the vision in an opossum in early development. Research found that when vision is lost early in development, other sensory systems will begin to take over that area and the cortex will re-organize itself to make up for the loss of this sensation.",
            "score": 62.71363651752472
        },
        {
            "docid": "30525054_7",
            "document": "Anders Dale . In a 2003 interview, Dale explained that he had \u201calways been interested in using quantitative modeling methods and simulations to answer biological questions,\u201d and that as a Harvard student he had been \u201cinterested in approaching connectionist neural networks from a more biological angle.\u201d When he went to UCSD to continue his graduate work his interest \u201cshifted to learning how to test models of how the brain works. Ideally you'd like to test your models not in anesthetized animals and brain slices, but by measuring brain activity in humans non-invasively. I wanted to study normal people doing normal tasks. That was what brought me to imaging. My goal was to see what kind of things we can measure non-invasively that can be quantitatively related to the models we want to build...I wanted to know what exactly we are measuring, how can you model it, and how can you relate the signal to what is going on in the brain physiologically...at a level that say you could measure invasively and that you could relate to parameters of quantitative models.\u201d His thesis work at UCSD, he said, \u201cwas on the EEG and MEG forward and inverse problems, and how to use anatomical information to constrain the solutions. It is clear that if you only use EEG or MEG measures, the spatial precision is not good enough to make inferences at a scale that's most useful to neuroscience. That led us into trying to use information with higher spatial resolution to constrain or bias our estimations of the signal sources in the brain.\u201d",
            "score": 86.69748532772064
        },
        {
            "docid": "26685721_36",
            "document": "Methods used to study memory . Songbirds are excellent animal models for studying learning and auditory memory. These birds have highly developed vocal organs, which give them the ability to create diverse and elaborate birdsongs. Neurotoxic lesions can be used to study how specific brain structures are key for this type of learning and scientists can also manipulate the environment in which these birds are raised. In addition, with functional MRI the response to different auditory stimuli has been studied in vivo. In experiments with songbirds, using these two methodologies has led to very interesting discoveries about auditory learning and memory. Much like humans, birds have a critical period when they must be exposed to adult birdsong. The cognitive systems of vocal production and auditory recognition parallel those in humans, and experiments have shown that there are critical brain structures for these processes.",
            "score": 88.65887689590454
        },
        {
            "docid": "985619_4",
            "document": "Agent-based model . Most agent-based models are composed of: (1) numerous agents specified at various scales (typically referred to as agent-granularity); (2) decision-making heuristics; (3) learning rules or adaptive processes; (4) an interaction topology; and (5) an environment. ABMs are typically implemented as computer simulations, either as custom software, or via ABM toolkits, and this software can be then used to test how changes in individual behaviors will affect the system's emerging overall behavior.",
            "score": 107.54475712776184
        },
        {
            "docid": "38916472_8",
            "document": "Leon Glass . Glass may be best known for his work with colleagues at McGill University, suggesting that certain physiological disorders may be considered dynamical diseases. These are characterized by sudden changes in the qualitative dynamics of a physiological control mechanism, which leads to disease. These features are illustrated in the Mackey-Glass equation. According to James Gleick, who recounted conversations with Glass in his book , foundational work in chaos by the McGill group was performed using animal models. He quotes Glass saying: \"Many different rhythms can be established between a stimulus and a little piece of chicken heart\". Since the initial description of dynamical diseases, a large number of researchers have analyzed mathematical models of physiological systems. Examples of dynamical diseases have been described in medical fields as diverse as hematology, cardiology, neurology, and psychiatry. Dynamical disease modeling has been used to understand cardiac arrhythmia, and specific model detection algorithms are now being programmed into pacemakers so that pathological patterns can be detected and corrected.",
            "score": 74.05348837375641
        },
        {
            "docid": "9536113_16",
            "document": "Computer audition . Listening to music and general audio is commonly not a task directed activity. People enjoy music for various poorly understood reasons, which are commonly referred to the emotional effect of music due to creation of expectations and their realization or violation. Animals attend to signs of danger in sounds, which could be either specific or general notions of surprising and unexpected change. Generally, this creates a situation where computer audition can not rely solely on detection of specific features or sound properties and has to come up with general methods of adapting to changing auditory environment and monitoring its structure. This consists of analysis of larger repetition and self-similarity structures in audio to detect innovation, as well as ability to predict local feature dynamics.",
            "score": 68.54410481452942
        },
        {
            "docid": "21559539_5",
            "document": "Computer-aided ergonomics . The question is \u201cIn what scenario\u201d it would be beneficial to use computer-aided ergonomics compared to traditional ergonomics. First of all computer aided ergonomics using for example a musculo-skeletal modeling system as The [AnyBody Modeling system], would be beneficial in physical ergonomics, which traditional combines aspects from the human anatomical, anthropometric, physiological and biomechanical characteristics related to some physical activity. The model can provide a quantitative foundation for ergonomic design and recommendations.  Traditionally \u201cErgonomics\u201d has been based on recommendations derived from empirical data from various working tasks; if many people get injured from working in a certain posture, it is recommended to avoid working in that posture. However, when applying the recommendations to another related working posture, the posture or the movement often does not match exactly. This means that the theory and recommendations does not apply to the new situation. In this case it could be beneficial to model the situation in order to find out how the reaction forces and muscle activities differ from the first situation, where the recommendations were based on empirical data.  A combination of risk factors can be derived from the model output. For example, when designing an office chair, one would like to design it to fulfill several demands; comfortable, relaxing, supporting and so on. Some of the criteria related to the demands might be conflicting for example; comfort is often related to the shear force on the seat, which should be kept as low as possible. The seat shear force could be removed by making a horizontal seat and rising the backrest to 90 degrees however this would not be relaxing. Therefore, a combination of seat and backrest angles needs to be considered in order to find optimal seated postures related to only the two design variables.  Computer-aided ergonomics is an interdisciplinary field of work, that involve the use of a computer to solve complex problems that involve a person interacting with an environment. As the title reveals, it is all about having a computer to help finding the best ergonomic solution. Ergonomics involves many disciplines, such as biomechanics, anthropometry, mechanical engineering, industrial engineering, kinesiology, health sciences and physiology. Due to the highly interdisciplinary it is hard to get a full understanding of a situation based on knowledge, unless the knowledge in some way is built into a computer system.",
            "score": 71.2527152299881
        },
        {
            "docid": "3010875_12",
            "document": "Richard Levins . Prior to Levins' work, population genetics had assumed the environment to be constant, while mathematical ecology assumed the genetic makeup of the species involved to be constant. Levins modelled the situation in which evolution is taking place while the environment changes. One of the surprising consequences of his model is that selection need not maximize adaptation, and that species can select themselves to extinction. He encapsulated his major early results in \"Evolution in Changing Environments\", a book based on lectures he delivered in Cuba in the early 1960s. Levins made extensive use of mathematics, some of which he invented himself, although it had been previously developed in other areas of pure mathematics or economics without his awareness of it. For instance, Levins makes extensive use of convex set theory for fitness sets, (resembling the economic formulations of J. R. Hicks) and extends Sewall Wright's path analysis to the analysis of causal feedback loops.",
            "score": 83.50575137138367
        },
        {
            "docid": "47152350_36",
            "document": "Human performance modeling . Another critical cognitive activity of interest to human factors is that of judgement and decision making. These activities starkly contrast to routine cognitive skills, for which the procedures are known in advance, as many situations require operators to make judgments under uncertaintly - to produce a rating of quality, or perhaps choose among many possible alternatives. Although many disciplines including mathematics and economics make significant contributions to this area of study, the majority of these models do not model human behavior but rather model optimal behavior, such as \"subjective expected utility theory\" (Savage, 1954; von Neumann & Morgenstern, 1944). While models of optimal behavior are important and useful, they do not consider a baseline of comparison for human performance - though much research on human decision making in this domain compares human performance to mathematically optimal formulations. Examples of this include Kahneman and Tversky's (1979) \"prospect theory\" and Tversky's (1972) \"elimination by aspects model\". Less formal approaches include Tversky and Kahneman's seminal work on heuristics and biases, Gigerenzer's work on 'fast and frugal' shortcuts (Gigerenzer, Todd, & ABC Research Group, 2000), and the descriptive models of Paune, Bettman, and Johnson (1993) on adaptive strategies.",
            "score": 63.85945105552673
        },
        {
            "docid": "7214278_2",
            "document": "Decision field theory . Decision field theory (DFT) is a dynamic-cognitive approach to human decision making. It is a cognitive model that describes how people actually make decisions rather than a rational or normative theory that prescribes what people should or ought to do. It is also a dynamic model of decision making rather than a static model, because it describes how a person's preferences evolve across time until a decision is reached rather than assuming a fixed state of preference. The preference evolution process is mathematically represented as a stochastic process called a diffusion process. It is used to predict how humans make decisions under uncertainty, how decisions change under time pressure, and how choice context changes preferences. This model can be used to predict not only the choices that are made but also decision or response times.",
            "score": 89.40885019302368
        },
        {
            "docid": "56345675_12",
            "document": "Predation risk allocation hypothesis . The predation risk allocation hypothesis can help researchers learn how animals make behavioural responses to predators, since it is the first research that observes temporal variation in different risk situations. Animals' responses to predators can be better understood by observing behaviour adjustments to modified risk levels. The hypothesis however, does not explain behaviour in all types of variable risk situations, since this concept assumes that risk levels in every environment will change over time. The risk allocation hypothesis best supports observations of animal behaviour for those animals that developed and evolved in the same environments where they received information about that zone's local predators. These animals would therefore be most informed on what to expect and how to react in their environments. Animals that are exposed to risky situations i.e. predation, more frequently, may demonstrate similar behaviours in both high-risk and safe situations due to habituation. These animals become used to the constant threat and therefore would not act the same compared to animals who are not used to high-risk situations since they have become more immune to these instances.",
            "score": 82.63808274269104
        },
        {
            "docid": "3869283_30",
            "document": "Dual inheritance theory . In DIT, the evolution of culture is dependent on the evolution of social learning. Analytic models show that social learning becomes evolutionarily beneficial when the environment changes with enough frequency that genetic inheritance can not track the changes, but not fast enough that individual learning is more efficient. For environments that have very little variability, social learning is not needed since genes can adapt fast enough to the changes that occur, and innate behaviour is able to deal with the constant environment. In fast changing environments cultural learning would not be useful because what the previous generation knew is now outdated and will provide no benefit in the changed environment, and hence individual learning is more beneficial. It is only in the moderately changing environment where cultural learning becomes useful since each generation shares a mostly similar environment but genes have insufficient time to change to changes in the environment. While other species have social learning, and thus some level of culture, only humans, some birds and chimpanzees are known to have cumulative culture. Boyd and Richerson argue that the evolution of cumulative culture depends on observational learning and is uncommon in other species because it is ineffective when it is rare in a population. They propose that the environmental changes occurring in the Pleistocene may have provided the right environmental conditions. Michael Tomasello argues that cumulative cultural evolution results from a ratchet effect that began when humans developed the cognitive architecture to understand others as mental agents. Furthermore, Tomasello proposed in the 80s that there are some disparities between the observational learning mechanisms found in humans and great apes - which go some way to explain the observable difference between great ape traditions and human types of culture (see Emulation (observational learning)).",
            "score": 136.27143895626068
        },
        {
            "docid": "39377992_9",
            "document": "Reduced muscle mass, strength and performance in space . Animal studies, conducted both during spaceflight and in ground-based simulations of the skeletal muscle unloading associated with spaceflight, have contributed to the scientific knowledge base in a manner not totally achievable by means of human spaceflight and ground-based analog studies alone. This is because many of the variables present with human subject investigations can be more tightly controlled in animal studies, and the much larger number of animals typical of such experiments contributes to a greater statistical power to detect differences. A major advantage in use of rodent models is that the adaptive changes to both spaceflight and hind-limb suspension occur in a much shorter time frame than they do in humans (hours to days versus days to weeks). This enables prediction of long-term changes in human skeletal muscle based on the shorter absolute time frame of the rodent investigations. Additionally, it is possible to perform a highly controlled, straightforward experiment in rodents without a requirement to provide some type of countermeasure intervention that introduces a confounding variable. In human studies, it is not possible on ethical grounds to withhold countermeasures known to have some degree of effectiveness to provide a population of true control subjects, in which only the effects of spaceflight are seen, for comparison to subjects utilizing countermeasures modalities. Animal studies do not suffer from such restrictions. Further work is needed to provide a better understanding of the problem, which will allow novel approaches to countering loss of skeletal muscle function associated with spaceflight in humans. Relevant animal spaceflight studies, as well as investigations using muscle unloading paradigms that contribute to our current knowledge base, are presented.",
            "score": 86.74511420726776
        }
    ],
    "r": [
        {
            "docid": "39182600_2",
            "document": "Heterosynaptic plasticity . Synaptic plasticity refers to a chemical synapse's ability to undergo changes in strength. Synaptic plasticity is typically input-specific (i.e. homosynaptic plasticity), meaning that the activity in a particular neuron alters the efficacy of a synaptic connection between that neuron and its target. However, in the case of heterosynaptic plasticity, the activity of a particular neuron leads to input unspecific changes in the strength of synaptic connections from other unactivated neurons. A number of distinct forms of heterosynaptic plasticity have been found in a variety of brain regions and organisms. These different forms of heterosynaptic plasticity contribute to a variety of neural processes including associative learning, the development of neural circuits, and homeostasis of synaptic input.",
            "score": 179.1058807373047
        },
        {
            "docid": "21944_34",
            "document": "Nervous system . One very important subset of synapses are capable of forming memory traces by means of long-lasting activity-dependent changes in synaptic strength. The best-known form of neural memory is a process called long-term potentiation (abbreviated LTP), which operates at synapses that use the neurotransmitter glutamate acting on a special type of receptor known as the NMDA receptor. The NMDA receptor has an \"associative\" property: if the two cells involved in the synapse are both activated at approximately the same time, a channel opens that permits calcium to flow into the target cell. The calcium entry initiates a second messenger cascade that ultimately leads to an increase in the number of glutamate receptors in the target cell, thereby increasing the effective strength of the synapse. This change in strength can last for weeks or longer. Since the discovery of LTP in 1973, many other types of synaptic memory traces have been found, involving increases or decreases in synaptic strength that are induced by varying conditions, and last for variable periods of time. The reward system, that reinforces desired behaviour for example, depends on a variant form of LTP that is conditioned on an extra input coming from a reward-signalling pathway that uses dopamine as neurotransmitter. All these forms of synaptic modifiability, taken collectively, give rise to neural plasticity, that is, to a capability for the nervous system to adapt itself to variations in the environment.",
            "score": 175.79502868652344
        },
        {
            "docid": "39182600_13",
            "document": "Heterosynaptic plasticity . A neural network that undergoes plastic changes between synapses must initiate normalization mechanisms in order to combat unrestrained potentiation or depression. One mechanism assures that the average firing rate of these neurons is kept at a reasonable rate through synaptic scaling. In this process, input levels are changed in cells to maintain average firing rate. For example, inhibitory synapses are strengthened or excitatory synapses are weakened to normalize the neural network and allow single neurons to regulate their firing rate. Another mechanism is the cell-wide redistribution of synaptic weight. This mechanism conserves the total synaptic weight across the cell by introducing competition between synapses. Thus, normalizing a single neuron after plasticity. During development, cells can be refined when some synapses are preserved and others are discarded to normalize total synaptic weight. In this way, homeostasis is conserved in cells that are undergoing plasticity and normal operation of learning networks is also preserved, allowing new information to be learned.",
            "score": 173.2769317626953
        },
        {
            "docid": "21445461_4",
            "document": "Nonsynaptic plasticity . Synaptic plasticity is the ability of a synapse between two neurons to change in strength over time. Synaptic plasticity is caused by changes in use of the synaptic pathway, namely, the frequency of synaptic potentials and the receptors used to relay chemical signals. Synaptic plasticity plays a large role in learning and memory in the brain. Synaptic plasticity can occur through intrinsic mechanisms, in which changes in synapse strength occur because of its own activity, or through extrinsic mechanisms, in which the changes in synapse strength occur via other neural pathways. Short-term inhibitory synaptic plasticity often occurs because of limited neurotransmitter supply at the synapse, and long-term inhibition can occur through decreased receptor expression in the postsynaptic cell. Short-term complementary synaptic plasticity often occurs because of residual or increased ion flow in either the presynaptic or postsynaptic terminal, while long-term synaptic plasticity can occur through the increased production of AMPA and NMDA glutamate receptors, among others, in the postsynaptic cell.",
            "score": 172.28839111328125
        },
        {
            "docid": "52588865_9",
            "document": "William T. Greenough . This view of brain structure, neural activity, and learning was completely overturned by Greenough's research. Greenough initially worked with mice and rat models, later studying primates and humans. His studies demonstrated that fundamental physical changes occurred in neurons in the brain in response to stimulating environments. At the most basic cellular level, the brains of rats that lived in stimulating environments developed more synapses than those that did not. He went on to demonstrate that new synapses were formed as a result of activities that involved learning, not just increased activity. Moreover, changes occurred in areas of the brain that were associated with the performance of specific learned tasks. Observed changes in learning, memory, and synapse formation persisted after training. Learning and memory formation were therefore fundamentally related to ongoing synapse formation. The result of Greenough's work has been a new model of brain 'plasticity' in which long-term memories are formed at a structural level in the brain as part of lifelong processes of learning.",
            "score": 171.41796875
        },
        {
            "docid": "25253854_4",
            "document": "Developmental plasticity . The underlying principle of synaptic plasticity is that synapses undergo an activity-dependent and selective strengthening or weakening so new information can be stored. Synaptic plasticity depends on numerous factors including the threshold of the presynaptic stimulus in addition to the relative concentrations of neurotransmitter molecules. Synaptic plasticity has long been implicated for its role in memory storage and is thought to play a key role in learning. However, during developmental periods synaptic plasticity is of particular importance as changes in the network of synaptic connections can ultimately lead to changes in developmental milestones. For instance, the initial overproduction of synapses during development is key to plasticity that occurs in the visual and auditory cortex. In experiments conducted by Hubel and Wiesel, the visual cortex of kittens exhibits synaptic plasticity in the refinement neural connections following visual inputs. Correspondingly, in the absence of such inputs during development, the visual field fails to develop properly and can lead to abnormal structures and behavior. Furthermore, research suggests that this initial overproduction of synapses during developmental periods provides the foundation by which many synaptic connections can be formed, thus resulting in more synaptic plasticity. In the same way that synapses are abundant during development, there are also refining mechanisms that coincidentally refine the connectivity of neural circuits. This regulatory process allows the strengthening of important or frequently used synaptic connections while reducing the amount of weak connections.",
            "score": 169.0078887939453
        },
        {
            "docid": "20510214_13",
            "document": "Activity-dependent plasticity . Activity-dependent plasticity plays a very important role in learning and in the ability of understanding new things. It is responsible for helping to adapt an individual's brain according to the relative amount of usage and functioning. In essence, it is the brain's ability to retain and develop memories based on activity-driven changes of synaptic strength that allow stronger learning of information. It is thought to be the growing and adapting quality of dendritic spines that provide the basis for synaptic plasticity connected to learning and memory. Dendritic spines accomplish this by transforming synaptic input into neuronal output and also by helping to define the relationship between synapses.",
            "score": 166.6485137939453
        },
        {
            "docid": "27122321_14",
            "document": "Synaptic tagging . Synaptic tagging is likely to involve the acquisition of molecular maintenance mechanisms by a synapse that would then allow for the conservation of synaptic changes. There are several proposed processes through which synaptic tagging functions. One model suggests that the tag allows for local protein synthesis at the specified synapse that then leads to modifications in synaptic strength. One example of this suggested mechanism involves the anchoring of PKMzeta mRNA to the tagged synapse. This anchor would then restrict the activity of translated PKMzeta, an important plasticity related protein, to this location. A different model proposes that short-term synaptic changes induced by the stimulus are themselves the tag; subsequently delivered or translated protein products act to strengthen this change. For example, the removal of AMPA receptors due to low-frequency stimulation leading to LTD is stabilized by a new protein product that would be inactive at synapses where internalization had not occurred. The tag could also be a latent memory trace, as another model suggests. The activity of proteins would then be required for the memory trace to lead to sustained changes in synaptic strength. According to this model, changes induced by the latent memory trace, such as the growth of new filipodia, are themselves the tag. These tags require protein products for stabilization, synapse formation, and synapse stabilization. Finally, another model proposes that the required molecular products get directed into the appropriate dendritic branches and then find the specific synapses under efficacy modification, by following Ca++ microconcentration gradients through voltage-gated Ca++ channels.",
            "score": 163.83201599121094
        },
        {
            "docid": "423771_11",
            "document": "Synaptic plasticity . If the strength of a synapse is only reinforced by stimulation or weakened by its lack, a positive feedback loop will develop, causing some cells never to fire and some to fire too much. But two regulatory forms of plasticity, called scaling and metaplasticity, also exist to provide negative feedback. Synaptic scaling is a primary mechanism by which a neuron is able to stabilize firing rates up or down. Synaptic scaling serves to maintain the strengths of synapses relative to each other, lowering amplitudes of small excitatory postsynaptic potentials in response to continual excitation and raising them after prolonged blockage or inhibition. This effect occurs gradually over hours or days, by changing the numbers of NMDA receptors at the synapse (P\u00e9rez-Ota\u00f1o and Ehlers, 2005). Metaplasticity varies the threshold level at which plasticity occurs, allowing integrated responses to synaptic activity spaced over time and preventing saturated states of LTP and LTD. Since LTP and LTD (long-term depression) rely on the influx of Ca through NMDA channels, metaplasticity may be due to changes in NMDA receptors, altered calcium buffering, altered states of kinases or phosphatases and a priming of protein synthesis machinery. Synaptic scaling is a primary mechanism by which a neuron to be selective to its varying inputs. The neuronal circuitry affected by LTP/LTD and modified by scaling and metaplasticity leads to reverberatory neural circuit development and regulation in a Hebbian manner which is manifested as memory, whereas the changes in neural circuitry, which begin at the level of the synapse, are an integral part in the ability of an organism to learn.",
            "score": 161.57887268066406
        },
        {
            "docid": "21445461_7",
            "document": "Nonsynaptic plasticity . Although much more is known about the role of synaptic plasticity in memory and learning, both synaptic and nonsynaptic plasticity are essential to memory and learning in the brain. There is much evidence that the two mechanisms both work to achieve the observed effects synergistically. A key example of this is memory formation in the synapse, in which modification of presynaptic release mechanisms and postsynaptic receptors affects either long-term potentiation or depression. Continuous somal depolarization, on the other hand, has been proposed as a method for learned behavior and memory by nonsynaptic plasticity. Nonsynaptic plasticity also augments the effectiveness of synaptic memory formation by regulation of voltage-gated ion channels. Nonsynaptic plasticity is the mechanism responsible for modifications of these channels in the axon, leading to a change in strength of the neuronal action potential, invariably affecting the strength of synaptic mechanisms, and thus the depth and length of memory encoding.  Nonsynaptic plasticity also has the ability to regulate the effects of synaptic plasticity through negative feedback mechanisms. Change in the number and properties of ion channels in the axon or dendrites has the ability to diminish the effects of a hyperstimulated synapse. In the case of extreme overexcitation of these ion channels, backwards flow of ions into the cell will occur, leading to excitotoxicity and cell death by apoptosis or necrosis.",
            "score": 159.2431182861328
        },
        {
            "docid": "613539_5",
            "document": "Spike-timing-dependent plasticity . Early experiments on associative plasticity were carried out by W. B. Levy and O. Steward in 1983 and examined the effect of relative timing of pre and postsynaptic action potentials at millisecond level on plasticity. Bruce McNaughton contributed much to this area, too. In studies on neuromuscular synapses carried out by Y. Dan and Mu-ming Poo in 1992, and on the hippocampus by D. Debanne, B. G\u00e4hwiler, and S. Thompson in 1994, showed that asynchronous pairing of postsynaptic and synaptic activity induced long-term synaptic depression. However, STDP was more definitively demonstrated by Henry Markram in his postdoc period till 1993 in Bert Sakmann's lab (SFN and Phys Soc abstracts in 1994\u20131995) which was only published in 1997. C. Bell and co-workers also found a form of STDP in the cerebellum. Henry Markram used dual patch clamping techniques to repetitively activate pre-synaptic neurons 10 milliseconds before activating the post-synaptic target neurons, and found the strength of the synapse increased. When the activation order was reversed so that the pre-synaptic neuron was activated 10 milliseconds after its post-synaptic target neuron, the strength of the pre-to-post synaptic connection decreased. Further work, by Guoqiang Bi, Li Zhang, and Huizhong Tao in Mu-Ming Poo's lab in 1998, continued the mapping of the entire time course relating pre- and post-synaptic activity and synaptic change, to show that in their preparation synapses that are activated within 5-20 ms before a postsynaptic spike are strengthened, and those that are activated within a similar time window after the spike are weakened. This phenomenon has been observed in various other preparations, with some variation in the time-window relevant for plasticity. Several reasons for timing-dependent plasticity have been suggested. For example, STDP might provide a substrate for Hebbian learning during development, or, as suggested by Taylor in 1973, the associated Hebbian and anti-Hebbian learning rules might create informationally efficient coding in bundles of related neurons. Works from Y. Dan's lab advanced to study STDP in \"in vivo\" systems.",
            "score": 157.9712677001953
        },
        {
            "docid": "20510214_18",
            "document": "Activity-dependent plasticity . Another research model of activity-dependent plasticity includes the excitatory corticostriatal pathway that is involved in information processing related to adaptive motor behaviors and displays long-lasting synaptic changes. The change in synaptic strength is responsible for motor learning and is dependent on the simultaneous activation of glutamatergic corticostriatal and dopaminergic nigrostriatal pathways. These are the same pathways affected in Parkinson's disease, and the degeneration of synapses within this disorder may be responsible for the loss of some cognitive abilities.",
            "score": 156.5060577392578
        },
        {
            "docid": "5128182_18",
            "document": "Encoding (memory) . \"Synaptic plasticity\" is the ability of the brain to strengthen, weaken, destroy and create neural synapses and is the basis for learning. These molecular distinctions will identify and indicate the strength of each neural connection. The effect of a learning experience depends on the content of such an experience. Reactions that are favoured will be reinforced and those that are deemed unfavourable will be weakened. This shows that the synaptic modifications that occur can operate either way, in order to be able to make changes over time depending on the current situation of the organism. In the short term, synaptic changes may include the strengthening or weakening of a connection by modifying the preexisting proteins leading to a modification in synapse connection strength. In the long term, entirely new connections may form or the number of synapses at a connection may be increased, or reduced.",
            "score": 156.4170379638672
        },
        {
            "docid": "27809_18",
            "document": "Chemical synapse . The strength of a synapse has been defined by Sir Bernard Katz as the product of (presynaptic) release probability \"pr\", quantal size \"q\" (the postsynaptic response to the release of a single neurotransmitter vesicle, a 'quantum'), and \"n\", the number of release sites. \"Unitary connection\" usually refers to an unknown number of individual synapses connecting a presynaptic neuron to a postsynaptic neuron.  The amplitude of postsynaptic potentials (PSPs) can be as low as 0.4mV to as high as 20mV. The amplitude of a PSP can be modulated by neuromodulators or can change as a result of previous activity. Changes in the synaptic strength can be short-term, lasting seconds to minutes, or long-term (long-term potentiation, or LTP), lasting hours. Learning and memory are believed to result from long-term changes in synaptic strength, via a mechanism known as synaptic plasticity.",
            "score": 155.38638305664062
        },
        {
            "docid": "5064345_2",
            "document": "Metaplasticity . Metaplasticity is a term originally coined by W.C. Abraham and M.F. Bear to refer to the plasticity of synaptic plasticity. Until that time synaptic plasticity had referred to the plastic nature of \"individual\" synapses. However this new form referred to the plasticity of the plasticity itself, thus the term \"meta\"-plasticity. The idea is that the synapse's previous history of activity determines its current plasticity. This may play a role in some of the underlying mechanisms thought to be important in memory and learning such as long-term potentiation (LTP), long-term depression (LTD) and so forth. These mechanisms depend on current synaptic \"state\", as set by ongoing extrinsic influences such as the level of synaptic inhibition, the activity of modulatory afferents such as catecholamines, and the pool of hormones affecting the synapses under study. Recently, it has become clear that the prior history of synaptic activity is an additional variable that influences the synaptic state, and thereby the degree, of LTP or LTD produced by a given experimental protocol. In a sense, then, synaptic plasticity is governed by an activity-dependent plasticity of the synaptic state; such plasticity of synaptic plasticity has been termed metaplasticity. There is little known about metaplasticity, and there is much research currently underway on the subject, despite its difficulty of study, because of its theoretical importance in brain and cognitive science. Most research of this type is done via cultured hippocampus cells or hippocampal slices.",
            "score": 154.98545837402344
        },
        {
            "docid": "33822344_2",
            "document": "Synaptic scaling . In neuroscience, synaptic scaling (or homeostatic scaling) is a form of homeostatic plasticity, in which the brain responds to chronically elevated activity in a neural circuit with negative feedback, allowing individual neurons to reduce their overall action potential firing rate. Where Hebbian plasticity mechanisms modify neural synaptic connections selectively, synaptic scaling normalizes all neural synaptic connections by decreasing the strength of each synapse by the same factor (multiplicative change), so that the relative synaptic weighting of each synapse is preserved.",
            "score": 152.48487854003906
        },
        {
            "docid": "27122321_15",
            "document": "Synaptic tagging . While the information gained on the synaptic tagging hypothesis mainly resulted from experiments that apply stimulation to synapse, a similar model can be applied when considering the process of learning in a broader behavioral sense. Fabricio Ballarini and colleagues created this behavioral tagging model by testing spatial object recognition, contextual conditioning, and conditioned taste aversion in rats with weak training, which is a training that normal only creates a short term memory. However, they paired this weak training with a separate, arbitrary behavioral event that induces protein synthesis and found that so long as the two behavioral events were coupled within a certain time frame, the weak training was sufficient to produce long term memory of that learning task. The researchers believed that the weak learning established a \"learning tag\" to be used later when proteins arrived as a result of the other task, resulting in the formation of long-term memory for even the weak training. This behavioral learning model mirrors the synaptic tagging model, in which a weak stimulation establishes E-LTP that may be serve as the tag used in converting the weak potentiation to the stronger, more persistent L-LTP, once the high-frequency stimulation presents itself.",
            "score": 152.37831115722656
        },
        {
            "docid": "1678822_50",
            "document": "Perceptual control theory . LTP has received much support since it was first observed by Terje L\u00f8mo in 1966 and is still the subject of many modern studies and clinical research. However, there are possible alternative mechanisms underlying LTP, as presented by Enoki, Hu, Hamilton and Fine in 2009, published in the journal \"Neuron\". They concede that LTP is the basis of learning. However, they firstly propose that LTP occurs in individual synapses, and this plasticity is graded (as opposed to in a binary mode) and bidirectional (Enoki et al., 2009). Secondly, the group suggest that the synaptic changes are expressed solely presynaptically, via changes in the probability of transmitter release (Enoki et al., 2009). Finally, the team predict that the occurrence of LTP could be age-dependent, as the plasticity of a neonatal brain would be higher than that of a mature one. Therefore, the theories differ, as one proposes an on/off occurrence of LTP by pre- and postsynaptic mechanisms and the other proposes only presynaptic changes, graded ability, and age-dependence.",
            "score": 146.56727600097656
        },
        {
            "docid": "21312297_5",
            "document": "Memory consolidation . Synaptic consolidation, or late-phase LTP, is one form of memory consolidation seen across all species and long-term memory tasks. Long-term memory, when discussed in the context of synaptic consolidation, is memory that lasts for at least 24 hours. An exception to this 24-hour rule is long-term potentiation, or LTP, a model of synaptic plasticity related to learning, in which an hour is thought to be sufficient. Synaptic consolidation is achieved faster than systems consolidation, within only minutes to hours of learning in goldfish. LTP, one of the best understood forms of synaptic plasticity, is thought to be a possible underlying process in synaptic consolidation.",
            "score": 145.346435546875
        },
        {
            "docid": "20848680_10",
            "document": "Cellular neuroscience . Synaptic plasticity is the process whereby strengths of synaptic connections are altered. For example, long-term changes in synaptic connection may result in more postsynaptic receptors being embedded in the postsynaptic membrane, resulting in the strengthening of the synapse. Synaptic plasticity is also found to be the neural mechanism that underlies learning and memory. The basic properties, activity and regulation of membrane currents, synaptic transmission and synaptic plasticity, neurotransmisson, neuroregensis, synaptogenesis and ion channels of cells are a few other fields studied by cellular neuroscientists. Tissue, cellular and subcellular anatomy are studied to provide insight into mental retardation at the Mental Retardation Research Center MRRC Cellular Neuroscience Core. Journals such as \"Frontiers in Cellular Neuroscience\" and \"Molecular and Cellular Neuroscience\" are published regarding cellular neuroscientific topics.",
            "score": 144.65432739257812
        },
        {
            "docid": "5064345_3",
            "document": "Metaplasticity . The brain is \"plastic\", meaning it can be molded and formed. This plasticity is what allows you to learn throughout your lifetime; your synapses change based on your experience. New synapses can be made, old ones destroyed, or existing ones can be strengthened or weakened. The original theory of plasticity is called \"Hebbian plasticity\", named after Donald Hebb in 1949. A quick but effective summary of Hebbian theory is that \"cells that fire together, wire together\", together being the key word here which will be explained shortly. Hebb described an early concept of the theory, not the actual mechanics themselves. Hebbian plasticity involves two mechanisms: LTP and LTD, discovered by Bliss and Lomo in 1973. LTP, or long-term potentiation, is the increase of synapse sensitivity due to a prolonged period of activity in both the presynaptic and postsynaptic neuron. This prolonged period of activity is normally concentrated electric impulses, usually around 100\u00a0Hz. It is called \"coincidence\" detection in that it only strengthens the synapse if there was sufficient activity in both the presynaptic and postsynaptic cells. If the postsynaptic cell does not become sufficiently depolarized then there is no coincidence detection and LTP/LTD do not occur. LTD, or long-term depression, works the same way however it focuses on a lack of depolarization coincidence. LTD can be induced by electrical impulses at around 5\u00a0Hz. These changes are synapse specific. A neuron can have many different synapses all controlled via the same mechanisms defined here.",
            "score": 144.5358428955078
        },
        {
            "docid": "50397_4",
            "document": "Cerebellum . In addition to its direct role in motor control, the cerebellum is necessary for several types of motor learning, most notably learning to adjust to changes in sensorimotor relationships. Several theoretical models have been developed to explain sensorimotor calibration in terms of synaptic plasticity within the cerebellum. These models derive from those formulated by David Marr and James Albus, based on the observation that each cerebellar Purkinje cell receives two dramatically different types of input: one comprises thousands of weak inputs from the parallel fibers of the granule cells; the other is an extremely strong input from a single climbing fiber. The basic concept of the Marr\u2013Albus theory is that the climbing fiber serves as a \"teaching signal\", which induces a long-lasting change in the strength of parallel fiber inputs. Observations of long-term depression in parallel fiber inputs have provided support for theories of this type, but their validity remains controversial.",
            "score": 143.55096435546875
        },
        {
            "docid": "25253854_6",
            "document": "Developmental plasticity . While synaptic plasticity is considered to be a by-product of learning, learning requires interaction with the environment to acquire the new information or behavior, whereas synaptic plasticity merely represents the change in strength or configuration of neural circuits. Learning is of crucial importance postnatally as there is considerable interaction with the environment and the potential for acquiring new information is greatest. By depending largely upon selective experiences, neural connections are altered and strengthened in a manner that is unique to those experiences. Experimentally this can be seen when rats are raised in an environment that allows ample social interaction, resulting in increased brain weight and cortical thickness. In contrast, the adverse is seen following rearing in an environment devoid of interaction. Also, learning plays a sizeable role in the selective acquisition of information and is markedly demonstrated as children develop one language as opposed to another. Another example of such experience dependent plasticity that is critical during development is the occurrence of imprinting. This occurs as a result of the young child or animal experiencing a novel stimuli and rapidly learning the behavior in response.",
            "score": 143.08560180664062
        },
        {
            "docid": "1726672_5",
            "document": "Neural circuit . On the electrophysiological level, there are various phenomena which alter the response characteristics of individual synapses (called synaptic plasticity) and individual neurons (intrinsic plasticity). These are often divided into short-term plasticity and long-term plasticity. Long-term synaptic plasticity is often contended to be the most likely memory substrate. Usually the term \"neuroplasticity\" refers to changes in the brain that are caused by activity or experience.",
            "score": 142.73519897460938
        },
        {
            "docid": "10459803_12",
            "document": "Neuron (software) . Neuron allows for the generation of mixed models, populated with both artificial cells and neurons. Artificial cells essentially function as point processes, implemented into the network. Artificial cells require only a point process, with defined parameters. The user can create the structure and dynamics of network cells. The user can create synapses, using simulated synapse point processes as archetypes. Parameters on these point processes can be manipulated to simulate both inhibitory and excitatory responses. Synapses can be placed on specific segments of the constructed cell, wherein, again, they will behave as point processes, except that they are sensitive to the activity of a pre-synaptic element. Cells can be managed. The user creates the basic grid of network cells, taking previously completed network cells as archetypes. Connections can be defined between source cells and target synapses on other cells. The cell containing the target synapse becomes the post-synaptic element, whereas the source cells function as pre-synaptic elements. Weights can be added to define strength of activation of a synapse by the pre-synaptic cell. A plot option can be activated to open a graph of spikes across time for individual neurons.",
            "score": 142.66136169433594
        },
        {
            "docid": "162435_25",
            "document": "Mind uploading . Since learning and long-term memory are believed to result from strengthening or weakening the synapses via a mechanism known as synaptic plasticity or synaptic adaptation, the model should include this mechanism. The response of sensory receptors to various stimuli must also be modelled.",
            "score": 142.58050537109375
        },
        {
            "docid": "4669202_3",
            "document": "Postsynaptic density . The structure and composition of the PSD have been the focus of numerous molecular studies of synaptic plasticity, a cellular model of learning and memory. PSDs are sized on the order of 250 to 500 nanometres in diameter and 25 to 50 nanometres in thickness, depending on the activity state of the synapse. During synaptic plasticity, the total size of the PSD is increasing along with an increase in synaptic size and strength after inducing long-term potentiation at single synapses.",
            "score": 142.26014709472656
        },
        {
            "docid": "404084_13",
            "document": "Hebbian theory . Experiments on Hebbian synapse modification mechanisms at the central nervous system synapses of vertebrates are much more difficult to control than are experiments with the relatively simple peripheral nervous system synapses studied in marine invertebrates. Much of the work on long-lasting synaptic changes between vertebrate neurons (such as long-term potentiation) involves the use of non-physiological experimental stimulation of brain cells. However, some of the physiologically relevant synapse modification mechanisms that have been studied in vertebrate brains do seem to be examples of Hebbian processes. One such study reviews results from experiments that indicate that long-lasting changes in synaptic strengths can be induced by physiologically relevant synaptic activity working through both Hebbian and non-Hebbian mechanisms.",
            "score": 141.99598693847656
        },
        {
            "docid": "3717_9",
            "document": "Brain . Axons transmit signals to other neurons by means of specialized junctions called synapses. A single axon may make as many as several thousand synaptic connections with other cells. When an action potential, traveling along an axon, arrives at a synapse, it causes a chemical called a neurotransmitter to be released. The neurotransmitter binds to receptor molecules in the membrane of the target cell. Synapses are the key functional elements of the brain. The essential function of the brain is cell-to-cell communication, and synapses are the points at which communication occurs. The human brain has been estimated to contain approximately 100 trillion synapses; even the brain of a fruit fly contains several million. The functions of these synapses are very diverse: some are excitatory (exciting the target cell); others are inhibitory; others work by activating second messenger systems that change the internal chemistry of their target cells in complex ways. A large number of synapses are dynamically modifiable; that is, they are capable of changing strength in a way that is controlled by the patterns of signals that pass through them. It is widely believed that activity-dependent modification of synapses is the brain's primary mechanism for learning and memory.",
            "score": 141.8062286376953
        },
        {
            "docid": "20510214_11",
            "document": "Activity-dependent plasticity . Cell adhesion molecules (CAMs) are also important in plasticity as they help coordinate the signaling across the synapse. More specifically, integrins, which are receptors for extracellular matrix proteins and involved with CAMs, are explicitly incorporated in synapse maturation and memory formation. They play a crucial role in the feedback regulation of excitatory synaptic strength, or long-term potentiation (LTP), and help to control synaptic strength by regulating AMPA receptors, which result in quick, short synaptic currents. But, it is the metabotropic glutamate receptor 1 (mGlu1) that has been discovered to be required for activity-dependent synaptic plasticity in associative learning.",
            "score": 141.03993225097656
        },
        {
            "docid": "21445461_2",
            "document": "Nonsynaptic plasticity . Nonsynaptic plasticity is a form of neuroplasticity that involves modification of ion channel function in the axon, dendrites, and cell body that results in specific changes in the integration of excitatory postsynaptic potentials (EPSPs) and inhibitory postsynaptic potentials (IPSPs). Nonsynaptic plasticity is a modification of the intrinsic excitability of the neuron. It interacts with synaptic plasticity, but it is considered a separate entity from synaptic plasticity. Intrinsic modification of the electrical properties of neurons plays a role in many aspects of plasticity from homeostatic plasticity to learning and memory itself. Nonsynaptic plasticity affects synaptic integration, subthreshold propagation, spike generation, and other fundamental mechanisms of neurons at the cellular level. These individual neuronal alterations can result in changes in higher brain function, especially learning and memory. However, as an emerging field in neuroscience, much of the knowledge about nonsynaptic plasticity is uncertain and still requires further investigation to better define its role in brain function and behavior.",
            "score": 140.8144989013672
        },
        {
            "docid": "423771_12",
            "document": "Synaptic plasticity . There is also a specificity element of biochemical interactions to create synaptic plasticity, namely the importance of location. Processes occur at microdomains \u2013 such as exocytosis of AMPA receptors is spatially regulated by the t-SNARE STX4. Specificity is also an important aspect of CAMKII signaling involving nanodomain calcium. The spatial gradient of PKA between dendritic spines and shafts is also important for the strength and regulation of synaptic plasticity. It is important to remember that the biochemical mechanisms altering synaptic plasticity occur at the level of individual synapses of a neuron. Since the biochemical mechanisms are confined to these \"microdomains,\" the resulting synaptic plasticity affects only the specific synapse at which it took place.",
            "score": 140.45872497558594
        }
    ]
}