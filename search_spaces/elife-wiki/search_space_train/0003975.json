{
    "q": [
        {
            "docid": "52992700_2",
            "document": "List of manual image annotation tools . Manual image annotation is the process of manually defining regions in an image and creating a textual description of those regions. This is a list of computer software which can be used for manual annotation of images.",
            "score": 41.45856285095215
        },
        {
            "docid": "44674478_13",
            "document": "Rigid motion segmentation . It is a very useful technique for detecting changes in images due to its simplicity and ability to deal with occlusion and multiple motions. These techniques assume constant light source intensity. The algorithm first considers two frames at a time and then computes the pixel by pixel intensity difference. On this computation it thresholds the intensity difference and maps the changes onto a contour. Using this contour it extracts the spatial and temporal information required to define the motion in the scene. Though it is a simple technique to implement it is not robust to noise. Another difficulty with these techniques is the camera movement. When the camera moves there is a change in the entire image which has to be accounted for. Many new algorithm have been introduced to overcome these difficulties. Motion segmentation can be seen as a classification problem where each pixel has to be classified as background or foreground. Such classifications are modeled under statistic theory and can be used in segmentation algorithms. These approaches can be further divided depending on the statistical framework used. Most commonly used frameworks are maximum a posteriori probability (MAP), Particle Filter (PF) and Expectation Maximization (EM). MAP uses Bayes' Rule for implementation where a particular pixel has to be classified under predefined classes. PF is based on the concept of evolution of a variable with varying weights over time. The final estimation is the weighted sum of all the variables. Both of these methods are iterative. The EM algorithm is also an iterative estimation method. It computes the maximum likelihood (ML) estimate of the model parameters in presence of missing or hidden data and decided the most likely fit of the observed data.",
            "score": 42.35065770149231
        },
        {
            "docid": "23167397_21",
            "document": "GENCODE . The main approach to manual gene annotation is to annotate transcripts aligned to the genome and take the genomic sequences as the reference rather than the cDNAs. The finished genomic sequence is analyzed using a modified Ensembl pipeline, and BLAST results of cDNAs/ESTs and proteins, along with various ab initio predictions, can be analyzed manually in the annotation browser tool Otterlace. Thus, more alternative spliced variants can be predicted compared with cDNA annotation. Moreover, genomic annotation produces a more comprehensive analysis of pseudogenes. There are several analysis groups in the GENCODE consortium that run pipelines that aid the manual annotators in producing models in unannotated regions, and to identify potential missed or incorrect manual annotation, including completely missing loci, missing alternative isoforms, incorrect splice sites and incorrect biotypes. These are fed back to the manual annotators using the AnnoTrack tracking system. Some of these pipelines use data from other ENCODE subgroups including RNASeq data, histone modification and CAGE and Ditag data. RNAseq data is an important new source of evidence, but generating complete gene models from it is a difficult problem. As part of GENCODE, a competition was run to assess the quality of predictions produced by various RNAseq prediction pipelines (Refer to RGASP below). To confirm uncertain models, GENCODE also has an experimental validation pipeline using RNA sequencing and RACE",
            "score": 49.87815761566162
        },
        {
            "docid": "29467449_19",
            "document": "Protein function prediction . Several networks based on different data sources can be combined into a composite network, which can then be used by a prediction algorithm to annotate candidate genes or proteins. For example, the developers of the bioPIXIE system used a wide variety of \"Saccharomyces cerevisiae\" (yeast) genomic data to produce a composite functional network for that species. This resource allows the visualization of known networks representing biological processes, as well as the prediction of novel components of those networks. Many algorithms have been developed to predict function based on the integration of several data sources (e.g. genomic, proteomic, protein interaction, etc.), and testing on previously annotated genes indicates a high level of accuracy. Disadvantages of some function prediction algorithms have included a lack of accessibility, and the time required for analysis. Faster, more accurate algorithms such as GeneMANIA (multiple association network integration algorithm) have however been developed in recent years and are publicly available on the web, indicating the future direction of function prediction.",
            "score": 61.72980606555939
        },
        {
            "docid": "23167397_19",
            "document": "GENCODE . A comparison of key statistics from 3 major GENCODE releases is shown below. It is evident that although the coverage, in terms of total number of genes discovered, is steady increasing, the number of protein-coding genes has actually decreased. This is mostly attributed to new experimental evidence obtained using Cap Analysis Gene Expression (CAGE) clusters, annotated PolyA sites, and peptide hits. The general process to create an annotation for GENCODE involves manual curation, different computational analysis and targeted experimental approaches. Putative loci can be verified by wet-lab experiments and computational predictions are analysed manually. Currently, to ensure a set of annotation covers the complete genome rather than just the regions that have been manually annotated, a merged data set is created using manual annotations from HAVANA, together with automatic annotations from the Ensembl automatically annotated gene set. This process also adds unique full-length CDS predictions from the Ensembl protein coding set into manually annotated genes, to provide the most complete and up-to-date annotation of the genome possible.",
            "score": 66.41379570960999
        },
        {
            "docid": "40791915_6",
            "document": "Imaging particle analysis . Finally, beginning roughly in the late 1970s, CCD digital sensors for capturing images and computers which could process those images, began to revolutionize the process by using digital imaging. Although the actual algorithms for performing digital image processing had been around for some time, it was not until the significant computing power needed to perform these analyses became available at reasonable prices that digital imaging techniques could be brought to bear in the mainstream. The first dynamic imaging particle analysis system was patented in 1982. As faster computing resources became available at lowered costs, the task of making measurements from microscope images of particles could now be performed automatically by machine without human intervention, making it possible to measure significantly larger numbers of particles in much less time.",
            "score": 69.12426090240479
        },
        {
            "docid": "1143981_9",
            "document": "Electrostatic lens . Instead of an electrostatic field we can also use a magnetic field to focus charged particles. The Lorentz force acting on the electron is perpendicular to both the direction of motion, and to the direction of the magnetic field (vxB). A homogeneous field deflects charged particles, but does not focus them. The simplest magnetic lens is a donut-shaped coil through which the beam passes, preferably along the axis of the coil. To generate the magnetic field an electric current is passed through the coil. The magnetic field is strongest in the plane of the coil, and gets weaker as we move away from it. In the plane of the coil, the field gets stronger as we move away from the axis. Thus, a charged particle further from the axis experiences a stronger Lorentz force than a particle closer to the axis (assuming that they have the same velocity). This gives rise to the focusing action. Unlike the paths in an electrostatic lens, the paths in a magnetic lens contain a spiraling component, i.e. the charged particles spiral around the optical axis. As a consequence, the image formed by a magnetic lens is rotated relative to the object. This rotation is absent for an electrostatic lens.  The spatial extent of the magnetic field can be controlled by using an iron (or other magnetically soft material) magnetic circuit. This makes it possible to design and build more compact magnetic lenses with well defined optical properties. The vast majority of electron microscopes in use today use magnetic lenses due to their superior imaging properties, and the absence of the high voltages that are required for electrostatic lenses.",
            "score": 39.35795331001282
        },
        {
            "docid": "29173338_3",
            "document": "Wrapper (data mining) . There are two main approaches to wrapper generation: wrapper induction and automated data extraction. Wrapper induction uses supervised learning to learn data extraction rules from manually labeled training examples. The disadvantages of wrapper induction are Due to the manual labeling effort, it is hard to extract data from a large number of sites as each site has its own templates and requires separate manual labeling for wrapper learning. Wrapper maintenance is also a major issue because whenever a site changes the wrappers built for the site become obsolete. Due to these shortcomings, researchers have studied automated wrapper generation using unsupervised pattern mining. Automated extraction is possible because most Web data objects follow fixed templates. Discovering such templates or patterns enables the system to perform extraction automatically. More recently, the increasing availability of Linked Data has enabled methods that can automatically learn and maintain wrappers using such resources, based on the principle of 'distant supervision' . In this case, sample instances of concepts are firstly collected from publicly available Linked Datasets. These are then searched within a collection of Webpages and their matched occurrences are annotated. Although these annotations can be noisy, they prove to be useful training data for learning Webpage wrappers.",
            "score": 54.39761674404144
        },
        {
            "docid": "24027777_3",
            "document": "Figure Eight Inc. . Figure Eight uses human intelligence to do simple tasks such as transcribing text or annotating images to train machine learning algorithms. Figure Eight's software automates tasks for machine learning algorithms, which can be used to improve catalog search results, approve photos or support customers and the technology can be used in the development of self-driving cars, intelligent personal assistants and other technology that uses machine learning.",
            "score": 40.70894527435303
        },
        {
            "docid": "56612525_18",
            "document": "Commercial augmented reality . AR software should be capable of carrying image registration process where software is working independently from camera and camera images, and it drives real-world coordinates to accomplish the AR process. AR software can achieve augmented reality using two-step methods: It detects Interest Points, fiduciary marker, and optical flows in camera images or videos. Now, it restores the real-world coordinate system from the data collecting in the first step. To restore the real-world coordinates data some methods used to employ such as SLAM (Simultaneous Localization and Mapping), structure from Motion methods including-Bundle Adjustment, and mathematical methods like-Projective or Epipolar Geometry, Geometric Algebra, Rotation representation (with an exponential map, Kalman & particle filters, non-linear optimization, and robust statistics).",
            "score": 26.790592432022095
        },
        {
            "docid": "4366478_3",
            "document": "Nanoparticle tracking analysis . The technique is used in conjunction with an ultramicroscope and a laser illumination unit that together allow small particles in liquid suspension to be visualized moving under Brownian motion. The light scattered by the particles is captured using a CCD or EMCCD camera over multiple frames. Computer software is then used to track the motion of each particle from frame to frame. The rate of particle movement is related to a sphere equivalent hydrodynamic radius as calculated through the Stokes\u2013Einstein equation. The technique calculates particle size on a particle-by particle basis, overcoming inherent weaknesses in ensemble techniques such as dynamic light scattering. Since video clips form the basis of the analysis, accurate characterization of real time events such as aggregation and dissolution is possible. Samples require minimal preparation, minimizing the time required to process each sample. Speculators suggest that eventually the analysis may be done in real-time with no preparation, e.g. when detecting the presence of airborne viruses or biological weapons.",
            "score": 77.39064419269562
        },
        {
            "docid": "35691407_3",
            "document": "Amira (software) . Amira is an extendable software system for scientific visualization, data analysis, and presentation of 3D and 4D data. It is being used by several thousand researchers and engineers in academia and industry around the world. Its flexible user interface and modular architecture make it a universal tool for processing and analysis of data from various modalities; e.g. micro-CT, PET, Ultrasound. Its ever-expanding functionality has made it a versatile data analysis and visualization solution, applicable to and being used in many fields, such as microscopy in biology and materials science, molecular biology, quantum physics, astrophysics, computational fluid dynamics (CFD), finite element modeling (FEM), non-destructive testing (NDT), and many more.  One of the key features, besides data visualization, is Amira\u2019s set of tools for image segmentation and geometry reconstruction. This allows the user to mark (or segment) structures and regions of interest in 3D image volumes using automatic, semi-automatic, and manual tools. The segmentation can then be used for a variety of subsequent tasks, such as volumetric analysis, density analysis, shape analysis, or the generation of 3D computer models for visualization, numerical simulations, or rapid prototyping or 3D printing, to name a few.  Other key Amira features are multi-planar and volume visualization, image registration, filament tracing, cell separation and analysis, tetrahedral mesh generation, fiber-tracking from diffusion tensor imaging (DTI) data, skeletonization, spatial graph analysis, and stereoscopic rendering of 3D data over multiple displays including CAVEs (Cave automatic virtual environments). As a commercial product Amira requires the purchase of a license or an academic subscription. A time-limited, but full-featured evaluation version is available for download free of charge.",
            "score": 100.92158234119415
        },
        {
            "docid": "21652_10",
            "document": "Natural language processing . Recent research has increasingly focused on unsupervised and semi-supervised learning algorithms. Such algorithms are able to learn from data that has not been hand-annotated with the desired answers, or using a combination of annotated and non-annotated data. Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data. However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the inferior results.",
            "score": 34.50001049041748
        },
        {
            "docid": "27837170_12",
            "document": "History of natural language processing . Recent research has increasingly focused on unsupervised and semi-supervised learning algorithms. Such algorithms are able to learn from data that has not been hand-annotated with the desired answers, or using a combination of annotated and non-annotated data. Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data. However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the inferior results.",
            "score": 34.50001049041748
        },
        {
            "docid": "19116930_12",
            "document": "Digital microscope . Devices which connect to a computer require software to operate. Basic operation includes viewing the microscope image and recording \"snapshots\". More advanced functionality, possible even with simpler devices, includes recording moving images, time-lapse photography, measurement, image enhancement, annotation, etc. Many of the simpler units which connect to a computer use standard operating system facilities, and do not require device-specific drivers. A consequence of this is that many different microscope software packages can be used interchangeably with different microscopes, although such software may not support features unique to the more advanced devices. Basic operation may be possible with software included as part of computer operating systems\u2014in Windows XP, images from microscopes which do not require special drivers can be viewed and recorded from \"Scanners and Cameras\" in Control Panel.",
            "score": 40.484602212905884
        },
        {
            "docid": "1931185_3",
            "document": "Automatic image annotation . This method can be regarded as a type of multi-class image classification with a very large number of classes - as large as the vocabulary size. Typically, image analysis in the form of extracted feature vectors and the training annotation words are used by machine learning techniques to attempt to automatically apply annotations to new images. The first methods learned the correlations between image features and training annotations, then techniques were developed using machine translation to try to translate the textual vocabulary with the 'visual vocabulary', or clustered regions known as \"blobs\". Work following these efforts have included classification approaches, relevance models and so on.",
            "score": 62.216129779815674
        },
        {
            "docid": "42556508_2",
            "document": "BacMap . BacMap is a freely available web-accessible database containing fully annotated, fully zoomable and fully searchable chromosome maps from more than 2500 prokaryotic (archaebacterial and eubacterial) species. BacMap was originally developed in 2005 to address the challenges of viewing and navigating through the growing numbers of bacterial genomes that were being generated through large-scale sequencing efforts. Since it was first introduced, the number of bacterial genomes in BacMap has grown by more than 15X. Essentially BacMap functions as an on-line visual atlas of microbial genomes. All of the genome annotations in BacMap were generated through the BASys genome annotation system. BASys is a widely used microbial annotation infrastructure that performs comprehensive bionformatic analyses on raw (or labeled) bacterial genome sequence data. All of the genome (chromosome) maps in BacMap were constructed using the program known as CGView. CGView is a popular visualization program for generating interactive, web-compatible circular chromosome maps (Fig. 1). Each chromosome map in BacMap is extensively hyperlinked and each chromosome image can be interactively navigated, expanded and rotated using navigation buttons or hyperlinks. All identified genes in a BacMap chromosome map are colored according to coding directions and when sufficiently zoomed-in, gene labels are visible. Each gene label on a BacMap genome map is also hyperlinked to a 'gene card' (Fig. 2). The gene cards provide detailed information about the corresponding DNA and protein sequences. Each genome map in BacMap is searchable via BLAST and a gene name/synonym search.",
            "score": 30.61717915534973
        },
        {
            "docid": "4754533_4",
            "document": "Particle counter . Direct imaging particle counting employs the use of a high resolution camera and a light to detect particles. Vision based particle sizing units obtain two dimensional images that are analyzed by computer software to obtain particle size measurement in both the laboratory and online. Along with particle size, color and shape analysis can also be determined.",
            "score": 51.31573438644409
        },
        {
            "docid": "44674478_11",
            "document": "Rigid motion segmentation . Image segmentation techniques are interested in segmenting out different parts of the image as per the region of interest. As videos are sequences of images, motion segmentation aims at decomposing a video in moving objects and background by segmenting the objects that undergo different motion patterns. The analysis of these spatial and temporal changes occurring in the image sequence by separating visual features from the scenes into different groups lets us extract visual information. Each group corresponds to the motion of an object in the dynamic sequence. In the simplest case motion segmentation can mean extracting moving objects from a stationary camera but the camera can also move which introduces the relative motion of the static background. Depending upon the type of visual features that are extracted, motion segmentation algorithms can be broadly divided into two categories. The first is known as direct motion segmentation that uses pixel intensities from the image. Such algorithms assume constant illumination. The second category of algorithms computes a set of features corresponding to actual physical points on the objects. These sparse features are then used to characterize either the 2-D motion of the scene or the 3-D motion of the objects in the scene. There are a number of requirements to design a good motion segmentation algorithm. The algorithm must extract distinct features (corners or salient points) that represent the object by a limited number of points and it must have the ability to deal with occlusions. The images will also be affected by noise and will have missing data, thus they must be robust. Some algorithms detect only one object but the video sequence may have different motions. Thus the algorithm must be multiple object detectors. Moreover, the type of camera model, if used, also characterizes the algorithm. Depending upon the object characterization of an algorithm it can detect rigid, non-rigid motion or both. Moreover, algorithms used to estimate single rigid-body motions can provide accurate results with robustness to noise and outliers but when extended to multiple rigid-body motions they fail. In case of view-based segmentation techniques described below, this happens because the single fundamental matrix assumption is violated as each motion will now be represented by means of a new fundamental matrix corresponding to that motion.",
            "score": 34.846213817596436
        },
        {
            "docid": "19961701_24",
            "document": "Goal-line technology . The Hawk-Eye system was first developed in 1999. Hawk-Eye is an existing technology currently used in cricket, tennis, snooker and Gaelic games being played in Croke Park by the Gaelic Athletic Association. It is based on the principle of triangulation using the visual images and timing data provided by high-speed video cameras at different locations around the area of play. The system uses high frame rate cameras to triangulate and track the ball in flight. The software calculates the ball\u2019s location in each frame by identifying the pixels that correspond to the ball. The software can track the ball and predict the flight path, even if several cameras are being blocked. The system also records the ball's flight path and stores it in a database that is used to create a graphic image of the flight path, so the images can be shown to commentators, coaches and audiences. The data from the system can also be used to determine statistics for players and analyse trends. The proposal involves placing seven cameras for each goal mouth around the stadium. The system is near real-time and referees will be notified on their encrypted watch in less than one second from the ball crossing the line. Critics of the system claim the system will slow down the game and that the statistical margin of error is too large. Both Roger Federer and Rafael Nadal have criticised the accuracy of the system in tennis (though Roger Federer now supports the use of the system in football).",
            "score": 41.730608105659485
        },
        {
            "docid": "1058299_39",
            "document": "Particle image velocimetry . A variety of issues degrade the quality of HPIV results. The first class of issues involves the reconstruction itself. In holography, the object wave of a particle is typically assumed to be spherical; however, due to Mie scattering theory, this wave is a complex shape which can distort the reconstructed particle. Another issue is the presence of substantial speckle noise which lowers the overall signal-to-noise ratio of particle images. This effect is of greater concern for in-line holographic systems because the reference beam is propagated through the volume along with the scattered object beam. Noise can also be introduced through impurities in the scattering medium, such as temperature variations and window blemishes. Because holography requires coherent imaging, these effects are much more severe than traditional imaging conditions. The combination of these factors increases the complexity of the correlation process. In particular, the speckle noise in an HPIV recording often prevents traditional image-based correlation methods from being used. Instead, single particle identification and correlation are implemented, which set limits on particle number density. A more comprehensive outline of these error sources is given in Meng et al.",
            "score": 50.88090491294861
        },
        {
            "docid": "40149201_3",
            "document": "Docker (software) . Docker is used to run software packages called \"containers\". In a typical example use case, one container runs a web server and web application, while a second container runs a database server that is used by the web application. Containers are isolated from each other and use their own set of tools and libraries; they can communicate through well-defined channels. All containers use the same kernel and are therefore more lightweight than virtual machines. Containers are created from \"images\" which specify their precise contents. Images are often created by combining and modifying standard images downloaded from repositories.",
            "score": 33.56566548347473
        },
        {
            "docid": "28055575_2",
            "document": "Single particle analysis . Single particle analysis is a group of related computerized image processing techniques used to analyze images from transmission electron microscopy (TEM). These methods were developed to improve and extend the information obtainable from TEM images of particulate samples, typically proteins or other large biological entities such as viruses. Individual images of stained or unstained particles are very noisy, and so hard to interpret. Combining several digitized images of similar particles together gives an image with stronger and more easily interpretable features. An extension of this technique uses single particle methods to build up a three-dimensional reconstruction of the particle. Using cryogenic transmission electron microscopy it has become possible to generate reconstructions with sub-nanometer resolution and near-atomic resolution first in the case of highly symmetric viruses, and now in smaller, asymmetric proteins as well.",
            "score": 70.787642121315
        },
        {
            "docid": "29150377_18",
            "document": "Empirical theory of perception . Perception of motion is also confounded by an inverse problem: movement in three-dimensional space does not map perfectly onto movement on the retinal plane. A distant object moving at a given speed will translate more slowly on the retina than a nearby object moving at the same speed, and as mentioned previously size, distance and orientation are also ambiguous given only the retinal image. As with other aspects of perception, empirical theorists propose that this problem is solved by trial-and-error experience with moving stimuli, their associated retinal images and the consequences of behavior.  One way to test this hypothesis is by seeing whether it can explain the flash lag illusion, a visual effect in which a flash superimposed on a moving bar is falsely seen to lag behind the bar. The task for empirical theorists is to explain why individuals perceive the flash in this way, and further, why the perceived lag increases with the speed of the moving bar. To investigate this question, Wojtach et al. (2008) simulated a three-dimensional environment full of moving virtual particles. They modeled the transformation from three dimensions to the two-dimensional image plane and tallied up the frequency of occurrence of particle speeds, particle distances, image speeds, and image distances (image meaning the path projected across the computer-modeled \u201cretina\u201d). The probability distributions they obtained in this way predicted the magnitude of the bar-flash disparity quite well. The authors concluded that the flash-lag effect was a signature of the way brains evolve and develop to behave appropriately in response to moving retinal images.",
            "score": 63.776458859443665
        },
        {
            "docid": "6435232_19",
            "document": "Sentiment analysis . Open source software tools deploy machine learning, statistics, and natural language processing techniques to automate sentiment analysis on large collections of texts, including web pages, online news, internet discussion groups, online reviews, web blogs, and social media. Knowledge-based systems, on the other hand, make use of publicly available resources, to extract the semantic and affective information associated with natural language concepts. Sentiment analysis can also be performed on visual content, i.e., images and videos (see Multimodal sentiment analysis). One of the first approaches in this direction is SentiBank utilizing an adjective noun pair representation of visual content. In addition, the vast majority of sentiment classification approaches rely on the bag-of-words model, which disregards context, grammar and even word order. Approaches that analyses the sentiment based on how words compose the meaning of longer phrases have shown better result, but they incur an additional annotation overhead.",
            "score": 60.14891171455383
        },
        {
            "docid": "42691862_5",
            "document": "Microwave imaging . To increase the cross-range resolution of the imaging system, several antennas should be distributed over an area (which is called the sampling area) with a spacing less than the operating wavelength. However, the mutual coupling between the antennas, which are placed close to each other, may degrade the accuracy of the collected signals. Moreover, the transmitter and receiver system will become very complex. To address these problems, one single scanning antenna is used instead of several antennas. In this configuration, the antenna scans over the entire sampling area, and the collected data is mapped together with their antenna position coordinates. In fact, a synthetic (virtual) aperture is produced by moving the antenna (similar to the synthetic aperture radar principle).  Later, the collected data, which is sometimes referred to as raw data, is fed into the software for processing. Depending on the applied processing algorithm, microwave imaging techniques can be categorized as quantitative and qualitative.",
            "score": 20.26607632637024
        },
        {
            "docid": "26681002_25",
            "document": "Text annotation . Since 2011, the non-profit Hypothes Is Project has offered the free, open web annotation service Hypothes.is. The service features annotation via a Chrome extension, bookmarklet or proxy server, as well as integration into a LMS or CMS. Both webpages and PDFs can be annotated. Other web-based text annotation systems are collaborative software for distributed text editing and versioning, which also feature annotation and commenting interfaces. For example, HyLighter supports synchronous and asynchronous interactions, general commenting, comment tagging, threaded discussions and comment filtering. Other annotation tools under these category are more focused on NLP tasks as Named-entity recognition, relationship extraction or normalization. There are some tools which supports manual tagging of data, for example DataTurks and Prodigy . There is also tagtog that supports automatic annotations via supervised learning.",
            "score": 40.19125735759735
        },
        {
            "docid": "1931185_4",
            "document": "Automatic image annotation . The advantages of automatic image annotation versus content-based image retrieval (CBIR) are that queries can be more naturally specified by the user. CBIR generally (at present) requires users to search by image concepts such as color and texture, or finding example queries. Certain image features in example images may override the concept that the user is really focusing on. The traditional methods of image retrieval such as those used by libraries have relied on manually annotated images, which is expensive and time-consuming, especially given the large and constantly growing image databases in existence.",
            "score": 46.22058725357056
        },
        {
            "docid": "50063858_7",
            "document": "X-ray motion analysis . In planar X-ray imaging, the motions of the markers or bodies are tracked in a specialized software. An initial location guess is supplied by the user for the markers or bodies. The software, depending on its capabilities, requires the user to manually locate the markers or bodies for each frame of the video, or can automatically track the locations throughout the video. The automatic tracking has to be monitored for accuracy and may require manually relocating the markers or bodies. After the tracking data is generated for each marker or body of interest, the tracking is applied to the local anatomical bodies. For example, markers placed at the hip and knee would track the motion of the femur. Using knowledge of the local anatomy, these motions can then be translated into anatomical terms of motion in the plane of the X-ray.",
            "score": 43.16191029548645
        },
        {
            "docid": "346470_2",
            "document": "Image retrieval . An image retrieval system is a computer system for browsing, searching and retrieving images from a large database of digital images. Most traditional and common methods of image retrieval utilize some method of adding metadata such as captioning, keywords, or descriptions to the images so that retrieval can be performed over the annotation words. Manual image annotation is time-consuming, laborious and expensive; to address this, there has been a large amount of research done on automatic image annotation. Additionally, the increase in social web applications and the semantic web have inspired the development of several web-based image annotation tools.",
            "score": 63.58232259750366
        },
        {
            "docid": "38722262_6",
            "document": "Ayasdi . Ayasdi is a machine intelligence platform. It includes dozens of statistical and both supervised and unsupervised machine learning algorithms and can be extended to include whatever algorithms are required for a particular class of analysis. The platform is extensively automated and is in production at scale at many global 100 companies and at governments in the world. It features Topological Data Analysis as a unifying analytical framework, which automatically calculates groupings and similarity across large and highly dimensional data sets, generating network maps with greatly assist analysts in understanding how data clusters and which variables are relevant. When compared with manual approaches to statistical analysis and machine learning, results with Ayasdi will typically be achieved much faster to achieve and more accurate due to the automation and scalability built into the platform. The Ayasdi platform also develops mathematical models, including predictive models, based on the results of the analysis. This allows Ayasdi to deployed as an operational system, or as a part of operational systems, and not just for analysis.",
            "score": 61.799437522888184
        },
        {
            "docid": "4754533_3",
            "document": "Particle counter . The nature of particle counting is based upon either light scattering, light obscuration, or direct imaging. A high intensity light source is used to illuminate the particle as it passes through the detection chamber. The particle passes through the light source (typically a laser or halogen light) and if light scattering is used, then the redirected light is detected by a photo detector. If direct imaging is used, a halogen light illuminates particles from the back within a cell while a high definition, high magnification camera records passing particles. Recorded video is then analyzed by computer software to measure particle attributes. If light blocking (obscuration) is used the loss of light is detected. The amplitude of the light scattered or light blocked is measured and the particle is counted and tabulated into standardized counting bins. The image to the right shows a light scattering particle counter diagram. More information about types of particle counters and types of particle detection follow in this article.",
            "score": 41.164727568626404
        }
    ],
    "r": [
        {
            "docid": "1735527_4",
            "document": "Kymograph . Kymographs were also used outside medical science to measure atmospheric pressure, tuning fork vibrations, the functioning of steam engines, animal habits and the movement of molecules in cells. Kymograph is generally used to study the effects of xenobiotics on tissue preparations. It is a standalone recording apparatus used along side with other apparatus such as organ bath. Writing levers are used to trace the recording from muscle contractions. Some of the commonly used writing levers are simple lever, frontal writing lever and starling heart lever to name a few. These writing levers are connected to a fulcrum which are found on secondary apparatus.",
            "score": 111.14482116699219
        },
        {
            "docid": "1735527_3",
            "document": "Kymograph . The kymograph was initially a mechanical and hydraulic device, invented by German physiologist Carl Ludwig in the 1840s, and found its first use as a means to monitor blood pressure. The blood pressure was conveyed by hydraulics and levers to move a stylus that scratched a white trace into soot-covered paper on the revolving drum. Time is represented by the drum's rotation rate, and was recorded by a further stylus driven by a clock or tuning fork. The kymograph almost immediately became the central instrument in physiology and physiology education. Throughout the nineteenth and twentieth centuries, researchers and technicians devised many improvements to the device, plus numerous new sensory components, to measure a wide range of physiological phenomena such as breathing, muscle movement, speech. New detection and registration systems included electrical and electronic methods, and plotted in ink.",
            "score": 108.37669372558594
        },
        {
            "docid": "35691407_3",
            "document": "Amira (software) . Amira is an extendable software system for scientific visualization, data analysis, and presentation of 3D and 4D data. It is being used by several thousand researchers and engineers in academia and industry around the world. Its flexible user interface and modular architecture make it a universal tool for processing and analysis of data from various modalities; e.g. micro-CT, PET, Ultrasound. Its ever-expanding functionality has made it a versatile data analysis and visualization solution, applicable to and being used in many fields, such as microscopy in biology and materials science, molecular biology, quantum physics, astrophysics, computational fluid dynamics (CFD), finite element modeling (FEM), non-destructive testing (NDT), and many more.  One of the key features, besides data visualization, is Amira\u2019s set of tools for image segmentation and geometry reconstruction. This allows the user to mark (or segment) structures and regions of interest in 3D image volumes using automatic, semi-automatic, and manual tools. The segmentation can then be used for a variety of subsequent tasks, such as volumetric analysis, density analysis, shape analysis, or the generation of 3D computer models for visualization, numerical simulations, or rapid prototyping or 3D printing, to name a few.  Other key Amira features are multi-planar and volume visualization, image registration, filament tracing, cell separation and analysis, tetrahedral mesh generation, fiber-tracking from diffusion tensor imaging (DTI) data, skeletonization, spatial graph analysis, and stereoscopic rendering of 3D data over multiple displays including CAVEs (Cave automatic virtual environments). As a commercial product Amira requires the purchase of a license or an academic subscription. A time-limited, but full-featured evaluation version is available for download free of charge.",
            "score": 100.92158508300781
        },
        {
            "docid": "227982_92",
            "document": "Atomic force microscopy . The latest efforts in integrating nanotechnology and biological research have been successful and show much promise for the future. Since nanoparticles are a potential vehicle of drug delivery, the biological responses of cells to these nanoparticles are continuously being explored to optimize their efficacy and how their design could be improved. Pyrgiotakis et al. were able to study the interaction between CeO2 and Fe2O3 engineered nanoparticles and cells by attaching the engineered nanoparticles to the AFM tip. Beyond the interactions with external synthetic materials, cells have been imaged with X-ray crystallography and there has been much curiosity about their behavior \"in vivo\". Studies have taken advantage of AFM to obtain further information on the behavior of live cells in biological media. Real-time atomic force spectroscopy (or nanoscopy) and dynamic atomic force spectroscopy have been used to study live cells and membrane proteins and their dynamic behavior at high resolution, on the nanoscale. Imaging and obtaining information on the topography and the properties of the cells has also given insight into chemical processes and mechanisms that occur through cell-cell interaction and interactions with other signaling molecules (ex. ligands). Evans and Calderwood used single cell force microscopy to study cell adhesion forces, bond kinetics/dynamic bond strength and its role in chemical processes such as cell signaling.  Scheuring, L\u00e9vy, and Rigaud reviewed studies in which AFM to explore the crystal structure of membrane proteins of photosynthetic bacteria. Alsteen et al. have used AFM-based nanoscopy to perform a real-time analysis of the interaction between live mycobacteria and antimycobacterial drugs (specifically isoniazid, ethionamide, ethambutol, and streptomycine), which serves as an example of the more in-depth analysis of pathogen-drug interactions that can be done through AFM.",
            "score": 97.37863159179688
        },
        {
            "docid": "364299_48",
            "document": "Experimental psychology . Developed by Carl Ludwig in the 19th century, the kymograph is a revolving drum on which a moving stylus tracks the size of some measurement as a function of time. The kymograph is similar to the polygraph, which has a strip of paper moving under one or more pens. The kymograph was originally used to measure blood pressure and it later was used to measure muscle contractions and speech sounds. In psychology, it was often used to record response times.",
            "score": 93.0547103881836
        },
        {
            "docid": "31156047_48",
            "document": "Christian Cambillau . \u201c\"Christian CAMBILLAU, 44, CNRS research director, supervises the Laboratory of crystallography and crystallization of biological macromolecules, an associated CNRS-University of Aix-Marseille 2 research unit. He is an international specialist in the field of structural biology (crystallography) and has developed a software for modeling molecules using data from X rays or NMR, making France one of the very few countries to have developed software in the field of biological crystallography. His most innovative and spectacular work concerns the interactions between proteins and sugars (structural glycobiology) and the structural analysis of lipases (enzymes which hydrolyse fat). Thus, he was able to crystallize for the first time a lectin and glycoprotein complex, making it possible to carry out a precise three-dimensional analysis of interactions between proteins and complex sugars. He has also described the structural movements which can activate pancreatic lipase in the presence of its lipid substrate. This research is extremely important from both a medical and a biotechnological point of view.\"\u201d",
            "score": 86.91319274902344
        },
        {
            "docid": "2567511_12",
            "document": "Neural engineering . Neuromechanics is the coupling of neurobiology, biomechanics, sensation and perception, and robotics (Edwards 2010). Researchers are using advanced techniques and models to study the mechanical properties of neural tissues and their effects on the tissues' ability to withstand and generate force and movements as well as their vulnerability to traumatic loading (Laplaca & Prado 2010). This area of research focuses on translating the transformations of information among the neuromuscular and skeletal systems to develop functions and governing rules relating to operation and organization of these systems (Nishikawa et al. 2007). Neuromechanics can be simulated by connecting computational models of neural circuits to models of animal bodies situated in virtual physical worlds (Edwards 2010). Experimental analysis of biomechanics including the kinematics and dynamics of movements, the process and patterns of motor and sensory feedback during movement processes, and the circuit and synaptic organization of the brain responsible for motor control are all currently being researched to understand the complexity of animal movement. Dr. Michelle LaPlaca's lab at Georgia Institute of Technology is involved in the study of mechanical stretch of cell cultures, shear deformation of planar cell cultures, and shear deformation of 3D cell containing matrices. Understanding of these processes is followed by development of functioning models capable of characterizing these systems under closed loop conditions with specially defined parameters. The study of neuromechanics is aimed at improving treatments for physiological health problems which includes optimization of prostheses design, restoration of movement post injury, and design and control of mobile robots. By studying structures in 3D hydrogels, researchers can identify new models of nerve cell mechanoproperties. For example, LaPlaca et al. developed a new model showing that strain may play a role in cell culture (LaPlaca et al. 2005).",
            "score": 86.55662536621094
        },
        {
            "docid": "46581687_2",
            "document": "Pathway analysis . In bioinformatics research, pathway analysis software is used to identify related proteins within a pathway or building pathway de novo from the proteins of interest. This is helpful when studying differential expression of a gene in a disease or analyzing any omics dataset with a large number of proteins. By examining the changes in gene expression in a pathway, its biological causes can be explored. Pathway is the term from molecular biology which depicts an artificial simplified model of a process within a cell or tissue. Typical pathway model starts with extracellular signaling molecule that activates a specific protein. Thus triggers a chain of protein-protein or protein-small molecule interactions. Pathway analysis helps to understand or interpret omics data from the point of view of canonical prior knowledge structured in the form of pathways diagrams. It allows finding distinct cell processes (), diseases or signaling pathways that are statistically associated with selection of differentially expressed genes between two samples. Often but erroneously pathway analysis is used as synonym for network analysis (functional enrichment analysis and gene set analysis).",
            "score": 86.04399871826172
        },
        {
            "docid": "4214_4",
            "document": "Bioinformatics . Bioinformatics has become an important part of many areas of biology. In experimental molecular biology, bioinformatics techniques such as image and signal processing allow extraction of useful results from large amounts of raw data. In the field of genetics and genomics, it aids in sequencing and annotating genomes and their observed mutations. It plays a role in the text mining of biological literature and the development of biological and gene ontologies to organize and query biological data. It also plays a role in the analysis of gene and protein expression and regulation. Bioinformatics tools aid in the comparison of genetic and genomic data and more generally in the understanding of evolutionary aspects of molecular biology. At a more integrative level, it helps analyze and catalogue the biological pathways and networks that are an important part of systems biology. In structural biology, it aids in the simulation and modeling of DNA, RNA, proteins as well as biomolecular interactions.",
            "score": 83.51988220214844
        },
        {
            "docid": "68686_22",
            "document": "Raman spectroscopy . Raman spectroscopy has a wide variety of applications in biology and medicine. It has helped confirm the existence of low-frequency phonons in proteins and DNA, promoting studies of low-frequency collective motion in proteins and DNA and their biological functions. Raman reporter molecules with olefin or alkyne moieties are being developed for tissue imaging with SERS-labeled antibodies. Raman spectroscopy has also been used as a noninvasive technique for real-time, in situ biochemical characterization of wounds. Multivariate analysis of Raman spectra has enabled development of a quantitative measure for wound healing progress. Spatially offset Raman spectroscopy (SORS), which is less sensitive to surface layers than conventional Raman, can be used to discover counterfeit drugs without opening their packaging, and to non-invasively study biological tissue. A huge reason why Raman spectroscopy is so useful in biological applications is because its results often do not face interference from water molecules, due to the fact that they have permanent dipole moments, and as a result, the Raman scattering cannot be picked up on. This is a large advantage, specifically in biological applications. Raman spectroscopy also has a wide usage for studying biominerals. Lastly, Raman gas analyzers have many practical applications, including real-time monitoring of anesthetic and respiratory gas mixtures during surgery.",
            "score": 83.25888061523438
        },
        {
            "docid": "42287747_75",
            "document": "Acoustic tweezers . Manipulating single cells is important to many biological studies, such as in controlling the cellular microenvironment and isolating specific cells of interest. Acoustic tweezers has been demonstrated to manipulate each individual cell with micrometer-level resolution. Cells generally have a diameter of 10\u201320 \u03bcm. To meet the resolution requirements of manipulating single cells, short-wavelength acoustic waves should be employed. In this case, surface acoustic wave (SAW) is a more favored option than bulk acoustic wave (BAW) as it allows using shorter-wavelength acoustic waves (normally less than 200 \u03bcm). Ding \"et al.\" reported a surface standing acoustic wave (SSAW) microdevice that is able to manipulate single cells with prescribed paths. Fig.18 shows that the movement traces of single cells can be controlled to form the letter sequences \"PNAS\" and \"PSU\", respectively. The working principle of the device lies in the controlled movement of pressure nodes in a SSAW field. Ding \"et al.\" employed chirped interdigital transducers (IDTs) that are able to generate SSAWs with adjustable positions of pressure nodes. By simply changing the input frequency, the position of pressure nodes can be changed. Ding \"et al.\" also showed that not only micrometer-sized cells, but also the millimeter-sized microorganism \"C. elegan\" can be manipulated with the same manner. In this manner, it can be programmed to move single cells along any desired paths. It should be noted that Ding et al. also examined cell metabolism and proliferation after the acoustic treatment. No significant differences were found comparing to the control group, indicating the non-invasive nature of acoustic base manipulation. In addition to using chirped IDTs, phase shift based single particle/cell manipulation has also been reported.",
            "score": 80.86018371582031
        },
        {
            "docid": "1735527_2",
            "document": "Kymograph . A kymograph (a Greek-derived name which means 'wave writer') is a device that draws a graphical representation of spatial position over time in which a spatial axis represents time. It basically consists of a revolving drum wrapped with a sheet of paper on which a stylus moves back and forth recording perceived changes of phenomena such as motion or pressure.",
            "score": 80.6471939086914
        },
        {
            "docid": "4241575_5",
            "document": "Cytochalasin . Actin microfilaments have been widely studied using cytochalasins. Due to their chemical nature, cytochalasins can help researchers understand the importance of actin in various biological processes. The use of cytochalasins has allowed researchers to better understand actin polymerization, cell motility, ruffling, cell division, contraction, and cell stiffness. The use of cytochalasins has been so important to understanding cytoskeletal movement and many other biological processes, researchers have created two synthetic cytochalasins.",
            "score": 79.6861801147461
        },
        {
            "docid": "13358658_2",
            "document": "Demographic analysis . Demographic analysis includes the sets of methods that allow us to measure the dimensions and dynamics of populations. These methods have primarily been developed to study human populations, but are extended to a variety of areas where researchers want to know how populations of social actors can change across time through processes of birth, death, and migration. In the context of human biological populations, demographic analysis uses administrative records to develop an independent estimate of the population. Demographic analysis estimates are often considered a reliable standard for judging the accuracy of the census information gathered at any time. In the labor force, demographic analysis is used to estimate sizes and flows of populations of workers; in population ecology the focus is on the birth, death, migration and immigration of individuals in a population of living organisms, alternatively, in social human sciences could involve movement of firms and institutional forms. Demographic analysis is used in a wide variety of contexts. For example, it is often used in business plans, to describe the population connected to the geographic location of the business. Demographic analysis is usually abbreviated as DA. For the 2010 U.S. Census, The U.S. Census Bureau has expanded its DA categories. Also as part of the 2010 U.S. Census, DA now also includes comparative analysis between independent housing estimates, and census address lists at different key time points.",
            "score": 79.34488677978516
        },
        {
            "docid": "1735527_1",
            "document": "Kymograph . Kymograph",
            "score": 78.95832824707031
        },
        {
            "docid": "2506529_29",
            "document": "Cellular neural network . Due to their processing capabilities and flexibility, CNN processors have been used & prototyped for novel field applications such as flame analysis for monitoring combustion at a waste incinerator, mine-detection using infrared imagery, calorimeter cluster peak for high energy physics, anomaly detection in potential field maps for geophysics, laser dot detection, metal inspection for detecting manufacturing defects, and seismic horizon picking. They have also been used to perform biometric functions such as fingerprint recognition, vein feature extraction, face tracking, and generating visual stimuli via emergent patterns to gauge perceptual resonances. CNN processors have been used for medical and biological research in performing automated nucleated cell counting for detecting hyperplasia, segment images into anatomically and pathologically meaningful regions, measure and quantify cardiac function, measure the timing of neurons, and detect brain abnormalities that would lead to seizures. One potential future application of CNN microprocessors is to combine them with DNA microarrays to allow for a near-real time DNA analysis of hundreds of thousands of different DNA sequences. Currently, the major bottleneck of DNA microarray analysis is the amount of time needed to process data in the form of images, and using a CNN microprocessor, researchers have reduced the amount of time needed to perform this calculation to 7ms.",
            "score": 78.43769073486328
        },
        {
            "docid": "4366478_3",
            "document": "Nanoparticle tracking analysis . The technique is used in conjunction with an ultramicroscope and a laser illumination unit that together allow small particles in liquid suspension to be visualized moving under Brownian motion. The light scattered by the particles is captured using a CCD or EMCCD camera over multiple frames. Computer software is then used to track the motion of each particle from frame to frame. The rate of particle movement is related to a sphere equivalent hydrodynamic radius as calculated through the Stokes\u2013Einstein equation. The technique calculates particle size on a particle-by particle basis, overcoming inherent weaknesses in ensemble techniques such as dynamic light scattering. Since video clips form the basis of the analysis, accurate characterization of real time events such as aggregation and dissolution is possible. Samples require minimal preparation, minimizing the time required to process each sample. Speculators suggest that eventually the analysis may be done in real-time with no preparation, e.g. when detecting the presence of airborne viruses or biological weapons.",
            "score": 77.39064025878906
        },
        {
            "docid": "24578359_20",
            "document": "Handwriting movement analysis . MovAlyzeR was developed by NeuroScript, Tempe, AZ, USA. NeuroScript was founded in 1997 by Prof. Dr. George Stelmach, who has since retired, and Dr. Hans-Leo Teulings. In 1999 Gregory M. Baker joined as MovAlyzeR\u2019s designer and implementer. This handwriting movement analysis software is the first to demonstrate that it can discern movement side-effects due to schizophrenia medication better than with any conventional evaluation method used in psychiatry today (international patent pending) (Caligiuri et al., 2009a, b). MovAlyzeR is currently the only handwriting movement analysis software that is certified for Microsoft Windows XP and Vista. It can be integrated with MATLAB and perform image processing on scanned handwriting exemplars. It is used in fields ranging from research in human movement sciences, kinesiology, psychology, education, aging research, psychiatry, neurology, occupational therapy, forensic document examination, computer science (handwriting recognition, signature verification), to educational demonstrations or student projects in these fields. The oldest references to MovAlyzeR are Teulings and Romero (2003), Teulings and Van Gemmert, (2003), Romero and Teulings (2003).",
            "score": 77.21295928955078
        },
        {
            "docid": "5054730_12",
            "document": "Biomolecular structure . Structure probing is the process by which biochemical techniques are used to determine biomolecular structure. This analysis can be used to define the patterns that can be used to infer the molecular structure, experimental analysis of molecular structure and function, and further understanding on development of smaller molecules for further biological research. Structure probing analysis can be done through many different methods, which include chemical probing, hydroxyl radical probing, nucleotide analog interference mapping (NAIM), and in-line probing.",
            "score": 77.08001708984375
        },
        {
            "docid": "31100446_5",
            "document": "Immortalised cell line . Immortalized cell lines are widely used as a simple model for more complex biological systems, for example for the analysis of the biochemistry and cell biology of mammalian (including human) cells. The main advantage of using an immortal cell line for research is its immortality; the cells can be grown indefinitely in culture. This simplifies analysis of the biology of cells which may otherwise have a limited lifetime.",
            "score": 77.02754211425781
        },
        {
            "docid": "22217265_2",
            "document": "Nucleic acid structure determination . Structure probing of nucleic acids is the process by which biochemical techniques are used to determine nucleic acid structure. This analysis can be used to define the patterns which can infer the molecular structure, experimental analysis of molecular structure and function, and further understanding on development of smaller molecules for further biological research. Structure probing analysis can be done through many different methods, which include chemical probing, hydroxyl radical probing, SHAPE, nucleotide analog interference mapping (NAIM), and in-line probing.",
            "score": 76.955078125
        },
        {
            "docid": "1041078_6",
            "document": "Lab-on-a-chip . Although the application of LOCs is still novel and modest, a growing interest of companies and applied research groups is observed in different fields such as analysis (e.g. chemical analysis, environmental monitoring, medical diagnostics and cellomics) but also in synthetic chemistry (e.g. rapid screening and microreactors for pharmaceutics). Besides further application developments, research in LOC systems is expected to extend towards downscaling of fluid handling structures as well, by using nanotechnology. Sub-micrometre and nano-sized channels, DNA labyrinths, single cell detection and analysis, and nano-sensors, might become feasible, allowing new ways of interaction with biological species and large molecules. Many books have been written that cover various aspects of these devices, including the fluid transport, system properties, sensing techniques, and bioanalytical applications.",
            "score": 76.93653106689453
        },
        {
            "docid": "14368827_6",
            "document": "Ideomotor apraxia . The prevailing hypothesis for the pathophysiology of ideomotor apraxia is that the various brain lesions associated with the disorder somehow disrupt portions of the praxis system. The praxis system is the brain regions that are involved in taking processed sensory input, accessing of stored information about tools and gestures, and translating these into a motor output. Buxbaum et al. have proposed that the praxis system involves three distinct parts: stored gesture representations, stored tool knowledge, and a \"dynamic body schema.\" The first two store information about the representation of gestures in the brain and the characteristic movements of tools. The body schema is a brain model of the body and its position in space. The praxis system relates the stored information about a movement type to how the dynamic, i.e. changing, body representation varies as the movement progresses. It is still not clear how this system maps out onto the brain itself, although some research has given indications to possible locations for certain portions. The dynamic body schema has been suggested to be localized in the superior posterior parietal cortex. There is also evidence that the inferior parietal lobule may be the locus for storage of the characteristic movements of a tool. This area showed inverse activation to the cerebellum in a study of tool use and tool mime. If the connections between these areas become severed, the praxis system would be disrupted, possibly resulting in the symptoms observed in ideomotor apraxia. There is no one definitive test for ideomotor apraxia; there are several that are used clinically to make an ideomotor apraxia diagnosis. The criteria for a diagnosis are not entirely conserved among clinicians, for apraxia in general or distinguishing subtypes. Almost all the tests laid out here that enable a diagnosis of ideomotor apraxia share a common feature: assessment of the ability to imitate gestures. A test developed by Georg Goldenberg uses imitation assessment of 10 gestures. The tester demonstrates the gesture to the patient and rates him on how whether the gesture was correctly imitated. If the first attempt to imitate the gesture was unsuccessful, the gesture is presented a second time; a higher score is given for correct imitation on the first trial, then for the second, and the lowest score is for not correctly imitating the gesture. The gestures used here are all meaningless, such as placing the hand flat on the top of the head or flat outward with the fingers towards the ear. This test is specifically designed for ideomotor apraxia. The main variation from this is in the type and number of gestures used. One test uses twenty-four movements with three trials for each and a trial-based scoring system similar to the Goldenberg protocol. The gestures here are also copied by the patient from the tester and are divided into finger movements, e.g. making a scissor movement with the forefinger and middle finger, and hand and arm movements, e.g. doing a salute. This protocol combines meaningful and meaningless gestures. Another test uses five meaningful gestures, such as waving goodbye or scratching your head and five meaningless gestures. Additional differences in this test are a verbal command to initiate the movement and it distinguishes between accurate performance and inaccurate but recognizable performance. One test utilizes tools, including a hammer and a key, with both a verbal command to use the tools and the patient copying the tester's demonstrated use of the tools. These tests have been shown to be individually unreliable, with considerable variability between the diagnoses delivered by each. If a battery of tests is used, however, the reliability and validity may be improved. It is also highly advisable to include assessments of how the patient performs activities in daily life. One of the newer tests that has been developed may provide greater reliability without relying on a multitude of tests. It combines three types of tool use with imitation of gestures. The tool use section includes having the patient pantomime use with no tool present, with visual contact with the tool, and finally with tactile contact with the tool. This test screens for ideational and ideomotor apraxia, with the second portion aimed specifically at ideomotor apraxia. One study showed great potential for this test, but further studies are needed to reproduce these results before this can be said with confidence. This disorder often occurs with other degenerative neurological disorders such as Parkinson's disease and Alzheimer's Disease. These comorbidities can make it difficult to pick out the specific features of ideomotor apraxia. The important point in distinguishing ideomotor apraxia is that basic motor control is intact; it is a high level dysfunction involving tool use and gesturing. Additionally, clinicians must be careful to exclude aphasia as a possible diagnosis, as, in the tests involving verbal command, an aphasic patient could fail to perform a task properly because they do not understand what the directions are.",
            "score": 76.88994598388672
        },
        {
            "docid": "9999924_34",
            "document": "Semen analysis . Computer Assisted Semen Analysis (CASA) is a catch-all phrase for automatic or semi-automatic semen analysis techniques. Most systems are based on image analysis, but alternative methods exist such as tracking cell movement on a digitizing tablet. Computer-assisted techniques are most-often used for the assessment of sperm concentration and mobility characteristics, such as velocity and linear velocity. Nowadays, there are CASA systems, based on image analysis and using new techniques, with near perfect results, and doing full analysis in a few seconds. With some techniques, sperm concentration and motility measurements are at least as reliable as current manual methods.",
            "score": 76.66328430175781
        },
        {
            "docid": "34128_47",
            "document": "Wilhelm Wundt . Wundt describes apperceptive processes as psychologically highly differentiated and, in many regards, bases this on methods and results from his experimental research. One example is the wide-ranging series of experiments on the mental chronometry of complex reaction times. In research on feelings, certain effects are provoked while pulse and breathing are recorded using a kymograph. The observed differences were intended to contribute towards supporting Wundt's theory of emotions with its three dimensions: pleasant \u2013 unpleasant, tense \u2013 relaxed, excited \u2013 depressed.",
            "score": 76.58496856689453
        },
        {
            "docid": "40504424_4",
            "document": "Gravity current intrusion . Work analysing gravity currents propagating within a single fluid host was broadened to consider intrusions within sharply stratified fluids by Hoyler & Huppert in 1980. Since then there have been further significant analytical and experimental advancements into understanding specifically particle laden intrusions by researchers including Bonnecaze, et al., (1993, 1995, 1996), Rimoldi et al. (1996), and Rooij, et al. (1999). As of 2012 the most recent rigorous analytical analysis, designed to determine the propagation speed of a classically extending intrusion, was performed by Flynn and Linden. Practical experimentation into intrusions has typically employed a lock exchange to study intrusion dynamics.",
            "score": 76.55522155761719
        },
        {
            "docid": "149353_4",
            "document": "Computational biology . Computational Biology, which includes many aspects of bioinformatics, is the science of using biological data to develop algorithms or models to understand biological systems and relationships.  Until recently, biologists did not have access to very large amounts of data. This data has now become commonplace, particularly in molecular biology and genomics. Researchers were able to develop analytical methods for interpreting biological information, but were unable to share them quickly among colleagues.  Bioinformatics began to develop in the early 1970s. It was considered the science of analyzing informatics processes of various biological systems. At this time, research in artificial intelligence was using network models of the human brain in order to generate new algorithms. This use of biological data to develop other fields pushed biological researchers to revisit the idea of using computers to evaluate and compare large data sets. By 1982, information was being shared amongst researchers through the use of punch cards. The amount of data being shared began to grow exponentially by the end of the 1980s. This required the development of new computational methods in order to quickly analyze and interpret relevant information. Since the late 1990s, computational biology has become an important part of developing emerging technologies for the field of biology. The terms computational biology and evolutionary computation have a similar name, but are not to be confused. Unlike computational biology, evolutionary computation is not concerned with modeling and analyzing biological data. It instead creates algorithms based on the ideas of evolution across species. Sometimes referred to as genetic algorithms, the research of this field can be applied to computational biology. While evolutionary computation is not inherently a part of computational biology, Computational evolutionary biology is a subfield of it.",
            "score": 76.50260162353516
        },
        {
            "docid": "22072718_14",
            "document": "Biological network . In biology, pairwise interactions have historically been the focus of intense study. With the recent advances in network science, it has become possible to scale up pairwise interactions to include individuals of many species involved in many sets of interactions to understand the structure and function of larger ecological networks. The use of network analysis can allow for both the discovery and understanding how these complex interactions link together within the system\u2019s network, a property which has previously been overlooked. This powerful tool allows for the study of various types of interactions (from competitive to cooperative) using the same general framework. For example, plant-pollinator interactions are mutually beneficial and often involve many different species of pollinators as well as many different species of plants. These interactions are critical to plant reproduction and thus the accumulation of resources at the base of the food chain for primary consumers, yet these interaction networks are threatened by anthropogenic change. The use of network analysis can illuminate how pollination networks work and may in turn inform conservation efforts. Within pollination networks, nestedness (i.e., specialists interact with a subset of species that generalists interact with), redundancy (i.e., most plants are pollinated by many pollinators), and modularity play a large role in network stability. These network properties may actually work to slow the spread of disturbance effects through the system and potentially buffer the pollination network from anthropogenic changes somewhat. More generally, the structure of species interactions within an ecological network can tell us something about the diversity, richness, and robustness of the network. Researchers can even compare current constructions of species interactions networks with historical reconstructions of ancient networks to determine how networks have changed over time. Recent research into these complex species interactions networks is highly concerned with understanding what factors (e.g., diversity) lead to network stability.",
            "score": 76.21859741210938
        },
        {
            "docid": "41190687_6",
            "document": "Hydrodynamic trapping . Several approaches were applied in microfluidics facilitating single cell trapping for single cell analysis, which can be broadly classified into contact based approach and contact-less approach. Different cell trapping methods are hydrodynamic trapping, dielectrophoresis trapping, chemical trapping, gel trapping, magnetic trapping, acoustic trapping and laser trapping. Each method has its own merits and demerits. Mostly these process requires auxiliary facilities like a pump or a pressurecontrolling system for fluid introduction and guidance, and some need specialized electronics or optical equipment which are not common in a medical or biological laboratory and induce further obstacles for operation. The auxiliary parts occupy much space and thereby greatly lowers the area density of single-cell arrays. Design structures of many microfluidic chips are complicated due to the need for multiple layers, channels, and valves, which are difficult to either fabricate or manipulate. Thus the practical applications in clinics or general biological laboratories are limited due to these drawbacks. Increasing demand for trapping based on the design itself, involving unsophisticated basic laboratory instruments for single cell measurements led to the evolution of hydrodynamic trapping technique. This way, cells which may be affected due to the presence of an external field/force are preserved. Apart from the simplicity of the process, the natural state of the separated particles can be useful for many kinds research. Based on the approach used, hydrodynamic trapping can be broadly classified into the contact-based and contact-less approach. Figure 1 shows the diverse approaches involved in single cell trapping. Different principal mechanisms were used in hydrodynamic trapping as shown in Figure 2. Hydrodynamic trapping mechanism includes cross-stream migration, vortices based trapping and externally controlled signal approach. Hydrodynamic trapping can also be used to trap and study molecules in lipid bilayers. This is done using hydrodynamic drag forces that are created by a fluid flow through a very small cone shaped pipet located about one micrometer away from the lipid bilayer. This allows particles protruding from the lipid bilayer to be trapped and studied.",
            "score": 76.01883697509766
        },
        {
            "docid": "39198919_2",
            "document": "Cancer systems biology . Cancer systems biology encompasses the application of systems biology approaches to cancer research, in order to study the disease as a complex adaptive system with emerging properties at multiple biological scales. Cancer systems biology represents the application of systems biology approaches to the analysis of how the intracellular networks of normal cells are perturbed during carcinogenesis to develop effective predictive models that can assist scientists and clinicians in the validations of new therapies and drugs. Tumours are characterized by genomic and epigenetic instability that alters the functions of many different molecules and networks in a single cell as well as altering the interactions with the local environment. Cancer systems biology approaches, therefore, are based on the use of computational and mathematical methods to decipher the complexity in tumorigenesis as well as cancer heterogeneity.  Cancer systems biology encompasses concrete applications of systems biology approaches to cancer research, notably (a) the need for better methods to distill insights from large-scale networks, (b) the importance of integrating multiple data types in constructing more realistic models, (b) challenges in translating insights about tumorigenic mechanisms into therapeutic interventions, and (d) the role of the tumor microenvironment, at the physical, cellular, and molecular levels. Cancer systems biology therefore adopts a holistic view of cancer aimed at integrating its many biological scales, including genetics, signaling networks, epigenetics, cellular behavior, histology, (pre)clinical manifestations and epidemiology. Ultimately, cancer properties at one scale, e.g., histology, are explained by properties at a scale below, e.g., cell behavior.",
            "score": 75.82795715332031
        },
        {
            "docid": "22072718_15",
            "document": "Biological network . Network analysis provides the ability to quantify associations between individuals, which makes it possible to infer details about the network as a whole at the species and/or population level. Researchers interested in animal behavior across a multitude of taxa, from insects to primates, are starting to incorporate network analysis into their research. Researchers interested in social insects (e.g., ants and bees) have used network analyses to better understand division of labor, task allocation, and foraging optimization within colonies; Other researchers are interested in how certain network properties at the group and/or population level can explain individual level behaviors. For instance, a study on wire-tailed manakins (a small passerine bird) found that a male\u2019s degree in the network largely predicted the ability of the male to rise in the social hierarchy (i.e. eventually obtain a territory and matings). In bottlenose dolphin groups, an individual\u2019s degree and betweenness centrality values may predict whether or not that individual will exhibit certain behaviors, like the use of side flopping and upside-down lobtailing to lead group traveling efforts; individuals with high betweenness values are more connected and can obtain more information, and thus are better suited to lead group travel and therefore tend to exhibit these signaling behaviors more than other group members. Network analysis can also be used to describe the social organization within a species more generally, which frequently reveals important proximate mechanisms promoting the use of certain behavioral strategies. These descriptions are frequently linked to ecological properties (e.g., resource distribution). For example, network analyses revealed subtle differences in the group dynamics of two related equid fission-fusion species, Grevy\u2019s zebra and onagers, living in variable environments; Grevy\u2019s zebras show distinct preferences in their association choices when they fission into smaller groups, whereas onagers do not. Similarly, researchers interested in primates have also utilized network analyses to compare social organizations across the diverse primate order, suggesting that using network measures (such as centrality, assortativity, modularity, and betweenness) may be useful in terms of explaining the types of social behaviors we see within certain groups and not others. Finally, social network analysis can also reveal important fluctuations in animal behaviors across changing environments. For example, network analyses in female chacma baboons (\"Papio hamadryas ursinus\") revealed important dynamic changes across seasons which were previously unknown; instead of creating stable, long-lasting social bonds with friends, baboons were found to exhibit more variable relationships which were dependent on short-term contingencies related to group level dynamics as well as environmental variability. This is a very small set of broad examples of how researchers can use network analysis to study animal behavior. Research in this area is currently expanding very rapidly. Social network analysis is a valuable tool for studying animal behavior across all animal species, and has the potential to uncover new information about animal behavior and social ecology that was previously poorly understood.",
            "score": 75.5646743774414
        },
        {
            "docid": "16961974_4",
            "document": "Videokymography . Videokymography was developed in 1994 by Czech and Dutch scientists, Jan G. \u0160vec and Harm K. Schutte, as a low-cost, high-speed imaging method for examination of vocal fold vibrations. After being developed in the Netherlands, it was introduced to clinical practices in Prague where it now complements another visualization method known as videostroboscopy for early diagnostics of voice disorders and therapy evaluation at the Center for Communication Disorders, Medical Healthcom. Since then, videokymography has spread as a clinical and research tool to clinics around the world, and kymographic display was adopted also for digital high-speed endoscopy.",
            "score": 75.5113754272461
        }
    ]
}