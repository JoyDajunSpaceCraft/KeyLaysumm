{
    "q": [
        {
            "docid": "6082997_2",
            "document": "Filling-in . In vision, filling-in phenomena are those responsible for the completion of missing information across the physiological blind spot, and across natural and artificial scotomata. There is also evidence for similar mechanisms of completion in normal visual analysis. Classical demonstrations of perceptual filling-in involve filling in at the blind spot in monocular vision, and images stabilized on the retina either by means of special lenses, or under certain conditions of steady fixation. For example, naturally in monocular vision at the physiological blind spot, the percept is not a hole in the visual field, but the content is \u201cfilled-in\u201d based on information from the surrounding visual field. When a textured stimulus is presented centered on but extending beyond the region of the blind spot, a continuous texture is perceived. This partially inferred percept is paradoxically considered more reliable than a percept based on external input. (Ehinger \"et al.\" 2017).",
            "score": 216.23306047916412
        },
        {
            "docid": "37210110_3",
            "document": "Context effect . Context effects employ top-down design when analyzing information. Top down design fuels understanding of an image by using prior experiences and knowledge to interpret a stimulus. This process helps us analyze familiar scenes and objects when encountering them. During perception of any kind, people generally use either sensory data (bottom-up design) or prior knowledge of the stimulus (top-down design) when analyzing the stimulus. Individuals generally use both types of processing to examine stimuli. The use of both sensory data and prior knowledge to reach a conclusion is a feature of optimal probabilistic reasoning, known as Bayesian inference; cognitive scientists have shown mathematically how context effects can emerge from the Bayesian inference process. When context effects occur, individuals are using environmental cues perceived while examining the stimuli in order to help analyze it. In other words, individuals often make relative decisions that are influenced by the environment or previous exposure to objects.",
            "score": 108.47178018093109
        },
        {
            "docid": "24978422_3",
            "document": "Visual adaptation . The aftereffects of exposure to a visual stimulus or pattern causes loss of sensitivity to that pattern and induces stimulus bias. An example of this phenomenon is the \"lilac chaser\", introduced by Jeremy Hinton. The stimulus here are lilac circles, that once removed, leave green circles that then become the most prominent stimulus. The fading of the lilac circles is due to a loss of sensitivity to that stimulus and the adaptation to the new stimulus. To experience the \"lilac chaser\" effect, the subject needs to fixate their eyes on the cross in the middle of the image, and after a while the effect will settle in. Visual coding, a process involved in visual adaptation, is the means by which the brain adapts to certain stimuli, resulting in a biased perception of those stimuli. This phenomenon is referred to as visual plasticity; the brain's ability to change and adapt according to certain, repeated stimuli, altering the way information is perceived and processed. The rate and strength of visual adaptation depends heavily on the number of stimuli presented simultaneously, as well as the amount of time for which the stimulus is present. Visual adaptation was found to be weaker when there were more stimuli present. Moreover, studies have found that stimuli can rival each other, which explains why higher numbers of simultaneous stimuli lead to lower stimulus adaptation. Studies have also found that visual adaptation can have a reversing effect; if the stimulus is absent long enough, the aftereffects of visual adaptation will subside. Studies have also shown that visual adaptation occurs in the early stages of processing.",
            "score": 153.09277653694153
        },
        {
            "docid": "53953041_3",
            "document": "Predictive coding . Theoretical ancestors to predictive coding date back as early as 1860 with Helmholz\u2019s concept of unconscious inference (Clark, 2013). Unconscious inference refers to the idea that the human brain fills in visual information to make sense of a scene. For example, if something is relatively smaller than another object in the visual field, the brain uses that information as a likely cue of depth, such that the perceiver ultimately (and involuntarily) experiences depth. The understanding of perception as the interaction between sensory stimuli (bottom-up) and conceptual knowledge (top-down) continued to be established by Jerome Bruner (psychologist) who, starting in the 1940s, studied the ways in which needs, motivations and expectations influence perception, research that came to be known as 'New Look' psychology. In 1981, McClelland and Rumelhart in their seminal paper examined the interaction between processing features (lines and contours) which form letters, which in turn form words. While the features suggest the presence of a word, they found that when letters were situated in the context of a word, people were able to identify them faster than when they were situated in a non-word without semantic context. McClelland and Rumelhart\u2019s parallel processing model describes perception as the meeting of top-down (conceptual) and bottom-up (sensory) elements.",
            "score": 122.09140396118164
        },
        {
            "docid": "35982062_8",
            "document": "Biased Competition Theory . Bottom-up processes are characterized by an absence of higher level direction in sensory processing. It primarily relies on sensory information and incoming sensory information is the starting point for all Bottom-up processing. Bottom-up refers to when a feature stands out in a visual search. This is commonly called the \u201cpop-out\u201d effect. Salient features like bright colors, movement and big objects make the object \u201cpop-out\u201d of the visual search. \u201cPop-out\u201d features can often attract attention without conscious processing. Objects that stand out are often given priority (bias) in processing. Bottom-up processing is data driven, and according to this stimuli are perceived on the basis of the data which is being experienced through the senses. Evidence suggests that simultaneously presented stimuli do in fact compete in order to be represented in the visual cortex, with stimuli mutually suppressing each other to gain this representation. This was examined by Reynolds and colleagues, who looked at the size of neurons\u2019 receptive field\u2019s within the visual cortex. It was found that the presentation of a single stimulus resulted in a low firing rate while two stimuli presented together resulted in a higher firing rate. Reynolds and colleagues also found that when comparing the neural response of an individually presented visual stimulus to responses gathered from simultaneously presented stimuli, the responses of the concurrent presented stimuli were less than the sum of the responses gathered when each stimuli was presented alone. This suggests that two stimuli presented together increase neural work load required for attention. This increased neural load creates suppressive processes and causes the stimuli to compete for neural representation in the brain. Proulx and Egeth predicted that brighter objects would bias attention in favor of that object. Another prediction is that larger objects would bias the attention in favor of that object. The experiment was a computer-based visual search task, where participants searched for a target among distractions. The results of the study suggested that when irrelevant stimuli were large or bright, attention was biased towards the irrelevant objects, prioritizing them for cognitive processing. This research shows the effects of Bottom-up (stimulus-driven) processing on biased competition theory.",
            "score": 142.68439292907715
        },
        {
            "docid": "433584_4",
            "document": "McGurk effect . Vision is the primary sense for humans, but speech perception is multimodal, which means that it involves information from more than one sensory modality, in particular, audition and vision. The McGurk effect arises during phonetic processing because the integration of audio and visual information happens early in speech perception. The McGurk effect is very robust; that is, knowledge about it seems to have little effect on one's perception of it. This is different from certain optical illusions, which break down once one 'sees through' them. Some people, including those that have been researching the phenomenon for more than twenty years, experience the effect even when they are aware that it is taking place. With the exception of people who can identify most of what is being said from speech-reading alone, most people are quite limited in their ability to identify speech from visual-only signals. A more extensive phenomenon is the ability of visual speech to increase the intelligibility of heard speech in a noisy environment. Visible speech can also alter the perception of perfectly audible speech sounds when the visual speech stimuli are mismatched with the auditory speech. Normally, speech perception is thought to be an auditory process; however, our use of information is immediate, automatic, and, to a large degree, unconscious and therefore, despite what is widely accepted as true, speech is not only something we hear. Speech is perceived by all of the senses working together (seeing, touching, and listening to a face move). The brain is often unaware of the separate sensory contributions of what it perceives. Therefore, when it comes to recognizing speech the brain cannot differentiate whether it is seeing or hearing the incoming information.",
            "score": 161.12126743793488
        },
        {
            "docid": "7283_11",
            "document": "Controversy . Bayesian decision theory allows these failures of rationality to be described as part of a statistically optimized system for decision making. Experiments and computational models in multisensory integration have shown that sensory input from different senses is integrated in a statistically optimal way, in addition, it appears that the kind of inferences used to infer single sources for multiple sensory inputs uses a Bayesian inference about the causal origin of the sensory stimuli. As such, it appears neurobiologically plausible that the brain implements decision-making procedures that are close to optimal for Bayesian inference.",
            "score": 96.06371641159058
        },
        {
            "docid": "54842715_43",
            "document": "Interoception . The EPIC model proposes a method of understanding the brain\u2019s response to stimuli contrary to the classic \"stimulus-response\" model. The classical view of information processing is that when a peripheral stimulus provided information to the central nervous system, it was processed in the brain, and a response was elicited. The EPIC model deviates from this and proposes that the brain is involved in a process of active inference, that is, assiduously making predictions about situations based on previous experiences. These predictions, when coupled with incoming sensory signals, allow the brain to compute a prediction error. Interoceptive prediction errors signal the occurrence of discrepancies within the body, which the brain attempts to minimize. This can be done by 1) modifying the predictions through brain-related pathways, 2) altering the body position/location in order to better align incoming sensory signals with the prediction, or 3) altering the brain\u2019s method of receiving incoming stimuli. Interoceptive prediction error signals are a key component of many theories of interoceptive dysfunction in physical and mental health.",
            "score": 116.59161281585693
        },
        {
            "docid": "2534964_14",
            "document": "Sensory processing . Perhaps one of the most studied sensory integrations is the relationship between vision and audition. These two senses perceive the same objects in the world in different ways, and by combining the two, they help us understand this information better. Vision dominates our perception of the world around us. This is because visual spatial information is one of the most reliable sensory modalities. Visual stimuli are recorded directly onto the retina, and there are few, if any, external distortions that provide incorrect information to the brain about the true location of an object. Other spatial information is not as reliable as visual spatial information. For example, consider auditory spatial input. The location of an object can sometimes be determined solely on its sound, but the sensory input can easily be modified or altered, thus giving a less reliable spatial representation of the object. Auditory information therefore is not spatially represented unlike visual stimuli. But once one has the spatial mapping from the visual information, multisensory integration helps bring the information from both the visual and auditory stimuli together to make a more robust mapping.",
            "score": 183.71603560447693
        },
        {
            "docid": "35982062_6",
            "document": "Biased Competition Theory . There are two major neural pathways that process the information in the visual field; the ventral stream and the dorsal stream. The two pathways run in parallel and are both working simultaneously. The ventral stream is important for object recognition and often referred to as the \u201cwhat\u201d system of the brain; it projects to the inferior temporal cortex. The dorsal stream is important for spatial perception and performance and is referred to as the \u201cwhere\u201d system which projects to the posterior parietal cortex. According to the biased competition theory, an individual\u2019s visual system has limited capacity to process information about multiple objects at any given time. For example, if an individual was presented with two stimuli (objects) and was asked to identify attributes of each object at the same time, the individual\u2019s performance would be worse in comparison to if the objects were presented separately. This suggests multiple objects presented simultaneously in the visual field will compete for neural representation due to limited processing resources. Single cell recording studies conducted by Kastner and Ungerleider examined the neural mechanisms behind the biased competition theory. In their experiment the size of the receptive field's (RF) of neurons within the visual cortex were examined. A single visual stimulus was presented alone in a neuron\u2019s RF, followed with another stimulus presented simultaneously within the same RF. The single \u2018effective\u2019 stimuli produced a low firing rate, whereas the two stimuli presented together produced a high firing rate. The response to the paired stimuli was reduced. This suggests that when two stimuli are presented together within a neuron\u2019s RF, the stimuli are processed in a mutually suppressive manner, rather than being processed independently. This suppression process, according to Kastner and Ungerleider, occurs when two stimuli are presented together because they compete for neural representation, due to limited cognitive processing capacity. The RF experiment suggests that as the number of objects increase, the information available for each object will decrease due to increased neural workload (suppression), and decreased cognitive capacity. In order for an object in the visual field or RF be efficiently processed, there needs to be a way to bias these neurological resources towards the object. Attention prioritizes task relevant objects, biasing this process. For example, this bias can be towards an object which is currently attended to in the visual field or RF, or towards the object that is most relevant to one\u2019s behavior. Functional magnetic resonance imaging (fMRI) has shown that biased competition theory can explain the observed attention effects at a neuronal level. Attention effects bias the internal weight (strengthens connections) of task relevant features toward the attended object. This was shown by Reddy, Kanwisher, and van Rullen who found an increase in oxygenated blood to a specific neuron following a locational cue. Further neurological support comes from neurophysiological studies which have shown that attention results from Top-down biasing, which in turn influences neuronal spiking. In sum, external inputs affect the Top-down guidance of attention, which bias specific neurons in the brain.",
            "score": 133.90372455120087
        },
        {
            "docid": "35970915_13",
            "document": "Colavita visual dominance effect . The Colavita effect has been shown to be affected by factors that contribute to the intermodal binding of auditory and visual stimuli during perception. These factors of interest are spatial and temporal coincidence between the auditory and visual stimuli, which modulate the Colavita effect through temporal separation and temporal order . For example, results from an experiment, conducted by Koppen and Spence (2007b), showed a larger Colavita effect when auditory and visual stimuli were presented closer together in time. When the stimuli were presented further apart in time, the Colavita effect was reduced. Their results also showed that the Colavita effect was largest when the visual stimulus was presented before the auditory stimulus during the bimodal trials. Conversely, the Colavita effect was reversed or reduced when the auditory stimulus preceded the visual stimulus . In addition, Koppen and Spence conducted an experiment in which participants showed a significantly larger Colavita effect when the auditory and visual stimuli were presented from the same spatial location, rather than from different locations. Based on these results, Koppen and his colleagues proposed that the \u2018unity effect\u2019 can adequately explain the role of spatial and temporal coincidence between stimuli in modulating the Colavita effect. According to the Unity effect, intersensory bias is greater when the participants unconsciously bind the two sensory events and believe that a single unimodal object is being perceived, rather than two separate events.",
            "score": 103.76674365997314
        },
        {
            "docid": "10751304_11",
            "document": "Motion-induced blindness . Hsu \"et al.\" (2004) compared MIB to a similar phenomenon of perceptual filling-in (PFI), which likewise reveals a striking dissociation between the percept and the sensory input. They describe both as visual attributes which are perceived in a certain region of the visual field regardless of being in the background (in the same manner as colour, brightness or texture) thus inducing target disappearance. They argue that because in both MIB and PFI the disappearance; or the incorporation of the background motion stimuli; becomes more profound with an increase in eccentricity, decrease in contrast and when perceptual grouping with other stimuli is controlled for; the two illusions are very likely to be a result of intermutual processes. Since MBI and PFI show to be structurally similar, it seems plausible that MIB can be a phenomenon responsible for completing missing information across the blind spot and scotomas where motion is involved.",
            "score": 142.02210235595703
        },
        {
            "docid": "5664_64",
            "document": "Consciousness . In neuroscience, a great deal of effort has gone into investigating how the perceived world of conscious awareness is constructed inside the brain. The process is generally thought to involve two primary mechanisms: (1) hierarchical processing of sensory inputs, and (2) memory. Signals arising from sensory organs are transmitted to the brain and then processed in a series of stages, which extract multiple types of information from the raw input. In the visual system, for example, sensory signals from the eyes are transmitted to the thalamus and then to the primary visual cortex; inside the cerebral cortex they are sent to areas that extract features such as three-dimensional structure, shape, color, and motion. Memory comes into play in at least two ways. First, it allows sensory information to be evaluated in the context of previous experience. Second, and even more importantly, working memory allows information to be integrated over time so that it can generate a stable representation of the world\u2014Gerald Edelman expressed this point vividly by titling one of his books about consciousness \"The Remembered Present\". In computational neuroscience, Bayesian approaches to brain function have been used to understand both the evaluation of sensory information in light of previous experience, and the integration of information over time. Bayesian models of the brain are probabilistic inference models, in which the brain takes advantage of prior knowledge to interpret uncertain sensory inputs in order to formulate a conscious percept; Bayesian models have successfully predicted many perceptual phenomena in vision and the nonvisual senses.",
            "score": 152.71766090393066
        },
        {
            "docid": "3883287_8",
            "document": "Tranquillity . Within tranquillity studies, much of the emphasis has been placed on understanding the role of vision in the perception of natural environments, which is probably not surprising, considering that upon first viewing a scene its configurational coherence can be established with incredible speed. Indeed, scene information can be captured in a single glance and the gist of a scene determined in as little as 100ms. The speed of processing of complex natural images was tested by Thorpe \"et al.\" using colour photographs of a wide range of animals (mammals, birds, reptiles and fish), in their natural environments, mixed with distracters that included pictures of forests, mountains, lakes, buildings and fruit. During this experiment, subjects were shown an image for 20ms and asked to determine whether it contained an animal or not. The electrophysiological brain responses obtained in this study showed that a decision could be made within 150ms of the image being seen, indicating the speed at which cognitive visual processing occurs. However, audition, and in particular the individual components that collectively comprise the soundscape, a term coined by Schafer to describe the ever-present array of sounds that constitute the sonic environment, also significantly inform the various schemata used to characterise differing landscape types. This interpretation is supported by the auditory reaction times, which are 50 to 60ms faster than that of the visual modality. It is also known that sound can alter visual perception and that under certain conditions areas of the brain involved in processing auditory information can be activated in response to visual stimuli.  Research conducted by Pheasant \"et al.\" has shown that when individuals make tranquillity assessments based on a uni-modal auditory or visual sensory input, they characterise the environment by drawing upon a number of key landscape and soundscape characteristics. For example, when making assessments in response to visual-only stimuli the percentage of water, flora and geological features present within a scene, positively influence how tranquil a location is perceived to be. Likewise when responding to uni-modal auditory stimuli, the perceived loudness of biological sounds positively influences the perception of tranquillity, whilst the perceived loudness of mechanical sounds have a negative effect. However, when presented with bi-modal auditory-visual stimuli the individual soundscape and landscape components alone no longer influenced the perception of tranquillity. Rather configurational coherence was provided by the percentage of natural and contextual features present within the scene and the equivalent continuous sound pressure level (LAeq).",
            "score": 135.5767022371292
        },
        {
            "docid": "37759941_2",
            "document": "Crossmodal attention . Crossmodal attention refers to the distribution of attention to different senses. Attention is the cognitive process of selectively emphasizing and ignoring sensory stimuli. According to the crossmodal attention perspective, attention often occurs simultaneously through multiple sensory modalities. These modalities process information from the different sensory fields, such as: visual, auditory, spatial, and tacitile. While each of these is designed to process a specific type of sensory information, there is considerable overlap between them which has led researchers to question whether attention is modality-specific or the result of shared \"cross-modal\" resources. \"Cross-modal attention\" is considered to be the overlap between modalities that can both enhance and limit attentional processing. The most common example given of crossmodal attention is the Cocktail Party Effect, which is when a person is able to focus and attend to one important stimulus instead of other less important stimuli. This phenomenon allows deeper levels of processing to occur for one stimuli while others are then ignored.",
            "score": 113.03816533088684
        },
        {
            "docid": "490258_31",
            "document": "Split-brain . One of the experiments involving VP attempted to investigate systematically the types of visual information that could be transferred via VP's spared splenial fibers. The first experiment was designed to assess VP's ability to make between-field perceptual judgements about simultaneously presented pairs of stimuli. The stimuli were presented in varying positions with respect to the horizontal and vertical midline with VP's vision fixated on a central crosshair. The judgements were based on differences in color, shape or size. The testing procedure was the same for all three types of stimuli; after presentation of each pair, VP verbally responded \"yes\" if the two items in the pair were identical and \"no\" if they were not. The results show that there was no perceptual transfer for color, size or shape with binomial tests showing that VP's accuracy was not greater than chance.",
            "score": 95.41083455085754
        },
        {
            "docid": "35347567_7",
            "document": "Antti Revonsuo . According to Revonsuo, the dreaming brain is particularly suitable model system for the study of consciousness because it generates a conscious experience while being isolated from both sensory input and motor output. Regarding the rival paradigm of visual awareness, Revonsuo argues that it does not allow one to distinguish between consciousness and perception. Revonsuo holds that there is a \"'double dissociation' between consciousness and perceptual input\". Accordingly, dreams are conscious experiences, which occur without any perceptual stimuli, and, conversely, perceptual input does not automatically engender conscious experience. In support of the independence of consciousness from perception, Revonsuo cites Stephen LaBerge's case study on a lucid dreamer performing previously agreed upon eye movements to signal to the experimenters that he had become conscious of the fact that he was dreaming. A second study that supports Revonsuo's view of dreams was conducted by Allan Rechtschaffen and Foulkes (1965). In this study, subjects were made to sleep with their eyelids open, thus allowing the visual cortex to receive visual stimuli. Though their eyes were open, and the perceptual input was accessible, the subjects could not see the stimuli and did not report dreaming of it. It is the brain that is having the internal experience, independent of perceptual input. This internalist view of consciousness leads Revonsuo to compare both dreaming and waking consciousness with a virtual reality simulation decoupled from or only indirectly informed by a brain's external environment.",
            "score": 140.95518338680267
        },
        {
            "docid": "2613534_22",
            "document": "Visual extinction . Visual extinction has also been used to demonstrate brain bias towards gestalt processing. When presented with a figure containing illusory contours, patients were able to correctly report the presence of stimuli in both contralesional and ipsilesional hemispheres, due to their unconscious processing of the whole field to produce the illusion. This experiment implied that the attention center prioritizes the visualization of surfaces over other stimuli \u2013 therefore, although under race model the ipsilesional stimuli should extinguish the contralesional, the creation of the gestalt takes priority over detection of both. Further, a study using Gabor signals (alternating blurred and noisy black and white bars, commonly used by opticians in diagnostic tests) investigated how the orientation of these signals affected their extinction rate. Bilateral stimuli were least extinguished when both stimuli were oriented horizontally, although both stimuli being oriented vertically also showed a reduction in extinguishing rate when compared to one stimulus vertical and one horizontal \u2013 in what could be assumed by the brain to represent two different surfaces.",
            "score": 118.6964819431305
        },
        {
            "docid": "176997_2",
            "document": "Blindsight . Blindsight is the ability of people who are cortically blind due to lesions in their striate cortex, also known as primary visual cortex or V1, to respond to visual stimuli that they do not consciously see. The majority of studies on blindsight are conducted on patients who have the conscious blindness on only one side of their visual field. Following the destruction of the striate cortex, patients are asked to detect, localize, and discriminate amongst visual stimuli that are presented to their blind side, often in a forced-response or guessing situation, even though they do not consciously recognize the visual stimulus. Research shows that blind patients achieve a higher accuracy than would be expected from chance alone. \"Type 1 blindsight\" is the term given to this ability to guess\u2014at levels significantly above chance\u2014aspects of a visual stimulus (such as location or type of movement) without any conscious awareness of any stimuli. \"Type 2 blindsight\" occurs when patients claim to have a feeling that there has been a change within their blind area\u2014e.g. movement\u2014but that it was not a visual percept. Blindsight challenges the common belief that perceptions must enter consciousness to affect our behavior; showing that our behavior can be guided by sensory information of which we have no conscious awareness. It may be thought of as a converse of the form of anosognosia known as Anton\u2013Babinski syndrome, in which there is full cortical blindness along with the confabulation of visual experience.",
            "score": 185.39319396018982
        },
        {
            "docid": "2768467_2",
            "document": "Interstimulus interval . The interstimulus interval (often abbreviated as ISI) is the temporal interval between the offset of one stimulus to the onset of another. For instance, Max Wertheimer did experiments with two stationary, flashing lights that at some interstimulus intervals appeared to the subject as moving instead of stationary. In these experiments, the interstimulus interval is simply the time between the two flashes. The ISI plays a large role in the phi phenomenon (Wertheimer) since the illusion of motion is directly due to the length of the interval between stimuli. When the ISI is shorter, for example between two flashing lines alternating back and forth, we perceive the change in stimuli to be movement. Wertheimer discovered that the space between the two lines is filled in by our brains and that the faster the lines alternate, the more likely we are to perceive it as one line moving back and forth. When the stimuli move fast enough, this creates the illusion of a moving picture like a movie or cartoon. Phi phenomenon is very similar to beta movement.",
            "score": 133.63702249526978
        },
        {
            "docid": "3522879_5",
            "document": "P300 (neuroscience) . In 1965, Sutton and colleagues published results from two experiments that further explored this late positivity. They presented subjects with either a cue that indicated whether the following stimulus would be a click or a flash, or a cue which required subjects to guess whether the following stimulus would be a click or a flash. They found that when subjects were required to guess what the following stimulus would be, the amplitude of the \"late positive complex\" was larger than when they knew what the stimulus would be. In a second experiment, they presented two cue types. For one cue there was a 2 in 3 chance that the following stimulus would be a click and a 1 in 3 chance that the following stimulus would be a flash. The second cue type had probabilities that were the reverse of the first. They found that the amplitude of the positive complex was larger in response to the less probable stimuli, or the one that only had a 1 in 3 chance of appearing. Another important finding from these studies is that this late positive complex was observed for both the clicks and flashes, indicating that the physical type of the stimulus (auditory or visual) did not matter.",
            "score": 85.88635158538818
        },
        {
            "docid": "27336635_5",
            "document": "P3b . In 1965, Sutton and colleagues published results from two experiments that further explored this late positivity. They presented subjects with either a cue that indicated whether the following stimulus would be a click or a flash, or a cue which required subjects to guess whether the following stimulus would be a click or a flash. They found that when subjects were required to guess what the following stimulus would be, the amplitude of the \"late positive complex\" was larger than when they knew what the stimulus would be. In a second experiment, they presented two cue types. For one cue there was a 2 in 3 chance that the following stimulus would be a click and a 1 in 3 chance that the following stimulus would be a flash. The second cue type had probabilities that were the reverse of the first. They found that the amplitude of the positive complex was larger in response to the less probable stimuli, or the one that only had a 1 in 3 chance of appearing. Another important finding from these studies is that this late positive complex was observed for both the clicks and flashes, indicating that the physical type of the stimulus (auditory or visual) did not matter.",
            "score": 85.88635158538818
        },
        {
            "docid": "35970915_2",
            "document": "Colavita visual dominance effect . The Colavita visual dominance effect refers to the phenomenon where participants respond more often to the visual component of an audiovisual stimulus, when presented with bimodal stimuli. Research has shown that vision is the most dominant sense for human beings who do not suffer from sensory difficulties (e.g. blindness, cataracts). Theorists have proposed that the Colavita visual dominance effect demonstrates a bias toward visual sensory information, because the presence of auditory stimuli is commonly neglected during audiovisual events.",
            "score": 131.28276419639587
        },
        {
            "docid": "5366050_50",
            "document": "Speech perception . Neurophysiological methods rely on utilizing information stemming from more direct and not necessarily conscious (pre-attentative) processes. Subjects are presented with speech stimuli in different types of tasks and the responses of the brain are measured. The brain itself can be more sensitive than it appears to be through behavioral responses. For example, the subject may not show sensitivity to the difference between two speech sounds in a discrimination test, but brain responses may reveal sensitivity to these differences. Methods used to measure neural responses to speech include event-related potentials, magnetoencephalography, and near infrared spectroscopy. One important response used with event-related potentials is the mismatch negativity, which occurs when speech stimuli are acoustically different from a stimulus that the subject heard previously.",
            "score": 129.90957689285278
        },
        {
            "docid": "27920631_5",
            "document": "Visual capture . When two sensory stimuli are presented simultaneously, vision is capable of dominating and capturing the other. This occurs as visual cues can distract from other sensations, causing the origin of the stimulus to appear as if it is being produced by the visual cue. Therefore, when an individual is in an environment, and multiple stimuli reach the brain at once, there is a hierarchy that vision will guide the rest of the somatosensory cues to be perceived as though they align with the visual experience, despite where their original source may be. Research has found that the visual and auditory reflexive spatial orienting are controlled through a common underlying neural substrate. Furthermore, studies have shown that vision has an effect in cognitive neuroscience, and provides for a significant effect when visually attended to. This dominance is seen again through a visual-haptic task that vision is capable of making better judgements of an object that physically touching it. It has also been determined, that there are certain amounts of visual capture that occur depending on the task, sometimes allowing the visual system to be entirely dominant, while others provide haptic cues to be prominent.",
            "score": 114.80599355697632
        },
        {
            "docid": "386062_12",
            "document": "Wishful thinking . Some speculate that wishful seeing results from cognitive penetrability in that higher cognitive functions are able to directly influence perceptual experience instead of only influencing perception at higher levels of processing. Those that argue against cognitive penetrability feel that sensory systems operate in a modular fashion with cognitive states exerting their influence only after the stimuli has been perceived. The phenomenon of wishful seeing implicates cognitive penetrability in the perceptual experience. Wishful seeing has been observed to occur in early stages of categorization. Research using ambiguous figures and binocular rivalry exhibit this tendency. Perception is influenced by both top-down and bottom-up processing. In visual processing, bottom-up processing is a rigid route compared to flexible top-down processing. Within bottom-up processing, the stimuli are recognized by fixation points, proximity and focal areas to build objects, while top-down processing is more context sensitive. This effect can be observed via priming as well as with emotional states. The traditional hierarchical models of information processing describe early visual processing as a one-way street: early visual processing goes into conceptual systems, but conceptual systems do not affect visual processes. Currently, research rejects this model and suggests conceptual information can penetrate early visual processing rather than just biasing the perceptual systems. This occurrence is called conceptual or cognitive penetrability. Research on conceptual penetrability utilize stimuli of conceptual-category pairs and measure the reaction time to determine if the category effect influenced visual processing, The category effect is the difference in reaction times within the pairs such as \"Bb\" to \"Bp\". To test conceptual penetrability, there were simultaneous and sequential judgments of pairs. The reaction times decreased as the stimulus onset asynchrony increased, supporting categories affect visual representations and conceptual penetrability. Research with richer stimuli such as figures of cats and dogs allow for greater perceptual variability and analysis of stimulus typicality (cats and dogs were arranged in various positions, some more or less typical for recognition). Differentiating the pictures took longer when they were within the same category (dog-dog) compared between categories (dog-cat) supporting category knowledge influences categorization. Therefore, visual processing measured by physical differential judgments is affected by non-visual processing supporting conceptual penetrability.",
            "score": 112.48467719554901
        },
        {
            "docid": "1903855_7",
            "document": "Sensory substitution . In a regular visual system, the data collected by the retina is converted into an electrical stimulus in the optic nerve and relayed to the brain, which re-creates the image and perceives it. Because it is the brain that is responsible for the final perception, sensory substitution is possible. During sensory substitution an intact sensory modality relays information to the visual perception areas of the brain so that the person can perceive to see. With sensory substitution, information gained from one sensory modality can reach brain structures physiologically related to other sensory modalities. Touch-to-visual sensory substitution transfers information from touch receptors to the visual cortex for interpretation and perception. For example, through fMRI, we can determine which parts of the brain are activated during sensory perception. In blind persons, we can see that while they are only receiving tactile information, their visual cortex is also activated as they perceive to \"see\" objects. We can also have touch to touch sensory substitution where information from touch receptors of one region can be used to perceive touch in another region. For example, in one experiment by Bach-y-Rita, he was able to restore the touch perception in a patient who lost peripheral sensation from leprosy.",
            "score": 200.59748792648315
        },
        {
            "docid": "25225295_15",
            "document": "Consumer neuroscience . A study by McClure et al. investigated the difference in branding between Coca-Cola and Pepsi. The study found that when the two drinks were tasted blind there was no difference in consumer preference between the brands. Both drinks produced equal activation in the ventromedial prefrontal cortex, which is thought to be activated because the taste is rewarding. When the subjects were informed of the brand names the consumers preferred Coke, and only Coke activated the ventromedial prefrontal cortex, suggesting that drinking the Coke brand is rewarding beyond simply the taste itself. More subjects preferred Coke when they knew it was Coke than when the taste testing was anonymous, which demonstrates the power of branding to influence consumer behavior. There was also significant activation in the hippocampus and dorsolateral prefrontal cortex when subjects knew they were drinking Coke. These brain structures are known to play a role in memory and recollection, which indicates they are helping the subjects to connect their present drinking experience to previous brand associations. The study proposes that there are two separate processes contributing to consumer decision making: the ventromedial prefrontal cortex responds to sensory inputs and the hippocampus and dorsolateral prefrontal cortex recall previous associations to cultural information. According to the results of this study, the Coke brand has much more firmly established itself as a rewarding experience.",
            "score": 102.66147112846375
        },
        {
            "docid": "8534246_4",
            "document": "Implicit personality theory . One of the most notable characteristics of implicit personality theories is that they are, in fact, implicit. In this context, \"implicit\" is taken to mean \"automatic\". It is a common belief that much of the process of social perception actually is automated. For example, it is possible for a person to experience automatic thought processes, and for those processes occur without that person's intention or awareness of their occurrence. In terms of impression formation, this means that an observer may perceive another person's behavior and automatically make trait inferences from that behavior, without being aware that these inferences were being made. The strongest evidence for the implicitness of impression formation comes from observed \"savings effects\" when trying to learn another person's traits. In a study by Carlston & Skowronski (1994), participants who were exposed to descriptive stimuli containing implied trait information learned the target person's traits more easily than participants who had not been previously exposed to implied trait information. Moreover, this effect could not be accounted for by simple priming mechanisms. The participants exhibited a true savings effect, which suggested that they had gained implicit trait information from the descriptive stimuli.",
            "score": 84.79848194122314
        },
        {
            "docid": "569399_2",
            "document": "Stimulus (physiology) . In physiology, a stimulus (plural stimuli) is a detectable change in the internal or external environment. The ability of an organism or organ to respond to external stimuli is called sensitivity. When a stimulus is applied to a sensory receptor, it normally elicits or influences a reflex via stimulus transduction. These sensory receptors can receive information from outside the body, as in touch receptors found in the skin or light receptors in the eye, as well as from inside the body, as in chemoreceptors and mechanoreceptors. An internal stimulus is often the first component of a homeostatic control system. External stimuli are capable of producing systemic responses throughout the body, as in the fight-or-flight response. In order for a stimulus to be detected with high probability, its level must exceed the absolute threshold; if a signal does reach threshold, the information is transmitted to the central nervous system (CNS), where it is integrated and a decision on how to react is made. Although stimuli commonly cause the body to respond, it is the CNS that finally determines whether a signal causes a reaction or not.",
            "score": 124.09973084926605
        },
        {
            "docid": "53472_6",
            "document": "Illusion . An optical illusion is characterised by visually perceived images that are deceptive or misleading. Therefore, the information gathered by the eye is processed by the brain to give, on the face of it, a percept that does not tally with a physical measurement of the stimulus source. A conventional assumption is that there are physiological illusions that occur naturally and cognitive illusions that can be demonstrated by specific visual tricks that say something more basic about how human perceptual systems work. The human brain constructs a world inside our head based on what it samples from the surrounding environment. However, sometimes it tries to organise this information it thinks best while other times it fills in the gaps. This way in which our brain works is the basis of an illusion.",
            "score": 150.07145142555237
        },
        {
            "docid": "386062_23",
            "document": "Wishful thinking . Balcetis and Dunning (2013) investigated wishful seeing by conducting two experiments, one involving two ambiguous stimuli that could be perceived as \"B\" or \"13\", and the other either a horse or a seal. The second experiment was a binocular rivalry test in which the participants were presented simultaneously with the letter \"H\" or number \"4\" (one stimuli in each eye). In each experiment, the experimenters associated one of the stimuli with desirable outcomes, and the other with a negative outcome (i.e. the \"B\" was associated with freshly squeezed orange juice while the \"13\" was associated with an undesirable health food smoothie, and in the binocular rivalry experiment, letters were associated with economic gain while numbers were associated with economic loss). The results of the experiment demonstrated that participants were more likely to perceive the stimulus that was associated with a positive situation or outcome than the stimulus associated with negative situations. This strong correlation between perception and positive versus negative stimuli demonstrates that we tend to see the world based on our own desires. The concept of wishful seeing hints towards a motivation-based perception process.",
            "score": 113.20320463180542
        }
    ],
    "r": [
        {
            "docid": "6082997_2",
            "document": "Filling-in . In vision, filling-in phenomena are those responsible for the completion of missing information across the physiological blind spot, and across natural and artificial scotomata. There is also evidence for similar mechanisms of completion in normal visual analysis. Classical demonstrations of perceptual filling-in involve filling in at the blind spot in monocular vision, and images stabilized on the retina either by means of special lenses, or under certain conditions of steady fixation. For example, naturally in monocular vision at the physiological blind spot, the percept is not a hole in the visual field, but the content is \u201cfilled-in\u201d based on information from the surrounding visual field. When a textured stimulus is presented centered on but extending beyond the region of the blind spot, a continuous texture is perceived. This partially inferred percept is paradoxically considered more reliable than a percept based on external input. (Ehinger \"et al.\" 2017).",
            "score": 216.23306274414062
        },
        {
            "docid": "1903855_7",
            "document": "Sensory substitution . In a regular visual system, the data collected by the retina is converted into an electrical stimulus in the optic nerve and relayed to the brain, which re-creates the image and perceives it. Because it is the brain that is responsible for the final perception, sensory substitution is possible. During sensory substitution an intact sensory modality relays information to the visual perception areas of the brain so that the person can perceive to see. With sensory substitution, information gained from one sensory modality can reach brain structures physiologically related to other sensory modalities. Touch-to-visual sensory substitution transfers information from touch receptors to the visual cortex for interpretation and perception. For example, through fMRI, we can determine which parts of the brain are activated during sensory perception. In blind persons, we can see that while they are only receiving tactile information, their visual cortex is also activated as they perceive to \"see\" objects. We can also have touch to touch sensory substitution where information from touch receptors of one region can be used to perceive touch in another region. For example, in one experiment by Bach-y-Rita, he was able to restore the touch perception in a patient who lost peripheral sensation from leprosy.",
            "score": 200.5974884033203
        },
        {
            "docid": "4561848_3",
            "document": "Blind spot (vision) . Because there are no cells to detect light on the optic disc, the corresponding part of the field of vision is invisible. Some process in our brains interpolates the blind spot based on surrounding detail and information from the other eye, so we do not normally perceive the blind spot.",
            "score": 190.82850646972656
        },
        {
            "docid": "176997_2",
            "document": "Blindsight . Blindsight is the ability of people who are cortically blind due to lesions in their striate cortex, also known as primary visual cortex or V1, to respond to visual stimuli that they do not consciously see. The majority of studies on blindsight are conducted on patients who have the conscious blindness on only one side of their visual field. Following the destruction of the striate cortex, patients are asked to detect, localize, and discriminate amongst visual stimuli that are presented to their blind side, often in a forced-response or guessing situation, even though they do not consciously recognize the visual stimulus. Research shows that blind patients achieve a higher accuracy than would be expected from chance alone. \"Type 1 blindsight\" is the term given to this ability to guess\u2014at levels significantly above chance\u2014aspects of a visual stimulus (such as location or type of movement) without any conscious awareness of any stimuli. \"Type 2 blindsight\" occurs when patients claim to have a feeling that there has been a change within their blind area\u2014e.g. movement\u2014but that it was not a visual percept. Blindsight challenges the common belief that perceptions must enter consciousness to affect our behavior; showing that our behavior can be guided by sensory information of which we have no conscious awareness. It may be thought of as a converse of the form of anosognosia known as Anton\u2013Babinski syndrome, in which there is full cortical blindness along with the confabulation of visual experience.",
            "score": 185.3931884765625
        },
        {
            "docid": "2534964_14",
            "document": "Sensory processing . Perhaps one of the most studied sensory integrations is the relationship between vision and audition. These two senses perceive the same objects in the world in different ways, and by combining the two, they help us understand this information better. Vision dominates our perception of the world around us. This is because visual spatial information is one of the most reliable sensory modalities. Visual stimuli are recorded directly onto the retina, and there are few, if any, external distortions that provide incorrect information to the brain about the true location of an object. Other spatial information is not as reliable as visual spatial information. For example, consider auditory spatial input. The location of an object can sometimes be determined solely on its sound, but the sensory input can easily be modified or altered, thus giving a less reliable spatial representation of the object. Auditory information therefore is not spatially represented unlike visual stimuli. But once one has the spatial mapping from the visual information, multisensory integration helps bring the information from both the visual and auditory stimuli together to make a more robust mapping.",
            "score": 183.71603393554688
        },
        {
            "docid": "15559385_11",
            "document": "Tactile discrimination . When a person has become blind, in order to \u201csee\u201d the world, their other senses become heightened. An important sense for the blind is their sense of touch, which becomes more frequently used to help them perceive the world. People that are blind have displayed that their visual cortices become more responsive to auditory and tactile stimulation. Braille allows the blind to be able to use their sense of touch to feel the roughness, and distance of various patterns to be used as a form of language. Within the brain, the activation of the occipital cortex is functionally relevant for tactile braille reading, as well as the somatosensory cortex. These various parts of the brain function in their own way, in which they each contribute to the effectiveness of how braille is read by the blind. People that are blind also rely heavily on Tactile Gnosis, Spatial discrimination, Graphesthesia, and Two-point discrimination. Essentially, the occipital cortex allows one to effectively make judgements on the distance of braille patterns, which is related to spatial discrimination. Meanwhile, the somatosensory cortex allows one to effectively make judgements on the roughness of braille patterns, which is related to two-point discrimination. The various visual areas in the brain are very essential for a blind person to read braille, just as much as it is for a person that has sight. Essentially, whether one is blind or not, the perception of objects that involves tactile discrimination is not impaired if one cannot see. When comparing people that are blind to people that have sight, the amount of activity within the their somatosensory and visual areas of the brain do differ. The activity in the somatosensory and visual areas are not as high in tactile gnosis for people that are not blind, and are more-so active for more visual related stimuli that does not involve touch. Nonetheless, there is a difference in these various areas within the brain when comparing the blind to the sighted, which is that shape discrimination causes a difference in brain activity, as well as tactile gnosis. The visual cortices of blind individuals are active during various vision related tasks including tactile discrimination, and the function of the cortices resemble the activity of adults with sight.",
            "score": 169.322265625
        },
        {
            "docid": "6082997_17",
            "document": "Filling-in . \"Perceptual filling-in\", in its simplest definition, is simply the filling-in of information that is not directly given to the sensory input. The missing information is inferred or extrapolated from visual data acquired in a different part of the visual field. Examples of filling-in phenomena include lightness assignment to surfaces from information of contrast across the edges and completion of features and textures across the blind spot, based on the features and textures that are detected in the visible part of the image. In this definition, it is clear that a filling-in process involves a rearrangement of visual information, in which activity in one region of the visual field (i.e. edges) is assigned to other regions (surfaces). In any event, the total amount of information available is not increased, being determined by the retinal input, and any rearrangement of information is useful only if it brings the information contained in the image into a form that is more easily analyzed by our brain.",
            "score": 169.0050048828125
        },
        {
            "docid": "433584_4",
            "document": "McGurk effect . Vision is the primary sense for humans, but speech perception is multimodal, which means that it involves information from more than one sensory modality, in particular, audition and vision. The McGurk effect arises during phonetic processing because the integration of audio and visual information happens early in speech perception. The McGurk effect is very robust; that is, knowledge about it seems to have little effect on one's perception of it. This is different from certain optical illusions, which break down once one 'sees through' them. Some people, including those that have been researching the phenomenon for more than twenty years, experience the effect even when they are aware that it is taking place. With the exception of people who can identify most of what is being said from speech-reading alone, most people are quite limited in their ability to identify speech from visual-only signals. A more extensive phenomenon is the ability of visual speech to increase the intelligibility of heard speech in a noisy environment. Visible speech can also alter the perception of perfectly audible speech sounds when the visual speech stimuli are mismatched with the auditory speech. Normally, speech perception is thought to be an auditory process; however, our use of information is immediate, automatic, and, to a large degree, unconscious and therefore, despite what is widely accepted as true, speech is not only something we hear. Speech is perceived by all of the senses working together (seeing, touching, and listening to a face move). The brain is often unaware of the separate sensory contributions of what it perceives. Therefore, when it comes to recognizing speech the brain cannot differentiate whether it is seeing or hearing the incoming information.",
            "score": 161.1212615966797
        },
        {
            "docid": "2534964_12",
            "document": "Sensory processing . It may seem redundant that we are being provided with multiple sensory inputs about the same object, but that is not necessarily the case. This so-called \"redundant\" information is in fact verification that what we are experiencing is in fact happening. Perceptions of the world are based on models that we build of the world. Sensory information informs these models, but this information can also confuse the models. Sensory illusions occur when these models do not match up. For example, where our visual system may fool us in one case, our auditory system can bring us back to a ground reality. This prevents sensory misrepresentations, because through the combination of multiple sensory modalities, the model that we create is much more robust and gives a better assessment of the situation. Thinking about it logically, it is far easier to fool one sense than it is to simultaneously fool two or more senses.",
            "score": 156.7516632080078
        },
        {
            "docid": "1677048_42",
            "document": "Inattentional blindness . The theory behind inattentional blindness research suggests that we consciously experience only those objects and events to which we directly attend. That means that the vast majority of information in our field of vision goes unnoticed. Thus if we miss the target stimulus in an experiment, but are later told about the existence of the stimulus, this sufficient awareness allows participants to report and recall the stimulus now that attention has been allocated to it. Mack and Rock, and their colleagues discovered a striking array of visual events to which people are inattentionally blind. However the debate arises whether this inattentional blindness was due to memory or perceptual processing limitations.",
            "score": 155.02101135253906
        },
        {
            "docid": "4427536_5",
            "document": "Stumbling on Happiness . Also, Gilbert covers the topic of 'filling in' or the frequent use of patterns, by the mind, to connect events which we do actually recall with other events we expect or anticipate fit into the expected experience. This 'filling in' is also used by our eyes and optic nerves to remove our blind spot or scotoma, and instead substitute what our mind expects to be present in the blind spot.",
            "score": 154.5107421875
        },
        {
            "docid": "20395179_7",
            "document": "Vittorio Gallese . Observing the world is more complex than the mere activation of the visual brain. Vision is multimodal: it encompasses the activation of motor, somatosensory and emotion-related brain networks. Any intentional relation entertained with the external world has an intrinsic pragmatic nature, hence it always bears a motor content. The same motor circuits that control our motor behavior also map the space around us, the objects at hand in that very same space, thus defining and shaping in motor terms their representational content. The space around us is defined by the motor potentialities of our body. Motor neurons also respond to visual, tactile and auditory stimuli. Indeed, premotor neurons controlling the movements of the upper arm also respond to tactile stimuli applied to it, to visual stimuli moved within the arm's peripersonal space, or to auditory stimuli also coming from the same peri-personal space. The same applies to artifacts, like three-dimensional objects. The manipulable objects we look at are classified by the motor brain as potential targets of the interactions we might entertain with them. Premotor and parietal 'canonical neurons' control the grasping and manipulation of objects and also respond to their mere observation. The functional architecture of embodied simulation seems to constitute a basic characteristic of our brain, making possible our rich and diversified experiences of space, objects and other individuals, being at the basis of our capacity to empathize with them.\"",
            "score": 153.5865936279297
        },
        {
            "docid": "24978422_3",
            "document": "Visual adaptation . The aftereffects of exposure to a visual stimulus or pattern causes loss of sensitivity to that pattern and induces stimulus bias. An example of this phenomenon is the \"lilac chaser\", introduced by Jeremy Hinton. The stimulus here are lilac circles, that once removed, leave green circles that then become the most prominent stimulus. The fading of the lilac circles is due to a loss of sensitivity to that stimulus and the adaptation to the new stimulus. To experience the \"lilac chaser\" effect, the subject needs to fixate their eyes on the cross in the middle of the image, and after a while the effect will settle in. Visual coding, a process involved in visual adaptation, is the means by which the brain adapts to certain stimuli, resulting in a biased perception of those stimuli. This phenomenon is referred to as visual plasticity; the brain's ability to change and adapt according to certain, repeated stimuli, altering the way information is perceived and processed. The rate and strength of visual adaptation depends heavily on the number of stimuli presented simultaneously, as well as the amount of time for which the stimulus is present. Visual adaptation was found to be weaker when there were more stimuli present. Moreover, studies have found that stimuli can rival each other, which explains why higher numbers of simultaneous stimuli lead to lower stimulus adaptation. Studies have also found that visual adaptation can have a reversing effect; if the stimulus is absent long enough, the aftereffects of visual adaptation will subside. Studies have also shown that visual adaptation occurs in the early stages of processing.",
            "score": 153.0927734375
        },
        {
            "docid": "5664_64",
            "document": "Consciousness . In neuroscience, a great deal of effort has gone into investigating how the perceived world of conscious awareness is constructed inside the brain. The process is generally thought to involve two primary mechanisms: (1) hierarchical processing of sensory inputs, and (2) memory. Signals arising from sensory organs are transmitted to the brain and then processed in a series of stages, which extract multiple types of information from the raw input. In the visual system, for example, sensory signals from the eyes are transmitted to the thalamus and then to the primary visual cortex; inside the cerebral cortex they are sent to areas that extract features such as three-dimensional structure, shape, color, and motion. Memory comes into play in at least two ways. First, it allows sensory information to be evaluated in the context of previous experience. Second, and even more importantly, working memory allows information to be integrated over time so that it can generate a stable representation of the world\u2014Gerald Edelman expressed this point vividly by titling one of his books about consciousness \"The Remembered Present\". In computational neuroscience, Bayesian approaches to brain function have been used to understand both the evaluation of sensory information in light of previous experience, and the integration of information over time. Bayesian models of the brain are probabilistic inference models, in which the brain takes advantage of prior knowledge to interpret uncertain sensory inputs in order to formulate a conscious percept; Bayesian models have successfully predicted many perceptual phenomena in vision and the nonvisual senses.",
            "score": 152.71766662597656
        },
        {
            "docid": "53472_6",
            "document": "Illusion . An optical illusion is characterised by visually perceived images that are deceptive or misleading. Therefore, the information gathered by the eye is processed by the brain to give, on the face of it, a percept that does not tally with a physical measurement of the stimulus source. A conventional assumption is that there are physiological illusions that occur naturally and cognitive illusions that can be demonstrated by specific visual tricks that say something more basic about how human perceptual systems work. The human brain constructs a world inside our head based on what it samples from the surrounding environment. However, sometimes it tries to organise this information it thinks best while other times it fills in the gaps. This way in which our brain works is the basis of an illusion.",
            "score": 150.07144165039062
        },
        {
            "docid": "3132998_13",
            "document": "Lilac chaser . If one closes the right eye and moves close to the stimulus so that the nine-o'clock disc falls in the blind spot, one sees that the movement is no longer smooth. There is a noticeable pause when the disappearance of the disc occurs on the region of the retina having no rods or cones. This suggests there are limits to the filling-in that normally prevents us from noticing a black hole in our visual fields at the location of the blind spot.",
            "score": 149.56744384765625
        },
        {
            "docid": "26945761_5",
            "document": "Cross modal plasticity . The somatosensory cortex is also able to recruit the visual cortex to assist with tactile sensation. Cross modal plasticity reworks the network structure of the brain, leading to increased connections between the somatosensory and visual cortices. Furthermore, the somatosensory cortex acts as a hub region of nerve connections in the brain for the early blind but not for the sighted. With this cross-modal networking the early blind are able to react to tactile stimuli with greater speed and accuracy, as they have more neural pathways to work with. One element of the visual system that the somatosensory cortex is able to recruit is the dorsal-visual stream. The dorsal stream is used by the sighted to identify spatial information visually, but the early blind use it during tactile sensation of 3D objects. However, both sighted and blind participants used the dorsal stream to process spatial information, suggesting that cross modal plasticity in the blind re-routed the dorsal visual stream to work with the sense of touch rather than changing the overall function of the stream.",
            "score": 148.4586944580078
        },
        {
            "docid": "25559670_8",
            "document": "Distorted vision . Migraine headaches may be preceded by a visual \"aura\", lasting for 20 to 30 minutes, and then proceeding to the headache. Some people, however, experience the aura but do not have a headache. This visual aura can be very dramatic. Classically, a small blind spot appears in the central vision with a shimmering, zig-zag light inside of it. This enlarges, and moves to one side or the other of the vision, over a 20 to 30 minute period. When it is large, this crescent shaped blind spot containing this brightly flashing light can be difficult to ignore, and some people fear that they are having a stroke. In reality, it is generally a harmless phenomenon, except in people who subsequently get the headache of migraine. Since migraine originates in the brain, the visual effect typically involves the same side of vision in each eye, although it may seem more prominent in one eye or the other.",
            "score": 145.70098876953125
        },
        {
            "docid": "490620_37",
            "document": "Human brain . Vision is generated by light that hits the retina of the eye. Photoreceptors in the retina transduce the sensory stimulus of light into an electrical nerve signal that is sent to the visual cortex in the occipital lobe. Vision from the left visual field is received on the right side of each retina (and vice versa) and passes through the optic nerve until some information changes sides, so that all information about one side of the visual field passes through tracts in the opposite side of the brain. The nerves reach the brain at the lateral geniculate nucleus, and travel through the optic radiation to reach the visual cortex.",
            "score": 144.82579040527344
        },
        {
            "docid": "893668_12",
            "document": "Bias blind spot . Pronin also hypothesizes ways to use awareness of the bias blind spot to reduce conflict, and to think in a more \"scientifically informed\" way. Although we are unable to control bias on our own cognitions, one may keep in mind that biases are acting on everyone. Pronin suggests that people might use this knowledge to separate other's intentions from their actions.",
            "score": 144.781494140625
        },
        {
            "docid": "23047_41",
            "document": "Pseudoscience . Michael Shermer's theory of belief-dependent realism is driven by the belief that the brain is essentially a \"belief engine,\" which scans data perceived by the senses and looks for patterns and meaning. There is also the tendency for the brain to create cognitive biases, as a result of inferences and assumptions made without logic and based on instinct \u2014 usually resulting in patterns in cognition. These tendencies of patternicity and agenticity are also driven \"by a meta-bias called the bias blind spot, or the tendency to recognize the power of cognitive biases in other people but to be blind to their influence on our own beliefs.\" Lindeman states that social motives (i.e., \"to comprehend self and the world, to have a sense of control over outcomes, to belong, to find the world benevolent and to maintain one's self-esteem\") are often \"more easily\" fulfilled by pseudoscience than by scientific information. Furthermore, pseudoscientific explanations are generally not analyzed rationally, but instead experientially. Operating within a different set of rules compared to rational thinking, experiential thinking regards an explanation as valid if the explanation is \"personally functional, satisfying and sufficient\", offering a description of the world that may be more personal than can be provided by science and reducing the amount of potential work involved in understanding complex events and outcomes.",
            "score": 144.04177856445312
        },
        {
            "docid": "35982062_8",
            "document": "Biased Competition Theory . Bottom-up processes are characterized by an absence of higher level direction in sensory processing. It primarily relies on sensory information and incoming sensory information is the starting point for all Bottom-up processing. Bottom-up refers to when a feature stands out in a visual search. This is commonly called the \u201cpop-out\u201d effect. Salient features like bright colors, movement and big objects make the object \u201cpop-out\u201d of the visual search. \u201cPop-out\u201d features can often attract attention without conscious processing. Objects that stand out are often given priority (bias) in processing. Bottom-up processing is data driven, and according to this stimuli are perceived on the basis of the data which is being experienced through the senses. Evidence suggests that simultaneously presented stimuli do in fact compete in order to be represented in the visual cortex, with stimuli mutually suppressing each other to gain this representation. This was examined by Reynolds and colleagues, who looked at the size of neurons\u2019 receptive field\u2019s within the visual cortex. It was found that the presentation of a single stimulus resulted in a low firing rate while two stimuli presented together resulted in a higher firing rate. Reynolds and colleagues also found that when comparing the neural response of an individually presented visual stimulus to responses gathered from simultaneously presented stimuli, the responses of the concurrent presented stimuli were less than the sum of the responses gathered when each stimuli was presented alone. This suggests that two stimuli presented together increase neural work load required for attention. This increased neural load creates suppressive processes and causes the stimuli to compete for neural representation in the brain. Proulx and Egeth predicted that brighter objects would bias attention in favor of that object. Another prediction is that larger objects would bias the attention in favor of that object. The experiment was a computer-based visual search task, where participants searched for a target among distractions. The results of the study suggested that when irrelevant stimuli were large or bright, attention was biased towards the irrelevant objects, prioritizing them for cognitive processing. This research shows the effects of Bottom-up (stimulus-driven) processing on biased competition theory.",
            "score": 142.68438720703125
        },
        {
            "docid": "23416874_9",
            "document": "Sense . Sight or vision (adjectival form: visual/optical) is the capability of the eye(s) to focus and detect images of visible light on photoreceptors in the retina of each eye that generates electrical nerve impulses for varying colors, hues, and brightness. There are two types of photoreceptors: rods and cones. Rods are very sensitive to light, but do not distinguish colors. Cones distinguish colors, but are less sensitive to dim light. There is some disagreement as to whether this constitutes one, two or three senses. Neuroanatomists generally regard it as two senses, given that different receptors are responsible for the perception of color and brightness. Some argue that stereopsis, the perception of depth using both eyes, also constitutes a sense, but it is generally regarded as a cognitive (that is, post-sensory) function of the visual cortex of the brain where patterns and objects in images are recognized and interpreted based on previously learned information. This is called visual memory.",
            "score": 142.47113037109375
        },
        {
            "docid": "490620_47",
            "document": "Human brain . The cerebrum has a contralateral organisation with each hemisphere of the brain interacting primarily with one half of the body: the left side of the brain interacts with the right side of the body, and vice versa. The developmental cause for this is uncertain. Motor connections from the brain to the spinal cord, and sensory connections from the spinal cord to the brain, both cross sides in the brainstem. Visual input follows a more complex rule: the optic nerves from the two eyes come together at a point called the optic chiasm, and half of the fibres from each nerve split off to join the other. The result is that connections from the left half of the retina, in both eyes, go to the left side of the brain, whereas connections from the right half of the retina go to the right side of the brain. Because each half of the retina receives light coming from the opposite half of the visual field, the functional consequence is that visual input from the left side of the world goes to the right side of the brain, and vice versa. Thus, the right side of the brain receives somatosensory input from the left side of the body, and visual input from the left side of the visual field.",
            "score": 142.19735717773438
        },
        {
            "docid": "10751304_11",
            "document": "Motion-induced blindness . Hsu \"et al.\" (2004) compared MIB to a similar phenomenon of perceptual filling-in (PFI), which likewise reveals a striking dissociation between the percept and the sensory input. They describe both as visual attributes which are perceived in a certain region of the visual field regardless of being in the background (in the same manner as colour, brightness or texture) thus inducing target disappearance. They argue that because in both MIB and PFI the disappearance; or the incorporation of the background motion stimuli; becomes more profound with an increase in eccentricity, decrease in contrast and when perceptual grouping with other stimuli is controlled for; the two illusions are very likely to be a result of intermutual processes. Since MBI and PFI show to be structurally similar, it seems plausible that MIB can be a phenomenon responsible for completing missing information across the blind spot and scotomas where motion is involved.",
            "score": 142.0220947265625
        },
        {
            "docid": "33106906_52",
            "document": "Eyewitness memory . It has been suggested that blind individuals have an enhanced ability to hear and recall auditory information in order to compensate for a lack of vision. However, whilst blind adults' neural systems demonstrate heightened excitability and activity compared to sighted adults, it is still not exactly clear to what extent this compensatory hypothesis is accurate. Nevertheless, many studies have found that there appears to be a high activation of certain visual brain areas in blind individuals when they perform non-visual tasks. This suggests that in blind individuals' brains, a reorganization of what are normally visual areas has occurred in order for them to process non-visual input. This supports a compensatory hypothesis in the blind.",
            "score": 141.8946533203125
        },
        {
            "docid": "1316947_4",
            "document": "Ambiguous image . When we see an image, the first thing we do is attempt to organize all the parts of the scene into different groups. To do this, one of the most basic methods used is finding the edges. Edges can include obvious perceptions such as the edge of a house, and can include other perceptions that the brain needs to process deeper, such as the edges of a person's facial features. When finding edges, the brain's visual system detects a point on the image with a sharp contrast of lighting. Being able to detect the location of the edge of an object aids in recognizing the object. In ambiguous images, detecting edges still seems natural to the person perceiving the image. However, the brain undergoes deeper processing to resolve the ambiguity. For example, consider an image that involves an opposite change in magnitude of luminance between the object and the background (e.g. From the top, the background shifts from black to white, and the object shifts from white to black). The opposing gradients will eventually come to a point where there is an equal degree of luminance of the object and the background. At this point, there is no edge to be perceived. To counter this, the visual system connects the image as a whole rather than a set of edges, allowing one to see an object rather than edges and non-edges. Although there is no complete image to be seen, the brain is able to accomplish this because of its understanding of the physical world and real incidents of ambiguous lighting. In ambiguous images, an illusion is often produced from illusory contours. An illusory contour is a perceived contour without the presence of a physical gradient. In examples where a white shape appears to occlude black objects on a white background, the white shape appears to be brighter than the background, and the edges of this shape produce the illusory contours. These illusory contours are processed by the brain in a similar way as real contours. The visual system accomplishes this by making inferences beyond the information that is presented in much the same way as the luminance gradient.",
            "score": 141.7654571533203
        },
        {
            "docid": "525667_10",
            "document": "Human echolocation . In a 2014 study by Thaler and colleagues, the researchers first made recordings of the clicks and their very faint echoes using tiny microphones placed in the ears of the blind echolocators as they stood outside and tried to identify different objects such as a car, a flag pole, and a tree. The researchers then played the recorded sounds back to the echolocators while their brain activity was being measured using functional magnetic resonance imaging. Remarkably, when the echolocation recordings were played back to the blind experts, not only did they perceive the objects based on the echoes, but they also showed activity in those areas of their brain that normally process visual information in sighted people, primarily primary visual cortex or V1. This result is surprising, as visual areas, as their names suggest, are only active during visual tasks. The brain areas that process auditory information were no more activated by sound recordings of outdoor scenes containing echoes than they were by sound recordings of outdoor scenes with the echoes removed. Importantly, when the same experiment was carried out with sighted people who did not echolocate, these individuals could not perceive the objects and there was no echo-related activity anywhere in the brain. This suggests that the cortex of blind echolocators is plastic and reorganizes such that primary visual cortex, rather than any auditory area, becomes involved in the computation of echolocation tasks.",
            "score": 141.3724365234375
        },
        {
            "docid": "53497_13",
            "document": "Optical illusion . In the Ponzo illusion the converging parallel lines tell the brain that the image higher in the visual field is farther away therefore the brain perceives the image to be larger, although the two images hitting the retina are the same size. The optical illusion seen in a diorama/false perspective also exploits assumptions based on monocular cues of depth perception. The M.C. Escher painting \"Waterfall\" exploits rules of depth and proximity and our understanding of the physical world to create an illusion. Like depth perception, motion perception is responsible for a number of sensory illusions. Film animation is based on the illusion that the brain perceives a series of slightly varied images produced in rapid succession as a moving picture. Likewise, when we are moving, as we would be while riding in a vehicle, stable surrounding objects may appear to move. We may also perceive a large object, like an airplane, to move more slowly than smaller objects, like a car, although the larger object is actually moving faster. The phi phenomenon is yet another example of how the brain perceives motion, which is most often created by blinking lights in close succession.",
            "score": 141.169921875
        },
        {
            "docid": "35347567_7",
            "document": "Antti Revonsuo . According to Revonsuo, the dreaming brain is particularly suitable model system for the study of consciousness because it generates a conscious experience while being isolated from both sensory input and motor output. Regarding the rival paradigm of visual awareness, Revonsuo argues that it does not allow one to distinguish between consciousness and perception. Revonsuo holds that there is a \"'double dissociation' between consciousness and perceptual input\". Accordingly, dreams are conscious experiences, which occur without any perceptual stimuli, and, conversely, perceptual input does not automatically engender conscious experience. In support of the independence of consciousness from perception, Revonsuo cites Stephen LaBerge's case study on a lucid dreamer performing previously agreed upon eye movements to signal to the experimenters that he had become conscious of the fact that he was dreaming. A second study that supports Revonsuo's view of dreams was conducted by Allan Rechtschaffen and Foulkes (1965). In this study, subjects were made to sleep with their eyelids open, thus allowing the visual cortex to receive visual stimuli. Though their eyes were open, and the perceptual input was accessible, the subjects could not see the stimuli and did not report dreaming of it. It is the brain that is having the internal experience, independent of perceptual input. This internalist view of consciousness leads Revonsuo to compare both dreaming and waking consciousness with a virtual reality simulation decoupled from or only indirectly informed by a brain's external environment.",
            "score": 140.95518493652344
        },
        {
            "docid": "21944_40",
            "document": "Nervous system . Feature detection is the ability to extract biologically relevant information from combinations of sensory signals. In the visual system, for example, sensory receptors in the retina of the eye are only individually capable of detecting \"points of light\" in the outside world. Second-level visual neurons receive input from groups of primary receptors, higher-level neurons receive input from groups of second-level neurons, and so on, forming a hierarchy of processing stages. At each stage, important information is extracted from the signal ensemble and unimportant information is discarded. By the end of the process, input signals representing \"points of light\" have been transformed into a neural representation of objects in the surrounding world and their properties. The most sophisticated sensory processing occurs inside the brain, but complex feature extraction also takes place in the spinal cord and in peripheral sensory organs such as the retina.",
            "score": 140.8768768310547
        },
        {
            "docid": "7913402_16",
            "document": "Paul Baltes . Neuronal plasticity, or the capability of the brain to adapt to new requirements, is a prime example of plasticity stressing that the individual\u2019s ability to change is a lifelong process. Recently, researchers have been analyzing how the spared senses compensate for the loss of vision. Without visual input, blind humans have demonstrated that tactile and auditory functions still fully develop. A superiority of the blind has even been observed when they are presented with tactile and auditory tasks. This superiority may suggest that the specific sensory experiences of the blind may influence the development of certain sensory functions, namely tactile and auditory. One experiment was designed by R\u00f6der and colleagues to clarify the auditory localization skills of the blind in comparison to the sighted. They examined both blind human adults\u2019 and sighted human adults\u2019 abilities to locate sounds presented either central or peripheral (lateral) to them. Both congenitally blind adults and sighted adults could locate a sound presented in front of them with precision but the blind were clearly superior in locating sounds presented laterally. Currently, brain-imaging studies have revealed that the sensory cortices in the brain are reorganized after visual deprivation. These findings suggest that when vision is absent in development, the auditory cortices in the brain recruit areas that are normally devoted to vision, thus becoming further refined.",
            "score": 139.66392517089844
        }
    ]
}