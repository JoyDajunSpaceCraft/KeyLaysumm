{
    "q": [
        {
            "docid": "28346382_5",
            "document": "Organelle biogenesis . Several processes are known to have developed for organelle biogenesis. These can range from \"de novo\" synthesis to the copying of a template organelle; the formation of an organelle 'from scratch' and using a preexisting organelle as a template to manufacture an organelle, respectively. The distinct structures of each organelle are thought to be caused by the different mechanisms of the processes which create them and the proteins that they are made up of. Organelles may also be 'split' between two cells during the process of cellular division (known as organelle inheritance), where the organelle of the parent cell doubles in size and then splits with each half being delivered to their respective daughter cells.",
            "score": 182.22030544281006
        },
        {
            "docid": "10072717_28",
            "document": "Categorical distribution . Observe data points one by one and each time consider their predictive probability before observing the data point and updating the posterior. For any given data point, the probability of that point assuming a given category depends on the number of data points already in that category. In this scenario, if a category has a high frequency of occurrence, then new data points are more likely to join that category \u2014 further enriching the same category. This type of scenario is often termed a preferential attachment (or \"rich get richer\") model. This models many real-world processes, and in such cases the choices made by the first few data points have an outsize influence on the rest of the data points.",
            "score": 65.60648155212402
        },
        {
            "docid": "8104647_5",
            "document": "Collaborative model . In the same paper, they proposed the Collaborative Model as an alternative. They believed this model was more able to explain the aforementioned features of conversation. They had conducted an experiment to support this theory and also to further determine how the acceptance process worked.  The experiment consisted of two participants seated at tables separated by an opaque screen. On the tables in front of each participant were a series of Tangram figures arranged in different orders. One participant, called the director, was tasked with getting the other participant, called the matcher, to accurately match his configuration of figures through conversation alone. This process was to be repeated 5 additional times by the same individuals, playing the same roles. The collaborative model they proposed allowed them to make several predictions about what would happen. They predicted that it would require many more words to establish reference the first time, as the participants would need to use non-standard noun phrases which would make it difficult to determine which figures were being talked about. However, they hypothesized that later references to the same figures would take fewer words and a shorter amount of time, because by this point definite reference would have been mutually established, and also because the subjects would be able to rely on established standard noun phrases. The results of the study confirmed many of their beliefs, and outlined some of the processes of collaborative reference, including establishing the types of noun phrases used in presentation, and their frequency.",
            "score": 67.28854024410248
        },
        {
            "docid": "25988629_16",
            "document": "Robust decision-making . RDM analyses often employ a process called \"scenario discovery\" to facilitate the identification of vulnerabilities of proposed strategies. The process begins by specifying some performance metric, such as the total cost of a policy or its deviation from optimality (regret), which can be used to distinguish those cases in the results database where the strategy is judged successful from those where it is judged unsuccessful. Statistical or data-mining algorithms are applied to the database to generate simple descriptions of regions in the space of uncertain input parameters to the model that best describe the cases where the strategy is unsuccessful. That is, the algorithm for describing these cases is tuned to optimize both the predictability and interpretability by decision-makers. The resulting clusters have many characteristics of scenarios and can be used to help decision makers understand the vulnerabilities of the proposed policies and potential response options. A review conducted by the European Environment Agency of the rather sparse literature evaluating how scenarios actually perform in practice when used by organizations to inform decisions identified several key weaknesses of traditional scenario approaches. Scenario-discovery methods are designed to address these weaknesses. In addition, scenario discovery supports analysis for multiple stressors because it characterizes vulnerabilities as combinations of very different types of uncertain parameters (e.g. climate, economic, organizational capabilities, etc.).",
            "score": 64.80304217338562
        },
        {
            "docid": "24574814_10",
            "document": "Models of collaborative tagging . Descriptive models mentioned above were based on analyses of word-word relations as revealed by the various statistical structures in the organization of tags (e.g., how likely one tag would co-occur with other tags or how likely each tag was reused over time). These models are therefore descriptive models at the aggregate level, and have little to offer about predictions at the level of interface interactions and cognitive processes of individual.  Rather than imitating other users at the word level, one possible explanation for this kind of social cohesion could be grounded on the natural tendency for people to process tags at the semantic level, and it was at this level of processing that most imitation occurred. This explanation was supported by research in the area of reading comprehension, which showed that people tended to be influenced by meanings of words, rather than the words themselves during comprehension. Assuming that background knowledge of people in the same culture tend to have shared structures (e.g., using similar vocabularies and their corresponding meanings in order to conform and communicate with each), users of the same social tagging system may also share similar semantic representations of words and concepts, even when the use of tags may vary across individuals at the word level. In other words, we argued that part of the reason for the stability of social tagging systems can be attributed to the shared semantic representations among the users, such that users may have relatively stable and coherent interpretation of information contents and tags as they interact with the system. Based on this assumption, the semantic imitation model predicts how different semantic representations may lead to differences in individual tag choices and eventually different emergent properties at the aggregate behavioral level. The model also predicts that the folksonomies (i.e., knowledge structures) in the system reflect the shared semantic representations of the users.",
            "score": 58.20687425136566
        },
        {
            "docid": "21082137_3",
            "document": "ESCRT . The ESCRT machinery plays a vital role in a number of cellular processes including multivesicular body (MVB) biogenesis, cellular abscission, and viral budding. Multivesicular body (MVB) biogenesis is a process in which ubiquitin tagged proteins enter organelles called endosomes via the formation of vesicles. This process is essential for cells to destroy misfolded and damaged proteins. Without ESCRT machinery, these proteins can build up and lead to neurodegenerative disease. For example, abnormalities in ESCRT-III components can lead to neurological disorders such as hereditary spastic paraplegia (HSP). Cellular abscission, the process by which the membrane connecting two daughter cells is cleaved, is also mediated by ESCRT machinery. Without the ESCRT complexes, daughter cells could not separate and abnormal cells containing twice the amount of DNA would be generated. These cells would inevitably be destroyed through a process known as apoptosis. Lastly, viral budding, or the process by which specific types of viruses exit cells, may not occur in the absence of ESCRT machinery. This would inevitably prevent viruses from spreading from cell to cell.",
            "score": 111.21168255805969
        },
        {
            "docid": "27051151_69",
            "document": "Big data . Much in the same line, it has been pointed out that the decisions based on the analysis of big data are inevitably \"informed by the world as it was in the past, or, at best, as it currently is\". Fed by a large number of data on past experiences, algorithms can predict future development if the future is similar to the past. If the systems dynamics of the future change (if it is not a stationary process), the past can say little about the future. In order to make predictions in changing environments, it would be necessary to have a thorough understanding of the systems dynamic, which requires theory. As a response to this critique Alemany Oliver and Vayre suggested to use \"abductive reasoning as a first step in the research process in order to bring context to consumers\u2019 digital traces and make new theories emerge\". Additionally, it has been suggested to combine big data approaches with computer simulations, such as agent-based models and Complex Systems. Agent-based models are increasingly getting better in predicting the outcome of social complexities of even unknown future scenarios through computer simulations that are based on a collection of mutually interdependent algorithms. Finally, use of multivariate methods that probe for the latent structure of the data, such as factor analysis and cluster analysis, have proven useful as analytic approaches that go well beyond the bi-variate approaches (cross-tabs) typically employed with smaller data sets.",
            "score": 74.24418354034424
        },
        {
            "docid": "4384325_11",
            "document": "Robert Gilbert (chemist) . Thirty years ago there was neither real predictability nor qualitative understanding of the dominant mechanisms in emulsion polymerisation. Mechanisms had been \u2018proved\u2019 by comparing model predictions with experimental data. The data field was limited and the models had many adjustable parameters, or else fitting parameters had values that were subject to wide uncertainty: it was possible to choose values that could suit any model. It was not uncommon to find two papers claiming that quite different mechanisms were dominant in the same system, a result of not being able to isolate the individual steps. As a result of Gilbert\u2019s work, all individual processes in emulsion polymerisation, one of the commonest ways of making everyday products, are now qualitatively and quantitatively understood. It is now possible to polymerise simple systems and to predict the molecular architecture that will be formed under chosen conditions, while for more complex conditions, trends can be semiquantitatively predicted and understood. The international scientific and technical community in this field now uses the mechanistic knowledge that he obtained as the key to understanding current processes and creating new processes and products. His work has put this industrially important field on a rigorous scientific footing.",
            "score": 72.53705978393555
        },
        {
            "docid": "162453_10",
            "document": "George Ritzer . Ritzer's idea of McDonaldization is an extension of Max Weber's (1864\u20131920) classical theory of the rationalization of modern society and culture. Weber famously used the terminology \"iron cage\" to describe the stultifying, Kafkaesque effects of bureaucratized life, and Ritzer applied this idea to an influential social system in the twenty-first century: McDonald's. Ritzer argues that McDonald's restaurants have become the better example of current forms of instrumental rationality and its ultimately irrational and harmful consequences on people Ritzer identifies four rationalizing dimensions of McDonald's that contribute to the process of McDonaldization, claiming that McDonald's aims to increase: 1. Efficiency: McDonald's delivers products quickly and easily without inputting an excessive amount of money. The \"McDonald's model\" and therefore the McDonald's operations follow a predesigned process that leads to a specified end, using productive means. The efficiency of the McDonald's model has infiltrated other modern day services such as completing tax forms online, easy weight loss programs, The Walt Disney Company FASTPASSes, and online dating services, eHarmony and match.com. 2. Calculability: America has grown to connect the quantity of a product with the quality of a product and that \"bigger is better\". The \"McDonald's model\" is influential in this conception due to providing a lot of food for not that much money. While the end products feed into the connection between the quantity and quality of the product, so does the McDonald's production process. Throughout the food production, everything is standardized and highly calculated: the size of the beef patty, the amount of french fries per order, and the time spent in a franchise. The high calculability of the McDonald's franchise also extends over into academics. It is thought that the academic experience, in high school and higher education, can be quantified into one number, the GPA. Also, calculability leads to the idea that the longer the resume or list of degrees, the better the candidate, during an application process. In addition to academics being affected by the McDonaldization in society, sports, specifically basketball have also been affected. It used to be that basketball was a more laid-back, slow-paced sort of game, yet through the creation of fast-food and McDonald's, a shot clock was added to increase not only the speed of the game, but also the number of points scored.  3. Predictability: Related to calculability, customers know what to expect from a given producer of goods or services. For example, customers know that every Big Mac from McDonald's is going to be the same as the next one; there is an understood predictability to the menu as well as the overall experience. In order to maintain the predictability for each franchise, there has to be \"discipline, order, systematization, formalization, routine, consistency, and a methodical operation\". The predictability of the McDonald's franchise also appears through the golden arches in front of every franchise as well as the scripts that the employees use on the customers. The Walt Disney Company also has regulations in place, like dress code for men and women, in order to add to the predictability of each amusement park or Disney operation. Predictability has also extended into movie sequels and TV shows. With each movie sequel, like \"Spy Kids 4\", or TV show, \"Law & Order\" and its spinoffs, the plot is predictable and usually follow a preconceived model.  4. Control: McDonald's restaurants pioneered the idea of highly specialized tasks for all employees to ensure that all human workers are operating at exactly the same level. This is a way to keep a complicated system running smoothly; rules and regulations that make efficiency, calculability, and predictability possible. Oftentimes, the use of non-human technology, such as computers, is used. The McDonald's food is already \"pre-prepared\", the potatoes are already cut and processed, just needing to be fried and heated, and the food preparation process is monitored and tracked. The computers tell the managers how many hamburgers are needed at the lunchtime rush and other peak times and the size and shape of the pickles as well as how many go on a hamburger is managed and control. The control aspect of McDonaldization has extended to other businesses, Sylvan Learning and phone operating systems, and even birth and death. Every step of the learning process at Sylvan, the U-shaped tables and instruction manuals, is controlled as well as each step of the birthing process, in modern-day hospitals, and the process of dying.",
            "score": 66.20859599113464
        },
        {
            "docid": "12142270_3",
            "document": "GENESIS (software) . GENESIS works by creating simulation environments for constructing models of neurons or neural systems. \"Nerve cells are capable of communicating with each other in such a highly structured manner as to form neuronal networks. To understand neural networks, it is necessary to understand the ways in which one neuron communicates with another through synaptic connections and the process called synaptic transmission\". Neurons have a specialized structure for their function, they \"are different from most other cells in the body in that they are polarized and have distinct morphological regions, each with specific functions\". The two important regions of a neuron are the dendrite and the axon. \"Dendrites are the region where one neuron receives connections from other neurons. The cell body or soma contains the nucleus and the other organelles necessary for cellular function. The axon is a key component of nerve cells over which information is transmitted from one part of the neuron (e.g., the cell body) to the terminal regions of the neuron\". The third important piece of a neuron is the synapse. \"The synapse is the terminal region of the axon this is where one neuron forms a connection with another and conveys information through the process of synaptic transmission\".",
            "score": 106.36279320716858
        },
        {
            "docid": "37822732_10",
            "document": "History of network traffic models . Early traffic models were derived from telecommunications models and focused on simplicity of analysis. They generally operated under the assumption that aggregating traffic from a large number of sources tended to smooth out bursts; that burstiness decreased as the number of traffic sources increased. One of the most widely used and oldest traffic models is the Poisson Model. The memoryless Poisson distribution is the predominant model used for analyzing traffic in traditional telephony networks. The Poisson process is characterized as a renewal process. In a Poisson process the inter-arrival times are exponentially distributed with a rate parameter \u03bb: P{An \u2264 t} = 1 \u2013 exp(-\u03bbt). The Poisson distribution is appropriate if the arrivals are from a large number of independent sources, referred to as Poisson sources. The distribution has a mean and variance equal to the parameter \u03bb. The Poisson distribution can be visualized as a limiting form of the binomial distribution, and is also used widely in queuing models. There are a number of interesting mathematical properties exhibited by Poisson processes. Primarily, superposition of independent Poisson processes results in a new Poisson process whose rate is the sum of the rates of the independent Poisson processes. Further, the independent increment property renders a Poisson process memoryless. Poisson processes are common in traffic applications scenarios that consist of a large number of independent traffic streams. The reason behind the usage stems from Palm's Theorem which states that under suitable conditions, such large number of independent multiplexed streams approach a Poisson process as the number of processes grows, but the individual rates decrease in order to keep the aggregate rate constant. Nevertheless, it is to be noted that traffic aggregation need not always result in a Poisson process. The two primary assumptions that the Poisson model makes are: 1. The number of sources is infinite 2. The traffic arrival pattern is random. In the compound Poisson model, the base Poisson model is extended to deliver batches of packets at once. The inter-batch arrival times are exponentially distributed, while the batch size is geometric Mathematically, this model has two parameters, \u03bb, the arrival rate, and \u03c1 in (0,1), the batch parameter. Thus, the mean number of packets in a batch is 1/ \u03c1, while the mean inter-batch arrival time is 1/ \u03bb. Mean packet arrivals over time period t are t\u03bb/ \u03c1. The compound Poisson model shares some of the analytical benefits of the pure Poisson model: the model is still memoryless, aggregation of streams is still (compound) Poisson, and the steady-state equation is still reasonably simple to calculate, although varying batch parameters for differing flows would complicate the derivation. Markov models attempt to model the activities of a traffic source on a network, by a finite number of states. The accuracy of the model increases linearly with the number of states used in the model. However, the complexity of the model also increases proportionally with increasing number of states. An important aspect of the Markov model - the Markov Property, states that the next (future) state depends only on the current state. In other words, the probability of the next state, denoted by some random variable Xn+1, depends only on the current state, indicated by Xn, and not on any other state Xi, where i<n. The set of random variables referring to different states {Xn} is referred to as a Discrete Markov Chain. Another attempt at providing a bursty traffic model is found in Jain and Routhier\u2019s Packet Trains model. This model was principally designed to recognize that address locality applies to routing decisions; that is, packets that arrive near each other in time are frequently going to the same destination. In generating a traffic model that allows for easier analysis of locality, the authors created the notion of packet trains, a sequence of packets from the same source, traveling to the same destination (with replies in the opposite direction). Packet trains are optionally sub-divided into tandem trailers. Traffic between a source and a destination usually consists of a series of messages back and forth. Thus, a series of packets go one direction, followed by one or more reply packets, followed by a new series in the initial direction. Traffic quantity is then a superposition of packet trains, which generates substantial bursty behavior. This refines the general conception of the compound Poisson model, which recognized that packets arrived in groups, by analyzing why they arrive in groups, and better characterizing the attributes of the group. Finally, the authors demonstrate that packet arrival times are not Poisson distributed, which led to a model that departs from variations on the Poisson theme. The packet train model is characterized by the following parameters and their associated probability distributions: The train model is designed for analyzing and categorizing real traffic, not for generating synthetic loads for simulation. Thus, little claim has been made about the feasibility of packet trains for generating synthetic traffic. Given accurate parameters and distributions, generation should be straightforward, but derivation of these parameters is not addressed.",
            "score": 77.83843803405762
        },
        {
            "docid": "375416_33",
            "document": "Computer simulation . The final step is to validate the model by comparing the results with what is expected based on historical data from the study area. Ideally, the model should produce similar results to what has happened historically. This is typically verified by nothing more than quoting the R-squared statistic from the fit. This statistic measures the fraction of variability that is accounted for by the model. A high R-squared value does not necessarily mean the model fits the data well. Another tool used to validate models is graphical residual analysis. If model output values drastically differ from historical values, it probably means there is an error in the model. Before using the model as a base to produce additional models, it is important to verify it for different scenarios to ensure that each one is accurate. If the outputs do not reasonably match historic values during the validation process, the model should be reviewed and updated to produce results more in line with expectations. It is an iterative process that helps to produce more realistic models.",
            "score": 56.056774377822876
        },
        {
            "docid": "25896864_20",
            "document": "Ishwar Puri . \"Mathematical model for the cancer stem cell hypothesis\" discusses how cancers can occur because of mutations in normal stem cells and presents a predictive mathematical model. It is the first work to show mathematically how repeated insult to mature cells increases the risk of cancer.",
            "score": 50.191694259643555
        },
        {
            "docid": "237704_38",
            "document": "Saccharomyces cerevisiae . The availability of the \"S.\u00a0cerevisiae\" genome sequence and a set of deletion mutants covering 90% of the yeast genome has further enhanced the power of \"S.\u00a0cerevisiae\" as a model for understanding the regulation of eukaryotic cells. A project underway to analyze the genetic interactions of all double-deletion mutants through synthetic genetic array analysis will take this research one step further. The goal is to form a functional map of the cell's processes. As of 2010 a model of genetic interactions is most comprehensive yet to be constructed, containing \"the interaction profiles for ~75% of all genes in the Budding yeast\". This model was made from 5.4 million two-gene comparisons in which a double gene knockout for each combination of the genes studied was performed. The effect of the double knockout on the fitness of the cell was compared to the expected fitness. Expected fitness is determined from the sum of the results on fitness of single-gene knockouts for each compared gene. When there is a change in fitness from what is expected, the genes are presumed to interact with each other. This was tested by comparing the results to what was previously known. For example, the genes Par32, Ecm30, and Ubp15 had similar interaction profiles to genes involved in the Gap1-sorting module cellular process. Consistent with the results, these genes, when knocked out, disrupted that process, confirming that they are part of it. From this, 170,000 gene interactions were found and genes with similar interaction patterns were grouped together. Genes with similar genetic interaction profiles tend to be part of the same pathway or biological process. This information was used to construct a global network of gene interactions organized by function. This network can be used to predict the function of uncharacterized genes based on the functions of genes they are grouped with.",
            "score": 99.4792548418045
        },
        {
            "docid": "24044102_6",
            "document": "Cellular model . The eukaryotic cell cycle is very complex and is one of the most studied topics, since its misregulation leads to cancers. It is possibly a good example of a mathematical model as it deals with simple calculus but gives valid results. Two research groups have produced several models of the cell cycle simulating several organisms. They have recently produced a generic eukaryotic cell cycle model which can represent a particular eukaryote depending on the values of the parameters, demonstrating that the idiosyncrasies of the individual cell cycles are due to different protein concentrations and affinities, while the underlying mechanisms are conserved (Csikasz-Nagy et al., 2006). By means of a system of ordinary differential equations these models show the change in time (dynamical system) of the protein inside a single typical cell; this type of model is called a deterministic process (whereas a model describing a statistical distribution of protein concentrations in a population of cells is called a stochastic process). To obtain these equations an iterative series of steps must be done: first the several models and observations are combined to form a consensus diagram and the appropriate kinetic laws are chosen to write the differential equations, such as rate kinetics for stoichiometric reactions, Michaelis-Menten kinetics for enzyme substrate reactions and Goldbeter\u2013Koshland kinetics for ultrasensitive transcription factors, afterwards the parameters of the equations (rate constants, enzyme efficiency coefficients and Michaelis constants) must be fitted to match observations; when they cannot be fitted the kinetic equation is revised and when that is not possible the wiring diagram is modified. The parameters are fitted and validated using observations of both wild type and mutants, such as protein half-life and cell size. In order to fit the parameters the differential equations need to be studied. This can be done either by simulation or by analysis.  In a simulation, given a starting vector (list of the values of the variables), the progression of the system is calculated by solving the equations at each time-frame in small increments. In analysis, the properties of the equations are used to investigate the behavior of the system depending of the values of the parameters and variables. A system of differential equations can be represented as a vector field, where each vector described the change (in concentration of two or more protein) determining where and how fast the trajectory (simulation) is heading. Vector fields can have several special points: a stable point, called a sink, that attracts in all directions (forcing the concentrations to be at a certain value), an unstable point, either a source or a saddle point which repels (forcing the concentrations to change away from a certain value), and a limit cycle, a closed trajectory towards which several trajectories spiral towards (making the concentrations oscillate). A better representation which can handle the large number of variables and parameters is called a bifurcation diagram (bifurcation theory): the presence of these special steady-state points at certain values of a parameter (e.g. mass) is represented by a point and once the parameter passes a certain value, a qualitative change occurs, called a bifurcation, in which the nature of the space changes, with profound consequences for the protein concentrations: the cell cycle has phases (partially corresponding to G1 and G2) in which mass, via a stable point, controls cyclin levels, and phases (S and M phases) in which the concentrations change independently, but once the phase has changed at a bifurcation event (cell cycle checkpoint), the system cannot go back to the previous levels since at the current mass the vector field is profoundly different and the mass cannot be reversed back through the bifurcation event, making a checkpoint irreversible. In particular the S and M checkpoints are regulated by means of special bifurcations called a Hopf bifurcation and an infinite period bifurcation. Cell Collective is a modeling software that enables one to house dynamical biological data, build computational models, stimulate, break and recreate models. The development is led by Tomas Helikar, a researcher within the field of computational biology. It is designed for biologists, students learning about computational biology, teachers focused on teaching life sciences, and researchers within the field of life science. The complexities of math and computer science are built into the backend and one can learn about the methods used for modeling biological species, but complex math equations, algorithms, programming are not required and hence won't impede model building.",
            "score": 84.98709559440613
        },
        {
            "docid": "377202_2",
            "document": "Branching process . In probability theory, a branching process is a type of mathematical object known as a stochastic process, which consists of collections of random variables. The random variables of a stochastic process are indexed by the natural numbers. The original purpose of branching processes was to serve as a mathematical model of a population in which each individual in generation \u00a0\"n\" produces some random number of individuals in generation\u00a0\"n\"\u00a0+\u00a01, according, in the simplest case, to a fixed probability distribution that does not vary from individual to individual. Branching processes are used to model reproduction; for example, the individuals might correspond to bacteria, each of which generates\u00a00,\u00a01,\u00a0or\u00a02 offspring with some probability in a single time unit. Branching processes can also be used to model other systems with similar dynamics, e.g., the spread of surnames in genealogy or the propagation of neutrons in a nuclear reactor.",
            "score": 61.54483652114868
        },
        {
            "docid": "1169523_2",
            "document": "Hybridoma technology . Hybridoma technology is a method for producing large numbers of identical antibodies (also called monoclonal antibodies). This process starts by injecting a mouse (or other mammal) with an antigen that provokes an immune response. A type of white blood cell, the B cell that produces antibodies that bind to the antigen are then harvested from the mouse. These isolated B cells are in turn fused with immortal B cell cancer cells, a myeloma, to produce a hybrid cell line called a hybridoma, which has both the antibody-producing ability of the B-cell and the exaggerated longevity and reproductivity of the myeloma. The hybridomas can be grown in culture, each culture starting with one viable hybridoma cell, producing cultures each of which consists of genetically identical hybridomas which produce one antibody per culture (monoclonal) rather than mixtures of different antibodies (polyclonal). The myeloma cell line that is used in this process is selected for its ability to grow in tissue culture and for an absence of antibody synthesis. In contrast to polyclonal antibodies, which are mixtures of many different antibody molecules, the monoclonal antibodies produced by each hybridoma line are all chemically identical.",
            "score": 76.61508750915527
        },
        {
            "docid": "318669_27",
            "document": "Schizosaccharomyces pombe . Cytokinesis is one of the components of cell division that is often observed in fission yeast. Well-conserved components of cytokinesis are observed in fission yeast and allow us to look at various genomic scenarios and pinpoint mutations. Cytokinesis is a permanent step and very crucial to the wellbeing of the cell. Contractile ring formation in particular is heavily studied by researchers using \"S. pombe\" as a model system. The contractile ring is highly conserved in both fission yeast and human cytokinesis. Mutations in cytokinesis can result in many malfunctions of the cell including cell death and development of cancerous cells. This is a complex process in human cell division, but in \"S. pombe\" simpler experiments can yield results that can then be applied for research in higher-order model systems such as humans.",
            "score": 94.69803047180176
        },
        {
            "docid": "50227596_8",
            "document": "Spike-and-slab variable selection . A possible drawback of the Spike-and-Slab model can be its mathematical complexity (in comparison to linear regression). A deep understanding of this model requires sound knowledge in stochastic processes. On the other hand, some modern statistical software (e.g. R) have ready-to-use solutions for calculating various Bayesian variable selection models. In this case, it would be enough for a researcher to know the idea of the method, required model parameters and input variables. The analysis of the model outcomes (distribution of \"\u03b3\", \"\u03b2\", and corresponding predictions of \"y\") can be more challenging in comparison to linear regression case. The spike-and-slab model produces inclusion probabilities for each of possible predictors. This can cause difficulties when comparing results to the studies with simple regression (usually only regression coefficients with corresponding statistics are available).",
            "score": 61.88980269432068
        },
        {
            "docid": "1365349_5",
            "document": "MM5 (weather model) . MM5 is written in FORTRAN. These FORTRAN programs must be compiled on a local computer, and some need to be recompiled each time a model configuration is changed. The program uses pointers to assign variables to values. These pointers go into parts of the memory to assign specific values to the desired variables. MM5 can also perform multiple tasks at once. Specifically, two different tasks can be executed on different processors simultaneously and MM5 uses this as much as possible. This multi-tasking also uses nesting, and MM5 allows up to nine domains (processes) running at the same time and they interact throughout the whole process . The model utilizes two-way nesting, which occurs when the input from one nest\u2019s coarse mesh, which is a high density of cells in one area, comes from one of the four boundaries, but the feedback to the coarser mesh happens through the nest interior. Each domain gathers information from its parent domain each timestep, then it runs three timesteps, and then delivers the information back to its parent domain. There are three different ways to do two-way nesting: Nest interpolation, nest analysis input, and nest terrain input. Interpolation occurs when the terrain is smooth, such as water. There is no input that is required for this type of two-way nesting. Nest input requires a file called MMINPUT, and that file contains the meteorological and the terrain information so that initially, a better analysis can be done. Finally terrain input requires a TERRAIN file. Then the meteorological fields are interpolated. When multi-tasking occurs, the variables must be marked as either shared or private. Shared implies that the processors all have access to the same part of memory, while private implies that each processor must have its own private copy of an array with its personal memory location. The multi-tasking occurs specifically in the subroutines Solve1, Solve3, and Sound.",
            "score": 64.23266243934631
        },
        {
            "docid": "21914777_3",
            "document": "Galactic orientation . There are mainly three scenarios for the origin of galaxy clusters and superclusters. These models are based on different assumptions of the primordial conditions, so they predict different spin vector alignments of the galaxies. The three hypotheses are the \"pancake model\", the \"hierarchy model\", and the \"primordial vorticity theory\". The three are mutually exclusive as they produce contradictory predictions. However, the predictions made by all three theories are based on the precepts of cosmology. Thus, these models can be tested using a database with appropriate methods of analysis.",
            "score": 55.38880181312561
        },
        {
            "docid": "35881441_3",
            "document": "HH-suite . Proteins are central players in all of life's processes. To understand how life in cells is organised, we have to understand what each of the proteins involved in these molecular processes does. This is particularly important in order to understand the origin of diseases. But for a large fraction of the approximately 20 000 human proteins the structures and functions remain unknown. Many proteins have been investigated in model organisms such as many bacteria, baker's yeast, fruit flies, zebra fish or mice, for which experiments can be often done more easily than with human cells. To predict the function, structure, or other properties of a protein for which only its sequence of amino acids is known, the protein sequence is compared to the sequences of other proteins in public databases. If a protein with sufficiently similar sequence is found, the two proteins are likely to be evolutionarily related (\"homologous\"). In that case, they are likely to share similar structures and functions. Therefore, if a protein with a sufficiently similar sequence and with known functions and/or structure can be found by the sequence search, the unknown protein's functions, structure, and domain composition can be predicted. Such predictions greatly facilitate the determination of the function or structure by targeted validation experiments.",
            "score": 67.43037509918213
        },
        {
            "docid": "63185_25",
            "document": "Norbert Wiener . For signal processing, the Wiener filter is a filter proposed by Wiener during the 1940s and published in 1942 as a classified document. Its purpose is to reduce the amount of noise present in a signal by comparison with an estimate of the desired noiseless signal. Wiener developed the filter at the Radiation Laboratory at MIT to predict the position of German bombers from radar reflections. It is necessary to predict the future, because by the time the shell reaches the vicinity of the target, the target has moved, and maybe has changed direction slightly. They even modeled the muscle response of the pilot, which led eventually to cybernetics. The unmanned V1's were particularly easy to model, and on a good day, American guns fitted with Wiener filters would shoot down 99 out of 100 V1's as they entered Britain from the English channel, on their way to London. What emerged was a mathematical theory of great generality---a theory for predicting the future as best one can on the basis of incomplete information about the past. It was a statistical theory that included applications that did not, strictly speaking, predict the future, but only tried to remove noise. It made use of Wiener's earlier work on integral equations and Fourier transforms. Wiener took a great interest in the mathematical theory of Brownian motion (named after Robert Brown) proving many results now widely known such as the non-differentiability of the paths. Consequently, the one-dimensional version of Brownian motion was named the Wiener process. It is the best known of the L\u00e9vy processes, c\u00e0dl\u00e0g stochastic processes with stationary statistically independent increments, and occurs frequently in pure and applied mathematics, physics and economics (e.g. on the stock-market).",
            "score": 54.891056537628174
        },
        {
            "docid": "3292213_3",
            "document": "Hazard analysis . A hazard analysis is used as the first step in a process used to assess risk. The result of a hazard analysis is the identification of different type of hazards. A hazard is a potential condition and exists or not (probability is 1 or 0). It may in single existence or in combination with other hazards (sometimes called events) and conditions become an actual Functional Failure or Accident (Mishap). The way this exactly happens in one particular sequence is called a scenario. This scenario has a probability (between 1 and 0) of occurrence. Often a system has many potential failure scenarios. It also is assigned a classification, based on the worst case severity of the end condition. Risk is the combination of probability and severity. Preliminary risk levels can be provided in the hazard analysis. The validation, more precise prediction (verification) and acceptance of risk is determined in the Risk assessment (analysis). The main goal of both is to provide the best selection of means of controlling or eliminating the risk. The term is used in several engineering specialties, including avionics, chemical process safety, safety engineering, reliability engineering and food safety.",
            "score": 66.18680250644684
        },
        {
            "docid": "123418_14",
            "document": "Transdifferentiation . Determining the unique set of cellular factors that is needed to be manipulated for each cell conversion is a long and costly process that involved much trial and error. As a result, this first step of identifying the key set of cellular factors for cell conversion is the major obstacle researchers face in the field of cell reprogramming. An international team of researchers have developed an algorithm, called Mogrify(1), that can predict the optimal set of cellular factors required to convert one human cell type to another. When tested, Mogrify was able to accurately predict the set of cellular factors required for previously published cell conversions correctly. To further validate Mogrify's predictive ability, the team conducted two novel cell conversions in the laboratory using human cells, and these were successful in both attempts solely using the predictions of Mogrify. Mogrify has been made available online for other researchers and scientists.",
            "score": 83.96259152889252
        },
        {
            "docid": "5824073_12",
            "document": "High-content screening . This technology allows a (very) large number of experiments to be performed, allowing explorative screening. Cell-based systems are mainly used in chemical genetics where large, diverse small molecule collections are systematically tested for their effect on cellular model systems. Novel drugs can be found using screens of tens of thousands of molecules, and these have promise for the future of drug development.  Beyond drug discovery, chemical genetics is aimed at functionalizing the genome by identifying small molecules that acts on most of the 21,000 gene products in a cell. High-content technology will be part of this effort which could provide useful tools for learning where and when proteins act by knocking them out chemically. This would be most useful for gene where knock out mice (missing one or several genes) can not be made because the protein is required for development, growth or otherwise lethal when it is not there. Chemical knock out could address how and where these genes work. Further the technology is used in combination with RNAi to identify sets of genes involved in specific mechanisms, for example cell division. Here, libraries of RNAis, covering a whole set of predicted genes inside the target organism's genome can be used to identify relevant subsets, facilitating the annotation of genes for which no clear role has been established beforehand. The large datasets produced by automated cell biology contain spatially resolved, quantitative data which can be used for building for systems level models and simulations of how cells and organisms function. Systems biology models of cell function would permit prediction of why, where and how the cell responds to external changes, growth and disease.",
            "score": 82.04416763782501
        },
        {
            "docid": "7214278_2",
            "document": "Decision field theory . Decision field theory (DFT) is a dynamic-cognitive approach to human decision making. It is a cognitive model that describes how people actually make decisions rather than a rational or normative theory that prescribes what people should or ought to do. It is also a dynamic model of decision making rather than a static model, because it describes how a person's preferences evolve across time until a decision is reached rather than assuming a fixed state of preference. The preference evolution process is mathematically represented as a stochastic process called a diffusion process. It is used to predict how humans make decisions under uncertainty, how decisions change under time pressure, and how choice context changes preferences. This model can be used to predict not only the choices that are made but also decision or response times.",
            "score": 68.00508856773376
        },
        {
            "docid": "22594732_15",
            "document": "Process simulation . Dynamic simulation can be used in both an online and offline fashion. The online case being model predictive control, where the real-time simulation results are used to predict the changes that would occur for a control input change, and the control parameters are optimised based on the results. Offline process simulation can be used in the design, troubleshooting and optimisation of process plant as well as the conduction of case studies to assess the impacts of process modifications. Dynamic simulation is also used for operator training.",
            "score": 55.20522379875183
        },
        {
            "docid": "996341_4",
            "document": "Spindle checkpoint . When cells are ready to divide, because cell size is big enough or because they receive the appropriate stimulus, they activate the mechanism to enter into the cell cycle, and they duplicate most organelles during S (synthesis) phase, including their centrosome. Therefore, when the cell division process will end, each daughter cell will receive a complete set of organelles. At the same time, during S phase all cells must duplicate their DNA very precisely, a process termed DNA replication. Once DNA replication has finished, in eukaryotes the DNA molecule is compacted and condensed, to form the mitotic chromosomes, each one constituted by two sister chromatids, which stay held together by the establishment of cohesion between them; each chromatid is a complete DNA molecule, attached via microtubules to one of the two centrosomes of the dividing cell, located at opposed poles of the cell. The structure formed by the centrosomes and the microtubules is named mitotic spindle, due to its characteristic shape, holding the chromosomes between the two centrosomes. Both sister chromatids stay together until anaphase; at this moment they separate from each other and they travel towards the centrosome to which they are attached. In this way, when the two daughter cells separate at the end of the division process, each one will receive a complete set of chromatids. The mechanism responsible for the correct distribution of sister chromatids during cell division is named chromosome segregation.",
            "score": 132.09122109413147
        },
        {
            "docid": "53559567_22",
            "document": "Flash-gas (petroleum) . More realistic modelling techniques encompass variable conditions that can occur on-site during petroleum processing by using sample analysis software at the flash site, e.g. ProMax Software, which is able to predict emissions and losses due to flash, i.e. working and standing losses. Other techniques that are used to make calculations of flash losses (without sampling and analysis) are referred to as chemical process simulators, e.g. WinSim, Designer II, HYSIM, and VMG. These programs can also have the ability to incorporate data from site-specific samples to give more accurate results. Alternatively, a captured liquid or gas sample can be analyzed in a laboratory setting to determine the composition and dissolved gas-oil-ratio (GOR) using precise measurement techniques. However, this only provides insight on the sample of flash gas and does not account for real time fluctuations of all on-site sources of flash gas, including working and standing losses.",
            "score": 48.42488098144531
        },
        {
            "docid": "1726672_12",
            "document": "Neural circuit . Connectionist models serve as a test platform for different hypotheses of representation, information processing, and signal transmission. Lesioning studies in such models, e.g. artificial neural networks, where parts of the nodes are deliberately destroyed to see how the network performs, can also yield important insights in the working of several cell assemblies. Similarly, simulations of dysfunctional neurotransmitters in neurological conditions (e.g., dopamine in the basal ganglia of Parkinson's patients) can yield insights into the underlying mechanisms for patterns of cognitive deficits observed in the particular patient group. Predictions from these models can be tested in patients or via pharmacological manipulations, and these studies can in turn be used to inform the models, making the process iterative.",
            "score": 65.10639715194702
        },
        {
            "docid": "44415727_72",
            "document": "Development communication policy science . Scenario construction is an assessment of what could happen in the future based on input assumptions. It is not a prediction of what will happen in the future, and is therefore an ideal tool to use to create thought provoking content in markets and sectors that have many variable influences. A scenario report can be deterministic or stochastic, historical or hypothetical. Analysis based on deterministic scenarios typically considers just a few scenarios which might be historical or hypothetical. Although, scenarios tend to be mostly quantitative, they can be used to develop alternative views of the future that are meant to offer strategic thinking about how a company might best respond to rapid changes in the business environment. These scenarios, developed through an internal and external review process, are qualitative and descriptive.",
            "score": 42.57723927497864
        }
    ],
    "r": [
        {
            "docid": "28346382_5",
            "document": "Organelle biogenesis . Several processes are known to have developed for organelle biogenesis. These can range from \"de novo\" synthesis to the copying of a template organelle; the formation of an organelle 'from scratch' and using a preexisting organelle as a template to manufacture an organelle, respectively. The distinct structures of each organelle are thought to be caused by the different mechanisms of the processes which create them and the proteins that they are made up of. Organelles may also be 'split' between two cells during the process of cellular division (known as organelle inheritance), where the organelle of the parent cell doubles in size and then splits with each half being delivered to their respective daughter cells.",
            "score": 182.22030639648438
        },
        {
            "docid": "28346382_2",
            "document": "Organelle biogenesis . Organelle biogenesis is the biogenesis, or creation, of cellular organelles in cells. Organelle biogenesis includes the process by which cellular organelles are split between daughter cells during mitosis; this process is called organelle inheritance.",
            "score": 155.97784423828125
        },
        {
            "docid": "318669_29",
            "document": "Schizosaccharomyces pombe . Researchers using fission yeast as a model system also look at organelle dynamics and responses and the possible correlations between yeast cells and mammalian cells. Mitochondria diseases, and various organelle systems such as the Golgi apparatus and endoplasmic reticulum, can be further understood, by observing fission yeast\u2019s chromosome dynamics and protein expression levels and regulation.",
            "score": 155.21102905273438
        },
        {
            "docid": "20377_25",
            "document": "Microorganism . Most living things that are visible to the naked eye in their adult form are eukaryotes, including humans. However, a large number of eukaryotes are also microorganisms. Unlike bacteria and archaea, eukaryotes contain organelles such as the cell nucleus, the Golgi apparatus and mitochondria in their cells. The nucleus is an organelle that houses the DNA that makes up a cell's genome. DNA (Deoxyribonucleic acid) itself is arranged in complex chromosomes. Mitochondria are organelles vital in metabolism as they are the site of the citric acid cycle and oxidative phosphorylation. They evolved from symbiotic bacteria and retain a remnant genome. Like bacteria, plant cells have cell walls, and contain organelles such as chloroplasts in addition to the organelles in other eukaryotes. Chloroplasts produce energy from light by photosynthesis, and were also originally symbiotic bacteria.",
            "score": 147.48876953125
        },
        {
            "docid": "1989166_26",
            "document": "Phagosome . Autophagosomes are different from phagosomes in that they are mainly used to selectively degrade damaged cytosolic organelles such as mitochondria (mitophagy). However, when the cell is starved or stressed, autophagosomes can also non-selectively degrade organelles to provide the cell with amino acids and other nutrients. Autophagy is not limited to professional phagocytes, it is first discovered in rat hepatocytes by cell biologist Christian de Duve. Autophagosomes have a double membrane, the inner one from the engulfed organelle, and the outer membrane is speculated to be formed from the endoplasmic reticulum or the ER-Golgi Intermediate Compartment (ERGIC). The autophagosome also fuses with lysosomes to degrade its contents. When \"M. tuberculosis\" inhibit phagosome acidification, Interferon gamma can induce autophagy and rescue the maturation process.",
            "score": 141.03773498535156
        },
        {
            "docid": "22393_10",
            "document": "Organelle . Under the more restricted definition of membrane-bound structures, some parts of the cell do not qualify as organelles. Nevertheless, the use of organelle to refer to non-membrane bound structures such as ribosomes is common. This has led some texts to delineate between membrane-bound and non-membrane bound organelles. The non-membrane bound organelles, also called large biomolecular complexes, are large assemblies of macromolecules that carry out particular and specialized functions, but they lack membrane boundaries. Such cell structures include:",
            "score": 140.9430694580078
        },
        {
            "docid": "9927_33",
            "document": "Endomembrane system . Lysosomes carry out intracellular digestion, in a process called phagocytosis (from the Greek phagein, to eat and kytos, vessel, referring here to the cell), by fusing with a vacuole and releasing their enzymes into the vacuole. Through this process, sugars, amino acids, and other monomers pass into the cytosol and become nutrients for the cell. Lysosomes also use their hydrolytic enzymes to recycle the cell's obsolete organelles in a process called autophagy. The lysosome engulfs another organelle and uses its enzymes to take apart the ingested material. The resulting organic monomers are then returned to the cytosol for reuse. The last function of a lysosome is to digest the cell itself through autolysis.",
            "score": 140.02578735351562
        },
        {
            "docid": "512768_8",
            "document": "Differential centrifugation . With the rotator turned on, the tissue sample is ground by the porcelain pores and the container wall into tiny fragments. This grinding process will break the cell membranes of the sample's cells, leaving individual organelles suspended in the solution. This process is called cell lysis. A portion of cells will remain intact after grinding and some organelles will be damaged, and these will be catered for in the later stages of centrifugation.",
            "score": 139.9764862060547
        },
        {
            "docid": "22393_3",
            "document": "Organelle . The name \"organelle\" comes from the idea that these structures are parts of cells, as organs are to the body, hence \"organelle,\" the suffix \"-elle\" being a diminutive. Organelles are identified by microscopy, and can also be purified by cell fractionation. There are many types of organelles, particularly in eukaryotic cells. While prokaryotes do not possess organelles \"per se\", some do contain protein-based bacterial microcompartments, which are thought to act as primitive organelles.",
            "score": 138.98013305664062
        },
        {
            "docid": "624361_5",
            "document": "Autophagy . Autophagy was first observed by Keith R. Porter and his student Thomas Ashford at the Rockefeller Institute. In January 1962 they reported an increased number of lysosomes in rat liver cells after the addition of glucagon, and that some displaced lysosomes towards the centre of the cell contained other cell organelles such as mitochondria. They called this autolysis after Christian de Duve and Alex B. Novikoff. However Porter and Ashford wrongly interpreted their data as lysosome formation (ignoring the pre-existing organelles). Lysosomes could not be cell organelles, but part of cytoplasm such as mitochondria, and that hydrolytic enzymes were produced by microbodies. In 1963 Hruban, Spargo and colleagues published a detailed ultrastructural description of \"focal cytoplasmic degradation,\" which referenced a 1955 German study of injury-induced sequestration. Hruban, Spargo and colleagues recognized three continuous stages of maturation of the sequestered cytoplasm to lysosomes, and that the process was not limited to injury states that functioned under physiological conditions for \"reutilization of cellular materials,\" and the \"disposal of organelles\" during differentiation. Inspired by this discovery, de Duve christened the phenomena \"autophagy\". Unlike Porter and Ashford, de Duve conceived the term as a part of lysosomal function while describing the role of glucagon as a major inducer of cell degradation in the liver. With his student Russell Deter, he established that lysosomes are responsible for glucagon-induced autophagy. This was the first time the fact that lysosomes were established as the sites of intracellular autophagy.",
            "score": 138.1058807373047
        },
        {
            "docid": "22393_12",
            "document": "Organelle . Not all eukaryotic cells have each of the organelles listed below. Exceptional organisms have cells that do not include some organelles that might otherwise be considered universal to eukaryotes (such as mitochondria). There are also occasional exceptions to the number of membranes surrounding organelles, listed in the tables below (e.g., some that are listed as double-membrane are sometimes found with single or triple membranes). In addition, the number of individual organelles of each type found in a given cell varies depending upon the function of that cell. Mitochondria and chloroplasts, which have double-membranes and their own DNA, are believed to have originated from incompletely consumed or invading prokaryotic organisms, which were adopted as a part of the invaded cell. This idea is supported in the Endosymbiotic theory.",
            "score": 137.67579650878906
        },
        {
            "docid": "9568579_2",
            "document": "Fatty acid synthesis . Fatty acid synthesis is the creation of fatty acids from acetyl-CoA and NADPH through the action of enzymes called fatty acid synthases. This process takes place in the cytoplasm of the cell. Most of the acetyl-CoA which is converted into fatty acids is derived from carbohydrates via the glycolytic pathway. The glycolytic pathway also provides the glycerol with which three fatty acids can combine (by means of ester bonds) to form triglycerides (also known as \"triacylglycerols\", to distinguish them from fatty \"acids\" \u2013 or simply as \"fat\"), the final product of the lipogenic process. When only two fatty acids combine with glycerol and the third alcohol group is phosphorylated with a group such as phosphatidylcholine, a phospholipid is formed. Phospholipids form the bulk of the lipid bilayers that make up cell membranes and surround the organelles within the cells (e.g. the cell nucleus, mitochondria, endoplasmic reticulum, Golgi apparatus etc.)",
            "score": 135.73704528808594
        },
        {
            "docid": "34907176_2",
            "document": "Polar organelle . Polar organelle (German: Basalk\u00f6rper-Membran, also referred to in some microbiological literature articles as \"polar membrane\" or \"polar cap\") is a specialized region of the lipid cell membrane in bacterial cells, usually located at or near the so-called poles (e.g. the ends of a cylindrical cell). This region can easily be distinguished from the \"normal\" membrane regions in ultrathin sections of embedded bacteria by electron microscopy when the cell membrane is orientated perpendicular to the viewing direction. There, the membrane appears slightly thickened with a finely frilled layer facing the inside of the cell. It is also possible to isolate these polar organelles from the bacterial cells and study them in face view in negatively stained preparations. The polar organelle bears a fine array of attached particles in hexagonal close packing and these have been shown to possess ATPase activity. The polar organelle is invariably found in close juxtaposition to the points of insertion of the bacterial flagella into the plasma membrane, especially where multiple flagella bases are grouped in a region of the cell membrane. It may thus be inferred that the polar organelle could be of importance in the supply and transfer of energy to the bidirectional molecular rotational motor situated at the base of each individual bacterial flagellum (see also electrochemical gradient).",
            "score": 135.00836181640625
        },
        {
            "docid": "624361_9",
            "document": "Autophagy . Macroautophagy is the main pathway, used primarily to eradicate damaged cell organelles or unused proteins. First the phagophore engulfs the material that needs to be degraded, which forms a double membrane known as an autophagosome, around the organelle marked for destruction. The autophagosome then travels through the cytoplasm of the cell to a lysosome, and the two organelles fuse. Within the lysosome, the contents of the autophagosome are degraded via acidic lysosomal hydrolases.",
            "score": 132.70616149902344
        },
        {
            "docid": "329117_25",
            "document": "Christian de Duve . He was skeptical of referring to them as microbodies because, as he noted, \"too little is known of their enzyme complement and of their role in the physiology of the liver cells to substantiate a proposal at the present time\". He suggested that these enzymes belonged to the same cell organelle, but different from previously known organelles. But it would take some years before he publicised his hypothesis, as strong evidences were still lacking. In 1955 his team demonstrated similar cell fractions with same biochemical properties from the ciliated protozoan \"Tetrahymena pyriformis\", from which it was indicated that these particles were new cell organelles unrelated to mitochondria. He presented his discovery at a meeting of the American Society for Cell Biology in 1955, and formally published in 1966 in which he created the name peroxisomes for the organelles as they are involved in peroxidase reactions. In 1968 he achieved the first large-scale preparation of peroxisomes, confirming that l-\u03b1 hydroxyacid oxidase, d-amino acid oxidase, and catalase were all the unique enzymes of peroxisomes.",
            "score": 132.48602294921875
        },
        {
            "docid": "30064130_3",
            "document": "Fission (biology) . Organisms in the domains of Archaea and Bacteria reproduce with binary fission. This form of asexual reproduction and cell division is also used by some organelles within eukaryotic organisms (e.g., mitochondria). Binary fission results in the reproduction of a living prokaryotic cell (or organelle) by dividing the cell into two parts, each with the potential to grow to the size of the original.",
            "score": 132.3590545654297
        },
        {
            "docid": "996341_4",
            "document": "Spindle checkpoint . When cells are ready to divide, because cell size is big enough or because they receive the appropriate stimulus, they activate the mechanism to enter into the cell cycle, and they duplicate most organelles during S (synthesis) phase, including their centrosome. Therefore, when the cell division process will end, each daughter cell will receive a complete set of organelles. At the same time, during S phase all cells must duplicate their DNA very precisely, a process termed DNA replication. Once DNA replication has finished, in eukaryotes the DNA molecule is compacted and condensed, to form the mitotic chromosomes, each one constituted by two sister chromatids, which stay held together by the establishment of cohesion between them; each chromatid is a complete DNA molecule, attached via microtubules to one of the two centrosomes of the dividing cell, located at opposed poles of the cell. The structure formed by the centrosomes and the microtubules is named mitotic spindle, due to its characteristic shape, holding the chromosomes between the two centrosomes. Both sister chromatids stay together until anaphase; at this moment they separate from each other and they travel towards the centrosome to which they are attached. In this way, when the two daughter cells separate at the end of the division process, each one will receive a complete set of chromatids. The mechanism responsible for the correct distribution of sister chromatids during cell division is named chromosome segregation.",
            "score": 132.09121704101562
        },
        {
            "docid": "28346382_6",
            "document": "Organelle biogenesis . The process of organelle biogenesis is known to be regulated by specialized transcription networks that modulate the expression of the genes that code for specific organellar proteins. In order for organelle biogenesis to be carried out properly, the specific genes coding for the organellar proteins must be transcribed properly and the translation of the resulting mRNA must be successful. In addition to this, the process requires the transfer of polypeptides to their site of function, guided by signaling peptides. If proteins are not directed to their respective sites of subcellular function, a defective organelle that fails to fulfill its tasks within the cell properly may result.",
            "score": 132.07833862304688
        },
        {
            "docid": "286021_23",
            "document": "Centrifugation . Differential Centrifugation is a type of centrifugation in which one selectively spins down components of a mixture by a series of increasing centrifugation forces. This method is commonly used to separate organelles and membranes found in cells. Organelles generally differ from each other in density in size, making the use of differential centrifugation, and centrifugation in general, possible. The organelles can then be identified by testing for indicators that are unique to the specific organelles.",
            "score": 131.9783935546875
        },
        {
            "docid": "156970_33",
            "document": "Cytoskeleton . Cytoplasmic streaming, also known as cylcosis, is the active movement of a cell\u2019s contents along the components of the cytoskeleton. While mainly seen in plants, all cell types use this process for transportation of waste, nutrients, and organelles to other parts of the cell.\u00a0 Plant and algae cells are generally larger than many other cells; so cytoplasmic streaming is important in these types of cells. This is because the cell\u2019s extra volume requires cytoplasmic streaming in order to move organelles throughout the entire cell. Organelles move along microfilaments in the cytoskeleton driven by myosin motors binding and pushing along actin filament bundles.\u00a0",
            "score": 131.81114196777344
        },
        {
            "docid": "8631841_14",
            "document": "Glycosome . Unlike peroxisomes, for most of the trypanosomes their glycosomes are needed for them to be able to survive. Because of this need for the glycosome, it has been suggested as a possible drug target to find a drug to halt its function. When the glycosome is not functioning correctly there is a severe lack of enzymes in the cell. These enzymes are those associated with ether-lipid synthesis or the beta oxidation of certain fatty acids. Cells without glycosomes are deficient in these enzymes as without the compartmentalization of the glycosome the enzymes are degraded in the cell in the cytosol. The organelle keeps metabolism of the enzymes from occurring. For parasites, ether-lipid synthesis is vital to be able to complete its life cycle, making the enzymes protected by the glycosome also vital. In their life cycle, glycolysis partly through the glycosome is very high in the blood stream form comparatively to the pro-cyclic form. The glycosomal glycolysis pathway is necessary in stress situations for the pathogen as glycolysis can be started when the substrates for the pathway are available even when ATP is not available yet. So as this organelle is so essential for the trypanosome, if a drug could target this organelle, it could be a successful therapy as studies have shown without the glycosome parasite death occurs.",
            "score": 131.3640899658203
        },
        {
            "docid": "21501970_74",
            "document": "History of evolutionary thought . Indeed, the endosymbiotic theory for the origin of organelles sees a form of horizontal gene transfer as a critical step in the evolution of eukaryotes such as fungi, plants, and animals. The endosymbiotic theory holds that organelles within the cells of eukorytes such as mitochondria and chloroplasts, had descended from independent bacteria that came to live symbiotically within other cells. It had been suggested in the late 19th century when similarities between mitochondria and bacteria were noted, but largely dismissed until it was revived and championed by Lynn Margulis in the 1960s and 1970s; Margulis was able to make use of new evidence that such organelles had their own DNA that was inherited independently from that in the cell's nucleus.",
            "score": 130.6912384033203
        },
        {
            "docid": "4817_12",
            "document": "Biological membrane . Membranes in cells typically define enclosed spaces or compartments in which cells may maintain a chemical or biochemical environment that differs from the outside. For example, the membrane around peroxisomes shields the rest of the cell from peroxides, chemicals that can be toxic to the cell, and the cell membrane separates a cell from its surrounding medium. Peroxisomes are one form of vacuole found in the cell that contain by-products of chemical reactions within the cell. Most organelles are defined by such membranes, and are called \"membrane-bound\" organelles.",
            "score": 130.6506805419922
        },
        {
            "docid": "6235_54",
            "document": "Cell nucleus . An anucleated cell contains no nucleus and is, therefore, incapable of dividing to produce daughter cells. The best-known anucleated cell is the mammalian red blood cell, or erythrocyte, which also lacks other organelles such as mitochondria, and serves primarily as a transport vessel to ferry oxygen from the lungs to the body's tissues. Erythrocytes mature through erythropoiesis in the bone marrow, where they lose their nuclei, organelles, and ribosomes. The nucleus is expelled during the process of differentiation from an erythroblast to a reticulocyte, which is the immediate precursor of the mature erythrocyte. The presence of mutagens may induce the release of some immature \"micronucleated\" erythrocytes into the bloodstream. Anucleated cells can also arise from flawed cell division in which one daughter lacks a nucleus and the other has two nuclei.",
            "score": 130.11277770996094
        },
        {
            "docid": "60426_8",
            "document": "Symbiogenesis . According to Keeling and Archibald, the usual way to distinguish organelles from endosymbionts is by their reduced genome sizes. As an endosymbiont evolves into an organelle, most of their genes are transferred to the host cell genome. The host cell and organelle need to develop a transport mechanism that enables the return of the protein products needed by the organelle but now manufactured by the cell. Cyanobacteria and \u03b1-proteobacteria are the most closely related free-living organisms to plastids and mitochondria respectively. Both cyanobacteria and \u03b1-proteobacteria maintain a large (>6Mb) genome encoding thousands of proteins. Plastids and mitochondria exhibit a dramatic reduction in genome size when compared to their bacterial relatives. Chloroplast genomes in photosynthetic organisms are normally 120-200kb encoding 20-200 proteins and mitochondrial genomes in humans are approximately 16kb and encode 37 genes, 13 of which are proteins. Using the example of the freshwater amoeboid, however, \"Paulinella chromatophora\", which contains chromatophores found to be evolved from cyanobacteria, Keeling and Archibald argue that this is not the only possible criterion; another is that the host cell has assumed control of the regulation of the former endosymbiont's division, thereby synchronizing it with the cell's own division. Nowack and her colleagues performed gene sequencing on the chromatophore (1.02 Mb) and found that only 867 proteins were encoded by these photosynthetic cells. Comparisons with their closest free living cyanobacteria of the genus \"Synechococcus\" (having a genome size 3 Mb, with 3300 genes) revealed that chromatophores underwent a drastic genome shrinkage. Chromatophores contained genes that were accountable for photosynthesis but were deficient in genes that could carry out other biosynthetic functions; this observation suggests that these endosymbiotic cells are highly dependent on their hosts for their survival and growth mechanisms. Thus, these chromatophores were found to be non-functional for organelle-specific purposes when compared to mitochondria and plastids. This distinction could have promoted the early evolution of photosynthetic organelles.",
            "score": 129.4409942626953
        },
        {
            "docid": "609812_6",
            "document": "Dynein . Cytoplasmic dynein helps to position the Golgi complex and other organelles in the cell. It also helps transport cargo needed for cell function such as vesicles made by the endoplasmic reticulum, endosomes, and lysosomes (Karp, 2005). Dynein is involved in the movement of chromosomes and positioning the mitotic spindles for cell division. Dynein carries organelles, vesicles and possibly microtubule fragments along the axons of neurons toward the cell body in a process called retrograde axoplasmic transport.",
            "score": 129.09182739257812
        },
        {
            "docid": "2957025_2",
            "document": "Axonal transport . Axonal transport, also called axoplasmic transport or axoplasmic flow, is a cellular process responsible for movement of mitochondria, lipids, synaptic vesicles, proteins, and other cell parts (i.e. organelles) to and from a neuron's cell body, through the cytoplasm of its axon (the axoplasm). Since some axons are on the order of meters long, neurons cannot rely on diffusion to carry products of the nucleus and organelles to the end of their axons. Axonal transport is also responsible for moving molecules destined for degradation from the axon back to the cell body, where they are broken down by lysosomes.",
            "score": 128.79681396484375
        },
        {
            "docid": "466746_10",
            "document": "Mycoplasma pneumoniae . Adherence of \"M. pneumoniae\" to a host cell (usually a respiratory tract cell, but occasionally an erythrocyte or urogenital lining cell) is the initiating event for pneumonic disease and related symptoms. The specialized attachment organelle is a polar, electron dense and elongated cell extension that facilitates motility and cytadherence to host cells. It is composed of a central filament surrounded by an intracytoplasmic space, along with a number of adhesins and structural and accessory proteins localized at the tip of the organelle. A variety of proteins are known to contribute to the formation and functionality of the attachment organelle, including the accessory proteins HMW1\u2013HMW5, P30, P56, and P90 that confer structure and adhesin support, and P1, P30 and P116 which are involved directly in attachment. This network of proteins participates not only in the initiation of attachment organelle formation and adhesion but also in motility.  The P1 adhesin (trypsin-sensitive protein) is a 120 kDa protein highly clustered on the surface of the attachment organelle tip in virulent mycoplasmas. Both the presence of P1 and its concentration on the cell surface are required for the attachment of \"M. pneumoniae\" to the host cell. \"M. pneumoniae\" cells treated with monoclonal antibodies specific to the immunogenic C-terminus of the P1 adhesin have been shown to be inhibited in their ability to attach to the host cell surface by approximately 75%, suggesting P1 is a major component in cytadherence. These antibodies also decreased the ability of the cell to glide quickly, which may contribute to decreased adherence to the host by hindering their capacity to locate a host cell. Furthermore, mutations in P1 or degradation by trypsin treatment yield avirulent \"M. pneumoniae\" cells. Loss of proteins in the cytoskeleton involved in the localization of P1 in the tip structure, such as HMW1\u2013HMW3, also cause avirulence due to the lack of adhesin clustering. Another protein considered to play an important role in cytadherence is P30, as \"M. pneumoniae\" cells with mutations in this protein or that have had antibodies raised against P30 are incapable of adhering to host cells. P30 is not involved in the localization of P1 in the tip structure since P1 is trafficked to the attachment organelle in P30 mutants, but rather it may function as a receptor-binding accessory adhesin. P30 mutants also display distinct morphological features such as multiple lobes and a rounded shape as opposed to elongated, which suggests P30 may interact with the cytoskeleton during formation of the attachment organelle. A number of eukaryotic cell surface components have been implicated in the adherence of \"M. pneumoniae\" cells to the respiratory tract epithelium. Among them are sialoglycoconjugates, sulfated glycolipids, glycoproteins, fibronectin, and neuraminic acid receptors. Lectins on the surface of the bacterial cells are capable of binding oligosaccharide chains on glycolipids and glycoproteins to facilitate attachment, in addition to the proteins TU and pyruvate dehydrogenase E1 \u03b2, which bind to fibronectin.",
            "score": 128.21444702148438
        },
        {
            "docid": "41048441_6",
            "document": "Alex B. Novikoff . In 1956 Novikoff described a new class of membrane-bound organelles that he called \"dense bodies.\" was the first to describe the actual lysosomal functions with respect to degradation of mitochondria. However he thought that the digestive activities he observed were due to other intracelluar organelles which he called \"cytolysomes\". It was at the \"Ciba Foundation Symposium on Lysosomes\" held in London on 12\u201314 February 1963, that he explained this phenomenon in which organelles such as endoplasmic reticulum, ribosomes, mitochondria and other cell debris were degraded by autolysis in the cytolysomes. Then the following speaker de Duve correctly identified that these organelles were lysosomes, and named them autophagic vacuoles, and he introduced the term \"autophagy\" for the process of such intracellular digestion. In 1962 he established for the first time the functional relationship between ER, Golgi and lysosomes. He specifically showed that smooth-surfaced derivatives of the ER fused with the Golgi membranes and the Golgi membranes in turn fused with lysosomes. He was the first to show that this GERL is responsible vesicular transport during synthesis and sorting of proteins. He gave this functional organisation an acronym GERL, for \"G\"olgi-\"e\"ndoplasmic \"r\"eticulum-\"l\"ysosome. Novikoff's further works became a milestone in understanding the importance of autophagy in diseases such as cancer.",
            "score": 127.66120910644531
        },
        {
            "docid": "9927_6",
            "document": "Endomembrane system . Most lipids are synthesized in yeast either in the endoplasmic reticulum, lipid particles, or the mitochondrion, with little or no lipid synthesis occurring in the plasma membrane or nuclear membrane. Sphingolipid biosynthesis begins in the endoplasmic reticulum, but is completed in the Golgi apparatus. The situation is similar in mammals, with the exception of the first few steps in ether lipid biosynthesis, which occur in peroxisomes. The various membranes that enclose the other subcellular organelles must therefore be constructed by transfer of lipids from these sites of synthesis. However, although it is clear that lipid transport is a central process in organelle biogenesis, the mechanisms by which lipids are transported through cells remain poorly understood.",
            "score": 127.57083892822266
        },
        {
            "docid": "23257478_11",
            "document": "Labyrinthula . The defining characteristic of the genera \"Labyrinthula\" is the formation of an ectoplasmic net around the cells and embedding the whole colony. The ectoplasmic net is secreted and attached to the cell by specialized organelles called segenetosome or bothrosomes. A bothrosome is an electron-opaque organelle, which prevents the leaking of the organelles into the net. The etymology of \"bothrosome\" and \"sagenetosome\" originated from \"bothros\": hole and \"soma\": body, as well as from \"sagena\": net, \"genetes\": ancestor and \"soma\": body respectively. The net is composed of secreted ectoplasm and is delimited by a plasma membrane. It lacks a cell wall and contains no organelles. By forming long filaments, the ectoplasmic net allows the colony to attach to surfaces and it secretes digestive enzymes for absorptive nutrition. These enzymes can be surface-bound or secreted into the medium to help the digestion of organic substances. Individual cells use the ectoplasmic net for movement by gliding inside it. They move in all directions, but they tend to go towards the periphery of the ectoplasmic net to enlarge the net and thus increase its surface area.",
            "score": 127.5030746459961
        },
        {
            "docid": "61899_4",
            "document": "Phloem . Sieve elements are the type of cell that are responsible for transporting sugars throughout the plant. At maturity they lack a nucleus and have very few organelles, so they rely on companion cells or albuminous cells for most of their metabolic needs. Sieve tube cells do contain vacuoles and other organelles, such as ribosomes, before they mature, but these generally migrate to the cell wall and dissolve at maturity; this ensures there is little to impede the movement of fluids. One of the few organelles they do contain at maturity is the smooth endoplasmic reticulum, which can be found at the plasma membrane, often nearby the plasmodesmata that connect them to their companion or albuminous cells. All sieve cells have groups of pores at their ends that grow from modified and enlarged plasmodesmata, called sieve areas. The pores are reinforced by platelets of a polysaccharide called callose.",
            "score": 127.49555206298828
        }
    ]
}