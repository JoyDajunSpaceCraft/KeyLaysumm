{
    "q": [
        {
            "docid": "33932515_22",
            "document": "Social cue . Benjamin Straube, Antonia Green, Andreas Jansen, Anjan Chatterjee, and Tilo Kircher found that social cues influence the neural processing of speech-gesture utterances. Past studies have focused on mentalizing as being a part of perception of social cues and it is believed that this process relies on the neural system, which consists of: When people focus on things in a social context, the medial prefrontal cortex and precuneus areas of the brain are activated, however when people focus on a non-social context there is no activation of these areas. Straube et al. hypothesized that the areas of the brain involved in mental processes were mainly responsible for social cue processing. It is believed that when iconic gestures are involved, the left temporal and occipital regions would be activated and when emblematic gestures were involved the temporal poles would be activated. When it came to abstract speech and gestures, the left frontal gyrus would be activated according to Straube et al. After conducting an experiment on how body position, speech and gestures affected activation in different areas of the brain Straube et al. came to the following conclusions:",
            "score": 140.0093286037445
        },
        {
            "docid": "23158496_4",
            "document": "Superior temporal sulcus . In individuals without autism, the superior temporal sulcus also activates when hearing human voices. It is thought to be a source of sensory encoding linked to motor output through the superior parietal-temporal areas of the brain inferred from the time course of activation. The conclusion of pertinence to vocal processing can be drawn from data showing that the regions of the STS (superior temporal sulcus) are more active when people are listening to vocal sounds rather than non-vocal environmentally based sounds and corresponding control sounds, which can be scrambled or modulated voices. These experimental results indicate the involvement of the STS in the areas of speech and language recognition.",
            "score": 178.80229914188385
        },
        {
            "docid": "6541938_8",
            "document": "Lexical-gustatory synesthesia . SC is a synesthete who automatically experiences smells, tastes, and feelings of textures in her mouth and throat when she reads, speaks, or hears language, music, and certain environmental sounds. In SC\u2019s case study, researchers utilized fMRI to determine the areas of the brain that were activated during her synesthetic experiences. They compared areas of activation in SC\u2019s brain to those found in literature for other synesthetes, speech processing, language, and sound processing. In SC\u2019s scans, two important regions of the brain were largely activated during her taste sensations: the left anterior insula and the left superior parietal lobe. The scans led researchers to speculate that the anterior insula may play a role in SC\u2019s taste experiences while the superior parietal lobe binds together all of the sensory information for processing. Based off the findings of this study and others like it, it could be possible to determine the type of inducer that leads to synesthetic sensations based on the patterns of brain activity.",
            "score": 147.9666885137558
        },
        {
            "docid": "525667_10",
            "document": "Human echolocation . In a 2014 study by Thaler and colleagues, the researchers first made recordings of the clicks and their very faint echoes using tiny microphones placed in the ears of the blind echolocators as they stood outside and tried to identify different objects such as a car, a flag pole, and a tree. The researchers then played the recorded sounds back to the echolocators while their brain activity was being measured using functional magnetic resonance imaging. Remarkably, when the echolocation recordings were played back to the blind experts, not only did they perceive the objects based on the echoes, but they also showed activity in those areas of their brain that normally process visual information in sighted people, primarily primary visual cortex or V1. This result is surprising, as visual areas, as their names suggest, are only active during visual tasks. The brain areas that process auditory information were no more activated by sound recordings of outdoor scenes containing echoes than they were by sound recordings of outdoor scenes with the echoes removed. Importantly, when the same experiment was carried out with sighted people who did not echolocate, these individuals could not perceive the objects and there was no echo-related activity anywhere in the brain. This suggests that the cortex of blind echolocators is plastic and reorganizes such that primary visual cortex, rather than any auditory area, becomes involved in the computation of echolocation tasks.",
            "score": 197.7156971693039
        },
        {
            "docid": "5366050_42",
            "document": "Speech perception . Research into the relationship between music and cognition is an emerging field related to the study of speech perception. Originally it was theorized that the neural signals for music were processed in a specialized \"module\" in the right hemisphere of the brain. Conversely, the neural signals for language were to be processed by a similar \"module\" in the left hemisphere. However, utilizing technologies such as fMRI machines, research has shown that two regions of the brain traditionally considered exclusively to process speech, Broca's and Wernicke's areas, also become active during musical activities such as listening to a sequence of musical chords. Other studies, such as one performed by Marques et al. in 2006 showed that 8-year-olds who were given six months of musical training showed an increase in both their pitch detection performance and their electrophysiological measures when made to listen to an unknown foreign language.",
            "score": 117.94040524959564
        },
        {
            "docid": "23060403_12",
            "document": "Motor theory of speech perception . Using a speech synthesizer, speech sounds can be varied in place of articulation along a continuum from to to , or in voice onset time on a continuum from to (for example). When listeners are asked to discriminate between two different sounds, they perceive sounds as belonging to discrete categories, even though the sounds vary continuously. In other words, 10 sounds (with the sound on one extreme being and the sound on the other extreme being , and the ones in the middle varying on a scale) may all be acoustically different from one another, but the listener will hear all of them as either or . Likewise, the English consonant may vary in its acoustic details across different phonetic contexts (the /d/ in does not technically sound the same as the one in , for example), but all 's as perceived by a listener fall within one category (voiced alveolar plosive) and that is because \"linguistic representations are abstract, canonical, phonetic segments or the gestures that underlie these segments.\" This suggests that humans identify speech using categorical perception, and thus that a specialized module, such as that proposed by the motor theory of speech perception, may be on the right track.",
            "score": 176.03226745128632
        },
        {
            "docid": "40621603_8",
            "document": "Linguistic intelligence . Generation of written language is thought to be closely related to speech generation, relying on Broca's area for early processing and on the inferior frontal gyrus for semantic processing. However, writing differs in two major ways. First, instead of relating the thought to sounds, the brain must relate the thought to symbols or letters, and second, the motor cortex activates a different set of muscles to write, than when speaking.",
            "score": 195.12298345565796
        },
        {
            "docid": "315084_25",
            "document": "Lip reading . Following the discovery that auditory brain regions, including Heschl's gyrus, were activated by seen speech, the neural circuitry for speechreading was shown to include supra-modal processing regions, especially superior temporal sulcus (all parts) as well as posterior inferior occipital-temporal regions including regions specialised for the processing of faces and biological motion. In some but not all studies, activation of Broca's area is reported for speechreading, suggesting that articulatory mechanisms can be activated in speechreading. Studies of the time course of audiovisual speech processing showed that sight of speech can prime auditory processing regions in advance of the acoustic signal. Better lipreading skill is associated with greater activation in (left) superior temporal sulcus and adjacent inferior temporal (visual) regions in hearing people. In deaf people, the circuitry devoted to speechreading appears to be very similar to that in hearing people, with similar associations of (left) superior temporal activation and lipreading skill.",
            "score": 94.69640624523163
        },
        {
            "docid": "14405771_9",
            "document": "Speech science . Speech perception refers to the understanding of speech. The beginning of the process towards understanding speech is first hearing the message that is spoken. The auditory system receives sound signals starting at the outer ear. They enter the pinna and continue into the external auditory canal (ear canal) and then to the eardrum. Once in the middle ear, which consists of the malleus, the incus, and the stapes; the sounds are changed into mechanical energy. After being converted into mechanical energy, the message reaches the oval window, which is the beginning of the inner ear. Once inside the inner ear, the message is transferred into hydraulic energy by going through the cochlea, which is filled with fluid, and on to the Organ of Corti. This organ again helps the sound to be transferred into a neural impulse that stimulates the auditory pathway and reaches the brain. Sound is then processed in Heschl's gyrus and associated with meaning in Wernicke's area. As for theories of speech perception, there are a motor and an auditory theory. The motor theory is based upon the premise that speech sounds are encoded in the acoustic signal rather than enciphered in it. The auditory theory puts greater emphasis on the sensory and filtering mechanisms of the listener and suggests that speech knowledge is a minor role that\u2019s only used in hard perceptual conditions.",
            "score": 153.93974030017853
        },
        {
            "docid": "17893852_4",
            "document": "Humor research . Cognitive neuroscience has provided insight into how humor is neurologically realized. Brain imaging techniques such as fMRI and PET scans have been implemented in this subfield of humor research.  There are a few main regions of the human brain associated with humor and laughter. The production of laughter involves two primary brain pathways, one for both involuntary and voluntary laughter (cf. Duchenne and non-Duchenne). Involuntary laughter is usually emotionally driven, and includes key emotional brain areas such as the amygdala, thalamic areas, and the brainstem. Voluntary laughter instead begins in the premotor opercular area (in the temporal lobe) and moves to the motor cortex and pyramidal tract before moving to the brainstem. Wild et al. (2003) propose that the generation of laughter is mostly influenced by neural pathways that go from the premotor and motor cortex to the ventral side of the brainstem, through the cerebral peduncles. It is also suggested that real laughter is not produced from the motor cortex, but that the normal inhibition of cortical frontal areas stops during laughter. When the electrical activity of the brain is measured during and after hearing a joke, a prominent response can be seen approximately 300ms after the punchline, followed by a depolarization about 100ms later. The fact that humor response occurs in two separate waves of activity supports the idea that humor processing occurs in two stages. Functional MRI and PET studies further illuminate which parts of the brain are participating in the experience of humor. A study by Ozawa, et al., (2000) found that hearing sentences which participants rated as humorous resulted in activation in Broca's area and the middle frontal gyrus, in addition to Wernicke's area and the transverse temporal gyri, which were activated in control (non-humorous) conditions as well.",
            "score": 192.67496752738953
        },
        {
            "docid": "37970096_5",
            "document": "Sign language in the brain . When communicating in their respective languages, similar brain regions are activated for both deaf and hearing subjects with a few exceptions. During the processing of auditory stimuli for spoken languages there is detectable activity within Broca's Area, Wernicke's Area, the angular gyrus, dorsolateral prefrontal cortex, and superior temporal sulcus. Right hemisphere activity was detectable in less than 50% of trials for hearing subjects reciting English sentences. When deaf subjects were tasked with reading English, none of the left hemisphere structures seen with hearing subjects were visible. Deaf subjects also displayed obvious middle and posterior temporal-parietal activation within the right hemisphere. When hearing subjects were presented various signs designed to evoke emotion within native signers, there was no clear changes in brain activity in traditional language processing centers. Brain activity of deaf native signers when processing signs was similar to activity of hearing subjects processing English. However, processing of ASL extensively recruited right hemisphere structures including significant activation of the entire superior temporal lobe, the angular region, and inferior prefrontal cortex. Since native hearing signers also exhibited this right hemisphere activation when processing ASL, it has been proposed that this right hemisphere activation is due to the temporal visuospatial decoding necessary to process signed languages.",
            "score": 142.6399985551834
        },
        {
            "docid": "7330954_31",
            "document": "Pattern recognition (psychology) . MIT researchers conducted a study to examine this notion. The results showed six neural clusters in the auditory cortex responding to the sounds. Four were triggered when hearing standard acoustic features, one specifically responded to speech, and the last exclusively responded to music. Researchers who studied the correlation between temporal evolution of timbral, tonal and rhythmic features of music, came to the conclusion that music engages the brain regions connected to motor actions, emotions and creativity. The research indicates that the whole brain \"lights up\" when listening to music. This amount of activity boosts memory preservation, hence pattern recognition.",
            "score": 187.84296333789825
        },
        {
            "docid": "433584_4",
            "document": "McGurk effect . Vision is the primary sense for humans, but speech perception is multimodal, which means that it involves information from more than one sensory modality, in particular, audition and vision. The McGurk effect arises during phonetic processing because the integration of audio and visual information happens early in speech perception. The McGurk effect is very robust; that is, knowledge about it seems to have little effect on one's perception of it. This is different from certain optical illusions, which break down once one 'sees through' them. Some people, including those that have been researching the phenomenon for more than twenty years, experience the effect even when they are aware that it is taking place. With the exception of people who can identify most of what is being said from speech-reading alone, most people are quite limited in their ability to identify speech from visual-only signals. A more extensive phenomenon is the ability of visual speech to increase the intelligibility of heard speech in a noisy environment. Visible speech can also alter the perception of perfectly audible speech sounds when the visual speech stimuli are mismatched with the auditory speech. Normally, speech perception is thought to be an auditory process; however, our use of information is immediate, automatic, and, to a large degree, unconscious and therefore, despite what is widely accepted as true, speech is not only something we hear. Speech is perceived by all of the senses working together (seeing, touching, and listening to a face move). The brain is often unaware of the separate sensory contributions of what it perceives. Therefore, when it comes to recognizing speech the brain cannot differentiate whether it is seeing or hearing the incoming information.",
            "score": 143.74057805538177
        },
        {
            "docid": "25490263_16",
            "document": "Speech repetition . Two cortical processing streams exist: a ventral one which maps sound onto meaning, and a dorsal one, that maps sound onto motor representations. The dorsal stream projects from the posterior Sylvian fissure at the temporoparietal junction, onto frontal motor areas, and is not normally involved in speech perception.  Carl Wernicke identified a pathway between the left posterior superior temporal sulcus (a cerebral cortex region sometimes called the Wernicke's area) as a centre of the sound \"images\" of speech and its syllables that connected through the arcuate fasciculus with part of the inferior frontal gyrus (sometimes called the Broca's area) responsible for their articulation. This pathway is now broadly identified as the dorsal speech pathway, one of the two pathways (together with the ventral pathway) that process speech. The posterior superior temporal gyrus is specialized for the transient representation of the phonetic sequences used for vocal repetition. Part of the auditory cortex also can represent aspects of speech such as its consonantal features.",
            "score": 167.91781520843506
        },
        {
            "docid": "25146378_20",
            "document": "Functional specialization (brain) . Other researchers who provide evidence to support the theory of distributive processing include Anthony McIntosh and William Uttal, who question and debate localization and modality specialization within the brain. McIntosh's research suggests that human cognition involves interactions between the brain regions responsible for processes sensory information, such as vision, audition, and other mediating areas like the prefrontal cortex. McIntosh explains that modularity is mainly observed in sensory and motor systems, however, beyond these very receptors, modularity becomes \"fuzzier\" and you see the cross connections between systems increase. He also illustrates that there is an overlapping of functional characteristics between the sensory and motor systems, where these regions are close to one another. These different neural interactions influence each other, where activity changes in one area influence other connected areas. With this, McIntosh suggest that if you only focus on activity in one area, you may miss the changes in other integrative areas. Neural interactions can be measured using analysis of covariance in neuroimaging. McIntosh used this analysis to convey a clear example of the interaction theory of distributive processing. In this study, subjects learned that an auditory stimulus signalled a visual event. McIntosh found activation (an increase blood flow), in an area of the occipital cortex, a region of the brain involved in visual processing, when the auditory stimulus was presented alone. Correlations between the occipital cortex and different areas of the brain such as the prefrontal cortex, premotor cortex and superior temporal cortex showed a pattern of co-variation and functional connectivity.",
            "score": 186.4667751789093
        },
        {
            "docid": "21312301_12",
            "document": "Context-dependent memory . A number of neuroanatomical structures are thought to play a role in context-dependent memory, These include the hippocampus and prefrontal cortex. For example, functional magnetic resonance imaging (fMRI) has been used to demonstrate elevated activation in the hippocampus when contextual information matches from encoding to retrieval, suggesting that the hippocampus may be important in mediating context-dependent memory processes. Kalisch et al. provide further support for this role by demonstrating that context-dependent extinction memory is correlated with activation in both the hippocampus and ventromedial prefrontal cortex. Similarly, an experiment by Wagner et al. using fMRI demonstrated that activation of the right prefrontal cortex depended on contextual information. The authors of this study suggest that differential activation of the prefrontal cortex occurs because the different contexts require unique attempt processes for retrieval. In other words, depending on the retrieval context, participants used different strategies to recall information. Overall, the patterns of activation in the hippocampus and the prefrontal cortex following changes in contextual information suggest that these brain regions play an important role in context-dependent memory.",
            "score": 152.61517465114594
        },
        {
            "docid": "5505463_3",
            "document": "Bimodal bilingualism . Most modern neurological studies of bilingualism employ functional neuroimaging techniques to elucidate the neurological underpinnings of multilingualism and how multilingualism is beneficial to the brain. Neuroimaging and other neurological studies have demonstrated in recent years that multilingualism has a significant impact on the human brain. The mechanisms required by bilinguals to \"code switch\" (a linguistic term used to describe the rapid alternating between multiple languages within a conversation or discourse), not only demonstrate increased connectivity and density of the neural network in multilinguals, but also appear to provide protection against damage due to age and age-related pathologies, such as Alzheimer's. Multilingualism, especially bimodal multilingualism, can help slow to process of cognitive decline in aging. It is thought that this is a result of the increased work load that the executive system, housed mostly in the frontal cortex, must assume in order to successfully control the use of multiple languages at once. This means that the cortex must be more finely tuned, which results in a \"neural reserve\" that then has neuroprotective benefits. Gray matter volume (GMV) has been shown to be significantly preserved in bimodal bilinguals as compared to monolinguals in multiple brain areas, including the hippocampus, amygdala, anterior temporal lobes, and left insula. Similarly, neuroimaging studies that have compared monolinguals, unimodal bilinguals, and bimodal bilinguals provide evidence that deaf signers exhibit brain activation in patterns different than those of hearing signers, especially in regards to the left superior temporal sulcus. In deaf signers, activation of the superior temporal sulcus is highly lateralized to the left side during facial recognition tasks, while this lateralization was not present in hearing, bimodal signers. Bilinguals also require an effective and fast neural control system to allow them to select and control their languages even while code switching rapidly. Evidence indicates that the left caudate nucleus\u2014a centrally located brain feature that is near the thalamus and the basal ganglia\u2014is an important part of this mechanism, as bilinguals tend to have significantly increased GMV and activation in this region as compared to monolinguals, especially during active code switching tasks. As implied by the significant preservation of gray matter in the hippocampi (an area of the brain largely associated with memory consolidation and higher cognitive function, such as decision-making) of bimodal bilinguals, areas of the brain that help control phonological working memory tend to also have higher activation in those individuals who are proficient in two or more languages. There is also evidence that suggests that the age at which an individual acquires a second language may play a significant role in the varying brain functions associated with bilingualism. For example, individuals who acquired their second language early (before the age of 10) tend to have drastically different activation patterns than do late learners. However, late learners who achieve full proficiency in their second language tend to show similar patterns of activation during auditory tasks regardless of which language is being used, whereas early learners tend to activate different brain areas depending upon which language is being used. Along with the neuroprotective benefits that help to prevent onset of age-related cognitive issues such as dementia, bimodal bilinguals also experience a slightly different pattern of organization of language in the brain. While non-hearing-impaired bimodal bilinguals showed less parietal activation than deaf signers when asked to use only sign language, those same bimodal bilinguals demonstrated greater left parietal activation than did monolinguals. Parietal activation is not typically associated with language production bur rather with motor activity. Therefore, it is logical that bimodal bilinguals, when switching between speech- and sign-based language, stimulate their left parietal areas as a result of their increased need to combine both motor action and language production.",
            "score": 169.34314513206482
        },
        {
            "docid": "24514_35",
            "document": "Psychosis . Studies during acute experience of hallucinations demonstrate increased activity in primary or secondary sensory cortices. As auditory hallucinations are most common in psychosis, most robust evidence exists for increased activity in the left middle temporal gyrus, left superior temporal gyrus, and left inferior frontal gyrus (i.e. Broca's area). Activity in the ventral striatum, hippocampus, and ACC are related to the lucidity of hallucinations, and indicate that activation or involvement of emotional circuitry are key to the impact of abnormal activity in sensory cortices. Together, these findings indicate abnormal processing of internally generated sensory experiences, coupled with abnormal emotional processing, results in hallucinations. One proposed model involves a failure of feedforward networks from sensory cortices to the inferior frontal cortex, which normal cancel out sensory cortex activity during internally generated speech. The resulting disruption in expected and perceived speech is thought to produce lucid hallucinatory experiences.",
            "score": 128.306755900383
        },
        {
            "docid": "14241792_3",
            "document": "TRACE (psycholinguistics) . TRACE was created during the formative period of connectionism, and was included as a chapter in \"Parallel Distributed Processing: Explorations in the Microstructures of Cognition\". The researchers found that certain problems regarding speech perception could be conceptualized in terms of a connectionist interactive activation model. The problems were that (1) speech is extended in time, (2) the sounds of speech (phonemes) overlap with each other, (3) the articulation of a speech sound is affected by the sounds that come before and after it, and (4) there is natural variability in speech (e.g. foreign accent) as well as noise in the environment (e.g. busy restaurant). Each of these causes the speech signal to be complex and often ambiguous, making it difficult for the human mind/brain to decide what words it is really hearing. In very simple terms, an interactive activation model solves this problem by placing different kinds of processing units (phonemes, words) in isolated layers, allowing activated units to pass information between layers, and having units within layers compete with one another, until the \u201cwinner\u201d is considered \u201crecognized\u201d by the model.",
            "score": 137.0621156692505
        },
        {
            "docid": "5366050_59",
            "document": "Speech perception . To provide a theoretical account of the categorical perception data, Liberman and colleagues worked out the motor theory of speech perception, where \"the complicated articulatory encoding was assumed to be decoded in the perception of speech by the same processes that are involved in production\" (this is referred to as analysis-by-synthesis). For instance, the English consonant may vary in its acoustic details across different phonetic contexts (see above), yet all 's as perceived by a listener fall within one category (voiced alveolar plosive) and that is because \"linguistic representations are abstract, canonical, phonetic segments or the gestures that underlie these segments\". When describing units of perception, Liberman later abandoned articulatory movements and proceeded to the neural commands to the articulators and even later to intended articulatory gestures, thus \"the neural representation of the utterance that determines the speaker's production is the distal object the listener perceives\". The theory is closely related to the modularity hypothesis, which proposes the existence of a special-purpose module, which is supposed to be innate and probably human-specific.",
            "score": 103.8077986240387
        },
        {
            "docid": "33993614_28",
            "document": "Neurocomputational speech processing . On the other hand the speech sound map, if activated for a specific speech unit (single neuron activation; punctual activation), activates sensory information by synaptic projections between speech sound map and auditory target region map and between speech sound map and somatosensory target region map. Auditory and somatosensory target regions are assumed to be located in higher-order auditory cortical regions and in higher-order somatosensory cortical regions respectively. These target region sensory activation patterns - which exist for each speech unit - are learned during speech acquisition (by imitation training; see below: learning).",
            "score": 109.9138560295105
        },
        {
            "docid": "25140_47",
            "document": "Perception . \"Speech perception\" is the process by which spoken languages are heard, interpreted and understood. Research in speech perception seeks to understand how human listeners recognize speech sounds and use this information to understand spoken language. The sound of a word can vary widely according to words around it and the tempo of the speech, as well as the physical characteristics, accent and mood of the speaker. Listeners manage to perceive words across this wide range of different conditions. Another variation is that reverberation can make a large difference in sound between a word spoken from the far side of a room and the same word spoken up close. Experiments have shown that people automatically compensate for this effect when hearing speech.",
            "score": 138.86043119430542
        },
        {
            "docid": "1095131_20",
            "document": "Kinesthetic learning . The cerebral cortex is the brain tissue covering the top and sides of the brain in most vertebrates. It is involved in storing and processing of sensory inputs and motor outputs. In the human brain, the cerebral cortex is actually a sheet of neural tissue about 1/8th inch thick. The sheet is folded so that it can fit inside the skull. The neural circuits in this area of the brain expand with practice of an activity, just like the synaptic plasticity grows with practice. Clarification of some of the mechanisms of learning by neuro science has been advanced, in part, by the advent of non-invasive imaging technologies, such as positron emission tomography (PET) and functional magnetic resonance imaging (FMRI). These technologies have allowed researchers to observe human learning processes directly. Through these types of technologies, we are now able to see and study what happens in the process of learning. In different tests performed the brain being imaged showed a greater blood flow and activation to that area of the brain being stimulated through different activities such as finger tapping in a specific sequence. It has been revealed that the process at the beginning of learning a new skill happens quickly, and later on slows down to almost a plateau. This process can also be referred to as The Law of Learning. The slower learning showed in the FMRI that in the cerebral cortex this was when the long term learning was occurring, suggesting that the structural changes in the cortex reflect the enhancement of skill memories during later stages of training. When a person studies a skill for a longer duration of time, but in a shorter amount of time they will learn quickly, but also only retain the information into their short-term memory. Just like studying for an exam; if a student tries to learn everything the night before, it will not stick in the long run. If a person studies a skill for a shorter duration of time, but more frequently and long-term, their brain will retain this information much longer as it is stored in the long-term memory. Functional and structural studies of the brain have revealed a vast interconnectivity between diverse regions of the cerebral cortex. For example, large numbers of axons interconnect the posterior sensory areas serving vision, audition, and touch with anterior motor regions. Constant communication between sensation and movement makes sense, because to execute smooth movement through the environment, movement must be continuously integrated with knowledge about one's surroundings obtained via sensory perception. The cerebral cortex plays a role in allowing humans to do this.",
            "score": 190.86478650569916
        },
        {
            "docid": "10042066_2",
            "document": "Developmental linguistics . Developmental linguistics is the study of the development of linguistic ability in an individual, particularly the acquisition of language in childhood. It involves research into the different stages in language acquisition, language retention, and language loss in both first and second languages, in addition to the area of bilingualism. Before infants can speak, the neural circuits in their brains are constantly being influenced by exposure to language. The neurobiology of language contains a \"critical period\" in which children are most sensitive to language. The different aspects of language have varying \"critical periods\". Studies show that the critical period for phonetics is toward the end of the first year. At 18 months, a toddler's vocabulary vastly expands. The critical period for syntactic learning is 18-36 months. Infants of different mother languages can be differentiated at the age of 10 months. At 20 weeks they begin vocal imitation. Beginning when babies are about 12 months, they take on computational learning and social learning. Social interactions for infants and toddlers is important because it helps associate \"perception and action\". In-person social interaction rather than audio or video better facilitates learning in babies because they learn from how other people respond to them, especially their mothers. Babies have to learn to mimic certain syllables, which takes practice in manipulating tongue and lip movement. Sensory-motor learning in speech is linked to exposure to speech, which is very sensitive to language. Infants exposed to Spanish exhibit a different vocalization than infants exposed to English. One study took infants that were learning English and made them listen to Spanish in 12 sessions. The result showed consequent alterations in their vocalization, which demonstrated Spanish prosody.  One study used MEG to record activation in the brains of newborns, 6 months olds and 12 months olds while presenting them with syllables, harmonics and non-speech sounds. For the 6 month and 12 month old, the auditory and motor areas responded to speech. The newborn showed auditory activation but not motor activation. Another study presented 3 month olds with sentences and recorded their brain activity via fMRI motor speech areas did activate. These studies suggest that the link between perception and action begins to develop at 3 months. When babies are young, they are actually the most sensitive to distinguishing all phonetic units. During an infant\u2019s 1st year of life, they have to differentiate between about 40 phonetic units. When they are older they have usually been exposed to their native language so much that they lose this ability and can only distinguish phonetic units in their native language. Even at 12 months babies exhibit a deficit in differentiated non-native sounds. However, their ability to distinguish sounds in their native language continues to improve and become more fine-tuned. For example, Japanese learning infants learn that there is no differentiation between /r/ and /l/. However, in English, \"rake\" and \"lake\" are two different words. Japanese babies eventually lose their ability to distinguish between /r/ and /l/. Similarly, a Spanish learning infant cannot form words until they learn the difference between works like \"bano\" and \"pano\", because the /p/ sound is different than the /b/ sound. English learning babies do not learn to differentiate between the two.",
            "score": 176.2171504497528
        },
        {
            "docid": "33993614_23",
            "document": "Neurocomputational speech processing . Each neuron (model cell, artificial neuron) within the speech sound map can be activated and subsequently activates a forward motor command towards the motor map, called articulatory velocity and position map. The activated neural representation on the level of that motor map determines the articulation of a speech unit, i.e. controls all articulators (lips, tongue, velum, glottis) during the time interval for producing that speech unit. Forward control also involves subcortical structures like the cerebellum, not modelled in detail here.",
            "score": 141.92024779319763
        },
        {
            "docid": "36560848_4",
            "document": "Temporal dynamics of music and language . The primary auditory cortex is located on the temporal lobe of the cerebral cortex. This region is important in music processing and plays an important role in determining the pitch and volume of a sound. Brain damage to this region often results in a loss of the ability to hear any sounds at all. The frontal cortex has been found to be involved in processing melodies and harmonies of music. For example, when a patient is asked to tap out a beat or try to reproduce a tone, this region is very active on fMRI and PET scans. The cerebellum is the \"mini\" brain at the rear of the skull. Similar to the frontal cortex, brain imaging studies suggest that the cerebellum is involved in processing melodies and determining tempos. The medial prefrontal cortex along with the primary auditory cortex has also been implicated in tonality, or determining pitch and volume.",
            "score": 183.19844448566437
        },
        {
            "docid": "31075772_15",
            "document": "Thought identification . On 31 January 2012 Brian Pasley and colleagues of University of California Berkeley published their paper in PLoS Biology wherein subjects' internal neural processing of auditory information was decoded and reconstructed as sound on computer by gathering and analyzing electrical signals directly from subjects' brains. The research team conducted their studies on the superior temporal gyrus, a region of the brain that is involved in higher order neural processing to make semantic sense from auditory information. The research team used a computer model to analyze various parts of the brain that might be involved in neural firing while processing auditory signals. Using the computational model, scientists were able to identify the brain activity involved in processing auditory information when subjects were presented with recording of individual words. Later, the computer model of auditory information processing was used to reconstruct some of the words back into sound based on the neural processing of the subjects. However the reconstructed sounds were not of good quality and could be recognized only when the audio wave patterns of the reconstructed sound were visually matched with the audio wave patterns of the original sound that was presented to the subjects. However this research marks a direction towards more precise identification of neural activity in cognition.",
            "score": 139.2050859928131
        },
        {
            "docid": "485309_16",
            "document": "Face perception . There are several parts of the brain that play a role in face perception. Rossion, Hanseeuw, and Dricot used BOLD fMRI mapping to identify activation in the brain when subjects viewed both cars and faces. The majority of BOLD fMRI studies use blood oxygen level dependent (BOLD) contrast to determine which areas of the brain are activated by various cognitive functions. They found that the occipital face area, located in the occipital lobe, the fusiform face area, the superior temporal sulcus, the amygdala, and the anterior/inferior cortex of the temporal lobe, all played roles in contrasting the faces from the cars, with the initial face perception beginning in the area and occipital face areas. This entire region links to form a network that acts to distinguish faces. The processing of faces in the brain is known as a \"sum of parts\" perception. However, the individual parts of the face must be processed first in order to put all of the pieces together. In early processing, the occipital face area contributes to face perception by recognizing the eyes, nose, and mouth as individual pieces. Furthermore, Arcurio, Gold, and James used BOLD fMRI mapping to determine the patterns of activation in the brain when parts of the face were presented in combination and when they were presented singly. The occipital face area is activated by the visual perception of single features of the face, for example, the nose and mouth, and preferred combination of two-eyes over other combinations. This research supports that the occipital face area recognizes the parts of the face at the early stages of recognition. On the contrary, the fusiform face area shows no preference for single features, because the fusiform face area is responsible for \"holistic/configural\" information, meaning that it puts all of the processed pieces of the face together in later processing. This theory is supported by the work of Gold et al. who found that regardless of the orientation of a face, subjects were impacted by the configuration of the individual facial features. Subjects were also impacted by the coding of the relationships between those features. This shows that processing is done by a summation of the parts in the later stages of recognition.",
            "score": 118.32997274398804
        },
        {
            "docid": "51547415_5",
            "document": "Interindividual differences in perception . The McGurk effect is an auditory illusion in which people perceive a different syllable when incongruent audiovisual speech is presented: an auditory syllable \"ba\" is presented while the mouth movement is \"ga\". As a result, the listener perceives the syllable \"da\". However, according to Gentilucci and Cattaneo (2005), not everyone perceives this illusion; only about 26% to 98% of the population are susceptible to this illusion. One of the psychological models that explains the interindividual differences in speech perception is the fuzzy logic model of speech perception According to this model, a categorization process is carried out when processing speech sounds. When listening to a stimulus, the features of the acoustic signal are analyzed. Subsequently, this signal is compared with the features that are stored in the memory; finally the sound is classified into the category that best fits. However, this classification may have a blurred boundary respectively to the category which the sound belongs to. As a result, the final decision may depend on integration of multiple sources of information. When the McGurk effect is presented the auditory and visual components of the speech are separately evaluated before being integrated. In those who perceive the McGurk effect, the visual information has a higher influence on the perception of the ambiguous audiovisual information and thus the sound is classified as \"da\".",
            "score": 135.30432188510895
        },
        {
            "docid": "5366050_57",
            "document": "Speech perception . Some of the earliest work in the study of how humans perceive speech sounds was conducted by Alvin Liberman and his colleagues at Haskins Laboratories. Using a speech synthesizer, they constructed speech sounds that varied in place of articulation along a continuum from to to . Listeners were asked to identify which sound they heard and to discriminate between two different sounds. The results of the experiment showed that listeners grouped sounds into discrete categories, even though the sounds they were hearing were varying continuously. Based on these results, they proposed the notion of categorical perception as a mechanism by which humans can identify speech sounds.",
            "score": 129.032301902771
        },
        {
            "docid": "2640086_28",
            "document": "Affective neuroscience . Instead of investigating specific emotions, Kober, et al. 2008 reviewed 162 neuroimaging studies published between 1990-2005 to determine if groups of brain regions show consistent patterns of activation during emotional experience (that is, actively experiencing an emotion first-hand) and during emotion perception (that is, perceiving a given emotion as experienced by another). This meta-analysis used multilevel kernal density analysis (MKDA) to examine fMRI and PET studies, a technique that prevents single studies from dominating the results (particularly if they report multiple nearby peaks) and that enables studies with large sample sizes (those involving more participants) to exert more influence upon the results. MKDA was used to establish a neural reference space that includes the set of regions showing consistent increases across all studies (for further discussion of MDKA see Wager et al. 2007). Next, this neural reference space was partitioned into functional groups of brain regions showing similar activation patterns across studies by first using multivariate techniques to determine co-activation patterns and then using data-reduction techniques to define the functional groupings (resulting in six groups). Consistent with a psychological construction approach to emotion, the authors discuss each functional group in terms more basic psychological operations. The first \u201cCore Limbic\u201d group included the left amygdala, hypothalamus, periaqueductal gray/thalamus regions, and amygdala/ventral striatum/ventral globus pallidus/thalamus regions, which the authors discuss as an integrative emotional center that plays a general role in evaluating affective significance. The second \u201cLateral Paralimbic\u201d group included the ventral anterior insula/frontal operculum/right temporal pole/ posterior orbitofrontal cortex, the anterior insula/ posterior orbitofrontal cortex, the ventral anterior insula/ temporal cortex/ orbitofrontal cortex junction, the midinsula/ dorsal putamen, and the ventral striatum /mid insula/ left hippocampus, which the authors suggest plays a role in motivation, contributing to the general valuation of stimuli and particularly in reward. The third \u201cMedial Prefrontal Cortex\u201d group included the dorsal medial prefrontal cortex, pregenual anterior cingulate cortex, and rostral dorsal anterior cingulate cortex, which the authors discuss as playing a role in both the generation and regulation of emotion. The fourth \u201cCognitive/ Motor Network\u201d group included right frontal operculum, the right interior frontal gyrus, and the pre-supplementray motor area/ left interior frontal gyrus, regions that are not specific to emotion, but instead appear to play a more general role in information processing and cognitive control. The fifth \u201cOccipital/ Visual Association\u201d group included areas V8 and V4 of the primary visual cortex, the medial temporal lobe, and the lateral occipital cortex, and the sixth \u201cMedial Posterior\u201d group included posterior cingulate cortex and area V1 of the primary visual cortex. The authors suggest that these regions play a joint role in visual processing and attention to emotional stimuli.",
            "score": 159.0034567117691
        },
        {
            "docid": "2534964_4",
            "document": "Sensory processing . It has been believed for some time that inputs from different sensory organs are processed in different areas in the brain, relating to systems neuroscience. Using functional neuroimaging, it can be seen that sensory-specific cortices are activated by different inputs. For example, regions in the occipital cortex are tied to vision and those on the superior temporal gyrus are recipients of auditory inputs. There exist studies suggesting deeper multisensory convergences than those at the sensory-specific cortices, which were listed earlier. This convergence of multiple sensory modalities is known as multisensory integration.",
            "score": 113.02372002601624
        }
    ],
    "r": [
        {
            "docid": "525667_10",
            "document": "Human echolocation . In a 2014 study by Thaler and colleagues, the researchers first made recordings of the clicks and their very faint echoes using tiny microphones placed in the ears of the blind echolocators as they stood outside and tried to identify different objects such as a car, a flag pole, and a tree. The researchers then played the recorded sounds back to the echolocators while their brain activity was being measured using functional magnetic resonance imaging. Remarkably, when the echolocation recordings were played back to the blind experts, not only did they perceive the objects based on the echoes, but they also showed activity in those areas of their brain that normally process visual information in sighted people, primarily primary visual cortex or V1. This result is surprising, as visual areas, as their names suggest, are only active during visual tasks. The brain areas that process auditory information were no more activated by sound recordings of outdoor scenes containing echoes than they were by sound recordings of outdoor scenes with the echoes removed. Importantly, when the same experiment was carried out with sighted people who did not echolocate, these individuals could not perceive the objects and there was no echo-related activity anywhere in the brain. This suggests that the cortex of blind echolocators is plastic and reorganizes such that primary visual cortex, rather than any auditory area, becomes involved in the computation of echolocation tasks.",
            "score": 197.7156982421875
        },
        {
            "docid": "40621603_8",
            "document": "Linguistic intelligence . Generation of written language is thought to be closely related to speech generation, relying on Broca's area for early processing and on the inferior frontal gyrus for semantic processing. However, writing differs in two major ways. First, instead of relating the thought to sounds, the brain must relate the thought to symbols or letters, and second, the motor cortex activates a different set of muscles to write, than when speaking.",
            "score": 195.12298583984375
        },
        {
            "docid": "17893852_4",
            "document": "Humor research . Cognitive neuroscience has provided insight into how humor is neurologically realized. Brain imaging techniques such as fMRI and PET scans have been implemented in this subfield of humor research.  There are a few main regions of the human brain associated with humor and laughter. The production of laughter involves two primary brain pathways, one for both involuntary and voluntary laughter (cf. Duchenne and non-Duchenne). Involuntary laughter is usually emotionally driven, and includes key emotional brain areas such as the amygdala, thalamic areas, and the brainstem. Voluntary laughter instead begins in the premotor opercular area (in the temporal lobe) and moves to the motor cortex and pyramidal tract before moving to the brainstem. Wild et al. (2003) propose that the generation of laughter is mostly influenced by neural pathways that go from the premotor and motor cortex to the ventral side of the brainstem, through the cerebral peduncles. It is also suggested that real laughter is not produced from the motor cortex, but that the normal inhibition of cortical frontal areas stops during laughter. When the electrical activity of the brain is measured during and after hearing a joke, a prominent response can be seen approximately 300ms after the punchline, followed by a depolarization about 100ms later. The fact that humor response occurs in two separate waves of activity supports the idea that humor processing occurs in two stages. Functional MRI and PET studies further illuminate which parts of the brain are participating in the experience of humor. A study by Ozawa, et al., (2000) found that hearing sentences which participants rated as humorous resulted in activation in Broca's area and the middle frontal gyrus, in addition to Wernicke's area and the transverse temporal gyri, which were activated in control (non-humorous) conditions as well.",
            "score": 192.6749725341797
        },
        {
            "docid": "31251362_5",
            "document": "Beat deafness . When sound waves reach the ears, the energy they contain is converted into electrical signals, which are sent via the auditory nerves to the brain. Sound processing begins when these electrical signals reach the primary auditory receiving area in the core part of the temporal lobe. Signals then travel to the area surrounding the core, known as the belt area, and are then transmitted to the parabelt area, which is located next to the belt. Simple sounds such as pure tones are able to activate the core area of the brain, but both the belt and parabelt areas are activated by only complex sounds, such as those found in speech and music. The auditory cortex in the left hemisphere of the brain is responsible for processing beat and rhythm in music. The right auditory cortex is primarily used in distinguishing between different harmonics, which are simple pure tones that combine to create complex tones.",
            "score": 190.96160888671875
        },
        {
            "docid": "1095131_20",
            "document": "Kinesthetic learning . The cerebral cortex is the brain tissue covering the top and sides of the brain in most vertebrates. It is involved in storing and processing of sensory inputs and motor outputs. In the human brain, the cerebral cortex is actually a sheet of neural tissue about 1/8th inch thick. The sheet is folded so that it can fit inside the skull. The neural circuits in this area of the brain expand with practice of an activity, just like the synaptic plasticity grows with practice. Clarification of some of the mechanisms of learning by neuro science has been advanced, in part, by the advent of non-invasive imaging technologies, such as positron emission tomography (PET) and functional magnetic resonance imaging (FMRI). These technologies have allowed researchers to observe human learning processes directly. Through these types of technologies, we are now able to see and study what happens in the process of learning. In different tests performed the brain being imaged showed a greater blood flow and activation to that area of the brain being stimulated through different activities such as finger tapping in a specific sequence. It has been revealed that the process at the beginning of learning a new skill happens quickly, and later on slows down to almost a plateau. This process can also be referred to as The Law of Learning. The slower learning showed in the FMRI that in the cerebral cortex this was when the long term learning was occurring, suggesting that the structural changes in the cortex reflect the enhancement of skill memories during later stages of training. When a person studies a skill for a longer duration of time, but in a shorter amount of time they will learn quickly, but also only retain the information into their short-term memory. Just like studying for an exam; if a student tries to learn everything the night before, it will not stick in the long run. If a person studies a skill for a shorter duration of time, but more frequently and long-term, their brain will retain this information much longer as it is stored in the long-term memory. Functional and structural studies of the brain have revealed a vast interconnectivity between diverse regions of the cerebral cortex. For example, large numbers of axons interconnect the posterior sensory areas serving vision, audition, and touch with anterior motor regions. Constant communication between sensation and movement makes sense, because to execute smooth movement through the environment, movement must be continuously integrated with knowledge about one's surroundings obtained via sensory perception. The cerebral cortex plays a role in allowing humans to do this.",
            "score": 190.8647918701172
        },
        {
            "docid": "47921_101",
            "document": "Free will . In alien hand syndrome, the afflicted individual's limb will produce unintentional movements without the will of the person. The affected limb effectively demonstrates 'a will of its own.' The sense of agency does not emerge in conjunction with the overt appearance of the purposeful act even though the sense of ownership in relationship to the body part is maintained. This phenomenon corresponds with an impairment in the premotor mechanism manifested temporally by the appearance of the readiness potential (see section on the Neuroscience of Free Will above) recordable on the scalp several hundred milliseconds before the overt appearance of a spontaneous willed movement. Using functional magnetic resonance imaging with specialized multivariate analyses to study the temporal dimension in the activation of the cortical network associated with voluntary movement in human subjects, an anterior-to-posterior sequential activation process beginning in the supplementary motor area on the medial surface of the frontal lobe and progressing to the primary motor cortex and then to parietal cortex has been observed. The sense of agency thus appears to normally emerge in conjunction with this orderly sequential network activation incorporating premotor association cortices together with primary motor cortex. In particular, the supplementary motor complex on the medial surface of the frontal lobe appears to activate prior to primary motor cortex presumably in associated with a preparatory pre-movement process. In a recent study using functional magnetic resonance imaging, alien movements were characterized by a relatively isolated activation of the primary motor cortex contralateral to the alien hand, while voluntary movements of the same body part included the concomitant activation of motor association cortex associated with the premotor process. The clinical definition requires \"feeling that one limb is foreign or has a \"will of its own,\" together with observable involuntary motor activity\" (emphasis in original). This syndrome is often a result of damage to the corpus callosum, either when it is severed to treat intractable epilepsy or due to a stroke. The standard neurological explanation is that the felt will reported by the speaking left hemisphere does not correspond with the actions performed by the non-speaking right hemisphere, thus suggesting that the two hemispheres may have independent senses of will.",
            "score": 188.5618438720703
        },
        {
            "docid": "40621603_5",
            "document": "Linguistic intelligence . Speech production is process by which a thought in the brain is converted into an understandable auditory form. This is a multistage mechanism that involves many different areas of the brain. The first stage is planning, where the brain constructs words and sentences that turn the thought into an understandable form. This occurs primarily in the inferior frontal cortex, specifically in an area known as Broca's area. Next, the brain must plan how to physically create the sounds necessary for speech by linking the planned speech with known sounds, or phonemes. While the location of these associations is not known, it is known that the supplementary motor area plays a key role in this step. Finally, the brain must signal for the words to actually be spoken. This is carried out by the premotor cortex and the motor cortex. In most cases, speech production is controlled by the left hemisphere. In a series of studies, Wilder Penfield, among others, probed the brains of both right-handed (generally left-hemisphere dominant) and left-handed (generally right-hemisphere dominant) patients. They discovered that, regardless of handedness, the left hemisphere was almost always the speech controlling side. However, it has been discovered that in cases of neural stress (hemorrhage, stroke, etc.) the right hemisphere has the ability to take control of speech functions.",
            "score": 187.92947387695312
        },
        {
            "docid": "7330954_31",
            "document": "Pattern recognition (psychology) . MIT researchers conducted a study to examine this notion. The results showed six neural clusters in the auditory cortex responding to the sounds. Four were triggered when hearing standard acoustic features, one specifically responded to speech, and the last exclusively responded to music. Researchers who studied the correlation between temporal evolution of timbral, tonal and rhythmic features of music, came to the conclusion that music engages the brain regions connected to motor actions, emotions and creativity. The research indicates that the whole brain \"lights up\" when listening to music. This amount of activity boosts memory preservation, hence pattern recognition.",
            "score": 187.8429718017578
        },
        {
            "docid": "25146378_20",
            "document": "Functional specialization (brain) . Other researchers who provide evidence to support the theory of distributive processing include Anthony McIntosh and William Uttal, who question and debate localization and modality specialization within the brain. McIntosh's research suggests that human cognition involves interactions between the brain regions responsible for processes sensory information, such as vision, audition, and other mediating areas like the prefrontal cortex. McIntosh explains that modularity is mainly observed in sensory and motor systems, however, beyond these very receptors, modularity becomes \"fuzzier\" and you see the cross connections between systems increase. He also illustrates that there is an overlapping of functional characteristics between the sensory and motor systems, where these regions are close to one another. These different neural interactions influence each other, where activity changes in one area influence other connected areas. With this, McIntosh suggest that if you only focus on activity in one area, you may miss the changes in other integrative areas. Neural interactions can be measured using analysis of covariance in neuroimaging. McIntosh used this analysis to convey a clear example of the interaction theory of distributive processing. In this study, subjects learned that an auditory stimulus signalled a visual event. McIntosh found activation (an increase blood flow), in an area of the occipital cortex, a region of the brain involved in visual processing, when the auditory stimulus was presented alone. Correlations between the occipital cortex and different areas of the brain such as the prefrontal cortex, premotor cortex and superior temporal cortex showed a pattern of co-variation and functional connectivity.",
            "score": 186.46678161621094
        },
        {
            "docid": "12223156_10",
            "document": "Dance and health . Additionally, a recent study done in Perth Western Australia by Debbie Duignan (WA Alzheimers Association) explored the use of Wu Tao Dance as a therapy for people with dementia. It was shown that Wu Tao dance helped to reduce symptoms of agitation in people with dementia. The complex mental coordination involved with dancing activates both sensory and motor circuits. Therefore, when one dances, one's brain is both stimulated by the sound of the music and by the dance movements themselves. PET imaging has shown brain regions that become activated during dance learning and performance, including the motor cortex, somatosensory cortex, basal ganglia, and cerebellum. The benefits of dancing on the brain includes memory improvement and strengthened neural connections. Consequently, not only can dance help to reduce symptoms experienced by those with dementia, but it can also reduce the risk of developing dementia in the first place, as shown in a 2003 study in the \"New England Journal of Medicine\" by researchers at the Albert Einstein College of Medicine.",
            "score": 184.03262329101562
        },
        {
            "docid": "45155414_12",
            "document": "Embodied bilingual language . Embodied bilingual processing is rooted in motor processing because research shows that the motor cortex activates during language processing. In first language processing, for example, leg-related words like \"kick\" and \"run\" stimulate the part of the motor cortex that controls leg motions. This illustrates that language describing motor actions activates motor systems in the brain, but only when the words provide literal meaning as opposed to figurative meaning. Following L1 embodiment, L2 embodiment assumes that the words \"punch\" and \"throw\" in a second language will also stimulate the same parts of the motor cortex as does first language words. In essence, language that describes motor actions activates motor systems in the brain. If this holds true for all languages, then the processing that occurs when understanding and using a second language must also activate motor regions of the brain, just as native language processing does.",
            "score": 183.36785888671875
        },
        {
            "docid": "36560848_4",
            "document": "Temporal dynamics of music and language . The primary auditory cortex is located on the temporal lobe of the cerebral cortex. This region is important in music processing and plays an important role in determining the pitch and volume of a sound. Brain damage to this region often results in a loss of the ability to hear any sounds at all. The frontal cortex has been found to be involved in processing melodies and harmonies of music. For example, when a patient is asked to tap out a beat or try to reproduce a tone, this region is very active on fMRI and PET scans. The cerebellum is the \"mini\" brain at the rear of the skull. Similar to the frontal cortex, brain imaging studies suggest that the cerebellum is involved in processing melodies and determining tempos. The medial prefrontal cortex along with the primary auditory cortex has also been implicated in tonality, or determining pitch and volume.",
            "score": 183.1984405517578
        },
        {
            "docid": "53686950_13",
            "document": "Bi-directional hypothesis of language and action . A similar experiment has been performed on the articulatory motor cortex, or the mouth and lip regions of the motor cortex used in the production of words. Two categories of words were used as language stimuli: words that involved the lips for production (e.g. \"pool\") or the tongue (e.g. \"tool). Subjects listened to the words, were shown pairs of pictures, and were asked to indicate which picture matched the word they heard with a button press. TMS was used prior to presentation of the language stimuli to selectively facilitate either the lip or tongue regions of the left motor cortex; these two TMS conditions were compared to a control condition where TMS was not applied. It was found that stimulation of the lip region of the motor cortex lead to a significantly decreased response time for lip words as compared to tongue words. In addition, during recognition of tongue words, reduced reaction times were seen with tongue TMS as compared to lip TMS and no TMS. Although this same effect was not seen with lip words, authors attribute this to the complexity of tongue as opposed to lip movements, and the increase difficulty of tongue words as opposed to lip. Overall, this study demonstrates that the activity in the articulatory motor cortex influences the comprehension of single spoken words, and highlights the importance of the motor cortex in speech comprehension",
            "score": 182.00596618652344
        },
        {
            "docid": "490620_10",
            "document": "Human brain . The cortex is mapped by divisions into about fifty different functional areas known as Brodmann's areas. These areas are distinctly different when seen under a microscope. The cortex is divided into two main functional areas \u2013 a motor cortex and a sensory cortex. The primary motor cortex, which sends axons down to motor neurons in the brainstem and spinal cord, occupies the rear portion of the frontal lobe, directly in front of the somatosensory area. The primary sensory areas receive signals from the sensory nerves and tracts by way of relay nuclei in the thalamus. Primary sensory areas include the visual cortex of the occipital lobe, the auditory cortex in parts of the temporal lobe and insular cortex, and the somatosensory cortex in the parietal lobe. The remaining parts of the cortex, are called the association areas. These areas receive input from the sensory areas and lower parts of the brain and are involved in the complex cognitive processes of perception, thought, and decision-making. The main functions of the frontal lobe are to control attention, abstract thinking, behaviour, problem solving tasks, and physical reactions and personality. The occipital lobe is the smallest lobe; its main functions are visual reception, visual-spatial processing, movement, and colour recognition. There is a smaller occipital lobule in the lobe known as the cuneus. The temporal lobe controls auditory and visual memories, language, and some hearing and speech.",
            "score": 180.82290649414062
        },
        {
            "docid": "1274232_8",
            "document": "Cognitive development . Cognitive development and motor development may also be closely interrelated. When a person experience a neurodevelopmental disorder and their cognitive development is disturbed, we often see adverse effects in motor development as well. Cerebellum, which is the part of brain that is most responsible for motor skills, has been shown to have significant importance in cognitive functions in the same way that prefrontal cortex has important duties in not only cognitive abilities but also development of motor skills. To support this, there is evidence of close co-activation of neocerebellum and dorsolateral prefrontal cortex in functional neuroimaging as well as abnormalities seen in both cerebellum and prefrontal cortex in the same developmental disorder. In this way, we see close interrelation of motor development and cognitive development and they cannot operate in their full capacity when either of them are impaired or delayed.",
            "score": 180.80364990234375
        },
        {
            "docid": "23631964_20",
            "document": "Primary motor cortex . A second modification of the classical somatotopic ordering of body parts is a double representation of the digits and wrist studied mainly in the human motor cortex. One representation lies in a posterior region called area 4p, and the other lies in an anterior region called area 4a. The posterior area can be activated by attention without any sensory feedback and has been suggested to be important for initiation of movements, while the anterior area is dependent on sensory feedback. It can also be activated by imaginary finger movements and listening to speech while making no actual movements. This anterior representation area has been suggested to be important in executing movements involving complex sensoriomotor interactions. It is possible that area 4a in humans corresponds to some parts of the caudal premotor cortex as described in the monkey cortex.",
            "score": 180.05062866210938
        },
        {
            "docid": "30053816_10",
            "document": "L\u00fcder Deecke . In 2002 the term \"Bereitschafts-BOLD response\" was coined by Ross Cunnington et al. in event-related fMRI studies at the Department of Clinical Neurology and the Department of Radiodiagnostics Medical University of Vienna. Thus, according to Deecke und Kornhuber [7],[15],[16] the early component of the BP (BP1 or BPearly) is generated by the following areas: the SMA proper, the pre-SMA and the cingulate motor area, CMA. This is now called anterior mid-cingulate cortex, aMCC. The second component (BP2 or BP late) is generated by the motor cortex (M1). Contrary to earlier views, the intentional activity according to Kornhuber and Deecke does not travel directly from the SMA to motor cortex M1 but is running via the cortico-basalganglio-thalamo-cortical loop in short motor loop. The motor loop has been discovered in patients with Parkinson's Disease (PD).It could be shown, that deep brain stimulation improves frontal cortex function in PD patients. This means that the formation of the will has already taken place in the frontal lobe and the preparation and planning of the action has been transferred initially to the unconscious routine processes of the basal ganglia, which do the groundwork for the motor cortex, M1. M1 finally generates the volley for the pyramidal tract, which then enters consciousness. During the early BP, BP1, the action planning is not yet conscious, but during BP2 it is. From this observation Benjamin Libet, postulated that we do not have free will (BP1) but with the control of the action (BP2) we do have free will. However, Kornhuber and Deecke, have shown that consciousness is not a sine qua non for free will. There are conscious and unconscious agendas in the brain, and both are important. The unconscious agendas far outweigh the conscious agendas, consciousness being only the \u2018tip of an iceberg\u2019. Therefore, free will is involved with both, the initiation of the action and for the control of the action.",
            "score": 179.66293334960938
        },
        {
            "docid": "485578_12",
            "document": "Exhalation . The neurological pathway of voluntary exhalation is complex and not fully understood. However, a few basics are known. The motor cortex within the cerebral cortex of the brain is known to control voluntary respiration because the motor cortex controls voluntary muscle movement. This is referred to as the corticospinal pathway or ascending respiratory pathway. The pathway of the electrical signal starts in the motor cortex, goes to the spinal cord, and then to the respiratory muscles. The spinal neurons connect directly to the respiratory muscles. Initiation of voluntary contraction and relaxation of the internal and external internal costals has been shown to take place in the superior portion of the primary motor cortex. Posterior to the location of thoracic control (within the superior portion of the primary motor cortex) is the center for diaphragm control. Studies indicate that there are numerous other sites within the brain that may be associated with voluntary expiration. The inferior portion of the primary motor cortex may be involved, specifically, in controlled exhalation. Activity has also been seen within the supplementary motor area and the premotor cortex during voluntary respiration. This is most likely due to the focus and mental preparation of the voluntary muscular movement.",
            "score": 179.30926513671875
        },
        {
            "docid": "4643899_5",
            "document": "Categorical perception . According to the (now abandoned) motor theory of speech perception, the reason people perceive an abrupt change between /ba/ and /pa/ is that the way we hear speech sounds is influenced by how people produce them when they speak. What is varying along this continuum is voice-onset-time: the \"b\" in /ba/ is voiced and the \"p\" in /pa/ is not. But unlike the synthetic \"morphing\" apparatus, people's natural vocal apparatus is not capable of producing anything in between ba and pa. So when one hears a sound from the voicing continuum, their brain perceives it by trying to match it with what it would have had to do to produce it. Since the only thing they can produce is /ba/ or /pa/, they will perceive any of the synthetic stimuli along the continuum as either /ba/ or /pa/, whichever it is closer to. A similar CP effect is found with ba/da; these too lie along a continuum acoustically, but vocally, /ba/ is formed with the two lips, /da/ with the tip of the tongue and the alveolar ridge, and our anatomy does not allow any intermediates.",
            "score": 178.88540649414062
        },
        {
            "docid": "23158496_4",
            "document": "Superior temporal sulcus . In individuals without autism, the superior temporal sulcus also activates when hearing human voices. It is thought to be a source of sensory encoding linked to motor output through the superior parietal-temporal areas of the brain inferred from the time course of activation. The conclusion of pertinence to vocal processing can be drawn from data showing that the regions of the STS (superior temporal sulcus) are more active when people are listening to vocal sounds rather than non-vocal environmentally based sounds and corresponding control sounds, which can be scrambled or modulated voices. These experimental results indicate the involvement of the STS in the areas of speech and language recognition.",
            "score": 178.8022918701172
        },
        {
            "docid": "620396_41",
            "document": "Origin of language . Proponents of the motor theory of language evolution have primarily focused on the visual domain and communication through observation of movements. The \"Tool-use sound hypothesis\" suggests that the production and perception of sound, also contributed substantially, particularly \"incidental sound of locomotion\" (\"ISOL\") and \"tool-use sound\" (\"TUS\"). Human bipedalism resulted in rhythmic and more predictable \"ISOL\". That may have stimulated the evolution of musical abilities, auditory working memory, and abilities to produce complex vocalizations, and to mimic natural sounds. Since the human brain proficiently extracts information about objects and events from the sounds they produce, \"TUS\", and mimicry of \"TUS\", might have achieved an iconic function. The prevalence of sound symbolism in many extant languages supports this idea. Self-produced TUS activates multimodal brain processing (motor neurons, hearing, proprioception, touch, vision), and \"TUS\" stimulates primate audiovisual mirror neurons, which is likely to stimulate the development of association chains. Tool use and auditory gestures involve motor-processing of the forelimbs, which is associated with the evolution of vertebrate vocal communication. The production, perception, and mimicry of \"TUS\" may have resulted in a limited number of vocalizations or protowords that were associated with tool use. A new way to communicate about tools, especially when out of sight, would have had selective advantage. A gradual change in acoustic properties and/or meaning could have resulted in arbitrariness and an expanded repertoire of words. Humans have been increasingly exposed to \"TUS\" over millions of years, coinciding with the period during which spoken language evolved.",
            "score": 177.1547088623047
        },
        {
            "docid": "35075711_26",
            "document": "Spontaneous recovery . The pathway of recall associated with the retrieval of sound memories is the auditory system. Within the auditory system is the auditory cortex, which can be broken down into the primary auditory cortex and the belt areas. The primary auditory cortex is the main region of the brain that processes sound and is located on the superior temporal gyrus in the temporal lobe where it receives point-to-point input from the medial geniculate nucleus. From this, the primary auditory complex had a topographic map of the cochlea. The belt areas of the auditory complex receive more diffuse input from peripheral areas of the medial geniculate nucleus and therefore are less precise in tonotopic organization compared to the primary visual cortex. A 2001 study by Trama examined how different kinds of brain damage interfere with normal perception of music. One of his studied patients lost most of his auditory cortex to strokes, allowing him to still hear but making it difficult to understand music since he could not recognize harmonic patterns. Detecting a similarity between speech perception and sound perception, spontaneous recovery of lost auditory information is possible in those patients who have experienced a stroke or other major head trauma. Amusia is a disorder manifesting itself as a defect in processing pitch but also affects one's memory and recognition for music.",
            "score": 176.27908325195312
        },
        {
            "docid": "10042066_2",
            "document": "Developmental linguistics . Developmental linguistics is the study of the development of linguistic ability in an individual, particularly the acquisition of language in childhood. It involves research into the different stages in language acquisition, language retention, and language loss in both first and second languages, in addition to the area of bilingualism. Before infants can speak, the neural circuits in their brains are constantly being influenced by exposure to language. The neurobiology of language contains a \"critical period\" in which children are most sensitive to language. The different aspects of language have varying \"critical periods\". Studies show that the critical period for phonetics is toward the end of the first year. At 18 months, a toddler's vocabulary vastly expands. The critical period for syntactic learning is 18-36 months. Infants of different mother languages can be differentiated at the age of 10 months. At 20 weeks they begin vocal imitation. Beginning when babies are about 12 months, they take on computational learning and social learning. Social interactions for infants and toddlers is important because it helps associate \"perception and action\". In-person social interaction rather than audio or video better facilitates learning in babies because they learn from how other people respond to them, especially their mothers. Babies have to learn to mimic certain syllables, which takes practice in manipulating tongue and lip movement. Sensory-motor learning in speech is linked to exposure to speech, which is very sensitive to language. Infants exposed to Spanish exhibit a different vocalization than infants exposed to English. One study took infants that were learning English and made them listen to Spanish in 12 sessions. The result showed consequent alterations in their vocalization, which demonstrated Spanish prosody.  One study used MEG to record activation in the brains of newborns, 6 months olds and 12 months olds while presenting them with syllables, harmonics and non-speech sounds. For the 6 month and 12 month old, the auditory and motor areas responded to speech. The newborn showed auditory activation but not motor activation. Another study presented 3 month olds with sentences and recorded their brain activity via fMRI motor speech areas did activate. These studies suggest that the link between perception and action begins to develop at 3 months. When babies are young, they are actually the most sensitive to distinguishing all phonetic units. During an infant\u2019s 1st year of life, they have to differentiate between about 40 phonetic units. When they are older they have usually been exposed to their native language so much that they lose this ability and can only distinguish phonetic units in their native language. Even at 12 months babies exhibit a deficit in differentiated non-native sounds. However, their ability to distinguish sounds in their native language continues to improve and become more fine-tuned. For example, Japanese learning infants learn that there is no differentiation between /r/ and /l/. However, in English, \"rake\" and \"lake\" are two different words. Japanese babies eventually lose their ability to distinguish between /r/ and /l/. Similarly, a Spanish learning infant cannot form words until they learn the difference between works like \"bano\" and \"pano\", because the /p/ sound is different than the /b/ sound. English learning babies do not learn to differentiate between the two.",
            "score": 176.21714782714844
        },
        {
            "docid": "2363287_6",
            "document": "Visual learning . Various areas of the brain work together in a multitude of ways in order to produce the images that we see with our eyes and that are encoded by our brains. The basis of this work takes place in the visual cortex of the brain. The visual cortex is located in the occipital lobe of the brain and harbors many other structures that aid in visual recognition, categorization, and learning. One of the first things the brain must do when acquiring new visual information is recognize the incoming material. Brain areas involved in recognition are the inferior temporal cortex, the superior parietal cortex, and the cerebellum. During tasks of recognition, there is increased activation in the left inferior temporal cortex and decreased activation in the right superior parietal cortex. Recognition is aided by neural plasticity, or the brain's ability to reshape itself based on new information. Next the brain must categorize the material. The three main areas that are used when categorizing new visual information are the orbitofrontal cortex and two dorsolateral prefrontal regions which begin the process of sorting new information into groups and further assimilating that information into things that you might already know. After recognizing and categorizing new material entered into the visual field, the brain is ready to begin the encoding process \u2013 the process which leads to learning. Multiple brain areas are involved in this process such as the frontal lobe, the right extrastriate cortex, the neocortex, and again, the neostriatum. One area in particular, the limbic-diencephalic region, is essential for transforming perceptions into memories. With the coming together of tasks of recognition, categorization and learning; schemas help make the process of encoding new information and relating it to things you already know much easier. One can remember visual images much better when they can apply it to an already known schema. Schemas actually provide enhancement of visual memory and learning.",
            "score": 176.05516052246094
        },
        {
            "docid": "23060403_12",
            "document": "Motor theory of speech perception . Using a speech synthesizer, speech sounds can be varied in place of articulation along a continuum from to to , or in voice onset time on a continuum from to (for example). When listeners are asked to discriminate between two different sounds, they perceive sounds as belonging to discrete categories, even though the sounds vary continuously. In other words, 10 sounds (with the sound on one extreme being and the sound on the other extreme being , and the ones in the middle varying on a scale) may all be acoustically different from one another, but the listener will hear all of them as either or . Likewise, the English consonant may vary in its acoustic details across different phonetic contexts (the /d/ in does not technically sound the same as the one in , for example), but all 's as perceived by a listener fall within one category (voiced alveolar plosive) and that is because \"linguistic representations are abstract, canonical, phonetic segments or the gestures that underlie these segments.\" This suggests that humans identify speech using categorical perception, and thus that a specialized module, such as that proposed by the motor theory of speech perception, may be on the right track.",
            "score": 176.0322723388672
        },
        {
            "docid": "27179535_7",
            "document": "Leah Krubitzer . The parietal cortex is another area of interest for Krubitzer. The parietal cortex allows us to coordinate movements between our eyes and our hands. This ability allows for smooth reaching movements, as well as, grasping. Past research has been done on Old and New World monkeys, as well as humans, to see how the parietal cortex functions in hand use. Imaging used on humans shows that there are similar cortical patterns shared across human and non-human primates, but the extent to which these pathways are used depends on the somatosensory organization and connectivity in the parietal cortex. Krubitzer and her team took this information and investigated a little deeper. Because humans have an opposable thumb, our ability to grip objects and reach for objects is much greater than monkeys. For this reason, the connectivity in the human parietal cortex is much more complex than that of a non-human primate. In Krubitzer's lab, her team investigated different areas of the parietal cortex in order to better pin point which part controls which motor movement. Krubitzer found that when one area of the cortex responsible for a certain motor movement is compromised, the rest of the cortex will reorganize itself to make up for the loss. This finding shows how the parietal cortex can rewire itself in order to maintain functional motor capabilities. Currently in the lab, Krubitzer and colleagues are testing a microchip that may be placed in the posterior parietal cortex of the brain to deactivate certain areas at a time. Using this technique, they are able to see how deactivation of a certain portion of the cortex impacts hand grasping and reaching in monkeys. This technique is performed while the monkeys are performing different manual tasks in order to see the action of the cortex live.",
            "score": 175.842041015625
        },
        {
            "docid": "32018467_7",
            "document": "Christian Keysers . After finishing his master, Christian Keysers decided to concentrate on a subfield of cognitive neuroscience called social neuroscience that uses neuroscience methods to understand how we process the social world. He therefore performed his doctoral studies at the University of St Andrews with David Ian Perrett, one of the founding father of the field, to understand how the brain processes faces and facial expressions. This thesis work led to new insights into how quickly the brain can process the faces of others. During this period, Keysers became fascinated with the question of how the brain can attach meaning to the faces of others. How is it for instance, that we understand that a certain grimace would signal that another person is happy? How do we understand that a certain bodily movement towards a glass indicates that the other person aims to grasp a glass? In 1999, Keysers was exposed to a visit of Vittorio Gallese, who presented his recent discovery of mirror neurons in the Psychology department lecture series. This deeply influenced Keysers who decided to move to the lab of Giacomo Rizzolatti to undertake further studies on how these fascinating neurons could contribute to social perception. In 2000, after finishing his doctorate, Christian Keysers moved to the University of Parma to study mirror neurons. In early work there demonstrated that mirror neurons in the premotor cortex not only respond to the sight of actions, but also when actions can only be deduced or heard, leading to a publication in the journal \"Science\". This work had tremendous impact on the field, as it suggested that the premotor cortex could play a central, modality independent role in perception and may lay the origin for the evolution of speech in humans.  Together this work indicated that brain regions involved in our own actions play a role in how we process the actions of others. Keysers wondered whether a similar principle may underlie how we process the tactile sensations and emotions of others, and became increasingly independent of the research focus on the motor system in Parma. At the time, Keysers had also met his to be wife, Valeria Gazzola, a biologist in the final phases of her studies, and together they decided to explore if the somatosensory system might be involved in perceiving the sensations of others. Via a fruitful collaboration with the French neuroimaging specialist Bruno Wicker, they used functional magnetic resonance imaging, and showed for the first time, that the secondary somatosensory cortex, previously thought only to represent a persons own experiences of touch, is also activated when seeing someone or something else be touched. They also showed that the insula, thought only to respond to the experience of first-hand emotions, was also activated when we see another individual experience similar emotions. Together this indicated a much more general principle than the original mirror neuron theory, in which people process the actions, sensations and emotions of others by vicariously activating owns own actions, sensations and emotions. Jointly, this work laid the foundation of the neuroscientific investigation of empathy.",
            "score": 174.37522888183594
        },
        {
            "docid": "263801_13",
            "document": "Alien hand syndrome . A 2007 fMRI study examining the difference in functional brain activation patterns associated with alien as compared to non-alien \"volitional\" movement in a patient with alien hand syndrome found that alien movement involved anomalous \"isolated\" activation of the primary motor cortex in the damaged hemisphere contralateral to the alien hand, while non-alien movement involved the normal process of activation described in the preceding paragraph in which primary motor cortex in the intact hemisphere activates in concert with frontal premotor cortex and posterior parietal cortex presumably involved in a normal cortical network generating premotor influences on the primary motor cortex along with immediate post-motor re-afferent activation of the posterior parietal cortex.",
            "score": 173.74293518066406
        },
        {
            "docid": "1605494_10",
            "document": "Auditory imagery . As associations between pieces of sound such as music or repetitive dialogue become stronger and more complex even the silence involved in the sound can initiate auditory images in the brain. Studies have been done in which people listen to a CD over and over with silence in between tracks and the neural activity was analyzed using fMRI. It was consistently found the prefrontal cortex and premotor cortical areas were active during the anticipation of auditory imagery. The caudal PFC was used a lot during the early stages of learning of the song while in later stages the rostral PFC was used more indicating a shift in the cortex regions used during auditory imaging association.",
            "score": 172.82437133789062
        },
        {
            "docid": "27034699_3",
            "document": "Supernumerary phantom limb . An fMRI study of a subject with a supernumerary phantom left arm was done by Khateb \"et al.\" at the Laboratory of Experimental Neuropsychology at the University of Geneva. When the subject was told to touch her right cheek with the phantom limb, there was increased activity in the motor cortex of her brain in the area roughly corresponding to the left arm. When she announced that she had touched the phantom limb to her cheek, activity was monitored in the area of the somatosensory cortex that corresponded to the right cheek. At times during the experiment, the subject was asked to move the phantom limb to a location that was obstructed or otherwise unfeasible. In these instances, there was similar activation of the motor cortex but no such activity in the somatosensory cortex.",
            "score": 172.81414794921875
        },
        {
            "docid": "27942343_6",
            "document": "Psychological effects of Internet use . Specialised MRI brain scans showed changes in the white matter of the brain\u2014the part that contains nerve fibres\u2014in those classed as being web addicts, compared with non-addicts. Furthermore, the study says, \"We provided evidences demonstrating the multiple structural changes of the brain in IAD subjects. VBM results indicated the decreased gray matter volume in the bilateral dorsolateral prefrontal cortex (DLPFC), the supplementary motor area (SMA), the orbitofrontal cortex (OFC), the cerebellum and the left rostral ACC (rACC).\" UCLA professor of psychiatry Gary Small studied brain activity in experienced web surfers versus casual web surfers. He used MRI scans on both groups to evaluate brain activity. The study showed that when Internet surfing, the brain activity of the experienced Internet users was far more extensive than that of the novices, particularly in areas of the prefrontal cortex associated with problem-solving and decision making. However, the two groups had no significant differences in brain activity when reading blocks of text. This evidence suggested that the distinctive neural pathways of experienced Web users had developed because of their Web use. Dr. Small concluded that \u201cThe current explosion of digital technology not only is changing the way we live and communicate, but is rapidly and profoundly altering our brains.\u201d",
            "score": 172.18356323242188
        },
        {
            "docid": "1732213_15",
            "document": "Language processing in the brain . Broca's area is involved mostly in the production of speech. Given its proximity to the motor cortex, neurons from Broca's area send signals to the larynx, tongue and mouth motor areas, which in turn send the signals to the corresponding muscles, thus allowing the creation of sounds.",
            "score": 171.31333923339844
        }
    ]
}