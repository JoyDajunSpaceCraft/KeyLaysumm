{
    "q": [
        {
            "docid": "1316947_4",
            "document": "Ambiguous image . When we see an image, the first thing we do is attempt to organize all the parts of the scene into different groups. To do this, one of the most basic methods used is finding the edges. Edges can include obvious perceptions such as the edge of a house, and can include other perceptions that the brain needs to process deeper, such as the edges of a person's facial features. When finding edges, the brain's visual system detects a point on the image with a sharp contrast of lighting. Being able to detect the location of the edge of an object aids in recognizing the object. In ambiguous images, detecting edges still seems natural to the person perceiving the image. However, the brain undergoes deeper processing to resolve the ambiguity. For example, consider an image that involves an opposite change in magnitude of luminance between the object and the background (e.g. From the top, the background shifts from black to white, and the object shifts from white to black). The opposing gradients will eventually come to a point where there is an equal degree of luminance of the object and the background. At this point, there is no edge to be perceived. To counter this, the visual system connects the image as a whole rather than a set of edges, allowing one to see an object rather than edges and non-edges. Although there is no complete image to be seen, the brain is able to accomplish this because of its understanding of the physical world and real incidents of ambiguous lighting. In ambiguous images, an illusion is often produced from illusory contours. An illusory contour is a perceived contour without the presence of a physical gradient. In examples where a white shape appears to occlude black objects on a white background, the white shape appears to be brighter than the background, and the edges of this shape produce the illusory contours. These illusory contours are processed by the brain in a similar way as real contours. The visual system accomplishes this by making inferences beyond the information that is presented in much the same way as the luminance gradient.",
            "score": 173.29860472679138
        },
        {
            "docid": "1316947_18",
            "document": "Ambiguous image . To go further than just perceiving the object is to recognize the object. Recognizing an object plays a crucial role in resolving ambiguous images, and relies heavily on memory and prior knowledge. To recognize an object, the visual system detects familiar components of it, and compares the perceptual representation of it with a representation of the object stored in memory. This can be done using various templates of an object, such as \"dog\" to represent dogs in general. The template method is not always successful because members of a group may significantly differ visually from each other, and may look much different if viewed from different angles. To counter the problem of viewpoint, the visual system detects familiar components of an object in 3-dimensional space. If the components of an object perceived are in the same position and orientation of an object in memory, recognition is possible. Research has shown that people that are more creative in their imagery are better able to resolve ambiguous images. This may be due to their ability to quickly identify patterns in the image. When making a mental representation of an ambiguous image, in the same way as normal images, each part is defined and then put onto the mental representation. The more complex the scene is, the longer it takes to process and add to the representation. Figures drawn in a way that avoids depth cues may become ambiguous. Classic examples of this phenomenon are the Necker cube, and the rhombille tiling (viewed as an isometric drawing of cubes).",
            "score": 163.9367117881775
        },
        {
            "docid": "32197396_2",
            "document": "Form perception . Form perception is the recognition of visual elements of objects, specifically those to do with shapes, patterns and previously identified important characteristics. An object is perceived by the retina as a two-dimensional image, but the image can vary for the same object in terms of the context with which it is viewed, the apparent size of the object, the angle from which it is viewed, how illuminated it is, as well as where it resides in the field of vision.  Despite the fact that each instance of observing an object leads to a unique retinal response pattern, the visual processing in the brain is capable of recognizing these experiences as analogous, allowing invariant object recognition. Visual processing occurs in a hierarchy with the lowest levels recognizing lines and contours, and slightly higher levels performing tasks such as completing boundaries and recognizing contour combinations. The highest levels integrate the perceived information to recognize an entire object. Essentially object recognition is the ability to assign labels to objects in order to categorize and identify them, thus distinguishing one object from another. During visual processing information is not created, but rather reformatted in a way that draws out the most detailed information of the stimulus.",
            "score": 138.93796277046204
        },
        {
            "docid": "53497_13",
            "document": "Optical illusion . In the Ponzo illusion the converging parallel lines tell the brain that the image higher in the visual field is farther away therefore the brain perceives the image to be larger, although the two images hitting the retina are the same size. The optical illusion seen in a diorama/false perspective also exploits assumptions based on monocular cues of depth perception. The M.C. Escher painting \"Waterfall\" exploits rules of depth and proximity and our understanding of the physical world to create an illusion. Like depth perception, motion perception is responsible for a number of sensory illusions. Film animation is based on the illusion that the brain perceives a series of slightly varied images produced in rapid succession as a moving picture. Likewise, when we are moving, as we would be while riding in a vehicle, stable surrounding objects may appear to move. We may also perceive a large object, like an airplane, to move more slowly than smaller objects, like a car, although the larger object is actually moving faster. The phi phenomenon is yet another example of how the brain perceives motion, which is most often created by blinking lights in close succession.",
            "score": 121.96958267688751
        },
        {
            "docid": "35982062_6",
            "document": "Biased Competition Theory . There are two major neural pathways that process the information in the visual field; the ventral stream and the dorsal stream. The two pathways run in parallel and are both working simultaneously. The ventral stream is important for object recognition and often referred to as the \u201cwhat\u201d system of the brain; it projects to the inferior temporal cortex. The dorsal stream is important for spatial perception and performance and is referred to as the \u201cwhere\u201d system which projects to the posterior parietal cortex. According to the biased competition theory, an individual\u2019s visual system has limited capacity to process information about multiple objects at any given time. For example, if an individual was presented with two stimuli (objects) and was asked to identify attributes of each object at the same time, the individual\u2019s performance would be worse in comparison to if the objects were presented separately. This suggests multiple objects presented simultaneously in the visual field will compete for neural representation due to limited processing resources. Single cell recording studies conducted by Kastner and Ungerleider examined the neural mechanisms behind the biased competition theory. In their experiment the size of the receptive field's (RF) of neurons within the visual cortex were examined. A single visual stimulus was presented alone in a neuron\u2019s RF, followed with another stimulus presented simultaneously within the same RF. The single \u2018effective\u2019 stimuli produced a low firing rate, whereas the two stimuli presented together produced a high firing rate. The response to the paired stimuli was reduced. This suggests that when two stimuli are presented together within a neuron\u2019s RF, the stimuli are processed in a mutually suppressive manner, rather than being processed independently. This suppression process, according to Kastner and Ungerleider, occurs when two stimuli are presented together because they compete for neural representation, due to limited cognitive processing capacity. The RF experiment suggests that as the number of objects increase, the information available for each object will decrease due to increased neural workload (suppression), and decreased cognitive capacity. In order for an object in the visual field or RF be efficiently processed, there needs to be a way to bias these neurological resources towards the object. Attention prioritizes task relevant objects, biasing this process. For example, this bias can be towards an object which is currently attended to in the visual field or RF, or towards the object that is most relevant to one\u2019s behavior. Functional magnetic resonance imaging (fMRI) has shown that biased competition theory can explain the observed attention effects at a neuronal level. Attention effects bias the internal weight (strengthens connections) of task relevant features toward the attended object. This was shown by Reddy, Kanwisher, and van Rullen who found an increase in oxygenated blood to a specific neuron following a locational cue. Further neurological support comes from neurophysiological studies which have shown that attention results from Top-down biasing, which in turn influences neuronal spiking. In sum, external inputs affect the Top-down guidance of attention, which bias specific neurons in the brain.",
            "score": 169.31648588180542
        },
        {
            "docid": "3883287_8",
            "document": "Tranquillity . Within tranquillity studies, much of the emphasis has been placed on understanding the role of vision in the perception of natural environments, which is probably not surprising, considering that upon first viewing a scene its configurational coherence can be established with incredible speed. Indeed, scene information can be captured in a single glance and the gist of a scene determined in as little as 100ms. The speed of processing of complex natural images was tested by Thorpe \"et al.\" using colour photographs of a wide range of animals (mammals, birds, reptiles and fish), in their natural environments, mixed with distracters that included pictures of forests, mountains, lakes, buildings and fruit. During this experiment, subjects were shown an image for 20ms and asked to determine whether it contained an animal or not. The electrophysiological brain responses obtained in this study showed that a decision could be made within 150ms of the image being seen, indicating the speed at which cognitive visual processing occurs. However, audition, and in particular the individual components that collectively comprise the soundscape, a term coined by Schafer to describe the ever-present array of sounds that constitute the sonic environment, also significantly inform the various schemata used to characterise differing landscape types. This interpretation is supported by the auditory reaction times, which are 50 to 60ms faster than that of the visual modality. It is also known that sound can alter visual perception and that under certain conditions areas of the brain involved in processing auditory information can be activated in response to visual stimuli.  Research conducted by Pheasant \"et al.\" has shown that when individuals make tranquillity assessments based on a uni-modal auditory or visual sensory input, they characterise the environment by drawing upon a number of key landscape and soundscape characteristics. For example, when making assessments in response to visual-only stimuli the percentage of water, flora and geological features present within a scene, positively influence how tranquil a location is perceived to be. Likewise when responding to uni-modal auditory stimuli, the perceived loudness of biological sounds positively influences the perception of tranquillity, whilst the perceived loudness of mechanical sounds have a negative effect. However, when presented with bi-modal auditory-visual stimuli the individual soundscape and landscape components alone no longer influenced the perception of tranquillity. Rather configurational coherence was provided by the percentage of natural and contextual features present within the scene and the equivalent continuous sound pressure level (LAeq).",
            "score": 133.5650554895401
        },
        {
            "docid": "613052_11",
            "document": "Direct and indirect realism . Another potential counter-example involves vivid hallucinations: phantom elephants, for instance, might be interpreted as sense-data. A direct realist response would differentiate hallucination from genuine perception: no perception of elephants is going on, only the different and related mental process of hallucination. However, if there are visual images when we hallucinate it seems reasonable that there are visual images when we see. Similarly if dreaming involves visual and auditory images in our minds it seems reasonable to think there are visual and auditory images, or sense-data, when we are awake and perceiving things. This argument has been challenged in a number of different ways. First it has been questioned whether there must be some object present that actually has the experienced qualities, which would then seemingly have to be something like a sense-datum. Why couldn't it be that the perceiver is simply in a state of seeming to experience such an object without any object actually being present? Second, in cases of illusion and perceptual relativity there is an object present which is simply misperceived, usually in readily explainable ways, and no need to suppose that an additional object is also involved. Third, the last part of the perceptual relativity version of the argument has been challenged by questioning whether there is really no experiential difference between veridical and non-veridical perception; and by arguing that even if sense-data are experienced in non-veridical cases and even if the difference between veridical and non-veridical cases is, as claimed, experientially indiscernible, there is still no reason to think that sense-data are the immediate objects of experience in veridical cases. Fourth, do sense-data exist through time or are they momentary? Can they exist when not being perceived? Are they public or private? Can they be themselves misperceived? Do they exist in minds or are they extra-mental, even if not physical? On the basis of the intractability of these questions, it has been argued that the conclusion of the argument from illusion is unacceptable or even unintelligible, even in the absence of a clear diagnosis of exactly where and how it goes wrong.",
            "score": 138.10339295864105
        },
        {
            "docid": "35982062_8",
            "document": "Biased Competition Theory . Bottom-up processes are characterized by an absence of higher level direction in sensory processing. It primarily relies on sensory information and incoming sensory information is the starting point for all Bottom-up processing. Bottom-up refers to when a feature stands out in a visual search. This is commonly called the \u201cpop-out\u201d effect. Salient features like bright colors, movement and big objects make the object \u201cpop-out\u201d of the visual search. \u201cPop-out\u201d features can often attract attention without conscious processing. Objects that stand out are often given priority (bias) in processing. Bottom-up processing is data driven, and according to this stimuli are perceived on the basis of the data which is being experienced through the senses. Evidence suggests that simultaneously presented stimuli do in fact compete in order to be represented in the visual cortex, with stimuli mutually suppressing each other to gain this representation. This was examined by Reynolds and colleagues, who looked at the size of neurons\u2019 receptive field\u2019s within the visual cortex. It was found that the presentation of a single stimulus resulted in a low firing rate while two stimuli presented together resulted in a higher firing rate. Reynolds and colleagues also found that when comparing the neural response of an individually presented visual stimulus to responses gathered from simultaneously presented stimuli, the responses of the concurrent presented stimuli were less than the sum of the responses gathered when each stimuli was presented alone. This suggests that two stimuli presented together increase neural work load required for attention. This increased neural load creates suppressive processes and causes the stimuli to compete for neural representation in the brain. Proulx and Egeth predicted that brighter objects would bias attention in favor of that object. Another prediction is that larger objects would bias the attention in favor of that object. The experiment was a computer-based visual search task, where participants searched for a target among distractions. The results of the study suggested that when irrelevant stimuli were large or bright, attention was biased towards the irrelevant objects, prioritizing them for cognitive processing. This research shows the effects of Bottom-up (stimulus-driven) processing on biased competition theory.",
            "score": 161.12435257434845
        },
        {
            "docid": "599917_31",
            "document": "Mental image . As cognitive neuroscience approaches to mental imagery continued, research expanded beyond questions of serial versus parallel or topographic processing to questions of the relationship between mental images and perceptual representations. Both brain imaging (fMRI and ERP) and studies of neuropsychological patients have been used to test the hypothesis that a mental image is the reactivation, from memory, of brain representations normally activated during the perception of an external stimulus. In other words, if perceiving an apple activates contour and location and shape and color representations in the brain\u2019s visual system, then imagining an apple activates some or all of these same representations using information stored in memory. Early evidence for this idea came from neuropsychology. Patients with brain damage that impairs perception in specific ways, for example by damaging shape or color representations, seem to generally to have impaired mental imagery in similar ways. Studies of brain function in normal human brains support this same conclusion, showing activity in the brain\u2019s visual areas while subjects imagined visual objects and scenes.",
            "score": 136.62296152114868
        },
        {
            "docid": "25140_21",
            "document": "Perception . The principle of \"proximity\" states that, all else being equal, perception tends to group stimuli that are close together as part of the same object, and stimuli that are far apart as two separate objects. The principle of \"similarity\" states that, all else being equal, perception lends itself to seeing stimuli that physically resemble each other as part of the same object, and stimuli that are different as part of a different object. This allows for people to distinguish between adjacent and overlapping objects based on their visual texture and resemblance. The principle of \"closure\" refers to the mind's tendency to see complete figures or forms even if a picture is incomplete, partially hidden by other objects, or if part of the information needed to make a complete picture in our minds is missing. For example, if part of a shape's border is missing people still tend to see the shape as completely enclosed by the border and ignore the gaps. The principle of \"good continuation\" makes sense of stimuli that overlap: when there is an intersection between two or more objects, people tend to perceive each as a single uninterrupted object. The principle of \"common fate\" groups stimuli together on the basis of their movement. When visual elements are seen moving in the same direction at the same rate, perception associates the movement as part of the same stimulus. This allows people to make out moving objects even when other details, such as color or outline, are obscured. The principle of \"good form\" refers to the tendency to group together forms of similar shape, pattern, color, etc. Later research has identified additional grouping principles.",
            "score": 125.77235078811646
        },
        {
            "docid": "33702464_5",
            "document": "Extrastriate body area . The experiment had subjects view images of different objects, including faces (as a control group), body parts, animals, parts of the face and intimate objects. While viewing the images, the subjects were scanned with an fMRI to see what area of the brain was activated. Through the trials a compilation of the fMRI\u2019s was made. From this compilation image a specific region was determined to have increased activity when shown visual stimuli of body parts and even more activity when viewing whole bodies. There have been no studies involving brain damage to the EBA. Thus far, only scans of brain activity, as well as transcranial magnetic stimulation, have been used to study the EBA. To find the specific functions of the EBA, Comimo Urgesi, Giovanni Berlucchi and Salvatore M. Aglioti used repetitive transcranial magnetic stimulation (rTMS) to disrupt part of the brain, making the brain less responsive in the target area. The study used event-related rTMS to disrupt the EBA, resulting in inactivation of cortical areas. This inactivation caused a slower response time in discriminating body parts. The study used facial features and motorcycle parts as non human parts for control groups. The facial features and motorcycle body parts did not display any change in response time. The neural activity data shows the EBA handles some of the visual processing of human body and parts but is not related to the processing of the face or other objects.",
            "score": 153.22109055519104
        },
        {
            "docid": "1764639_15",
            "document": "Levels-of-processing effect . Tactile memory representations are similar in nature to visual representations, although there is not enough data to reliably compare the strength of the two kinds of stimuli. One study suggests that there is a difference in mental processing level due to innate differences between visual and tactile stimuli representations. In this study, subjects were presented with an object in both visual and tactile form (a subject is shown a sphere but cannot touch it, and later is given a similar sphere to only hold and not view). Subjects had more trouble identifying size difference in visual fields than using tactile feedback. A suggestion for the lower level of size processing in visual fields is that it results from the high variance in viewed object size due to perspective and distance.",
            "score": 125.68145442008972
        },
        {
            "docid": "2892491_9",
            "document": "Visual language . The perception of a shape requires the grasping of the essential structural features, to produce a \"whole\" or \"gestalt\". The theory of the \"gestalt\" was proposed by Christian von Ehrenfels in 1890. He pointed out that a melody is still recognisable when played in different keys and argued that the whole is not simply the sum of its parts but a total structure. Max Wertheimer researched von Ehrenfels' idea, and in his \"Theory of Form\" (1923) \u2013 nicknamed \"the dot essay\" because it was illustrated with abstract patterns of dots and lines \u2013 he concluded that the perceiving eye tends to bring together elements that look alike (similarity groupings) and will complete an incomplete form (object hypothesis). An array of random dots tends to form configurations (constellations). All these innate abilities demonstrate how the eye and the mind are seeking pattern and simple whole shapes. When we look at more complex visual images such as paintings we can see that art has been a continuous attempt to \"notate\" visual information.",
            "score": 137.4075858592987
        },
        {
            "docid": "613052_12",
            "document": "Direct and indirect realism . Direct realists can potentially deny the existence of any such thing as a mental image but this is difficult to maintain, since we seem able to visually imagine all sorts of things with ease. Even if perception does not involve images other mental processes like imagination certainly seem to. One view, similar to Reid's, is that we do have images of various sorts in our minds when we perceive, dream, hallucinate and imagine but when we actually perceive things, our sensations cannot be considered objects of perception or attention. The only objects of perception are external objects. Even if perception is accompanied by images, or sensations, it is wrong to say we perceive sensations. Direct realism defines perception as perception of external objects where an \"external object\" is allowed to be a photon in the eye but not an impulse in a nerve leading from the eye. Recent work in neuroscience suggests a shared ontology for perception, imagination and dreaming, with similar areas of brain being used for all of these.",
            "score": 111.73028016090393
        },
        {
            "docid": "4231622_6",
            "document": "Inferior temporal gyrus . The light energy that comes from the rays bouncing off of an object is converted into chemical energy by the cells in the retina of the eye. This chemical energy is then converted into action potentials that are transferred through the optic nerve and across the optic chiasm, where it is first processed by the lateral geniculate nucleus of the thalamus. From there the information is sent to the primary visual cortex, region V1. It then travels from the visual areas in the occipital lobe to the parietal and temporal lobes via two distinct anatomical streams. These two cortical visual systems were classified by Ungerleider and Mishkin (1982, see two-streams hypothesis). One stream travels ventrally to the inferior temporal cortex (from V1 to V2 then through V4 to ITC) while the other travels dorsally to the posterior parietal cortex. They are labeled the \u201cwhat\u201d and \u201cwhere\u201d streams, respectively. The Inferior Temporal Cortex receives information from the ventral stream, understandably so, as it is known to be a region essential in recognizing patterns, faces, and objects.  The understanding at the single-cell level of the IT cortex and its role of utilizing memory to identify objects and or process the visual field based on color and form visual information is a relatively recent in neuroscience. Early research indicated that the cellular connections of the temporal lobe to other memory associated areas of the brain \u2013 namely the hippocampus, the amygdala, the prefrontal cortex, among others. These cellular connections have recently been found to explain unique elements of memory, suggesting that unique single-cells can be linked to specific unique types and even specific memories. Research into the single-cell understanding of the IT cortex reveals many compelling characteristics of these cells: single-cells with similar selectivity of memory are clustered together across the cortical layers of the IT cortex; the temporal lobe neurons have recently been shown to display learning behaviors and possibly relate to long-term memory; and, cortical memory within the IT cortex is likely to be enhanced over time thanks to the influence of the afferent-neurons of the medial-temporal region. Further research of the single-cells of the IT cortex suggests that these cells not only have a direct link to the visual system pathway but also are deliberate in the visual stimuli they respond to: in certain cases, the single-cell IT cortex neurons do not initiate responses when spots or slits, namely simple visual stimuli, are present in the visual field; however, when complicated objects are put in place, this initiates a response in the single-cell neurons of the IT cortex. This provides evidence that not only are the single-cell neurons of the IT cortex related in having a unique specific response to visual stimuli but rather that each individual single-cell neuron has a specific response to a specific stimuli. The same study also reveals how the magnitude of the response of these single-cell neurons of the IT cortex do not change due to color and size but are only influenced by the shape. This led to even more interesting observations where specific IT neurons have been linked to the recognition of faces and hands. This is very interesting as to the possibility of relating to neurological disorders of prosopagnosia and explaining the complexity and interest in the human hand. Additional research form this study goes into more depth on the role of \"face neurons\" and \"hand neurons\" involved in the IT cortex.  The significance of the single-cell function in the IT cortex is that it is another pathway in addition to the lateral geniculate pathway that processes most visual system: this raises questions about how does it benefit our visual information processing in addition to normal visual pathways and what other functional units are involved in additional visual information processing.",
            "score": 166.18150675296783
        },
        {
            "docid": "386062_12",
            "document": "Wishful thinking . Some speculate that wishful seeing results from cognitive penetrability in that higher cognitive functions are able to directly influence perceptual experience instead of only influencing perception at higher levels of processing. Those that argue against cognitive penetrability feel that sensory systems operate in a modular fashion with cognitive states exerting their influence only after the stimuli has been perceived. The phenomenon of wishful seeing implicates cognitive penetrability in the perceptual experience. Wishful seeing has been observed to occur in early stages of categorization. Research using ambiguous figures and binocular rivalry exhibit this tendency. Perception is influenced by both top-down and bottom-up processing. In visual processing, bottom-up processing is a rigid route compared to flexible top-down processing. Within bottom-up processing, the stimuli are recognized by fixation points, proximity and focal areas to build objects, while top-down processing is more context sensitive. This effect can be observed via priming as well as with emotional states. The traditional hierarchical models of information processing describe early visual processing as a one-way street: early visual processing goes into conceptual systems, but conceptual systems do not affect visual processes. Currently, research rejects this model and suggests conceptual information can penetrate early visual processing rather than just biasing the perceptual systems. This occurrence is called conceptual or cognitive penetrability. Research on conceptual penetrability utilize stimuli of conceptual-category pairs and measure the reaction time to determine if the category effect influenced visual processing, The category effect is the difference in reaction times within the pairs such as \"Bb\" to \"Bp\". To test conceptual penetrability, there were simultaneous and sequential judgments of pairs. The reaction times decreased as the stimulus onset asynchrony increased, supporting categories affect visual representations and conceptual penetrability. Research with richer stimuli such as figures of cats and dogs allow for greater perceptual variability and analysis of stimulus typicality (cats and dogs were arranged in various positions, some more or less typical for recognition). Differentiating the pictures took longer when they were within the same category (dog-dog) compared between categories (dog-cat) supporting category knowledge influences categorization. Therefore, visual processing measured by physical differential judgments is affected by non-visual processing supporting conceptual penetrability.",
            "score": 163.93419408798218
        },
        {
            "docid": "24965027_7",
            "document": "Cognitive neuroscience of visual object recognition . Viewpoint-invariant theories suggest that object recognition is based on structural information, such as individual parts, allowing for recognition to take place regardless of the object's viewpoint. Accordingly, recognition is possible from any viewpoint as individual parts of an object can be rotated to fit any particular view.[10] This form of analytical recognition requires little memory as only structural parts need to be encoded, which can produce multiple object representations through the interrelations of these parts and mental rotation.[10] Participants in a study were presented with one encoding view from each of 24 preselected objects, as well as five filler images. Objects were then represented in the central visual field at either the same orientation or a different orientation than the original image. Then participants were asked to name if the same or different depth- orientation views of these objects presented. The same procedure was then executed when presenting the images to the left or right visual field. Viewpoint-dependent priming was observed when test views were presented directly to the right hemisphere, but not when test views were presented directly to the left hemisphere. The results support the model that objects are stored in a manner that is viewpoint dependent because the results did not depend on whether the same or a different set of parts could be recovered from the different-orientation views.",
            "score": 118.77778506278992
        },
        {
            "docid": "156431_10",
            "document": "M\u00fcller-Lyer illusion . Neural nets in the visual system of human beings learn how to make a very efficient interpretation of 3D scenes. That is why when somebody goes away from us, we do not perceive them as getting shorter. And when we stretch one arm and look at the two hands we do not perceive one hand smaller than the other. Visual illusions are sometimes held to show us that what we see is an image created in our brain. Our brain supposedly projects the image of the smaller hand to its correct distance in our internal 3D model. This is what is called the size constancy mechanism hypothesis.",
            "score": 115.374356508255
        },
        {
            "docid": "10833249_4",
            "document": "Visual ethics . Visual ethics is equally concerned with the ethics of reception, that is, with seeing as an ethical act. How do different images influence our ethical responses and moral behavior in different ways? To what extent do our ethical responses to images take place pre-reflectively, by visual-perceptual processes in the body-mind, before images even come to consciousness? It can be looked upon more into the cultural perception. It always depends on the cultural background.",
            "score": 124.25137495994568
        },
        {
            "docid": "32197396_4",
            "document": "Form perception . In addition to photoreceptors, the eye requires a properly functioning lens, retina, and an undamaged optic nerve to recognize form. Light travels through the lens, hits the retina, activates the appropriate photoreceptors, depending on available light, which convert the light into an electrical signal that travels along the optic nerve to the lateral geniculate nucleus of the thalamus and then to the primary visual cortex. In the cortex, the adult brain processes information such as lines, orientation, and color. These inputs are integrated in the occipito-temporal cortex where a representation of the object as a whole is created. Visual information continues to be processed in the posterior parietal cortex, also known as the dorsal stream, where the representation of an object\u2019s shape is formed using motion-based cues. It is believed that simultaneously information is processed in the anterior temporal cortex, also known as the ventral stream, where object recognition, identification and naming occur. In the process of recognizing an object, both the dorsal and ventral streams are active, but the ventral stream is more important in discriminating between and recognizing objects. The dorsal stream contributes to object recognition only when two objects have similar shapes and the images are degraded. Observed latency in activation of different parts of the brain supports the idea of hierarchal processing of visual stimuli, with object representations progressing from simple to complex.",
            "score": 128.91692125797272
        },
        {
            "docid": "25216602_3",
            "document": "2.5D (visual perception) . 2.5D is the construction of a three-dimensional environment from 2D retinal projections. 2.5D is inherently the ability to perceive the physical environment, which allows for the understanding of relationships between objects and ourselves within an environment. Perception of the physical environment is limited because of the visual and cognitive problem. The visual problem is the lack of objects in three-dimensional space to be imaged with the same projection and the cognitive problem is that any object can be a different object depending on the perceiver. David Marr\u2019s work on the 2.5D Sketch has found that 2.5D has visual projection constraints. 2.5D projection constraints exist because \"parts of images are always (deformed) discontinuities in luminance\"; therefore, in reality we do not see all of our surroundings but construct the viewer-centered three-dimensional view of our environment.",
            "score": 102.53637230396271
        },
        {
            "docid": "5212945_3",
            "document": "Visual neuroscience . A recent study using Event-Related Potentials (ERPs) linked an increased neural activity in the occipito-temporal region of the brain to the visual categorization of facial expressions. Results focus on a negative peak in the ERP that occurs 170 milliseconds after the stimulus onset. This action potential, called the N170, was measured using electrodes in the occipito-temporal region, an area already known to be changed by face stimuli. Studying by using the EEG, and ERP methods allow for an extremely high temporal resolution of 4 milliseconds, which makes these kinds of experiments extremely well suited for accurately estimating and comparing the time it takes the brain to perform a certain function. Scientists used classification image techniques, to determine what parts of complex visual stimuli (such as a face) will be relied on when patients are asked to assign them to a category, or emotion. They computed the important features when the stimulus face exhibited one of five different emotions. Stimulus faces exhibiting fear had the distinguishing feature of widening eyes, and stimuli exhibiting happiness exhibited a change in the mouth to make a smile. Regardless of the expression of the stimuli's face, the region near the eyes affected the EEG before the regions near the mouth. This revealed a sequential, and predetermined order to the perception and processing of faces, with the eye being the first, and the mouth, and nose being processed after. This process of downward integration only occurred when the inferior facial features were crucial to the categorization of the stimuli. This is best explained by comparing what happens when participants were shown a face exhibiting fear, versus happiness. The N170 peaked slightly earlier for the fear stimuli at about 175 milliseconds, meaning that it took a participants less time to recognize the facial expression. This is expected because only the eyes need to be processed to recognize the emotion. However, when processing a happy expression, where the mouth is crucial to categorization, downward integration must take place, and thus the N170 peak occurred later at around 185 milliseconds. Eventually visual neuroscience aims to completely explain how the visual system processes all changes in faces as well as objects. This will give a complete view to how the world is constantly visually perceived, and may provide insight into a link between perception and consciousness.",
            "score": 128.57912480831146
        },
        {
            "docid": "5626_22",
            "document": "Cognitive science . Perception is the ability to take in information via the senses, and process it in some way. Vision and hearing are two dominant senses that allow us to perceive the environment. Some questions in the study of visual perception, for example, include: (1) How are we able to recognize objects?, (2) Why do we perceive a continuous visual environment, even though we only see small bits of it at any one time? One tool for studying visual perception is by looking at how people process optical illusions. The image on the right of a Necker cube is an example of a bistable percept, that is, the cube can be interpreted as being oriented in two different directions.",
            "score": 116.85202288627625
        },
        {
            "docid": "1316947_5",
            "document": "Ambiguous image . In mid-level vision, the visual system utilizes a set of heuristic methods, called Gestalt grouping rules, to quickly identify a basic perception of an object that helps to resolve an ambiguity. This allows perception to be fast and easy by observing patterns and familiar images rather than a slow process of identifying each part of a group. This aids in resolving ambiguous images because the visual system will accept small variations in the pattern and still perceive the pattern as a whole. The Gestalt grouping rules are the result of the experience of the visual system. Once a pattern is perceived frequently, it is stored in memory and can be perceived again easily without the requirement of examining the entire object again. For example, when looking at a chess board, we perceive a checker pattern and not a set of alternating black and white squares.",
            "score": 155.80143189430237
        },
        {
            "docid": "33702464_6",
            "document": "Extrastriate body area . The actual experiment had people make a \u201ctwo-choice matching-to-sample task. Fourteen right handed participants were required to decide which of two similar upper-limb images matched a single sample previously seen during a tachistoscopic exposure. Photos of face parts and motorcycle parts served as control stimuli in two-matching-to-sample tasks that were comparable to the former task.\u201d rTMS was then applied 150 ms after each sample exposure. The section of the graph, Body Parts, shows the response time while using rTMS. The SHAM category refers to measuring the EBA without rTMS while viewing the images. The Visual Cortex category measures the response of the FFA to be used as the control of the experiment. This measurement of response time, occurring while rTMS was effecting the EBA, further shows that the two areas process visual data of the human form, yet respond to different stimuli. While the magnetic stimulation was temporarily disabling the extrastraite body area, reaction time decreased by about 100 ms. The data from the Sham and Visual Cortex categories on the graph show what was the expected normal results from the experiment. This evidence reveals that applying rTMS to the EBA slowed the response of recognizing images of body parts.",
            "score": 93.3961044549942
        },
        {
            "docid": "32197396_5",
            "document": "Form perception . By five months of age infants are capable of using line junction information to perceive three-D images, including depth and shape, like adults are able. However, there are differences between younger infants and adults in the ability to use motion and color cues to discriminate between two objects. Visual information then continues to be processed in the posterior parietal cortex, also known as the dorsal stream, where the representation of an objects shape is formed using motion-based cues. The identification of differences between the infant and adult brain make it clear that there is either functional reorganization of the infant\u2019s cortex or simply age related differences in which the breed impulses have been observed in infants. Although the infant brain is not identical to the adult brain, it is similar with areas of specialization and a hierarchy of processing,[7] however, adult abilities to perceive form from stationary viewing are not fully understood.",
            "score": 110.5535968542099
        },
        {
            "docid": "5611461_2",
            "document": "Contrast (vision) . Contrast is the difference in luminance or colour that makes an object (or its representation in an image or display) distinguishable. In visual perception of the real world, contrast is determined by the difference in the color and brightness of the object and other objects within the same field of view. The human visual system is more sensitive to contrast than absolute luminance, we can perceive the world similarly regardless of the huge changes in illumination over the day or from place to place. The maximum \"contrast\" of an image is the contrast ratio or dynamic range.",
            "score": 121.85477375984192
        },
        {
            "docid": "649382_5",
            "document": "Pareidolia . Pareidolia can cause people to interpret random images, or patterns of light and shadow, as faces. A 2009 magnetoencephalography study found that objects perceived as faces evoke an early (165 ms) activation of the fusiform face area at a time and location similar to that evoked by faces, whereas other common objects do not evoke such activation. This activation is similar to a slightly faster time (130 ms) that is seen for images of real faces. The authors suggest that face perception evoked by face-like objects is a relatively early process, and not a late cognitive reinterpretation phenomenon. A functional magnetic resonance imaging (fMRI) study in 2011 similarly showed that repeated presentation of novel visual shapes that were interpreted as meaningful led to decreased fMRI responses for real objects. These results indicate that the interpretation of ambiguous stimuli depends upon processes similar to those elicited by known objects.",
            "score": 102.94069743156433
        },
        {
            "docid": "1038052_32",
            "document": "Neuroesthetics . Different artistic styles may also be processed differently by the brain. In a study between filtered forms of abstract and representation art, the bilateral occipital gyri, left cingulate sulcus, and bilateral fusiform gyrus showed increased activation with increased preference when viewing art. However, activation in the bilateral occipital gyri may be caused by the large processing requirements placed on the visual system when viewing high levels of visual detail in artwork such as representational paintings. Several areas of the brain have been shown to respond particularly to forms representational art perhaps due to the brain's ability to make object associations and other functions relating to attention and memory. This form of stimuli leads to increased activation in the left frontal lobe and bilaterally in the parietal and limbic lobes. Also, the left superior parietal lobule, Brodmann's area 7, has been shown to play a role in active image construction during the viewing of art specifically containing indeterminate forms such as soft edge paintings. Bottom up processes such as edge detection and the exploration of visual stimuli are engaged during this type of aesthetic perception. These roles are consistent with previously known parietal lobe responsibilities in spatial cognition and visual imagery.",
            "score": 129.11008620262146
        },
        {
            "docid": "4231622_7",
            "document": "Inferior temporal gyrus . The information for color and form comes from P-cells that receive their information mainly from cones, so they are sensitive to differences in form and color, as opposed to the M-cells that receive information about motion mainly from rods. The neurons in the inferior temporal cortex, also called the inferior temporal visual association cortex, process this information from the P-cells.  The neurons in the ITC have several unique properties that offer an explanation as to why this area is essential in recognizing patterns. They only respond to visual stimuli and their receptive fields always include the fovea, which is one of the densest areas of the retina and is responsible for acute central vision. These receptive fields tend to be larger than those in the striate cortex and often extend across the midline to unite the two visual half fields for the first time. IT neurons are selective for shape and/or color of stimulus and are usually more responsive to complex shapes as opposed to simple ones. A small percentage of them are selective for specific parts of the face. Faces and likely other complex shapes are seemingly coded by a sequence of activity across a group of cells, and IT cells can display both short or long term memory for visual stimuli based on experience.",
            "score": 162.68902468681335
        },
        {
            "docid": "2872287_23",
            "document": "Neural binding . Much of the experimental evidence for neural binding has traditionally revolved around sensory awareness. Sensory awareness is accomplished by integrating things together by cognitively perceiving them and then segmenting them so that, in total, there is an image created. Since there can be an infinite number of possibilities in the perception of an object, this has been a unique area of study. The way the brain then collectively pieces certain things together via networking is important not only in the global way of perceiving but also in segmentation. Much of sensory awareness has to do with the taking of a single piece of an object's makeup and then binding its total characteristics so that the brain perceives the object in its final form. Much of the research for the understanding of segmentation and how the brain perceives an object has been done by studying cats. A major finding of this research has to do with the understanding of gamma waves oscillating at 40\u00a0Hz. The information was extracted from a study using the cat visual cortex. It was shown that the cortical neurons responded differently to spatially different objects. These firings of neurons ranged from 40\u201360\u00a0Hz in measure and when observed showed that they fired synchronously when observing different parts of the object. Such coherent responses point to the fact that the brain is doing a kind of coding where it is piecing certain neurons together in the works of making the form of an object. Since the brain is putting these segmented pieces together unsupervised, a significant consonance is found with many philosophers (like Sigmund Freud) who theorize an underlying subconscious that helps to form every aspect of our conscious thought processes.",
            "score": 110.9634428024292
        },
        {
            "docid": "22751270_2",
            "document": "Illusory conjunctions . Illusory conjunctions are psychological effects in which participants combine features of two objects into one object. There are visual illusory conjunctions, auditory illusory conjunctions, and illusory conjunctions produced by combinations of visual and tactile stimuli. Visual illusory conjunctions are thought to occur due to a lack of visual spatial attention, which depends on fixation and (amongst other things) the amount of time allotted to focus on an object. With a short span of time to interpret an object, blending of different aspects within a region of the visual field \u2013 like shapes and colors \u2013 can occasionally be skewed, which results in visual illusory conjunctions. For example, in a study designed by Anne Treisman and Schmidt participants were required to view a visual presentation of numbers and shapes in different colors. Some shapes were larger than others but all shapes and numbers were evenly spaced and shown for just 200 ms (followed by a mask). When the participants were asked to recall the shapes they reported answers such as a \"small green triangle\" instead of a \"small green circle\". If the space between the objects is smaller, illusory conjunctions occur more often.",
            "score": 139.8650403022766
        }
    ],
    "r": [
        {
            "docid": "41121858_8",
            "document": "Binocular neurons . The correspondence problem questions how the visual system determines what features or objects contained within the two retinal images come from the same real world objects. For example, when looking at a picture of a tree, the visual system must determine that the two retinal images of the tree come from the same actual object in space. If the correspondence problem is not overcome in this case, the organism would perceive two trees when there is only one. In order to solve this problem, the visual system must have a way of avoiding false-matches of the two retinal images. A possible way the visual system avoids false-matches is that binocular complex cells have cross-matching patches between their receptive fields, meaning that multiple complex cells would be stimulated by same feature. Simulation of real binocular complex cells involves a hierarchical squared summation of multiple simple cell receptive fields where the simple cells sum the contribution from both the right and left retinal images.",
            "score": 185.67642211914062
        },
        {
            "docid": "1316947_4",
            "document": "Ambiguous image . When we see an image, the first thing we do is attempt to organize all the parts of the scene into different groups. To do this, one of the most basic methods used is finding the edges. Edges can include obvious perceptions such as the edge of a house, and can include other perceptions that the brain needs to process deeper, such as the edges of a person's facial features. When finding edges, the brain's visual system detects a point on the image with a sharp contrast of lighting. Being able to detect the location of the edge of an object aids in recognizing the object. In ambiguous images, detecting edges still seems natural to the person perceiving the image. However, the brain undergoes deeper processing to resolve the ambiguity. For example, consider an image that involves an opposite change in magnitude of luminance between the object and the background (e.g. From the top, the background shifts from black to white, and the object shifts from white to black). The opposing gradients will eventually come to a point where there is an equal degree of luminance of the object and the background. At this point, there is no edge to be perceived. To counter this, the visual system connects the image as a whole rather than a set of edges, allowing one to see an object rather than edges and non-edges. Although there is no complete image to be seen, the brain is able to accomplish this because of its understanding of the physical world and real incidents of ambiguous lighting. In ambiguous images, an illusion is often produced from illusory contours. An illusory contour is a perceived contour without the presence of a physical gradient. In examples where a white shape appears to occlude black objects on a white background, the white shape appears to be brighter than the background, and the edges of this shape produce the illusory contours. These illusory contours are processed by the brain in a similar way as real contours. The visual system accomplishes this by making inferences beyond the information that is presented in much the same way as the luminance gradient.",
            "score": 173.29859924316406
        },
        {
            "docid": "27313901_12",
            "document": "Visual N1 . Although spatial attention has been shown to be unique in selection for perceptual information that will be further processed, objects have also been shown to be important in filtering information for further processing. For example, in a Filtering Paradigm (see above), rectangles were presented on either side of the visual field. Participants were directed to attend to one side of the visual field and to the top 50% of the object within that visual field. The target was a shaded region of the top right-hand side corner; however, similar targets were presented in the unattended bottom half of the object in the attended visual field and in the top and bottom halves of the object in the unattended visual field. As expected, when comparing targets in the attended visual field to targets in the unattended visual field, it was found that the amplitude of the N1 was greater for attended (vs. unattended) objects. Additionally, although the amplitude of the N1 was greatest for targets in the attended visual field and the attended part of object, the amplitude of the N1 for targets in the unattended portion of the attended object was larger than the amplitude of the N1 for targets at an equivalent distance from the locus of attention but on an unattended object. These results provide evidence that while spatial attention does serve as a selection mechanism for further processing, spatial attention can spread across objects and influences further perceptual processing.",
            "score": 172.40591430664062
        },
        {
            "docid": "1316947_3",
            "document": "Ambiguous image . Middle vision is the stage in visual processing that combines all the basic features in the scene into distinct, recognizable object groups. This stage of vision comes before high-level vision (understanding the scene) and after early vision (determining the basic features of an image). When perceiving and recognizing images, mid-level vision comes into use when we need to classify the object we are seeing. Higher-level vision is used when the object classified must now be recognized as a specific member of its group. For example, through mid-level vision we perceive a face, then through high-level vision we recognize a face of a familiar person. Mid-level vision and high-level vision are crucial for understanding a reality that is filled with ambiguous perceptual inputs.",
            "score": 172.15792846679688
        },
        {
            "docid": "35982062_6",
            "document": "Biased Competition Theory . There are two major neural pathways that process the information in the visual field; the ventral stream and the dorsal stream. The two pathways run in parallel and are both working simultaneously. The ventral stream is important for object recognition and often referred to as the \u201cwhat\u201d system of the brain; it projects to the inferior temporal cortex. The dorsal stream is important for spatial perception and performance and is referred to as the \u201cwhere\u201d system which projects to the posterior parietal cortex. According to the biased competition theory, an individual\u2019s visual system has limited capacity to process information about multiple objects at any given time. For example, if an individual was presented with two stimuli (objects) and was asked to identify attributes of each object at the same time, the individual\u2019s performance would be worse in comparison to if the objects were presented separately. This suggests multiple objects presented simultaneously in the visual field will compete for neural representation due to limited processing resources. Single cell recording studies conducted by Kastner and Ungerleider examined the neural mechanisms behind the biased competition theory. In their experiment the size of the receptive field's (RF) of neurons within the visual cortex were examined. A single visual stimulus was presented alone in a neuron\u2019s RF, followed with another stimulus presented simultaneously within the same RF. The single \u2018effective\u2019 stimuli produced a low firing rate, whereas the two stimuli presented together produced a high firing rate. The response to the paired stimuli was reduced. This suggests that when two stimuli are presented together within a neuron\u2019s RF, the stimuli are processed in a mutually suppressive manner, rather than being processed independently. This suppression process, according to Kastner and Ungerleider, occurs when two stimuli are presented together because they compete for neural representation, due to limited cognitive processing capacity. The RF experiment suggests that as the number of objects increase, the information available for each object will decrease due to increased neural workload (suppression), and decreased cognitive capacity. In order for an object in the visual field or RF be efficiently processed, there needs to be a way to bias these neurological resources towards the object. Attention prioritizes task relevant objects, biasing this process. For example, this bias can be towards an object which is currently attended to in the visual field or RF, or towards the object that is most relevant to one\u2019s behavior. Functional magnetic resonance imaging (fMRI) has shown that biased competition theory can explain the observed attention effects at a neuronal level. Attention effects bias the internal weight (strengthens connections) of task relevant features toward the attended object. This was shown by Reddy, Kanwisher, and van Rullen who found an increase in oxygenated blood to a specific neuron following a locational cue. Further neurological support comes from neurophysiological studies which have shown that attention results from Top-down biasing, which in turn influences neuronal spiking. In sum, external inputs affect the Top-down guidance of attention, which bias specific neurons in the brain.",
            "score": 169.3164825439453
        },
        {
            "docid": "25522368_8",
            "document": "Feature detection (nervous system) . In the late 1950s, Jerome Lettvin and his colleagues began to expand the feature detection hypothesis and clarify the relationship between single neurons and sensory perception. In their paper \"What the Frog's Eye Tells the Frog's Brain\", Lettvin et al. (1959) looked beyond the mechanisms for signal-noise discrimination in the frog's retina and were able to identify four classes of ganglion cells in the frog retina: \"sustained contrast detectors\", \"net convexity detectors\" (or \"bug detectors\"), \"moving edge detectors\", and \"net dimming detectors.\"  In the same year, David Hubel and Torsten Wiesel began investigating properties of neurons in the visual cortex of cats, processing in the mammalian visual system. In their first paper in 1959, Hubel and Wiesel took recording from single cells in the striate cortex of lightly anesthetized cats. The retinas of the cats were stimulated either individually or simultaneously with spots of light of various sizes and shapes. From the analysis of these recordings, Hubel and Wiesel identified orientation-selective cells in the cat's visual cortex and generated a map of the receptive field of cortical cells. At the time, circular spots of light were used as stimuli in studies of the visual cortex. However, Hubel and Wiesel noticed that rectangular bars of light were more effective stimuli (i.e. more natural stimuli) than circular spots of light, as long as the orientation was adjusted to the correct angle appropriate for each ganglion cell. These so-called simple cells were later called bar detectors or edge detectors. While comparing the receptive fields of neurons in the cat striate cortex with the concentric \"on\" and \"off\" receptive fields identified in cat ganglion cells by Kuffler et al., Hubel and Wiesel noticed that, although \"on\" and \"off\" regions were present in the striate cortex, they were not arranged in concentric circles. From their discovery of these uniquely orienting receptive fields, Hubel and Wiesel concluded that orientation-selective cells exist within the cat's visual cortex.",
            "score": 167.9461669921875
        },
        {
            "docid": "4231622_6",
            "document": "Inferior temporal gyrus . The light energy that comes from the rays bouncing off of an object is converted into chemical energy by the cells in the retina of the eye. This chemical energy is then converted into action potentials that are transferred through the optic nerve and across the optic chiasm, where it is first processed by the lateral geniculate nucleus of the thalamus. From there the information is sent to the primary visual cortex, region V1. It then travels from the visual areas in the occipital lobe to the parietal and temporal lobes via two distinct anatomical streams. These two cortical visual systems were classified by Ungerleider and Mishkin (1982, see two-streams hypothesis). One stream travels ventrally to the inferior temporal cortex (from V1 to V2 then through V4 to ITC) while the other travels dorsally to the posterior parietal cortex. They are labeled the \u201cwhat\u201d and \u201cwhere\u201d streams, respectively. The Inferior Temporal Cortex receives information from the ventral stream, understandably so, as it is known to be a region essential in recognizing patterns, faces, and objects.  The understanding at the single-cell level of the IT cortex and its role of utilizing memory to identify objects and or process the visual field based on color and form visual information is a relatively recent in neuroscience. Early research indicated that the cellular connections of the temporal lobe to other memory associated areas of the brain \u2013 namely the hippocampus, the amygdala, the prefrontal cortex, among others. These cellular connections have recently been found to explain unique elements of memory, suggesting that unique single-cells can be linked to specific unique types and even specific memories. Research into the single-cell understanding of the IT cortex reveals many compelling characteristics of these cells: single-cells with similar selectivity of memory are clustered together across the cortical layers of the IT cortex; the temporal lobe neurons have recently been shown to display learning behaviors and possibly relate to long-term memory; and, cortical memory within the IT cortex is likely to be enhanced over time thanks to the influence of the afferent-neurons of the medial-temporal region. Further research of the single-cells of the IT cortex suggests that these cells not only have a direct link to the visual system pathway but also are deliberate in the visual stimuli they respond to: in certain cases, the single-cell IT cortex neurons do not initiate responses when spots or slits, namely simple visual stimuli, are present in the visual field; however, when complicated objects are put in place, this initiates a response in the single-cell neurons of the IT cortex. This provides evidence that not only are the single-cell neurons of the IT cortex related in having a unique specific response to visual stimuli but rather that each individual single-cell neuron has a specific response to a specific stimuli. The same study also reveals how the magnitude of the response of these single-cell neurons of the IT cortex do not change due to color and size but are only influenced by the shape. This led to even more interesting observations where specific IT neurons have been linked to the recognition of faces and hands. This is very interesting as to the possibility of relating to neurological disorders of prosopagnosia and explaining the complexity and interest in the human hand. Additional research form this study goes into more depth on the role of \"face neurons\" and \"hand neurons\" involved in the IT cortex.  The significance of the single-cell function in the IT cortex is that it is another pathway in addition to the lateral geniculate pathway that processes most visual system: this raises questions about how does it benefit our visual information processing in addition to normal visual pathways and what other functional units are involved in additional visual information processing.",
            "score": 166.18150329589844
        },
        {
            "docid": "1316947_18",
            "document": "Ambiguous image . To go further than just perceiving the object is to recognize the object. Recognizing an object plays a crucial role in resolving ambiguous images, and relies heavily on memory and prior knowledge. To recognize an object, the visual system detects familiar components of it, and compares the perceptual representation of it with a representation of the object stored in memory. This can be done using various templates of an object, such as \"dog\" to represent dogs in general. The template method is not always successful because members of a group may significantly differ visually from each other, and may look much different if viewed from different angles. To counter the problem of viewpoint, the visual system detects familiar components of an object in 3-dimensional space. If the components of an object perceived are in the same position and orientation of an object in memory, recognition is possible. Research has shown that people that are more creative in their imagery are better able to resolve ambiguous images. This may be due to their ability to quickly identify patterns in the image. When making a mental representation of an ambiguous image, in the same way as normal images, each part is defined and then put onto the mental representation. The more complex the scene is, the longer it takes to process and add to the representation. Figures drawn in a way that avoids depth cues may become ambiguous. Classic examples of this phenomenon are the Necker cube, and the rhombille tiling (viewed as an isometric drawing of cubes).",
            "score": 163.93670654296875
        },
        {
            "docid": "386062_12",
            "document": "Wishful thinking . Some speculate that wishful seeing results from cognitive penetrability in that higher cognitive functions are able to directly influence perceptual experience instead of only influencing perception at higher levels of processing. Those that argue against cognitive penetrability feel that sensory systems operate in a modular fashion with cognitive states exerting their influence only after the stimuli has been perceived. The phenomenon of wishful seeing implicates cognitive penetrability in the perceptual experience. Wishful seeing has been observed to occur in early stages of categorization. Research using ambiguous figures and binocular rivalry exhibit this tendency. Perception is influenced by both top-down and bottom-up processing. In visual processing, bottom-up processing is a rigid route compared to flexible top-down processing. Within bottom-up processing, the stimuli are recognized by fixation points, proximity and focal areas to build objects, while top-down processing is more context sensitive. This effect can be observed via priming as well as with emotional states. The traditional hierarchical models of information processing describe early visual processing as a one-way street: early visual processing goes into conceptual systems, but conceptual systems do not affect visual processes. Currently, research rejects this model and suggests conceptual information can penetrate early visual processing rather than just biasing the perceptual systems. This occurrence is called conceptual or cognitive penetrability. Research on conceptual penetrability utilize stimuli of conceptual-category pairs and measure the reaction time to determine if the category effect influenced visual processing, The category effect is the difference in reaction times within the pairs such as \"Bb\" to \"Bp\". To test conceptual penetrability, there were simultaneous and sequential judgments of pairs. The reaction times decreased as the stimulus onset asynchrony increased, supporting categories affect visual representations and conceptual penetrability. Research with richer stimuli such as figures of cats and dogs allow for greater perceptual variability and analysis of stimulus typicality (cats and dogs were arranged in various positions, some more or less typical for recognition). Differentiating the pictures took longer when they were within the same category (dog-dog) compared between categories (dog-cat) supporting category knowledge influences categorization. Therefore, visual processing measured by physical differential judgments is affected by non-visual processing supporting conceptual penetrability.",
            "score": 163.93418884277344
        },
        {
            "docid": "31329046_6",
            "document": "Pre-attentive processing . Information for pre-attentive processing is detected through the five senses. In the visual system, the receptive fields at the back of the eye (retina) transfer the image via axons to the thalamus, specifically the lateral geniculate nuclei. The image then travels to the primary visual cortex and continues on to be processed by the visual association cortex. At each stage, the image is processed with increasing complexity. Pre-attentive processing starts with the retinal image; this image is magnified as it moves from retina to the cortex of the brain. Shades of light and dark are processed in the lateral geniculate nuclei of the thalamus. Simple and complex cells in the brain process boundary and surface information by deciphering the image's contrast, orientation, and edges. When the image hits the fovea, it is highly magnified, facilitating object recognition. The images in the periphery are less clear but help to create a complete image used for scene perception.",
            "score": 163.6179656982422
        },
        {
            "docid": "4231622_7",
            "document": "Inferior temporal gyrus . The information for color and form comes from P-cells that receive their information mainly from cones, so they are sensitive to differences in form and color, as opposed to the M-cells that receive information about motion mainly from rods. The neurons in the inferior temporal cortex, also called the inferior temporal visual association cortex, process this information from the P-cells.  The neurons in the ITC have several unique properties that offer an explanation as to why this area is essential in recognizing patterns. They only respond to visual stimuli and their receptive fields always include the fovea, which is one of the densest areas of the retina and is responsible for acute central vision. These receptive fields tend to be larger than those in the striate cortex and often extend across the midline to unite the two visual half fields for the first time. IT neurons are selective for shape and/or color of stimulus and are usually more responsive to complex shapes as opposed to simple ones. A small percentage of them are selective for specific parts of the face. Faces and likely other complex shapes are seemingly coded by a sequence of activity across a group of cells, and IT cells can display both short or long term memory for visual stimuli based on experience.",
            "score": 162.68902587890625
        },
        {
            "docid": "35982062_8",
            "document": "Biased Competition Theory . Bottom-up processes are characterized by an absence of higher level direction in sensory processing. It primarily relies on sensory information and incoming sensory information is the starting point for all Bottom-up processing. Bottom-up refers to when a feature stands out in a visual search. This is commonly called the \u201cpop-out\u201d effect. Salient features like bright colors, movement and big objects make the object \u201cpop-out\u201d of the visual search. \u201cPop-out\u201d features can often attract attention without conscious processing. Objects that stand out are often given priority (bias) in processing. Bottom-up processing is data driven, and according to this stimuli are perceived on the basis of the data which is being experienced through the senses. Evidence suggests that simultaneously presented stimuli do in fact compete in order to be represented in the visual cortex, with stimuli mutually suppressing each other to gain this representation. This was examined by Reynolds and colleagues, who looked at the size of neurons\u2019 receptive field\u2019s within the visual cortex. It was found that the presentation of a single stimulus resulted in a low firing rate while two stimuli presented together resulted in a higher firing rate. Reynolds and colleagues also found that when comparing the neural response of an individually presented visual stimulus to responses gathered from simultaneously presented stimuli, the responses of the concurrent presented stimuli were less than the sum of the responses gathered when each stimuli was presented alone. This suggests that two stimuli presented together increase neural work load required for attention. This increased neural load creates suppressive processes and causes the stimuli to compete for neural representation in the brain. Proulx and Egeth predicted that brighter objects would bias attention in favor of that object. Another prediction is that larger objects would bias the attention in favor of that object. The experiment was a computer-based visual search task, where participants searched for a target among distractions. The results of the study suggested that when irrelevant stimuli were large or bright, attention was biased towards the irrelevant objects, prioritizing them for cognitive processing. This research shows the effects of Bottom-up (stimulus-driven) processing on biased competition theory.",
            "score": 161.12435913085938
        },
        {
            "docid": "22391885_6",
            "document": "Neural processing for individual categories of objects . It may be that the use of distinct brain regions for processing different object categories results from different processing requirements necessary for each class. Indeed, Malach et al. (2002) detail findings that buildings and faces require processing at different resolutions in order to be recognised - face recognition requires the analysis of fine detail, while buildings can be recognised using larger scale feature integration. As a result, faces are associated with central visual field processing while buildings are processed more peripherally. Malach et al. (2002) report that points on the retina sharing foveal centricity are mapped onto parallel cortical bands and it therefore follows that object classes that are processed differently by retinal cells should be represented distinctly within the brain. Consistently, faces and buildings were found to be processed independently of each other and in discrete cortical regions suggesting that processing is facilitated by assigning object categories to distinct cortical regions according to the level and type of processing that they require.",
            "score": 159.4603271484375
        },
        {
            "docid": "21944_39",
            "document": "Nervous system . Although the simplest reflexes may be mediated by circuits lying entirely within the spinal cord, more complex responses rely on signal processing in the brain. For example, when an object in the periphery of the visual field moves, and a person looks toward it many stages of signal processing are initiated. The initial sensory response, in the retina of the eye, and the final motor response, in the oculomotor nuclei of the brain stem, are not all that different from those in a simple reflex, but the intermediate stages are completely different. Instead of a one or two step chain of processing, the visual signals pass through perhaps a dozen stages of integration, involving the thalamus, cerebral cortex, basal ganglia, superior colliculus, cerebellum, and several brainstem nuclei. These areas perform signal-processing functions that include feature detection, perceptual analysis, memory recall, decision-making, and motor planning.",
            "score": 159.32032775878906
        },
        {
            "docid": "24965027_13",
            "document": "Cognitive neuroscience of visual object recognition . The lateral occipital complex (LOC) has been found to be particularly important for object recognition at the perceptual structural level. In an event-related fMRI study that looked at the adaptation of neurons activated in visual processing of objects, it was discovered that the similarity of an object's shape is necessary for subsequent adaptation in the LOC, but specific object features such as edges and contours are not. This suggests that activation in the LOC represents higher-level object shape information and not simple object features. In a related fMRI study, the activation of the LOC, which occurred regardless of the presented object's visual cues such as motion, texture, or luminance contrasts, suggests that the different low-level visual cues used to define an object converge in \"object-related areas\" to assist in the perception and recognition process. None of the mentioned higher-level object shape information seems to provide any semantic information about the object as the LOC shows a neuronal response to varying forms including non-familiar, abstract objects.",
            "score": 159.31417846679688
        },
        {
            "docid": "33431597_15",
            "document": "Attentional control . Our brains have distinct attention systems that have been shaped throughout time by evolution. Visual attention operates mainly on three different representations: location , feature, and object-based. The spatial separation between two objects has an effect on attention. People can selectively pay attention to one of two objects in the same general location. Research has also been done on attention to non-object based things like motion. When directing attention to a feature like motion, neuronal activity increases in areas specific for the feature. When visually searching for a non-spatial feature or a perceptual feature, selectively enhancing the sensitivity to that specific feature plays a role in directing attention. When people are told to look for motion, then motion will capture their attention, but attention is not captured by motion if they are told to look for color.",
            "score": 158.0781707763672
        },
        {
            "docid": "1619306_35",
            "document": "Multisensory integration . In contrast, the dorsal auditory pathway, projecting from the temporal lobe is largely concerned with processing spatial information, and contains receptive fields that are topographically organized. Fibers from this region project directly to neurons governing corresponding receptive fields in V1. The perceptual consequences of this have not yet been empirically acknowledged. However, it can be hypothesized that these projections may be the precursors of increased acuity and emphasis of visual stimuli in relevant areas of perceptual space. Consequently, this finding rejects Jones and Powell's (1970) hypothesis and thus is in conflict with Sadato \"et al.\"'s (2004) findings. A resolution to this discrepancy includes the possibility that primary sensory areas can not be classified as a single group, and thus may be far more different from what was previously thought.",
            "score": 157.93077087402344
        },
        {
            "docid": "31148473_12",
            "document": "Transsaccadic memory . This is an area within the visual cortex that has been found to play an important role in the target selection of saccades. In other words, this area is important for determining which objects our eyes shift to when they move. Studies have shown that there is a large amount of activation within the visual area V4 before the saccade even takes place. This occurs in the form of shrinking receptive fields. The receptive fields of these brain cells tend to shift towards the object that the eye is about to move towards, generally more so if the object is close to the original fixation point. This dynamic change in receptive fields is thought to enhance the perception and recognition of objects in a visual scene. Because the receptive fields become smaller around the targeted objects, attention within the visual scene is very focused on these objects. Increased attention to target objects within a visual scene help direct eye movements from one object to another. Understanding of the visual scene becomes more efficient because these attention shifts guide the eyes towards relevant objects as opposed to objects that may not be as important.",
            "score": 156.18479919433594
        },
        {
            "docid": "1316947_5",
            "document": "Ambiguous image . In mid-level vision, the visual system utilizes a set of heuristic methods, called Gestalt grouping rules, to quickly identify a basic perception of an object that helps to resolve an ambiguity. This allows perception to be fast and easy by observing patterns and familiar images rather than a slow process of identifying each part of a group. This aids in resolving ambiguous images because the visual system will accept small variations in the pattern and still perceive the pattern as a whole. The Gestalt grouping rules are the result of the experience of the visual system. Once a pattern is perceived frequently, it is stored in memory and can be perceived again easily without the requirement of examining the entire object again. For example, when looking at a chess board, we perceive a checker pattern and not a set of alternating black and white squares.",
            "score": 155.8014373779297
        },
        {
            "docid": "1626279_15",
            "document": "Anne Treisman . The first stage is called \"pre-attentive\" because it happens automatically, or without effort or attention by the perceiver. In this stage, an object is broken down into its elementary features for processing (i.e., color, texture, shape, etc.). Treisman posits we are unaware of this stage of attention because it occurs quickly and early in perceptual processes (before conscious awareness). Evidence for the pre-attentive state comes from Treisman's own studies. Treisman created a display of four objects flanked by two black numbers. The display flashed on a computer screen for 1/5 of a second and followed by a random-dot masking field to eliminate residual perception of the stimuli after the stimuli were turned off. Participants were asked to first report on the black numbers, followed by what they saw at each of the four locations where the shapes had been. Under these conditions, participants reported seeing illusory conjunctions in 18% of trials. That is, participants reported seeing objects that consisted of a combination of features from two different stimuli. For example, after seeing a big yellow circle, a big blue triangle, a small red triangle, and a small green circle, a person might report seeing a small red circle and a small green triangle. The reason illusory conjunctions occurred is that stimuli were presented rapidly and the observers' attention was distracted from the target object by having them focus on the black numbers; thus, elementary features had not yet been grouped or bound to an object. Having participants attend to the target objects eliminated the illusory conjunction",
            "score": 155.4796600341797
        },
        {
            "docid": "25140_19",
            "document": "Perception . \"Perceptual constancy\" is the ability of perceptual systems to recognize the same object from widely varying sensory inputs. For example, individual people can be recognized from views, such as frontal and profile, which form very different shapes on the retina. A coin looked at face-on makes a circular image on the retina, but when held at angle it makes an elliptical image. In normal perception these are recognized as a single three-dimensional object. Without this correction process, an animal approaching from the distance would appear to gain in size. One kind of perceptual constancy is \"color constancy\": for example, a white piece of paper can be recognized as such under different colors and intensities of light. Another example is \"roughness constancy\": when a hand is drawn quickly across a surface, the touch nerves are stimulated more intensely. The brain compensates for this, so the speed of contact does not affect the perceived roughness. Other constancies include melody, odor, brightness and words. These constancies are not always total, but the variation in the percept is much less than the variation in the physical stimulus. The perceptual systems of the brain achieve perceptual constancy in a variety of ways, each specialized for the kind of information being processed, with phonemic restoration as a notable example from hearing.",
            "score": 153.8118438720703
        },
        {
            "docid": "33702464_5",
            "document": "Extrastriate body area . The experiment had subjects view images of different objects, including faces (as a control group), body parts, animals, parts of the face and intimate objects. While viewing the images, the subjects were scanned with an fMRI to see what area of the brain was activated. Through the trials a compilation of the fMRI\u2019s was made. From this compilation image a specific region was determined to have increased activity when shown visual stimuli of body parts and even more activity when viewing whole bodies. There have been no studies involving brain damage to the EBA. Thus far, only scans of brain activity, as well as transcranial magnetic stimulation, have been used to study the EBA. To find the specific functions of the EBA, Comimo Urgesi, Giovanni Berlucchi and Salvatore M. Aglioti used repetitive transcranial magnetic stimulation (rTMS) to disrupt part of the brain, making the brain less responsive in the target area. The study used event-related rTMS to disrupt the EBA, resulting in inactivation of cortical areas. This inactivation caused a slower response time in discriminating body parts. The study used facial features and motorcycle parts as non human parts for control groups. The facial features and motorcycle body parts did not display any change in response time. The neural activity data shows the EBA handles some of the visual processing of human body and parts but is not related to the processing of the face or other objects.",
            "score": 153.22108459472656
        },
        {
            "docid": "49045837_6",
            "document": "Spatial ability . Spatial perception is also very relevant in sports. For example, a study found that cricket players who were faster at picking up information from briefly presented visual displays were significantly better batsmen in an actual game. A 2015 study published in the \"Journal of Vision\" found that soccer players had higher perceptual ability for body kinematics such as processing multitasking crowd scenes which involve pedestrians crossing a street or complex dynamic visual scenes. Another study published in the \"Journal of Human Kinetics\" on fencing athletes found that achievement level was highly correlated with spatial perceptual skills such as visual discrimination, visual-spatial relationships, visual sequential memory, narrow attentional focus and visual information processing. A review published in the journal of \"Neuropsychologia\" found that spatial perception involves attributing meaning to an object or space, so that their sensory processing is actually part of semantic processing of the incoming visual information. The review also found that spatial perception involves the human visual system in the brain and the parietal lobule which is responsible for visuomotor processing and visually goal-directed action. Studies have also found that individuals who played first person shooting games had better spatial perceptual skills like faster and more accurate performance in a peripheral and identification task while simultaneously performing a central search. Researchers suggested that, in addition to enhancing the ability to divide attention, playing action games significantly enhances perceptual skills like top-down guidance of attention to possible target locations.",
            "score": 152.75144958496094
        },
        {
            "docid": "1038052_20",
            "document": "Neuroesthetics . Perceptual grouping to delineate a figure from the background may be enjoyable. The source of the pleasure may have come about because of the evolutionary necessity to give organisms an incentive to uncover objects, such as predators, from noisy environments. For example, when viewing ink blots, the visual system segments the scene to defeat camouflage and link a subset of splotches together. This may be accomplished most effectively if limbic reinforcement is fed back to early vision at every stage of visual processing leading up to the discovery of the object. The key idea is that due to the limited attentional resources, constant feedback facilitates processing of features at earlier stages due to the discovery of a clue which produces limbic activation to draw one's attention to important features. Though not spontaneous, this reinforcement is the source of the pleasant sensation. The discovery of the object itself results in a pleasant 'aha' revelation causing the organism to hold onto the image.",
            "score": 152.586669921875
        },
        {
            "docid": "17290104_6",
            "document": "Apperceptive agnosia . Visual apperceptive agnosia is a visual impairment that results in a patient's inability to name objects. While agnosics suffer from severe deficits, patients' visual acuity and other visual abilities such as perceiving parts and colours remain intact. Deficits seem to occur because of damage to early-level perceptual processing. While patients are able to effectively allocate attention to locate the object and perceive the parts, they are unable to group together the parts they see and name the object accurately. This is demonstrated by the fact that patients are more effective at naming two attributes from a single object than they are able to name one attribute on each of the two superimposed objects. In addition they are still able to describe objects in detail and recognize objects by touch.",
            "score": 149.617919921875
        },
        {
            "docid": "27313901_15",
            "document": "Visual N1 . Additionally, research on the visual N1 suggests that spatial and object attention serve as an early selection mechanism that influences the selection of other perceptual features (e.g., color, motion) for further processing. The amplitude of the N1 is largest for perceptual features in attended (vs. unattended) locations and on attended (vs. unattended) objects, providing evidence that perceptual features are only selected for further perceptual processing if they are in attended locations or on attended objects.",
            "score": 148.6030731201172
        },
        {
            "docid": "941909_10",
            "document": "Receptive field . In the visual system, receptive fields are volumes in visual space. They are smallest in the fovea where they can be a few minutes of arc like a dot on this page, to the whole page. For example, the receptive field of a single photoreceptor is a cone-shaped volume comprising all the visual directions in which light will alter the firing of that cell. Its apex is located in the center of the lens and its base essentially at infinity in visual space. Traditionally, visual receptive fields were portrayed in two dimensions (e.g., as circles, squares, or rectangles), but these are simply slices, cut along the screen on which the researcher presented the stimulus, of the volume of space to which a particular cell will respond. In the case of binocular neurons in the visual cortex, receptive fields do not extend to optical infinity. Instead, they are restricted to a certain interval of distance from the animal, or from where the eyes are fixating (see Panum's area).",
            "score": 148.59446716308594
        },
        {
            "docid": "1695035_3",
            "document": "Feature integration theory . According to Treisman, the first stage of the feature integration theory is the preattentive stage. During this stage, different parts of the brain automatically gather information about basic features (colors, shape, movement) that are found in the visual field. The idea that features are automatically separated appears to be counterintuitive; however, we are not aware of this process because it occurs early in perceptual processing, before we become conscious of the object.",
            "score": 148.029541015625
        },
        {
            "docid": "176997_6",
            "document": "Blindsight . Blindsight patients show awareness of single visual features, such as edges and motion, but cannot gain a holistic visual percept. This suggests that perceptual awareness is modular and that\u2014in sighted individuals\u2014there is a \"binding process that unifies all information into a whole percept\", which is interrupted in patients with such conditions as blindsight and visual agnosia. Therefore, object identification and object recognition are thought to be separate processes and occur in different areas of the brain, working independently from one another. The modular theory of object perception and integration would account for the \"hidden perception\" experienced in blindsight patients. Research has shown that visual stimuli with the single visual features of sharp borders, sharp onset/offset times, motion, and low spatial frequency contribute to, but are not strictly necessary for, an object's salience in blindsight.",
            "score": 146.63182067871094
        },
        {
            "docid": "41121858_10",
            "document": "Binocular neurons . The stereo model is an energy model that integrates both the position-shift model and the phase-difference model. The position-shift model suggests that the receptive fields of left and right simple cells are identical in shape but are shifted horizontally relative to each other. This model was proposed by Bishop and Pettigrew in 1986. According to the phase-difference model the excitatory and inhibitory sub-regions of the left and right receptive fields of simple cells are shifted in phase such that their boundaries overlap. This model was developed by Ohzawa in 1990. The stereo model uses Fourier phase dependence of simple cell responses, and it suggests that the use of the response of only simple cells is not enough to accurately depict the physiological observations found in cat, monkey, and human visual pathways. In order to make the model more representative of physiological observations, the stereo model combines the responses of both simple and complex cells into a single signal. How this combination is done depends on the incoming stimulus. As one example, the model uses independent Fourier phases for some types of stimuli, and finds the preferred disparity of the complex cells equal to the left-right receptive field shift. For other stimuli, the complex cell becomes less phase sensitive than the simple cells alone, and when the complex cells larger receptive field is included in the model, the phase sensitivity is returns to results similar to normal physiological observations. In order to include the larger receptive fields of complex cells, the model averages several pairs of simple cells nearby and overlaps their receptive fields to construct the complex cell model. This allows the complex cell to be phase independent for all stimuli presented while still maintaining an equal receptive field shift to the simple cells it is composed of in the model.",
            "score": 146.53933715820312
        },
        {
            "docid": "7330954_8",
            "document": "Pattern recognition (psychology) . Multiple theories try to explain how humans are able to recognize patterns in their environment. Feature detection theory proposes that the nervous system sorts and filters incoming stimuli to allow the human (or animal) to make sense of the information. In the organism, this system is made up of feature detectors, which are individual neurons, or groups of neurons, that encode specific perceptual features. The theory proposes an increasing complexity in the relationship between detectors and the perceptual feature. The most basic feature detectors respond to simple properties of the stimuli. Further along the perceptual pathway, higher organized feature detectors are able to respond to more complex and specific stimuli properties. When features repeat or occur in a meaningful sequence, we are able to identify these patterns because of our feature detection system.",
            "score": 146.1524200439453
        },
        {
            "docid": "7725524_4",
            "document": "Colour centre . V1 has multiple areas that are colour-sensitive, which indicates that colour processing is not limited to one area. According to a paper by Dr Robert Shapley, V1 has an important role in colour perception. fMRI experimental results showed that V1 has two kinds of colour sensitive neurons: single-opponent and double-opponent cells. These cells are integral in the opponent process of interpreting colour signals. Single-opponent neurons respond to large areas of colour. This is advantageous for recognizing large colour scenes and atmospheres. In comparison, double opponent cells respond to patterns, textures, and colour boundaries. This is more important for perceiving the colour of objects and pictures. The double-opponent cells are receptive to opposite inputs from different cone cells in the retina. This is ideal for identifying contrasting colours, such as red and green. [1] Double-opponent cells are particularly important in computing local cone ratios from visual information from their receptive fields.",
            "score": 146.11007690429688
        }
    ]
}