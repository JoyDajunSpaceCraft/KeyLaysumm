{
    "q": [
        {
            "docid": "18614_35",
            "document": "Language acquisition . Prosody is the property of speech that conveys an emotional state of the utterance, as well as intended form of speech (whether it be a question, statement or command). Some researchers in the field of developmental neuroscience would argue that fetal auditory learning mechanisms are solely due to discrimination in prosodic elements. Although this would hold merit in an evolutionary psychology perspective (i.e. recognition of mother's voice/familiar group language from emotionally valent stimuli), some theorists argue that there is more than prosodic recognition in elements of fetal learning. Newer evidence shows that fetuses not only react to the native language differently from nonnative, but furthermore that fetuses react differently and can accurately discriminate between native and nonnative vowels (Moon, Lagercrantz, & Kuhl, 2013). Furthermore, a new study in 2016 showed that newborn infants encode the edges of multisyllabic sequences better than the internal components of the sequence (Ferry et al., 2016). Together, these results suggest that newborn infants have learned important properties of syntactic processing in utero, that can be seen in infant knowledge of native language vowels and the sequencing of heard multisyllabic phrases. This ability to sequence specific vowels gives newborn infants some of the fundamental mechanisms needed in order to learn the complex organization of a language.  From a neuroscientific perspective, there are neural correlates have been found that demonstrate human fetal learning of speech-like auditory stimulus that most other studies have been analyzing (Partanen et al., 2013). In a study conducted by Partanen et al. (2013), researchers presented fetuses with certain word variants and saw that these fetuses exhibited higher brain activity to the certain word variants compared to controls. In this same study, there was \"a significant correlation existed between the amount of prenatal exposure and brain activity, with greater activity being associated with a higher amount of prenatal speech exposure,\" pointing to the important learning mechanisms present before birth that is fine-tuned to features in speech (Partanen et al., 2013).",
            "score": 110.68455958366394
        },
        {
            "docid": "9736296_43",
            "document": "Linguistic performance . Unacceptable Sentences are ones which, although are grammatical, are not considered proper utterances. They are considered unacceptable due to the lack of our cognitive systems to process them. Speakers and listeners can be aided in the performance and processing of these sentences by eliminating time and memory constraints, increasing motivation to process these utterances and using pen and paper. In English there are three types of sentences that are grammatical but are considered unacceptable by speakers and listeners. When a speaker makes an utterance they must translate their ideas into words, then syntactically proper phrases with proper pronunciation. The speaker must have prior world knowledge and an understanding of the grammatical rules that their language enforces. When learning a second language or with children acquiring their first language, speakers usually have this knowledge before they are able to produce them. Their speech is usually slow and deliberate, using phrases they have already mastered, and with practice their skills increase. Errors of linguistic performance are judged by the listener giving many interpretations if an utterance is well-formed or ungrammatical depending on the individual. As well the context in which an utterance is used can determine if the error would be considered or not. When comparing \"Who must telephone her?\" and \"Who need telephone her?\" the former would be considered the ungrammatical phrase. However, when comparing it to \"Who want telephone her?\" it would be considered the grammatical phrase. The listener may also be the speaker. When repeating sentences with errors if the error is not comprehended then it is performed. As well if the speaker does notice the error in the sentence they are supposed to repeat they are unaware of the difference between their well-formed sentence and the ungrammatical sentence. An unacceptable utterance can also be performed due to a brain injury. Three types of brain injuries that could cause errors in performance were studied by Fromkin are dysarthria, apraxia and literal paraphasia. Dysarthria is a defect in the neuromuscular connection that involves speech movement. The speech organs involved can be paralyzed or weakened, making it difficult or impossible for the speaker to produce a target utterance. Apraxia is when there is damage to the ability to initiate speech sounds with no paralysis or weakening of the articulators. Literal paraphasia causes disorganization of linguistic properties, resulting in errors of word order of phonemes. Having a brain injury and being unable to perform proper linguistic utterances, some individuals are still able to process complex sentences and formulate syntactically well formed sentences in their mind. Child productions when they are acquiring language are full of errors of linguistic performance. Children must go from imitating adult speech to create new phrases of their own. They will need to use their cognitive operations of the knowledge of their language they are learning to determine the rules and properties of that language. The following are examples of errors in English speaking children's productions.",
            "score": 170.0981650352478
        },
        {
            "docid": "35182952_7",
            "document": "Embodied language processing . Experiential Trace Hypothesis states that each time an individual interacts with the world, traces of that particular experience are left in our brain. These traces can be accessed again when a person thinks of words or sentences that remind them of that experience. Additionally, these traces in our brain are linked to the action that they are related to. Words and sentences become those cues that retrieve these traces from our mind. Researchers have studied if the previous experience with a word, such as its location (up or down) in space, affects how people understand and then respond to that word. In one experiment, researchers hypothesized that if reading an object word also activates a location that is linked to that noun, then the following action response should be compatible with that association. They found that participants were faster to push a button higher than another button when the word was associated with being \"up\" or \"above\" than when the button was lower than the other for words associated with \"up\" and \"above\". The results of this study displayed that participants were faster to respond when the location of the word and the action they had to perform were similar. This demonstrates that language processing and action are connected. This research also found that the location information of a word is automatically activated after seeing the word. In a similar study, it was discovered that participants were equally as fast at responding to words that were associated with either an upward or downward location when the buttons to respond to these words were horizontal \u2013 meaning that the experiential trace effect was ruled out when the responding action did not link to either of the locations that were activated.",
            "score": 131.73172199726105
        },
        {
            "docid": "32018467_7",
            "document": "Christian Keysers . After finishing his master, Christian Keysers decided to concentrate on a subfield of cognitive neuroscience called social neuroscience that uses neuroscience methods to understand how we process the social world. He therefore performed his doctoral studies at the University of St Andrews with David Ian Perrett, one of the founding father of the field, to understand how the brain processes faces and facial expressions. This thesis work led to new insights into how quickly the brain can process the faces of others. During this period, Keysers became fascinated with the question of how the brain can attach meaning to the faces of others. How is it for instance, that we understand that a certain grimace would signal that another person is happy? How do we understand that a certain bodily movement towards a glass indicates that the other person aims to grasp a glass? In 1999, Keysers was exposed to a visit of Vittorio Gallese, who presented his recent discovery of mirror neurons in the Psychology department lecture series. This deeply influenced Keysers who decided to move to the lab of Giacomo Rizzolatti to undertake further studies on how these fascinating neurons could contribute to social perception. In 2000, after finishing his doctorate, Christian Keysers moved to the University of Parma to study mirror neurons. In early work there demonstrated that mirror neurons in the premotor cortex not only respond to the sight of actions, but also when actions can only be deduced or heard, leading to a publication in the journal \"Science\". This work had tremendous impact on the field, as it suggested that the premotor cortex could play a central, modality independent role in perception and may lay the origin for the evolution of speech in humans.  Together this work indicated that brain regions involved in our own actions play a role in how we process the actions of others. Keysers wondered whether a similar principle may underlie how we process the tactile sensations and emotions of others, and became increasingly independent of the research focus on the motor system in Parma. At the time, Keysers had also met his to be wife, Valeria Gazzola, a biologist in the final phases of her studies, and together they decided to explore if the somatosensory system might be involved in perceiving the sensations of others. Via a fruitful collaboration with the French neuroimaging specialist Bruno Wicker, they used functional magnetic resonance imaging, and showed for the first time, that the secondary somatosensory cortex, previously thought only to represent a persons own experiences of touch, is also activated when seeing someone or something else be touched. They also showed that the insula, thought only to respond to the experience of first-hand emotions, was also activated when we see another individual experience similar emotions. Together this indicated a much more general principle than the original mirror neuron theory, in which people process the actions, sensations and emotions of others by vicariously activating owns own actions, sensations and emotions. Jointly, this work laid the foundation of the neuroscientific investigation of empathy.",
            "score": 103.62687504291534
        },
        {
            "docid": "5366050_42",
            "document": "Speech perception . Research into the relationship between music and cognition is an emerging field related to the study of speech perception. Originally it was theorized that the neural signals for music were processed in a specialized \"module\" in the right hemisphere of the brain. Conversely, the neural signals for language were to be processed by a similar \"module\" in the left hemisphere. However, utilizing technologies such as fMRI machines, research has shown that two regions of the brain traditionally considered exclusively to process speech, Broca's and Wernicke's areas, also become active during musical activities such as listening to a sequence of musical chords. Other studies, such as one performed by Marques et al. in 2006 showed that 8-year-olds who were given six months of musical training showed an increase in both their pitch detection performance and their electrophysiological measures when made to listen to an unknown foreign language.",
            "score": 86.71974098682404
        },
        {
            "docid": "389579_2",
            "document": "Cognitive neuropsychology . Cognitive neuropsychology is a branch of cognitive psychology that aims to understand how the structure and function of the brain relates to specific psychological processes. Cognitive psychology is the science that looks at how mental processes are responsible for our cognitive abilities to store and produce new memories, produce language, recognize people and objects, as well as our ability to reason and problem solve. Cognitive neuropsychology places a particular emphasis on studying the cognitive effects of brain injury or neurological illness with a view to inferring models of normal cognitive functioning. Evidence is based on case studies of individual brain damaged patients who show deficits in brain areas and from patients who exhibit double dissociations. Double dissociations involve two patients and two tasks. One patient is impaired at one task but normal on the other, while the other patient is normal on the first task and impaired on the other. For example, patient A would be poor at reading printed words while still being normal at understanding spoken words, while the patient B would be normal at understanding written words and be poor at understanding spoken words. Scientists can interpret this information to explain how there is a single cognitive module for word comprehension. From studies like these, researchers infer that different areas of the brain are highly specialised. Cognitive neuropsychology can be distinguished from cognitive neuroscience, which is also interested in brain damaged patients, but is particularly focused on uncovering the neural mechanisms underlying cognitive processes.",
            "score": 114.70670199394226
        },
        {
            "docid": "10042066_2",
            "document": "Developmental linguistics . Developmental linguistics is the study of the development of linguistic ability in an individual, particularly the acquisition of language in childhood. It involves research into the different stages in language acquisition, language retention, and language loss in both first and second languages, in addition to the area of bilingualism. Before infants can speak, the neural circuits in their brains are constantly being influenced by exposure to language. The neurobiology of language contains a \"critical period\" in which children are most sensitive to language. The different aspects of language have varying \"critical periods\". Studies show that the critical period for phonetics is toward the end of the first year. At 18 months, a toddler's vocabulary vastly expands. The critical period for syntactic learning is 18-36 months. Infants of different mother languages can be differentiated at the age of 10 months. At 20 weeks they begin vocal imitation. Beginning when babies are about 12 months, they take on computational learning and social learning. Social interactions for infants and toddlers is important because it helps associate \"perception and action\". In-person social interaction rather than audio or video better facilitates learning in babies because they learn from how other people respond to them, especially their mothers. Babies have to learn to mimic certain syllables, which takes practice in manipulating tongue and lip movement. Sensory-motor learning in speech is linked to exposure to speech, which is very sensitive to language. Infants exposed to Spanish exhibit a different vocalization than infants exposed to English. One study took infants that were learning English and made them listen to Spanish in 12 sessions. The result showed consequent alterations in their vocalization, which demonstrated Spanish prosody.  One study used MEG to record activation in the brains of newborns, 6 months olds and 12 months olds while presenting them with syllables, harmonics and non-speech sounds. For the 6 month and 12 month old, the auditory and motor areas responded to speech. The newborn showed auditory activation but not motor activation. Another study presented 3 month olds with sentences and recorded their brain activity via fMRI motor speech areas did activate. These studies suggest that the link between perception and action begins to develop at 3 months. When babies are young, they are actually the most sensitive to distinguishing all phonetic units. During an infant\u2019s 1st year of life, they have to differentiate between about 40 phonetic units. When they are older they have usually been exposed to their native language so much that they lose this ability and can only distinguish phonetic units in their native language. Even at 12 months babies exhibit a deficit in differentiated non-native sounds. However, their ability to distinguish sounds in their native language continues to improve and become more fine-tuned. For example, Japanese learning infants learn that there is no differentiation between /r/ and /l/. However, in English, \"rake\" and \"lake\" are two different words. Japanese babies eventually lose their ability to distinguish between /r/ and /l/. Similarly, a Spanish learning infant cannot form words until they learn the difference between works like \"bano\" and \"pano\", because the /p/ sound is different than the /b/ sound. English learning babies do not learn to differentiate between the two.",
            "score": 122.30773782730103
        },
        {
            "docid": "524233_11",
            "document": "Brodmann area 45 . When reading aloud, people must decode written language to decipher its pronunciation. This processing takes place in Broca's area. The reader might use previous knowledge of a word in order to correctly vocalize it, or the reader might use knowledge of systematic letter combinations, which represent corresponding phonemes. Scientists can learn about what the brain is doing while people process language by looking at what it does with errors in language. As above, scientists can investigate the extra processing that occurs when people are challenged with a problem. In this case, scientists took advantage of the way pseudo-words and exception words by examining the brain as it interprets these problematic words. When people process language, they use different parts of Broca's area for different things. Pars triangularis is involved in a specific type of language processing. Specifically, pars triangularis becomes activated when people read exception words, which are words with atypical spelling-to-sound relationships. For example, \"have\" is an exception word because it is pronounced with a short \"a\", which is contrary to grammatical rules of pronunciation. The \"e\" at the end of the word should lead to the pronunciation of the long \"a\" sound, as in \"cave\" or \"rave\". Because we are so familiar with the word \"have\", we are able to remember its pronunciation, and we don't have to think through the rules each time we read it. Pars triangularis helps us do that.",
            "score": 158.4290143251419
        },
        {
            "docid": "35543733_7",
            "document": "Bilingual lexicon . With years of researches, how languages are stored and processed by bilinguals is still a main theme that many psycholinguists. One main topic is that bilinguals possess one or two internal lexicons, and even more with three stores. One for each language and the third one is for corresponding two languages. Reaction time of recognizing words in different languages is the most used method to figure out how our lexicon been activated. Researches in 1980s by Soares and Grosjean on English-Portuguese bilingual had two main findings. One is that although bilingual can access real words in English as quickly as English monolinguals, but they are slower at responding to non-words. The other finding is that bilingual took longer to access code-switched words than they did base-language words in the monolingual mode. These two findings can be seen as the evidence for more than one lexicon are existed in bilinguals' brains. As technology develops, functional magnetic resonance imaging (fMRI) is also used to study how brain activity is different in bilinguals' brain when both language are interact. Imaging studies have yielded that specific brain areas are involved in bilingual switching, which means this part of the brain can be said as the \"third lexicon\", the interconnected part of two lexicons for each language, where stores the guest words. Other research suggests only one combined lexicon is exists.",
            "score": 132.1259649991989
        },
        {
            "docid": "620396_42",
            "document": "Origin of language . In humans, functional MRI studies have reported finding areas homologous to the monkey mirror neuron system in the inferior frontal cortex, close to Broca's area, one of the language regions of the brain. This has led to suggestions that human language evolved from a gesture performance/understanding system implemented in mirror neurons. Mirror neurons have been said to have the potential to provide a mechanism for action-understanding, imitation-learning, and the simulation of other people's behavior. This hypothesis is supported by some cytoarchitectonic homologies between monkey premotor area F5 and human Broca's area. Rates of vocabulary expansion link to the ability of children to vocally mirror non-words and so to acquire the new word pronunciations. Such speech repetition occurs automatically, quickly and separately in the brain to speech perception. Moreover, such vocal imitation can occur without comprehension such as in speech shadowing and echolalia. Further evidence for this link comes from a recent study in which the brain activity of two participants was measured using fMRI while they were gesturing words to each other using hand gestures with a game of charades\u2014a modality that some have suggested might represent the evolutionary precursor of human language. Analysis of the data using Granger Causality revealed that the mirror-neuron system of the observer indeed reflects the pattern of activity of in the motor system of the sender, supporting the idea that the motor concept associated with the words is indeed transmitted from one brain to another using the mirror system.",
            "score": 125.17442238330841
        },
        {
            "docid": "1764639_17",
            "document": "Levels-of-processing effect . Several brain imaging studies using positron emission tomography and functional magnetic resonance imaging techniques have shown that higher levels of processing correlate with more brain activity and activity in different parts of the brain than lower levels. For example, in a lexical analysis task, subjects showed activity in the left inferior prefrontal cortex only when identifying whether the word represented a living or nonliving object, and not when identifying whether or not the word contained an \"a\". Similarly, an auditory analysis task showed increased activation in the left inferior prefrontal cortex when subjects performed increasingly semantic word manipulations. Synaptic aspects of word recognition have been correlated with the left frontal operculum and the cortex lining the junction of the inferior frontal and inferior precentral sulcus. The self-reference effect also has neural correlates with a region of the medial prefrontal cortex, which was activated in an experiment where subjects analyzed the relevance of data to themselves. Specificity of processing is explained on a neurological basis by studies that show brain activity in the same location when a visual memory is encoded and retrieved, and lexical memory in a different location. Visual memory areas were mostly located within the bilateral extrastriate visual cortex.",
            "score": 107.35520946979523
        },
        {
            "docid": "23060403_20",
            "document": "Motor theory of speech perception . The motor theory of speech perception faces the problem that the research linking speech perception to speech production is also consistent with the brain processing speech to imitate spoken words. The brain must have a means to do this if language is to exist, since a child's vocabulary expansion requires a means to learn novel spoken words, as does an adult's picking up of new names. Imitation has to be initiated for all vocalizations since a word's novelty cannot be known until after it is heard, and so after when the information needed to identify its articulation gestures and motor goals has gone. As result vocal imitation needs to be initiated by default into short term memory for every heard spoken vocalizations. If speech perception uses multiple sources of information, this default imitation processing would provide as a secondary use an extra source for word perception. Since imitation will be most needed for vocalizations that are not proper words, this could explain why sublexical tasks that do not use proper words so strongly link to processing of motor gestures.",
            "score": 105.39489448070526
        },
        {
            "docid": "12469094_5",
            "document": "Andrea Moro . As for the other field, he explored the neurological correlates of artificial languages which do not follow the principles of Universal Grammar providing evidence that Universal Grammar properties cannot be cultural, social or conventional artifacts: in fact, he and the team of people he worked with showed that recursive syntactic rules, that is rules based on recursion selectively activate a neurological network (including Broca's area) whereas non-recursive syntactic rules do not. These discoveries have appeared in a few international Journals, including, for example, Nature Neuroscience (Musso, Moro et al. 2003) or PNAS (Moro 2010): a comprehensive collection of the works in both fields has now become available in the \"Routledge Leading Linguist Series\" as \"The Equilibrium of Human Syntax\" (Routledge 2013). He also explored the correlates between the representation of the world in the brain and the structure of syntax, specifically the relationship between sentential negation and the brain) also available in Moro 2013. In recent papers he took position against the idea that the sequence of human actions can be described as having the same structure as the sequence of words in a well-formed syntactic structure. Furthermore, Moro pursued the study of the relationship between the brain and language by exploiting electrophysiological measure. The core of the experiment - done in a team with neurosurgeons and electric engineers - consists in comparing the shape of the electric waves of non-acoustic language areas (typically, Broca's area) with the shape of the corresponding sound waves. The result was that not only the shape of the two different waves correlate but they do so also in absence of sound production, that is during inner speech activity, opening the possibility to reading linguistic expression from direct measure of the cortex and skipping the actual utterance of the sentence. For a non technical synthesis of these discoveries and a critical discussion see \"Impossible Languages\" which received the honourable mention at the Prose Awards .",
            "score": 116.55264055728912
        },
        {
            "docid": "1095131_20",
            "document": "Kinesthetic learning . The cerebral cortex is the brain tissue covering the top and sides of the brain in most vertebrates. It is involved in storing and processing of sensory inputs and motor outputs. In the human brain, the cerebral cortex is actually a sheet of neural tissue about 1/8th inch thick. The sheet is folded so that it can fit inside the skull. The neural circuits in this area of the brain expand with practice of an activity, just like the synaptic plasticity grows with practice. Clarification of some of the mechanisms of learning by neuro science has been advanced, in part, by the advent of non-invasive imaging technologies, such as positron emission tomography (PET) and functional magnetic resonance imaging (FMRI). These technologies have allowed researchers to observe human learning processes directly. Through these types of technologies, we are now able to see and study what happens in the process of learning. In different tests performed the brain being imaged showed a greater blood flow and activation to that area of the brain being stimulated through different activities such as finger tapping in a specific sequence. It has been revealed that the process at the beginning of learning a new skill happens quickly, and later on slows down to almost a plateau. This process can also be referred to as The Law of Learning. The slower learning showed in the FMRI that in the cerebral cortex this was when the long term learning was occurring, suggesting that the structural changes in the cortex reflect the enhancement of skill memories during later stages of training. When a person studies a skill for a longer duration of time, but in a shorter amount of time they will learn quickly, but also only retain the information into their short-term memory. Just like studying for an exam; if a student tries to learn everything the night before, it will not stick in the long run. If a person studies a skill for a shorter duration of time, but more frequently and long-term, their brain will retain this information much longer as it is stored in the long-term memory. Functional and structural studies of the brain have revealed a vast interconnectivity between diverse regions of the cerebral cortex. For example, large numbers of axons interconnect the posterior sensory areas serving vision, audition, and touch with anterior motor regions. Constant communication between sensation and movement makes sense, because to execute smooth movement through the environment, movement must be continuously integrated with knowledge about one's surroundings obtained via sensory perception. The cerebral cortex plays a role in allowing humans to do this.",
            "score": 96.65765988826752
        },
        {
            "docid": "1061157_11",
            "document": "Dual-coding theory . Two different methods have been used to identify the regions involved in visual perception and visual imagery. First, functional magnetic resonance imaging (fMRI) is used to measure cerebral blood flow, which allows researchers to identify the amount of glucose and oxygen being consumed by a specific part of the brain, with an increase in blood flow providing a measure of brain activity. Second, an event related potential (ERP) can be used to show the amount of electrical brain activity that is occurring due to a particular stimulus. Researchers have used both methods to determine which areas of the brain are active with different stimuli, and results have supported the dual-coding theory. Other research has been done with positron emission tomography (PET) scans and fMRI to show that participants had improved memory for spoken words and sentences when paired with an image, imagined or real, and showed increased brain activation to process abstract words not easily paired with an image.",
            "score": 123.67423439025879
        },
        {
            "docid": "43527201_5",
            "document": "Usha Goswami . Dyslexia is a disorder in which the person affected has difficulty reading due to the reversal of letters in the brain that isn't linked to intelligence. In people with dyslexia, the brain processes certain signals in a specific way making it a very specific learning difficulty. Dr. Goswami's research is concerned with focusing on dyslexia as a language disorder rather than a visual disorder as she has found that the way that children with dyslexia hear language is slightly different than others. When sound waves approach the brain, they vary in pressure depending on the syllables within the words being spoken creating a rhythm. When these signals reach the brain they are lined up with speech rhythms and this process doesn't work properly in those with dyslexia. Goswami is currently researching whether or not reading poetry, nursery rhymes, and singing can be used to help children with dyslexia. The rhythm of the words could allow the child to match the syllable patterns to language before they begin reading as to catch them up to where children without the disability might be.",
            "score": 125.69549429416656
        },
        {
            "docid": "5505463_3",
            "document": "Bimodal bilingualism . Most modern neurological studies of bilingualism employ functional neuroimaging techniques to elucidate the neurological underpinnings of multilingualism and how multilingualism is beneficial to the brain. Neuroimaging and other neurological studies have demonstrated in recent years that multilingualism has a significant impact on the human brain. The mechanisms required by bilinguals to \"code switch\" (a linguistic term used to describe the rapid alternating between multiple languages within a conversation or discourse), not only demonstrate increased connectivity and density of the neural network in multilinguals, but also appear to provide protection against damage due to age and age-related pathologies, such as Alzheimer's. Multilingualism, especially bimodal multilingualism, can help slow to process of cognitive decline in aging. It is thought that this is a result of the increased work load that the executive system, housed mostly in the frontal cortex, must assume in order to successfully control the use of multiple languages at once. This means that the cortex must be more finely tuned, which results in a \"neural reserve\" that then has neuroprotective benefits. Gray matter volume (GMV) has been shown to be significantly preserved in bimodal bilinguals as compared to monolinguals in multiple brain areas, including the hippocampus, amygdala, anterior temporal lobes, and left insula. Similarly, neuroimaging studies that have compared monolinguals, unimodal bilinguals, and bimodal bilinguals provide evidence that deaf signers exhibit brain activation in patterns different than those of hearing signers, especially in regards to the left superior temporal sulcus. In deaf signers, activation of the superior temporal sulcus is highly lateralized to the left side during facial recognition tasks, while this lateralization was not present in hearing, bimodal signers. Bilinguals also require an effective and fast neural control system to allow them to select and control their languages even while code switching rapidly. Evidence indicates that the left caudate nucleus\u2014a centrally located brain feature that is near the thalamus and the basal ganglia\u2014is an important part of this mechanism, as bilinguals tend to have significantly increased GMV and activation in this region as compared to monolinguals, especially during active code switching tasks. As implied by the significant preservation of gray matter in the hippocampi (an area of the brain largely associated with memory consolidation and higher cognitive function, such as decision-making) of bimodal bilinguals, areas of the brain that help control phonological working memory tend to also have higher activation in those individuals who are proficient in two or more languages. There is also evidence that suggests that the age at which an individual acquires a second language may play a significant role in the varying brain functions associated with bilingualism. For example, individuals who acquired their second language early (before the age of 10) tend to have drastically different activation patterns than do late learners. However, late learners who achieve full proficiency in their second language tend to show similar patterns of activation during auditory tasks regardless of which language is being used, whereas early learners tend to activate different brain areas depending upon which language is being used. Along with the neuroprotective benefits that help to prevent onset of age-related cognitive issues such as dementia, bimodal bilinguals also experience a slightly different pattern of organization of language in the brain. While non-hearing-impaired bimodal bilinguals showed less parietal activation than deaf signers when asked to use only sign language, those same bimodal bilinguals demonstrated greater left parietal activation than did monolinguals. Parietal activation is not typically associated with language production bur rather with motor activity. Therefore, it is logical that bimodal bilinguals, when switching between speech- and sign-based language, stimulate their left parietal areas as a result of their increased need to combine both motor action and language production.",
            "score": 115.21679735183716
        },
        {
            "docid": "17524_35",
            "document": "Language . Early work in neurolinguistics involved the study of language in people with brain lesions, to see how lesions in specific areas affect language and speech. In this way, neuroscientists in the 19th century discovered that two areas in the brain are crucially implicated in language processing. The first area is Wernicke's area, which is in the posterior section of the superior temporal gyrus in the dominant cerebral hemisphere. People with a lesion in this area of the brain develop receptive aphasia, a condition in which there is a major impairment of language comprehension, while speech retains a natural-sounding rhythm and a relatively normal sentence structure. The second area is Broca's area, in the posterior inferior frontal gyrus of the dominant hemisphere. People with a lesion to this area develop expressive aphasia, meaning that they know what they want to say, they just cannot get it out. They are typically able to understand what is being said to them, but unable to speak fluently. Other symptoms that may be present in expressive aphasia include problems with fluency, articulation, word-finding, word repetition, and producing and comprehending complex grammatical sentences, both orally and in writing. Those with this aphasia also exhibit ungrammatical speech and show inability to use syntactic information to determine the meaning of sentences. Both expressive and receptive aphasia also affect the use of sign language, in analogous ways to how they affect speech, with expressive aphasia causing signers to sign slowly and with incorrect grammar, whereas a signer with receptive aphasia will sign fluently, but make little sense to others and have difficulties comprehending others' signs. This shows that the impairment is specific to the ability to use language, not to the physiology used for speech production.",
            "score": 138.95886826515198
        },
        {
            "docid": "33932515_22",
            "document": "Social cue . Benjamin Straube, Antonia Green, Andreas Jansen, Anjan Chatterjee, and Tilo Kircher found that social cues influence the neural processing of speech-gesture utterances. Past studies have focused on mentalizing as being a part of perception of social cues and it is believed that this process relies on the neural system, which consists of: When people focus on things in a social context, the medial prefrontal cortex and precuneus areas of the brain are activated, however when people focus on a non-social context there is no activation of these areas. Straube et al. hypothesized that the areas of the brain involved in mental processes were mainly responsible for social cue processing. It is believed that when iconic gestures are involved, the left temporal and occipital regions would be activated and when emblematic gestures were involved the temporal poles would be activated. When it came to abstract speech and gestures, the left frontal gyrus would be activated according to Straube et al. After conducting an experiment on how body position, speech and gestures affected activation in different areas of the brain Straube et al. came to the following conclusions:",
            "score": 82.4516863822937
        },
        {
            "docid": "26685741_42",
            "document": "Sleep and memory . Previous research has shown REM sleep to reactivate cortical neural assemblies post-training on a serial reaction time task (SRT), in other words REM sleep replays the processing that occurred while one learnt an implicit task in the previous waking hours. However, control subjects did not complete a SRT task, thus researchers could not assume the reactivation of certain networks to be a result of the implicitly learned sequence/grammar as it could simply be due to elementary visuomotor processing which was obtained in both groups. To answer this question the experiment was redone and another group was added who also took part in the SRT task. They experienced no sequence to the SRT task (random group), whereas the experimental group did experience a sequence (probabilistic group), although without conscious awareness. Results of PET scans indicate that bilateral cuneus were significantly more activated during SRT practice as well as post-training REM sleep in the Probabilistic group than the Random group. In addition, this activation was significantly increased during REM sleep versus the SRT task. This suggests that specific brain regions are specifically engaged in the post-processing of sequential information. This is further supported by the fact that regional CBF (rCBF) during post-training REM sleep are modulated by the level of high-order, but not low-order learning obtained prior to sleep. Therefore, brain regions that take part in a learning process are modulated by both the sequential structure of the learned material (increased activation in cuneus), and the amount of high-order learning (rCBF).",
            "score": 84.39295959472656
        },
        {
            "docid": "33827415_15",
            "document": "Unconscious cognition . Studies that test the way in which humans acquire language skills and learn how to apply the rules of grammar show that a large amount of language and grammar learning takes place unconsciously. Experiments were performed in which participants were asked to identify whether certain nonsensical and made up words belong to a group of words which they had been previously shown. Some participants were not informed that the word sets were based on rules. An analysis of their responses showed that the participants were more likely to associate words which were not shown previously as a part of the group if they followed the preset grammatical rules. This shows that it might not be necessary to be consciously aware of grammatical rules to know proper grammar. This theory might explain the feeling we undergo when we feel that a certain sentence structure is awkward or wrong even though we might not be able to clearly define the reason why the sentence is incorrect.",
            "score": 144.18525385856628
        },
        {
            "docid": "540571_8",
            "document": "Wernicke's area . Support for a broad range of speech processing areas was furthered by a recent study done at University of Rochester in which American Sign Language native speakers were subject to MRIs while interpreting sentences that identified a relationship using either syntax (relationship is determined by the word order) or inflection (relationship is determined by physical motion of \"moving hands through space or signing on one side of the body\"). Distinct areas of the brain were activated with the frontal cortex (associated with ability to put information into sequences) being more active in the syntax condition and the temporal lobes (associated with dividing information into its constituent parts) being more active in the inflection condition. However, these areas are not mutually exclusive and show a large amount of overlap. These findings imply that while speech processing is a very complex process, the brain may be using fairly basic, preexisting computational methods.",
            "score": 113.85760581493378
        },
        {
            "docid": "1790730_38",
            "document": "Syntactic Structures . In 2015, neuroscientists at New York University conducted experiments to verify if the human brain uses \"hierarchical structure building\" for processing languages. They measured the magnetic and electric activities in the brains of participants. The results showed that \"[human] brains distinctly tracked three components of the phrases they heard.\" This \"[reflected] a hierarchy in our neural processing of linguistic structures: words, phrases, and then sentences\u2014at the same time.\" These results bore out Chomsky's hypothesis in \"Syntactic Structures\" of an \"internal grammar mechanism\" inside the brain.",
            "score": 120.15211129188538
        },
        {
            "docid": "18614_3",
            "document": "Language acquisition . Linguists who are interested in child language acquisition for many years question how language is acquired, lidz et al. states \"The question of how these structures are acquired, then, is more properly understood as the question of how a learner takes the surface forms in the input and converts them into abstract linguistic rules and representations.\" So we know language acquisition involves structures, rules and representation. The capacity to successfully use language requires one to acquire a range of tools including phonology, morphology, syntax, semantics, and an extensive vocabulary. Language can be vocalized as in speech, or manual as in sign. Human language capacity is represented in the brain. Even though human language capacity is finite, one can say and understand an infinite number of sentences, which is based on a syntactic principle called recursion. Evidence suggests that every individual has three recursive mechanisms that allow sentences to go indeterminately. These three mechanisms are: \"relativization\", \"complementation\" and \"coordination\". Furthermore, there are actually two main guiding principles in first-language acquisition, that is, speech perception always precedes speech production and the gradually evolving system by which a child learns a language is built up one step at a time, beginning with the distinction between individual phonemes.",
            "score": 125.43236064910889
        },
        {
            "docid": "102958_11",
            "document": "Roger Wolcott Sperry . Working with his graduate student Michael Gazzaniga, Sperry invited several of the \"split-brain\" patients to volunteer to take part in his study to determine if the surgery affected their functioning. These tests were designed to test the patients' language, vision, and motor skills. When a person views something in the left visual field (that is on the left side of their body), the information travels to the right hemisphere of the brain and vice versa. In the first series of tests, Sperry would present a word to either the left or right visual field for a short period of time. If the word was shown to the right visual field, meaning the left hemisphere would process it, then the patient could report seeing the word. If the word was shown to the left visual field, meaning the right hemisphere would process it, then the patient could not report seeing the word. This led Sperry to believe that only the left side of the brain could articulate speech. However, in a follow-up experiment, Sperry discovered that the right hemisphere does have some language abilities. In this experiment, he had the patients place their left hands in a tray full of objects located under a partition so the patient would not be able to see the objects. Then a word was shown to the patient's left visual field, which was processed by the right side of the brain. This word described one of the objects in the tray, so the patient's left hand picked up the object corresponding to the word. When participants were asked about the word and the object in their hand, they claimed they had not seen the word and had no idea why they were holding the object. The right side of the brain had recognized the word and told the left hand to pick it up, but because the right side of the brain cannot speak and the left side of the brain had not seen the word, the patient could not articulate what they had seen.",
            "score": 101.9386979341507
        },
        {
            "docid": "25140_47",
            "document": "Perception . \"Speech perception\" is the process by which spoken languages are heard, interpreted and understood. Research in speech perception seeks to understand how human listeners recognize speech sounds and use this information to understand spoken language. The sound of a word can vary widely according to words around it and the tempo of the speech, as well as the physical characteristics, accent and mood of the speaker. Listeners manage to perceive words across this wide range of different conditions. Another variation is that reverberation can make a large difference in sound between a word spoken from the far side of a room and the same word spoken up close. Experiments have shown that people automatically compensate for this effect when hearing speech.",
            "score": 100.34909343719482
        },
        {
            "docid": "42616278_10",
            "document": "Charles Perfetti . Charles Perfetti and colleagues conducted a study called \"High Proficiency in a second Language is characterized by Greater Involvement of the First Language Network: Evidence from Chinese Learners of English\" to examine the processes of learning a second language. The first thing he does is talk about the assimilation and accommodation hypothesis that involve the process of learning a second language. The assimilation hypothesis argues that second language is learned through the brains access of networks used to process the native language. Accommodation hypothesis argues that learning of a second language depends on brain structures not involve in process of the native language. Two test these hypothesis, Perfetti and colleagues examined a group of Chinese speakers who happened to be late learners with various levels of proficiency in English. The experiment was divided into 3 groups. The (ce group), (cc group) and (ee group) consisted of Chinese speaking participants who performed an English word rhyming judgment task while fMRI was performed. Assimilation was analyzed by comparing the cc group to the ce group while accommodation was analyzed by comparing the (ee group). The study involved participants deciding whether two symbol patterns presented in sequence match or mismatched. Rhyming was seen as the same rhyme for the second character of the word while orthography was defined as having the same phonetic radical for the second character of the word.",
            "score": 101.76276433467865
        },
        {
            "docid": "236809_49",
            "document": "Recall (memory) . A key technique in improving and helping recall memory is to take advantage of Mnemonic devices and other cognitive strategies. Mnemonic devices are a type of cognitive strategy that enables individuals to memorize and recall new information in an easier fashion, rather than just having to remember a list of information that is not related to one another. An example of mnemonic devices are PEMDAS or Please Excuse My Dear Aunt Sally; this is a device for arithmetic when solving equations that have parenthesis, exponents, multiplication, division, addition, or subtraction and what order to do each calculation. Words or an acronym can stand for a process that individuals need to recall. The benefits of using these types of strategies to perform tasks are that encoding becomes more organized and it is easier to remember and process information. Also this device reduces the need of intentional resources at the point of retrieval, which means that recall does not need outside sources helping an individual remember what happened yesterday. Cognitive strategies can leverage semantic connections that will allow the brain to process and work more efficiently than just having to process the information as whole parts. By using the strategies the information becomes related to each other and the information sticks.  Another type of device people use to help their recall memory become efficient is chunking. Chunking is the process of breaking down numbers into smaller units to remember the information or data, this helps recall numbers and math facts. An example of this chunking process is a telephone number; this is chunked with three digits, three digits, then four digits. People read them off as such when reciting a phone number to another person. There has been research done about these techniques and an institution tested two groups of people to see if these types of devices work well for real people, the results came back determining a significant performance difference between the group who did not use cognitive strategies and the group who did. The group using the techniques immediately performed better than the other group and when taking a pre-test and post-test the results indicated that the group using the techniques improved while the other group did not.",
            "score": 128.2496017217636
        },
        {
            "docid": "368421_12",
            "document": "Garden path sentence . One way to determine the brain processes involved is the use of brain electrophysiology. Brain electrophysiology is used to study the impact of disfluencies in sentence processing by the brain, which specifically use event-related potentials (ERPs). ERPs are voltages generated by the brain that can be measured through a device placed on the scalp. It is also observed that specific components of the ERPs can be associated with the activation of different and specific linguistic processes of the brain.  Within ERPs, P600 is the most important component. Its activation occurs when the parser comes across a syntactic violation such as \"The broker persuaded to sell the stock\" or when the parser synthesizes an unsatisfactory disambiguation on an ambiguous string of words such as \"The doctor charged the patient was lying\". Hence the activation of P600 marks the parser's attempt to revise the sentence's structural mis-match or ambiguity. However it is also observed that the activation of P600 may be low or completely absent if the parser is asked to pay attention only to the semantic aspects of a sentence either through an explicit instruction or through the use of specific words as a way to force a semantic analysis of the sentence. The result of yet another study conducted by Osterhout in 1997 reveal that the activation of P600 varies with the parser's own attentions to the syntactic violations of the sentence.",
            "score": 116.44652318954468
        },
        {
            "docid": "8305_20",
            "document": "Dyslexia . In phonological dyslexia, sufferers can read familiar words but have difficulty with unfamiliar words, such as invented pseudo-words. Phonological dyslexia is associated with lesions in the parts of the brain supplied with blood by the middle cerebral artery. The superior temporal lobe is often also involved. Furthermore, dyslexics compensate by overusing a front-brain region called Broca's area, which is associated with aspects of language and speech. The Lindamood Phoneme Sequencing Program (LiPS) is used to treat phonological dyslexia. This system is based on a three-way sensory feedback process, using auditory, visual, and oral skills to learn to recognize words and word patterns. Case studies with a total of three patients found a significant improvement in spelling and reading ability after using LiPS.",
            "score": 96.34101152420044
        },
        {
            "docid": "28341040_5",
            "document": "Blue and Brown Books . While Wittgenstein in \"The Blue Book\" is not dogmatic nor systematic, he does provide arguments that point toward a more self-critical view of language. For example, he does not think that \"understanding\" and \"explaining\" are necessarily related. He suggests that when humans are learning a language-game they are actually being trained to understand it. He writes: \"If we are taught the meaning of the word 'yellow' by being given some sort of ostensive definition [in this case, \"ostensive\" means something like \"denoting a way of defining by direct demonstration, e.g., by pointing\"] (a rule of the usage of the word) this teaching can be looked at in two different ways: (a) The teaching is a drill. This drill causes us to associate a yellow image, yellow things, with the word 'yellow.' Thus when I gave the order 'Choose a yellow ball from this bag' the word 'yellow' might have brought up a yellow image, or a feeling of recognition when the person's eye fell on the yellow ball. The drill of teaching could in this case be said to have built up a psychical mechanism. This, however, would only be a hypothesis or else a metaphor. We could compare teaching with installing an electric connection between a switch and a bulb. The parallel to the connection going wrong or breaking down would then be what we call forgetting the explanation, or the meaning, of the word...[I]t is the hypothesis that the process of teaching should be needed in order to bring about these effects. It is conceivable, in this sense, that all the processes of understanding, obeying, etc., should have happened without the person ever having been taught the language; (b) The teaching may have supplied us with a rule which is itself involved in the processes of understanding, obeying, etc.: 'involved,' however, meaning that the expression of this rule forms part of these processes...\" As the citation suggests, Wittgenstein views understanding a language-game as being mostly concerned with \"training\" (which he calls \"drill[ing] in the above citation). Having said that, Wittgenstein is not one to believe that even understanding a language-game can be reduced to one process; like the plethora of language-games available to human beings, there are also plethora of \"understandings.\" For example, the \"understanding\" of a language may come about by the \"drilling\" of the association between the word \"yellow\" and a yellow-patch; or it may involve learning rules, like rules used in the game of chess. Moreover, Wittgenstein doesn't think that humans use language mechanically, as if following a calculus. He writes in \"The Blue Book\", \"[I]n general we don't use language according to strict rules--it hasn't been taught us by means of strict rules, either.\" Wittgenstein clarifies the problem of communicating using a human language when he discusses learning a language by \"ostensive defining.\" For example, if one wanted to teach someone that a pencil was called a \"pencil\" and pointed to a pencil and said, \"pencil,\" how does the listener know that what one is trying to convey is that the thing in front of me (e.g., the entire pencil) is called a \"pencil\"? Isn't it possible that the listener would associate \"pencil\" with \"wood\"? Maybe the listener would associate the word \"pencil\" with \"round\" instead (as pencils are, usually, in fact, round!). Wittgenstein writes regarding several possible \"interpretations\" which may arise after such a lesson. The student may interpret your pointing at a pencil and saying \"pencil\" to mean the following: (1) This is a pencil; (2) This is round; (3) This is wood; (4) This is one; (5) This is hard, etc., etc.",
            "score": 142.64404571056366
        },
        {
            "docid": "14158261_5",
            "document": "Temporoparietal junction . The left temporoparietal junction (lTPJ) contains both Wernicke's area and the angular gyrus, both prominent anatomical structures of the brain that are involved in language cognition, processing, and comprehension of both written and spoken language. This is the region of the brain wherein \u201cMentalese\u201d, a term coined by Steven Pinker to explain the brain\u2019s language that translates itself into written and spoken language, is processed. According to Pinker, \u201cknowing a language is knowing how to translate Mentalese into a string of words and vice versa.\u201d The lTPJ succeeds in this matter by taking in observations from external environments, such as conversations, making connections in the brain regarding past memories or incidents and then converting those thoughts and connections to written and spoken language. Pinker explains this in detail in The Language Instinct: How the Mind Creates Language. The lTPJ also plays an important role in reasoning of other\u2019s beliefs, intentions, and desires. Activation of the lTPJ was observed in patients processing mental states such as beliefs when an fMRI was used on patients as they were asked to make inferences regarding the mental states of others such as lying. This study was further supplemented by a study which identified that lesions to the left TPJ can impair cognitive processes specifically involved in the inference of someone else's belief, intention, or desire. Individuals with lesions in the lTPJ were no longer able to correctly identify when someone was lying or insinuating a false sense of belief or desire. The lTPJ is also involved in the processing of associating and remembering the names of individuals and objects.",
            "score": 89.29306614398956
        }
    ],
    "r": [
        {
            "docid": "9736296_43",
            "document": "Linguistic performance . Unacceptable Sentences are ones which, although are grammatical, are not considered proper utterances. They are considered unacceptable due to the lack of our cognitive systems to process them. Speakers and listeners can be aided in the performance and processing of these sentences by eliminating time and memory constraints, increasing motivation to process these utterances and using pen and paper. In English there are three types of sentences that are grammatical but are considered unacceptable by speakers and listeners. When a speaker makes an utterance they must translate their ideas into words, then syntactically proper phrases with proper pronunciation. The speaker must have prior world knowledge and an understanding of the grammatical rules that their language enforces. When learning a second language or with children acquiring their first language, speakers usually have this knowledge before they are able to produce them. Their speech is usually slow and deliberate, using phrases they have already mastered, and with practice their skills increase. Errors of linguistic performance are judged by the listener giving many interpretations if an utterance is well-formed or ungrammatical depending on the individual. As well the context in which an utterance is used can determine if the error would be considered or not. When comparing \"Who must telephone her?\" and \"Who need telephone her?\" the former would be considered the ungrammatical phrase. However, when comparing it to \"Who want telephone her?\" it would be considered the grammatical phrase. The listener may also be the speaker. When repeating sentences with errors if the error is not comprehended then it is performed. As well if the speaker does notice the error in the sentence they are supposed to repeat they are unaware of the difference between their well-formed sentence and the ungrammatical sentence. An unacceptable utterance can also be performed due to a brain injury. Three types of brain injuries that could cause errors in performance were studied by Fromkin are dysarthria, apraxia and literal paraphasia. Dysarthria is a defect in the neuromuscular connection that involves speech movement. The speech organs involved can be paralyzed or weakened, making it difficult or impossible for the speaker to produce a target utterance. Apraxia is when there is damage to the ability to initiate speech sounds with no paralysis or weakening of the articulators. Literal paraphasia causes disorganization of linguistic properties, resulting in errors of word order of phonemes. Having a brain injury and being unable to perform proper linguistic utterances, some individuals are still able to process complex sentences and formulate syntactically well formed sentences in their mind. Child productions when they are acquiring language are full of errors of linguistic performance. Children must go from imitating adult speech to create new phrases of their own. They will need to use their cognitive operations of the knowledge of their language they are learning to determine the rules and properties of that language. The following are examples of errors in English speaking children's productions.",
            "score": 170.09815979003906
        },
        {
            "docid": "524233_11",
            "document": "Brodmann area 45 . When reading aloud, people must decode written language to decipher its pronunciation. This processing takes place in Broca's area. The reader might use previous knowledge of a word in order to correctly vocalize it, or the reader might use knowledge of systematic letter combinations, which represent corresponding phonemes. Scientists can learn about what the brain is doing while people process language by looking at what it does with errors in language. As above, scientists can investigate the extra processing that occurs when people are challenged with a problem. In this case, scientists took advantage of the way pseudo-words and exception words by examining the brain as it interprets these problematic words. When people process language, they use different parts of Broca's area for different things. Pars triangularis is involved in a specific type of language processing. Specifically, pars triangularis becomes activated when people read exception words, which are words with atypical spelling-to-sound relationships. For example, \"have\" is an exception word because it is pronounced with a short \"a\", which is contrary to grammatical rules of pronunciation. The \"e\" at the end of the word should lead to the pronunciation of the long \"a\" sound, as in \"cave\" or \"rave\". Because we are so familiar with the word \"have\", we are able to remember its pronunciation, and we don't have to think through the rules each time we read it. Pars triangularis helps us do that.",
            "score": 158.42901611328125
        },
        {
            "docid": "524233_15",
            "document": "Brodmann area 45 . There is a difference between the processing patterns of primary and secondary languages in processing of passive sentences. These are sentences using some form of the verb \"be\" with a verb in the past participle form. For example, \"He is ruined\" is a passive sentence because the verb \"ruin\" is in the past participle form and used with \"is\", which is a form of the verb \"be\". This study shows that processing this sentence, late bilinguals used their pars triangularis much more than their counterparts. This result implies certain things about the way language is learned. For example, It has been suggested that the reason people often have such difficulty learning foreign languages during adulthood is that their brains are trying to code language information in a region of the brain that is not dedicated to understanding language. According to this view, this is the reason native speakers are able to speak so quickly while their late-bilingual counterparts are forced to stutter as they struggle to process grammatical rules.",
            "score": 152.28712463378906
        },
        {
            "docid": "3630374_3",
            "document": "Neural computation . When comparing the three main traditions of the computational theory of mind, as well as the different possible forms of computation in the brain, it is helpful to define what we mean by computation in a general sense. Computation is the processing of vehicles, otherwise known as variables or entities, according to a set of rules. A rule in this sense is simply an instruction for executing a manipulation on the current state of the variable, in order to produce an specified output. In other words, a rule dictates which output to produce given a certain input to the computing system. A computing system is a mechanism whose components must be functionally organized to process the vehicles in accordance with the established set of rules. The types of vehicles processed by a computing system determines which type of computations it performs. Traditionally, in cognitive science there have been two proposed types of computation related to neural activity - digital and analog, with the vast majority of theoretical work incorporating a digital understanding of cognition. Computing systems which perform digital computation are functionally organized to execute operations on strings of digits with respect to the type and location of the digit on the string. It has been argued that neural spike train signaling implements some form of digital computation, since neural spikes may be considered as discrete units or digits, like 0 or 1 - the neuron either fires an action potential or it does not. Accordingly, neural spike trains could be seen as strings of digits. Alternatively, analog computing systems perform manipulations on non-discrete, irreducibly continuous variables, that is, entities which vary continuously as a function of time. These sorts of operations are characterized by systems of differential equations.",
            "score": 144.6082763671875
        },
        {
            "docid": "33827415_15",
            "document": "Unconscious cognition . Studies that test the way in which humans acquire language skills and learn how to apply the rules of grammar show that a large amount of language and grammar learning takes place unconsciously. Experiments were performed in which participants were asked to identify whether certain nonsensical and made up words belong to a group of words which they had been previously shown. Some participants were not informed that the word sets were based on rules. An analysis of their responses showed that the participants were more likely to associate words which were not shown previously as a part of the group if they followed the preset grammatical rules. This shows that it might not be necessary to be consciously aware of grammatical rules to know proper grammar. This theory might explain the feeling we undergo when we feel that a certain sentence structure is awkward or wrong even though we might not be able to clearly define the reason why the sentence is incorrect.",
            "score": 144.1852569580078
        },
        {
            "docid": "179092_22",
            "document": "Neurolinguistics . Many studies in neurolinguistics take advantage of anomalies or \"violations\" of syntactic or semantic rules in experimental stimuli, and analyzing the brain responses elicited when a subject encounters these violations. For example, sentences beginning with phrases such as *\"the garden was on the worked\", which violates an English phrase structure rule, often elicit a brain response called the early left anterior negativity (ELAN). Violation techniques have been in use since at least 1980, when Kutas and Hillyard first reported ERP evidence that semantic violations elicited an N400 effect. Using similar methods, in 1992, Lee Osterhout first reported the P600 response to syntactic anomalies. Violation designs have also been used for hemodynamic studies (fMRI and PET): Embick and colleagues, for example, used grammatical and spelling violations to investigate the location of syntactic processing in the brain using fMRI. Another common use of violation designs is to combine two kinds of violations in the same sentence and thus make predictions about how different language processes interact with one another; this type of crossing-violation study has been used extensively to investigate how syntactic and semantic processes interact while people read or hear sentences.",
            "score": 143.83468627929688
        },
        {
            "docid": "16571023_6",
            "document": "Innateness hypothesis . In his argument for the existence of a LAD, Chomsky proposed that for a child to acquire a language, sufficient innate language-specific knowledge is needed. These constraints were later termed as universal grammar (UG). In this theory, it is suggested that all humans have a set of limited rules for grammar that are universal to all natural human languages. These rules are genetically wired into our brains and they can be altered in correspondence to the language children are exposed to. In other words, under this theory, language acquisition is seen as a process of filtering through the set of possible grammatical structures in natural languages pre-programmed in one's mind and this is guided by the language input in one's environment. Chomsky later introduced generative grammar. He argued that \"properties of a generative grammar arise from an \"innate\" universal grammar\". This theory of generative grammar describes a set of rules that are used to order words correctly in order to form grammatically-sound sentences. It also attempts to describe a speaker's innate grammatical knowledge.",
            "score": 143.76959228515625
        },
        {
            "docid": "28341040_5",
            "document": "Blue and Brown Books . While Wittgenstein in \"The Blue Book\" is not dogmatic nor systematic, he does provide arguments that point toward a more self-critical view of language. For example, he does not think that \"understanding\" and \"explaining\" are necessarily related. He suggests that when humans are learning a language-game they are actually being trained to understand it. He writes: \"If we are taught the meaning of the word 'yellow' by being given some sort of ostensive definition [in this case, \"ostensive\" means something like \"denoting a way of defining by direct demonstration, e.g., by pointing\"] (a rule of the usage of the word) this teaching can be looked at in two different ways: (a) The teaching is a drill. This drill causes us to associate a yellow image, yellow things, with the word 'yellow.' Thus when I gave the order 'Choose a yellow ball from this bag' the word 'yellow' might have brought up a yellow image, or a feeling of recognition when the person's eye fell on the yellow ball. The drill of teaching could in this case be said to have built up a psychical mechanism. This, however, would only be a hypothesis or else a metaphor. We could compare teaching with installing an electric connection between a switch and a bulb. The parallel to the connection going wrong or breaking down would then be what we call forgetting the explanation, or the meaning, of the word...[I]t is the hypothesis that the process of teaching should be needed in order to bring about these effects. It is conceivable, in this sense, that all the processes of understanding, obeying, etc., should have happened without the person ever having been taught the language; (b) The teaching may have supplied us with a rule which is itself involved in the processes of understanding, obeying, etc.: 'involved,' however, meaning that the expression of this rule forms part of these processes...\" As the citation suggests, Wittgenstein views understanding a language-game as being mostly concerned with \"training\" (which he calls \"drill[ing] in the above citation). Having said that, Wittgenstein is not one to believe that even understanding a language-game can be reduced to one process; like the plethora of language-games available to human beings, there are also plethora of \"understandings.\" For example, the \"understanding\" of a language may come about by the \"drilling\" of the association between the word \"yellow\" and a yellow-patch; or it may involve learning rules, like rules used in the game of chess. Moreover, Wittgenstein doesn't think that humans use language mechanically, as if following a calculus. He writes in \"The Blue Book\", \"[I]n general we don't use language according to strict rules--it hasn't been taught us by means of strict rules, either.\" Wittgenstein clarifies the problem of communicating using a human language when he discusses learning a language by \"ostensive defining.\" For example, if one wanted to teach someone that a pencil was called a \"pencil\" and pointed to a pencil and said, \"pencil,\" how does the listener know that what one is trying to convey is that the thing in front of me (e.g., the entire pencil) is called a \"pencil\"? Isn't it possible that the listener would associate \"pencil\" with \"wood\"? Maybe the listener would associate the word \"pencil\" with \"round\" instead (as pencils are, usually, in fact, round!). Wittgenstein writes regarding several possible \"interpretations\" which may arise after such a lesson. The student may interpret your pointing at a pencil and saying \"pencil\" to mean the following: (1) This is a pencil; (2) This is round; (3) This is wood; (4) This is one; (5) This is hard, etc., etc.",
            "score": 142.64404296875
        },
        {
            "docid": "17524_35",
            "document": "Language . Early work in neurolinguistics involved the study of language in people with brain lesions, to see how lesions in specific areas affect language and speech. In this way, neuroscientists in the 19th century discovered that two areas in the brain are crucially implicated in language processing. The first area is Wernicke's area, which is in the posterior section of the superior temporal gyrus in the dominant cerebral hemisphere. People with a lesion in this area of the brain develop receptive aphasia, a condition in which there is a major impairment of language comprehension, while speech retains a natural-sounding rhythm and a relatively normal sentence structure. The second area is Broca's area, in the posterior inferior frontal gyrus of the dominant hemisphere. People with a lesion to this area develop expressive aphasia, meaning that they know what they want to say, they just cannot get it out. They are typically able to understand what is being said to them, but unable to speak fluently. Other symptoms that may be present in expressive aphasia include problems with fluency, articulation, word-finding, word repetition, and producing and comprehending complex grammatical sentences, both orally and in writing. Those with this aphasia also exhibit ungrammatical speech and show inability to use syntactic information to determine the meaning of sentences. Both expressive and receptive aphasia also affect the use of sign language, in analogous ways to how they affect speech, with expressive aphasia causing signers to sign slowly and with incorrect grammar, whereas a signer with receptive aphasia will sign fluently, but make little sense to others and have difficulties comprehending others' signs. This shows that the impairment is specific to the ability to use language, not to the physiology used for speech production.",
            "score": 138.95887756347656
        },
        {
            "docid": "37691878_18",
            "document": "Phonemic restoration effect . Because languages are distinctly structured, the brain has some sense of what word is to come next in a proper sentence. When listeners were listening to sentences with proper structure with missing phonemes, they performed much better than with a nonsensical sentence without a proper structure. This comes from the predictive nature of the pre-frontal cortex in determining what word should be coming next in order for the sentence to make sense. Top-down processing relies on the surrounding information in a sentence to fill in the missing information. If the sentence does not make sense to the observer then there will be little at the top of the process for the observer to go off of. If a puzzle piece of a familiar picture was missing, it would be very simple for the brain to know what that puzzle piece would look like. If the picture of something that makes no sense to the human brain and has never been seen before, the brain will have much more difficulty understanding what is missing.",
            "score": 138.65550231933594
        },
        {
            "docid": "6352447_12",
            "document": "Artificial grammar learning . The mechanism behind the implicit learning that is hypothesized to occur while people engage in artificial grammar learning is statistical learning or, more specifically, Bayesian learning. Bayesian learning takes into account types of biases or \"prior probability distributions\" individuals have that contribute to the outcome of implicit learning tasks. These biases can be thought of as a probability distribution that contains the probability that each possible hypothesis is likely to be correct. Due to the structure of the Bayesian model, the inferences output by the model are in the form of a probability distribution rather than a single most probable event. This output distribution is a \"posterior probability distribution\". The posterior probability of each hypothesis in the original distribution is the probability of the hypothesis being true given the data and the probability of data given the hypothesis is true. This Bayesian model for learning is fundamental for understanding the pattern detection process involved in implicit learning and, therefore, the mechanisms that underlie the acquisition of artificial grammar learning rules. It is hypothesized that the implicit learning of grammar involves predicting co-occurrences of certain words in a certain order. For example, \"the dog chased the ball\" is a sentence that can be learned as grammatically correct on an implicit level due to the high co-occurrence of \"chase\" being one of the words to follow \"dog\". A sentence like \"the dog cat the ball\" is implicitly recognized as grammatically incorrect due to the lack of utterances that contain those words paired in that specific order. This process is important for teasing apart thematic roles and parts of speech in grammatical processing (see grammar). While the labeling of the thematic roles and parts of speech is explicit, the identification of words and parts of speech is implicit.",
            "score": 138.2832794189453
        },
        {
            "docid": "3136742_12",
            "document": "Lexis (linguistics) . The study of corpus linguistics provides us with many insights into the real nature of language, as shown above. In essence, the lexicon seems to be built on the premise that language use is best approached as an assembly process, whereby the brain links together ready-made chunks. Intuitively this makes sense: it is a natural short-cut to alleviate the burden of having to \"re-invent the wheel\" every time we speak. Additionally, using well-known expressions conveys loads of information rapidly, as the listener does not need to break down an utterance into its constituent parts. In \"Words and Rules\", Steven Pinker shows this process at work with regular and irregular verbs: we collect the former, which provide us with rules we can apply to unknown words (for example, the \"\u2011ed\" ending for past tense verbs allows us to decline the neologism \"to google\" into \"googled\"). Other patterns, the irregular verbs, we store separately as unique items to be memorized.",
            "score": 137.2103271484375
        },
        {
            "docid": "638633_26",
            "document": "Dictionary-based machine translation . Syntax (sin-taks) noun = a. the study of the rules for the formation of grammatical sentences in a language; b. the study of the patterns of formation of sentences and phrases from words; c. the rules or patterns so studied; \"Computers.\" the grammatical rules and structural patterns governing the ordered use of appropriate words and symbols for issuing commands, writing code, etc., in a particular software application or programming language.",
            "score": 135.2269287109375
        },
        {
            "docid": "35543733_7",
            "document": "Bilingual lexicon . With years of researches, how languages are stored and processed by bilinguals is still a main theme that many psycholinguists. One main topic is that bilinguals possess one or two internal lexicons, and even more with three stores. One for each language and the third one is for corresponding two languages. Reaction time of recognizing words in different languages is the most used method to figure out how our lexicon been activated. Researches in 1980s by Soares and Grosjean on English-Portuguese bilingual had two main findings. One is that although bilingual can access real words in English as quickly as English monolinguals, but they are slower at responding to non-words. The other finding is that bilingual took longer to access code-switched words than they did base-language words in the monolingual mode. These two findings can be seen as the evidence for more than one lexicon are existed in bilinguals' brains. As technology develops, functional magnetic resonance imaging (fMRI) is also used to study how brain activity is different in bilinguals' brain when both language are interact. Imaging studies have yielded that specific brain areas are involved in bilingual switching, which means this part of the brain can be said as the \"third lexicon\", the interconnected part of two lexicons for each language, where stores the guest words. Other research suggests only one combined lexicon is exists.",
            "score": 132.12596130371094
        },
        {
            "docid": "35182952_7",
            "document": "Embodied language processing . Experiential Trace Hypothesis states that each time an individual interacts with the world, traces of that particular experience are left in our brain. These traces can be accessed again when a person thinks of words or sentences that remind them of that experience. Additionally, these traces in our brain are linked to the action that they are related to. Words and sentences become those cues that retrieve these traces from our mind. Researchers have studied if the previous experience with a word, such as its location (up or down) in space, affects how people understand and then respond to that word. In one experiment, researchers hypothesized that if reading an object word also activates a location that is linked to that noun, then the following action response should be compatible with that association. They found that participants were faster to push a button higher than another button when the word was associated with being \"up\" or \"above\" than when the button was lower than the other for words associated with \"up\" and \"above\". The results of this study displayed that participants were faster to respond when the location of the word and the action they had to perform were similar. This demonstrates that language processing and action are connected. This research also found that the location information of a word is automatically activated after seeing the word. In a similar study, it was discovered that participants were equally as fast at responding to words that were associated with either an upward or downward location when the buttons to respond to these words were horizontal \u2013 meaning that the experiential trace effect was ruled out when the responding action did not link to either of the locations that were activated.",
            "score": 131.73171997070312
        },
        {
            "docid": "39178155_3",
            "document": "Bilingual lexical access . Early research of bilingual lexical access was generated from the theories of unilingual lexical access.  Theories derived from early unilingual research relied mainly upon generalizations without precise specification of how these specific systems of lexical access works. Due to the advancement of medical science within the last decade, the field of Psycholinguistics has evolved immensely, resulting in more detailed research and therefore, a deeper understanding of the mechanisms behind language production. \"Many early studies of second language acquisition focused on the morphosyntactic development of learners, and the general findings was that bound morphemes appear in the same order in the first and second language\"(Bardovi-Harlig 1999. In addition, \"second language learners are also able to produce and process simple sentences before complex sentences\" (Pienemann et al.2005), just like first language learners. For example, the theory of serial search models and parallel access models. \"Serial Search Models\" propose that when monolinguals encounter a word, they will look through all the lexical entries to distinguish whether the input item is a word not, and then they will only retrieve the necessary information about that word (i.e., its semantics or orthography). They also propose that the lexical access would process sequentially by activating only one lexical entry at a time. In contrast, the \"Parallel Access Models\" believe that multiple entries can be activated at once, which means that the perceptual input from a word would activate all lexical items directly, even though some of them might not be necessary. In this way, numbers of potential candidates would be activated simultaneously and then the lexical candidates which are most consistent with the input stimulus would be chosen. Later, the researchers addressed that both the serial and parallel process are accounted for the lexical organization and lexical access. Knowledge of unilingual access has inevitably led to the curiosity of bilingual lexical access. Early models of bilingual lexical access shared similar characteristics with these unilingual lexical access models. For example, the bilingual models began with focusing on whether the lexical access for bilinguals would be different from monolinguals. In addition to study the activation process in separate language, they also investigated whether the lexical activation would be processed in a parallel fashion for both languages or selectively processed for the target language. In this case, the bilingual models also studied whether the bilingual system has a single lexicon combining words from both languages or separate lexicons for words in each language. With the occurrence of widespread computational modeling, researchers extended the theoretical approaches for the studies of bilingual lexical access.The computational models are now essential component for mainstream theories, for example, the models of Bilingual Interactive Activation [BIA] model,  the Semantic, Orthographic and Phonological Interactive Activation [SOPIA] model, and the Bilingual interactive Model of Lexical Access [BLMOLA]. Since most computational models need to specify all the vague descriptive notions used in the earlier models, they force researchers to be more clarified with their theories. Those revised models can also serve as to test the viability of the original theories by comparing the empirical results with data generated from the model. In addition, the computational models can also help to generate new testable hypothesis and allow researchers to manipulate conditions which might not be possible in normal experimentation. For example, researchers can investigate and simulate the lexical access systems under various states of damage without using aphasic people.",
            "score": 130.22457885742188
        },
        {
            "docid": "34032792_3",
            "document": "Prediction in language comprehension . In the eyetracking visual world paradigm, experimental subjects listen to a sentence while staring at an array of pictures on a computer monitor. Their eye movements are recorded, allowing the experimenter to understand how language influences eye movements toward pictures related to the content of the sentence. Experiments of this type have shown that while listening to the verb in a sentence, comprehenders anticipatorily move their eyes to the picture of the verb's likely direct object (e.g. \"cake\" rather than \"ball\" while hearing, \"The boy will eat...\"). Subsequent investigations using the same experimental setup showed that the verb's subject can also determine which object comprehenders anticipate (e.g., comprehenders look at the merry-go-round rather than the motorcycle while hearing, \"The little girl will ride...\").  In short, comprehenders use the information in the sentence context to predict the meanings of upcoming words. In these experiments, comprehenders used the verb and its subject to activate information about the verb's direct object before hearing that word. However, another experiment has shown that in a language with more flexible word order (German), comprehenders can also use context to predict the sentence's subject.",
            "score": 129.1703338623047
        },
        {
            "docid": "236809_49",
            "document": "Recall (memory) . A key technique in improving and helping recall memory is to take advantage of Mnemonic devices and other cognitive strategies. Mnemonic devices are a type of cognitive strategy that enables individuals to memorize and recall new information in an easier fashion, rather than just having to remember a list of information that is not related to one another. An example of mnemonic devices are PEMDAS or Please Excuse My Dear Aunt Sally; this is a device for arithmetic when solving equations that have parenthesis, exponents, multiplication, division, addition, or subtraction and what order to do each calculation. Words or an acronym can stand for a process that individuals need to recall. The benefits of using these types of strategies to perform tasks are that encoding becomes more organized and it is easier to remember and process information. Also this device reduces the need of intentional resources at the point of retrieval, which means that recall does not need outside sources helping an individual remember what happened yesterday. Cognitive strategies can leverage semantic connections that will allow the brain to process and work more efficiently than just having to process the information as whole parts. By using the strategies the information becomes related to each other and the information sticks.  Another type of device people use to help their recall memory become efficient is chunking. Chunking is the process of breaking down numbers into smaller units to remember the information or data, this helps recall numbers and math facts. An example of this chunking process is a telephone number; this is chunked with three digits, three digits, then four digits. People read them off as such when reciting a phone number to another person. There has been research done about these techniques and an institution tested two groups of people to see if these types of devices work well for real people, the results came back determining a significant performance difference between the group who did not use cognitive strategies and the group who did. The group using the techniques immediately performed better than the other group and when taking a pre-test and post-test the results indicated that the group using the techniques improved while the other group did not.",
            "score": 128.24960327148438
        },
        {
            "docid": "44340_15",
            "document": "Steven Pinker . In psycholinguistics, Pinker became known early in his career for promoting computational learning theory as a way to understand language acquisition in children. He wrote a tutorial review of the field followed by two books that advanced his own theory of language acquisition, and a series of experiments on how children acquire the passive, dative, and locative constructions. These books were \"Language Learnability and Language Development\" (1984), in Pinker's words \"outlin[ing] a theory of how children acquire the words and grammatical structures of their mother tongue\", and \"Learnability and Cognition: The Acquisition of Argument Structure\" (1989), in Pinker's words \"focus[ing] on one aspect of this process, the ability to use different kinds of verbs in appropriate sentences, such as intransitive verbs, transitive verbs, and verbs taking different combinations of complements and indirect objects\". He then focused on verbs of two kinds that illustrate what he considers to be the processes required for human language: retrieving whole words from memory, like the past form of the irregular verb \"bring\", namely \"brought\"; and using rules to combine (parts of) words, like the past form of the regular verb \"walk\", namely \"walked\".",
            "score": 127.07205963134766
        },
        {
            "docid": "858038_42",
            "document": "Cassandra Cain . As a side effect of her father's training, Cassandra's brain developed learning functions different from most. Having been brought up by Cain deliberately without speech, the communication centers of her brain learned body language instead of spoken or written language. Thus, she originally had as much trouble learning spoken and written language as a normal individual would have in learning body language. Although she was able to learn some very basic things (\"no\", \"yes\", \"me\") the same way a normal person can learn to recognize smiles and frowns, it took a telepath \"rewiring\" her brain to teach her to speak and understand English. Even then, she only spoke with extreme difficulty (very falteringly, short sentences with long pauses, frequently using the wrong words, etc.). In \"Batgirl\" #67 (October 2005), Oracle performed a number of tests on Cassandra, determining the severity of the problem: \"The language centers of your brain are all over both hemispheres. Not centralized like with most people. When you try to read or write, your brain doesn't know how to keep it cohesive.\"",
            "score": 126.66194152832031
        },
        {
            "docid": "7024544_2",
            "document": "Statistical parsing . Statistical parsing is a group of parsing methods within natural language processing. The methods have in common that they associate grammar rules with a probability. Grammar rules are traditionally viewed in computational linguistics as defining the valid sentences in a language. Within this mindset, the idea of associating each rule with a probability then provides the relative frequency of any given grammar rule and, by deduction, the probability of a complete parse for a sentence. (The probability associated with a grammar rule may be induced, but the application of that grammar rule within a parse tree and the computation of the probability of the parse tree based on its component rules is a form of deduction.) Using this concept, statistical parsers make use of a procedure to search over a space of all candidate parses, and the computation of each candidate's probability, to derive the most probable parse of a sentence. The Viterbi algorithm is one popular method of searching for the most probable parse.",
            "score": 126.34793090820312
        },
        {
            "docid": "25688695_17",
            "document": "Implicit learning . It has been shown that people are able to implicitly learn underlying sequential structure in a series using sequence learning. Language is an example of daily sequential learning. Although individuals are unable to communicate how they have acquired such knowledge of rules, studies show people generally have knowledge of a number of factors that imply sequence learning. When reading, sentences that follow proper syntax and use proper context are read faster than those which are not. People are also able to fairly quickly predict an upcoming word that occurs in a sequence and are able to create sentences which follow sequence while following the rules of English. This implies the use of sequence learning in language.",
            "score": 125.716796875
        },
        {
            "docid": "43527201_5",
            "document": "Usha Goswami . Dyslexia is a disorder in which the person affected has difficulty reading due to the reversal of letters in the brain that isn't linked to intelligence. In people with dyslexia, the brain processes certain signals in a specific way making it a very specific learning difficulty. Dr. Goswami's research is concerned with focusing on dyslexia as a language disorder rather than a visual disorder as she has found that the way that children with dyslexia hear language is slightly different than others. When sound waves approach the brain, they vary in pressure depending on the syllables within the words being spoken creating a rhythm. When these signals reach the brain they are lined up with speech rhythms and this process doesn't work properly in those with dyslexia. Goswami is currently researching whether or not reading poetry, nursery rhymes, and singing can be used to help children with dyslexia. The rhythm of the words could allow the child to match the syllable patterns to language before they begin reading as to catch them up to where children without the disability might be.",
            "score": 125.69549560546875
        },
        {
            "docid": "17524_27",
            "document": "Language . The academic study of language is conducted within many different disciplinary areas and from different theoretical angles, all of which inform modern approaches to linguistics. For example, descriptive linguistics examines the grammar of single languages, theoretical linguistics develops theories on how best to conceptualize and define the nature of language based on data from the various extant human languages, sociolinguistics studies how languages are used for social purposes informing in turn the study of the social functions of language and grammatical description, neurolinguistics studies how language is processed in the human brain and allows the experimental testing of theories, computational linguistics builds on theoretical and descriptive linguistics to construct computational models of language often aimed at processing natural language or at testing linguistic hypotheses, and historical linguistics relies on grammatical and lexical descriptions of languages to trace their individual histories and reconstruct trees of language families by using the comparative method.",
            "score": 125.50106811523438
        },
        {
            "docid": "18614_3",
            "document": "Language acquisition . Linguists who are interested in child language acquisition for many years question how language is acquired, lidz et al. states \"The question of how these structures are acquired, then, is more properly understood as the question of how a learner takes the surface forms in the input and converts them into abstract linguistic rules and representations.\" So we know language acquisition involves structures, rules and representation. The capacity to successfully use language requires one to acquire a range of tools including phonology, morphology, syntax, semantics, and an extensive vocabulary. Language can be vocalized as in speech, or manual as in sign. Human language capacity is represented in the brain. Even though human language capacity is finite, one can say and understand an infinite number of sentences, which is based on a syntactic principle called recursion. Evidence suggests that every individual has three recursive mechanisms that allow sentences to go indeterminately. These three mechanisms are: \"relativization\", \"complementation\" and \"coordination\". Furthermore, there are actually two main guiding principles in first-language acquisition, that is, speech perception always precedes speech production and the gradually evolving system by which a child learns a language is built up one step at a time, beginning with the distinction between individual phonemes.",
            "score": 125.43236541748047
        },
        {
            "docid": "620396_42",
            "document": "Origin of language . In humans, functional MRI studies have reported finding areas homologous to the monkey mirror neuron system in the inferior frontal cortex, close to Broca's area, one of the language regions of the brain. This has led to suggestions that human language evolved from a gesture performance/understanding system implemented in mirror neurons. Mirror neurons have been said to have the potential to provide a mechanism for action-understanding, imitation-learning, and the simulation of other people's behavior. This hypothesis is supported by some cytoarchitectonic homologies between monkey premotor area F5 and human Broca's area. Rates of vocabulary expansion link to the ability of children to vocally mirror non-words and so to acquire the new word pronunciations. Such speech repetition occurs automatically, quickly and separately in the brain to speech perception. Moreover, such vocal imitation can occur without comprehension such as in speech shadowing and echolalia. Further evidence for this link comes from a recent study in which the brain activity of two participants was measured using fMRI while they were gesturing words to each other using hand gestures with a game of charades\u2014a modality that some have suggested might represent the evolutionary precursor of human language. Analysis of the data using Granger Causality revealed that the mirror-neuron system of the observer indeed reflects the pattern of activity of in the motor system of the sender, supporting the idea that the motor concept associated with the words is indeed transmitted from one brain to another using the mirror system.",
            "score": 125.17442321777344
        },
        {
            "docid": "20646_34",
            "document": "Morphology (linguistics) . Word-based morphology is (usually) a word-and-paradigm approach. The theory takes paradigms as a central notion. Instead of stating rules to combine morphemes into word forms or to generate word forms from stems, word-based morphology states generalizations that hold between the forms of inflectional paradigms. The major point behind this approach is that many such generalizations are hard to state with either of the other approaches. The examples are usually drawn from fusional languages, where a given \"piece\" of a word, which a morpheme-based theory would call an inflectional morpheme, corresponds to a combination of grammatical categories, for example, \"third-person plural\". Morpheme-based theories usually have no problems with this situation since one says that a given morpheme has two categories. Item-and-process theories, on the other hand, often break down in cases like these because they all too often assume that there will be two separate rules here, one for third person, and the other for plural, but the distinction between them turns out to be artificial. The approaches treat these as whole words that are related to each other by analogical rules. Words can be categorized based on the pattern they fit into. This applies both to existing words and to new ones. Application of a pattern different from the one that has been used historically can give rise to a new word, such as \"older\" replacing \"elder\" (where \"older\" follows the normal pattern of adjectival superlatives) and \"cows\" replacing \"kine\" (where \"cows\" fits the regular pattern of plural formation).",
            "score": 124.8188247680664
        },
        {
            "docid": "25181073_8",
            "document": "Neuroscience of multilingualism . Insights into language storage in the brain have come from studying multilingual individuals afflicted with a form of aphasia. The symptoms and severity of aphasia in multilingual individuals depend on the number of languages the individual knows, what order they learned them, and thus have them stored in the brain, the age at which they learned them, how frequently each language is used, and how proficient the individual is in using those languages. Two primary theoretical approaches to studying and viewing multilingual aphasics exist\u2014the localizationalist approach and the dynamic approach. The localizationalist approach views different languages as stored in different regions of the brain, explaining why multilingual aphasics may lose one language they know, but not the other(s). The dynamical theory (or shared representation) approach suggests that the language system is supervised by a dynamic equilibrium between the existing language capabilities and the constant alteration and adaptation to the communicative requirements of the environment. The dynamic approach views the representation and control aspects of the language system as compromised as a result of brain damage to the brain's language regions. The dynamic approach offers a satisfactory explanation for the various recovery times of each of the languages the aphasic has had impaired or lost because of the brain damage. Recovery of languages varies across aphasic patients. Some may recover all lost or impaired languages simultaneously. For some, one language is recovered before the others. In others, an involuntary mix of languages occurs in the recovery process; they intermix words from the various languages they know when speaking. Research affirms with the two approaches combined into the amalgamated hypothesis, it states that while languages do share some parts of the brain, they can also be allotted to some separate areas that are neutral.",
            "score": 124.81536102294922
        },
        {
            "docid": "27309832_17",
            "document": "P200 . The P2 has also been found to be involved in language processes including sentential constraint and expectancy for a given word. Researchers found that the P2 component varied with the level of expectancy for a particular item in a sentence for right but not left visual field presentations, suggesting that the left hemisphere of the brain may use contextual information to prepare for the visual analysis of upcoming stimuli. For presentation biased to the left hemisphere, the P2 is larger (more positive) for strongly constrained sentence endings, independent of whether the actual word was the expected one or not. This has been interpreted as suggesting that the left hemisphere in particular uses top-down attentional mechanisms to prepare to process words that are likely to be expected. In some cases (for example, with pictures instead of words in sentences), it may also reflect matching of input with expectation.",
            "score": 124.79298400878906
        },
        {
            "docid": "355240_10",
            "document": "Cognitive model . By taking into account the evolutionary development of the human nervous system and the similarity of the brain to other organs, Elman proposed that language and cognition should be treated as a dynamical system rather than a digital symbol processor. Neural networks of the type Elman implemented have come to be known as Elman networks. Instead of treating language as a collection of static lexical items and grammar rules that are learned and then used according to fixed rules, the dynamical systems view defines the lexicon as regions of state space within a dynamical system. Grammar is made up of attractors and repellers that constrain movement in the state space. This means that representations are sensitive to context, with mental representations viewed as trajectories through mental space instead of objects that are constructed and remain static. Elman networks were trained with simple sentences to represent grammar as a dynamical system. Once a basic grammar had been learned, the networks could then parse complex sentences by predicting which words would appear next according to the dynamical model.",
            "score": 124.06721496582031
        },
        {
            "docid": "1072943_15",
            "document": "Forward algorithm . The forward algorithm is mostly used in applications that need us to determine the probability of being in a specific state when we know about the sequence of observations. We first calculate the probabilities over the states computed for the previous observation and use them for the current observations, and then extend it out for the next step using the transition probability table.The approach basically caches all the intermediate state probabilities so they are computed only once. This helps us to compute a fixed state path. The process is also called posterior decoding. The algorithm computes probability much more efficiently than the naive approach, which very quickly ends up in a combinatorial explosion. Together, they can provide the probability of a given emission/observation at each position in the sequence of observations. It is from this information that a version of the most likely state path is computed (\"posterior decoding\"). The algorithm can be applied wherever we can train a model as we receive data using Baum-Welch or any general EM algorithm. The Forward algorithm will then tell us about the probability of data with respect to what is expected from our model. One of the applications can be in the domain of Finance, where it can help decide on when to buy or sell tangible assets. It can have applications in all fields where we apply Hidden Markov Models. The popular ones include Natural language processing domains like tagging part-of-speech and speech recognition. Recently it is also being used in the domain of Bioinformatics. Forward algorithm can also be applied to perform Weather speculations. We can have a HMM describing the weather and its relation to the state of observations for few consecutive days (some examples could be dry, damp, soggy, sunny, cloudy, rainy etc.). We can consider calculating the probability of observing any sequence of observations recursively given the HMM. We can then calculate the probability of reaching an intermediate state as the sum of all possible paths to that state. Thus the partial probabilities for the final observation will hold the probability of reaching those states going through all possible paths.",
            "score": 123.96412658691406
        },
        {
            "docid": "9627698_66",
            "document": "Child development . A child learns the syntax of their language when they are able to join words together into sentences and understand multiple-word sentences said by other people. There appear to be six major stages in which a child\u2019s acquisition of syntax develops. First, is the use of sentence-like words in which the child communicates using one word with additional vocal and bodily cues. This stage usually occurs between 12 and 18 months of age. Second, between 18 months to two years, there is the modification stage where children communicate relationships by modifying a topic word. The third stage, between two and three years old, involves the child using complete subject-predicate structures to communicate relationships. Fourth, children make changes on basic sentence structure that enables them to communicate more complex relationships. This stage occurs between the ages of two and a half years to four years. The fifth stage of categorization involves children aged three and a half to seven years refining their sentences with more purposeful word choice that reflects their complex system of categorizing word types. Finally, children use structures of language that involve more complicate syntactic relationships between the ages of five years old to ten years old. Infants begin with cooing and soft vowel sounds. Shortly after birth, this system is developed as the infants begin to understand that their noises, or non-verbal communication, lead to a response from their caregiver. This will then progress into babbling around 5 months of age, with infants first babbling consonant and vowel sounds together that may sound like \"ma\" or \"da\". At around 8 months of age, babbling increases to include repetition of sounds, such as \"da-da\" and infants learn the forms for words and which sounds are more likely to follow other sounds. At this stage, much of the child\u2019s communication is open to interpretation. For example, if a child says \u201cbah\u201d when they\u2019re in a toy room with their guardian, it is likely to be interpreted as \u201cball\u201d because the toy is in sight. However, if you were to listen to the same \u2018word\u2019 on a recorded tape without knowing the context, one might not be able to figure out what the child was trying to say. A child's receptive language, the understanding of others' speech, has a gradual development beginning at about 6 months. However, expressive language, the production of words, moves rapidly after its beginning at about a year of age, with a \"vocabulary explosion\" of rapid word acquisition occurring in the middle of the second year. Grammatical rules and word combinations appear at about age two. Between 20 and 28 months, children move from understanding the difference between high and low, hot and cold and begin to change \u201cno\u201d to \u201cwait a minute\u201d, \u201cnot now\u201d and \u201cwhy\u201d. Eventually, they are able to add pronouns to words and combine them to form short sentences. Mastery of vocabulary and grammar continue gradually through the preschool and school years. Adolescents still have smaller vocabularies than adults and experience more difficulty with constructions such as the passive voice.",
            "score": 123.7821044921875
        }
    ]
}