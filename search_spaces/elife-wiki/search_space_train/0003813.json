{
    "q": [
        {
            "docid": "5386671_2",
            "document": "Structure from motion . Structure from motion (SfM) is a photogrammetric range imaging technique for estimating three-dimensional structures from two-dimensional image sequences that may be coupled with local motion signals. It is studied in the fields of computer vision and visual perception. In biological vision, SfM refers to the phenomenon by which humans (and other living creatures) can recover 3D structure from the projected 2D (retinal) motion field of a moving object or scene.",
            "score": 119.4822850227356
        },
        {
            "docid": "15375961_5",
            "document": "Active vision . Autonomous cameras are cameras that can direct themselves in their environment. There has been some recent work using this approach. In work from Denzler et al., the motion of a tracked object is modeled using a Kalman filter while the focal length that minimizes the uncertainty in the state estimations is the one that is used. A stereo set-up with two zoom cameras was used. A handful of papers have been written for zoom control and do not deal with total object-camera position estimation. An attempt to join estimation and control in the same framework can be found in the work of Bagdanov et al., where a Pan-Tilt-Zoom camera is used to track faces. Both the estimation and control models used are ad hoc, and the estimation approach is based on image features rather than 3D properties of the target being tracked.",
            "score": 102.879953622818
        },
        {
            "docid": "48589354_40",
            "document": "Visual Turing Test . The Images considered for the Geman \"et al.\" work are that of \u2018Urban street scenes\u2019 dataset, which has scenes of streets from different cities across the world. This why the types of objects are constrained to people and vehicles for this experiment. Another dataset introduced by the Max Planck Institute for Informatics is known as DAQUAR dataset which has real world images of indoor scenes. But they propose a different version of the visual Turing test which takes on a holistic approach and expects the participating system to exhibit human like common sense. This is a very recent work published on March 9, 2015, in the journal \"Proceedings\" of the National Academy of Sciences, by researchers from Brown University and Johns Hopkins University. It evaluates how the computer vision systems understand the Images as compared to humans. Currently the test is written and the interrogator is a machine because having an oral evaluation by a human interrogator gives the humans an undue advantage of being subjective, and also expects real time answers.",
            "score": 108.8294049501419
        },
        {
            "docid": "5104401_3",
            "document": "Outline of computer vision . Computer vision \u2013 interdisciplinary field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do. Computer vision tasks include methods for acquiring digital images (through image sensors), image processing, and image analysis, to reach an understanding of digital images. In general, it deals with the extraction of high-dimensional data from the real world in order to produce numerical or symbolic information that the computer can interpret. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner. As a technological discipline, computer vision seeks to apply its theories and models for the construction of computer vision systems. As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images.",
            "score": 99.64804172515869
        },
        {
            "docid": "6596_6",
            "document": "Computer vision . Computer vision is an interdisciplinary field that deals with how computers can be made for gaining high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do. Computer vision is concerned with the automatic extraction, analysis and understanding of useful information from a single image or a sequence of images. It involves the development of a theoretical and algorithmic basis to achieve automatic visual understanding. Computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner \u201ctricorder\u201d medical scanner. As a technological discipline, computer vision seeks to apply its theories and models for the construction of computer vision systems.",
            "score": 93.31768369674683
        },
        {
            "docid": "45350085_6",
            "document": "Visual computing . At least the following disciplines are sub-fields of visual computing. More detailed descriptions of each of these fields can be found on the linked special pages. Computer graphics is a general term for all techniques that produce images as result with the help of a computer. To transform the description of objects to nice images is called rendering which is always a compromise between image quality and run-time. Techniques that can extract content information from images are called image analysis techniques. Computer vision is the ability of computers (or of robots) to recognize their environment and to interpret it correctly. Visualization is used to produce images that shall communicate messages. Data may be abstract or concrete, often with no a priori geometrical components. Visual analytics describes the discipline of interactive visual analysis of data, also described as \u201cthe science of analytical reasoning supported by the interactive visual interface\u201d. To represent objects for rendering it needs special methods and data structures, which subsumed with the term geometric modeling. In addition to describing and interactive geometric techniques, sensor data are more and more used to reconstruct geometrical models. Algorithms for the efficient control of 3D printers also belong to the field of visual computing. In contrast to image analysis image processing manipulates images to produce better images. \u201cBetter\u201d can have very different meanings subject to the respective application. Also, it has to be discriminated from image editing which describes interactive manipulation of images based on human validation. Techniques that produce the feeling of immersion into a fictive world are called virtual reality (VR). Requirements for VR include head-mounted displays, real-time tracking, and high-quality real-time rendering. Augmented reality enables the user to see the real environment in addition to the virtual objects, which augment this reality. Accuracy requirements on rendering speed and tracking precision are significantly higher here. The planning, design and uses of interfaces between people and computers is not only part of every system involving images. Due to the high bandwidth of the human visual channel (eye), images are also a preferred part of ergonomic user interfaces in any system, so that human-computer interaction is also an integral part of visual computing.",
            "score": 96.81478452682495
        },
        {
            "docid": "6319245_10",
            "document": "Camera resectioning . When a camera is used, light from the environment is focused on an image plane and captured. This process reduces the dimensions of the data taken in by the camera from three to two (light from a 3D scene is stored on a 2D image). Each pixel on the image plane therefore corresponds to a shaft of light from the original scene. Camera resectioning determines which incoming light is associated with each pixel on the resulting image. In an ideal pinhole camera, a simple projection matrix is enough to do this. With more complex camera systems, errors resulting from misaligned lenses and deformations in their structures can result in more complex distortions in the final image. The camera projection matrix is derived from the intrinsic and extrinsic parameters of the camera, and is often represented by the series of transformations; e.g., a matrix of camera intrinsic parameters, a 3\u00a0\u00d7\u00a03 rotation matrix, and a translation vector. The camera projection matrix can be used to associate points in a camera's image space with locations in 3D world space.",
            "score": 65.34455692768097
        },
        {
            "docid": "16928506_17",
            "document": "Visual servoing . A similar study was done in where the authors carry out experimental evaluation of a few uncalibrated visual servo systems that were popular in the 90\u2019s. The major outcome was the experimental evidence of the effectiveness of visual servo control over conventional control methods. Kyrki et al. analyze servoing errors for position based and 2-1/2-D visual servoing. The technique involves determining the error in extracting image position and propagating it to pose estimation and servoing control. Points from the image are mapped to points in the world a priori to obtain a mapping (which is basically the homography, although not explicitly stated in the paper). This mapping is broken down to pure rotations and translations. Pose estimation is performed using standard technique from Computer Vision. Pixel errors are transformed to the pose. These are propagating to the controller. An observation from the analysis shows that errors in the image plane are proportional to the depth and error in the depth-axis is proportional to square of depth. Measurement errors in visual servoing have been looked into extensively. Most error functions relate to two aspects of visual servoing. One being steady state error (once servoed) and two on the stability of the control loop. Other servoing errors that have been of interest are those that arise from pose estimation and camera calibration. In, the authors extend the work done in by considering global stability in the presence of intrinsic and extrinsic calibration errors. provides an approach to bound the task function tracking error. In, the authors use teaching-by-showing visual servoing technique. Where the desired pose is known a priori and the robot is moved from a given pose. The main aim of the paper is to determine the upper bound on the positioning error due to image noise using a convex- optimization technique. in depth estimates. The authors conclude the paper with the observation that for unknown target geometry a more accurate depth estimate is required in order to limit the error. Many of the visual servoing techniques implicitly assume that only one object is present in the image and the relevant feature for tracking along with the area of the object are available. Most techniques require either a partial pose estimate or a precise depth estimate of the current and desired pose.",
            "score": 90.68303513526917
        },
        {
            "docid": "1498790_2",
            "document": "Thomas Binford . Thomas Oriel Binford has been a leading researcher in image analysis and computer vision since 1967. He is known for pioneering a model-based approach to computer vision in which complex objects are represented as collections of generalized cylinders. His results reflect seminal work in numerous other areas of research including the interpretation of complex scenes using invariants and quasi-invariants, inference rules and evidential reasoning in extended Bayes networks of symbolic geometric constraints, the SUCCESSOR system, a portable, intelligent vision system, stereo and visual robot navigation, segmentation and feature estimation in complex images, color image analysis, surface material analysis, and image compression. He has led the development of numerous computer vision systems, including systems successfully employed in brain surgery on humans, high-precision automated machining, and helicopter navigation.",
            "score": 113.5622171163559
        },
        {
            "docid": "63452_14",
            "document": "Heuristic . There are several ways that humans form and use cognitive maps. Visual intake is a key part of mapping. The first is by using \"landmarks\". This is where a person uses a mental image to estimate a relationship, usually distance, between two objects. Second, is \"route-road\" knowledge, and this is generally developed after a person has performed a task and is relaying the information of that task to another person. Third, is survey. A person estimates a distance based on a mental image that, to them, might appear like an actual map. This image is generally created when a person's brain begins making image corrections. These are presented in five ways: 1. \"Right-angle bias\" is when a person straightens out an image, like mapping an intersection, and begins to give everything 90-degree angles, when in reality it may not be that way. 2. \"Symmetry heuristic\" is when people tend to think of shapes, or buildings, as being more symmetrical than they really are. 3. \"Rotation heuristic\" is when a person takes a naturally (realistically) distorted image and straightens it out for their mental image. 4. \"Alignment heuristic\" is similar to the previous, where people align objects mentally to make them straighter than they really are. 5. \"Relative-position heuristic\": people do not accurately distance landmarks in their mental image based on how well they remember that particular item.",
            "score": 104.59956860542297
        },
        {
            "docid": "1841851_2",
            "document": "Stereopsis . Stereopsis (from the Greek \u03c3\u03c4\u03b5\u03c1\u03b5\u03bf- \"stereo-\" meaning \"solid\", and \u1f44\u03c8\u03b9\u03c2 \"opsis\", \"appearance, sight\") is a term that is most often used to refer to the perception of depth and 3-dimensional structure obtained on the basis of visual information deriving from two eyes by individuals with normally developed binocular vision. Because the eyes of humans, and many animals, are located at different lateral positions on the head, binocular vision results in two slightly different images projected to the retinas of the eyes. The differences are mainly in the relative horizontal position of objects in the two images. These positional differences are referred to as horizontal disparities or, more generally, binocular disparities. Disparities are processed in the visual cortex of the brain to yield depth perception. While binocular disparities are naturally present when viewing a real 3-dimensional scene with two eyes, they can also be simulated by artificially presenting two different images separately to each eye using a method called stereoscopy. The perception of depth in such cases is also referred to as \"stereoscopic depth\".",
            "score": 93.91305494308472
        },
        {
            "docid": "1534483_3",
            "document": "Motion estimation . More often than not, the term motion estimation and the term optical flow are used interchangeably. It is also related in concept to image registration and stereo correspondence. In fact all of these terms refer to the process of finding corresponding points between two images or video frames. The points that correspond to each other in two views (images or frames) of a real scene or object are \"usually\" the same point in that scene or on that object. Before we do motion estimation, we must define our measurement of correspondence, i.e., the matching metric, which is a measurement of how similar two image points are. There is no right or wrong here; the choice of matching metric is usually related to what the final estimated motion is used for as well as the optimisation strategy in the estimation process.",
            "score": 97.95465552806854
        },
        {
            "docid": "181887_17",
            "document": "Night vision . Active infrared night-vision combines infrared illumination of spectral range 700\u20131,000\u00a0nm (just below the visible spectrum of the human eye) with CCD cameras sensitive to this light. The resulting scene, which is apparently dark to a human observer, appears as a monochrome image on a normal display device. Because active infrared night-vision systems can incorporate illuminators that produce high levels of infrared light, the resulting images are typically higher resolution than other night-vision technologies. Active infrared night vision is now commonly found in commercial, residential and government security applications, where it enables effective night time imaging under low-light conditions. However, since active infrared light can be detected by night-vision goggles, there can be a risk of giving away position in tactical military operations.",
            "score": 61.71288073062897
        },
        {
            "docid": "6596_33",
            "document": "Computer vision . Several tasks relate to motion estimation where an image sequence is processed to produce an estimate of the velocity either at each points in the image or in the 3D scene, or even of the camera that produces the images . Examples of such tasks are:",
            "score": 87.25000834465027
        },
        {
            "docid": "9871405_2",
            "document": "Image rectification . Image rectification is a transformation process used to project images onto a common image plane. This process has several degrees of freedom and there are many strategies for transforming images to the common plane. Computer stereo vision takes two or more images with known relative camera positions that show an object from different viewpoints. For each pixel it then determines the corresponding scene point's depth (i.e. distance from the camera) by first finding matching pixels (i.e. pixels showing the same scene point) in the other image(s) and then applying triangulation to the found matches to determine their depth. Finding matches in stereo vision is restricted by epipolar geometry: Each pixel's match in another image can only be found on a line called the epipolar line. If two images are coplanar, i.e. they were taken such that the right camera is only offset horizontally compared to the left camera (not being moved towards the object or rotated), then each pixel's epipolar line is horizontal and at the same vertical position as that pixel. However, in general settings (the camera did move towards the object or rotate) the epipolar lines are slanted. Image rectification warps both images such that they appear as if they have been taken with only a horizontal displacement and as a consequence all epipolar lines are horizontal, which slightly simplifies the stereo matching process. Note however, that rectification does not fundamentally change the stereo matching process: It searches on lines, slanted ones before and horizontal ones after rectification.",
            "score": 52.69598710536957
        },
        {
            "docid": "10450559_8",
            "document": "3D interaction . Users experience a sense of presence when engaged in an immersive virtual world. Enabling the users to interact with this world in 3D allows them to make use of natural and intrinsic knowledge of how information exchange takes place with physical objects in the real world. Texture, sound, and speech can all be used to augment 3D interaction. Currently, users still have difficulty in interpreting 3D space visuals and understanding how interaction occurs. Although it\u2019s a natural way for humans to move around in a three-dimensional world, the difficulty exists because many of the cues present in real environments are missing from virtual environments. Perception and occlusion are the primary perceptual cues used by humans. Also, even though scenes in virtual space appear three-dimensional, they are still displayed on a 2D surface so some inconsistencies in depth perception will still exist.",
            "score": 137.9947111606598
        },
        {
            "docid": "869825_10",
            "document": "Optical flow . Motion estimation and video compression have developed as a major aspect of optical flow research. While the optical flow field is superficially similar to a dense motion field derived from the techniques of motion estimation, optical flow is the study of not only the determination of the optical flow field itself, but also of its use in estimating the three-dimensional nature and structure of the scene, as well as the 3D motion of objects and the observer relative to the scene, most of them using the Image Jacobian.",
            "score": 107.24491238594055
        },
        {
            "docid": "1164_9",
            "document": "Artificial intelligence . The earliest (and easiest to understand) approach to AI was symbolism (such as formal logic): \"If an otherwise healthy adult has a fever, then they may have influenza\". A second, more general, approach is Bayesian inference: \"If the current patient has a fever, adjust the probability they have influenza in such-and-such way\". The third major approach, extremely popular in routine business AI applications, are analogizers such as SVM and nearest-neighbor: \"After examining the records of known past patients whose temperature, symptoms, age, and other factors mostly match the current patient, X% of those patients turned out to have influenza\". A fourth approach is harder to intuitively understand, but is inspired by how the brain's machinery works: the artificial neural network approach uses artificial \"neurons\" that can learn by comparing itself to the desired output and altering the strengths of the connections between its internal neurons to \"reinforce\" connections that seemed to be useful. These four main approaches can overlap with each other and with evolutionary systems; for example, neural nets can learn to make inferences, to generalize, and to make analogies. Some systems implicitly or explicitly use multiple of these approaches, alongside many other AI and non-AI algorithms; the best approach is often different depending on the problem. Learning algorithms work on the basis that strategies, algorithms, and inferences that worked well in the past are likely to continue working well in the future. These inferences can be obvious, such as \"since the sun rose every morning for the last 10,000 days, it will probably rise tomorrow morning as well\". They can be nuanced, such as \"X% of families have geographically separate species with color variants, so there is an Y% chance that undiscovered black swans exist\". Learners also work on the basis of \"Occam's razor\": The simplest theory that explains the data is the likeliest. Therefore, to be successful, a learner must be designed such that it prefers simpler theories to complex theories, except in cases where the complex theory is proven substantially better. Settling on a bad, overly complex theory gerrymandered to fit all the past training data is known as overfitting. Many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data, but penalizing the theory in accordance with how complex the theory is. Besides classic overfitting, learners can also disappoint by \"learning the wrong lesson\". A toy example is that an image classifier trained only on pictures of brown horses and black cats might conclude that all brown patches are likely to be horses. A real-world example is that, unlike humans, current image classifiers don't determine the spatial relationship between components of the picture; instead, they learn abstract patterns of pixels that humans are oblivious to, but that linearly correlate with images of certain types of real objects. Faintly superimposing such a pattern on a legitimate image results in an \"adversarial\" image that the system misclassifies. Compared with humans, existing AI lacks several features of human \"commonsense reasoning\"; most notably, humans have powerful mechanisms for reasoning about \"na\u00efve physics\" such as space, time, and physical interactions. This enables even young children to easily make inferences like \"If I roll this pen off a table, it will fall on the floor\". Humans also have a powerful mechanism of \"folk psychology\" that helps them to interpret natural-language sentences such as \"The city councilmen refused the demonstrators a permit because they advocated violence\". (A generic AI has difficulty inferring whether the councilmen or the demonstrators are the ones alleged to be advocating violence.) This lack of \"common knowledge\" means that AI often makes different mistakes than humans make, in ways that can seem incomprehensible. For example, existing self-driving cars cannot reason about the location nor the intentions of pedestrians in the exact way that humans do, and instead must use non-human modes of reasoning to avoid accidents.",
            "score": 95.03709435462952
        },
        {
            "docid": "1534483_2",
            "document": "Motion estimation . Motion estimation is the process of determining motion vectors that describe the transformation from one 2D image to another; usually from adjacent frames in a video sequence. It is an ill-posed problem as the motion is in three dimensions but the images are a projection of the 3D scene onto a 2D plane. The motion vectors may relate to the whole image (global motion estimation) or specific parts, such as rectangular blocks, arbitrary shaped patches or even per pixel. The motion vectors may be represented by a translational model or many other models that can approximate the motion of a real video camera, such as rotation and translation in all three dimensions and zoom.",
            "score": 92.97744679450989
        },
        {
            "docid": "48589354_2",
            "document": "Visual Turing Test . Computer Vision research is driven by standard evaluation practices. The current systems are tested by their accuracy for tasks like object detection, segmentation and localization. Methods like the convolutional neural networks seem to be doing pretty well in these tasks, but the current systems are still not close to solving the ultimate problem of understanding images the way humans do. So motivated by the ability of humans to understand an image and even tell a story about it, Geman \"et al.\" have introduced the Visual Turing Test for computer vision systems.",
            "score": 77.45926666259766
        },
        {
            "docid": "48371359_2",
            "document": "Enhanced flight vision system . An Enhanced flight vision system (EFVS, sometimes EVS) is an airborne system which provides an image of the scene and displays it to the pilot, in order to provide an image in which the scene and objects in it can be better detected. In other words, an EFVS is a system which provides the pilot with an image which is better than unaided human vision. An EFVS includes imaging sensors (one or many) such as a color camera, infrared camera or radar, and typically a display for the pilot, which can be a head-mounted display or head-up display. An EFVS may be combined with a synthetic vision system to create a combined vision system.",
            "score": 68.68212962150574
        },
        {
            "docid": "11027845_3",
            "document": "Canon TS-E 24mm lens . Shifting allows adjusting the position of the subject in the image area without changing the camera angle; it is often used to avoid convergence of parallel lines, such as when photographing a tall building. Tilting the lens relies on the Scheimpflug principle to rotate the plane of focus away from parallel to the image plane; this can be used either to have all parts of an inclined subject sharply rendered, or to restrict sharpness to a small part of a scene. Tilting the lens results in a wedge-shaped depth of field that may be a better fit to some scenes than the depth of field between two parallel planes that results without tilt.",
            "score": 92.44280290603638
        },
        {
            "docid": "18147993_4",
            "document": "Canon TS-E 45mm lens . Shifting allows adjusting the position of the subject in the image area without moving the camera back; it is often used to avoid convergence of parallel lines, such as when photographing a tall building. Tilting the lens relies on the Scheimpflug principle to rotate the plane of focus away from parallel to the image plane; this can be used either to have all parts of an inclined subject sharply rendered, or to restrict sharpness to a small part of a scene. Tilting the lens results in a wedge-shaped depth of field that may be a better fit to some scenes than the depth of field between two parallel planes that results without tilt.",
            "score": 92.44280290603638
        },
        {
            "docid": "24056575_4",
            "document": "Canon TS-E 17mm lens . Shifting allows adjusting the position of the subject in the image area without moving the camera back; it is often used to avoid convergence of parallel lines, such as when photographing a tall building. Tilting the lens relies on the Scheimpflug principle to rotate the plane of focus away from parallel to the image plane; this can be used either to have all parts of an inclined subject sharply rendered, or to restrict sharpness to a small part of a scene. Tilting the lens results in a wedge-shaped depth of field that may be a better fit to some scenes than the depth of field between two parallel planes that results without tilt.",
            "score": 92.44280290603638
        },
        {
            "docid": "18148973_4",
            "document": "Canon TS-E 90mm lens . Shifting allows adjusting the position of the subject in the image area without moving the camera back; it is often used to avoid convergence of parallel lines, such as when photographing a tall building. Tilting the lens relies on the Scheimpflug principle to rotate the plane of focus away from parallel to the image plane; this can be used either to have all parts of an inclined subject sharply rendered, or to restrict sharpness to a small part of a scene. Tilting the lens results in a wedge-shaped depth of field that may be a better fit to some scenes than the depth of field between two parallel planes that results without tilt.",
            "score": 92.44280290603638
        },
        {
            "docid": "55075806_4",
            "document": "Canon TS-E 50mm lens . Shifting allows adjusting the position of the subject in the image area without moving the camera back; it is often used to avoid convergence of parallel lines, such as when photographing a tall building. Tilting the lens relies on the Scheimpflug principle to rotate the plane of focus away from parallel to the image plane; this can be used either to have all parts of an inclined subject sharply rendered, or to restrict sharpness to a small part of a scene. Tilting the lens results in a wedge-shaped depth of field that may be a better fit to some scenes than the depth of field between two parallel planes that results without tilt.",
            "score": 92.44280290603638
        },
        {
            "docid": "55081942_4",
            "document": "Canon TS-E 135mm lens . Shifting allows adjusting the position of the subject in the image area without moving the camera back; it is often used to avoid convergence of parallel lines, such as when photographing a tall building. Tilting the lens relies on the Scheimpflug principle to rotate the plane of focus away from parallel to the image plane; this can be used either to have all parts of an inclined subject sharply rendered, or to restrict sharpness to a small part of a scene. Tilting the lens results in a wedge-shaped depth of field that may be a better fit to some scenes than the depth of field between two parallel planes that results without tilt.",
            "score": 92.44280290603638
        },
        {
            "docid": "5212945_2",
            "document": "Visual neuroscience . Visual Neuroscience is a branch of neuroscience that focuses on the visual system of the human body, mainly located in the brain's visual cortex. The main goal of visual neuroscience is to understand how neural activity results in visual perception, as well as behaviors dependent on vision. In the past, visual neuroscience has focused primarily on how the brain (and in particular the Visual Cortex) responds to light rays projected from static images and onto the retina. While this provides a reasonable explanation for the visual perception of a static image, it does not provide an accurate explanation for how we perceive the world as it really is, an ever-changing, and ever-moving 3-D environment. The topics summarized below are representative of this area, but far from exhaustive.",
            "score": 89.15653991699219
        },
        {
            "docid": "5730335_4",
            "document": "Monocular vision . Monopsia is a medical condition in humans who cannot perceive three-dimensionally even though their two eyes are medically normal, healthy, and spaced apart in a normal way. Vision that perceives three-dimensional depth requires more than parallax. In addition, the resolution of the two disparate images, though highly similar, must be simultaneous, subconscious, and complete. (After-images and \"phantom\" images are symptoms of incomplete visual resolution, even though the eyes themselves exhibit remarkable acuity.) A feature article in \"The New Yorker\" magazine published in early 2006 dealt with one individual in particular, who, learning to cope with her disability, eventually learned how to see three-dimensional depth in her daily life. Medical tests are available for determining monoptic conditions in humans.",
            "score": 69.94120168685913
        },
        {
            "docid": "639660_7",
            "document": "Olfactory receptor neuron . A widely publicized study suggested that humans can detect more than one trillion different odors. This finding has however been disputed. Critics argued that the methodology used for the estimation was fundamentally flawed, showing that applying the same argument for better-understood sensory modalities, such as vision or audition, leads to wrong conclusions. Other researchers have also showed that the result is extremely sensitive to the precise details of the calculation, with small variations changing the result over dozens of orders of magnitude, possibly going as low as a few thousand. The authors of the original study have argued that their estimate holds as long as it is assumed that odor space is sufficiently high-dimensional.",
            "score": 97.76979112625122
        },
        {
            "docid": "40393020_2",
            "document": "Homography (computer vision) . In the field of computer vision, any two images of the same planar surface in space are related by a homography (assuming a pinhole camera model). This has many practical applications, such as image rectification, image registration, or computation of camera motion\u2014rotation and translation\u2014between two images. Once camera rotation and translation have been extracted from an estimated homography matrix, this information may be used for navigation, or to insert models of 3D objects into an image or video, so that they are rendered with the correct perspective and appear to have been part of the original scene (see Augmented reality).",
            "score": 105.98445749282837
        },
        {
            "docid": "4653481_4",
            "document": "NeuronStudio . Deconvolution of imaged data is essential for accurate 3D reconstructions. Deconvolution is an image restoration approach where 'a priori' knowledge of the optical system in the form of a point spread function (PSF) is used to obtain a better estimate of the object. A point spread function can be either calculated from the actual microscope parameters, measured with beads, or estimated and iteratively refined ([Blind Deconvolution]). PSFs can be adjusted locally to account for variations in refractive characteristics of the tissue with depth and sample characteristics. For automated use with large, tiled tissue blocks, this is faster and more accurate than using an experimentally determined PSF.",
            "score": 78.90359163284302
        }
    ],
    "r": [
        {
            "docid": "10450559_8",
            "document": "3D interaction . Users experience a sense of presence when engaged in an immersive virtual world. Enabling the users to interact with this world in 3D allows them to make use of natural and intrinsic knowledge of how information exchange takes place with physical objects in the real world. Texture, sound, and speech can all be used to augment 3D interaction. Currently, users still have difficulty in interpreting 3D space visuals and understanding how interaction occurs. Although it\u2019s a natural way for humans to move around in a three-dimensional world, the difficulty exists because many of the cues present in real environments are missing from virtual environments. Perception and occlusion are the primary perceptual cues used by humans. Also, even though scenes in virtual space appear three-dimensional, they are still displayed on a 2D surface so some inconsistencies in depth perception will still exist.",
            "score": 137.9947052001953
        },
        {
            "docid": "5386671_2",
            "document": "Structure from motion . Structure from motion (SfM) is a photogrammetric range imaging technique for estimating three-dimensional structures from two-dimensional image sequences that may be coupled with local motion signals. It is studied in the fields of computer vision and visual perception. In biological vision, SfM refers to the phenomenon by which humans (and other living creatures) can recover 3D structure from the projected 2D (retinal) motion field of a moving object or scene.",
            "score": 119.48228454589844
        },
        {
            "docid": "7025440_12",
            "document": "Miniature faking . Despite the differences, for a scene that includes relatively little height, lens tilt can produce a result similar to that of a miniature scene, especially if the image is taken from above at a moderate angle to the ground. For a completely flat surface, the effect using tilt would be almost the same as that with a regular lens: the region of focus would be sharp, with progressive blurring toward the top or bottom of the image. The image of Jodhpur was made from such a scene; although the blurring was accomplished with digital postprocessing, a similar result could have been obtained using tilt.",
            "score": 117.26569366455078
        },
        {
            "docid": "1498790_2",
            "document": "Thomas Binford . Thomas Oriel Binford has been a leading researcher in image analysis and computer vision since 1967. He is known for pioneering a model-based approach to computer vision in which complex objects are represented as collections of generalized cylinders. His results reflect seminal work in numerous other areas of research including the interpretation of complex scenes using invariants and quasi-invariants, inference rules and evidential reasoning in extended Bayes networks of symbolic geometric constraints, the SUCCESSOR system, a portable, intelligent vision system, stereo and visual robot navigation, segmentation and feature estimation in complex images, color image analysis, surface material analysis, and image compression. He has led the development of numerous computer vision systems, including systems successfully employed in brain surgery on humans, high-precision automated machining, and helicopter navigation.",
            "score": 113.56221771240234
        },
        {
            "docid": "53785121_3",
            "document": "Visual Information Fidelity . Images and videos of the three dimensional visual environment come from a common class: the class of natural scenes. Natural scenes form a tiny subspace in the space of all possible signals, and researchers have developed sophisticated models to characterize these statistics. Most real-world distortion processes disturb these statistics and make the image or video signals unnatural. The VIF index employs natural scene statistical (NSS) models in conjunction with a distortion (channel) model to quantify the information shared between the test and the reference images. Further, the VIF index is based on the hypothesis that this shared information is an aspect of fidelity that relates well with visual quality. In contrast to prior approaches based on human visual system (HVS) error-sensitivity and measurement of structure, this statistical approach uses in an information-theoretic setting, yields a full reference (FR) quality assessment (QA) method that does not rely on any HVS or viewing geometry parameter, nor any constants requiring optimization, and yet is competitive with state of the art QA methods.",
            "score": 112.21173095703125
        },
        {
            "docid": "29209294_22",
            "document": "Gullies on Mars . It is estimated that a few million years ago, the tilt of the axis of Mars was 45 degrees instead of its present 25 degrees. Its tilt, also called obliquity, varies greatly because its two tiny moons cannot stabilize it, like our relatively large moon does to the Earth. During such periods of high tilt, the summer rays of the sun strike the mid-latitude crater surfaces straight on, thus the surface remains dry.",
            "score": 111.85270690917969
        },
        {
            "docid": "22414730_156",
            "document": "List of Puerto Ricans in the United States Space Program . Miguel Rom\u00e1n, is a research physical scientist with the National Aeronautics and Space Administration (NASA). A major focus of Rom\u00e1n's work is the quantification of uncertainty in long-term satellite measurements of the Earth's surface at both moderate and high spatial resolutions. Rom\u00e1n has developed a combination of global quality assessment and validation techniques that include visible, near-infrared, and multi-angular imagery and measurements acquired from in-situ and airborne platforms. His efforts to improve narrowband reflectance anisotropy models to estimate the Bidirectional Reflectance Distribution Function (BRDF) of land surfaces have led to major advancements in the estimation of terrestrial essential climate variables routinely used to monitor human activity and natural disturbances from satellite platforms. As a lead member of the Moderate Resolution Imaging Spectroradiometer (MODIS) and Visible Infrared Imaging Radiometer Suite (VIIRS) Land Discipline teams, Rom\u00e1n's research has enabled scientists to quantify how much of the variations in satellite observations are due to sensor performance, terrain, and/or geometry (view angle) effects; a critical factor when the satellite data are used to drive policy related to global climate change. Most recently, Dr. Rom\u00e1n's research has focused on capturing seasonal variations in nighttime lights using measurements from the VIIRS Day/Night Band on Suomi-NPP.",
            "score": 111.4027099609375
        },
        {
            "docid": "26580361_6",
            "document": "Digital materialization . Commonplace computer-aided design and manufacturing systems currently represent real objects as \"2.5 dimensional\" shells. In contrast, DM proposes a deeper understanding and sophisticated manipulation of matter by directly using rigorous mathematics as complete volumetric descriptions of real objects. By utilizing technologies such as Function representation (FRep) it becomes possible to compactly describe and understand the surface and internal structures or properties of an object at an infinite resolution. Thus models can accurately represent matter across all scales making it possible to capture the complexity and quality of natural and real objects and ideally suited for digital fabrication and other kinds of real world interactions. DM surpasses the previous limitations of static disassociated languages and simple human-made objects, to propose systems that are heterogeneous, interacting directly and more naturally with the complex world.",
            "score": 111.37469482421875
        },
        {
            "docid": "56397795_5",
            "document": "Humanity Star . Because of its highly reflective surface, Rocket Lab claimed \"Humanity Star\" could be seen by the naked eye from the surface of the Earth. Its apparent brightness was estimated to be magnitude 7.0 when half illuminated and viewed from a distance of , while its maximum brightness was estimated to be magnitude 1.6.",
            "score": 110.2538070678711
        },
        {
            "docid": "34021968_16",
            "document": "Visual tilt effects . Schwartz et al. (2009) proposed that natural scene statistics could also effect changes on orientation tuning curves with the presence of context. The coordination between the surround and the center across segmentation boundaries is greatly reduced, and our visual system takes advantage of this natural statistics feature: increased evidence for segmentation information leads the visual system to decouple the coordination between the center and surround. In their model, a segmentation probability between the test center and context is introduced to control the amount of contextual modulation. And they showed that this model predicts both the direct and indirect forms in the tilt illusion.",
            "score": 110.00495147705078
        },
        {
            "docid": "3201_27",
            "document": "Attribution of recent climate change . Finally, there is extensive statistical evidence from so-called \"fingerprint\" studies. Each factor that affects climate produces a unique pattern of climate response, much as each person has a unique fingerprint. Fingerprint studies exploit these unique signatures, and allow detailed comparisons of modelled and observed climate change patterns. Scientists rely on such studies to attribute observed changes in climate to a particular cause or set of causes. In the real world, the climate changes that have occurred since the start of the Industrial Revolution are due to a complex mixture of human and natural causes. The importance of each individual influence in this mixture changes over time. Of course, there are not multiple Earths, which would allow an experimenter to change one factor at a time on each Earth, thus helping to isolate different fingerprints. Therefore, climate models are used to study how individual factors affect climate. For example, a single factor (like greenhouse gases) or a set of factors can be varied, and the response of the modelled climate system to these individual or combined changes can thus be studied. For example, when climate model simulations of the last century include all of the major influences on climate, both human-induced and natural, they can reproduce many important features of observed climate change patterns. When human influences are removed from the model experiments, results suggest that the surface of the Earth would actually have cooled slightly over the last 50 years (see graph, opposite). The clear message from fingerprint studies is that the observed warming over the last half-century cannot be explained by natural factors, and is instead caused primarily by human factors.",
            "score": 109.73673248291016
        },
        {
            "docid": "30142141_14",
            "document": "Adaptive evolution in the human genome . Studies don\u2019t generally attempt to quantify the average strength of selection propagating advantageous mutations in the human genome. Many models make assumptions about how strong selection is, and some of the discrepancies between the estimates of the amounts of adaptive evolution occurring have been attributed to the use of differing such assumptions (Eyre-Walker 2006). The way to accurately estimate the average strength of positive selection acting on the human genome is by inferring the distribution of fitness effects (DFE) of new advantageous mutations in the human genome, but this DFE is difficult to infer because new advantageous mutations are very rare (Boyko et al. 2008). The DFE may be exponential shaped in an adapted population (Eyre-Walker and Keightley 2007). However, more research is required to produce more accurate estimates of the average strength of positive selection in humans, which will in turn improve the estimates of the amount of adaptive evolution occurring in the human genome (Boyko et al. 2008).",
            "score": 109.65936279296875
        },
        {
            "docid": "48589354_40",
            "document": "Visual Turing Test . The Images considered for the Geman \"et al.\" work are that of \u2018Urban street scenes\u2019 dataset, which has scenes of streets from different cities across the world. This why the types of objects are constrained to people and vehicles for this experiment. Another dataset introduced by the Max Planck Institute for Informatics is known as DAQUAR dataset which has real world images of indoor scenes. But they propose a different version of the visual Turing test which takes on a holistic approach and expects the participating system to exhibit human like common sense. This is a very recent work published on March 9, 2015, in the journal \"Proceedings\" of the National Academy of Sciences, by researchers from Brown University and Johns Hopkins University. It evaluates how the computer vision systems understand the Images as compared to humans. Currently the test is written and the interrogator is a machine because having an oral evaluation by a human interrogator gives the humans an undue advantage of being subjective, and also expects real time answers.",
            "score": 108.82939910888672
        },
        {
            "docid": "30142141_10",
            "document": "Adaptive evolution in the human genome . Many different studies have attempted to quantify the amount of adaptive evolution in the human genome, the vast majority using the comparative approaches outlined above. Although there are discrepancies between studies, generally there is relatively little evidence of adaptive evolution in protein coding DNA, with estimates of adaptive evolution often near 0% (see Table 1). The most obvious exception to this is the 35% estimate of \u03b1 (Fay et al. 2001). This comparatively early study used relatively few loci (fewer than 200) for their estimate, and the polymorphism and divergence data used was obtained from different genes, both of which may have led to an overestimate of \u03b1. The next highest estimate is the 20% value of \u03b1 (Zhang and Li 2005). However, the MK test used in this study was sufficiently weak that the authors state that this value of \u03b1 is not statistically significantly different from 0%. Nielsen et al. (2005a)\u2019s estimate that 9.8% of genes have undergone adaptive evolution also has a large margin of error associated with it, and their estimate shrinks dramatically to 0.4% when they stipulate that the degree of certainty that there has been adaptive evolution must be 95% or more.  This raises an important issue, which is that many of these tests for adaptive evolution are very weak. Therefore the fact that many estimates are at (or very near to) 0% does not rule out the occurrence of any adaptive evolution in the human genome, but simply shows that positive selection is not frequent enough to be detected by the tests. In fact, the most recent study I mention states that confounding variables, such as demographic changes, means that the true value of \u03b1 may be as high as 40% (Eyre-Walker and Keightley 2009). Another recent study, which uses a relatively robust methodology, estimates \u03b1 at 10-20% Boyko et al. (2008). I will comment on weaknesses in the methods in a subsequent section, but it is clear that the debate over the amount of adaptive evolution occurring in human coding DNA is not yet resolved. Even if low estimates of \u03b1 are accurate, a small proportion of substitutions evolving adaptively can still equate to a considerable amount of coding DNA. Many authors, whose studies have small estimates of the amount of adaptive evolution in coding DNA, nevertheless accept that there has been some adaptive evolution in this DNA, because these studies identify specific regions within the human genome which have been evolving adaptively (e.g. Bakewell et al. (2007)). More genes underwent positive selection in chimpanzee evolution than in human), something I will examine later. The generally low estimates of adaptive evolution in human coding DNA can be contrasted with other species. Bakewell et al. (2007) found more evidence of adaptive evolution in chimpanzees than humans, with 1.7% of chimpanzee genes showing evidence of adaptive evolution (compared with the 1.1% estimate for humans; see Table 1). Comparing humans with more distantly related animals, an early estimate for \u03b1 in Drosophila species was 45% (Smith and Eyre-Walker 2002), and later estimates largely agree with this (Eyre-Walker 2006). Bacteria and viruses generally show even more evidence of adaptive evolution; research shows values of \u03b1 in a range of 50-85%, depending on the species examined (Eyre-Walker 2006). Generally, there does appear to be a positive correlation between (effective) population size of the species, and amount of adaptive evolution occurring in the coding DNA regions. This may be because random genetic drift becomes less powerful at altering allele frequencies, compared to natural selection, as population size increases.",
            "score": 108.80184936523438
        },
        {
            "docid": "31001516_3",
            "document": "Smoothing group . By identifying the polygons in a mesh that should appear to be smoothly connected, smoothing groups allow 3D modeling software to estimate the surface normal at any point on the mesh, by averaging the surface normals or vertex normals in the mesh data that describes the mesh. The software can use this data to determine how light interacts with the model. If each polygon lies in a plane, the software could calculate a polygon's surface normal by calculating the normal of the polygon's plane, meaning this data would not have to be stored in the mesh. Thus, early 3D modeling software like 3D Studio Max DOS used smoothing groups as a way to avoid having to store accurate vertex normals for each vertex of the mesh, as a strategy for computer representation of surfaces.",
            "score": 108.44982147216797
        },
        {
            "docid": "19694_71",
            "document": "Mercury (planet) . The difficulties inherent in observing Mercury mean that it has been far less studied than the other planets. In 1800, Johann Schr\u00f6ter made observations of surface features, claiming to have observed mountains. Friedrich Bessel used Schr\u00f6ter's drawings to erroneously estimate the rotation period as 24 hours and an axial tilt of 70\u00b0. In the 1880s, Giovanni Schiaparelli mapped the planet more accurately, and suggested that Mercury's rotational period was 88 days, the same as its orbital period due to tidal locking. This phenomenon is known as synchronous rotation. The effort to map the surface of Mercury was continued by Eugenios Antoniadi, who published a book in 1934 that included both maps and his own observations. Many of the planet's surface features, particularly the albedo features, take their names from Antoniadi's map.",
            "score": 108.4211654663086
        },
        {
            "docid": "2057223_2",
            "document": "Path tracing . Path tracing is a computer graphics Monte Carlo method of rendering images of three-dimensional scenes such that the global illumination is faithful to reality. Fundamentally, the algorithm is integrating over all the illuminance arriving to a single point on the surface of an object. This illuminance is then reduced by a surface reflectance function (BRDF) to determine how much of it will go towards the viewpoint camera. This integration procedure is repeated for every pixel in the output image. When combined with physically accurate models of surfaces, accurate models of real light sources (light bulbs), and optically-correct cameras, path tracing can produce still images that are indistinguishable from photographs.",
            "score": 108.27599334716797
        },
        {
            "docid": "869825_10",
            "document": "Optical flow . Motion estimation and video compression have developed as a major aspect of optical flow research. While the optical flow field is superficially similar to a dense motion field derived from the techniques of motion estimation, optical flow is the study of not only the determination of the optical flow field itself, but also of its use in estimating the three-dimensional nature and structure of the scene, as well as the 3D motion of objects and the observer relative to the scene, most of them using the Image Jacobian.",
            "score": 107.24491882324219
        },
        {
            "docid": "24458151_4",
            "document": "Planetary boundaries . In 2009, a group of Earth system and environmental scientists led by Johan Rockstr\u00f6m from the Stockholm Resilience Centre and Will Steffen from the Australian National University collaborated with 26 leading academics, including Nobel laureate Paul Crutzen, Goddard Institute for Space Studies climate scientist James Hansen and the German Chancellor's chief climate adviser Hans Joachim Schellnhuber and identified nine \"planetary life support systems\" essential for human survival, attempting to quantify how far seven of these systems had been pushed already. They estimated how much further humans can go before planetary habitability is threatened.  Estimates indicated that three of these boundaries\u2014climate change, biodiversity loss, and the biogeochemical flow boundary\u2014appear to have been crossed. The boundaries were \"rough, first estimates only, surrounded by large uncertainties and knowledge gaps\" which interact in complex ways that are not yet well understood. Boundaries were defined to help define a \"safe space for human development\", which was an improvement on approaches aiming at minimizing human impacts on the planet. The 2009 report was presented to the General Assembly of the Club of Rome in Amsterdam. An edited summary of the report was published as the featured article in a special 2009 edition of \"Nature\".  alongside invited critical commentary from leading academics like Nobel laureate Mario J. Molina and biologist Cristi\u00e1n Samper.",
            "score": 106.68594360351562
        },
        {
            "docid": "14768005_2",
            "document": "Conservation psychology . Conservation psychology is the scientific study of the reciprocal relationships between humans and the rest of nature, with a particular focus on how to encourage conservation of the natural world. Rather than a specialty area within psychology itself, it is a growing field for scientists, researchers, and practitioners of all disciplines to come together and better understand the earth and what can be done to preserve it. This network seeks to understand why humans hurt or help the environment and what can be done to change such behavior. The term \"conservation psychology\" refers to any fields of psychology that have understandable knowledge about the environment and the effects humans have on the natural world. Conservation psychologists use their abilities in \"greening\" psychology and make society ecologically sustainable. The science of conservation psychology is oriented toward environmental sustainability, which includes concerns like the conservation of resources, conservation of ecosystems, and quality of life issues for humans and other species.",
            "score": 106.58963012695312
        },
        {
            "docid": "155869_10",
            "document": "Lux . The illuminance on a surface depends on how the surface is tilted with respect to the source. For example, a pocket flashlight aimed at a wall will produce a given level of illumination if aimed perpendicular to the wall, but if the flashlight is aimed at increasing angles to the perpendicular (maintaining the same distance), the illuminated spot becomes larger and so is less highly illuminated. When a surface is tilted at an angle to a source, the illumination provided on the surface is reduced because the tilted surface subtends a smaller solid angle from the source, and therefore it receives less light. For a point source, the illumination on the tilted surface is reduced by a factor equal to the cosine of the angle between a ray coming from the source and the normal to the surface. In practical lighting problems, given information on the way light is emitted from each source and the distance and geometry of the lighted area, a numerical calculation can be made of the illumination on a surface by adding the contributions of every point on every light source.",
            "score": 106.57623291015625
        },
        {
            "docid": "40393020_2",
            "document": "Homography (computer vision) . In the field of computer vision, any two images of the same planar surface in space are related by a homography (assuming a pinhole camera model). This has many practical applications, such as image rectification, image registration, or computation of camera motion\u2014rotation and translation\u2014between two images. Once camera rotation and translation have been extracted from an estimated homography matrix, this information may be used for navigation, or to insert models of 3D objects into an image or video, so that they are rendered with the correct perspective and appear to have been part of the original scene (see Augmented reality).",
            "score": 105.98445892333984
        },
        {
            "docid": "47152350_18",
            "document": "Human performance modeling . A developed area in attention is the control of visual attention - models that attempt to answer, \"where will an individual look next?\" A subset of this concerns the question of visual search: How rapidly can a specified object in the visual field be located? This is a common subject of concern for human factors in a variety of domains, with a substantial history in cognitive psychology. This research continues with modern conceptions of salience and salience maps. Human performance modeling techniques in this area include the work of Melloy, Das, Gramopadhye, and Duchowski (2006) regarding Markov models designed to provide upper and lower bound estimates on the time taken by a human operator to scan a homogeneous display. Another example from Witus and Ellis (2003) includes a computational model regarding the detection of ground vehicles in complex images. Facing the nonuniform probability that a menu option is selected by a computer user when certain subsets of the items are highlighted, Fisher, Coury, Tengs, and Duffy (1989) derived an equation for the optimal number of highlighted items for a given number of total items of a given probability distribution. Because visual search is an essential aspect of many tasks, visual search models are now developed in the context of integrating modeling systems. For example, Fleetwood and Byrne (2006) developed an ACT-R model of visual search through a display of labeled icons - predicting the effects of icon quality and set size not only on search time but on eye movements.",
            "score": 105.36148071289062
        },
        {
            "docid": "45663583_3",
            "document": "Human interactome . With the sequencing of the genomes of a diverse array or model organisms, it became clear that the number of genes does not correlate with the human perception of relative organism complexity \u2013 the human proteome contains some 20 000 genes, which is smaller than some species such as corn. A statistical approach to calculating the number of interactions in humans gives an estimate of around 650 000, one order of magnitude bigger than Drosophila and 3 times larger than C. Elegans. As of 2008, only about <0.3% of all estimated interactions among human proteins has been identified, although in recent years there has been exponential growth in discovery \u2013 as of 2015, over 210 000 unique human positive protein\u2013protein interactions are currently catalogued, and bioGRID database contains almost 750 000 literature-curated PPI's for 30 model organisms, 300 000 of which are verified or predicted human physical or genetic protein\u2013protein interactions, a 50% increase from 2013. The currently available information on the human interactome network originates from either literature-curated interactions, high-throughput experiments, or from potential interactions predicted from interactome data, whether through phylogenetic profiling (evolutionary similarity), statistical network inference, or text/literature mining methods.",
            "score": 105.19495391845703
        },
        {
            "docid": "29150377_17",
            "document": "Empirical theory of perception . Perception of line length is confounded by another optical inverse problem: the further away a line in the world, the smaller the projected line will be on the retina. Different orientations of a line relative to the observer may obscure true line length as well. It is well known that straight lines are erroneously reported as longer or shorter as a function of their angular orientation, as is evident in Fig. 3. While no generally accepted explanations of this phenomenon have been offered previously, the empirical approach has had some success in explaining the effect as a function of the distribution of lines in natural scenes.  Howe and Purves (2002) analyzed natural scene photographs to find projected lines that corresponded to straight line sources. They found that the ratios of the actual length of the lines to the projected lines on the retina, when classified by their respective orientations on the retina, almost perfectly matched subjective estimation of line length as a function of angle relative to the observer. For example, horizontal lines on the retinal image would typically have turned out to issue from relatively short physical sources, while lines at about 60 degrees relative to the observer would typically have signified longer physical sources, which explains why individuals tend to see the 60\u00b0 line in Fig. 3 as longer than the 0\u00b0 (horizontal) line. While there is no way for the visual system to know this \"a priori\", the fact that it seems to take this knowledge for granted in its construction of length estimation percepts strongly supports the wholly empirical view of perception.",
            "score": 105.12479400634766
        },
        {
            "docid": "81633_20",
            "document": "View camera . Tilting achieves the desired depth of field using the aperture at which the lens performs best. Too small an aperture risks losses to diffraction and camera/subject motion what is gained from depth of field. Only testing a given scene, or experience, shows whether tilting is better than leaving the standards neutral and relying on the aperture alone to achieve the desired depth of field. If the scene is sharp enough at f/32 with 2 degrees of tilt but would need f/64 with zero tilt, then tilt is the solution. If another scene would need f/45 with or without tilt, then nothing is gained. See Merklinger and Luong for extensive discussions on determining the optimal tilt (if any) in challenging situations.",
            "score": 105.09561157226562
        },
        {
            "docid": "14768005_6",
            "document": "Conservation psychology . What characterizes conservation psychology research is that in addition to descriptive and theoretical analyses, studies will explore how to cause the kinds of changes that lessen the impact of human behavior on the natural environment, and that lead to more sustainable and harmonious relationships. Some of the research being done with respect to conservation is estimating exactly how much land and water resources are being used by each human at this point along with projected future growth. Also important to consider is the partitioning of land for this future growth. Additionally, conservation efforts look at the positive and negative consequences for the biodiversity of plant and animal life after humans have used the land to their advantage. In addition to creating better conceptual models, more applied research is needed to: 1) identify the most promising strategies for fostering ways of caring about nature, 2) find ways to reframe debates and strategically communicate to the existing values that people have, 3) identify the most promising strategies for shifting the societal discourse about human\u2013nature relationships, and 4) measure the success of these applications with respect to the conservation psychology mission. The ultimate success of conservation psychology will be based on whether its research resulted in programs and applications that made a difference with respect to environmental sustainability. We need to be able to measure the effectiveness of the programs in terms of their impact on behavior formation or behavior change, using tools developed by conservation psychologists.",
            "score": 104.84602355957031
        },
        {
            "docid": "17159818_7",
            "document": "Joseph Somers . Photographs of his canvases give a sense of how they are constructed, but they give no real sense of the illusion of movement. The three wedges have six surfaces. The most common method of composition is to have two separate images painted onto the canvas, one for the left-facing surfaces and another for the right-facing surfaces. Often one image shows a landscape. The other image is often a door, window or other opening. When viewed from a single point, it seems that there are three parallel openings looking onto a single scene. The images are painted not on a two-dimensional surface but instead on three-dimensional wedges, as one moves, the lines of perspective of this 3-D surface change in a way different from that of a 2-D surface, creating a pleasantly disorienting effect of a shifting, swimming surface.",
            "score": 104.68672180175781
        },
        {
            "docid": "501462_5",
            "document": "Computer poker player . The issue of unfair advantage has much to do with what types of information and artificial intelligence are available to the computer program. In addition, bots can play for many hours at a time without human weaknesses such as fatigue and can endure the natural variances of the game without being influenced by human emotion (or \"tilt\"). On the other hand, bots have some significant disadvantages - for example, it is very difficult for a bot to accurately read a bluff or adjust to the strategy of opponents the way humans can.",
            "score": 104.65576934814453
        },
        {
            "docid": "63452_14",
            "document": "Heuristic . There are several ways that humans form and use cognitive maps. Visual intake is a key part of mapping. The first is by using \"landmarks\". This is where a person uses a mental image to estimate a relationship, usually distance, between two objects. Second, is \"route-road\" knowledge, and this is generally developed after a person has performed a task and is relaying the information of that task to another person. Third, is survey. A person estimates a distance based on a mental image that, to them, might appear like an actual map. This image is generally created when a person's brain begins making image corrections. These are presented in five ways: 1. \"Right-angle bias\" is when a person straightens out an image, like mapping an intersection, and begins to give everything 90-degree angles, when in reality it may not be that way. 2. \"Symmetry heuristic\" is when people tend to think of shapes, or buildings, as being more symmetrical than they really are. 3. \"Rotation heuristic\" is when a person takes a naturally (realistically) distorted image and straightens it out for their mental image. 4. \"Alignment heuristic\" is similar to the previous, where people align objects mentally to make them straighter than they really are. 5. \"Relative-position heuristic\": people do not accurately distance landmarks in their mental image based on how well they remember that particular item.",
            "score": 104.59956359863281
        },
        {
            "docid": "167191_19",
            "document": "Phong reflection model . The Phong reflection model in combination with Phong shading is an approximation of shading of objects in real life. This means that the Phong equation can relate the shading seen in a photograph with the surface normals of the visible object. Inverse refers to the wish to estimate the surface normals given a rendered image, natural or computer-made.",
            "score": 103.68291473388672
        },
        {
            "docid": "43127_12",
            "document": "Europa (moon) . Scientists analyzing the unique cracks lining Europa found evidence showing that it likely spun around a tilted axis at some point in time. If correct, this would explain many of Europa's features. Europa's immense network of crisscrossing cracks serves as a record of the stresses caused by massive tides in its global ocean. Europa's tilt could influence calculations of how much of its history is recorded in its frozen shell, how much heat is generated by tides in its ocean, and even how long the ocean has been liquid. Its ice layer must stretch to accommodate these changes. When there is too much stress, it cracks. A tilt in Europa's axis could suggest that its cracks may be much more recent than previously thought. The reason is that the direction of the spin pole may change by as much as a few degrees per day, completing one precession period over several months. A tilt also could affect the estimates of the age of Europa's ocean. Tidal forces are thought to generate the heat that keeps Europa's ocean liquid, and a tilt in the spin axis means that more heat is generated by tidal forces. This heat helps the ocean to remain liquid longer. Scientists did not specify when the tilt would have occurred and measurements have not been made of the tilt of Europa's axis.",
            "score": 103.57512664794922
        }
    ]
}