{
    "q": [
        {
            "docid": "3766002_15",
            "document": "Orbitofrontal cortex . Neurons in the OFC respond both to primary reinforcers, as well as cues that predict rewards across multiple sensory domains. The evidence for responses to visual, gustatory, somatosensory, and olfactory stimuli is robust, but evidence or auditory responses are weaker. In a subset of OFC neurons, neural responses to rewards or reward cues are modulated by individual preference and by internal motivational states such as hunger. A fraction of neurons that respond to sensory cues predicting a reward are selective for reward, and exhibit reversal behavior when cue outcome relationships are swapped. Neurons in the OFC also exhibit responses to the absence of an expected reward, and punishment. Another population of neurons exhibits responses to novel stimuli and can \u201cremember\u201d familiar stimuli for up to a day.",
            "score": 161.2535572052002
        },
        {
            "docid": "33932515_11",
            "document": "Social cue . Cognitive learning models illustrate how people connect cues with certain outcomes or responses. Learning can strengthen associations between predictive cues and outcomes and weaken the link between nondescriptive cues and outcomes. Two aspects of the EXIT model learning phenomena have been focused on by Collins et al. The first is blocking which happens when a new cue is introduced with a cue that already has meaning. The second is highlighting which happens when an individual pays close attention to a cue that will change the meaning of a cue that they already know. When a new cue is added along with a previous one it is said that individuals only focus on the new cue to gain a better understanding as to what is going on.",
            "score": 123.66421961784363
        },
        {
            "docid": "4433814_11",
            "document": "Multinomial logistic regression . The difference between the multinomial logit model and numerous other methods, models, algorithms, etc. with the same basic setup (the perceptron algorithm, support vector machines, linear discriminant analysis, etc.) is the procedure for determining (training) the optimal weights/coefficients and the way that the score is interpreted. In particular, in the multinomial logit model, the score can directly be converted to a probability value, indicating the probability of observation \"i\" choosing outcome \"k\" given the measured characteristics of the observation. This provides a principled way of incorporating the prediction of a particular multinomial logit model into a larger procedure that may involve multiple such predictions, each with a possibility of error. Without such means of combining predictions, errors tend to multiply. For example, imagine a large predictive model that is broken down into a series of submodels where the prediction of a given submodel is used as the input of another submodel, and that prediction is in turn used as the input into a third submodel, etc. If each submodel has 90% accuracy in its predictions, and there are five submodels in series, then the overall model has only .9 = 59% accuracy. If each submodel has 80% accuracy, then overall accuracy drops to .8 = 33% accuracy. This issue is known as error propagation and is a serious problem in real-world predictive models, which are usually composed of numerous parts. Predicting probabilities of each possible outcome, rather than simply making a single optimal prediction, is one means of alleviating this issue.",
            "score": 86.88075876235962
        },
        {
            "docid": "14083964_36",
            "document": "Animal psychopathology . Sugar addiction has been examined in laboratory rats and it develops in the same way that drug addiction develops. Eating sugary foods causes the brain to release natural chemicals called opioids and dopamine in the limbic system. Tasty food can activate opioid receptors in the ventral tegmental area and thereby stimulate cells that release dopamine in the nucleus accumbens (NAc). The brain recognizes the intense pleasure derived from the dopamine and opioids release and learns to crave more sugar. Dependence is created through these natural rewards, the sugary treats, and the opioid and dopamine released into the synapses of the mesolimbic system. The hippocampus, the insula and the caudate activate when rats crave sugar, which are the same areas that become active when drug addicts crave the drug. Sugar is good because it provides energy, but if the nervous system goes through a change and the body becomes dependent on the sugar intake, somatic signs of withdrawal begin to appear like chattering teeth, forepaw tremors and head shakes when sugar is not ingested. Morphine tolerance, a measure of addiction, was observed in rats and their tolerance on Morphine was attributed to environmental cues and the systemic effects of the drug. Morphine tolerance does not depend merely on the frequency of pharmacological stimulation, but rather on both the number of pairings of a drug-predictive cue with the systemic effects of the drug. Rats became significantly more tolerant to morphine when they had been exposed to a paired administration than those rats that were not administered a drug-predictive cue along with the morphine.",
            "score": 155.16350293159485
        },
        {
            "docid": "21312273_15",
            "document": "Metamemory . \"Paired-Associate Judgment of Learning\": These judgments are made at the time of study on cue-target pairs and are responsible for predicting later memory performance (on cued recall or cued recognition). One example of paired-associate JOLs is the cue-target JOL, where the subject determines the retrievability of the target when both the cue and target of the to-be-learned pair are presented. Another example is the cue-only JOL, where the subject must determine the retrievability of the target when only the cue is presented at the time of judgment. These two types of JOLs differ in their accuracy in predicting future performance, and delayed judgments tend to be more accurate.",
            "score": 92.88507556915283
        },
        {
            "docid": "5212259_6",
            "document": "Pars compacta . \"Dopamine neurons are activated by novel, unexpected stimuli, by primary rewards in the absence of predictive stimuli and during learning\". Dopamine neurons are thought to be involved in learning to predict which behaviours will lead to a reward (for example food or sex). In particular, it is suggested that dopamine neurons fire when a reward is greater than that previously expected; a key component of many reinforcement learning models. This signal can then be used to update the expected value of that action. Many recreational drugs, such as cocaine, mimic this reward response\u2014providing an explanation for their addictive nature.",
            "score": 203.09541988372803
        },
        {
            "docid": "33827596_9",
            "document": "Frequency format hypothesis . The \u0394P value as a result is always bound between -1 and 1. Even though the contingency rule is a good model of what humans do in predicting one event causation of another, when it comes to predicting outcomes of events with multiple causes, there exists a large deviation from the contingency rule called the cue-interaction-effect.",
            "score": 85.10618948936462
        },
        {
            "docid": "31285233_13",
            "document": "Motor program . This modular system can be used to describe both motor control and motor learning and requires adaptable internal forward and inverse models. Forward models describe the forward or causal relationship between system inputs, predicting sensory feedback that will occur. Inverse models (controllers) generate the motor command that will cause a desired change in state, given an environmental context. During motor learning, the forward and inverse models are paired and tightly coupled by a responsibility signal within modules. Using the forward model\u2019s predictions and sensory contextual cues, responsibility signals indicate the degree to which each pair should be responsible for controlling current behavior.",
            "score": 93.02123284339905
        },
        {
            "docid": "188540_48",
            "document": "Classical conditioning . An organism's need to predict future events is central to modern theories of conditioning. Most theories use associations between stimuli to take care of these predictions. For example: In the R\u2013W model, the associative strength of a CS tells us how strongly that CS predicts a US. A different approach to prediction is suggested by models such as that proposed by Gallistel & Gibbon (2000, 2002). Here the response is not determined by associative strengths. Instead, the organism records the times of onset and offset of CSs and USs and uses these to calculate the probability that the US will follow the CS. A number of experiments have shown that humans and animals can learn to time events (see Animal cognition), and the Gallistel & Gibbon model yields very good quantitative fits to a variety of experimental data. However, recent studies have suggested that duration-based models cannot account for some empirical findings as well as associative models.",
            "score": 90.4796690940857
        },
        {
            "docid": "26891474_2",
            "document": "PVLV . The primary value learned value (PVLV) model is a possible explanation for the reward-predictive firing properties of dopamine (DA) neurons. It simulates behavioral and neural data on Pavlovian conditioning and the midbrain dopaminergic neurons that fire in proportion to unexpected rewards. It is an alternative to the temporal-differences (TD) algorithm.",
            "score": 164.41504979133606
        },
        {
            "docid": "2538775_2",
            "document": "Predictive modelling . Predictive modelling uses statistics to predict outcomes. Most often the event one wants to predict is in the future, but predictive modelling can be applied to any type of unknown event, regardless of when it occurred. For example, predictive models are often used to detect crimes and identify suspects, after the crime has taken place.",
            "score": 68.63414025306702
        },
        {
            "docid": "673153_6",
            "document": "Dopaminergic pathways . The dopaminergic pathways that project from the substantia nigra pars compacta and ventral tegmental area into the striatum (i.e., the nigrostriatal and mesolimbic pathways, respectively) form one component of a sequence of pathways known as the cortico-basal ganglia-thalamo-cortical loop. This method of classification is used in the study of many psychiatric illness. The nigrostriatal component of the loop consists of the SNc, giving rise to both inhibitory and excitatory pathways that run from the striatum into the globus pallidus, before carrying on to the thalamus, or into the subthalamic nucleus before heading into the thalamus. The dopaminergic neurons in this circuit increase the magnitude of phasic firing in response to positive reward error, that is when the reward exceeds the expected reward. These neurons do not decrease phasic firing during a negative reward prediction (less reward than expected), leading to hypothesis that serotonergic, rather than dopaminergic neurons encode reward loss. Dopamine phasic activity also increases during cues that signal negative events, however dopaminergic neuron stimulation still induces place preference, indicating its main role in evaluating a positive stimulus. From these findings, two hypotheses have developed, as to the role of the basal ganglia and nigrostiatal dopamine circuits in action selection. The first model suggests a \"critic\" which encodes value, and an actor which encodes responses to stimuli based on perceived value. However, the second model proposes that the actions do not originate in the basal ganglia, and instead originate in the cortex and are selected by the basal ganglia. This model proposes that the direct pathway controls appropriate behavior and the indirect suppresses actions not suitable for the situation. This model proposes that tonic dopaminergic firing increases the activity of the direct pathway, causing a bias towards executing actions faster.",
            "score": 184.46588850021362
        },
        {
            "docid": "26565579_50",
            "document": "Neuroscience of free will . Multivariate pattern analysis using EEG has suggested that an evidence based perceptual decision model may be applicable to free will decisions. It was found that decisions could be predicted by neural activity immediately after stimulus perception. Furthermore, when the participant was unable to determine the nature of the stimulus the recent decision history predicted the neural activity (decision). The starting point of evidence accumulation was in effect shifted towards a previous choice (suggesting a priming bias). Another study has found that subliminally priming a participant for a particular decision outcome (showing a cue for 13ms) could be used to influence free decision outcomes. Likewise, it has been found that decision history alone can be used to predict future decisions. The prediction capacities of the Soon et al. (2008) experiment were successfully replicated using a linear SVM model based on participant decision history alone (without any brain activity data). Despite this, a recent study has sought to confirm the applicability of a perceptual decision model to free will decisions. When shown a masked and therefore invisible stimulus, participants were asked to either guess between a category or make a free decision for a particular category. Multivariate pattern analysis using fMRI could be trained on \"free decision\" data to successfully predict \"guess decisions\", and trained on \"guess data\" in order to predict \"free decisions\" (in the precuneus and cuneus region).",
            "score": 95.14243936538696
        },
        {
            "docid": "3766002_10",
            "document": "Orbitofrontal cortex . The OFC and basolteral amygdala (BLA) are highly interconnected, and their connectivity is necessary for devaluation tasks. Damage to either the BLA or the OFC before, but only the OFC after devaluation impairs performance. While the BLA only responds to cues predicting salient outcomes in a graded fashion in accordance with value, the OFC responds to both value and the specific sensory attributes of cue-outcome associations. While OFC neurons that, early in learning, respond to outcome receipt normally transfer their response to the onset of cues that predict the outcome, damage to the BLA impairs this form of learning.",
            "score": 121.30376410484314
        },
        {
            "docid": "22509570_9",
            "document": "Lateralized readiness potential . In a basic cuing paradigm, for an LRP to occur there must be a cue presented that predicts a meaningful stimulus is about to be presented, to which the subject will have to respond. This creates a foreperiod when their response or some instructed behavior is contingent on some event they\u2019ve just been warned will happen. The cue that predicts a future stimulus is usually called the warning stimulus, or cue, and the future stimulus to respond to is usually called the imperative stimulus, or target. Importantly, for the LRP to occur the imperative stimulus must be a cue that indicates which hand the subject should prepare to respond with, so that a period of response preparation occurs. For example, if a cue indicates a 50% chance of responding with the right or left hand, then no LRP is likely to occur. The amplitude of the lateralization effect is thought to represent the amount of differential response preparation elicited by the warning stimulus. The amplitude of the LRP also indicates how close one is to the response threshold\u2014the point in the LRP just before response initiation occurs.",
            "score": 99.66208565235138
        },
        {
            "docid": "13149599_14",
            "document": "Habit . The following is a description of a classic goal devaluation experiment (from a Scientific American MIND guest blog post called Should Habits or Goals Direct Your Life? It Depends) which demonstrates the difference between goal-directed and habitual behavior: A series of elegant experiments conducted by Anthony Dickinson and colleagues in the early 1980s at the University of Cambridge in England clearly exposes the behavioral differences between goal-directed and habitual processes. Basically, in the training phase, a rat was trained to press a lever in order to receive some food. Then, in a second phase, the rat was placed in a different cage without a lever and was given the food, but it was made ill whenever it ate the food. This caused the rat to \"devalue\" the food, because it associated the food with being ill, without directly associating the action of pressing the lever with being ill. Finally, in the test phase, the rat was placed in the original cage with the lever. (To prevent additional learning, no food was delivered in the test phase.) Rats that had undergone an extensive training phase continued to press the lever in the test phase even though the food was devalued; their behavior was called habitual. Rats that had undergone a moderate training phase did not, and their behavior was called goal-directed. \u2026 [G]oal-directed behavior is explained by the rat using an explicit prediction of the consequence, or outcome, of an action to select that action. If the rat wants the food, it presses the lever, because it predicts that pressing the lever will deliver the food. If the food has been devalued, the rat will not press the lever. Habitual behavior is explained by a strong association between an action and the situation from which the action was executed. The rat presses the lever when it sees the lever, not because of the predicted outcome.",
            "score": 98.33846342563629
        },
        {
            "docid": "902940_11",
            "document": "Recognition heuristic . The recognition heuristic is a model that relies on recognition only. This leads to the testable prediction that people who rely on it will ignore strong, contradicting cues (i.e., do not make trade-offs; so-called noncompensatory inferences). In an experiment by Daniel M. Oppenheimer participants were presented with pairs of cities, which included actual cities and fictional cities. Although the recognition heuristic predicts that participants would judge the actual (recognizable) cities to be larger, participants judged the fictional (unrecognizable) cities to be larger, showing that more than recognition can play a role in such inferences.",
            "score": 98.1844094991684
        },
        {
            "docid": "40646055_26",
            "document": "Alignment-free sequence analysis . In the context modeling complexity the next-symbol predictions, of one or more statistical models, are combined or competing to yield a prediction that is based on events recorded in the past. The algorithmic information content derived from each symbol prediction can be used to compute algorithmic information profiles with a time proportional to the length of the sequence. The process has been applied to DNA sequence analysis.",
            "score": 70.67132949829102
        },
        {
            "docid": "52324876_8",
            "document": "Distress tolerance . There are several candidate biological neural network mechanisms for distress tolerance. These proposed brain areas are based on the conceptualization of distress tolerance as a function of reward learning. Within this framework, individuals learn to attune to and pursue reward; reduction of tension in escaping from a stressor is similarly framed as a reward and thus can be learned. Individuals differ in how quickly and for how long they display preferences for pursuing reward or in the case of distress tolerance, escaping from a distressful stimulus. Therefore, brain regions that are activated during reward processing and learning are hypothesized to also serve as neurobiological substrates for distress tolerance. For instance, activation intensity of dopamine neurons projecting to the nucleus accumbens, ventral striatum, and prefrontal cortex is associated with an individual's predicted value of an immediate reward during a learning task. As the firing rate for these neurons increases, individuals predict high values of an immediate reward. During instances in which the predicted value is correct, the basal rate of neuronal firing remains the same. When the predicted reward value is below the actual value, neuronal firing rates increase when the reward is received, resulting in a learned response. When the expected reward value is below the actual value, the firing rate of these neurons decreases below baseline levels, resulting in a learned shift that reduces expectancies about reward value. It is posited that these same dopaminergic firing rates are associated with distress tolerance, in that learning the value of escaping a distressing stimulus is analogous to an estimation of an immediate reward There are several potential clinical implications if these posited distress tolerance substrates are corroborated. It may suggest that distress tolerance is malleable among individuals; interventions that change neuronal firing rates may shift predicted values of behaviors intended to escape a distressor and provide relief, thereby increasing distress tolerance.",
            "score": 188.86246287822723
        },
        {
            "docid": "21167712_2",
            "document": "Comparator hypothesis . The comparator hypothesis is a psychological model of associative learning and performance. To understand the model, it helps to consider how associative learning is usually studied. For example, to study the learning of an association between cues, such as lights and sounds, and an outcome such as food, an experimenter typically pairs the cues and the food a number of times (the learning phase) and then tests with one or more of the cues to see if a response has been learned (the test phase). Most theories of associative learning have assumed that phenomena of interest (see Classical conditioning for a list of phenomena) depend on what happens during the learning phase. The comparator hypothesis assumes, on the contrary, that what happens during the learning phase is fairly simple, and that most interesting phenomena depend on what happens during the test phase. The comparator hypothesis arose primarily in response to so-called \u201ccue competition\u201d effects. If for example in classical conditioning, two conditioned stimuli A and B are presented with an unconditioned stimulus, one may find on test that the subject responds to A or to B or to both or not very much to either. How can one account for such varied results?  First proposed by Ralph Miller' the comparator hypothesis is a model of Pavlovian associations which posits that cue competition effects arise at the time of test, that is during \"performance\", not during learning. The model assumes, essentially, that during conditioning the subject acquires both CS-US and context-US associations. At the time of the test, the associations are compared, and a response to a CS occurs only if the CS-US association is stronger than the context-US association. The model was initially proposed to account for unexplained variations in cue competition effects such as recovery from blocking, but has been expanded to apply more broadly to learning phenomena. The success of the hypothesis has led to modifications in existing theories, such as Wagner's SOP and the Rescorla-Wagner model, enabling them to explain such phenomena as retrospective reevaluation, but other phenomena such as counteraction still pose difficulties for most models.",
            "score": 94.55694091320038
        },
        {
            "docid": "11714184_3",
            "document": "Prediction game . Prediction games are a type of trivia game with a focus on the outcome of guessing future events rather than testing a player's knowledge of the past. They are not considered gambling and are not governed by gambling regulations. Prediction Games have recently become popular on the internet. These speculative games offer topics ranging from the world of entertainment, sports, and finance to various world current events. On the surface, Prediction Games may appear similar to prediction market games; however, there are major differences between the two. Prediction Games are free to play, simple to learn, and pay out real cash to winners. In contrast, Predictive Market Games often require real cash deposits, are more complicated to play (may rely on some knowledge of the stock market or other type of open market exchange), or don't pay out real cash at all.",
            "score": 84.47551083564758
        },
        {
            "docid": "56439577_25",
            "document": "Temporal envelope and fine structure . A computational model of the peripheral auditory system may be used to simulate auditory-nerve fiber responses to complex sounds such as speech, and quantify the transmission (i.e., internal representation) of ENV and TFS cues. In two simulation studies, the mean-rate and spike-timing information was quantified at the output of such a model to characterize, respectively, the short-term rate of neural firing (ENV) and the level of synchronization due to phase locking (TFS) in response to speech sounds degraded by vocoders. The best model predictions of vocoded-speech intelligibility were found when both ENV and TFS cues were included, providing evidence that TFS cues are important for intelligibility when the speech ENV cues are degraded.",
            "score": 77.64992356300354
        },
        {
            "docid": "40933018_7",
            "document": "Heuristics and sports . Take-the-best (TTB) is a heuristic for making inferences about known options based on limited search. Take the best search cues in order of their validity, beginning with the most valid cue. If this cue discriminates between the two objects being compared, the information search is ended and the object with the higher value on this cue is inferred to have a higher criterion value. If the cue does not discriminate between the objects, TTB moves on the next most valid cue, continuing down the line of cues in order of validity until it comes upon a cue that does discriminate. Some evidence from basketball (NBA) demonstrates that TTB can predict the game results as well as an optimizing model based on Bayes' rule.",
            "score": 110.42878437042236
        },
        {
            "docid": "53953041_9",
            "document": "Predictive coding . Evaluating the empirical evidence that suggests a neurologically plausible basis for predictive coding is a broad and varied task. For one thing, and according to the model, predictive coding occurs at every iterative step in the perceptual and cognitive processes; accordingly, manifestations of predictive coding in the brain include genetics, specific cytoarchitecture of cells, systemic networks of neurons, and whole brain analyses. Due to this range of specificity, different methods of investigating the neural mechanisms of predictive coding have been applied, where available; more generally, however, and at least as it relates to humans, there are significant methodological limitations to investigating the potential evidence and much of the work is based on computational modeling of microcircuits in the brain. Notwithstanding, there has been substantial [theoretical] work that has been applied to understanding predictive coding mechanisms in the brain. This section will focus on specific evidence as it relates to the predictive coding phenomenon, rather than analogues, such as homeostasis (which are, nonetheless, integral to our overall understanding of Bayesian inference but already supported heavily; see Clark, 2012 for a review).",
            "score": 115.3949648141861
        },
        {
            "docid": "12865068_6",
            "document": "Familiarity heuristic . The hindsight bias is the inclination to see events that have already occurred as being more predictable than they were before they took place. For example, after a situation occurs for the first time, you begin to notice it when it reoccurs and therefore because you have now experienced it, it's more readily available in your consciousness and you pull information and predict aspects of the future because of this and think that you \"knew it all along.\" \"Hindsight bias results from a biased reconstruction of the original memory trace, using the outcome as a cue\" Hindsight bias can alter memories and therefore future predictions.",
            "score": 100.40562200546265
        },
        {
            "docid": "20638729_12",
            "document": "Causal reasoning . Humans understand cause and effect. Research suggests that other animals, such as rats and monkeys, may or may not understand cause and effect. Animals may use information about cause and effect to improve decision-making and make inferences about past and future events. A constant which guides human reasoning and learning about events is causality. Causal considerations are integral to how people reason about their environment. Humans use causal cues and their related effects to make decisions and predictions and to understand mechanisms leading to change.",
            "score": 108.56363129615784
        },
        {
            "docid": "2094955_32",
            "document": "Salience (language) . Esber and Haselgrove (2011) looked at the use of predictiveness and uncertainty on stimulus salience. They cite the example of a bird watching the water for the presence of fish. Through learning, the bird associates the ripples with the closeness of the fish, but they must be careful of the uncertainty that ripple is not caused by a crocodile. The ripples are very salient, if they are caused by fish suitable for use as food. The likelihood increases that the ripples will catch the attention of the bird and improve its probability of eating the fish, if the ripple cue is a predictor of fish behavior and presence. The predictiveness of ripples reinforces its salience. The Esber\u2013Haselgrove model argues that (1) \u201cstimuli acquire added salience to the degree that they predict motivationally relevant consequences\u201d, and (2) \u201ca predictor of multiple reinforcers should have more salience than a predictor of just one.\u201d (Esber & Haselgrove, 2011, 2555-25557). So, for example, if ripples predict both the presence of fish and the increased likelihood that the fish can be caught, they will be much more salient to the bird, than if the ripples only predict the presence of the fish, but tell the bird nothing about the probability of making a catch.",
            "score": 83.54646182060242
        },
        {
            "docid": "53953041_15",
            "document": "Predictive coding . The empirical evidence for predictive coding is most robust for perceptual processing. As early as 1999, Rao and Ballard proposed a hierarchical visual processing model in which higher-order visual cortical area sends down predictions and the feedforward connections carry the residual errors between the predictions and the actual lower-level activities (Rao and Ballard, 1999). According to this model, each level in the hierarchical model network (except the lowest level, which represents the image) attempts to predict the responses at the next lower level via feedback connections, and the error signal is used to correct the estimate of the input signal at each level concurrently (Rao and Ballard, 1999). Emberson et al. established the top-down modulation in infants using a cross-modal audiovisual omission paradigm, determining that even infant brains have expectation about future sensory input that is carried downstream from visual cortices and are capable of expectation-based feedback (Emberson et al., 2015). Functional near-infrared spectroscopy (fNIRS) data showed that infant occipital cortex responded to unexpected visual omission (with no visual information input) but not to expected visual omission. These results establish that in a hierarchically organized perception system, higher-order neurons send down predictions to lower-order neurons, which in turn sends back up the prediction error signal.",
            "score": 133.70094752311707
        },
        {
            "docid": "175470_65",
            "document": "Magnetic monopole . Cosmological models of the events following the big bang make predictions about what the horizon volume was, which lead to predictions about present-day monopole density. Early models predicted an enormous density of monopoles, in clear contradiction to the experimental evidence. This was called the \"monopole problem\". Its widely accepted resolution was not a change in the particle-physics prediction of monopoles, but rather in the cosmological models used to infer their present-day density. Specifically, more recent theories of cosmic inflation drastically reduce the predicted number of magnetic monopoles, to a density small enough to make it unsurprising that humans have never seen one. This resolution of the \"monopole problem\" was regarded as a success of cosmic inflation theory. (However, of course, it is only a noteworthy success if the particle-physics monopole prediction is correct.) For these reasons, monopoles became a major interest in the 1970s and 80s, along with the other \"approachable\" predictions of GUTs such as proton decay.",
            "score": 78.22930121421814
        },
        {
            "docid": "50773876_14",
            "document": "Algorithm selection . A common approach for multi-class classification is to learn pairwise models between every pair of classes (here algorithms)  and choose the class that was predicted most often by the pairwise models. We can weight the instances of the pairwise prediction problem by the performance difference between the two algorithms. This is motivated by the fact that we care most about getting predictions with large differences correct, but the penalty for an incorrect prediction is small if there is almost no performance difference. Therefore, each instance formula_12 for training a classification model formula_19 vs formula_20 is associated with a cost formula_21.",
            "score": 68.40850305557251
        },
        {
            "docid": "416612_2",
            "document": "Cross-validation (statistics) . Cross-validation, sometimes called rotation estimation, or out-of-sample testing is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice. In a prediction problem, a model is usually given a dataset of \"known data\" on which training is run (\"training dataset\"), and a dataset of \"unknown data\" (or \"first seen\" data) against which the model is tested (called the validation dataset or \"testing set\"). The goal of cross-validation is to test the model\u2019s ability to predict new data that were not used in estimating it, in order to flag problems like overfitting and to give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem).",
            "score": 57.87079095840454
        },
        {
            "docid": "48548_34",
            "document": "Dopamine . Evidence from microelectrode recordings from the brains of animals shows that dopamine neurons in the ventral tegmental area (VTA) and substantia nigra are strongly activated by a wide variety of rewarding events. These reward-responsive dopamine neurons in the VTA and substantia nigra are crucial for reward-related cognition and serve as the central component of the reward system. The function of dopamine varies in each axonal projection from the VTA and substantia nigra; for example, the VTA\u2013nucleus accumbens shell projection assigns incentive salience (\"want\") to rewarding stimuli and its associated cues, the VTA\u2013orbitofrontal cortex projection updates the value of different goals in accordance with their incentive salience, the VTA\u2013amygdala and VTA\u2013hippocampus projections mediate the consolidation of reward-related memories, and both the VTA\u2013nucleus accumbens core and substantia nigra\u2013dorsal striatum pathways are involved in learning motor responses that facilitate the acquisition of rewarding stimuli. Some activity within the VTA dopaminergic projections appears to be associated with reward prediction as well.",
            "score": 178.71832585334778
        }
    ],
    "r": [
        {
            "docid": "5212259_6",
            "document": "Pars compacta . \"Dopamine neurons are activated by novel, unexpected stimuli, by primary rewards in the absence of predictive stimuli and during learning\". Dopamine neurons are thought to be involved in learning to predict which behaviours will lead to a reward (for example food or sex). In particular, it is suggested that dopamine neurons fire when a reward is greater than that previously expected; a key component of many reinforcement learning models. This signal can then be used to update the expected value of that action. Many recreational drugs, such as cocaine, mimic this reward response\u2014providing an explanation for their addictive nature.",
            "score": 203.0954132080078
        },
        {
            "docid": "41578765_21",
            "document": "Paul Glimcher . Glimcher\u2019s laboratory has conducted extensive research on the brain\u2019s reward system, in particular the dopamine system and reinforcement learning. In 2005, with Hannah Bayer, he published the first quantitative test of the Dopamine Reward Prediction Error Hypothesis based on single neuron recordings from dopamine neurons and a novel kernel-based analysis in \"Neuron\".",
            "score": 198.32591247558594
        },
        {
            "docid": "52324876_8",
            "document": "Distress tolerance . There are several candidate biological neural network mechanisms for distress tolerance. These proposed brain areas are based on the conceptualization of distress tolerance as a function of reward learning. Within this framework, individuals learn to attune to and pursue reward; reduction of tension in escaping from a stressor is similarly framed as a reward and thus can be learned. Individuals differ in how quickly and for how long they display preferences for pursuing reward or in the case of distress tolerance, escaping from a distressful stimulus. Therefore, brain regions that are activated during reward processing and learning are hypothesized to also serve as neurobiological substrates for distress tolerance. For instance, activation intensity of dopamine neurons projecting to the nucleus accumbens, ventral striatum, and prefrontal cortex is associated with an individual's predicted value of an immediate reward during a learning task. As the firing rate for these neurons increases, individuals predict high values of an immediate reward. During instances in which the predicted value is correct, the basal rate of neuronal firing remains the same. When the predicted reward value is below the actual value, neuronal firing rates increase when the reward is received, resulting in a learned response. When the expected reward value is below the actual value, the firing rate of these neurons decreases below baseline levels, resulting in a learned shift that reduces expectancies about reward value. It is posited that these same dopaminergic firing rates are associated with distress tolerance, in that learning the value of escaping a distressing stimulus is analogous to an estimation of an immediate reward There are several potential clinical implications if these posited distress tolerance substrates are corroborated. It may suggest that distress tolerance is malleable among individuals; interventions that change neuronal firing rates may shift predicted values of behaviors intended to escape a distressor and provide relief, thereby increasing distress tolerance.",
            "score": 188.86245727539062
        },
        {
            "docid": "48548_33",
            "document": "Dopamine . Within the brain, dopamine functions partly as a \"global reward signal\", where an initial phasic dopamine response to a rewarding stimulus encodes information about the salience, value, and context of a reward. In the context of reward-related learning, dopamine also functions as a \"reward prediction error\" signal, that is, the degree to which the value of a reward is unexpected. According to this hypothesis of Wolfram Schultz, rewards that are expected do not produce a second phasic dopamine response in certain dopaminergic cells, but rewards that are unexpected, or greater than expected, produce a short-lasting increase in synaptic dopamine, whereas the omission of an expected reward actually causes dopamine release to drop below its background level. The \"prediction error\" hypothesis has drawn particular interest from computational neuroscientists, because an influential computational-learning method known as temporal difference learning makes heavy use of a signal that encodes prediction error. This confluence of theory and data has led to a fertile interaction between neuroscientists and computer scientists interested in machine learning.",
            "score": 187.7454071044922
        },
        {
            "docid": "673153_6",
            "document": "Dopaminergic pathways . The dopaminergic pathways that project from the substantia nigra pars compacta and ventral tegmental area into the striatum (i.e., the nigrostriatal and mesolimbic pathways, respectively) form one component of a sequence of pathways known as the cortico-basal ganglia-thalamo-cortical loop. This method of classification is used in the study of many psychiatric illness. The nigrostriatal component of the loop consists of the SNc, giving rise to both inhibitory and excitatory pathways that run from the striatum into the globus pallidus, before carrying on to the thalamus, or into the subthalamic nucleus before heading into the thalamus. The dopaminergic neurons in this circuit increase the magnitude of phasic firing in response to positive reward error, that is when the reward exceeds the expected reward. These neurons do not decrease phasic firing during a negative reward prediction (less reward than expected), leading to hypothesis that serotonergic, rather than dopaminergic neurons encode reward loss. Dopamine phasic activity also increases during cues that signal negative events, however dopaminergic neuron stimulation still induces place preference, indicating its main role in evaluating a positive stimulus. From these findings, two hypotheses have developed, as to the role of the basal ganglia and nigrostiatal dopamine circuits in action selection. The first model suggests a \"critic\" which encodes value, and an actor which encodes responses to stimuli based on perceived value. However, the second model proposes that the actions do not originate in the basal ganglia, and instead originate in the cortex and are selected by the basal ganglia. This model proposes that the direct pathway controls appropriate behavior and the indirect suppresses actions not suitable for the situation. This model proposes that tonic dopaminergic firing increases the activity of the direct pathway, causing a bias towards executing actions faster.",
            "score": 184.46588134765625
        },
        {
            "docid": "18345642_15",
            "document": "Behavioral addiction . Behaviors like gambling have been linked to the new found idea of the brain\u2019s capacity to anticipate rewards. The reward system can be triggered by early detectors of the behavior, and trigger dopamine neurons to begin stimulating behaviors. But in some cases, it can lead to many issues due to error, or reward-prediction errors. These errors can act as teaching signals to create a complex behavior task over time.",
            "score": 182.45382690429688
        },
        {
            "docid": "48548_34",
            "document": "Dopamine . Evidence from microelectrode recordings from the brains of animals shows that dopamine neurons in the ventral tegmental area (VTA) and substantia nigra are strongly activated by a wide variety of rewarding events. These reward-responsive dopamine neurons in the VTA and substantia nigra are crucial for reward-related cognition and serve as the central component of the reward system. The function of dopamine varies in each axonal projection from the VTA and substantia nigra; for example, the VTA\u2013nucleus accumbens shell projection assigns incentive salience (\"want\") to rewarding stimuli and its associated cues, the VTA\u2013orbitofrontal cortex projection updates the value of different goals in accordance with their incentive salience, the VTA\u2013amygdala and VTA\u2013hippocampus projections mediate the consolidation of reward-related memories, and both the VTA\u2013nucleus accumbens core and substantia nigra\u2013dorsal striatum pathways are involved in learning motor responses that facilitate the acquisition of rewarding stimuli. Some activity within the VTA dopaminergic projections appears to be associated with reward prediction as well.",
            "score": 178.7183380126953
        },
        {
            "docid": "18345642_14",
            "document": "Behavioral addiction . One of the most important discoveries of addictions has been the drug based reinforcement and, even more important, reward based learning processes. Several structures of the brain are important in the conditioning process of behavioral addiction; these subcortical structures form the brain regions known as the reward system. One of the major areas of study is the amygdala, a brain structure which involves emotional significance and associated learning. Research shows that dopaminergic projections from the ventral tegmental area facilitate a motivational or learned association to a specific behavior.  Dopamine neurons take a role in the learning and sustaining of many acquired behaviors. Research specific to Parkinson\u2019s disease has led to identifying the intracellular signaling pathways that underlie the immediate actions of dopamine. The most common mechanism of dopamine is to create addictive properties along with certain behaviors. There are three stages to the dopamine reward system: bursts of dopamine, triggering of behavior, and further impact to the behavior. Once electronically signaled, possibly through the behavior, dopamine neurons let out a \u2018burst-fire\u2019 of elements to stimulate areas along fast transmitting pathways. The behavior response then perpetuates the striated neurons to further send stimuli. The fast firing of dopamine neurons can be monitored over time by evaluating the amount of extracellular concentrations of dopamine through micro dialysis and brain imaging. This monitoring can lead to a model in which one can see the multiplicity of triggering over a period of time. Once the behavior is triggered, it is hard to work away from the dopamine reward system.",
            "score": 173.73109436035156
        },
        {
            "docid": "11233144_3",
            "document": "Read Montague . Montague\u2019s work has long focused on computational neuroscience \u2013 the connection between physical mechanisms present in real neural tissue and the computational functions that these mechanisms embody. His early theoretical work focused on the hypothesis that dopaminergic systems encode a particular kind of computational process, a reward prediction error signal, similar to those used in areas of artificial intelligence like optimal control. This work, carried out in collaboration with Peter Dayan and Terry Sejnowski, focused on prediction as a guiding concept in terms of synaptic learning rules that would underlie learning, valuation, and choice. This work proposed a modification to the then dominant idea of Hebbian or correlational learning. In particular, it was shown that dopamine neurons and homologous octopaminergic neurons in bees display a reward prediction error signal exactly consonant with the temporal difference error signal familiar from models of conditioning proposed by Sutton and Barto during the 1980s.",
            "score": 173.39080810546875
        },
        {
            "docid": "716908_14",
            "document": "Ventral tegmental area . As stated above, the VTA, in particular the VTA dopamine neurons, serve several functions in the reward system, motivation, cognition, and drug addiction, and may be the focus of several psychiatric disorders. It has also been shown to process various types of emotion output from the amygdala, where it may also play a role in avoidance and fear-conditioning. Electrophysiological recordings have demonstrated that VTA neurons respond to novel stimuli, unexpected rewards, and reward-predictive sensory cues. The firing pattern of these cells is consistent with the encoding of a reward expectancy error.",
            "score": 171.0343475341797
        },
        {
            "docid": "2982535_8",
            "document": "Habenular nuclei . LHb is especially important in understanding the reward and motivation relationship as it relates to addictive behaviors. The LHb inhibits dopaminergic neurons, decreasing the release of dopamine. It was determined by several animal studies that receiving a reward coincided with elevated dopamine levels, but once the learned association was learned by the animal, dopamine levels remain elevated, only decreasing when the reward is removed. Therefore, dopamine levels only increase with unpredicted rewards and with a \"negative prediction error\". Moreover, it was determined that removal of an anticipated award activated LHb, inhibited dopamine levels. This finding helps explain why addictive drugs are associated with elevated dopamine levels.",
            "score": 169.3542022705078
        },
        {
            "docid": "26891474_2",
            "document": "PVLV . The primary value learned value (PVLV) model is a possible explanation for the reward-predictive firing properties of dopamine (DA) neurons. It simulates behavioral and neural data on Pavlovian conditioning and the midbrain dopaminergic neurons that fire in proportion to unexpected rewards. It is an alternative to the temporal-differences (TD) algorithm.",
            "score": 164.41505432128906
        },
        {
            "docid": "3766002_15",
            "document": "Orbitofrontal cortex . Neurons in the OFC respond both to primary reinforcers, as well as cues that predict rewards across multiple sensory domains. The evidence for responses to visual, gustatory, somatosensory, and olfactory stimuli is robust, but evidence or auditory responses are weaker. In a subset of OFC neurons, neural responses to rewards or reward cues are modulated by individual preference and by internal motivational states such as hunger. A fraction of neurons that respond to sensory cues predicting a reward are selective for reward, and exhibit reversal behavior when cue outcome relationships are swapped. Neurons in the OFC also exhibit responses to the absence of an expected reward, and punishment. Another population of neurons exhibits responses to novel stimuli and can \u201cremember\u201d familiar stimuli for up to a day.",
            "score": 161.25355529785156
        },
        {
            "docid": "8582684_26",
            "document": "Reward system . Addictive drugs and behaviors are rewarding and reinforcing (i.e., are \"addictive\") due to their effects on the dopamine reward pathway. The lateral hypothalamus and medial forebrain bundle has been the most-frequently-studied brain-stimulation reward site, particularly in studies of the effects of drugs on brain stimulation reward. The neurotransmitter system that has been most-clearly identified with the habit-forming actions of drugs-of-abuse is the mesolimbic dopamine system, with its efferent targets in the nucleus accumbens and its local GABAergic afferents. The reward-relevant actions of amphetamine and cocaine are in the dopaminergic synapses of the nucleus accumbens and perhaps the medial prefrontal cortex. Rats also learn to lever-press for cocaine injections into the medial prefrontal cortex, which works by increasing dopamine turnover in the nucleus accumbens. Nicotine infused directly into the nucleus accumbens also enhances local dopamine release, presumably by a presynaptic action on the dopaminergic terminals of this region. Nicotinic receptors localize to dopaminergic cell bodies and local nicotine injections increase dopaminergic cell firing that is critical for nicotinic reward. Some additional habit-forming drugs are also likely to decrease the output of medium spiny neurons as a consequence, despite activating dopaminergic projections. For opiates, the lowest-threshold site for reward effects involves actions on GABAergic neurons in the ventral tegmental area, a secondary site of opiate-rewarding actions on medium spiny output neurons of the nucleus accumbens. Thus GABAergic afferents to the mesolimbic dopamine neurons (primary substrate of opiate reward), the mesolimbic dopamine neurons themselves (primary substrate of psychomotor stimulant reward), and GABAergic efferents to the mesolimbic dopamine neurons (a secondary site of opiate reward) form the core of currently characterized drug-reward circuitry.",
            "score": 160.00291442871094
        },
        {
            "docid": "5166384_30",
            "document": "Primate basal ganglia . Contrarily to the neurons of the pars reticulata-lateralis, dopaminergic neurons are \"low-spiking pacemakers\", spiking at low frequency (0,2 to 10\u00a0Hz) (below 8, Schultz). The role of the dopaminergic neurons has been the source of a considerable literature. As the pathological disappearance of the black neurons was linked to the appearance of Parkinson's disease, their activity was thought to be \"motor\" . A major discovery has been that the stimulation of the black neurons had no motor effect. Their activity is in fact linked to reward and prediction of reward. In a recent review (Schultz 2007), it is demonstrated that phasic responses to reward-related events, notably reward-prediction errors, ...lead to ..dopamine release...\" While it is thought that there could be different behavioral processes including long time regulation. Due to its widespread distribution, the dopaminergic system may regulate the basal ganglia system in many places.",
            "score": 159.83010864257812
        },
        {
            "docid": "1209759_13",
            "document": "Temporal difference learning . Dopamine cells appear to behave in a similar manner. In one experiment measurements of dopamine cells were made while training a monkey to associate a stimulus with the reward of juice. Initially the dopamine cells increased firing rates when the monkey received juice, indicating a difference in expected and actual rewards. Over time this increase in firing back propagated to the earliest reliable stimulus for the reward. Once the monkey was fully trained, there was no increase in firing rate upon presentation of the predicted reward. Continually, the firing rate for the dopamine cells decreased below normal activation when the expected reward was not produced. This mimics closely how the error function in TD is used for reinforcement learning.",
            "score": 156.6533203125
        },
        {
            "docid": "14083964_36",
            "document": "Animal psychopathology . Sugar addiction has been examined in laboratory rats and it develops in the same way that drug addiction develops. Eating sugary foods causes the brain to release natural chemicals called opioids and dopamine in the limbic system. Tasty food can activate opioid receptors in the ventral tegmental area and thereby stimulate cells that release dopamine in the nucleus accumbens (NAc). The brain recognizes the intense pleasure derived from the dopamine and opioids release and learns to crave more sugar. Dependence is created through these natural rewards, the sugary treats, and the opioid and dopamine released into the synapses of the mesolimbic system. The hippocampus, the insula and the caudate activate when rats crave sugar, which are the same areas that become active when drug addicts crave the drug. Sugar is good because it provides energy, but if the nervous system goes through a change and the body becomes dependent on the sugar intake, somatic signs of withdrawal begin to appear like chattering teeth, forepaw tremors and head shakes when sugar is not ingested. Morphine tolerance, a measure of addiction, was observed in rats and their tolerance on Morphine was attributed to environmental cues and the systemic effects of the drug. Morphine tolerance does not depend merely on the frequency of pharmacological stimulation, but rather on both the number of pairings of a drug-predictive cue with the systemic effects of the drug. Rats became significantly more tolerant to morphine when they had been exposed to a paired administration than those rats that were not administered a drug-predictive cue along with the morphine.",
            "score": 155.1634979248047
        },
        {
            "docid": "54842715_43",
            "document": "Interoception . The EPIC model proposes a method of understanding the brain\u2019s response to stimuli contrary to the classic \"stimulus-response\" model. The classical view of information processing is that when a peripheral stimulus provided information to the central nervous system, it was processed in the brain, and a response was elicited. The EPIC model deviates from this and proposes that the brain is involved in a process of active inference, that is, assiduously making predictions about situations based on previous experiences. These predictions, when coupled with incoming sensory signals, allow the brain to compute a prediction error. Interoceptive prediction errors signal the occurrence of discrepancies within the body, which the brain attempts to minimize. This can be done by 1) modifying the predictions through brain-related pathways, 2) altering the body position/location in order to better align incoming sensory signals with the prediction, or 3) altering the brain\u2019s method of receiving incoming stimuli. Interoceptive prediction error signals are a key component of many theories of interoceptive dysfunction in physical and mental health.",
            "score": 153.1902313232422
        },
        {
            "docid": "8757510_5",
            "document": "Nidopallium . The nidopallium is also heavily innervated by dopaminergic neurons from the direction of the brainstem. It is thought that the high concentration of dopamine (a neurotransmitter often involved with motivation, reward circuits and motor control) in this area may contribute to the ability of the NCL to execute higher order cognitive functions. Furthermore, the neural activity of the nidopallium greatly increases when birds are exposed to reward-predicting visual stimuli. This, once more, evidences the considerable presence of dopaminergic neurons in this area, as implied by their stereotypical activation in anticipation of reward-predicting stimuli.",
            "score": 150.9958953857422
        },
        {
            "docid": "640637_8",
            "document": "Aplysia . In \"Aplysia\", the primary reflex studied by scientists while studying operant conditioning is the gill and siphon withdrawal reflex. The gill and siphon withdrawal reflex allows the \"Aplysia\" to pull back its siphon and gill for protection. The links between the synapses during the gill and siphon withdrawal reflex are directly correlated with many behavioral traits in the \"Aplysia\" such as its habits, reflexes, and conditioning. Scientists have studied the conditioning of the \"Aplysia\" to identify correlations with conditioning in mammals, mainly regarding behavioral responses such as addiction. Through experiments on the conditioning of the \"Aplysia\", links have been discovered with the synaptic plasticity for reward functions involved in the trait of addiction within mammals. Synaptic plasticity is the idea that the synapses will become stronger or weaker depending on how much those specific synapses are used. Conditioning of these synapses can lead them to become stronger or weaker by causing the neurons to fire or not fire when influenced by a stimulus. The conditioning of behavioral traits is based on the idea of a reward function. A reward function is when a stimulus is conditioned to fire according to a certain stimulus. The neurons will adapt to that stimulus, and fire those neurons more easily, even if the stimulus has a negative effect on the subject (in this case the Aplysia). In mammals, the reward function is mainly controlled by ventral tegmental area (VTA) dopamine neurons. During conditioning (in mammals), the VTA dopamine neurons have an increased effect on the stimuli being conditioned, and a decreased effect on the stimuli not being conditioned. This induces the synapses to form an expectation for reward for the stimuli being conditioned. The properties of the synapses displayed in the tests on conditioning involving the \"Aplysia\" (which has dopamine neurons but not a ventral tegmental area) are proposed to be directly comparable to behavioral responses such as addiction in mammals.",
            "score": 147.3088836669922
        },
        {
            "docid": "53918629_5",
            "document": "Evolutionary models of human drug use . Ideas concerning the neural bases of motivation and reinforcement in behavior can be traced back to the 1950s. In 1953, Olds and Milner published findings implicating a brain region, specifically a cluster of dopamine neurons, with reward-based learning. Drugs of abuse were later discovered to increase dopamine in the region of the brain associated with reward-based-learning (see: brain stimulation reward).",
            "score": 146.66006469726562
        },
        {
            "docid": "1209759_12",
            "document": "Temporal difference learning . The TD algorithm has also received attention in the field of neuroscience. Researchers discovered that the firing rate of dopamine neurons in the ventral tegmental area (VTA) and substantia nigra (SNc) appear to mimic the error function in the algorithm. The error function reports back the difference between the estimated reward at any given state or time step and the actual reward received. The larger the error function, the larger the difference between the expected and actual reward. When this is paired with a stimulus that accurately reflects a future reward, the error can be used to associate the stimulus with the future reward.",
            "score": 144.7848663330078
        },
        {
            "docid": "8582684_10",
            "document": "Reward system . Two theories exist with regard to the activity of the nucleus accumbens and the generation liking and wanting. The inhibition (or hyperpolarization) hypothesis proposes that the nucleus accumbens exerts tonic inhibitory effects on downstream structures such as the ventral pallidum, hypothalamus or ventral tegmental area, and that in inhibiting in the nucleus accumbens (NAcc), these structures are excited, \"releasing\" reward related behavior. While GABA receptor agonists are capable of eliciting both \"liking\" and \"wanting\" reactions in the nucleus accumbens, glutaminergic inputs from the basolateral amygdala, ventral hippocampus, and medial prefrontal cortex can drive incentive salience. Furthermore, while most studies find that NAcc neurons reduce firing in response to reward, a number of studies find the opposite response. This had lead to the proposal of the disinhibition (or depolarization) hypothesis, that proposes that excitation or NAcc neurons, or at least certain subsets, drives reward related behavior. After nearly 50 years of research on brain-stimulation reward, experts have certified that dozens of sites in the brain will maintain intracranial self-stimulation. Regions include the lateral hypothalamus and medial forebrain bundles, which are especially effective. Stimulation there activates fibers that form the ascending pathways; the ascending pathways include the mesolimbic dopamine pathway, which projects from the ventral tegmental area to the nucleus accumbens. There are several explanations as to why the mesolimbic dopamine pathway is central to circuits mediating reward. First, there is a marked increase in dopamine release from the mesolimbic pathway when animals engage in intracranial self-stimulation. Second, experiments consistently indicate that brain-stimulation reward stimulates the reinforcement of pathways that are normally activated by natural rewards, and drug reward or intracranial self-stimulation can exert more powerful activation of central reward mechanisms because they activate the reward center directly rather than through the peripheral nerves. Third, when animals are administered addictive drugs or engage in naturally rewarding behaviors, such as feeding or sexual activity, there is a marked release of dopamine within the nucleus accumbens. However, dopamine is not the only reward compound in the brain.",
            "score": 144.38079833984375
        },
        {
            "docid": "37691875_19",
            "document": "Addiction-related structural neuroplasticity . There are neurons with cell bodies in the VTA that release dopamine onto specific parts of the brain, including many of the limbic regions such as the NAc, the medial prefrontal cortex (mPFC), dorsal striatum, amygdala, and the hippocampus. The VTA has both dopaminergic and GABAergic neurons that both project to the NAc and mPFC. GABAergic neurons in the VTA also synapse on local dopamine cells. In non-drug models, the VTA dopamine neurons are stimulated by rewarding experiences. A release of dopamine from the VTA neurons seems to be the driving action behind drug-induced pleasure and reward.",
            "score": 141.74343872070312
        },
        {
            "docid": "2806072_12",
            "document": "Habenula . The habenular nuclei are involved in pain processing, reproductive behavior, nutrition, sleep-wake cycles, stress responses, and learning. Recent demonstrations using fMRI and single unit electrophysiology have closely linked the function of the lateral habenula with reward processing, in particular with regard to encoding negative feedback or negative rewards. Matsumoto and Hikosaka suggested in 2007 that this reward and reward-negative information in the brain might \"be elaborated through the interplay among the lateral habenula, the basal ganglia, and monoaminergic (dopaminergic and serotonergic) systems\" and that the lateral habenula may play a pivotal role in this \"integrative function\". Recent evidence suggests that neurons in the lateral habenula signal positive and negative information-prediction errors in addition to positive and negative reward-prediction errors.",
            "score": 141.34840393066406
        },
        {
            "docid": "40149914_4",
            "document": "Metalearning (neuroscience) . Dopamine is proposed to act as a \"global learning\" signal, critical to prediction of rewards and action reinforcement. In this way, dopamine is involved in a learning algorithm in which Actor, Environment and Critic are bound in a dynamic interplay that ultimately seeks to maximise the sum of future rewards by producing an optimal action selection policy. In this context, Critic and Actor are characterised as independent network edges that also form a single Complex Agent. This Agent collectively influences the information state of the Environment, which is fed back to the Agent for future computations. Through a separate pathway, Environment is also fed back to Critic in the form of the reward gained though the given action, meaning an equilibrium can be reached between the predicted reward of given policy for a given state, and the evolving prospect of future rewards.",
            "score": 140.36087036132812
        },
        {
            "docid": "19477293_43",
            "document": "Biology of depression . Regions involved in reward are common targets of manipulation in animal models of depression, including the nucleus accumbens (NAc), ventral tegmental area (VTA), ventral pallidum (VP), lateral habenula (LHb) and medial prefrontal cortex (mPFC). Tentative fMRI studies in humans demonstrate elevated LHb activity in depression. The lateral habenula projects to the RMTg to drive inhibition of dopamine neurons in the VTA during omission of reward. In animal models of depression, elevated activity has been reported in LHb neurons that project to the ventral tegmental area(ostensibly reducing dopamine release). The LHb also projects to aversion reactive mPFC neurons, which may provide an indirect mechanism for producing depressive behaviors. Learned helplessness induced potentiation of LHb synapses are reversed by antidepressant treatment, providing predictive validity. A number of inputs to the LHb have been implicated in producing depressive behaviors. Silencing GABAergic projections from the NAc to the LHb reduces conditioned place preference induced in social aggression, and activation of these terminals induces CPP. Ventral pallidum firing is also elevated by stress induced depression, an effect that is pharmacologically valid, and silencing of these neurons alleviates behavioral correlates of depression. Tentative in vivo evidence from patients with major depression suggests abnormalities in dopamine signalling. This led to early studies investigating VTA activity and manipulations in animal models of depression. Massive destruction of VTA neurons enhances depressive behaviors, while VTA neurons reduce firing in response to chronic stress. However, more recent specific manipulations of the VTA produce varying results, with the specific animal model, duration of VTA manipulation, method of VTA manipulation, and subregion of VTA manipulation all potentially leading to differential outcomes. Stress and social defeat induced depressive symptoms, including anhedonia, are associated with potentiation of excitatory inputs to Dopamine D2 receptor expressing medium spiny neurons (D2-MSNs) and depression of excitatory inputs to Dopamine D1 receptor expressing medium spiny neurons (D1-MSNs). Optogenetic excitation of D1-MSNs alleviates depressive symptoms and is rewarding, while the same with D2-MSNs enhances depressive symptoms. Excitation of glutaminergic inputs from the ventral hippocampus reduces social interactions, and enhancing these projections produces susceptibility to stress induced depression. Manipulations of different regions of the mPFC can produce and attenuate depressive behaviors. For example, inhibiting mPFC neurons specifically in the intralimbic cortex attenuates depressive behaviors. The conflicting findings associated with mPFC stimulation, when compared to the relatively specific findings in the infralimbic cortex, suggest that the prelimbic cortex and infralimbic cortex may mediate opposing effects. mPFC projections to the raphe nuclei are largely GABAergic, and inhibit the firing of serotonergic neurons. Specific activation of these regions reduce immobility in the forced swim test, but do not affect open field or forced swim behavior. Inhibition of the raphe shifts the behavioral phenotype of uncontrolled stress to a phenotype closer to that of controlled stress.",
            "score": 139.24850463867188
        },
        {
            "docid": "53953041_13",
            "document": "Predictive coding . \u201c...prediction neurons... in deep layers of agranular cortex drive active inference by sending sensory predictions via projections ...to supragranular layers of dysgranular and granular sensory cortices. Prediction-error neurons \u2026.in the supragranular layers of granular cortex compute the difference between the predicted and received sensory signal, and send prediction-error signals via projections...back to the deep layers of agranular cortical regions. Precision cells \u2026 tune the gain on predictions and prediction error dynamically, thereby giving these signals reduced (or, in some cases, greater) weight depending on the relative confidence in the descending predictions or the reliability of incoming sensory signals.\u201d (Barrett & Simmons, 2015)",
            "score": 138.78465270996094
        },
        {
            "docid": "128027_28",
            "document": "Operant conditioning . The first scientific studies identifying neurons that responded in ways that suggested they encode for conditioned stimuli came from work by Mahlon deLong and by R.T. Richardson. They showed that nucleus basalis neurons, which release acetylcholine broadly throughout the cerebral cortex, are activated shortly after a conditioned stimulus, or after a primary reward if no conditioned stimulus exists. These neurons are equally active for positive and negative reinforcers, and have been shown to be related to neuroplasticity in many cortical regions. Evidence also exists that dopamine is activated at similar times. There is considerable evidence that dopamine participates in both reinforcement and aversive learning. Dopamine pathways project much more densely onto frontal cortex regions. Cholinergic projections, in contrast, are dense even in the posterior cortical regions like the primary visual cortex. A study of patients with Parkinson's disease, a condition attributed to the insufficient action of dopamine, further illustrates the role of dopamine in positive reinforcement. It showed that while off their medication, patients learned more readily with aversive consequences than with positive reinforcement. Patients who were on their medication showed the opposite to be the case, positive reinforcement proving to be the more effective form of learning when dopamine activity is high.",
            "score": 138.38558959960938
        },
        {
            "docid": "684801_4",
            "document": "Andy Clark . In contrast to traditional models of cognition, which often posit the one-way flow of sensory information from the periphery towards more remote areas of the brain, Clark has suggested a two-way \"cascade of cortical processing\" underlying perception, action, and learning. The concept of predictive processing lies at the heart of this view, wherein top-down predictions attempt to correctly guess or \"explain away\" bottom-up sensory information in an iterative, hierarchical manner. Discrepancies between the expected signal and actual signal, in essence the \"prediction error,\" travel upward to help refine the accuracy of future predictions. Interactions between forward flow of error (conveyed by \"error units\") and backward flow of prediction are dynamic, with attention playing a key role in weighting the relative influence of either at each level of the cascade (dopamine is mentioned as \"one possible mechanism for encoding precision\" with regard to error units). Action (or action-oriented predictive processing) also plays an important role in Clark's account as another means by which the brain can reduce prediction error by directly influencing the environment. To this, he adds that \"personal, affective, and hedonic\" factors would be implicated along with the minimization of prediction error, creating a more nuanced model for the relationship between action and perception.",
            "score": 138.3201446533203
        },
        {
            "docid": "8582684_9",
            "document": "Reward system . Most of the dopamine pathways (i.e., neurons that use the neurotransmitter dopamine to communicate with other neurons) that project out of the ventral tegmental area are part of the reward system; in these pathways, dopamine acts on D1-like receptors or D2-like receptors to either stimulate (D1-like) or inhibit (D2-like) the production of cAMP. The GABAergic medium spiny neurons of the striatum are components of the reward system as well. The glutamatergic projection nuclei in the subthalamic nucleus, prefrontal cortex, hippocampus, thalamus, and amygdala connect to other parts of the reward system via glutamate pathways. The medial forebrain bundle, which is a set of many neural pathways that mediate brain stimulation reward (i.e., reward derived from direct electrochemical stimulation of the lateral hypothalamus), is also a component of the reward system.",
            "score": 137.69374084472656
        },
        {
            "docid": "515094_13",
            "document": "Neuroeconomics . In addition to the importance of specific brain areas to the decision process, there is also evidence that the neurotransmitter dopamine may transmit information about uncertainty throughout the cortex. Dopaminergic neurons are strongly involved in the reward process and become highly active after an unexpected reward occurs. In monkeys, the level of dopaminergic activity is highly correlated with the level of uncertainty such that the activity increases with uncertainty. Furthermore, rats with lesions to the nucleus accumbens, which is an important part of the dopamine reward pathway through the brain, are far more risk averse than normal rats. This suggests that dopamine may be an important mediator of risky behavior.",
            "score": 135.97079467773438
        }
    ]
}