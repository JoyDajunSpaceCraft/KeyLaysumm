{
    "q": [
        {
            "docid": "20395179_7",
            "document": "Vittorio Gallese . Observing the world is more complex than the mere activation of the visual brain. Vision is multimodal: it encompasses the activation of motor, somatosensory and emotion-related brain networks. Any intentional relation entertained with the external world has an intrinsic pragmatic nature, hence it always bears a motor content. The same motor circuits that control our motor behavior also map the space around us, the objects at hand in that very same space, thus defining and shaping in motor terms their representational content. The space around us is defined by the motor potentialities of our body. Motor neurons also respond to visual, tactile and auditory stimuli. Indeed, premotor neurons controlling the movements of the upper arm also respond to tactile stimuli applied to it, to visual stimuli moved within the arm's peripersonal space, or to auditory stimuli also coming from the same peri-personal space. The same applies to artifacts, like three-dimensional objects. The manipulable objects we look at are classified by the motor brain as potential targets of the interactions we might entertain with them. Premotor and parietal 'canonical neurons' control the grasping and manipulation of objects and also respond to their mere observation. The functional architecture of embodied simulation seems to constitute a basic characteristic of our brain, making possible our rich and diversified experiences of space, objects and other individuals, being at the basis of our capacity to empathize with them.\"",
            "score": 141.58006763458252
        },
        {
            "docid": "156431_10",
            "document": "M\u00fcller-Lyer illusion . Neural nets in the visual system of human beings learn how to make a very efficient interpretation of 3D scenes. That is why when somebody goes away from us, we do not perceive them as getting shorter. And when we stretch one arm and look at the two hands we do not perceive one hand smaller than the other. Visual illusions are sometimes held to show us that what we see is an image created in our brain. Our brain supposedly projects the image of the smaller hand to its correct distance in our internal 3D model. This is what is called the size constancy mechanism hypothesis.",
            "score": 140.4873185157776
        },
        {
            "docid": "599917_9",
            "document": "Mental image . The biological foundation of the mind's eye is not fully understood. Studies using fMRI have shown that the lateral geniculate nucleus and the V1 area of the visual cortex are activated during mental imagery tasks. Ratey writes: The visual pathway is not a one-way street. Higher areas of the brain can also send visual input back to neurons in lower areas of the visual cortex. [...] As humans, we have the ability to see with the mind's eye \u2013 to have a perceptual experience in the absence of visual input. For example, PET scans have shown that when subjects, seated in a room, imagine they are at their front door starting to walk either to the left or right, activation begins in the visual association cortex, the parietal cortex, and the prefrontal cortex - all higher cognitive processing centers of the brain.",
            "score": 141.67636442184448
        },
        {
            "docid": "2363287_6",
            "document": "Visual learning . Various areas of the brain work together in a multitude of ways in order to produce the images that we see with our eyes and that are encoded by our brains. The basis of this work takes place in the visual cortex of the brain. The visual cortex is located in the occipital lobe of the brain and harbors many other structures that aid in visual recognition, categorization, and learning. One of the first things the brain must do when acquiring new visual information is recognize the incoming material. Brain areas involved in recognition are the inferior temporal cortex, the superior parietal cortex, and the cerebellum. During tasks of recognition, there is increased activation in the left inferior temporal cortex and decreased activation in the right superior parietal cortex. Recognition is aided by neural plasticity, or the brain's ability to reshape itself based on new information. Next the brain must categorize the material. The three main areas that are used when categorizing new visual information are the orbitofrontal cortex and two dorsolateral prefrontal regions which begin the process of sorting new information into groups and further assimilating that information into things that you might already know. After recognizing and categorizing new material entered into the visual field, the brain is ready to begin the encoding process \u2013 the process which leads to learning. Multiple brain areas are involved in this process such as the frontal lobe, the right extrastriate cortex, the neocortex, and again, the neostriatum. One area in particular, the limbic-diencephalic region, is essential for transforming perceptions into memories. With the coming together of tasks of recognition, categorization and learning; schemas help make the process of encoding new information and relating it to things you already know much easier. One can remember visual images much better when they can apply it to an already known schema. Schemas actually provide enhancement of visual memory and learning.",
            "score": 170.24514412879944
        },
        {
            "docid": "176997_2",
            "document": "Blindsight . Blindsight is the ability of people who are cortically blind due to lesions in their striate cortex, also known as primary visual cortex or V1, to respond to visual stimuli that they do not consciously see. The majority of studies on blindsight are conducted on patients who have the conscious blindness on only one side of their visual field. Following the destruction of the striate cortex, patients are asked to detect, localize, and discriminate amongst visual stimuli that are presented to their blind side, often in a forced-response or guessing situation, even though they do not consciously recognize the visual stimulus. Research shows that blind patients achieve a higher accuracy than would be expected from chance alone. \"Type 1 blindsight\" is the term given to this ability to guess\u2014at levels significantly above chance\u2014aspects of a visual stimulus (such as location or type of movement) without any conscious awareness of any stimuli. \"Type 2 blindsight\" occurs when patients claim to have a feeling that there has been a change within their blind area\u2014e.g. movement\u2014but that it was not a visual percept. Blindsight challenges the common belief that perceptions must enter consciousness to affect our behavior; showing that our behavior can be guided by sensory information of which we have no conscious awareness. It may be thought of as a converse of the form of anosognosia known as Anton\u2013Babinski syndrome, in which there is full cortical blindness along with the confabulation of visual experience.",
            "score": 103.2372921705246
        },
        {
            "docid": "37527148_5",
            "document": "Psychology of film . Film cuts are instantaneous, perceptual, and sometimes temporal discontinuities that do not exist in our own realities. However, despite this, viewers accept cuts as a natural storytelling technique in film. Even though we see reality in a continuous flow of linked images, in movies, cuts seem to work, regardless of how experienced a viewer is. Walter Murch suggests that this is because viewers are in fact used to cuts in their everyday lives through the act of blinking. When you turn to look at an object, for example, you normally blink, thus creating a visual break in continuity between what you \"were\" looking at and what you are now looking at. Another possibility that Murch explores to explain humans\u2019 innate acceptance of film cuts is the way in which we dream. Our dreams tend to jump around from place to place and situation to situation without any real sense of continuity. Thus the oneiric nature of films is familiar to viewers and allows them to innately understand the editing despite discontinuities.",
            "score": 157.25673854351044
        },
        {
            "docid": "599917_31",
            "document": "Mental image . As cognitive neuroscience approaches to mental imagery continued, research expanded beyond questions of serial versus parallel or topographic processing to questions of the relationship between mental images and perceptual representations. Both brain imaging (fMRI and ERP) and studies of neuropsychological patients have been used to test the hypothesis that a mental image is the reactivation, from memory, of brain representations normally activated during the perception of an external stimulus. In other words, if perceiving an apple activates contour and location and shape and color representations in the brain\u2019s visual system, then imagining an apple activates some or all of these same representations using information stored in memory. Early evidence for this idea came from neuropsychology. Patients with brain damage that impairs perception in specific ways, for example by damaging shape or color representations, seem to generally to have impaired mental imagery in similar ways. Studies of brain function in normal human brains support this same conclusion, showing activity in the brain\u2019s visual areas while subjects imagined visual objects and scenes.",
            "score": 155.873774766922
        },
        {
            "docid": "1466175_15",
            "document": "Insight . fMRI and EEG scans of participants completing RAT's demonstrated unique brain activity corresponding to problems solved by insight. For one, there is high EEG activity in the alpha- and gamma-band about 300 milliseconds before participants indicated a solution to insight problems, but not to non-insight problems. Additionally, problems solved by insight corresponded to increased activity in the temporal lobes and mid-frontal cortex, while more activity in the posterior cortex corresponded to non-insight problems. The data suggests there is something different occurring in the brain when solving insight versus non-insight problems that happens right before the solving of the problem. This conclusion has been supported also by eye tracking data which shows an increased eye blink duration and frequency when people solve problems via Insight.  This latter result, paired with an eye pattern oriented to look away from sources of visual inputs ( such as looking at blank wall, or out the window at the sky )proves different attention involvement in Insight problem solving vs problem solving via anlaysis.",
            "score": 141.32078182697296
        },
        {
            "docid": "3205540_14",
            "document": "Reduced affect display . Individuals with schizophrenia with flat affect show decreased activation in the limbic system when viewing emotional stimuli. In individuals with schizophrenia with blunted affect neural processes begin in the occipitotemporal region of the brain and go through the ventral visual pathway and the limbic structures until they reach the inferior frontal areas. Damage to the amygdala of adult rhesus macaques early in life can permanently alter affective processing. Lesioning the amygdala causes blunted affect responses to both positive and negative stimuli. This effect is irreversible in the rhesus macaques; neonatal damage produces the same effect as damage that occurs later in life. The macaques' brain cannot compensate for early amygdala damage even though significant neuronal growth may occur. There is some evidence that blunted affect symptoms in schizophrenia patients are not a result of just amygdala responsiveness, but a result of the amygdala not being integrated with other areas of the brain associated with emotional processing, particularly in amygdala-PFC coupling. Damage in the limbic region prevents the amygdala from being able to correctly interpret emotional stimuli in individuals with schizophrenia by compromising the link between the amygdala and other brain regions associated with emotion.",
            "score": 112.33757281303406
        },
        {
            "docid": "36745368_13",
            "document": "ThinkPad Tablet 2 . In a review for \"IT PRO\" Khidr Suleman wrote, \"During our hands-on, we found the stylus glided across the surface of the display smoothly. The accuracy of the handwriting recognition software was also reasonable and it converted our scribbles into text in the blink of an eye.\" He also wrote, \"We found the device to be easy to hold and Windows 8 ran was smoothly. However, we did find there was a noticeable stutter when playing full HD video and switching between applications. However, as this is a pre-production model, we expect this to be ironed out.\"",
            "score": 125.791428565979
        },
        {
            "docid": "32018467_7",
            "document": "Christian Keysers . After finishing his master, Christian Keysers decided to concentrate on a subfield of cognitive neuroscience called social neuroscience that uses neuroscience methods to understand how we process the social world. He therefore performed his doctoral studies at the University of St Andrews with David Ian Perrett, one of the founding father of the field, to understand how the brain processes faces and facial expressions. This thesis work led to new insights into how quickly the brain can process the faces of others. During this period, Keysers became fascinated with the question of how the brain can attach meaning to the faces of others. How is it for instance, that we understand that a certain grimace would signal that another person is happy? How do we understand that a certain bodily movement towards a glass indicates that the other person aims to grasp a glass? In 1999, Keysers was exposed to a visit of Vittorio Gallese, who presented his recent discovery of mirror neurons in the Psychology department lecture series. This deeply influenced Keysers who decided to move to the lab of Giacomo Rizzolatti to undertake further studies on how these fascinating neurons could contribute to social perception. In 2000, after finishing his doctorate, Christian Keysers moved to the University of Parma to study mirror neurons. In early work there demonstrated that mirror neurons in the premotor cortex not only respond to the sight of actions, but also when actions can only be deduced or heard, leading to a publication in the journal \"Science\". This work had tremendous impact on the field, as it suggested that the premotor cortex could play a central, modality independent role in perception and may lay the origin for the evolution of speech in humans.  Together this work indicated that brain regions involved in our own actions play a role in how we process the actions of others. Keysers wondered whether a similar principle may underlie how we process the tactile sensations and emotions of others, and became increasingly independent of the research focus on the motor system in Parma. At the time, Keysers had also met his to be wife, Valeria Gazzola, a biologist in the final phases of her studies, and together they decided to explore if the somatosensory system might be involved in perceiving the sensations of others. Via a fruitful collaboration with the French neuroimaging specialist Bruno Wicker, they used functional magnetic resonance imaging, and showed for the first time, that the secondary somatosensory cortex, previously thought only to represent a persons own experiences of touch, is also activated when seeing someone or something else be touched. They also showed that the insula, thought only to respond to the experience of first-hand emotions, was also activated when we see another individual experience similar emotions. Together this indicated a much more general principle than the original mirror neuron theory, in which people process the actions, sensations and emotions of others by vicariously activating owns own actions, sensations and emotions. Jointly, this work laid the foundation of the neuroscientific investigation of empathy.",
            "score": 144.2423437833786
        },
        {
            "docid": "1903855_7",
            "document": "Sensory substitution . In a regular visual system, the data collected by the retina is converted into an electrical stimulus in the optic nerve and relayed to the brain, which re-creates the image and perceives it. Because it is the brain that is responsible for the final perception, sensory substitution is possible. During sensory substitution an intact sensory modality relays information to the visual perception areas of the brain so that the person can perceive to see. With sensory substitution, information gained from one sensory modality can reach brain structures physiologically related to other sensory modalities. Touch-to-visual sensory substitution transfers information from touch receptors to the visual cortex for interpretation and perception. For example, through fMRI, we can determine which parts of the brain are activated during sensory perception. In blind persons, we can see that while they are only receiving tactile information, their visual cortex is also activated as they perceive to \"see\" objects. We can also have touch to touch sensory substitution where information from touch receptors of one region can be used to perceive touch in another region. For example, in one experiment by Bach-y-Rita, he was able to restore the touch perception in a patient who lost peripheral sensation from leprosy.",
            "score": 168.63182282447815
        },
        {
            "docid": "525667_10",
            "document": "Human echolocation . In a 2014 study by Thaler and colleagues, the researchers first made recordings of the clicks and their very faint echoes using tiny microphones placed in the ears of the blind echolocators as they stood outside and tried to identify different objects such as a car, a flag pole, and a tree. The researchers then played the recorded sounds back to the echolocators while their brain activity was being measured using functional magnetic resonance imaging. Remarkably, when the echolocation recordings were played back to the blind experts, not only did they perceive the objects based on the echoes, but they also showed activity in those areas of their brain that normally process visual information in sighted people, primarily primary visual cortex or V1. This result is surprising, as visual areas, as their names suggest, are only active during visual tasks. The brain areas that process auditory information were no more activated by sound recordings of outdoor scenes containing echoes than they were by sound recordings of outdoor scenes with the echoes removed. Importantly, when the same experiment was carried out with sighted people who did not echolocate, these individuals could not perceive the objects and there was no echo-related activity anywhere in the brain. This suggests that the cortex of blind echolocators is plastic and reorganizes such that primary visual cortex, rather than any auditory area, becomes involved in the computation of echolocation tasks.",
            "score": 125.04496252536774
        },
        {
            "docid": "613052_11",
            "document": "Direct and indirect realism . Another potential counter-example involves vivid hallucinations: phantom elephants, for instance, might be interpreted as sense-data. A direct realist response would differentiate hallucination from genuine perception: no perception of elephants is going on, only the different and related mental process of hallucination. However, if there are visual images when we hallucinate it seems reasonable that there are visual images when we see. Similarly if dreaming involves visual and auditory images in our minds it seems reasonable to think there are visual and auditory images, or sense-data, when we are awake and perceiving things. This argument has been challenged in a number of different ways. First it has been questioned whether there must be some object present that actually has the experienced qualities, which would then seemingly have to be something like a sense-datum. Why couldn't it be that the perceiver is simply in a state of seeming to experience such an object without any object actually being present? Second, in cases of illusion and perceptual relativity there is an object present which is simply misperceived, usually in readily explainable ways, and no need to suppose that an additional object is also involved. Third, the last part of the perceptual relativity version of the argument has been challenged by questioning whether there is really no experiential difference between veridical and non-veridical perception; and by arguing that even if sense-data are experienced in non-veridical cases and even if the difference between veridical and non-veridical cases is, as claimed, experientially indiscernible, there is still no reason to think that sense-data are the immediate objects of experience in veridical cases. Fourth, do sense-data exist through time or are they momentary? Can they exist when not being perceived? Are they public or private? Can they be themselves misperceived? Do they exist in minds or are they extra-mental, even if not physical? On the basis of the intractability of these questions, it has been argued that the conclusion of the argument from illusion is unacceptable or even unintelligible, even in the absence of a clear diagnosis of exactly where and how it goes wrong.",
            "score": 129.7643678188324
        },
        {
            "docid": "20429570_14",
            "document": "Motor imagery . Motor imagery is close to the notion of simulation used in cognitive and social neuroscience to account for different processes. An individual who is engaging in simulation may replay his own past experience in order to extract from it pleasurable, motivational or strictly informational properties. Such a view was clearly described by the Swedish physiologist Hesslow. For this author, the simulation hypothesis states that thinking consists of simulated interaction with the environment, and rests on the following three core assumptions: (1) Simulation of actions: we can activate motor structures of the brain in a way that resembles activity during a normal action but does not cause any overt movement; (2) Simulation of perception: imagining perceiving something is essentially the same as actually perceiving it, only the perceptual activity is generated by the brain itself rather than by external stimuli; (3) Anticipation: there exist associative mechanisms that enable both behavioral and perceptual activity to elicit other perceptual activity in the sensory areas of the brain. Most importantly, a simulated action can elicit perceptual activity that resembles the activity that would have occurred if the action had actually been performed.",
            "score": 140.56034338474274
        },
        {
            "docid": "613052_12",
            "document": "Direct and indirect realism . Direct realists can potentially deny the existence of any such thing as a mental image but this is difficult to maintain, since we seem able to visually imagine all sorts of things with ease. Even if perception does not involve images other mental processes like imagination certainly seem to. One view, similar to Reid's, is that we do have images of various sorts in our minds when we perceive, dream, hallucinate and imagine but when we actually perceive things, our sensations cannot be considered objects of perception or attention. The only objects of perception are external objects. Even if perception is accompanied by images, or sensations, it is wrong to say we perceive sensations. Direct realism defines perception as perception of external objects where an \"external object\" is allowed to be a photon in the eye but not an impulse in a nerve leading from the eye. Recent work in neuroscience suggests a shared ontology for perception, imagination and dreaming, with similar areas of brain being used for all of these.",
            "score": 160.51119530200958
        },
        {
            "docid": "23544667_5",
            "document": "Daniel Simons . Professor Simons' research has focused on the cognitive underpinnings of our experience of a stable and continuous visual world. One line of research focuses on change blindness. These failures to notice large changes to scenes suggest that we are aware of far less of our visual world than we think. Related studies explore what aspects of our environment automatically capture attention and what objects and events go unnoticed. Such studies reveal the surprising extent of inattentional blindness - the failure to notice unusual and salient events in their visual world when attention is otherwise engaged and the events are unexpected. Other active research interests include scene perception, object recognition, visual memory, visual fading, attention, and driving and distraction. Research in his laboratory adopts methods ranging from real-world and video-based approaches to computer-based psychophysical techniques, and it includes basic behavioral measures, eye tracking, simulator studies, and training studies. This diversity of approaches helps establish closer links between basic research on the mechanisms of attention and the real-world implications and consequences of the findings.",
            "score": 99.67395794391632
        },
        {
            "docid": "725992_3",
            "document": "Blinking . Blinking may have other functions since it occurs more often than necessary just to keep the eye lubricated. Researchers think blinking may help us disengage our attention; following blink onset, cortical activity decreases in the dorsal network and increases in the default-mode network, associated with internal processing. Blink speed can be affected by elements such as fatigue, eye injury, medication, and disease. The blinking rate is determined by the \"blinking center\", but it can also be affected by external stimulus.",
            "score": 134.31344079971313
        },
        {
            "docid": "1215674_2",
            "document": "Visual memory . Visual memory describes the relationship between perceptual processing and the encoding, storage and retrieval of the resulting neural representations. Visual memory occurs over a broad time range spanning from eye movements to years in order to visually navigate to a previously visited location. Visual memory is a form of memory which preserves some characteristics of our senses pertaining to visual experience. We are able to place in memory visual information which resembles objects, places, animals or people in a mental image. The experience of visual memory is also referred to as the mind's eye through which we can retrieve from our memory a mental image of original objects, places, animals or people. Visual memory is one of several cognitive systems, which are all interconnected parts that combine to form the human memory. Types of palinopsia, the persistence or recurrence of a visual image after the stimulus has been removed, is a dysfunction of visual memory.",
            "score": 97.69832515716553
        },
        {
            "docid": "33246145_4",
            "document": "Neural decoding . When looking at a picture, people's brains are constantly making decisions about what object they are looking at, where they need to move their eyes next, and what they find to be the most salient aspects of the input stimulus. As these images hit the back of the retina, these stimuli are converted from varying wavelengths to a series of neural spikes called action potentials. These pattern of action potentials are different for different objects and different colors; we therefore say that the neurons are encoding objects and colors by varying their spike rates or temporal pattern. Now, if someone were to probe the brain by placing electrodes in the primary visual cortex, they may find what appears to be random electrical activity. These neurons are actually firing in response to the lower level features of visual input, possibly the edges of a picture frame. This highlights the crux of the neural decoding hypothesis: that it is possible to reconstruct a stimulus from the response of the ensemble of neurons that represent it. In other words, it is possible to look at spike train data and say that the person or animal being recorded is looking at a red ball.",
            "score": 150.38882219791412
        },
        {
            "docid": "18345264_16",
            "document": "Neural correlates of consciousness . In a related perceptual phenomenon, \"flash suppression\", the percept associated with an image projected into one eye is suppressed by flashing another image into the other eye while the original image remains. Its methodological advantage over binocular rivalry is that the timing of the perceptual transition is determined by an external trigger rather than by an internal event. The majority of cells in the inferior temporal cortex and the superior temporal sulcus of monkeys trained to report their percept during flash suppression follow the animal's percept: when the cell's preferred stimulus is perceived, the cell responds. If the picture is still present on the retina but is perceptually suppressed, the cell falls silent, even though primary visual cortex neurons fire. Single-neuron recordings in the medial temporal lobe of epilepsy patients during flash suppression likewise demonstrate abolishment of response when the preferred stimulus is present but perceptually masked.",
            "score": 104.72014951705933
        },
        {
            "docid": "3205540_13",
            "document": "Reduced affect display . Individuals with schizophrenia with blunted affect show different regional brain activity in fMRI scans when presented with emotional stimuli compared to individuals with schizophrenia without blunted affect. Individuals with schizophrenia without blunted affect show activation in the following brain areas when shown emotionally negative pictures: midbrain, pons, anterior cingulate cortex, insula, ventrolateral orbitofrontal cortex, anterior temporal pole, amygdala, medial prefrontal cortex, and extrastriate visual cortex. Individuals with schizophrenia with blunted affect show activation in the following brain regions when shown emotionally negative pictures: midbrain, pons, anterior temporal pole, and extrastriate visual cortex.",
            "score": 92.04286646842957
        },
        {
            "docid": "1677048_3",
            "document": "Inattentional blindness . Research on inattentional blindness suggests that the phenomenon can occur in any individual, independent of cognitive deficits. However, recent evidence shows that patients with ADHD performed better attentionally when engaging in inattentional blindness tasks than control patients did, suggesting that some mental deficits may decrease the effects of this phenomenon. Recent studies have also looked at age differences and inattentional blindness scores, and results show that the effect increases as humans age. There is mixed evidence that consequential unexpected objects are noticed more: Some studies suggest that we can detect threatening unexpected stimuli more easily than nonthreatening ones, but other studies suggest that this is not the case. There is some evidence that objects associated with reward are noticed more.",
            "score": 52.40416741371155
        },
        {
            "docid": "12510615_4",
            "document": "Disjunctive cognition . Neurobiological research has identified separate areas of the brain responsible for recognizing faces. In humans, identifying unfamiliar faces activates one region of the brain (the Fusiform face area) while recognizing familiar faces also activates another area of the brain (in the lateral midtemporal cortex). A similar division of function is found in macaque monkeys. Such findings indicate that the process of recognizing faces may be achieved by special parts of the brain that are different from the brain areas involved in analyzing the general visual features of things. Since the brain has separate systems for deciding what a person looks like and who the person is, this division of labor may be responsible not only for disjunctive cognitions, but also the phenomenon of transference. In psychoanalytic treatment, patients frequently experience transference, in which the psychoanalyst is perceived to be very much like someone from the patient's past. As in disjunctive cognitions of dreams, the patient may feel \"You look like Dr. X, but you feel like my mother.\" The separate areas of the brain involved in telling us what the person looks like and who the person is may give a neurobiological basis for transference, the phenomenon in which we know who a person is, yet we react emotionally to that person as if they are someone else.",
            "score": 140.4054902791977
        },
        {
            "docid": "53497_13",
            "document": "Optical illusion . In the Ponzo illusion the converging parallel lines tell the brain that the image higher in the visual field is farther away therefore the brain perceives the image to be larger, although the two images hitting the retina are the same size. The optical illusion seen in a diorama/false perspective also exploits assumptions based on monocular cues of depth perception. The M.C. Escher painting \"Waterfall\" exploits rules of depth and proximity and our understanding of the physical world to create an illusion. Like depth perception, motion perception is responsible for a number of sensory illusions. Film animation is based on the illusion that the brain perceives a series of slightly varied images produced in rapid succession as a moving picture. Likewise, when we are moving, as we would be while riding in a vehicle, stable surrounding objects may appear to move. We may also perceive a large object, like an airplane, to move more slowly than smaller objects, like a car, although the larger object is actually moving faster. The phi phenomenon is yet another example of how the brain perceives motion, which is most often created by blinking lights in close succession.",
            "score": 183.25105476379395
        },
        {
            "docid": "5212945_3",
            "document": "Visual neuroscience . A recent study using Event-Related Potentials (ERPs) linked an increased neural activity in the occipito-temporal region of the brain to the visual categorization of facial expressions. Results focus on a negative peak in the ERP that occurs 170 milliseconds after the stimulus onset. This action potential, called the N170, was measured using electrodes in the occipito-temporal region, an area already known to be changed by face stimuli. Studying by using the EEG, and ERP methods allow for an extremely high temporal resolution of 4 milliseconds, which makes these kinds of experiments extremely well suited for accurately estimating and comparing the time it takes the brain to perform a certain function. Scientists used classification image techniques, to determine what parts of complex visual stimuli (such as a face) will be relied on when patients are asked to assign them to a category, or emotion. They computed the important features when the stimulus face exhibited one of five different emotions. Stimulus faces exhibiting fear had the distinguishing feature of widening eyes, and stimuli exhibiting happiness exhibited a change in the mouth to make a smile. Regardless of the expression of the stimuli's face, the region near the eyes affected the EEG before the regions near the mouth. This revealed a sequential, and predetermined order to the perception and processing of faces, with the eye being the first, and the mouth, and nose being processed after. This process of downward integration only occurred when the inferior facial features were crucial to the categorization of the stimuli. This is best explained by comparing what happens when participants were shown a face exhibiting fear, versus happiness. The N170 peaked slightly earlier for the fear stimuli at about 175 milliseconds, meaning that it took a participants less time to recognize the facial expression. This is expected because only the eyes need to be processed to recognize the emotion. However, when processing a happy expression, where the mouth is crucial to categorization, downward integration must take place, and thus the N170 peak occurred later at around 185 milliseconds. Eventually visual neuroscience aims to completely explain how the visual system processes all changes in faces as well as objects. This will give a complete view to how the world is constantly visually perceived, and may provide insight into a link between perception and consciousness.",
            "score": 124.1614283323288
        },
        {
            "docid": "8165347_43",
            "document": "Psychology of art . Humans innately tend to see and have a visual preference for symmetry, an identified quality yielding a positive aesthetic experience that uses an automatic bottom-up factor. This bottom-up factor is speculated to rely on learning experience and visual processing in the brain, suggesting a biological basis. Many studies have ventured to explain this innate preference for symmetry with methods including the Implicit Association Test (IAT). Research suggests that we may prefer symmetry because it is easy to process; hence we have a higher perceptual fluency when works are symmetrical. Fluency research draws on evidence from humans and animals that point to the importance of symmetry regardless of biological necessity. This research highlights the efficiency with which computers recognize and process symmetrical objects relative to non-symmetrical models. There have been investigations regarding the objective features that stimuli contain that may affect the fluency and therefore the preferences. Factors such as amount of information given, the extent of symmetry, and figure-ground contrast are only a few listed in the literature. This preference for symmetry has led to question on how fluency affects our implicit preferences by using the Implicit Association Test. Findings suggest that perceptual fluency is a factor that elicits implicit responses, as shown with the Implicit Association Test results. Research has branched from studying aesthetic pleasure and symmetry on an explicit but also implicit level. In fact, research tries to integrate priming (psychology), cultural influences and the different types of stimuli that may elicit an aesthetic preference.",
            "score": 91.79223549365997
        },
        {
            "docid": "292906_160",
            "document": "Biofeedback . Christopher deCharms (of Omneuron in San Francisco) in conjunction with Stanford University School of Medicine has developed a real-time fMRI for the purpose of training the brain to activate its own endogenous opiates. deCharms believes this will revolutionize the treatment of chronic pain. The patient can control his own pain by visually looking at his rtfMRI, watching his own reactions in real time, and then blocking the pathways causing pain. deCharms mentions that clinical trials with rtfMRI is measuring a 44 to 64 percent decrease in chronic pain. With 8 participants in the study, deCharms et al.(2005) demonstrated that subjects can control the pain of heat stimulus, by visually observing in real time their brain activity. The subjects were instructed to use techniques such as changing the focus of their attention to the pain, and changing the emotional value of the pain. Then while viewing their own fMRI in real time the subjects could observe the effect of their thoughts on the part of the brain called the rostral anterior cingulate cortex (rACC). When the subject 'controlled the pain' the virtual flame on the fMRI got dimmer. Results from this study indicate two things: 1. That subjects can learn to voluntarily control brain activity in a specific region of the brain, and 2. There is a significant increase in the ability of healthy subjects to control their pain with repeated training. This study was then repeated with 8 patients with chronic intractable pain. The results showed that these patients were successful in reducing their pain rating by 64% (using the McGill Pain Questionnaire). The authors state that this is not yet a 'treatment', but still under serious investigation.",
            "score": 98.44803178310394
        },
        {
            "docid": "31148473_6",
            "document": "Transsaccadic memory . Daniel Dennett argued that the way we think we are seeing the world is, for the most part, an illusion. Part of Dennett's argument is the claim that each of us possess what he calls a \"\"Cartesian theater\"\", in which we believe there is a full representation of the visual world in our mind and that there is a place in the mind where it is observed. According to Dennett, none of this exists. Instead, the only place where a full and rich representation exists is directly on the fovea, and every time a saccade occurs, the information is overwritten. Therefore, there is no such thing as transsaccadic memory. Information previously lost only appears to be retained in visual memory because we can look again. In this way, the outside world acts as a visual memory. Since our eyes are constantly moving, we are not aware that visual inputs are constantly being refreshed to give the illusion of the completed picture we think we are seeing.  Dennett makes a distinction between the presence of representation, and the representation of presence. The example he gives regarding this distinction is this: if you were to walk into a room covered in identical portraits of Marilyn Monroe, you would see that there are many of them, but you would not really be seeing them all at once. There would be no detailed representation of each individual portrait just the knowledge that they are present. Dennet's theory raises two relevant questions: 1) How does the visual system detect change in the environment? 2) How much information is retained in each saccade? The proposed answer to these questions lies in several mechanisms that support a high sensitivity to change in each visual fixation. These mechanisms are: retinal adaptation, \"pop-out\" systems, and motion detectors. The implication of this view is that little information is needed to be retained between each saccade.",
            "score": 119.03277480602264
        },
        {
            "docid": "15559385_11",
            "document": "Tactile discrimination . When a person has become blind, in order to \u201csee\u201d the world, their other senses become heightened. An important sense for the blind is their sense of touch, which becomes more frequently used to help them perceive the world. People that are blind have displayed that their visual cortices become more responsive to auditory and tactile stimulation. Braille allows the blind to be able to use their sense of touch to feel the roughness, and distance of various patterns to be used as a form of language. Within the brain, the activation of the occipital cortex is functionally relevant for tactile braille reading, as well as the somatosensory cortex. These various parts of the brain function in their own way, in which they each contribute to the effectiveness of how braille is read by the blind. People that are blind also rely heavily on Tactile Gnosis, Spatial discrimination, Graphesthesia, and Two-point discrimination. Essentially, the occipital cortex allows one to effectively make judgements on the distance of braille patterns, which is related to spatial discrimination. Meanwhile, the somatosensory cortex allows one to effectively make judgements on the roughness of braille patterns, which is related to two-point discrimination. The various visual areas in the brain are very essential for a blind person to read braille, just as much as it is for a person that has sight. Essentially, whether one is blind or not, the perception of objects that involves tactile discrimination is not impaired if one cannot see. When comparing people that are blind to people that have sight, the amount of activity within the their somatosensory and visual areas of the brain do differ. The activity in the somatosensory and visual areas are not as high in tactile gnosis for people that are not blind, and are more-so active for more visual related stimuli that does not involve touch. Nonetheless, there is a difference in these various areas within the brain when comparing the blind to the sighted, which is that shape discrimination causes a difference in brain activity, as well as tactile gnosis. The visual cortices of blind individuals are active during various vision related tasks including tactile discrimination, and the function of the cortices resemble the activity of adults with sight.",
            "score": 136.75331616401672
        },
        {
            "docid": "25146378_20",
            "document": "Functional specialization (brain) . Other researchers who provide evidence to support the theory of distributive processing include Anthony McIntosh and William Uttal, who question and debate localization and modality specialization within the brain. McIntosh's research suggests that human cognition involves interactions between the brain regions responsible for processes sensory information, such as vision, audition, and other mediating areas like the prefrontal cortex. McIntosh explains that modularity is mainly observed in sensory and motor systems, however, beyond these very receptors, modularity becomes \"fuzzier\" and you see the cross connections between systems increase. He also illustrates that there is an overlapping of functional characteristics between the sensory and motor systems, where these regions are close to one another. These different neural interactions influence each other, where activity changes in one area influence other connected areas. With this, McIntosh suggest that if you only focus on activity in one area, you may miss the changes in other integrative areas. Neural interactions can be measured using analysis of covariance in neuroimaging. McIntosh used this analysis to convey a clear example of the interaction theory of distributive processing. In this study, subjects learned that an auditory stimulus signalled a visual event. McIntosh found activation (an increase blood flow), in an area of the occipital cortex, a region of the brain involved in visual processing, when the auditory stimulus was presented alone. Correlations between the occipital cortex and different areas of the brain such as the prefrontal cortex, premotor cortex and superior temporal cortex showed a pattern of co-variation and functional connectivity.",
            "score": 143.71508872509003
        },
        {
            "docid": "32099529_11",
            "document": "Oblique effect . Nevertheless, there is an oblique effect for target configurations that do not directly address these \"oriented\" neural elements early in the visual path into the brain. Regardless of where in the brain of the human or animals an oblique effect is found, one would still like to know whether it is an inevitable consequence of the way neural signals are processed, or whether it is a minor error that nature hadn't been bothered to correct, or whether it fulfills a function in making us better in handling our visual environment. Proposing a \"purpose\" of the oblique effect, and developing scientific support for it is still a work in progress. A popular concept is that we live in a carpentered environment. Attempts at empirical explanations of perceptual visual phenomena have led to the examination of the orientation distribution of contours in the everyday visual world.",
            "score": 118.04654049873352
        }
    ],
    "r": [
        {
            "docid": "53497_13",
            "document": "Optical illusion . In the Ponzo illusion the converging parallel lines tell the brain that the image higher in the visual field is farther away therefore the brain perceives the image to be larger, although the two images hitting the retina are the same size. The optical illusion seen in a diorama/false perspective also exploits assumptions based on monocular cues of depth perception. The M.C. Escher painting \"Waterfall\" exploits rules of depth and proximity and our understanding of the physical world to create an illusion. Like depth perception, motion perception is responsible for a number of sensory illusions. Film animation is based on the illusion that the brain perceives a series of slightly varied images produced in rapid succession as a moving picture. Likewise, when we are moving, as we would be while riding in a vehicle, stable surrounding objects may appear to move. We may also perceive a large object, like an airplane, to move more slowly than smaller objects, like a car, although the larger object is actually moving faster. The phi phenomenon is yet another example of how the brain perceives motion, which is most often created by blinking lights in close succession.",
            "score": 183.2510528564453
        },
        {
            "docid": "2138419_3",
            "document": "Rapid serial visual presentation . There is a delay of several hundred milliseconds. A person might be asked to identify numbers in a string of letters which are shown one by one. The first number which is an important target, would be caught by the person, however, the second number flashed seconds later might not be observed. RSVP asks the question, What would reading be like if there were no eye movements? A text is delivered at a spot on the screen, like a series of flash cards. The user can set how long each card is to be displayed. The readers are liberated from having to decide how much time to spend on each word because that is set in advance, and saccades, regressive eye movements, line sweeps, and page turning have been eliminated. A reader can fully concentrate on comprehending the text as it flashes through, however, with longer texts the reading experience is found to be monotonous and exhausting. There are a number of theories to explain how and why this works and studies have explored its limitations and parameters to learn more about visual perception. The brain deals with a quick stream of incoming information at all times. With the attentional blink, the brain has to distribute its attentional resources to comprehend, interpret, and store the information properly. The human brain is capable of processing complex tasks, but it has restrictions. The attentional blink is an illustration that has a significant insinuation for individuals who work in environments where they are usually swamped with information. An example of this is an airport baggage screener who might see a knife in one bag, but misses a second knife in another bag that is right behind the first bag. The failure to recognize the second target is because of the attentional processes that are linked with the identification of the first target.",
            "score": 179.68894958496094
        },
        {
            "docid": "2363287_6",
            "document": "Visual learning . Various areas of the brain work together in a multitude of ways in order to produce the images that we see with our eyes and that are encoded by our brains. The basis of this work takes place in the visual cortex of the brain. The visual cortex is located in the occipital lobe of the brain and harbors many other structures that aid in visual recognition, categorization, and learning. One of the first things the brain must do when acquiring new visual information is recognize the incoming material. Brain areas involved in recognition are the inferior temporal cortex, the superior parietal cortex, and the cerebellum. During tasks of recognition, there is increased activation in the left inferior temporal cortex and decreased activation in the right superior parietal cortex. Recognition is aided by neural plasticity, or the brain's ability to reshape itself based on new information. Next the brain must categorize the material. The three main areas that are used when categorizing new visual information are the orbitofrontal cortex and two dorsolateral prefrontal regions which begin the process of sorting new information into groups and further assimilating that information into things that you might already know. After recognizing and categorizing new material entered into the visual field, the brain is ready to begin the encoding process \u2013 the process which leads to learning. Multiple brain areas are involved in this process such as the frontal lobe, the right extrastriate cortex, the neocortex, and again, the neostriatum. One area in particular, the limbic-diencephalic region, is essential for transforming perceptions into memories. With the coming together of tasks of recognition, categorization and learning; schemas help make the process of encoding new information and relating it to things you already know much easier. One can remember visual images much better when they can apply it to an already known schema. Schemas actually provide enhancement of visual memory and learning.",
            "score": 170.24514770507812
        },
        {
            "docid": "1903855_7",
            "document": "Sensory substitution . In a regular visual system, the data collected by the retina is converted into an electrical stimulus in the optic nerve and relayed to the brain, which re-creates the image and perceives it. Because it is the brain that is responsible for the final perception, sensory substitution is possible. During sensory substitution an intact sensory modality relays information to the visual perception areas of the brain so that the person can perceive to see. With sensory substitution, information gained from one sensory modality can reach brain structures physiologically related to other sensory modalities. Touch-to-visual sensory substitution transfers information from touch receptors to the visual cortex for interpretation and perception. For example, through fMRI, we can determine which parts of the brain are activated during sensory perception. In blind persons, we can see that while they are only receiving tactile information, their visual cortex is also activated as they perceive to \"see\" objects. We can also have touch to touch sensory substitution where information from touch receptors of one region can be used to perceive touch in another region. For example, in one experiment by Bach-y-Rita, he was able to restore the touch perception in a patient who lost peripheral sensation from leprosy.",
            "score": 168.63182067871094
        },
        {
            "docid": "53472_6",
            "document": "Illusion . An optical illusion is characterised by visually perceived images that are deceptive or misleading. Therefore, the information gathered by the eye is processed by the brain to give, on the face of it, a percept that does not tally with a physical measurement of the stimulus source. A conventional assumption is that there are physiological illusions that occur naturally and cognitive illusions that can be demonstrated by specific visual tricks that say something more basic about how human perceptual systems work. The human brain constructs a world inside our head based on what it samples from the surrounding environment. However, sometimes it tries to organise this information it thinks best while other times it fills in the gaps. This way in which our brain works is the basis of an illusion.",
            "score": 160.5532684326172
        },
        {
            "docid": "613052_12",
            "document": "Direct and indirect realism . Direct realists can potentially deny the existence of any such thing as a mental image but this is difficult to maintain, since we seem able to visually imagine all sorts of things with ease. Even if perception does not involve images other mental processes like imagination certainly seem to. One view, similar to Reid's, is that we do have images of various sorts in our minds when we perceive, dream, hallucinate and imagine but when we actually perceive things, our sensations cannot be considered objects of perception or attention. The only objects of perception are external objects. Even if perception is accompanied by images, or sensations, it is wrong to say we perceive sensations. Direct realism defines perception as perception of external objects where an \"external object\" is allowed to be a photon in the eye but not an impulse in a nerve leading from the eye. Recent work in neuroscience suggests a shared ontology for perception, imagination and dreaming, with similar areas of brain being used for all of these.",
            "score": 160.51119995117188
        },
        {
            "docid": "37527148_5",
            "document": "Psychology of film . Film cuts are instantaneous, perceptual, and sometimes temporal discontinuities that do not exist in our own realities. However, despite this, viewers accept cuts as a natural storytelling technique in film. Even though we see reality in a continuous flow of linked images, in movies, cuts seem to work, regardless of how experienced a viewer is. Walter Murch suggests that this is because viewers are in fact used to cuts in their everyday lives through the act of blinking. When you turn to look at an object, for example, you normally blink, thus creating a visual break in continuity between what you \"were\" looking at and what you are now looking at. Another possibility that Murch explores to explain humans\u2019 innate acceptance of film cuts is the way in which we dream. Our dreams tend to jump around from place to place and situation to situation without any real sense of continuity. Thus the oneiric nature of films is familiar to viewers and allows them to innately understand the editing despite discontinuities.",
            "score": 157.25674438476562
        },
        {
            "docid": "599917_31",
            "document": "Mental image . As cognitive neuroscience approaches to mental imagery continued, research expanded beyond questions of serial versus parallel or topographic processing to questions of the relationship between mental images and perceptual representations. Both brain imaging (fMRI and ERP) and studies of neuropsychological patients have been used to test the hypothesis that a mental image is the reactivation, from memory, of brain representations normally activated during the perception of an external stimulus. In other words, if perceiving an apple activates contour and location and shape and color representations in the brain\u2019s visual system, then imagining an apple activates some or all of these same representations using information stored in memory. Early evidence for this idea came from neuropsychology. Patients with brain damage that impairs perception in specific ways, for example by damaging shape or color representations, seem to generally to have impaired mental imagery in similar ways. Studies of brain function in normal human brains support this same conclusion, showing activity in the brain\u2019s visual areas while subjects imagined visual objects and scenes.",
            "score": 155.873779296875
        },
        {
            "docid": "2872287_23",
            "document": "Neural binding . Much of the experimental evidence for neural binding has traditionally revolved around sensory awareness. Sensory awareness is accomplished by integrating things together by cognitively perceiving them and then segmenting them so that, in total, there is an image created. Since there can be an infinite number of possibilities in the perception of an object, this has been a unique area of study. The way the brain then collectively pieces certain things together via networking is important not only in the global way of perceiving but also in segmentation. Much of sensory awareness has to do with the taking of a single piece of an object's makeup and then binding its total characteristics so that the brain perceives the object in its final form. Much of the research for the understanding of segmentation and how the brain perceives an object has been done by studying cats. A major finding of this research has to do with the understanding of gamma waves oscillating at 40\u00a0Hz. The information was extracted from a study using the cat visual cortex. It was shown that the cortical neurons responded differently to spatially different objects. These firings of neurons ranged from 40\u201360\u00a0Hz in measure and when observed showed that they fired synchronously when observing different parts of the object. Such coherent responses point to the fact that the brain is doing a kind of coding where it is piecing certain neurons together in the works of making the form of an object. Since the brain is putting these segmented pieces together unsupervised, a significant consonance is found with many philosophers (like Sigmund Freud) who theorize an underlying subconscious that helps to form every aspect of our conscious thought processes.",
            "score": 155.833251953125
        },
        {
            "docid": "6733469_3",
            "document": "Max Planck Institute for Biological Cybernetics . The institute is studying signal and information processing in the brain. We know that our brain is constantly processing a vast amount of sensory and intrinsic information by which our behavior is coordinated accordingly. How the brain actually achieves these tasks is less well understood, for example, how it perceives, recognizes, and learns new objects. The scientists at the Max Planck Institute for Biological Cybernetics aim to determine which signals and processes are responsible for creating a coherent percept of our environment and for eliciting the appropriate behavior. Scientists of three departments and seven research groups are working towards answering fundamental questions about processing in the brain, using different approaches and methods.",
            "score": 155.7928466796875
        },
        {
            "docid": "599252_7",
            "document": "Fallacy of the single cause . A notable scientific example of what can happen when this kind of fallacy is identified and resolved is the development in economics of the Coase theorem. In neuroscience, this fallacy is in evidence when a scientist electrically stimulates one area of the brain, observes an effect (say, the patient blinks his eyes), and assumes that the brain area is sufficient for the movement (the essence of blinking) rather than merely necessary.",
            "score": 155.71884155273438
        },
        {
            "docid": "21647661_3",
            "document": "Self model . The PSM is an entity that \u201cactually exists, not only as a distinct theoretical entity but something that will be empirically discovered in the future- for instance, as a specific stage of the global neural dynamics in the human brain\u201d. Involved in the PSM are three phenomenal properties that must occur in order to explain the concept of the self. The first is mineness, \u201ca higher order property of particular forms of phenomenal content,\u201d or the idea of ownership. The second is perspectivalness, which is \u201ca global, structural property of phenomenal space as a whole\u201d. More simply, it is what is commonly referred to as the ecological self, the immovable center of perception. The third phenomenal property is selfhood, which is \u201cthe phenomenal target property\u201d or the idea of the self over time. It is the property of phenomenal selfhood that plays the most important role in creating the fictional self and the first person perspective. Metzinger defines the first person perspective as the \u201cexistence of single coherent and temporally stable model of reality which is representationally centered around or on a single coherent and temporally stable phenomenal subject\u201d. The first-person perspective can be non-conceptual and is autonomously active due to the constant reception of perceptual information by the brain. The brain, specifically the brainstem and hypothalamus, processes this information into representational content, namely linguistic reflections. The PSM then uses this representational content to attribute phenomenal states to our perceived objects and ourselves. We are thus what Metzinger calls na\u00efve realists, who believe we are perceiving reality directly when in actuality we are only perceiving representations of reality. The data structures and transport mechanisms of the data are \u201ctransparent\u201d so that we can introspect on our representations of perceptions, but cannot introspect on the data or mechanisms themselves. These systemic representational experiences are then connected by subjective experience to generate the phenomenal property of selfhood. Subjective experience is the result of the Phenomenal Model of Intentionality Relationship (PMIR). The PMIR is a \u201cconscious mental model, and its content is an ongoing, episodic subject-object relation\u201d. The model is a result of the combination of our unique set of sensory receptors that acquire input, our unique set of experiences that shape connections within the brain, and our unique positions in space that give our perception perspectivalness.",
            "score": 155.07728576660156
        },
        {
            "docid": "727809_10",
            "document": "Engram (neuropsychology) . Studies have shown that declarative memories move between the limbic system, deep within the brain, and the outer, cortical regions. These are distinct from the mechanisms of the more primitive cerebellum, which dominates in the blinking response and receives the input of auditory information directly. It does not need to \"reach out\" to other brain structures for assistance in forming some memories of simple association.",
            "score": 153.29470825195312
        },
        {
            "docid": "226722_25",
            "document": "Functional magnetic resonance imaging . Researchers have checked the BOLD signal against both signals from implanted electrodes (mostly in monkeys) and signals of field potentials (that is the electric or magnetic field from the brain's activity, measured outside the skull) from EEG and MEG. The local field potential, which includes both post-neuron-synaptic activity and internal neuron processing, better predicts the BOLD signal. So the BOLD contrast reflects mainly the inputs to a neuron and the neuron's integrative processing within its body, and less the output firing of neurons. In humans, electrodes can be implanted only in patients who need surgery as treatment, but evidence suggests a similar relationship at least for the auditory cortex and the primary visual cortex. Activation locations detected by BOLD fMRI in cortical areas (brain surface regions) are known to tally with CBF-based functional maps from PET scans. Some regions just a few millimeters in size, such as the lateral geniculate nucleus (LGN) of the thalamus, which relays visual inputs from the retina to the visual cortex, have been shown to generate the BOLD signal correctly when presented with visual input. Nearby regions such as the pulvinar nucleus were not stimulated for this task, indicating millimeter resolution for the spatial extent of the BOLD response, at least in thalamic nuclei. In the rat brain, single-whisker touch has been shown to elicit BOLD signals from the somatosensory cortex.",
            "score": 153.04750061035156
        },
        {
            "docid": "3326958_9",
            "document": "Cartesian materialism . Descartes noted that, although our two eyes independently see an object, our conscious experience is not of two separate fields of vision each possessing an image of the object. Rather, we seem to experience one continuous, oval-shaped field of vision that possesses information from both eyes which seems to have somehow been 'merged' into a single image. (Consider in a movie, when a character looks through binoculars and the audience is shown what he is looking at, from the character's point of view. The image shown is never made up of two completely separate circular images-- rather the director shows us a single figure-eight shaped region made up of information from each eyepiece.) Descartes noted that information from both eyes seems to have been merged somehow before \"entering\" conscious perception. He also noted similar effects for the other senses. Based on this, Descartes hypothesized that there must be some single place in the brain where all the sensory information is assembled, before finally being relayed to the immaterial mind.",
            "score": 152.98223876953125
        },
        {
            "docid": "26685741_46",
            "document": "Sleep and memory . The brain is an ever-changing, plastic, model of information sharing and processing. In order for the brain to incorporate new experiences into a refined schema it has to undergo specific modifications to consolidate and assimilate all new information. Synaptic plasticity can be described as the changing in strength between two related neurons. Neuroplasticity is most clearly seen in the instances of REM sleep deprivation during brain maturation. Regional brain measurements in neo-natal REM sleep deprived rats displayed a significant size reduction in areas such as the cerebral cortex and the brain stem. The rats were deprived during critical periods after birth and thus anatomical size reduction is observed. Using a pursuit task (used to test visuomotor capabilities) in combination with an fMRI, Maquet et al., 2003, found that increases in activation were seen in the supplementary eye field and right dentate nucleus of subjects who were allowed to sleep as compared to sleep deprived individuals. The right superior temporal sulcus was also noticed to have higher activation levels. When functional connectivity was analyzed it was found that the dentate nucleus was more closely involved with the functions of the superior temporal sulcus. The results suggest that performance on the pursuit task relies on the subject's ability to comprehend appropriate movement patterns in order for recreation of the optimal movements. Sleep deprivation was found to interrupt the slow processes that lead to learning of this procedural skill and alter connectivity changes that would have normally been seen after a night of rest. Neuroplasticity has been thoroughly researched over the past few decades and results have shown that significant changes that occur in our cortical processing areas have the power to modulate neuronal firing to both new and previously experienced stimuli.",
            "score": 152.73641967773438
        },
        {
            "docid": "7330954_12",
            "document": "Pattern recognition (psychology) . Top-down processing refers to the use of background information in pattern recognition. It always begins with a person\u2019s previous knowledge, and makes predictions due to this already acquired knowledge. Psychologist Richard Gregory estimated that about 90% of the information is lost between the time it takes to go from the eye to the brain, which is why the brain must guess what the person sees based on past experiences. In other words, we construct our perception of reality, and these perceptions are hypotheses or propositions based on past experiences and stored information. The formation of incorrect propositions will lead to errors of perception such as visual illusions. Given a paragraph written with difficult handwriting, it is easier to understand what the writer wants to convey if one reads the whole paragraph rather than reading the words in separate terms. The brain may be able to perceive and understand the gist of the paragraph due to the context supplied by the surrounding words.",
            "score": 151.70071411132812
        },
        {
            "docid": "18345264_13",
            "document": "Neural correlates of consciousness . The possibility of precisely manipulating visual percepts in time and space has made vision a preferred modality in the quest for the NCC. Psychologists have perfected a number of techniques \u2013 masking, binocular rivalry, continuous flash suppression, motion induced blindness, change blindness, inattentional blindness \u2013 in which the seemingly simple and unambiguous relationship between a physical stimulus in the world and its associated percept in the privacy of the subject's mind is disrupted. In particular a stimulus can be perceptually suppressed for seconds or even minutes at a time: the image is projected into one of the observer's eyes but is invisible, not seen. In this manner the neural mechanisms that respond to the subjective percept rather than the physical stimulus can be isolated, permitting visual consciousness to be tracked in the brain. In a \"perceptual illusion\", the physical stimulus remains fixed while the percept fluctuates. The best known example is the \"Necker cube\" whose 12 lines can be perceived in one of two different ways in depth. A perceptual illusion that can be precisely controlled is \"binocular rivalry\". Here, a small image, e.g., a horizontal grating, is presented to the left eye, and another image, e.g., a vertical grating, is shown to the corresponding location in the right eye. In spite of the constant visual stimulus, observers consciously see the horizontal grating alternate every few seconds with the vertical one. The brain does not allow for the simultaneous perception of both images.",
            "score": 151.28793334960938
        },
        {
            "docid": "33246145_4",
            "document": "Neural decoding . When looking at a picture, people's brains are constantly making decisions about what object they are looking at, where they need to move their eyes next, and what they find to be the most salient aspects of the input stimulus. As these images hit the back of the retina, these stimuli are converted from varying wavelengths to a series of neural spikes called action potentials. These pattern of action potentials are different for different objects and different colors; we therefore say that the neurons are encoding objects and colors by varying their spike rates or temporal pattern. Now, if someone were to probe the brain by placing electrodes in the primary visual cortex, they may find what appears to be random electrical activity. These neurons are actually firing in response to the lower level features of visual input, possibly the edges of a picture frame. This highlights the crux of the neural decoding hypothesis: that it is possible to reconstruct a stimulus from the response of the ensemble of neurons that represent it. In other words, it is possible to look at spike train data and say that the person or animal being recorded is looking at a red ball.",
            "score": 150.38882446289062
        },
        {
            "docid": "5212945_2",
            "document": "Visual neuroscience . Visual Neuroscience is a branch of neuroscience that focuses on the visual system of the human body, mainly located in the brain's visual cortex. The main goal of visual neuroscience is to understand how neural activity results in visual perception, as well as behaviors dependent on vision. In the past, visual neuroscience has focused primarily on how the brain (and in particular the Visual Cortex) responds to light rays projected from static images and onto the retina. While this provides a reasonable explanation for the visual perception of a static image, it does not provide an accurate explanation for how we perceive the world as it really is, an ever-changing, and ever-moving 3-D environment. The topics summarized below are representative of this area, but far from exhaustive.",
            "score": 149.9630584716797
        },
        {
            "docid": "3020532_7",
            "document": "Corneal reflex . When awake, the lids spread the tear secretions over the corneal surface, on a typical basis of 2 to 10 seconds (though this may vary individually). However, blinking is not only dependent on dryness and/or irritation. A brain area, the globus pallidus of the basal ganglia, contains a blinking center that controls blinking. Nonetheless, the external stimuli are still involved. Blinking is linked with the extraocular muscles. Blinking is often concurrent with a shift in gaze, and it is believed that this helps the movement of the eye.",
            "score": 149.86880493164062
        },
        {
            "docid": "5664_64",
            "document": "Consciousness . In neuroscience, a great deal of effort has gone into investigating how the perceived world of conscious awareness is constructed inside the brain. The process is generally thought to involve two primary mechanisms: (1) hierarchical processing of sensory inputs, and (2) memory. Signals arising from sensory organs are transmitted to the brain and then processed in a series of stages, which extract multiple types of information from the raw input. In the visual system, for example, sensory signals from the eyes are transmitted to the thalamus and then to the primary visual cortex; inside the cerebral cortex they are sent to areas that extract features such as three-dimensional structure, shape, color, and motion. Memory comes into play in at least two ways. First, it allows sensory information to be evaluated in the context of previous experience. Second, and even more importantly, working memory allows information to be integrated over time so that it can generate a stable representation of the world\u2014Gerald Edelman expressed this point vividly by titling one of his books about consciousness \"The Remembered Present\". In computational neuroscience, Bayesian approaches to brain function have been used to understand both the evaluation of sensory information in light of previous experience, and the integration of information over time. Bayesian models of the brain are probabilistic inference models, in which the brain takes advantage of prior knowledge to interpret uncertain sensory inputs in order to formulate a conscious percept; Bayesian models have successfully predicted many perceptual phenomena in vision and the nonvisual senses.",
            "score": 149.103271484375
        },
        {
            "docid": "1947410_18",
            "document": "Critical period . In mammals, neurons in the brain that process vision actually develop after birth based on signals from the eyes. A landmark experiment by David H. Hubel and Torsten Wiesel (1963) showed that cats that had one eye sewn shut from birth to three months of age (monocular deprivation) only fully developed vision in the open eye. They showed that columns in the primary visual cortex receiving inputs from the other eye took over the areas that would normally receive input from the deprived eye. In general electrophysiological analyses of axons and neurons in the lateral geniculate nucleus showed that the visual receptive field properties was comparable to adult cats. However, the layers of cortex that were deprived had less activity and fewer responses were isolated. The kittens had abnormally small ocular dominance columns (part of the brain that processes sight) connected to the closed eye, and abnormally large, wide columns connected to the open eye. Because the critical period time had elapsed, it would be impossible for the kittens to alter and develop vision in the closed eye. This did not happen to adult cats even when one eye was sewn shut for a year because they had fully developed their vision during their critical period. Later experiments in monkeys found similar results.",
            "score": 148.83139038085938
        },
        {
            "docid": "1095131_20",
            "document": "Kinesthetic learning . The cerebral cortex is the brain tissue covering the top and sides of the brain in most vertebrates. It is involved in storing and processing of sensory inputs and motor outputs. In the human brain, the cerebral cortex is actually a sheet of neural tissue about 1/8th inch thick. The sheet is folded so that it can fit inside the skull. The neural circuits in this area of the brain expand with practice of an activity, just like the synaptic plasticity grows with practice. Clarification of some of the mechanisms of learning by neuro science has been advanced, in part, by the advent of non-invasive imaging technologies, such as positron emission tomography (PET) and functional magnetic resonance imaging (FMRI). These technologies have allowed researchers to observe human learning processes directly. Through these types of technologies, we are now able to see and study what happens in the process of learning. In different tests performed the brain being imaged showed a greater blood flow and activation to that area of the brain being stimulated through different activities such as finger tapping in a specific sequence. It has been revealed that the process at the beginning of learning a new skill happens quickly, and later on slows down to almost a plateau. This process can also be referred to as The Law of Learning. The slower learning showed in the FMRI that in the cerebral cortex this was when the long term learning was occurring, suggesting that the structural changes in the cortex reflect the enhancement of skill memories during later stages of training. When a person studies a skill for a longer duration of time, but in a shorter amount of time they will learn quickly, but also only retain the information into their short-term memory. Just like studying for an exam; if a student tries to learn everything the night before, it will not stick in the long run. If a person studies a skill for a shorter duration of time, but more frequently and long-term, their brain will retain this information much longer as it is stored in the long-term memory. Functional and structural studies of the brain have revealed a vast interconnectivity between diverse regions of the cerebral cortex. For example, large numbers of axons interconnect the posterior sensory areas serving vision, audition, and touch with anterior motor regions. Constant communication between sensation and movement makes sense, because to execute smooth movement through the environment, movement must be continuously integrated with knowledge about one's surroundings obtained via sensory perception. The cerebral cortex plays a role in allowing humans to do this.",
            "score": 148.4729766845703
        },
        {
            "docid": "739262_12",
            "document": "Neural correlate . Using such design, Nikos Logothetis and colleagues discovered perception-reflecting neurons in the temporal lobe. They created an experimental situation in which conflicting images were presented to different eyes (\"i.e.\", binocular rivalry). Under such conditions, human subjects report bistable percepts: they perceive alternatively one or the other image. Logothetis and colleagues trained the monkeys to report with their arm movements which image they perceived. Interestingly, temporal lobe neurons in Logothetis experiments often reflected what the monkeys' perceived. Neurons with such properties were less frequently observed in the primary visual cortex that corresponds to relatively early stages of visual processing. Another set of experiments using binocular rivalry in humans showed that certain layers of the cortex can be excluded as candidates of the neural correlate of consciousness. Logothetis and colleagues switched the images between eyes during the percept of one of the images. Surprisingly the percept stayed stable. This means that the conscious percept stayed stable and at the same time the primary input to layer 4, which is the input layer, in the visual cortex changed. Therefore layer 4 can not be a part of the neural correlate of consciousness. Mikhail Lebedev and their colleagues observed a similar phenomenon in monkey prefrontal cortex. In their experiments monkeys reported the perceived direction of visual stimulus movement (which could be an illusion) by making eye movements. Some prefrontal cortex neurons represented actual and some represented perceived displacements of the stimulus. Observation of perception related neurons in prefrontal cortex is consistent with the theory of Christof Koch and Francis Crick who postulated that neural correlate of consciousness resides in prefrontal cortex. Proponents of distributed neuronal processing may likely dispute the view that consciousness has a precise localization in the brain.",
            "score": 148.38597106933594
        },
        {
            "docid": "25984349_13",
            "document": "Sam Schmidt Paralysis Foundation . The central nervous system (CNS) controls most functions of the body and mind. It consists of two parts: the brain and the spinal cord. The brain is the center of our thoughts, the interpreter of our external environment, and the origin of control over body movement. Like a central computer, it interprets information from our eyes (sight), ears (sound), nose (smell), tongue (taste) and skin (touch), as well as from internal organs such as the stomach. It controls all voluntary movement, such as speech and walking, and involuntary movement like blinking and breathing. It is the core of our thoughts, perceptions and emotions.",
            "score": 148.24058532714844
        },
        {
            "docid": "7800961_12",
            "document": "Fusiform face area . The FFA is underdeveloped in children and does not fully develop until adolescence. This calls into question the evolutionary purpose of the FFA, as children show the ability to differentiate faces. Two-year-old babies have been shown to prefer the face of their mother. Although the FFA is underdeveloped in two-year-old babies, they have the ability to recognize their mother. Babies as early as three months old have shown the ability to distinguish between faces. During this time, babies exhibit the ability to differentiate between genders, showing a clear preference for female faces. It is theorized that, in terms of evolution, babies focus on women for food, although the preference could simply reflect a bias for the caregivers they experience. Infants do not appear to use this area for the perception of faces. Recent fMRI work has found no face selective area in the brain of infants 4 to 6 months old. However, given that the adult human brain has been studied far more extensively than the infant brain, and that infants are still undergoing major neurodevelopmental processes, it may simply be that the FFA is not located in anatomically familiar area. It may also be that activation for many different percepts and cognitive tasks in infants is diffuse in terms of neural circuitry, as infants are still undergoing periods of neurogenesis and neural pruning; this may make it more difficult to distinguish the signal, or what we would imagine as visual and complex familiar objects (like faces), from the noise, including static firing rates of neurons, and activity that is dedicated to a different task entirely than the activity of face processing. Infant vision involves only light and dark recognition, recognizing only major features of the face, activating the amygdala. These findings question the evolutionary purpose of the FFA.",
            "score": 148.05418395996094
        },
        {
            "docid": "433584_4",
            "document": "McGurk effect . Vision is the primary sense for humans, but speech perception is multimodal, which means that it involves information from more than one sensory modality, in particular, audition and vision. The McGurk effect arises during phonetic processing because the integration of audio and visual information happens early in speech perception. The McGurk effect is very robust; that is, knowledge about it seems to have little effect on one's perception of it. This is different from certain optical illusions, which break down once one 'sees through' them. Some people, including those that have been researching the phenomenon for more than twenty years, experience the effect even when they are aware that it is taking place. With the exception of people who can identify most of what is being said from speech-reading alone, most people are quite limited in their ability to identify speech from visual-only signals. A more extensive phenomenon is the ability of visual speech to increase the intelligibility of heard speech in a noisy environment. Visible speech can also alter the perception of perfectly audible speech sounds when the visual speech stimuli are mismatched with the auditory speech. Normally, speech perception is thought to be an auditory process; however, our use of information is immediate, automatic, and, to a large degree, unconscious and therefore, despite what is widely accepted as true, speech is not only something we hear. Speech is perceived by all of the senses working together (seeing, touching, and listening to a face move). The brain is often unaware of the separate sensory contributions of what it perceives. Therefore, when it comes to recognizing speech the brain cannot differentiate whether it is seeing or hearing the incoming information.",
            "score": 147.4664764404297
        },
        {
            "docid": "53953041_15",
            "document": "Predictive coding . The empirical evidence for predictive coding is most robust for perceptual processing. As early as 1999, Rao and Ballard proposed a hierarchical visual processing model in which higher-order visual cortical area sends down predictions and the feedforward connections carry the residual errors between the predictions and the actual lower-level activities (Rao and Ballard, 1999). According to this model, each level in the hierarchical model network (except the lowest level, which represents the image) attempts to predict the responses at the next lower level via feedback connections, and the error signal is used to correct the estimate of the input signal at each level concurrently (Rao and Ballard, 1999). Emberson et al. established the top-down modulation in infants using a cross-modal audiovisual omission paradigm, determining that even infant brains have expectation about future sensory input that is carried downstream from visual cortices and are capable of expectation-based feedback (Emberson et al., 2015). Functional near-infrared spectroscopy (fNIRS) data showed that infant occipital cortex responded to unexpected visual omission (with no visual information input) but not to expected visual omission. These results establish that in a hierarchically organized perception system, higher-order neurons send down predictions to lower-order neurons, which in turn sends back up the prediction error signal.",
            "score": 146.9383087158203
        },
        {
            "docid": "43389476_5",
            "document": "Consciousness and the Brain . He introduces the project of measuring neural correlates of consciousness using paradigms like minimal contrasts of images, masking (subliminal stimuli), binocular rivalry, and attentional blink. The attentional blink relates to the psychological refractory period, inattentional blindness, and change blindness. Olaf Blanke's studies on out-of-body experiences explore an example where conscious experience changes while external stimuli stay the same.",
            "score": 145.9036407470703
        },
        {
            "docid": "1316947_4",
            "document": "Ambiguous image . When we see an image, the first thing we do is attempt to organize all the parts of the scene into different groups. To do this, one of the most basic methods used is finding the edges. Edges can include obvious perceptions such as the edge of a house, and can include other perceptions that the brain needs to process deeper, such as the edges of a person's facial features. When finding edges, the brain's visual system detects a point on the image with a sharp contrast of lighting. Being able to detect the location of the edge of an object aids in recognizing the object. In ambiguous images, detecting edges still seems natural to the person perceiving the image. However, the brain undergoes deeper processing to resolve the ambiguity. For example, consider an image that involves an opposite change in magnitude of luminance between the object and the background (e.g. From the top, the background shifts from black to white, and the object shifts from white to black). The opposing gradients will eventually come to a point where there is an equal degree of luminance of the object and the background. At this point, there is no edge to be perceived. To counter this, the visual system connects the image as a whole rather than a set of edges, allowing one to see an object rather than edges and non-edges. Although there is no complete image to be seen, the brain is able to accomplish this because of its understanding of the physical world and real incidents of ambiguous lighting. In ambiguous images, an illusion is often produced from illusory contours. An illusory contour is a perceived contour without the presence of a physical gradient. In examples where a white shape appears to occlude black objects on a white background, the white shape appears to be brighter than the background, and the edges of this shape produce the illusory contours. These illusory contours are processed by the brain in a similar way as real contours. The visual system accomplishes this by making inferences beyond the information that is presented in much the same way as the luminance gradient.",
            "score": 145.88577270507812
        },
        {
            "docid": "35347567_7",
            "document": "Antti Revonsuo . According to Revonsuo, the dreaming brain is particularly suitable model system for the study of consciousness because it generates a conscious experience while being isolated from both sensory input and motor output. Regarding the rival paradigm of visual awareness, Revonsuo argues that it does not allow one to distinguish between consciousness and perception. Revonsuo holds that there is a \"'double dissociation' between consciousness and perceptual input\". Accordingly, dreams are conscious experiences, which occur without any perceptual stimuli, and, conversely, perceptual input does not automatically engender conscious experience. In support of the independence of consciousness from perception, Revonsuo cites Stephen LaBerge's case study on a lucid dreamer performing previously agreed upon eye movements to signal to the experimenters that he had become conscious of the fact that he was dreaming. A second study that supports Revonsuo's view of dreams was conducted by Allan Rechtschaffen and Foulkes (1965). In this study, subjects were made to sleep with their eyelids open, thus allowing the visual cortex to receive visual stimuli. Though their eyes were open, and the perceptual input was accessible, the subjects could not see the stimuli and did not report dreaming of it. It is the brain that is having the internal experience, independent of perceptual input. This internalist view of consciousness leads Revonsuo to compare both dreaming and waking consciousness with a virtual reality simulation decoupled from or only indirectly informed by a brain's external environment.",
            "score": 145.7689208984375
        }
    ]
}