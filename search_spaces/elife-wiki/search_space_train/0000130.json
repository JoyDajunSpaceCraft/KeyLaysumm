{
    "q": [
        {
            "docid": "48841414_5",
            "document": "Multiple instance learning . Keeler et al., in his work in early 1990s was the first one to explore the area of MIL. The actual term multi-instance learning was introduced in the middle of the 1990s, by Dietterich et al. while they were investigating the problem of drug activity prediction. They tried to create a learning systems that could predict whether new molecule was qualified to make some drug, or not, through analyzing a collection of known molecules. Molecules can have many alternative low-energy states, but only one, or some of them, are qualified to make a drug. The problem arose because scientists could only determine if molecule is qualified, or not, but they couldn\u2019t say exactly which of its low-energy shapes are responsible for that. One of the proposed ways to solve this problem was to use supervised learning, and regard all the low-energy shapes of the qualified molecule as positive training instances, while all of the low-energy shapes of unqualified molecules as negative instances. Dietterich et al. showed that such method would have a high false positive noise, from all low-energy shapes that are mislabeled as positive, and thus wasn\u2019t really useful. Their approach was to regard each molecule as a labeled bag, and all the alternative low-energy shapes of that molecule as instances in the bag, without individual labels. Thus formulating multiple-instance learning.  Solution to the multiple instance learning problem that Dietterich et al. proposed is three axis-parallel rectangle (APR) algorithm. It attempts to search for appropriate axis-parallel rectangles constructed by the conjunction of the features. They tested the algorithm on Musk dataset, which is a concrete test data of drug activity prediction and the most popularly used benchmark in multiple-instance learning. APR algorithm achieved the best result, but it should be noted that APR was designed with Musk data in mind.",
            "score": 142.88768792152405
        },
        {
            "docid": "149353_4",
            "document": "Computational biology . Computational Biology, which includes many aspects of bioinformatics, is the science of using biological data to develop algorithms or models to understand biological systems and relationships.  Until recently, biologists did not have access to very large amounts of data. This data has now become commonplace, particularly in molecular biology and genomics. Researchers were able to develop analytical methods for interpreting biological information, but were unable to share them quickly among colleagues.  Bioinformatics began to develop in the early 1970s. It was considered the science of analyzing informatics processes of various biological systems. At this time, research in artificial intelligence was using network models of the human brain in order to generate new algorithms. This use of biological data to develop other fields pushed biological researchers to revisit the idea of using computers to evaluate and compare large data sets. By 1982, information was being shared amongst researchers through the use of punch cards. The amount of data being shared began to grow exponentially by the end of the 1980s. This required the development of new computational methods in order to quickly analyze and interpret relevant information. Since the late 1990s, computational biology has become an important part of developing emerging technologies for the field of biology. The terms computational biology and evolutionary computation have a similar name, but are not to be confused. Unlike computational biology, evolutionary computation is not concerned with modeling and analyzing biological data. It instead creates algorithms based on the ideas of evolution across species. Sometimes referred to as genetic algorithms, the research of this field can be applied to computational biology. While evolutionary computation is not inherently a part of computational biology, Computational evolutionary biology is a subfield of it.",
            "score": 115.67721140384674
        },
        {
            "docid": "1072943_15",
            "document": "Forward algorithm . The forward algorithm is mostly used in applications that need us to determine the probability of being in a specific state when we know about the sequence of observations. We first calculate the probabilities over the states computed for the previous observation and use them for the current observations, and then extend it out for the next step using the transition probability table.The approach basically caches all the intermediate state probabilities so they are computed only once. This helps us to compute a fixed state path. The process is also called posterior decoding. The algorithm computes probability much more efficiently than the naive approach, which very quickly ends up in a combinatorial explosion. Together, they can provide the probability of a given emission/observation at each position in the sequence of observations. It is from this information that a version of the most likely state path is computed (\"posterior decoding\"). The algorithm can be applied wherever we can train a model as we receive data using Baum-Welch or any general EM algorithm. The Forward algorithm will then tell us about the probability of data with respect to what is expected from our model. One of the applications can be in the domain of Finance, where it can help decide on when to buy or sell tangible assets. It can have applications in all fields where we apply Hidden Markov Models. The popular ones include Natural language processing domains like tagging part-of-speech and speech recognition. Recently it is also being used in the domain of Bioinformatics. Forward algorithm can also be applied to perform Weather speculations. We can have a HMM describing the weather and its relation to the state of observations for few consecutive days (some examples could be dry, damp, soggy, sunny, cloudy, rainy etc.). We can consider calculating the probability of observing any sequence of observations recursively given the HMM. We can then calculate the probability of reaching an intermediate state as the sum of all possible paths to that state. Thus the partial probabilities for the final observation will hold the probability of reaching those states going through all possible paths.",
            "score": 106.38304698467255
        },
        {
            "docid": "41768230_4",
            "document": "Mihaela Zavolan . The main focus of the research in Mihaela Zavolan\u2019s group is of microRNAs (miRNAs). These 22 nucleotides long RNA molecules regulate the expression of protein coding genes, thereby controlling cell differentiation, metabolism and immune responses. Through the development of high-throughput experimental methods and computational analyses, Zavolan has contributed to the discovery of many miRNAs in various organisms ranging from viruses to humans. She has developed algorithms to predict miRNA genes and miRNA targets, and has worked on the development of the CLIP method (cross-linking and immunoprecipitation) for mapping the binding sites of RNA-binding proteins in RNAs. Recently, her group used CLIP binding site data to infer a biophysical model of miRNA-target interaction, which can be used to predict the strength between of interactions between miRNAs and their targets on mRNAs and long non-coding RNA.",
            "score": 99.59639608860016
        },
        {
            "docid": "32898112_8",
            "document": "TRANSFAC . The TRANSFAC database can be used as an encyclopedia of eukaryotic transcription factors. The target sequences and the regulated genes can be listed for each TF, which can be used as benchmark for TFBS recognition tools or as training sets for new TFBS recognition algorithms. The TF classification enables to analyze such data sets with regard to the properties of the DNA-binding domains. Another application is to retrieve all TFs that regulate a given (set of) gene(s). In the context of systems-biological studies, the TF-target gene relations documented in TRANSFAC were used to construct and analyze transcription regulatory networks. By far the most frequent use of TRANSFAC is the computational prediction of potential transcription factor binding sites (TFBS). A number of algorithms exist which either use the individual binding sites or the matrix library for this purpose:",
            "score": 109.1206784248352
        },
        {
            "docid": "17663305_2",
            "document": "LigandScout . LigandScout is computer software that allows creating three-dimensional (3D) pharmacophore models from structural data of macromolecule\u2013ligand complexes, or from training and test sets of organic molecules. It incorporates a complete definition of 3D chemical features (such as hydrogen bond donors, acceptors, lipophilic areas, positively and negatively ionizable chemical groups) that describe the interaction of a bound small organic molecule (ligand) and the surrounding binding site of the macromolecule. These pharmacophores can be overlaid and superimposed using a pattern-matching based alignment algorithm that is solely based on pharmacophoric feature points instead of chemical structure. From such an overlay, shared features can be interpolated to create a so-called \"shared-feature pharmacophore\" that shares all common interactions of several binding sites/ligands or extended to create a so-called \"merged-feature\" pharmacophore. The software has been successfully used to predict new lead structures in drug design, e.g., predicting biological activity of novel human immunodeficiency virus (HIV) reverse transcriptase inhibitors.",
            "score": 118.50052404403687
        },
        {
            "docid": "53970843_3",
            "document": "Machine learning in bioinformatics . Prior to the emergence of machine learning algorithms, bioinformatics algorithms had to be explicitly programmed by hand which, for problems such as protein structure prediction, proves extremely difficult. Machine learning techniques such as deep learning enable the algorithm to make use of automatic feature learning which means that based on the dataset alone, the algorithm can learn how to combine multiple features of the input data into a more abstract set of features from which to conduct further learning. This multi-layered approach to learning patterns in the input data allows such systems to make quite complex predictions when trained on large datasets. In recent years, the size and number of available biological datasets have skyrocketed, enabling bioinformatics researchers to make use of these machine learning systems. Machine learning has been applied to six main subfields of bioinformatics: genomics, proteomics, microarrays, systems biology, evolution, and text mining.",
            "score": 109.71792280673981
        },
        {
            "docid": "36216526_33",
            "document": "Noise-predictive maximum-likelihood detection . By modeling the data-dependent noise as a finite-order Markov process, the optimum MLSE for channels with ISI has been derived. In particular, it when the data-dependent noise is conditionally Gauss\u2013Markov, the branch metrics can be computed from the conditional second-order statistics of the noise process. In other words, the optimum MLSE can be implemented efficiently by using the Viterbi algorithm, in which the branch-metric computation involves data-dependent noise prediction. Because the predictor coefficients and prediction error both depend on the local data pattern, the resulting structure has been called a data-dependent NPML detector. Reduced-state sequence detection schemes can be applied to data-dependent NPML, reducing implementation complexity.",
            "score": 76.75557029247284
        },
        {
            "docid": "309261_4",
            "document": "Nonlinear dimensionality reduction . Consider a dataset represented as a matrix (or a database table), such that each row represents a set of attributes (or features or dimensions) that describe a particular instance of something. If the number of attributes is large, then the space of unique possible rows is exponentially large. Thus, the larger the dimensionality, the more difficult it becomes to sample the space. This causes many problems. Algorithms that operate on high-dimensional data tend to have a very high time complexity. Many machine learning algorithms, for example, struggle with high-dimensional data. This has become known as the curse of dimensionality. Reducing data into fewer dimensions often makes analysis algorithms more efficient, and can help machine learning algorithms make more accurate predictions.",
            "score": 87.65429592132568
        },
        {
            "docid": "27051151_69",
            "document": "Big data . Much in the same line, it has been pointed out that the decisions based on the analysis of big data are inevitably \"informed by the world as it was in the past, or, at best, as it currently is\". Fed by a large number of data on past experiences, algorithms can predict future development if the future is similar to the past. If the systems dynamics of the future change (if it is not a stationary process), the past can say little about the future. In order to make predictions in changing environments, it would be necessary to have a thorough understanding of the systems dynamic, which requires theory. As a response to this critique Alemany Oliver and Vayre suggested to use \"abductive reasoning as a first step in the research process in order to bring context to consumers\u2019 digital traces and make new theories emerge\". Additionally, it has been suggested to combine big data approaches with computer simulations, such as agent-based models and Complex Systems. Agent-based models are increasingly getting better in predicting the outcome of social complexities of even unknown future scenarios through computer simulations that are based on a collection of mutually interdependent algorithms. Finally, use of multivariate methods that probe for the latent structure of the data, such as factor analysis and cluster analysis, have proven useful as analytic approaches that go well beyond the bi-variate approaches (cross-tabs) typically employed with smaller data sets.",
            "score": 94.32019472122192
        },
        {
            "docid": "42253_18",
            "document": "Data mining . Data mining can unintentionally be misused, and can then produce results which appear to be significant; but which do not actually predict future behaviour and cannot be reproduced on a new sample of data and bear little use. Often this results from investigating too many hypotheses and not performing proper statistical hypothesis testing. A simple version of this problem in machine learning is known as overfitting, but the same problem can arise at different phases of the process and thus a train/test split - when applicable at all - may not be sufficient to prevent this from happening. The final step of knowledge discovery from data is to verify that the patterns produced by the data mining algorithms occur in the wider data set. Not all patterns found by the data mining algorithms are necessarily valid. It is common for the data mining algorithms to find patterns in the training set which are not present in the general data set. This is called overfitting. To overcome this, the evaluation uses a test set of data on which the data mining algorithm was not trained. The learned patterns are applied to this test set, and the resulting output is compared to the desired output. For example, a data mining algorithm trying to distinguish \"spam\" from \"legitimate\" emails would be trained on a training set of sample e-mails. Once trained, the learned patterns would be applied to the test set of e-mails on which it had \"not\" been trained. The accuracy of the patterns can then be measured from how many e-mails they correctly classify. A number of statistical methods may be used to evaluate the algorithm, such as ROC curves.",
            "score": 102.33013391494751
        },
        {
            "docid": "145128_23",
            "document": "Algorithmic efficiency . Analyse the algorithm, typically using time complexity analysis to get an estimate of the running time as a function of the size of the input data. The result is normally expressed using Big O notation. This is useful for comparing algorithms, especially when a large amount of data is to be processed. More detailed estimates are needed for algorithm comparison when the amount of data is small (though in this situation time is less likely to be a problem anyway). Algorithms which include parallel processing may be more difficult to analyse.",
            "score": 99.0620391368866
        },
        {
            "docid": "4438763_2",
            "document": "Gillespie algorithm . In probability theory, the Gillespie algorithm (or occasionally the Doob-Gillespie algorithm) generates a statistically correct trajectory (possible solution) of a stochastic equation. It was created by Joseph L. Doob and others (circa 1945), presented by Dan Gillespie in 1976, and popularized in 1977 in a paper where he uses it to simulate chemical or biochemical systems of reactions efficiently and accurately using limited computational power (see stochastic simulation). As computers have become faster, the algorithm has been used to simulate increasingly complex systems. The algorithm is particularly useful for simulating reactions within cells, where the number of reagents is low and keeping track of the position and behaviour of individual molecules is computationally feasible. Mathematically, it is a variant of a dynamic Monte Carlo method and similar to the kinetic Monte Carlo methods. It is used heavily in computational systems biology.",
            "score": 99.83606803417206
        },
        {
            "docid": "3069503_16",
            "document": "Structural health monitoring . The portion of the SHM process that has received the least attention in the technical literature is the development of statistical models for discrimination between features from the undamaged and damaged structures. Statistical model development is concerned with the implementation of the algorithms that operate on the extracted features to quantify the damage state of the structure. The algorithms used in statistical model development usually fall into three categories. When data are available from both the undamaged and damaged structure, the statistical pattern recognition algorithms fall into the general classification referred to as supervised learning. Group classification and regression analysis are categories of supervised learning algorithms. Unsupervised learning refers to algorithms that are applied to data not containing examples from the damaged structure. Outlier or novelty detection is the primary class of algorithms applied in unsupervised learning applications. All of the algorithms analyze statistical distributions of the measured or derived features to enhance the damage identification process.",
            "score": 93.00997650623322
        },
        {
            "docid": "48198256_8",
            "document": "Emotion recognition . Statistical methods commonly involve the use of different supervised machine learning algorithms in which a large set of annotated data is fed into the algorithms for the system to learn and predict the appropriate emotion types. This approach normally involves two sets of data: the training set and the testing set, where the former is used to learn the attributes of the data, while the latter is used to validate the performance of the machine learning algorithm. Machine learning algorithms generally provide more reasonable classification accuracy compared to other approaches, but one of the challenges in achieving good results in the classification process, is the need to have a sufficiently large training set.",
            "score": 94.49175834655762
        },
        {
            "docid": "73231_19",
            "document": "Weather forecasting . Models are \"initialized\" using this observed data. The irregularly spaced observations are processed by data assimilation and objective analysis methods, which perform quality control and obtain values at locations usable by the model's mathematical algorithms (usually an evenly spaced grid). The data are then used in the model as the starting point for a forecast. Commonly, the set of equations used to predict the known as the physics and dynamics of the atmosphere are called primitive equations. These equations are initialized from the analysis data and rates of change are determined. The rates of change predict the state of the atmosphere a short time into the future. The equations are then applied to this new atmospheric state to find new rates of change, and these new rates of change predict the atmosphere at a yet further time into the future. This \"time stepping\" procedure is continually repeated until the solution reaches the desired forecast time. The length of the time step is related to the distance between the points on the computational grid.",
            "score": 77.91978704929352
        },
        {
            "docid": "11512_33",
            "document": "Fast Fourier transform . All of the FFT algorithms discussed above compute the DFT exactly (i.e. neglecting floating-point errors). A few \"FFT\" algorithms have been proposed, however, that compute the DFT \"approximately\", with an error that can be made arbitrarily small at the expense of increased computations. Such algorithms trade the approximation error for increased speed or other properties. For example, an approximate FFT algorithm by Edelman et al. (1999) achieves lower communication requirements for parallel computing with the help of a fast multipole method. A wavelet-based approximate FFT by Guo and Burrus (1996) takes sparse inputs/outputs (time/frequency localization) into account more efficiently than is possible with an exact FFT. Another algorithm for approximate computation of a subset of the DFT outputs is due to Shentov et al. (1995). The Edelman algorithm works equally well for sparse and non-sparse data, since it is based on the compressibility (rank deficiency) of the Fourier matrix itself rather than the compressibility (sparsity) of the data. Conversely, if the data are sparse\u2014that is, if only \"K\" out of \"N\" Fourier coefficients are nonzero\u2014then the complexity can be reduced to O(\"K\"\u00a0log(\"N\")log(\"N\"/\"K\")), and this has been demonstrated to lead to practical speedups compared to an ordinary FFT for \"N\"/\"K\"\u00a0>\u00a032 in a large-\"N\" example (\"N\"\u00a0=\u00a02) using a probabilistic approximate algorithm (which estimates the largest \"K\" coefficients to several decimal places).",
            "score": 102.08241903781891
        },
        {
            "docid": "105837_5",
            "document": "Generic programming . For example, given \"N\" sequence data structures, e.g. singly linked list, vector etc., and \"M\" algorithms to operate on them, e.g. find, sort etc., a direct approach would implement each algorithm specifically for each data structure, giving combinations to implement. However, in the generic programming approach, each data structure returns a model of an iterator concept (a simple value type that can be dereferenced to retrieve the current value, or changed to point to another value in the sequence) and each algorithm is instead written generically with arguments of such iterators, e.g. a pair of iterators pointing to the beginning and end of the subsequence to process. Thus, only data structure-algorithm combinations need be implemented. Several iterator concepts are specified in the STL, each a refinement of more restrictive concepts e.g. forward iterators only provide movement to the next value in a sequence (e.g. suitable for a singly linked list or a stream of input data), whereas a random-access iterator also provides direct constant-time access to any element of the sequence (e.g. suitable for a vector). An important point is that a data structure will return a model of the most general concept that can be implemented efficiently\u2014computational complexity requirements are explicitly part of the concept definition. This limits the data structures a given algorithm can be applied to and such complexity requirements are a major determinant of data structure choice. Generic programming similarly has been applied in other domains, e.g. graph algorithms.",
            "score": 85.8847838640213
        },
        {
            "docid": "39582537_6",
            "document": "Dan Willard . In computer science, Willard is best known for his work with Michael Fredman in the early 1990s on integer sorting and related data structures. Before their research, it had long been known that comparison sorting required time formula_1 to sort a set of formula_2 items, but that faster algorithms were possible if the keys by which the items were sorted could be assumed to be integers of moderate size. For instance, sorting keys in the range from formula_3 to formula_4 could be accomplished in time formula_5 by radix sorting. However, it was assumed that integer sorting algorithms would necessarily have a time bound depending on formula_4, and would necessarily be slower than comparison sorting for sufficiently large values of formula_4. In research originally announced in 1990, Fredman and Willard changed these assumptions by introducing the transdichotomous model of computation. In this model, they showed that integer sorting could be done in time formula_8 by an algorithm using their fusion tree data structure as a priority queue. In a follow-up to this work, Fredman and Willard also showed that similar speedups could be applied to other standard algorithmic problems including minimum spanning trees and shortest paths.",
            "score": 78.54933249950409
        },
        {
            "docid": "40338559_4",
            "document": "Hybrid algorithm . In computer science, hybrid algorithms are very common in optimized real-world implementations of recursive algorithms, particularly implementations of  divide and conquer or decrease and conquer algorithms, where the size of the data decreases as one moves deeper in the recursion. In this case, one algorithm is used for the overall approach (on large data), but deep in the recursion, it switches to a different algorithm, which is more efficient on small data. A common example is in sorting algorithms, where the insertion sort, which is inefficient on large data, but very efficient on small data (say, five to ten elements), is used as the final step, after primarily applying another algorithm, such as merge sort or quicksort. Merge sort and quicksort are asymptotically optimal on large data, but the overhead becomes significant if applying them to small data, hence the use of a different algorithm at the end of the recursion. A highly optimized hybrid sorting algorithm is Timsort, which combines merge sort, insertion sort, together with additional logic (including binary search) in the merging logic.",
            "score": 86.18635582923889
        },
        {
            "docid": "32867182_3",
            "document": "Waffles (machine learning) . The Waffles machine learning toolkit contains command-line tools for performing various operations related to machine learning, data mining, and predictive modeling. The primary focus of Waffles is to provide tools that are simple to use in scripted experiments or processes. For example, the supervised learning algorithms included in Waffles are all designed to support multi-dimensional labels, classification and regression, automatically impute missing values, and automatically apply necessary filters to transform the data to a type that the algorithm can support, such that arbitrary learning algorithms can be used with arbitrary data sets. Many other machine learning toolkits provide similar functionality, but require the user to explicitly configure data filters and transformations to make it compatible with a particular learning algorithm. The algorithms provided in Waffles also have the ability to automatically tune their own parameters (with the cost of additional computational overhead).",
            "score": 84.0846757888794
        },
        {
            "docid": "10377_104",
            "document": "Euclidean algorithm . In the uniform cost model (suitable for analyzing the complexity of gcd calculation on numbers that fit into a single machine word), each step of the algorithm takes constant time, and Lam\u00e9's analysis implies that the total running time is also \"O\"(\"h\"). However, in a model of computation suitable for computation with larger numbers, the computational expense of a single remainder computation in the algorithm can be as large as \"O\"(\"h\"). In this case the total time for all of the steps of the algorithm can be analyzed using a telescoping series, showing that it is also \"O\"(\"h\"). Modern algorithmic techniques based on the Sch\u00f6nhage\u2013Strassen algorithm for fast integer multiplication can be used to speed this up, leading to quasilinear algorithms for the GCD.",
            "score": 82.65977263450623
        },
        {
            "docid": "3259720_7",
            "document": "Multifactor dimensionality reduction . As illustrated above, the basic constructive induction algorithm in MDR is very simple. However, its implementation for mining patterns from real data can be computationally complex. As with any machine learning algorithm there is always concern about overfitting. That is, machine learning algorithms are good at finding patterns in completely random data. It is often difficult to determine whether a reported pattern is an important signal or just chance. One approach is to estimate the generalizability of a model to independent datasets using methods such as cross-validation. Models that describe random data typically don't generalize. Another approach is to generate many random permutations of the data to see what the data mining algorithm finds when given the chance to overfit. Permutation testing makes it possible to generate an empirical p-value for the result. Replication in independent data may also provide evidence for an MDR model but can be sensitive to difference in the data sets. These approaches have all been shown to be useful for choosing and evaluating MDR models. An important step in an machine learning exercise is interpretation. Several approaches have been used with MDR including entropy analysis and pathway analysis. Tips and approaches for using MDR to model gene-gene interactions have been reviewed.",
            "score": 94.29998743534088
        },
        {
            "docid": "149353_16",
            "document": "Computational biology . Cancer computational biology is a field that aims to determine the future mutations in cancer through an algorithmic approach to analyzing data. Research in this field has led to the use of high-throughput measurement. High throughput measurement allows for the gathering of millions of data points using robotics and other sensing devices. This data is collected from DNA, RNA, and other biological structures. Areas of focus include determining the characteristics of tumors, analyzing molecules that are deterministic in causing cancer, and understanding how the human genome relates to the causation of tumors and cancer.",
            "score": 95.45724129676819
        },
        {
            "docid": "4878340_9",
            "document": "Algorithmic art . From one point of view, for a work of art to be considered algorithmic art, its creation must include a process based on an algorithm devised by the artist. Here, an algorithm is simply a detailed recipe for the design and possibly execution of an artwork, which may include computer code, functions, expressions, or other input which ultimately determines the form the art will take. This input may be mathematical, computational, or generative in nature. Inasmuch as algorithms tend to be deterministic, meaning that their repeated execution would always result in the production of identical artworks, some external factor is usually introduced. This can either be a random number generator of some sort, or an external body of data (which can range from recorded heartbeats to frames of a movie.) Some artists also work with organically based gestural input which is then modified by an algorithm. By this definition, fractals made by a fractal program are not art, as humans are not involved. However, defined differently, algorithmic art can be seen to include fractal art, as well as other varieties such as those using genetic algorithms. The artist Kerry Mitchell stated in his 1999 \"Fractal Art Manifesto\":",
            "score": 86.28310871124268
        },
        {
            "docid": "4141563_30",
            "document": "Predictive analytics . Big data is a collection of data sets that are so large and complex that they become awkward to work with using traditional database management tools. The volume, variety and velocity of big data have introduced challenges across the board for capture, storage, search, sharing, analysis, and visualization. Examples of big data sources include web logs, RFID, sensor data, social networks, Internet search indexing, call detail records, military surveillance, and complex data in astronomic, biogeochemical, genomics, and atmospheric sciences. Big Data is the core of most predictive analytic services offered by IT organizations.  Thanks to technological advances in computer hardware\u2014faster CPUs, cheaper memory, and MPP architectures\u2014and new technologies such as Hadoop, MapReduce, and in-database and text analytics for processing big data, it is now feasible to collect, analyze, and mine massive amounts of structured and unstructured data for new insights. It is also possible to run predictive algorithms on streaming data. Today, exploring big data and using predictive analytics is within reach of more organizations than ever before and new methods that are capable for handling such datasets are proposed.",
            "score": 98.28599953651428
        },
        {
            "docid": "34119149_26",
            "document": "Geotechnical centrifuge modeling . Centrifuge tests can also be used to obtain experimental data to verify a design procedure or a computer model. The rapid development of computational power over recent decades has revolutionized engineering analysis. Many computer models have been developed to predict the behavior of geotechnical structures during earthquakes and other loads. Before a computer model can be used with confidence, it must be proven to be valid based on evidence. The meager and unrepeatable data provided by natural earthquakes, for example, is usually insufficient for this purpose. Verification of the validity of assumptions made by a computational algorithm is especially important in the area of geotechnical engineering due to the complexity of soil behavior. Soils exhibit highly non-linear behavior, their strength and stiffness depend on their stress history and on the water pressure in the pore fluid, all of which may evolve during the loading caused by an earthquake. The computer models which are intended to simulate these phenomena are very complex and require extensive verification. Experimental data from centrifuge tests is useful for verifying assumptions made by a computational algorithm. If the results show the computer model to be inaccurate, the centrifuge test data provides insight into the physical processes which in turn stimulates the development of better computer models.",
            "score": 91.12629222869873
        },
        {
            "docid": "42674_4",
            "document": "Shor's algorithm . If a quantum computer with a sufficient number of qubits could operate without succumbing to noise and other quantum decoherence phenomena, Shor's algorithm could be used to break public-key cryptography schemes such as the widely used RSA scheme. RSA is based on the assumption that factoring large numbers is computationally intractable. So far as is known, this assumption is valid for classical (non-quantum) computers; no classical algorithm is known that can factor in polynomial time. However, Shor's algorithm shows that factoring is efficient on an ideal quantum computer, so it may be feasible to defeat RSA by constructing a large quantum computer. It was also a powerful motivator for the design and construction of quantum computers and for the study of new quantum computer algorithms. It has also facilitated research on new cryptosystems that are secure from quantum computers, collectively called post-quantum cryptography.",
            "score": 75.33490931987762
        },
        {
            "docid": "22212276_25",
            "document": "Ensemble learning . Stacking (sometimes called \"stacked generalization\") involves training a learning algorithm to combine the predictions of several other learning algorithms. First, all of the other algorithms are trained using the available data, then a combiner algorithm is trained to make a final prediction using all the predictions of the other algorithms as additional inputs. If an arbitrary combiner algorithm is used, then stacking can theoretically represent any of the ensemble techniques described in this article, although, in practice, a logistic regression model is often used as the combiner.",
            "score": 84.5598132610321
        },
        {
            "docid": "1881722_2",
            "document": "External memory algorithm . In computing, external memory algorithms or out-of-core algorithms are algorithms that are designed to process data that is too large to fit into a computer's main memory at one time. Such algorithms must be optimized to efficiently fetch and access data stored in slow bulk memory (auxiliary memory) such as hard drives or tape drives, or when memory is on a computer network. External memory algorithms are analyzed in the external memory model.",
            "score": 69.68029880523682
        },
        {
            "docid": "21652_14",
            "document": "Natural language processing . Many different classes of machine learning algorithms have been applied to natural language processing tasks. These algorithms take as input a large set of \"features\" that are generated from the input data. Some of the earliest-used algorithms, such as decision trees, produced systems of hard if-then rules similar to the systems of hand-written rules that were then common. Increasingly, however, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to each input feature. Such models have the advantage that they can express the relative certainty of many different possible answers rather than only one, producing more reliable results when such a model is included as a component of a larger system.",
            "score": 96.74498903751373
        },
        {
            "docid": "55817338_5",
            "document": "Algorithmic bias . Algorithms are difficult to define, but may be generally understood as sets of instructions within computer programs that determine how these programs read, collect, process, and analyze data to generate some readable form of analysis or output. Newer computers can process millions of these algorithmic instructions per second, which has boosted the design and adoption of technologies such as machine learning and artificial intelligence. By analyzing and processing data, algorithms drive search engines, social media websites, recommendation engines, online retail, online advertising, and more.",
            "score": 83.27245163917542
        }
    ],
    "r": [
        {
            "docid": "48841414_5",
            "document": "Multiple instance learning . Keeler et al., in his work in early 1990s was the first one to explore the area of MIL. The actual term multi-instance learning was introduced in the middle of the 1990s, by Dietterich et al. while they were investigating the problem of drug activity prediction. They tried to create a learning systems that could predict whether new molecule was qualified to make some drug, or not, through analyzing a collection of known molecules. Molecules can have many alternative low-energy states, but only one, or some of them, are qualified to make a drug. The problem arose because scientists could only determine if molecule is qualified, or not, but they couldn\u2019t say exactly which of its low-energy shapes are responsible for that. One of the proposed ways to solve this problem was to use supervised learning, and regard all the low-energy shapes of the qualified molecule as positive training instances, while all of the low-energy shapes of unqualified molecules as negative instances. Dietterich et al. showed that such method would have a high false positive noise, from all low-energy shapes that are mislabeled as positive, and thus wasn\u2019t really useful. Their approach was to regard each molecule as a labeled bag, and all the alternative low-energy shapes of that molecule as instances in the bag, without individual labels. Thus formulating multiple-instance learning.  Solution to the multiple instance learning problem that Dietterich et al. proposed is three axis-parallel rectangle (APR) algorithm. It attempts to search for appropriate axis-parallel rectangles constructed by the conjunction of the features. They tested the algorithm on Musk dataset, which is a concrete test data of drug activity prediction and the most popularly used benchmark in multiple-instance learning. APR algorithm achieved the best result, but it should be noted that APR was designed with Musk data in mind.",
            "score": 142.88768005371094
        },
        {
            "docid": "9774017_5",
            "document": "Biomolecular Object Network Databank . The Small Molecule Interaction Database is a database containing protein domain-small molecule interactions. It uses a domain-based approach to identify domain families, found in the Conserved Domain Database (CDD), which interact with a query small molecule. The CDD from NCBI amalgamates data from several different sources; Protein FAMilies (PFAM), Simple Modular Architecture Research Tool (SMART), Cluster of Orthologous Genes (COGs), and NCBI\u2019s own curated sequences. The data in SMID is derived from the Protein Data Bank (PDB), a database of known protein crystal structures. SMID can be queried by entering a protein GI, domain identifier, PDB ID or SMID ID. The results of a search provide small molecule, protein, and domain information for each interaction identified in the database. Interactions with non-biological contacts are normally screened out by default.  SMID-BLAST is a tool developed to annotate known small-molecule binding sites as well as to predict binding sites in proteins whose crystal structures have not yet been determined. The prediction is based on extrapolation of known interactions, found in the PDB, to interactions between an uncrystallized protein with a small molecule of interest. SMID-BLAST was validated against a test set of known small molecule interactions from the PDB. It was shown to be an accurate predictor of protein-small molecule interactions; 60% of predicted interactions identically matched the PDB annotated binding site, and of these 73% had greater than 80% of the binding residues of the protein correctly identified. Hogue, C et al. estimated that 45% of predictions that were not observed in the PDB data do in fact represent true positives.",
            "score": 126.29846954345703
        },
        {
            "docid": "1872854_31",
            "document": "Biochemical cascade . In the post-genomic age, high-throughput sequencing and gene/protein profiling techniques have transformed biological research by enabling comprehensive monitoring of a biological system, yielding a list of differentially expressed genes or proteins, which is useful in identifying genes that may have roles in a given phenomenon or phenotype. With DNA microarrays and genome-wide gene engineering, it is possible to screen global gene expression profiles to contribute a wealth of genomic data to the public domain. With RNA interference, it is possible to distill the inferences contained in the experimental literature and primary databases into knowledge bases that consist of annotated representations of biological pathways. In this case, individual genes and proteins are known to be involved in biological processes, components, or structures, as well as how and where gene products interact with each other. Pathway-oriented approaches for analyzing microarray data, by grouping long lists of individual genes, proteins, and/or other biological molecules according to the pathways they are involved in into smaller sets of related genes or proteins, which reduces the complexity, have proven useful for connecting genomic data to specific biological processes and systems. Identifying active pathways that differ between two conditions can have more explanatory power than a simple list of different genes or proteins. In addition, a large number of pathway analytic methods exploit pathway knowledge in public repositories such as Gene Ontology (GO) or Kyoto Encyclopedia of Genes and Genomes (KEGG), rather than inferring pathways from molecular measurements. Furthermore, different research focuses have given the word \"pathway\" different meanings. For example, 'pathway' can denote a metabolic pathway involving a sequence of enzyme-catalyzed reactions of small molecules, or a signaling pathway involving a set of protein phosphorylation reactions and gene regulation events. Therefore, the term \"pathway analysis\" has a very broad application. For instance, it can refer to the analysis physical interaction networks (e.g., protein\u2013protein interactions), kinetic simulation of pathways, and steady-state pathway analysis (e.g., flux-balance analysis), as well as its usage in the inference of pathways from expression and sequence data. Several functional enrichment analysis tools and algorithms have been developed to enhance data interpretation. The existing knowledge base\u2013driven pathway analysis methods in each generation have been summarized in recent literature.",
            "score": 125.48148345947266
        },
        {
            "docid": "27083115_13",
            "document": "Single-molecule FRET . A major application of smFRET is to analyze the minute biochemical nuances that facilitate protein folding. In recent years, multiple techniques have been developed to investigate single molecule interactions that are involved in protein folding and unfolding. Force-probe techniques, using atomic force microscopy and laser tweezers, have provided information on protein stability. smFRET allows researchers to investigate molecular interactions using fluorescence. Forster resonance energy transfer (FRET) was first applied to single molecules by Ha et al. and applied to protein folding in work by Hochstrasser, Weiss, et al. The benefit that smFRET as a whole has afforded to analyzing molecular interactions is the ability to test single molecule interactions directly without having to average ensembles of data. In protein folding analysis, ensemble experiments involve taking measurements of multiple proteins that are in various states of transition between their folded and unfolded state. When averaged, the protein structure that can be inferred from the ensemble of data only provides a rudimentary structural model of protein folding. However, true understanding of protein folding requires deciphering the sequence of structural events along the folding pathways between the folded and unfolded states. It is this particular branch of research that smFRET is highly applicable.",
            "score": 124.88742065429688
        },
        {
            "docid": "17663305_2",
            "document": "LigandScout . LigandScout is computer software that allows creating three-dimensional (3D) pharmacophore models from structural data of macromolecule\u2013ligand complexes, or from training and test sets of organic molecules. It incorporates a complete definition of 3D chemical features (such as hydrogen bond donors, acceptors, lipophilic areas, positively and negatively ionizable chemical groups) that describe the interaction of a bound small organic molecule (ligand) and the surrounding binding site of the macromolecule. These pharmacophores can be overlaid and superimposed using a pattern-matching based alignment algorithm that is solely based on pharmacophoric feature points instead of chemical structure. From such an overlay, shared features can be interpolated to create a so-called \"shared-feature pharmacophore\" that shares all common interactions of several binding sites/ligands or extended to create a so-called \"merged-feature\" pharmacophore. The software has been successfully used to predict new lead structures in drug design, e.g., predicting biological activity of novel human immunodeficiency virus (HIV) reverse transcriptase inhibitors.",
            "score": 118.50052642822266
        },
        {
            "docid": "871906_8",
            "document": "Plate reader . Fluorescence polarization measurement is also very close to FI detection. The difference is that the optical system includes polarizing filters on the light path: the samples in the microplate are excited using polarized light (instead of non-polarized light in FI and TRF modes). Depending on the mobility of the fluorescent molecules found in the wells, the light emitted will either be polarized or not. For example, large molecules (e.g. proteins) in solution, which rotate relatively slowly because of their size, will emit polarized light when excited with polarized light. On the other hand, the fast rotation of smaller molecules will result in a depolarization of the signal. The emission system of the plate reader uses polarizing filters to analyze the polarity of the emitted light. A low level of polarization indicates that small fluorescent molecules move freely in the sample. A high level of polarization indicates that fluorescent is attached to a larger molecular complex. As a result, one of the basic applications of FP detection is molecular binding assays, since they allow to detect if a small fluorescent molecule binds (or not) to a larger, non-fluorescent molecule: binding results in a slower rotation speed of the fluorescent molecule, and in an increase in the polarization of the signal.",
            "score": 118.0136947631836
        },
        {
            "docid": "256141_2",
            "document": "Fluorescent tag . In molecular biology and biotechnology, a fluorescent tag, also known as a label or probe, is a molecule that is attached chemically to aid in the labeling and detection of a biomolecule such as a protein, antibody, or amino acid. Generally, fluorescent tagging, or labeling, uses a reactive derivative of a fluorescent molecule known as a fluorophore. The fluorophore selectively binds to a specific region or functional group on the target molecule and can be attached chemically or biologically. Various labeling techniques such as enzymatic labeling, protein labeling, and genetic labeling are widely utilized. Ethidium bromide, fluorescein and green fluorescent protein are common tags. The most commonly labelled molecules are antibodies, proteins, amino acids and peptides which are then used as specific probes for detection of a particular target.",
            "score": 116.71363067626953
        },
        {
            "docid": "31839460_23",
            "document": "Fatigue detection software . To overcome this problem, scientists developed the Universal Fatigue Algorithm based on a data-driven approach. Drowsiness is a state determined by independent non-EEG measures. The Oxford Sleep Resistance Test (OSLER test) and the Psychomotor Vigilance Test (PVT) are the most commonly used measures in sleep research. Both tests were used to establish the sample dataset for development of the Universal Fatigue Algorithm. The algorithm was developed from real EEG of a large number of individuals. Artificial intelligence techniques were then used to map the multitude of individual relationships. The implication is that the result gets progressively universal and significant as more data from a wider range of individuals are included in the algorithm. In addition to an unseen-blinded experiment approach, testing of the algorithm is also subject to independent external parties.",
            "score": 116.17327117919922
        },
        {
            "docid": "149353_4",
            "document": "Computational biology . Computational Biology, which includes many aspects of bioinformatics, is the science of using biological data to develop algorithms or models to understand biological systems and relationships.  Until recently, biologists did not have access to very large amounts of data. This data has now become commonplace, particularly in molecular biology and genomics. Researchers were able to develop analytical methods for interpreting biological information, but were unable to share them quickly among colleagues.  Bioinformatics began to develop in the early 1970s. It was considered the science of analyzing informatics processes of various biological systems. At this time, research in artificial intelligence was using network models of the human brain in order to generate new algorithms. This use of biological data to develop other fields pushed biological researchers to revisit the idea of using computers to evaluate and compare large data sets. By 1982, information was being shared amongst researchers through the use of punch cards. The amount of data being shared began to grow exponentially by the end of the 1980s. This required the development of new computational methods in order to quickly analyze and interpret relevant information. Since the late 1990s, computational biology has become an important part of developing emerging technologies for the field of biology. The terms computational biology and evolutionary computation have a similar name, but are not to be confused. Unlike computational biology, evolutionary computation is not concerned with modeling and analyzing biological data. It instead creates algorithms based on the ideas of evolution across species. Sometimes referred to as genetic algorithms, the research of this field can be applied to computational biology. While evolutionary computation is not inherently a part of computational biology, Computational evolutionary biology is a subfield of it.",
            "score": 115.67721557617188
        },
        {
            "docid": "42709653_6",
            "document": "Susan Band Horwitz . With the search for similar microtubulin binding molecules, scientists explored many natural products in the ocean, specifically sponges. It took around 15 years until another molecule with a similar mechanism was found. In more recent years, the molecules that have been discovered have differing structures from Taxol, however the mechanisms still remain to be similar. One in particular is called, discodermolide. Dr. Horwitz and her team were interested in not only the binding site for the molecule on the microtubule, but also the possible allosteric effects that the molecule may have on other parts of the microtubule. In order to test for these effects, the team used a hydrogen-deuterium exchange process. The results showed that there was in fact several changes that occurred along the microtubule separate from the binding site when the molecule was bound. They found that normal microbtubule-associated proteins, or MAPs, were not able to bind to the microtubules in the normal way. When discodermolide and Taxol\u00ae were both tested together, the results displayed that they do bind in the same location on the microtubules, however they bind in unique ways from each other. This opened a new door for the team as they decided to attempt making hybrid molecules that would put together the active parts of both of these molecules into one super molecule.",
            "score": 113.09336853027344
        },
        {
            "docid": "1686272_38",
            "document": "Chemical biology . The advances in modern sequencing technologies in the late 1990s allowed scientists to investigate DNA of communities of organisms in their natural environments, so-called \"eDNA\", without culturing individual species in the lab. This metagenomic approach enabled scientists to study a wide selection of organisms that were previously not characterized due in part to an incompetent growth condition. These sources of eDNA include, but are not limited to, soils, ocean, subsurface, hot springs, hydrothermal vents, polar ice caps, hypersaline habitats, and extreme pH environments. Of the many applications of metagenomics, chemical biologists and microbiologists such as Jo Handelsman, Jon Clardy, and Robert M. Goodman who are pioneers of metagenomics, explored metagenomic approaches toward the discovery of biologically active molecules such as antibiotics. Functional or homology screening strategies have been used to identify genes that produce small bioactive molecules. Functional metagenomic studies are designed to search for specific phenotypes that are associated with molecules with specific characteristics. Homology metagenomic studies, on the other hand, are designed to examine genes to identify conserved sequences that are previously associated with the expression of biologically active molecules. Functional metagenomic studies enable scientists to discover novel genes that encode biologically active molecules. These assays include top agar overlay assays where antibiotics generate zones of growth inhibition against test microbes, and pH assays that can screen for pH change due to newly synthesized molecules using pH indicator on an agar plate. Substrate-induced gene expression screening (SIGEX), a method to screen for the expression of genes that are induced by chemical compounds, has also been used to search genes with specific functions. These led to the discovery and isolation of several novel proteins and small molecules. For example, the Schipper group identified three eDNA derived AHL lactonases that inhibit biofilm formation of Pseudomonas aeruginosa via functional metagenomic assays. However, these functional screening methods require a good design of probes that detect molecules being synthesized and depend on the ability to express metagenomes in a host organism system. In contrast, homology metagenomic studies led to a faster discovery of genes that have homologous sequences as the previously known genes that are responsible for the biosynthesis of biologically active molecules. As soon as the genes are sequenced, scientists can compare thousands of bacterial genomes simultaneously. The advantage over functional metagenomic assays is that homology metagenomic studies do not require a host organism system to express the metagenomes, thus this method can potentially save the time spent on analyzing nonfunctional genomes. These also led to the discovery of several novel proteins and small molecules. For example, Banik et al. screened for clones containing genes associated with the synthesis of teicoplanin and vancomycin-like glycopeptide antibiotics and found two new biosynthetic gene clusters. In addition, an \"in silico\" examination from the Global Ocean Metagenomic Survey found 20 new lantibiotic cyclases.  There are challenges to metagenomic approaches to discover new biologically active molecules. Only 40% of enzymatic activities present in a sample can be expressed in \"E. coli.\". In addition, the purification and isolation of eDNA is essential but difficult when the sources of obtained samples are poorly understood. However, collaborative efforts from individuals from diverse fields including bacterial genetics, molecular biology, genomics, bioinformatics, robots, synthetic biology, and chemistry can solve this problem together and potentially lead to the discovery of many important biologically active molecules.",
            "score": 112.51933288574219
        },
        {
            "docid": "249254_8",
            "document": "Clique problem . Clique-finding algorithms have been used in chemistry, to find chemicals that match a target structure and to model molecular docking and the binding sites of chemical reactions. They can also be used to find similar structures within different molecules. In these applications, one forms a graph in which each vertex represents a matched pair of atoms, one from each of two molecules. Two vertices are connected by an edge if the matches that they represent are compatible with each other. Being compatible may mean, for instance, that the distances between the atoms within the two molecules are approximately equal, to within some given tolerance. A clique in this graph represents a set of matched pairs of atoms in which all the matches are compatible with each other. A special case of this method is the use of the modular product of graphs to reduce the problem of finding the maximum common induced subgraph of two graphs to the problem of finding a maximum clique in their product.",
            "score": 112.21266174316406
        },
        {
            "docid": "28569_8",
            "document": "Simplified molecular-input line-entry system . Typically, a number of equally valid SMILES strings can be written for a molecule. For example, codice_1, codice_2 and codice_3 all specify the structure of ethanol. Algorithms have been developed to generate the same SMILES string for a given molecule; of the many possible strings, these algorithms choose only one of them. This SMILES is unique for each structure, although dependent on the canonicalization algorithm used to generate it, and is termed the canonical SMILES. These algorithms first convert the SMILES to an internal representation of the molecular structure; an algorithm then examines that structure and produces a unique SMILES string. Various algorithms for generating canonical SMILES have been developed and include those by Daylight Chemical Information Systems, OpenEye Scientific Software, MEDIT, Chemical Computing Group, MolSoft LLC, and the Chemistry Development Kit. A common application of canonical SMILES is indexing and ensuring uniqueness of molecules in a database.",
            "score": 111.69676971435547
        },
        {
            "docid": "10290414_4",
            "document": "Tetramer assay . The centerpiece of each tetramer is a streptavidin complex. Streptavidin is a molecule that forms homotetramer complexes, with each monomer having an unusually high affinity for biotin. Exploiting these facts, scientists have bioengineered E. coli to produce soluble MHC molecules with a biotinylation protein domain, meaning a part of the MHC can be replaced by covalently bound biotin (via BirA enzyme activity). The MHC molecules must then be mixed with the antigenic peptide of interest, forming peptide-MHC (pMHC) complexes. The biotinylated domain then allows for up to 4 pMHCs to bind to a fluorescently tagged streptavidin complex with high affinity. The resulting pMHC-streptavidin-fluorophore tetramer can be added to a sample of cells. The tetramers bind to T-cells that are specific for both the MHC type and peptide being used in the tetramer. Once the tetramers are bound, T-cells are often stained with other fluorophores and the sample is washed to remove non-bound tetramers and ligands. The stained sample is then run through a flow cytometer for detection and sorting. The fluorophore on any bound tetramers can be excited to give a signal, indicating that the tetramer is bound to a T-cell, and thus that the bound T-cell is specific for the peptide antigen of interest. Ultimately, a signal means that there exists some cell-mediated immune response to the pathogen from which the antigenic peptide is derived, and the strength of the signal gives the strength of the immune response.",
            "score": 111.47943115234375
        },
        {
            "docid": "53367602_4",
            "document": "SMiLE-Seq . SMiLE-seq uses a microfluidic device into which transcription factors, which have been transcribed and translated \"in vitro\", are loaded. Transcription factor samples (~0.3\u00a0ng) are modified by the addition of an enhanced green fluorescent protein (eGFP) tag and combined with both target double-stranded DNA molecules (~8 pmol) tagged with Cyanine Dye5 (Cy5) and a double-stranded competitive DNA model, poly-dIdC, which operates as a negative control to limit spurious binding interactions. When multiple transcription factors are simultaneously analyzed (e.g., when characterization of potential heterodimeric binding interactions is performed), each transcription factor is tagged with a correspondingly unique fluorescent tag. Samples are pumped through the microfluidic device in a passive, twenty-minute process that utilizes capillary action in a series of parallel channels. eGFP-tagged transcription factors are immunocaptured using anchored biotinylated anti-eGFP antibodies. Mechanical depression of a button traps bound transcription factor-DNA complexes, and fluorescent analysis is performed. Fluorescent readouts that identify the presence of multiple fluorescent tags associated with a single antibody indicate heterodimeric binding interactions. The presence of DNA is confirmed by Cy5 signal detection. A polydimethylsiloxane membrane on the button surface captures successfully bound transcription factor-DNA complexes, while unbound transcription factors and targets are washed away. Following the removal of unbound components, bound DNA molecules are collected, pooled, and amplified. Sequencing is subsequently performed using NextSeq 500 or HiSeq2000 sequencing lanes. Sequence data is used to develop a seed sequence, which is then probed for functional motifs using a uniquely developed hidden Markov model-based software pipeline.",
            "score": 109.89307403564453
        },
        {
            "docid": "53970843_3",
            "document": "Machine learning in bioinformatics . Prior to the emergence of machine learning algorithms, bioinformatics algorithms had to be explicitly programmed by hand which, for problems such as protein structure prediction, proves extremely difficult. Machine learning techniques such as deep learning enable the algorithm to make use of automatic feature learning which means that based on the dataset alone, the algorithm can learn how to combine multiple features of the input data into a more abstract set of features from which to conduct further learning. This multi-layered approach to learning patterns in the input data allows such systems to make quite complex predictions when trained on large datasets. In recent years, the size and number of available biological datasets have skyrocketed, enabling bioinformatics researchers to make use of these machine learning systems. Machine learning has been applied to six main subfields of bioinformatics: genomics, proteomics, microarrays, systems biology, evolution, and text mining.",
            "score": 109.71792602539062
        },
        {
            "docid": "1349300_20",
            "document": "Cyclic nucleotide\u2013gated ion channel . By measuring the currents activated in excised inside-out membrane patches upon superfusion with varying ligand concentrations, the ligand sensitivity and selectivity of both endogenously and exogenously expressed CNG channels have been studied. All native CNG channels react to both cAMP and cGMP, but smaller concentrations of cGMP than of cAMP are needed to activate and open the channels. CNG channels are sharply selective between cGMP and cAMP in rods and cones, whereas in OSNs, the channels respond equally well to both ligands. CNG channels found in OSNs are much more sensitive to both cGMP and cAMP than photoreceptor CNG channels. Studies of dose response relations showed that channel activation is greatly dependent on cGMP concentration; several cGMP molecules bind to the channel in a cooperative manner. Since each subunit contains a single cNMP-binding site, and homomeric and heteromeric channels most likely form a tetrameric complex, a maximum of four ligand molecules can bind to the channel. Selectivity can be achieved by differential control of the affinity for binding of the ligand, efficacy of gating, or a combination of both. Binding affinity means how tightly cyclic nucleotides bind to the channel. Efficacy refers to the ability of ligand to activate and open the channel once it is bound. Although these processes are useful in understanding selectivity, they are inextricably coupled to each other that it is very difficult to experimentally separate one from another.",
            "score": 109.32828521728516
        },
        {
            "docid": "1697945_27",
            "document": "Host\u2013guest chemistry . Previous NMR studies have given useful information about the binding of different guest to hosts. Fox et al. calculated the hydrogen-bond interactions between pyridine molecules and poly(amido amine (PAMAM) dendrimer; on the basis of the chemical shift of the amine and the amide groups. In a similar study, Xu et al. titrated carboxylate based G4 PAMAM dendrimer (the host) with various amine based drugs (the guests) and monitored the chemical shifts of the dendrimer. In conjunction with the 2D-NOESY NMR techniques, they were able to precisely locate the position of the drugs on the dendrimers and the effect of functionality on the binding affinity of the drugs. They found conclusive evidence to show that the cationic drug molecules attach on the surface of anionic dendrimers by electrostatic interactions, whereas an anionic drug localizes both in the core and the surface of the dendrimers, and that the strength of these interactions are dependent on the pKa values of the molecules.",
            "score": 109.28888702392578
        },
        {
            "docid": "32898112_8",
            "document": "TRANSFAC . The TRANSFAC database can be used as an encyclopedia of eukaryotic transcription factors. The target sequences and the regulated genes can be listed for each TF, which can be used as benchmark for TFBS recognition tools or as training sets for new TFBS recognition algorithms. The TF classification enables to analyze such data sets with regard to the properties of the DNA-binding domains. Another application is to retrieve all TFs that regulate a given (set of) gene(s). In the context of systems-biological studies, the TF-target gene relations documented in TRANSFAC were used to construct and analyze transcription regulatory networks. By far the most frequent use of TRANSFAC is the computational prediction of potential transcription factor binding sites (TFBS). A number of algorithms exist which either use the individual binding sites or the matrix library for this purpose:",
            "score": 109.12068176269531
        },
        {
            "docid": "42676762_48",
            "document": "Quantum algorithm for linear systems of equations . Wiebe et al. provide a new quantum algorithm to determine the quality of a least-squares fit in which a continuous function is used to approximate a set of discrete points by extending the quantum algorithm for linear systems of equations. As the amount of discrete points increases, the time required to produce a least-squares fit using even a quantum computer running a quantum state tomography algorithm becomes very large. Wiebe et al. find that in many cases, their algorithm can efficiently find a concise approximation of the data points, eliminating the need for the higher-complexity tomography algorithm.",
            "score": 109.04139709472656
        },
        {
            "docid": "1697945_30",
            "document": "Host\u2013guest chemistry . UV-vis spectroscopy is one of the oldest and quickest methods of studying the binding activity of various molecules. The absorption of UV-light takes place at a time-scale of picoseconds, hence the individual signals from the species can be observed. At the same time, the intensity of absorption directly correlates with the concentration of the species, which enables easy calculation of the association constant. Most commonly, either the host or the guest is transparent to UV-light, whilst the other molecule is UV-sensitive. The change in the concentration of the UV-sensitive molecules is thus monitored and fitted onto a straight line using the Benesi-Hildebrand method, from which the association constant can be directly calculated. Additional information about the stoichiometry of the complexes is also obtained, as the Benesi-Hilderbrand method assumes a 1:1 stoichiometry between the host and the guest. The data will yield a straight line only if the complex formation also follows a similar 1:1 stoichiometry. A recent example of a similar calculation was done by Sun et al., wherein they titrated ruthenium trisbipyridyl-viologen molecules with cucurbit[7]urils and plotted the relative absorbance of the cucurbit molecules as a function of its total concentration at a specific wavelength. The data nicely fitted a 1:1 binding model with a binding constant of formula_52 . As an extension, one can fit the data to different stoichiometries to understand the kinetics of the binding events between the host and the guest. made use of this corollary to slightly modify the conventional Benesi-Hilderbrand plot to get the order of the complexation reaction between barium-containing crown ether bridged chiral heterotrinuclear salen Zn(II) complex formula_53 (host) with various guests imidazoles and amino acid methyl esters, along with the other parameters. They titrated a fixed concentration of the zinc complex with varying amounts of the imidazoles and methyl esters whilst monitoring the changes in the absorbance of the pi to pi* transition band at 368\u00a0nm. The data fit a model in which the ratio of guest to host of 2 in the complex. They further carried these experiments at various temperatures which enabled them to calculate the various thermodynamic parameters using the Van 't Hoff equation.",
            "score": 107.90225982666016
        },
        {
            "docid": "4927505_36",
            "document": "Molecular graphics . For greater realism and better comprehension of the 3D structure of a molecule many computer graphics algorithms can be used. For many years molecular graphics has stressed the capabilities of graphics hardware and has required hardware-specific approaches. With the increasing power of machines on the desktop, portability is more important and programs such as Jmol have advanced algorithms that do not rely on hardware. On the other hand, recent graphics hardware is able to interactively render very complex molecule shapes with a quality that would not be possible with standard software techniques.",
            "score": 107.77577209472656
        },
        {
            "docid": "736407_17",
            "document": "Opioid receptor . When an agonistic ligand binds to the opioid receptor, a conformational change occurs, and the GDP molecule is released from the G\u03b1 sub-unit. This mechanism is complex, and is a major stage of the signal transduction pathway. When the GDP molecule is attached, the G\u03b1 sub-unit is in its inactive state, and the nucleotide-binding pocket is closed off inside the protein complex. However, upon ligand binding, the receptor switches to an active conformation, and this is driven by intermolecular rearrangement between the trans-membrane helices. The receptor acitvation releases an \u2018ionic lock\u2019 which holds together the cytoplasmic sides of transmembrane helices three and six, causing them to rotate. This conformational change exposes the intracellular receptor domains at the cytosolic side, which further leads to the activation of the G protein. When the GDP molecule dissociates from the G\u03b1 sub-unit, a GTP molecule binds to the free nucleotide-binding pocket, and the G protein becomes active. A G\u03b1(GTP) complex is formed, which has a weaker affinity for the G\u03b2\u03b3 sub-unit than the G\u03b1(GDP) complex, causing the G\u03b1 sub-unit to separate from the G\u03b2\u03b3 sub-unit, forming two sections of the G protein. The sub-units are now free to interact with effector proteins; however, they are still attached to the plasma membrane by lipid anchors. After binding, the active G protein sub-units diffuses within the membrane and acts on various intracellular effector pathways. This includes inhibiting neuronal adenylate cyclase activity, as well as increasing membrane hyper-polarisation. When the adenylyl cyclase enzyme complex is stimulated, it results in the formation of Cyclic Adenosine 3', 5'-Monophosphate (cAMP), from Adenosine 5' Triphosphate (ATP). cAMP acts as a secondary messenger, as it moves from the plasma membrane into the cell and relays the signal.",
            "score": 107.49884796142578
        },
        {
            "docid": "6497232_3",
            "document": "Molecularly imprinted polymer . Molecular imprinting is, in fact, making an artificial tiny lock for a specific molecule that serve as miniature key. Like plastic receptors the imprinted polymer grabs specific chemicals. Many basic biological processes, from sensing of odors to signaling between nerve and muscle cells, rely on such lock-and-key combinations. For decades, scientists trying to understand these interactions often play locksmith, searching for the right key to fit a particular receptor. Now, the elegance of molecular imprinting in nature has been spurring many scientists to build the locks themselves. They etch a material to create specific cavities which in size, shape and functional groups, fit the target molecule.  However, one of the greatest advantages of artificial receptors over naturally occurring ones is freedom of molecular design. Their frameworks are never restricted to proteins, and a variety of skeletons (e.g., carbon chains and fused aromatic rings) can be used. Thus, the stability, flexibility, and other properties are freely modulated according to need. Even functional groups that are not found in nature can be employed in these man-made compounds. Furthermore, when necessary, the activity in response towards outer stimuli (photo-irradiation, pH change, electric or magnetic field, and others) can be provided by using appropriate functional groups. The spectrum of functions is far wider than that of naturally occurring ones. In a molecular imprinting processes, one needs a 1) template, 2) functional monomer(s) 3) cross-linker, 4) initiator, 5) porogenic solvent and 6) extraction solvent. According to polymerization method and final polymer format one or some of the reagent can be avoided. The self-assembly method has advantages in the fact that it forms a more natural binding site, and also offers additional flexibility in the types of monomers that can be polymerized. The covalent method has its advantages in generally offering a high yield of homogeneous binding sites, but first requires the synthesis of a derivatized imprint molecule and may not imitate the \"natural\" conditions that could be present elsewhere. Over the recent years, interest in the technique of molecular imprinting has increased rapidly, both in the academic community and in the industry. Consequently, significant progress has been made in developing polymerization methods that produce adequate MIP formats with rather good binding properties expecting an enhancement in the performance or in order to suit the desirable final application, such as beads, films or nanoparticles. One of the key issues that have limited the performance of MIPs in practical applications so far is the lack of simple and robust methods to synthesize MIPs in the optimum formats required by the application. Chronologically, the first polymerization method encountered for MIP was based on \"bulk\" or solution polymerization. This method is the most common technique used by groups working on imprinting especially due to its simplicity and versatility. It is used exclusively with organic solvents mainly with low dielectric constant and consists basically of mixing all the components (template, monomer, solvent and initiator) and subsequently polymerizing them. The resultant polymeric block is then pulverized, freed from the template, crushed and sieved to obtain particles of irregular shape and size between 20 and 50\u00a0\u00b5m. Depending on the target (template) type and the final application of the MIP, MIPs are appeared in different formats such as nano/micro spherical particles, nanowires and thin film or membranes. They are produced with different polymerization techniques like bulk, precipitation, emulsion, suspension, dispersion, gelation, and multi-step swelling polymerization. Most of investigators in the field of MIP are making MIP with heuristic techniques such as hierarchical imprinting method. The technique for the first time was used for making MIP by Sellergren et al. for imprinting small target molecules. With the same concept, Nematollahzadeh et al. developed a general technique, so-called polymerization packed bed, to obtain a hierarchically structured high capacity protein imprinted porous polymer beads by using silica porous particles for protein recognition and capture.",
            "score": 107.06898498535156
        },
        {
            "docid": "1697945_35",
            "document": "Host\u2013guest chemistry . Raman spectroscopy is a spectroscopic technique used in the study of molecules which exhibit a Raman scattering effect when monochromatic light is incident on it. The basic requirement to get a Raman signal is that the incident light brings about an electronic transition in the chemical species from its ground state to a virtual energy state, which will emit a photon on returning to the ground state. The difference in energy between the absorbed and the emitted photon is unique for each chemical species depending on its electronic environment. Hence, the technique serves as an important tool for study of various binding events, as binding between molecules almost always results in a change in their electronic environment. However, what makes Raman spectroscopy a unique technique is that only transitions which are accompanied by a change in the polarization of the molecule are Raman active. The structural information derived from Raman spectra gives very specific information about the electronic configuration of the complex as compared to the individual host and guest molecules.",
            "score": 106.48580932617188
        },
        {
            "docid": "1072943_15",
            "document": "Forward algorithm . The forward algorithm is mostly used in applications that need us to determine the probability of being in a specific state when we know about the sequence of observations. We first calculate the probabilities over the states computed for the previous observation and use them for the current observations, and then extend it out for the next step using the transition probability table.The approach basically caches all the intermediate state probabilities so they are computed only once. This helps us to compute a fixed state path. The process is also called posterior decoding. The algorithm computes probability much more efficiently than the naive approach, which very quickly ends up in a combinatorial explosion. Together, they can provide the probability of a given emission/observation at each position in the sequence of observations. It is from this information that a version of the most likely state path is computed (\"posterior decoding\"). The algorithm can be applied wherever we can train a model as we receive data using Baum-Welch or any general EM algorithm. The Forward algorithm will then tell us about the probability of data with respect to what is expected from our model. One of the applications can be in the domain of Finance, where it can help decide on when to buy or sell tangible assets. It can have applications in all fields where we apply Hidden Markov Models. The popular ones include Natural language processing domains like tagging part-of-speech and speech recognition. Recently it is also being used in the domain of Bioinformatics. Forward algorithm can also be applied to perform Weather speculations. We can have a HMM describing the weather and its relation to the state of observations for few consecutive days (some examples could be dry, damp, soggy, sunny, cloudy, rainy etc.). We can consider calculating the probability of observing any sequence of observations recursively given the HMM. We can then calculate the probability of reaching an intermediate state as the sum of all possible paths to that state. Thus the partial probabilities for the final observation will hold the probability of reaching those states going through all possible paths.",
            "score": 106.38304901123047
        },
        {
            "docid": "29467449_17",
            "document": "Protein function prediction . This technique is a computational adaptation of 'wet lab' work from 1996. It was discovered that ascertaining the structure of a protein while it is suspended in different solvents and then superimposing those structures on one another produces data where the organic solvent molecules (that the proteins were suspended in) typically cluster at the protein's active site. This work was carried out as a response to realizing that water molecules are visible in the electron density maps produced by X-ray crystallography. The water molecules are interacting with the protein and tend to cluster at the protein's polar regions. This led to the idea of immersing the purified protein crystal in other solvents (e.g. ethanol, isopropanol, etc.) to determine where these molecules cluster on the protein. The solvents can be chosen based on what they approximate, that is, what molecule this protein may interact with (e.g. ethanol can probe for interactions with the amino acid serine, isopropanol a probe for threonine, etc.). It is vital that the protein crystal maintains its tertiary structure in each solvent. This process is repeated for multiple solvents and then this data can be used to try to determine potential active sites on the protein. Ten years later this technique was developed into an algorithm by Clodfelter et al.",
            "score": 105.30007934570312
        },
        {
            "docid": "1075046_16",
            "document": "Interactome . Some algorithms use experimental evidence on structural complexes, the atomic details of binding interfaces and produce detailed atomic models of protein\u2013protein complexes as well as other protein\u2013molecule interactions. Other algorithms use only sequence information, thereby creating unbiased complete networks of interaction with many mistakes.",
            "score": 104.50440979003906
        },
        {
            "docid": "21497644_4",
            "document": "Shridhar Ramachandra Gadre . After returning to India, Gadre joined as a lecturer at the Department of Chemistry of University of Pune and became a Professor of Physical Chemistry in 1988. Since joining the University of Pune, Gadre's work has focused on the study of molecular scalar fields and their topographical characteristics. In 1985 he proposed a new maximum entropy principle which has been since investigated by other researchers. Gadre has been at the forefront of investigating chemical reactivity and weak interactions through the use of molecular electrostatics potentials and momentum densities. Gadre has also contributed to algorithms and codes for efficiently parallelising quantum chemical ab inito calculations. Recently he has pioneered a technique called the Molecular Tailoring Approach that seeks to calculate accurate one-electron properties for large molecules by breaking down the molecules into fragments and then combining calculations on individual fragments. This approach is now extended to perform geometry optimisation as well as Hessian (vibrational frequency) calculation of large molecules in an efficient manner even on a common personal computer.",
            "score": 103.88329315185547
        },
        {
            "docid": "29732133_29",
            "document": "History of RNA biology . Experimental methods were invented that allowed investigators to use large, diverse populations of RNA molecules to carry out in vitro molecular experiments that utilized powerful selective replication strategies used by geneticists, and which amount to evolution in the test tube. These experiments have been described using different names, the most common of which are \"combinatorial selection\", \"in vitro selection\", and SELEX (for Systematic Evolution of Ligands by Exponential Enrichment). These experiments have been used for isolating RNA molecules with a wide range of properties, from binding to particular proteins, to catalyzing particular reactions, to binding low molecular weight organic ligands. They have equal applicability to elucidating interactions and mechanisms that are known properties of naturally occurring RNA molecules to isolating RNA molecules with biochemical properties that are not known in nature. In developing in vitro selection technology for RNA, laboratory systems for synthesizing complex populations of RNA molecules were established, and used in conjunction with the selection of molecules with user-specified biochemical activities, and in vitro schemes for RNA replication. These steps can be viewed as (a) mutation, (b) selection, and (c) replication. Together, then, these three processes enable in vitro molecular evolution.",
            "score": 103.87285614013672
        },
        {
            "docid": "35738500_6",
            "document": "MHC multimer . MHC tetramers consist of four MHC molecules, a tetramerization agent and a fluorescently labeled protein (usually streptavidin). Streptavidins have also been generated with 6 or 12 binding sites for MHC. MHC tetramers are used to identify and label specific T-cells by epitope specific binding, allowing the antigen specific immune response to be analyzed in both animal model and in man. MHC tetramers were originally developed using MHC class I molecules for the recognition of cytotoxic T cells, but over the last decade they have allowed for the recognition of CD4 T cells by a wide variety of antigens. Tetramer assays are used for single-cell phenotyping and cell counting, and offer an important advantage over other methods, such as ELISPOT and single-cell PCR because they enable the recovery and further study of sorted cells. As a flow-cytometry-based application, tetramers are also easy to use and have a short assay time, similar to Antibody-based flow cytometry studies.",
            "score": 103.85147094726562
        },
        {
            "docid": "39720117_4",
            "document": "Collision-induced absorption and emission . Ordinary spectroscopy is concerned with the spectra of single atoms or molecules. Here we outline the very different spectra of complexes consisting of two or more interacting atoms or molecules: the \"interaction-induced\" or \"collision-induced\" spectroscopy. Both ordinary and collision-induced spectra may be observed in emission and absorption and require an electric or magnetic multipole moment - in most cases an electric dipole moment - to exist for an optical transition to take place from an initial to a final quantum state of a molecule or a molecular complex. (For brevity of expression we will use here the term \"molecule\" interchangeably for atoms as well as molecules). A complex of interacting molecules may consist of two or more molecules in a collisional encounter, or else of a weakly bound van der Waals molecule. On first sight, it may seem strange to treat optical transitions of a collisional complex, which may exist just momentarily, for the duration of a fly-by encounter (roughly 10 seconds), in much the same way as this was long done for molecules in ordinary spectroscopy. But even transient complexes of molecules may be viewed as a new, \"supermolecular\" system which is subject to the same spectroscopic rules as ordinary molecules. Ordinary molecules may be viewed as complexes of atoms that have new and possibly quite different spectroscopic properties than the individual atoms the molecule consists of, when the atoms are not bound together as a molecule (or are not \"interacting\"). Similarly, complexes of interacting molecules may (and usually do) acquire new optical properties, which often are absent in the non-interacting, well separated individual molecules.",
            "score": 103.3812026977539
        }
    ]
}