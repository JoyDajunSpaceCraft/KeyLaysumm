{
    "q": [
        {
            "docid": "5198024_2",
            "document": "Efficient coding hypothesis . The efficient coding hypothesis was proposed by Horace Barlow in 1961 as a theoretical model of sensory coding in the brain. Within the brain, neurons often communicate with one another by sending electrical impulses referred to as action potentials or spikes. One goal of sensory neuroscience is to decipher the meaning of these spikes in order to understand how the brain represents and processes information about the outside world. Barlow hypothesized that the spikes in the sensory system formed a neural code for efficiently representing sensory information. By efficient Barlow meant that the code minimized the number of spikes needed to transmit a given signal. This is somewhat analogous to transmitting information across the internet, where different file formats can be used to transmit a given image. Different file formats require different number of bits for representing the same image at given distortion level, and some are better suited for representing certain classes of images than others. According to this model, the brain is thought to use a code which is suited for representing visual and audio information representative of an organism's natural environment.",
            "score": 88.69477677345276
        },
        {
            "docid": "21944_40",
            "document": "Nervous system . Feature detection is the ability to extract biologically relevant information from combinations of sensory signals. In the visual system, for example, sensory receptors in the retina of the eye are only individually capable of detecting \"points of light\" in the outside world. Second-level visual neurons receive input from groups of primary receptors, higher-level neurons receive input from groups of second-level neurons, and so on, forming a hierarchy of processing stages. At each stage, important information is extracted from the signal ensemble and unimportant information is discarded. By the end of the process, input signals representing \"points of light\" have been transformed into a neural representation of objects in the surrounding world and their properties. The most sophisticated sensory processing occurs inside the brain, but complex feature extraction also takes place in the spinal cord and in peripheral sensory organs such as the retina.",
            "score": 62.062171459198
        },
        {
            "docid": "505717_72",
            "document": "Image segmentation . Pulse-coupled neural networks (PCNNs) are neural models proposed by modeling a cat\u2019s visual cortex and developed for high-performance biomimetic image processing. In 1989, Eckhorn introduced a neural model to emulate the mechanism of a cat\u2019s visual cortex. The Eckhorn model provided a simple and effective tool for studying the visual cortex of small mammals, and was soon recognized as having significant application potential in image processing. In 1994, the Eckhorn model was adapted to be an image processing algorithm by Johnson, who termed this algorithm Pulse-Coupled Neural Network. Over the past decade, PCNNs have been utilized for a variety of image processing applications, including: image segmentation, feature generation, face extraction, motion detection, region growing, noise reduction, and so on. A PCNN is a two-dimensional neural network. Each neuron in the network corresponds to one pixel in an input image, receiving its corresponding pixel\u2019s color information (e.g. intensity) as an external stimulus. Each neuron also connects with its neighboring neurons, receiving local stimuli from them. The external and local stimuli are combined in an internal activation system, which accumulates the stimuli until it exceeds a dynamic threshold, resulting in a pulse output. Through iterative computation, PCNN neurons produce temporal series of pulse outputs. The temporal series of pulse outputs contain information of input images and can be utilized for various image processing applications, such as image segmentation and feature generation. Compared with conventional image processing means, PCNNs have several significant merits, including robustness against noise, independence of geometric variations in input patterns, capability of bridging minor intensity variations in input patterns, etc.",
            "score": 132.27118504047394
        },
        {
            "docid": "2920040_2",
            "document": "Neuronal tuning . Neuronal tuning refers to the hypothesized property of brain cells by which they selectively represent a particular type of sensory, association, motor, or cognitive information. Some neuronal responses have been hypothesized to be optimally tuned to specific patterns through experience. Neuronal tuning can be strong and sharp, as observed in primary visual cortex (area V1) (but see Carandini et al 2005 ), or weak and broad, as observed in neural ensembles. Single neurons are hypothesized to be simultaneously tuned to several modalities, such as visual, auditory, and olfactory. Neurons hypothesized to be tuned to different signals are often hypothesized to integrate information from the different sources. In computational models called neural networks, such integration is the major principle of operation. The best examples of neuronal tuning can be seen in the visual, auditory, olfactory, somatosensory, and memory systems, although due to the small number of stimuli tested the generality of neuronal tuning claims is still an open question.",
            "score": 65.9865448474884
        },
        {
            "docid": "3975854_2",
            "document": "Sensory neuroscience . Sensory neuroscience is a subfield of neuroscience which explores the anatomy and physiology of neurons that are part of sensory systems such as vision, hearing, and olfaction. Neurons in sensory regions of the brain respond to stimuli by firing one or more nerve impulses (action potentials) following stimulus presentation. How is information about the outside world encoded by the rate, timing, and pattern of action potentials? This so-called neural code is currently poorly understood and sensory neuroscience plays an important role in the attempt to decipher it. Looking at early sensory processing is advantageous since brain regions that are \"higher up\" (e.g. those involved in memory or emotion) contain neurons which encode more abstract representations. However, the hope is that there are unifying principles which govern how the brain encodes and processes information. Studying sensory systems is an important stepping stone in our understanding of brain function in general.",
            "score": 98.24550485610962
        },
        {
            "docid": "5198024_22",
            "document": "Efficient coding hypothesis . Observed redundancy: A comparison of the number of retinal ganglion cells to the number of neurons in the primary visual cortex shows an increase in the number of sensory neurons in the cortex as compared to the retina. Simoncelli notes that one major argument of critics in that higher up in the sensory pathway there are greater numbers of neurons that handle the processing of sensory information so this should seem to produce redundancy. However, this observation may not be fully relevant because neurons have different neural coding. In his review, Simoncelli notes \"cortical neurons tend to have lower firing rates and may use a different form of code as compared to retinal neurons\". Cortical Neurons may also have the ability to encode information over longer periods of time than their retinal counterparts. Experiments done in the auditory system have confirmed that redundancy is decreased.",
            "score": 28.78109908103943
        },
        {
            "docid": "5664_64",
            "document": "Consciousness . In neuroscience, a great deal of effort has gone into investigating how the perceived world of conscious awareness is constructed inside the brain. The process is generally thought to involve two primary mechanisms: (1) hierarchical processing of sensory inputs, and (2) memory. Signals arising from sensory organs are transmitted to the brain and then processed in a series of stages, which extract multiple types of information from the raw input. In the visual system, for example, sensory signals from the eyes are transmitted to the thalamus and then to the primary visual cortex; inside the cerebral cortex they are sent to areas that extract features such as three-dimensional structure, shape, color, and motion. Memory comes into play in at least two ways. First, it allows sensory information to be evaluated in the context of previous experience. Second, and even more importantly, working memory allows information to be integrated over time so that it can generate a stable representation of the world\u2014Gerald Edelman expressed this point vividly by titling one of his books about consciousness \"The Remembered Present\". In computational neuroscience, Bayesian approaches to brain function have been used to understand both the evaluation of sensory information in light of previous experience, and the integration of information over time. Bayesian models of the brain are probabilistic inference models, in which the brain takes advantage of prior knowledge to interpret uncertain sensory inputs in order to formulate a conscious percept; Bayesian models have successfully predicted many perceptual phenomena in vision and the nonvisual senses.",
            "score": 92.35718846321106
        },
        {
            "docid": "34004373_4",
            "document": "Sensory maps and brain development . The computational map is the \u201ckey building block in the infrastructure of information processing by the nervous system.\u201d Computation defined as the transformation in the representation of information is the essence of brain function. Computational maps are involved in processing sensory information and motor programming, and they contain derived information that is accessible to higher-order processing regions. The first computational map to be proposed was the Jeffress model (1948) which stated that the computation of sound localization was dependent upon timing differences of sensory input. Since the introduction of the Jeffress model, more general guiding principles for relating brain maps to the properties of the computations they perform have been proposed. One of the proposed models is that computations are distributed across parallel processors like computers; with this model, computer processing is a model for computations performed by the brain. More recently, the \u201celastic net\u201d model has been proposed after studying how the primary visual cortex overlaps multiple visual maps, such as visual field position, orientation, direction, ocular dominance, and spatial frequency. The elastic net uses parallel algorithms to analyze the visual field and allows for optimized trade-off between coverage and continuity.",
            "score": 81.81082713603973
        },
        {
            "docid": "2872287_23",
            "document": "Neural binding . Much of the experimental evidence for neural binding has traditionally revolved around sensory awareness. Sensory awareness is accomplished by integrating things together by cognitively perceiving them and then segmenting them so that, in total, there is an image created. Since there can be an infinite number of possibilities in the perception of an object, this has been a unique area of study. The way the brain then collectively pieces certain things together via networking is important not only in the global way of perceiving but also in segmentation. Much of sensory awareness has to do with the taking of a single piece of an object's makeup and then binding its total characteristics so that the brain perceives the object in its final form. Much of the research for the understanding of segmentation and how the brain perceives an object has been done by studying cats. A major finding of this research has to do with the understanding of gamma waves oscillating at 40\u00a0Hz. The information was extracted from a study using the cat visual cortex. It was shown that the cortical neurons responded differently to spatially different objects. These firings of neurons ranged from 40\u201360\u00a0Hz in measure and when observed showed that they fired synchronously when observing different parts of the object. Such coherent responses point to the fact that the brain is doing a kind of coding where it is piecing certain neurons together in the works of making the form of an object. Since the brain is putting these segmented pieces together unsupervised, a significant consonance is found with many philosophers (like Sigmund Freud) who theorize an underlying subconscious that helps to form every aspect of our conscious thought processes.",
            "score": 76.05263721942902
        },
        {
            "docid": "803249_7",
            "document": "Torsten Wiesel . The Hubel and Wiesel experiments greatly expanded the scientific knowledge of sensory processing. In one experiment, done in 1959, they inserted a microelectrode into the primary visual cortex of an anesthetized cat. They then projected patterns of light and dark on a screen in front of the cat. They found that some neurons fired rapidly when presented with lines at one angle, while others responded best to another angle. They called these neurons \"simple cells.\" Still other neurons, which they termed \"complex cells,\" responded best to lines of a certain angle moving in one direction. These studies showed how the visual system builds an image from simple stimuli into more complex representations.",
            "score": 70.78555178642273
        },
        {
            "docid": "832632_5",
            "document": "David H. Hubel . The Hubel and Wiesel experiments greatly expanded the scientific knowledge of sensory processing. The partnership lasted over twenty years and became known as one of the most prominent research pairings in science. In one experiment, done in 1959, they inserted a microelectrode into the primary visual cortex of an anesthetized cat. They then projected patterns of light and dark on a screen in front of the cat. They found that some neurons fired rapidly when presented with lines at one angle, while others responded best to another angle. Some of these neurons responded to light patterns and dark patterns differently. Hubel and Wiesel called these neurons simple cells.\" Still other neurons, which they termed complex cells, detected edges regardless of where they were placed in the receptive field of the neuron and could preferentially detect motion in certain directions. These studies showed how the visual system constructs complex representations of visual information from simple stimulus features.",
            "score": 52.79158532619476
        },
        {
            "docid": "31329046_6",
            "document": "Pre-attentive processing . Information for pre-attentive processing is detected through the five senses. In the visual system, the receptive fields at the back of the eye (retina) transfer the image via axons to the thalamus, specifically the lateral geniculate nuclei. The image then travels to the primary visual cortex and continues on to be processed by the visual association cortex. At each stage, the image is processed with increasing complexity. Pre-attentive processing starts with the retinal image; this image is magnified as it moves from retina to the cortex of the brain. Shades of light and dark are processed in the lateral geniculate nuclei of the thalamus. Simple and complex cells in the brain process boundary and surface information by deciphering the image's contrast, orientation, and edges. When the image hits the fovea, it is highly magnified, facilitating object recognition. The images in the periphery are less clear but help to create a complete image used for scene perception.",
            "score": 80.43005132675171
        },
        {
            "docid": "35982062_6",
            "document": "Biased Competition Theory . There are two major neural pathways that process the information in the visual field; the ventral stream and the dorsal stream. The two pathways run in parallel and are both working simultaneously. The ventral stream is important for object recognition and often referred to as the \u201cwhat\u201d system of the brain; it projects to the inferior temporal cortex. The dorsal stream is important for spatial perception and performance and is referred to as the \u201cwhere\u201d system which projects to the posterior parietal cortex. According to the biased competition theory, an individual\u2019s visual system has limited capacity to process information about multiple objects at any given time. For example, if an individual was presented with two stimuli (objects) and was asked to identify attributes of each object at the same time, the individual\u2019s performance would be worse in comparison to if the objects were presented separately. This suggests multiple objects presented simultaneously in the visual field will compete for neural representation due to limited processing resources. Single cell recording studies conducted by Kastner and Ungerleider examined the neural mechanisms behind the biased competition theory. In their experiment the size of the receptive field's (RF) of neurons within the visual cortex were examined. A single visual stimulus was presented alone in a neuron\u2019s RF, followed with another stimulus presented simultaneously within the same RF. The single \u2018effective\u2019 stimuli produced a low firing rate, whereas the two stimuli presented together produced a high firing rate. The response to the paired stimuli was reduced. This suggests that when two stimuli are presented together within a neuron\u2019s RF, the stimuli are processed in a mutually suppressive manner, rather than being processed independently. This suppression process, according to Kastner and Ungerleider, occurs when two stimuli are presented together because they compete for neural representation, due to limited cognitive processing capacity. The RF experiment suggests that as the number of objects increase, the information available for each object will decrease due to increased neural workload (suppression), and decreased cognitive capacity. In order for an object in the visual field or RF be efficiently processed, there needs to be a way to bias these neurological resources towards the object. Attention prioritizes task relevant objects, biasing this process. For example, this bias can be towards an object which is currently attended to in the visual field or RF, or towards the object that is most relevant to one\u2019s behavior. Functional magnetic resonance imaging (fMRI) has shown that biased competition theory can explain the observed attention effects at a neuronal level. Attention effects bias the internal weight (strengthens connections) of task relevant features toward the attended object. This was shown by Reddy, Kanwisher, and van Rullen who found an increase in oxygenated blood to a specific neuron following a locational cue. Further neurological support comes from neurophysiological studies which have shown that attention results from Top-down biasing, which in turn influences neuronal spiking. In sum, external inputs affect the Top-down guidance of attention, which bias specific neurons in the brain.",
            "score": 99.75487387180328
        },
        {
            "docid": "2860457_6",
            "document": "Neural ensemble . Neuronal ensembles encode information in a way somewhat similar to the principle of Wikipedia operation \u2013 multiple edits by many participants. Neuroscientists have discovered that individual neurons are very noisy. For example, by examining the activity of only a single neuron in the visual cortex, it is very difficult to reconstruct the visual scene that the owner of the brain is looking at. Like a single Wikipedia participant, an individual neuron does not 'know' everything and is likely to make mistakes. This problem is solved by the brain having billions of neurons. Information processing by the brain is population processing, and it is also distributed \u2013 in many cases each neuron knows a little bit about everything, and the more neurons participate in a job, the more precise the information encoding. In the distributed processing scheme, individual neurons may exhibit neuronal noise, but the population as a whole averages this noise out.",
            "score": 61.671035289764404
        },
        {
            "docid": "4231622_7",
            "document": "Inferior temporal gyrus . The information for color and form comes from P-cells that receive their information mainly from cones, so they are sensitive to differences in form and color, as opposed to the M-cells that receive information about motion mainly from rods. The neurons in the inferior temporal cortex, also called the inferior temporal visual association cortex, process this information from the P-cells.  The neurons in the ITC have several unique properties that offer an explanation as to why this area is essential in recognizing patterns. They only respond to visual stimuli and their receptive fields always include the fovea, which is one of the densest areas of the retina and is responsible for acute central vision. These receptive fields tend to be larger than those in the striate cortex and often extend across the midline to unite the two visual half fields for the first time. IT neurons are selective for shape and/or color of stimulus and are usually more responsive to complex shapes as opposed to simple ones. A small percentage of them are selective for specific parts of the face. Faces and likely other complex shapes are seemingly coded by a sequence of activity across a group of cells, and IT cells can display both short or long term memory for visual stimuli based on experience.",
            "score": 75.38988995552063
        },
        {
            "docid": "53953041_15",
            "document": "Predictive coding . The empirical evidence for predictive coding is most robust for perceptual processing. As early as 1999, Rao and Ballard proposed a hierarchical visual processing model in which higher-order visual cortical area sends down predictions and the feedforward connections carry the residual errors between the predictions and the actual lower-level activities (Rao and Ballard, 1999). According to this model, each level in the hierarchical model network (except the lowest level, which represents the image) attempts to predict the responses at the next lower level via feedback connections, and the error signal is used to correct the estimate of the input signal at each level concurrently (Rao and Ballard, 1999). Emberson et al. established the top-down modulation in infants using a cross-modal audiovisual omission paradigm, determining that even infant brains have expectation about future sensory input that is carried downstream from visual cortices and are capable of expectation-based feedback (Emberson et al., 2015). Functional near-infrared spectroscopy (fNIRS) data showed that infant occipital cortex responded to unexpected visual omission (with no visual information input) but not to expected visual omission. These results establish that in a hierarchically organized perception system, higher-order neurons send down predictions to lower-order neurons, which in turn sends back up the prediction error signal.",
            "score": 83.10522639751434
        },
        {
            "docid": "35982062_8",
            "document": "Biased Competition Theory . Bottom-up processes are characterized by an absence of higher level direction in sensory processing. It primarily relies on sensory information and incoming sensory information is the starting point for all Bottom-up processing. Bottom-up refers to when a feature stands out in a visual search. This is commonly called the \u201cpop-out\u201d effect. Salient features like bright colors, movement and big objects make the object \u201cpop-out\u201d of the visual search. \u201cPop-out\u201d features can often attract attention without conscious processing. Objects that stand out are often given priority (bias) in processing. Bottom-up processing is data driven, and according to this stimuli are perceived on the basis of the data which is being experienced through the senses. Evidence suggests that simultaneously presented stimuli do in fact compete in order to be represented in the visual cortex, with stimuli mutually suppressing each other to gain this representation. This was examined by Reynolds and colleagues, who looked at the size of neurons\u2019 receptive field\u2019s within the visual cortex. It was found that the presentation of a single stimulus resulted in a low firing rate while two stimuli presented together resulted in a higher firing rate. Reynolds and colleagues also found that when comparing the neural response of an individually presented visual stimulus to responses gathered from simultaneously presented stimuli, the responses of the concurrent presented stimuli were less than the sum of the responses gathered when each stimuli was presented alone. This suggests that two stimuli presented together increase neural work load required for attention. This increased neural load creates suppressive processes and causes the stimuli to compete for neural representation in the brain. Proulx and Egeth predicted that brighter objects would bias attention in favor of that object. Another prediction is that larger objects would bias the attention in favor of that object. The experiment was a computer-based visual search task, where participants searched for a target among distractions. The results of the study suggested that when irrelevant stimuli were large or bright, attention was biased towards the irrelevant objects, prioritizing them for cognitive processing. This research shows the effects of Bottom-up (stimulus-driven) processing on biased competition theory.",
            "score": 113.58025813102722
        },
        {
            "docid": "297924_23",
            "document": "Mantis shrimp . Research also shows their visual experience of colours is not very different from humans'. The eyes are actually a mechanism that operates at the level of individual cones and makes the brain more efficient. This system allows visual information to be preprocessed by the eyes instead of the brain, which would otherwise have to be larger to deal with the stream of raw data and thus require more time and energy. While the eyes themselves are complex and not yet fully understood, the principle of the system appears to be simple. It is similar in function to the human eye but works in the opposite manner. In the human brain, the inferior temporal cortex has a huge amount of colour-specific neurons which process visual impulses from the eyes to create colourful experiences. The mantis shrimp instead uses the different types of photoreceptors in its eyes to perform the same function as the human brain neurons, resulting in a hardwired and more efficient system for an animal that requires rapid colour identification. Humans have fewer types of photoreceptors, but more colour-tuned neurons, while mantis shrimps appears to have fewer colour neurons and more classes of photoreceptors.",
            "score": 62.86191713809967
        },
        {
            "docid": "4231622_6",
            "document": "Inferior temporal gyrus . The light energy that comes from the rays bouncing off of an object is converted into chemical energy by the cells in the retina of the eye. This chemical energy is then converted into action potentials that are transferred through the optic nerve and across the optic chiasm, where it is first processed by the lateral geniculate nucleus of the thalamus. From there the information is sent to the primary visual cortex, region V1. It then travels from the visual areas in the occipital lobe to the parietal and temporal lobes via two distinct anatomical streams. These two cortical visual systems were classified by Ungerleider and Mishkin (1982, see two-streams hypothesis). One stream travels ventrally to the inferior temporal cortex (from V1 to V2 then through V4 to ITC) while the other travels dorsally to the posterior parietal cortex. They are labeled the \u201cwhat\u201d and \u201cwhere\u201d streams, respectively. The Inferior Temporal Cortex receives information from the ventral stream, understandably so, as it is known to be a region essential in recognizing patterns, faces, and objects.  The understanding at the single-cell level of the IT cortex and its role of utilizing memory to identify objects and or process the visual field based on color and form visual information is a relatively recent in neuroscience. Early research indicated that the cellular connections of the temporal lobe to other memory associated areas of the brain \u2013 namely the hippocampus, the amygdala, the prefrontal cortex, among others. These cellular connections have recently been found to explain unique elements of memory, suggesting that unique single-cells can be linked to specific unique types and even specific memories. Research into the single-cell understanding of the IT cortex reveals many compelling characteristics of these cells: single-cells with similar selectivity of memory are clustered together across the cortical layers of the IT cortex; the temporal lobe neurons have recently been shown to display learning behaviors and possibly relate to long-term memory; and, cortical memory within the IT cortex is likely to be enhanced over time thanks to the influence of the afferent-neurons of the medial-temporal region. Further research of the single-cells of the IT cortex suggests that these cells not only have a direct link to the visual system pathway but also are deliberate in the visual stimuli they respond to: in certain cases, the single-cell IT cortex neurons do not initiate responses when spots or slits, namely simple visual stimuli, are present in the visual field; however, when complicated objects are put in place, this initiates a response in the single-cell neurons of the IT cortex. This provides evidence that not only are the single-cell neurons of the IT cortex related in having a unique specific response to visual stimuli but rather that each individual single-cell neuron has a specific response to a specific stimuli. The same study also reveals how the magnitude of the response of these single-cell neurons of the IT cortex do not change due to color and size but are only influenced by the shape. This led to even more interesting observations where specific IT neurons have been linked to the recognition of faces and hands. This is very interesting as to the possibility of relating to neurological disorders of prosopagnosia and explaining the complexity and interest in the human hand. Additional research form this study goes into more depth on the role of \"face neurons\" and \"hand neurons\" involved in the IT cortex.  The significance of the single-cell function in the IT cortex is that it is another pathway in addition to the lateral geniculate pathway that processes most visual system: this raises questions about how does it benefit our visual information processing in addition to normal visual pathways and what other functional units are involved in additional visual information processing.",
            "score": 87.65568888187408
        },
        {
            "docid": "5198024_12",
            "document": "Efficient coding hypothesis . One approach is to design a model for early sensory processing based on the statistics of a natural image and then compare this predicted model to how real neurons actually respond to the natural image.  The second approach is to measure a neural system responding to a natural environment, and analyze the results to see if there are any statistical properties to this response.",
            "score": 70.42363739013672
        },
        {
            "docid": "5198024_14",
            "document": "Efficient coding hypothesis . In one study by Doi et al. in 2012, the researchers created a predicted response model of the retinal ganglion cells that would be based on the statistics of the natural images used, while considering noise and biological constraints. They then compared the actual information transmission as observed in real retinal ganglion cells to this optimal model to determine the efficiency. They found that the information transmission in the retinal ganglion cells had an overall efficiency of about 80% and concluded that \"the functional connectivity between cones and retinal ganglion cells exhibits unique spatial structure...consistent with coding efficiency. A study by van Hateren and Ruderman in 1998 used ICA to analyze video-sequences and compared how a computer analyzed the independent components of the image to data for visual processing obtained from a cat in DeAngelis et al. 1993. The researchers described the independent components obtained from a video sequence as the \"basic building blocks of a signal\", with the independent component filter (ICF) measuring \"how strongly each building block is present\". They hypothesized that if simple cells are organized to pick out the \"underlying structure\" of images over time then cells should act like the independent component filters. They found that the ICFs determined by the computer were similar to the \"receptive fields\" that were observed in actual neurons.",
            "score": 96.3595415353775
        },
        {
            "docid": "627937_18",
            "document": "Pyramidal cell . Pyramidal neurons in the prefrontal cortex are implicated in cognitive ability. In mammals, the complexity of pyramidal cells increases from posterior to anterior brain regions. The degree of complexity of pyramidal neurons is likely linked to the cognitive capabilities of different anthropoid species. As the prefrontal cortex receives input from areas of the brain that are involved in processing all the sensory modalities, pyramidal cells within the prefrontal cortex appear to process different types of input. Pyramidal cells may play a critical role in complex object recognition within the visual processing areas of the cortex.",
            "score": 41.00242042541504
        },
        {
            "docid": "305136_18",
            "document": "Visual system . In the retina, the photoreceptors synapse directly onto bipolar cells, which in turn synapse onto ganglion cells of the outermost layer, which will then conduct action potentials to the brain. A significant amount of visual processing arises from the patterns of communication between neurons in the retina. About 130 million photo-receptors absorb light, yet roughly 1.2 million axons of ganglion cells transmit information from the retina to the brain. The processing in the retina includes the formation of center-surround receptive fields of bipolar and ganglion cells in the retina, as well as convergence and divergence from photoreceptor to bipolar cell. In addition, other neurons in the retina, particularly horizontal and amacrine cells, transmit information laterally (from a neuron in one layer to an adjacent neuron in the same layer), resulting in more complex receptive fields that can be either indifferent to color and sensitive to motion or sensitive to color and indifferent to motion.",
            "score": 73.18923926353455
        },
        {
            "docid": "3704475_50",
            "document": "Executive functions . Despite the growing currency of the 'biasing' model of executive functions, direct evidence for functional connectivity between the PFC and sensory regions when executive functions are used, is to date rather sparse. Indeed, the only direct evidence comes from studies in which a portion of frontal cortex is damaged, and a corresponding effect is observed far from the lesion site, in the responses of sensory neurons. However, few studies have explored whether this effect is specific to situations where executive functions are required. Other methods for measuring connectivity between distant brain regions, such as correlation in the fMRI response, have yielded indirect evidence that the frontal cortex and sensory regions communicate during a variety of processes thought to engage executive functions, such as working memory, but more research is required to establish how information flows between the PFC and the rest of the brain when executive functions are used. As an early step in this direction, an fMRI study on the flow of information processing during visuospatial reasoning has provided evidence for causal associations (inferred from the temporal order of activity) between sensory-related activity in occipital and parietal cortices and activity in posterior and anterior PFC. Such approaches can further elucidate the distribution of processing between executive functions in PFC and the rest of the brain.",
            "score": 56.60253024101257
        },
        {
            "docid": "3717_44",
            "document": "Brain . Each sensory system begins with specialized receptor cells, such as light-receptive neurons in the retina of the eye, or vibration-sensitive neurons in the cochlea of the ear. The axons of sensory receptor cells travel into the spinal cord or brain, where they transmit their signals to a first-order sensory nucleus dedicated to one specific sensory modality. This primary sensory nucleus sends information to higher-order sensory areas that are dedicated to the same modality. Eventually, via a way-station in the thalamus, the signals are sent to the cerebral cortex, where they are processed to extract the relevant features, and integrated with signals coming from other sensory systems.",
            "score": 51.5888786315918
        },
        {
            "docid": "2363287_6",
            "document": "Visual learning . Various areas of the brain work together in a multitude of ways in order to produce the images that we see with our eyes and that are encoded by our brains. The basis of this work takes place in the visual cortex of the brain. The visual cortex is located in the occipital lobe of the brain and harbors many other structures that aid in visual recognition, categorization, and learning. One of the first things the brain must do when acquiring new visual information is recognize the incoming material. Brain areas involved in recognition are the inferior temporal cortex, the superior parietal cortex, and the cerebellum. During tasks of recognition, there is increased activation in the left inferior temporal cortex and decreased activation in the right superior parietal cortex. Recognition is aided by neural plasticity, or the brain's ability to reshape itself based on new information. Next the brain must categorize the material. The three main areas that are used when categorizing new visual information are the orbitofrontal cortex and two dorsolateral prefrontal regions which begin the process of sorting new information into groups and further assimilating that information into things that you might already know. After recognizing and categorizing new material entered into the visual field, the brain is ready to begin the encoding process \u2013 the process which leads to learning. Multiple brain areas are involved in this process such as the frontal lobe, the right extrastriate cortex, the neocortex, and again, the neostriatum. One area in particular, the limbic-diencephalic region, is essential for transforming perceptions into memories. With the coming together of tasks of recognition, categorization and learning; schemas help make the process of encoding new information and relating it to things you already know much easier. One can remember visual images much better when they can apply it to an already known schema. Schemas actually provide enhancement of visual memory and learning.",
            "score": 83.28229224681854
        },
        {
            "docid": "3883287_8",
            "document": "Tranquillity . Within tranquillity studies, much of the emphasis has been placed on understanding the role of vision in the perception of natural environments, which is probably not surprising, considering that upon first viewing a scene its configurational coherence can be established with incredible speed. Indeed, scene information can be captured in a single glance and the gist of a scene determined in as little as 100ms. The speed of processing of complex natural images was tested by Thorpe \"et al.\" using colour photographs of a wide range of animals (mammals, birds, reptiles and fish), in their natural environments, mixed with distracters that included pictures of forests, mountains, lakes, buildings and fruit. During this experiment, subjects were shown an image for 20ms and asked to determine whether it contained an animal or not. The electrophysiological brain responses obtained in this study showed that a decision could be made within 150ms of the image being seen, indicating the speed at which cognitive visual processing occurs. However, audition, and in particular the individual components that collectively comprise the soundscape, a term coined by Schafer to describe the ever-present array of sounds that constitute the sonic environment, also significantly inform the various schemata used to characterise differing landscape types. This interpretation is supported by the auditory reaction times, which are 50 to 60ms faster than that of the visual modality. It is also known that sound can alter visual perception and that under certain conditions areas of the brain involved in processing auditory information can be activated in response to visual stimuli.  Research conducted by Pheasant \"et al.\" has shown that when individuals make tranquillity assessments based on a uni-modal auditory or visual sensory input, they characterise the environment by drawing upon a number of key landscape and soundscape characteristics. For example, when making assessments in response to visual-only stimuli the percentage of water, flora and geological features present within a scene, positively influence how tranquil a location is perceived to be. Likewise when responding to uni-modal auditory stimuli, the perceived loudness of biological sounds positively influences the perception of tranquillity, whilst the perceived loudness of mechanical sounds have a negative effect. However, when presented with bi-modal auditory-visual stimuli the individual soundscape and landscape components alone no longer influenced the perception of tranquillity. Rather configurational coherence was provided by the percentage of natural and contextual features present within the scene and the equivalent continuous sound pressure level (LAeq).",
            "score": 141.55028462409973
        },
        {
            "docid": "5128182_4",
            "document": "Encoding (memory) . Visual encoding is the process of encoding images and visual sensory information. This means that people can convert the new information that they stored into mental pictures (Harrison, C., Semin, A.,(2009). Psychology. New York p.\u00a0222) Visual sensory information is temporarily stored within our iconic memory and working memory before being encoded into permanent long-term storage. Baddeley's model of working memory states that visual information is stored in the visuo-spatial sketchpad.  The amygdala is a complex structure that has an important role in visual encoding. It accepts visual input in addition to input from other systems and encodes the positive or negative values of conditioned stimuli.",
            "score": 57.28305172920227
        },
        {
            "docid": "3380919_3",
            "document": "David Heeger . In the fields of perceptual psychology, systems neuroscience, cognitive neuroscience, and computational neuroscience, Heeger has developed computational theories of neuronal processing in the visual system, and he has performed psychophysics (perceptual psychology) and neuroimaging (functional magnetic resonance imaging, fMRI) experiments on human vision. His contributions to computational neuroscience include theories for how the brain can sense optic flow and egomotion, and a theory of neural processing called the normalization model. His empirical research has contributed to our understanding of the topographic organization of visual cortex (retinotopy), visual awareness, visual pattern detection/discrimination, visual motion perception, stereopsis (depth perception), attention, working memory, the control of eye and hand movements, neural processing of complex audio-visual and emotional experiences (movies, music, narrative), abnormal visual processing in dyslexia, and neurophysiological characteristics of autism.",
            "score": 65.88603472709656
        },
        {
            "docid": "3382372_2",
            "document": "Normalization model . The normalization model is an influential model of responses of neurons in primary visual cortex. David Heeger developed the model in the early 1990s, and later refined it together with Matteo Carandini and J. Anthony Movshon. The model involves a divisive stage. In the numerator is the output of the classical receptive field. In the denominator, a constant plus a measure of local stimulus contrast. Although the normalization model was initially developed to explain responses in the primary visual cortex, normalization is now thought to operate throughout the visual system, and in many other sensory modalities and brain regions, including the representation of odors, the modulatory effects of visual attention, the encoding of value, and the integration of multisensory information. Its presence in such a diversity of neural systems in multiple species, from invertebrates to mammals, suggests that normalization serves as a canonical neural computation.",
            "score": 28.085922718048096
        },
        {
            "docid": "6147487_30",
            "document": "Neural coding . To account for the fast encoding of visual stimuli, it has been suggested that neurons of the retina encode visual information in the latency time between stimulus onset and first action potential, also called latency to first spike. This type of temporal coding has been shown also in the auditory and somato-sensory system. The main drawback of such a coding scheme is its sensitivity to intrinsic neuronal fluctuations. In the primary visual cortex of macaques, the timing of the first spike relative to the start of the stimulus was found to provide more information than the interval between spikes. However, the interspike interval could be used to encode additional information, which is especially important when the spike rate reaches its limit, as in high-contrast situations. For this reason, temporal coding may play a part in coding defined edges rather than gradual transitions.",
            "score": 46.66291391849518
        },
        {
            "docid": "2483527_8",
            "document": "Thalamocortical radiations . The Lateral geniculate and pulvinar nuclei project to and terminate in V1, and carry motor information from the brain stem as well as other sensory input from the optic tract. The visual cortex connects with other sensory areas which allows for the integration of cognitive tasks such as selective and directed attention, and pre-motor planning, in relation to the processing of incoming visual stimuli. Models of the pulvinar projections to the visual cortex have been proposed by several imaging studies, though the mapping of pulvinar projections has been a difficult task due to the fact that pulvinar subdivisions are not conventionally organized and have been difficult to visualize using structural MRI. Evidence from several studies supports the idea that the Pulvinar nuclei and superior colliculus receive descending projections from CT fibers while TC fibers extending from the LGN carry visual information to the various areas of the visual cortex near the calcarine fissure.",
            "score": 73.59737277030945
        }
    ],
    "r": [
        {
            "docid": "6107563_7",
            "document": "Pulse-coupled networks . A PCNN is a two-dimensional neural network. Each neuron in the network corresponds to one pixel in an input image, receiving its corresponding pixel\u2019s color information (e.g. intensity) as an external stimulus. Each neuron also connects with its neighboring neurons, receiving local stimuli from them. The external and local stimuli are combined in an internal activation system, which accumulates the stimuli until it exceeds a dynamic threshold, resulting in a pulse output. Through iterative computation, PCNN neurons produce temporal series of pulse outputs. The temporal series of pulse outputs contain information of input images and can be used for various image processing applications, such as image segmentation and feature generation. Compared with conventional image processing means, PCNNs have several significant merits, including robustness against noise, independence of geometric variations in input patterns, capability of bridging minor intensity variations in input patterns, etc.",
            "score": 143.68907165527344
        },
        {
            "docid": "3883287_8",
            "document": "Tranquillity . Within tranquillity studies, much of the emphasis has been placed on understanding the role of vision in the perception of natural environments, which is probably not surprising, considering that upon first viewing a scene its configurational coherence can be established with incredible speed. Indeed, scene information can be captured in a single glance and the gist of a scene determined in as little as 100ms. The speed of processing of complex natural images was tested by Thorpe \"et al.\" using colour photographs of a wide range of animals (mammals, birds, reptiles and fish), in their natural environments, mixed with distracters that included pictures of forests, mountains, lakes, buildings and fruit. During this experiment, subjects were shown an image for 20ms and asked to determine whether it contained an animal or not. The electrophysiological brain responses obtained in this study showed that a decision could be made within 150ms of the image being seen, indicating the speed at which cognitive visual processing occurs. However, audition, and in particular the individual components that collectively comprise the soundscape, a term coined by Schafer to describe the ever-present array of sounds that constitute the sonic environment, also significantly inform the various schemata used to characterise differing landscape types. This interpretation is supported by the auditory reaction times, which are 50 to 60ms faster than that of the visual modality. It is also known that sound can alter visual perception and that under certain conditions areas of the brain involved in processing auditory information can be activated in response to visual stimuli.  Research conducted by Pheasant \"et al.\" has shown that when individuals make tranquillity assessments based on a uni-modal auditory or visual sensory input, they characterise the environment by drawing upon a number of key landscape and soundscape characteristics. For example, when making assessments in response to visual-only stimuli the percentage of water, flora and geological features present within a scene, positively influence how tranquil a location is perceived to be. Likewise when responding to uni-modal auditory stimuli, the perceived loudness of biological sounds positively influences the perception of tranquillity, whilst the perceived loudness of mechanical sounds have a negative effect. However, when presented with bi-modal auditory-visual stimuli the individual soundscape and landscape components alone no longer influenced the perception of tranquillity. Rather configurational coherence was provided by the percentage of natural and contextual features present within the scene and the equivalent continuous sound pressure level (LAeq).",
            "score": 141.55027770996094
        },
        {
            "docid": "505717_72",
            "document": "Image segmentation . Pulse-coupled neural networks (PCNNs) are neural models proposed by modeling a cat\u2019s visual cortex and developed for high-performance biomimetic image processing. In 1989, Eckhorn introduced a neural model to emulate the mechanism of a cat\u2019s visual cortex. The Eckhorn model provided a simple and effective tool for studying the visual cortex of small mammals, and was soon recognized as having significant application potential in image processing. In 1994, the Eckhorn model was adapted to be an image processing algorithm by Johnson, who termed this algorithm Pulse-Coupled Neural Network. Over the past decade, PCNNs have been utilized for a variety of image processing applications, including: image segmentation, feature generation, face extraction, motion detection, region growing, noise reduction, and so on. A PCNN is a two-dimensional neural network. Each neuron in the network corresponds to one pixel in an input image, receiving its corresponding pixel\u2019s color information (e.g. intensity) as an external stimulus. Each neuron also connects with its neighboring neurons, receiving local stimuli from them. The external and local stimuli are combined in an internal activation system, which accumulates the stimuli until it exceeds a dynamic threshold, resulting in a pulse output. Through iterative computation, PCNN neurons produce temporal series of pulse outputs. The temporal series of pulse outputs contain information of input images and can be utilized for various image processing applications, such as image segmentation and feature generation. Compared with conventional image processing means, PCNNs have several significant merits, including robustness against noise, independence of geometric variations in input patterns, capability of bridging minor intensity variations in input patterns, etc.",
            "score": 132.27117919921875
        },
        {
            "docid": "48876541_3",
            "document": "Integral channel feature . Typically, a \"channel\" refers to a certain component that defines pixel values in a digital image. A color image, for example is an aggregate of three channels (red, green and blue). The color data of an image is stored in three arrays of values, known as channels. While this definition of a \"channel\" is widely accepted across various domains, there exists a broader definition in computer vision, which allows one to exploit other features of an image besides the color information. One such definition refers to a channel as a \"registered map of the original image where the output pixels are mapped to input pixels by some linear or non-transformation\". According to this notion of a channel, color channels of an image can be redefined as output images that are obtained by extracting one specific color information point from the input image at a time. Similarly, a channel for a grayscale input image is simply equal to a grayscale input image. The simple MATLAB implementation below shows how color channels and grayscale channel can be extracted from an input image. It is clear from the above examples that a channel can be generated by either simply extracting specific information from the original image or by manipulating the input image in some form to obtain the desired channel. Doll\u00e1r \"et al\". defined a channel generation function as \u03a9, which can be used to relate a channel (that is, an output image) to the original image as follows.",
            "score": 117.96292877197266
        },
        {
            "docid": "5212945_3",
            "document": "Visual neuroscience . A recent study using Event-Related Potentials (ERPs) linked an increased neural activity in the occipito-temporal region of the brain to the visual categorization of facial expressions. Results focus on a negative peak in the ERP that occurs 170 milliseconds after the stimulus onset. This action potential, called the N170, was measured using electrodes in the occipito-temporal region, an area already known to be changed by face stimuli. Studying by using the EEG, and ERP methods allow for an extremely high temporal resolution of 4 milliseconds, which makes these kinds of experiments extremely well suited for accurately estimating and comparing the time it takes the brain to perform a certain function. Scientists used classification image techniques, to determine what parts of complex visual stimuli (such as a face) will be relied on when patients are asked to assign them to a category, or emotion. They computed the important features when the stimulus face exhibited one of five different emotions. Stimulus faces exhibiting fear had the distinguishing feature of widening eyes, and stimuli exhibiting happiness exhibited a change in the mouth to make a smile. Regardless of the expression of the stimuli's face, the region near the eyes affected the EEG before the regions near the mouth. This revealed a sequential, and predetermined order to the perception and processing of faces, with the eye being the first, and the mouth, and nose being processed after. This process of downward integration only occurred when the inferior facial features were crucial to the categorization of the stimuli. This is best explained by comparing what happens when participants were shown a face exhibiting fear, versus happiness. The N170 peaked slightly earlier for the fear stimuli at about 175 milliseconds, meaning that it took a participants less time to recognize the facial expression. This is expected because only the eyes need to be processed to recognize the emotion. However, when processing a happy expression, where the mouth is crucial to categorization, downward integration must take place, and thus the N170 peak occurred later at around 185 milliseconds. Eventually visual neuroscience aims to completely explain how the visual system processes all changes in faces as well as objects. This will give a complete view to how the world is constantly visually perceived, and may provide insight into a link between perception and consciousness.",
            "score": 116.51416015625
        },
        {
            "docid": "24978422_3",
            "document": "Visual adaptation . The aftereffects of exposure to a visual stimulus or pattern causes loss of sensitivity to that pattern and induces stimulus bias. An example of this phenomenon is the \"lilac chaser\", introduced by Jeremy Hinton. The stimulus here are lilac circles, that once removed, leave green circles that then become the most prominent stimulus. The fading of the lilac circles is due to a loss of sensitivity to that stimulus and the adaptation to the new stimulus. To experience the \"lilac chaser\" effect, the subject needs to fixate their eyes on the cross in the middle of the image, and after a while the effect will settle in. Visual coding, a process involved in visual adaptation, is the means by which the brain adapts to certain stimuli, resulting in a biased perception of those stimuli. This phenomenon is referred to as visual plasticity; the brain's ability to change and adapt according to certain, repeated stimuli, altering the way information is perceived and processed. The rate and strength of visual adaptation depends heavily on the number of stimuli presented simultaneously, as well as the amount of time for which the stimulus is present. Visual adaptation was found to be weaker when there were more stimuli present. Moreover, studies have found that stimuli can rival each other, which explains why higher numbers of simultaneous stimuli lead to lower stimulus adaptation. Studies have also found that visual adaptation can have a reversing effect; if the stimulus is absent long enough, the aftereffects of visual adaptation will subside. Studies have also shown that visual adaptation occurs in the early stages of processing.",
            "score": 113.58743286132812
        },
        {
            "docid": "35982062_8",
            "document": "Biased Competition Theory . Bottom-up processes are characterized by an absence of higher level direction in sensory processing. It primarily relies on sensory information and incoming sensory information is the starting point for all Bottom-up processing. Bottom-up refers to when a feature stands out in a visual search. This is commonly called the \u201cpop-out\u201d effect. Salient features like bright colors, movement and big objects make the object \u201cpop-out\u201d of the visual search. \u201cPop-out\u201d features can often attract attention without conscious processing. Objects that stand out are often given priority (bias) in processing. Bottom-up processing is data driven, and according to this stimuli are perceived on the basis of the data which is being experienced through the senses. Evidence suggests that simultaneously presented stimuli do in fact compete in order to be represented in the visual cortex, with stimuli mutually suppressing each other to gain this representation. This was examined by Reynolds and colleagues, who looked at the size of neurons\u2019 receptive field\u2019s within the visual cortex. It was found that the presentation of a single stimulus resulted in a low firing rate while two stimuli presented together resulted in a higher firing rate. Reynolds and colleagues also found that when comparing the neural response of an individually presented visual stimulus to responses gathered from simultaneously presented stimuli, the responses of the concurrent presented stimuli were less than the sum of the responses gathered when each stimuli was presented alone. This suggests that two stimuli presented together increase neural work load required for attention. This increased neural load creates suppressive processes and causes the stimuli to compete for neural representation in the brain. Proulx and Egeth predicted that brighter objects would bias attention in favor of that object. Another prediction is that larger objects would bias the attention in favor of that object. The experiment was a computer-based visual search task, where participants searched for a target among distractions. The results of the study suggested that when irrelevant stimuli were large or bright, attention was biased towards the irrelevant objects, prioritizing them for cognitive processing. This research shows the effects of Bottom-up (stimulus-driven) processing on biased competition theory.",
            "score": 113.58026123046875
        },
        {
            "docid": "920668_5",
            "document": "Hqx . A brief description of the implementation taken from the original hqx Library README ( see original hqx project at the external links section )\"The first step is an analysis of the 3x3 area of the source pixel. At first, we calculate the color difference between the central pixel and its 8 nearest neighbors. Then that difference is compared to a predefined threshold, and these pixels are sorted into two categories: \"close\" and \"distant\" colored. There are 8 neighbors, so we are getting 256 possible combinations.\"\"For the next step, which is filtering, a lookup table with 256 entries is used, one entry per each combination of close/distant colored neighbors. Each entry describes how to mix the colors of the source pixels from 3x3 area to get interpolated pixels of the filtered image.\"\"The present implementation is using YUV color space to calculate color differences, with more tolerance on Y (brightness) component, then on color components U and V. That color space conversion is quite easy to implement if the format of the source image is 16 bit per pixel, using a simple lookup table. It is also possible to calculate the color differences and compare them to a threshold very fast, using MMX instructions.\"\"Creating a lookup table was the most difficult part - for each combination the most probable vector representation of the area has to be determined, with the idea of edges between the different colored areas of the image to be preserved, with the edge direction to be as close to a correct one as possible. That vector representation is then rasterised with higher (3x) resolution using anti-aliasing, and the result is stored in the lookup table.\"\"The filter was not designed for photographs, but for images with clear sharp edges, like line graphics or cartoon sprites. It was also designed to be fast enough to process 256x256 images in real-time.\"",
            "score": 113.53520965576172
        },
        {
            "docid": "30860279_3",
            "document": "Image editing . Raster images are stored in a computer in the form of a grid of picture elements, or pixels. These pixels contain the image's color and brightness information. Image editors can change the pixels to enhance the image in many ways. The pixels can be changed as a group, or individually, by the sophisticated algorithms within the image editors. This article mostly refers to bitmap graphics editors, which are often used to alter photographs and other raster graphics. However, vector graphics software, such as Adobe Illustrator, CorelDRAW, Xara Designer Pro, PixelStyle Photo Editor, Inkscape or Vectr, are used to create and modify vector images, which are stored as descriptions of lines, B\u00e9zier curves, and text instead of pixels. It is easier to rasterize a vector image than to vectorize a raster image; how to go about vectorizing a raster image is the focus of much research in the field of computer vision. Vector images can be modified more easily, because they contain descriptions of the shapes for easy rearrangement. They are also scalable, being rasterizable at any resolution.",
            "score": 113.44750213623047
        },
        {
            "docid": "1677048_16",
            "document": "Inattentional blindness . For example, in an functional magnetic resonance imaging (fMRI) study by Rees and colleagues, brain activity was recorded while participants completed a perceptual task. Here they examined the neural processing of meaningful (words) and meaningless (consonant string) stimuli both when attended to, and when these same items were unattended. While no difference in activation patterns were found between the groups when the stimuli were unattended, differences in neural processing were observed for meaningful versus meaningless stimuli to which participants overtly attended. This pattern of results suggests that ignored stimuli are not processed to the level of meaning, i.e. less extensively than attended stimuli. Participants do not seem to be detecting meaning in stimuli to which they are not consciously attending.",
            "score": 112.90422821044922
        },
        {
            "docid": "2603725_18",
            "document": "Matte (filmmaking) . Another digital matting approach was proposed by McGuire et al. It makes use of two imaging sensors along the same optical axis, and uses data from both of them. (There are various ways to achieve this, such as using a beam-splitter or per-pixel polarization filters.) The system simultaneously captures two frames that differ by about half the dynamic range at background pixels but are identical at foreground pixels. Using the differences between the backgrounds of the two images, McGuire et al. are able to extract a high-resolution foreground matte from the scene. This method still retains some of the shortcomings of compositing techniques - namely, the background must be relatively neutral and uniform - but it introduces several benefits, such as precise sub-pixel results, better support for natural illumination, and allowing the foreground to be the color that a compositing technique would identify as part of the background matte. However, this means that intentionally masking something in the foreground by coating it in the same color as the background is impossible.",
            "score": 112.67961120605469
        },
        {
            "docid": "33246145_4",
            "document": "Neural decoding . When looking at a picture, people's brains are constantly making decisions about what object they are looking at, where they need to move their eyes next, and what they find to be the most salient aspects of the input stimulus. As these images hit the back of the retina, these stimuli are converted from varying wavelengths to a series of neural spikes called action potentials. These pattern of action potentials are different for different objects and different colors; we therefore say that the neurons are encoding objects and colors by varying their spike rates or temporal pattern. Now, if someone were to probe the brain by placing electrodes in the primary visual cortex, they may find what appears to be random electrical activity. These neurons are actually firing in response to the lower level features of visual input, possibly the edges of a picture frame. This highlights the crux of the neural decoding hypothesis: that it is possible to reconstruct a stimulus from the response of the ensemble of neurons that represent it. In other words, it is possible to look at spike train data and say that the person or animal being recorded is looking at a red ball.",
            "score": 110.66629791259766
        },
        {
            "docid": "2534964_14",
            "document": "Sensory processing . Perhaps one of the most studied sensory integrations is the relationship between vision and audition. These two senses perceive the same objects in the world in different ways, and by combining the two, they help us understand this information better. Vision dominates our perception of the world around us. This is because visual spatial information is one of the most reliable sensory modalities. Visual stimuli are recorded directly onto the retina, and there are few, if any, external distortions that provide incorrect information to the brain about the true location of an object. Other spatial information is not as reliable as visual spatial information. For example, consider auditory spatial input. The location of an object can sometimes be determined solely on its sound, but the sensory input can easily be modified or altered, thus giving a less reliable spatial representation of the object. Auditory information therefore is not spatially represented unlike visual stimuli. But once one has the spatial mapping from the visual information, multisensory integration helps bring the information from both the visual and auditory stimuli together to make a more robust mapping.",
            "score": 108.3775863647461
        },
        {
            "docid": "31329046_5",
            "document": "Pre-attentive processing . The \"contingent-capture\" model emphasizes the idea that a person\u2019s current intentions and/or goals affect the speed and efficiency of pre-attentive processing. The brain directs an individual\u2019s attention towards stimuli with features that fit in with their goals. Consequently, these stimuli will be processed faster at the pre-attentive stage and will be more likely to be selected for attentive processing. Since this model focuses on the importance of conscious processes (rather than properties of the stimulus itself) in selecting information for attentive processing, it is sometimes called \"top-down\" selection. In support of this model, it has been shown that a target stimulus can be located faster if it is preceded by the presentation of a similar, priming stimulus. For example, if an individual is shown the color green and then required to find a green circle among distractors, the initial exposure to the color will make it easier to find the green circle. This is because they are already thinking about and envisioning the color green, so when it shows up again as the green circle, their brain readily directs its attention towards it. This suggests that processing an initial stimulus speeds up a person\u2019s ability to select a similar target from pre-attentive processing. However, it could be that the speed of pre-attentive processing itself is not affected by the first stimulus, but rather that people are simply able to quickly abandon dissimilar stimuli, enabling them to re-engage to the correct target more quickly. This would mean that the difference in reaction time occurs at the attentive level, after pre-attentive processing and stimulus selection has already taken place.",
            "score": 107.9436264038086
        },
        {
            "docid": "30601657_12",
            "document": "Response priming . Response-priming effects have been demonstrated for a large number of stimuli and discrimination tasks, including geometric stimuli, color stimuli, various types of arrows, natural images (animals vs. objects), vowels and consonants, letters, and digits. In one study, chess configurations were presented as primes and targets, and participants had to decide whether the king was in check. Mattler (2003) could show that response priming can not only influence motor responses, but also works for cognitive operations like a spatial shift of visual attention or a shift between two different response time tasks. Different types of masking have been employed as well. Instead of measuring keypress responses (commonly with two response alternatives), some studies use more than two response alternatives or record speech responses, speeded finger pointing movements, eye movements, or so-called readiness potentials which reflect the degree of motor activation in the brain's motor cortex and can be measured by electro-encephalographic methods. Brain imaging methods like functional magnetic resonance imaging (fMRI) have been employed as well.",
            "score": 107.4157485961914
        },
        {
            "docid": "1316947_4",
            "document": "Ambiguous image . When we see an image, the first thing we do is attempt to organize all the parts of the scene into different groups. To do this, one of the most basic methods used is finding the edges. Edges can include obvious perceptions such as the edge of a house, and can include other perceptions that the brain needs to process deeper, such as the edges of a person's facial features. When finding edges, the brain's visual system detects a point on the image with a sharp contrast of lighting. Being able to detect the location of the edge of an object aids in recognizing the object. In ambiguous images, detecting edges still seems natural to the person perceiving the image. However, the brain undergoes deeper processing to resolve the ambiguity. For example, consider an image that involves an opposite change in magnitude of luminance between the object and the background (e.g. From the top, the background shifts from black to white, and the object shifts from white to black). The opposing gradients will eventually come to a point where there is an equal degree of luminance of the object and the background. At this point, there is no edge to be perceived. To counter this, the visual system connects the image as a whole rather than a set of edges, allowing one to see an object rather than edges and non-edges. Although there is no complete image to be seen, the brain is able to accomplish this because of its understanding of the physical world and real incidents of ambiguous lighting. In ambiguous images, an illusion is often produced from illusory contours. An illusory contour is a perceived contour without the presence of a physical gradient. In examples where a white shape appears to occlude black objects on a white background, the white shape appears to be brighter than the background, and the edges of this shape produce the illusory contours. These illusory contours are processed by the brain in a similar way as real contours. The visual system accomplishes this by making inferences beyond the information that is presented in much the same way as the luminance gradient.",
            "score": 106.64091491699219
        },
        {
            "docid": "2911654_5",
            "document": "Data conversion . For example, a true color image can easily be converted to grayscale, while the opposite conversion is a painstaking process. Converting a Unix text file to a Microsoft (DOS/Windows) text file involves adding characters, but this does not increase the entropy since it is rule-based; whereas the addition of color information to a grayscale image cannot be done programmatically, since only a human knows which colors are needed for each section of the picture\u2013there are no rules that can be used to automate that process. Converting a 24-bit PNG to a 48-bit one does not add information to it, it only pads existing RGB pixel values with zeroes, so that a pixel with a value of FF\u00a0C3\u00a056, for example, becomes FF00\u00a0C300\u00a05600. The conversion makes it possible to change a pixel to have a value of, for instance, FF80\u00a0C340\u00a056A0, but the conversion itself does not do that, only further manipulation of the image can. Converting an image or audio file in a lossy format (like JPEG or Vorbis) to a lossless (like PNG or FLAC) or uncompressed (like BMP or WAV) format only wastes space, since the same image with its loss of original information (the artifacts of lossy compression) becomes the target. A JPEG image can never be restored to the quality of the original image from which it was made, no matter how much the user tries the \"JPEG Artifact Removal\" feature of his or her image manipulation program.",
            "score": 106.40757751464844
        },
        {
            "docid": "25146378_15",
            "document": "Functional specialization (brain) . During the 1960s, Roger Sperry conducted a natural experiment on epileptic patients who had previously had their corpora callosa cut. The corpus callosum is the area of the brain dedicated to linking both the right and left hemisphere together. Sperry et al.'s experiment was based on flashing images in the right and left visual fields of his participants. Because the participant's corpus callosum was cut, the information processed by each visual field could not be transmitted to the other hemisphere. In one experiment, Sperry flashed images in the right visual field (RVF), which would subsequently be transmitted to the left hemisphere (LH) of the brain. When asked to repeat what they had previously seen, participants were fully capable of remembering the image flashed. However, when the participants were then asked to draw what they had seen, they were unable to. When Sperry et al. flashed images in the left visual field (LVF), the information processed would be sent to the right hemisphere (RH) of the brain. When asked to repeat what they had previously seen, participants were unable to recall the image flashed, but were very successful in drawing the image. Therefore, Sperry concluded that the left hemisphere of the brain was dedicated to language as the participants could clearly speak the image flashed. On the other hand, Sperry concluded that the right hemisphere of the brain was involved in more creative activities such as drawing.",
            "score": 104.8185043334961
        },
        {
            "docid": "11864935_3",
            "document": "Haar-like feature . Historically, working with only image intensities (i.e., the RGB pixel values at each and every pixel of image) made the task of feature calculation computationally expensive. A publication by Papageorgiou et al. discussed working with an alternate feature set based on Haar wavelets instead of the usual image intensities. Viola and Jones adapted the idea of using Haar wavelets and developed the so-called Haar-like features. A Haar-like feature considers adjacent rectangular regions at a specific location in a detection window, sums up the pixel intensities in each region and calculates the difference between these sums. This difference is then used to categorize subsections of an image. For example, let us say we have an image database with human faces. It is a common observation that among all faces the region of the eyes is darker than the region of the cheeks. Therefore a common Haar feature for face detection is a set of two adjacent rectangles that lie above the eye and the cheek region. The position of these rectangles is defined relative to a detection window that acts like a bounding box to the target object (the face in this case).",
            "score": 104.70010375976562
        },
        {
            "docid": "6258906_14",
            "document": "Numerical cognition . A study by Izard and colleagues investigated abstract number representations in infants using a different paradigm than the previous researchers because of the nature and developmental stage of the infants. For infants, they examined abstract number with both auditory and visual stimuli with a looking-time paradigm. The sets used were 4vs.12, 8vs.16, and 4vs.8. The auditory stimuli consisted of tones in different frequencies with a set number of tones, with some deviant trials where the tones were shorter but more numerous or longer and less numerous to account for duration and its potential confounds. After the auditory stimuli was presented with 2 minutes of familiarization, the visual stimuli was presented with a congruent or incongruent array of colorful dots with facial features. they remained on the screen until the infant looked away. They found that infants looked longer at the stimuli that matched the auditory tones, suggesting that the system for approximating non-symbolic number, even across modalities, is present in infancy. What is important to note across these three particular human studies on nonsymbolic numerosities is that it is present in infancy and develops over the lifetime. The honing of their approximation and number sense abilities as indicated by the improving Weber fractions across time, and usage of the left IPS to provide a wider berth for processing of computations and enumerations lend support for the claims that are made for a nonsymbolic number processing mechanism in human brains.",
            "score": 103.99835205078125
        },
        {
            "docid": "2613534_22",
            "document": "Visual extinction . Visual extinction has also been used to demonstrate brain bias towards gestalt processing. When presented with a figure containing illusory contours, patients were able to correctly report the presence of stimuli in both contralesional and ipsilesional hemispheres, due to their unconscious processing of the whole field to produce the illusion. This experiment implied that the attention center prioritizes the visualization of surfaces over other stimuli \u2013 therefore, although under race model the ipsilesional stimuli should extinguish the contralesional, the creation of the gestalt takes priority over detection of both. Further, a study using Gabor signals (alternating blurred and noisy black and white bars, commonly used by opticians in diagnostic tests) investigated how the orientation of these signals affected their extinction rate. Bilateral stimuli were least extinguished when both stimuli were oriented horizontally, although both stimuli being oriented vertically also showed a reduction in extinguishing rate when compared to one stimulus vertical and one horizontal \u2013 in what could be assumed by the brain to represent two different surfaces.",
            "score": 103.81460571289062
        },
        {
            "docid": "16526850_14",
            "document": "Trisynaptic circuit . The association cortex includes most of the cerebral surface of the brain and is responsible for processing that goes between the arrival of input in the primary sensory cortex and the generation of behaviour. Receives and integrates information from various parts of the brain and influences many cortical and subcortical targets. Inputs to the association cortices include the primary and secondary sensory and motor cortices, the thalamus, and the brain stem. The association cortex projects to places including the hippocampus, basal ganglia, and cerebellum, and other association cortices. Examination of patients with damages to one or more of these regions, as well as noninvasive brain imaging, it has been found that the association cortex is especially important for attending to complex stimuli in the external and internal environments. The temporal association cortex identifies the nature of stimuli, while the frontal association cortex plans behavioural responses to the stimuli.",
            "score": 103.22830200195312
        },
        {
            "docid": "11748426_3",
            "document": "Brainport . It has also been developed for use as a visual aid, demonstrating its ability to allow a blind person to see his or her surroundings in polygonal and pixel form. In this scenario, a camera picks up the image of the surrounding, the information is processed by a chip which converts it into impulses which are sent through an electrode array, via the tongue, to the person's brain. The human brain is able to interpret these impulses as visual signals and they are then redirected to the visual cortex, allowing the person to \"see.\" This is similar in part to how a cochlear implant works, in that it transmits electrical stimuli to a receiving device in the body.",
            "score": 102.75778198242188
        },
        {
            "docid": "943426_9",
            "document": "Core Image . All pixel processing provided by an Image Unit is performed in a pre-multiplied alpha (RGBA) color space, storing four color channels: red, green, blue, and transparency (alpha). Each color channel is represented by a 32-bit, floating point number. This provides exceptional color depth, far greater than can be perceived by the human eye, as each pixel is represented by a 128-bit vector (four 32-bit color channels). For color spaces of lower bit-depth, the floating-point calculation model employed by Core Image provides exceptional performance, which is useful when processing multiple images or video frames.",
            "score": 102.57258605957031
        },
        {
            "docid": "18667798_11",
            "document": "Pixel artist . The total number of pixels (\"image resolution\"), and the amount of information in each pixel (often called \"color depth\") determine the quality of an image. For example, an image that stores 24 bits of color-information per pixel (the standard for computer displays since around 1995) can represent smoother degrees of shading than one that only stores 16 bits per pixel, but not as smooth as one that stores 48 bits. Likewise, an image sampled at 640 x 480 pixels (and therefore containing 307,200 pixels) will look rough and blocky compared to one sampled at 1280 x 1024 (1,310,720 pixels). Because it takes a large amount of data to store a high-quality image, computer software often uses data compression techniques to reduce this size for images stored on disk. Some techniques sacrifice information, and therefore image quality, in order to achieve a smaller file-size. Computer scientists refer to compression techniques that lose information as lossy compression.",
            "score": 102.49227142333984
        },
        {
            "docid": "55780567_4",
            "document": "Snake detection theory . Many empirical studies have found evidences for the theory. Primates, including humans, are able to quickly detect snakes. Some studies have found that humans can detect snake images before subjective visual perception. However, the pre-conscious detection of snake stimuli is still under debate by the scientific community. Snakes images were proved to be detected more rapidly compared to other fear-relevant stimuli: empirical evidences have shown that snakes are more rapidly detected compared to spiders - according to the Snake Detection Theory - because the arachnids were, historically, a less relevant threat to primates. Snake stimuli are particularly distracting during perceptual tasks, suggesting that the brain preferentially processes snake stimuli, even when attentional processes are demanded by other targets. Snake enhanced detection was found also in young children.",
            "score": 102.04376220703125
        },
        {
            "docid": "51462681_4",
            "document": "Objective vision . Objective-Vision is a Human (Natural) Visionary simulation Project developed by Michael Bidollahkhany. Following an explosion of interest during the 1970s the 1980s and 1990s were characterized by the maturing of the field and the significant growth of active applications; remote sensing, technical diagnostics, autonomous vehicle guidance, medical imaging (2D and 3D) and automatic surveillance are the most rapidly developing areas. This progress can be seen in an increasing number of software and hardware products on the market, as well as in a number of digital image processing and machine vision courses offered at universities worldwide. Therefor OVC started in the first step of the project's development. One of important parts of this project is O.V.C.(Objective Vision Class library), that able companies and scientists to use the brain's most likely visionary libraries and image processing algorithms in their projects and develop them under MIT copy-right license. The class library and Developer's kit of project is used for researches based on natural visionary system, and image processing, optimization and description with most upgraded and near Techniques. For example, taking a picture of a jungle, or taking a picture of somewhere, with this library developer will be able to manipulate not only the pixel of images for mining data, but automatically based on which algorithm he is using and image quality, he can manipulate directly a list of objects, same pixels and every data project needs to have.",
            "score": 102.00696563720703
        },
        {
            "docid": "33702464_5",
            "document": "Extrastriate body area . The experiment had subjects view images of different objects, including faces (as a control group), body parts, animals, parts of the face and intimate objects. While viewing the images, the subjects were scanned with an fMRI to see what area of the brain was activated. Through the trials a compilation of the fMRI\u2019s was made. From this compilation image a specific region was determined to have increased activity when shown visual stimuli of body parts and even more activity when viewing whole bodies. There have been no studies involving brain damage to the EBA. Thus far, only scans of brain activity, as well as transcranial magnetic stimulation, have been used to study the EBA. To find the specific functions of the EBA, Comimo Urgesi, Giovanni Berlucchi and Salvatore M. Aglioti used repetitive transcranial magnetic stimulation (rTMS) to disrupt part of the brain, making the brain less responsive in the target area. The study used event-related rTMS to disrupt the EBA, resulting in inactivation of cortical areas. This inactivation caused a slower response time in discriminating body parts. The study used facial features and motorcycle parts as non human parts for control groups. The facial features and motorcycle body parts did not display any change in response time. The neural activity data shows the EBA handles some of the visual processing of human body and parts but is not related to the processing of the face or other objects.",
            "score": 101.95645904541016
        },
        {
            "docid": "3193455_6",
            "document": "Feature (computer vision) . When feature extraction is done without local decision making, the result is often referred to as a \"feature image\". Consequently, a feature image can be seen as an image in the sense that it is a function of the same spatial (or temporal) variables as the original image, but where the pixel values hold information about image features instead of intensity or color. This means that a feature image can be processed in a similar way as an ordinary image generated by an image sensor. Feature images are also often computed as integrated step in algorithms for feature detection.",
            "score": 101.84944152832031
        },
        {
            "docid": "644662_27",
            "document": "Pixel density . The following table show how pixel density is supported by often used image file formats. In the second column, length refers to horizontal and vertical size in inches, centimeters et cetera, whereas pixel refers only to the number of pixels found along the horizontal and vertical dimension. The cell colors used do not indicate how feature-rich a certain image file format is, but what density support can be expected of a certain image file format. Often-used image file formats that do not support pixel density are added for counter-example purposes.",
            "score": 101.84848022460938
        },
        {
            "docid": "14456808_5",
            "document": "Eriksen flanker task . Other variants of the Eriksen Flanker Task have used numbers, color patches, or arrows as stimuli. Also, although most Eriksen Flanker Tasks show the flankers on the left and right of the target, they can also be placed above or below the target, or in other spatial orientations.These examples all use an arbitrary mapping between the stimulus and the response. Another possibility is to use a natural mapping, with arrows as stimuli. For example, Kopp et al. (1994) used left and right arrows, with flanker stimuli above and below the target. The flankers could be arrows pointing in the same direction as the target (congruent) the opposite direction (incongruent) or squares (neutral). More commonly, flankers have been arranged in a horizontal array, as with letter stimuli, so \u00ab\u00ab< would be a congruent stimulus, \u00ab>\u00ab an incongruent stimulus.",
            "score": 101.81553649902344
        },
        {
            "docid": "3717_59",
            "document": "Brain . Neurophysiologists study the chemical, pharmacological, and electrical properties of the brain: their primary tools are drugs and recording devices. Thousands of experimentally developed drugs affect the nervous system, some in highly specific ways. Recordings of brain activity can be made using electrodes, either glued to the scalp as in EEG studies, or implanted inside the brains of animals for extracellular recordings, which can detect action potentials generated by individual neurons. Because the brain does not contain pain receptors, it is possible using these techniques to record brain activity from animals that are awake and behaving without causing distress. The same techniques have occasionally been used to study brain activity in human patients suffering from intractable epilepsy, in cases where there was a medical necessity to implant electrodes to localize the brain area responsible for epileptic seizures. Functional imaging techniques such as functional magnetic resonance imaging are also used to study brain activity; these techniques have mainly been used with human subjects, because they require a conscious subject to remain motionless for long periods of time, but they have the great advantage of being noninvasive. Another approach to brain function is to examine the consequences of damage to specific brain areas. Even though it is protected by the skull and meninges, surrounded by cerebrospinal fluid, and isolated from the bloodstream by the blood\u2013brain barrier, the delicate nature of the brain makes it vulnerable to numerous diseases and several types of damage. In humans, the effects of strokes and other types of brain damage have been a key source of information about brain function. Because there is no ability to experimentally control the nature of the damage, however, this information is often difficult to interpret. In animal studies, most commonly involving rats, it is possible to use electrodes or locally injected chemicals to produce precise patterns of damage and then examine the consequences for behavior.",
            "score": 101.65836334228516
        }
    ]
}