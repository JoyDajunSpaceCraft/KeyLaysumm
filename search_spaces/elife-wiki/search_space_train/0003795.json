{
    "q": [
        {
            "docid": "21244265_21",
            "document": "Olfaction . Although conventional wisdom and lay literature, based on impressionistic findings in the 1920s, have long presented human olfaction as capable of distinguishing between roughly 10,000 unique odors, recent research has suggested that the average individual is capable of distinguishing over one trillion unique odors. Researchers in the most recent study, which tested the psychophysical responses to combinations of over 128 unique odor molecules with combinations composed of up to 30 different component molecules, noted that this estimate is \"conservative\" and that some subjects of their research might be capable of deciphering between a thousand trillion odorants, adding that their worst performer could probably still distinguish between 80million scents. Authors of the study concluded, \"This is far more than previous estimates of distinguishable olfactory stimuli. It demonstrates that the human olfactory system, with its hundreds of different olfactory receptors, far out performs the other senses in the number of physically different stimuli it can discriminate.\" However, it was also noted by the authors that the ability to distinguish between smells is not analogous to being able to consistently identify them, and that subjects were not typically capable of identifying individual odor stimulants from within the odors the researchers had prepared from multiple odor molecules. In November 2014 the study was strongly criticized by Caltech scientist Markus Meister, who wrote that the study's \"extravagant claims are based on errors of mathematical logic\". The logic of his paper has in turn been criticized by the authors of the original paper.",
            "score": 275.54229760169983
        },
        {
            "docid": "36687154_4",
            "document": "Estimation . In making an estimate, the goal is often most useful to generate a range of possible outcomes that is precise enough to be useful, but not so precise that it is likely to be inaccurate. For example, in trying to guess the number of candies in the jar, if fifty were visible, and the total volume of the jar seemed to be about twenty times as large as the volume containing the visible candies, then one might simply project that there were a thousand candies in the jar. Such a projection, intended to pick the single value that is believed to be closest to the actual value, is called a point estimate. However, a point estimation is likely to be incorrect, because the sample size\u2014in this case, the number of candies that are visible\u2014is too small a number to be sure that it does not contain anomalies that differ from the population as a whole. A corresponding concept is an interval estimate, which captures a much larger range of possibilities, but is too broad to be useful. For example, if one were asked to estimate the percentage of people who like candy, it would clearly be correct that the number falls between zero and one hundred percent. Such an estimate would provide no guidance, however, to somebody who is trying to determine how many candies to buy for a party to be attended by a hundred people.",
            "score": 83.70903384685516
        },
        {
            "docid": "30142141_10",
            "document": "Adaptive evolution in the human genome . Many different studies have attempted to quantify the amount of adaptive evolution in the human genome, the vast majority using the comparative approaches outlined above. Although there are discrepancies between studies, generally there is relatively little evidence of adaptive evolution in protein coding DNA, with estimates of adaptive evolution often near 0% (see Table 1). The most obvious exception to this is the 35% estimate of \u03b1 (Fay et al. 2001). This comparatively early study used relatively few loci (fewer than 200) for their estimate, and the polymorphism and divergence data used was obtained from different genes, both of which may have led to an overestimate of \u03b1. The next highest estimate is the 20% value of \u03b1 (Zhang and Li 2005). However, the MK test used in this study was sufficiently weak that the authors state that this value of \u03b1 is not statistically significantly different from 0%. Nielsen et al. (2005a)\u2019s estimate that 9.8% of genes have undergone adaptive evolution also has a large margin of error associated with it, and their estimate shrinks dramatically to 0.4% when they stipulate that the degree of certainty that there has been adaptive evolution must be 95% or more.  This raises an important issue, which is that many of these tests for adaptive evolution are very weak. Therefore the fact that many estimates are at (or very near to) 0% does not rule out the occurrence of any adaptive evolution in the human genome, but simply shows that positive selection is not frequent enough to be detected by the tests. In fact, the most recent study I mention states that confounding variables, such as demographic changes, means that the true value of \u03b1 may be as high as 40% (Eyre-Walker and Keightley 2009). Another recent study, which uses a relatively robust methodology, estimates \u03b1 at 10-20% Boyko et al. (2008). I will comment on weaknesses in the methods in a subsequent section, but it is clear that the debate over the amount of adaptive evolution occurring in human coding DNA is not yet resolved. Even if low estimates of \u03b1 are accurate, a small proportion of substitutions evolving adaptively can still equate to a considerable amount of coding DNA. Many authors, whose studies have small estimates of the amount of adaptive evolution in coding DNA, nevertheless accept that there has been some adaptive evolution in this DNA, because these studies identify specific regions within the human genome which have been evolving adaptively (e.g. Bakewell et al. (2007)). More genes underwent positive selection in chimpanzee evolution than in human), something I will examine later. The generally low estimates of adaptive evolution in human coding DNA can be contrasted with other species. Bakewell et al. (2007) found more evidence of adaptive evolution in chimpanzees than humans, with 1.7% of chimpanzee genes showing evidence of adaptive evolution (compared with the 1.1% estimate for humans; see Table 1). Comparing humans with more distantly related animals, an early estimate for \u03b1 in Drosophila species was 45% (Smith and Eyre-Walker 2002), and later estimates largely agree with this (Eyre-Walker 2006). Bacteria and viruses generally show even more evidence of adaptive evolution; research shows values of \u03b1 in a range of 50-85%, depending on the species examined (Eyre-Walker 2006). Generally, there does appear to be a positive correlation between (effective) population size of the species, and amount of adaptive evolution occurring in the coding DNA regions. This may be because random genetic drift becomes less powerful at altering allele frequencies, compared to natural selection, as population size increases.",
            "score": 104.54269242286682
        },
        {
            "docid": "2161615_30",
            "document": "Rasch model . Verhelst & Glas (1995) derive Conditional Maximum Likelihood (CML) equations for a model they refer to as the One Parameter Logistic Model (OPLM). In algebraic form it appears to be identical with the 2PL model, but OPLM contains preset discrimination indexes rather than 2PL's estimated discrimination parameters. As noted by these authors, though, the problem one faces in estimation with estimated discrimination parameters is that the discriminations are unknown, meaning that the weighted raw score \"is not a mere statistic, and hence it is impossible to use CML as an estimation method\" (Verhelst & Glas, 1995, p.\u00a0217). That is, sufficiency of the weighted \"score\" in the 2PL cannot be used according to the way in which a sufficient statistic is defined. If the weights are imputed instead of being estimated, as in OPLM, conditional estimation is possible and some of the properties of the Rasch model are retained (Verhelst, Glas & Verstralen, 1995; Verhelst & Glas, 1995). In OPLM, the values of the discrimination index are restricted to between 1 and 15. A limitation of this approach is that in practice, values of discrimination indexes must be preset as a starting point. This means some type of estimation of discrimination is involved when the purpose is to avoid doing so.",
            "score": 72.8816545009613
        },
        {
            "docid": "40977477_17",
            "document": "Cross-species transmission . Two methods of measuring genetic variation, variable number tandem repeats (VNTRs) and single nucleotide polymorphisms (SNPs), have been very beneficial to the study of bacterial transmission. VNTRs, due to the low cost and high mutation rates, make them particularly useful to detect genetic differences in recent outbreaks, and while SNPs have a lower mutation rate per locus than VNTRs, they deliver more stable and reliable genetic relationships between isolates. Both methods are used to construct phylogenies for genetic analysis, however, SNPs are more suitable for studies on phylogenies contraction. However, it can be difficult for these methods accurately simulate CSTs everts. Estimates of CST based on phylogenys made using VNTR marker can be biased towards detecting CST events across a wide range of the parameters. SNPs tend to be less biased and variable in estimates of CST when estimations of CST rates are low and low number of SNPs is used. In general, CST rate estimates using these methods are most reliable in systems with more mutations, more markers, and high genetic differences between introduced strains. CST is very complex and models need to account for a lot of parameters to accurately represent the phenomena. Models that oversimplify reality can result in biased data. Multiple parameters such as number of mutations accumulated since introduction, stochasticity, the genetic difference of strains introduced, and the sampling effort can make unbiased estimates of CST difficult even with whole-genome sequences, especially if sampling is limited, mutation rates are low, or if pathogens were recently introduced. More information on the factors that influence CST rates is needed for the contraction of more appropriate models to study these events.",
            "score": 109.97149133682251
        },
        {
            "docid": "253568_17",
            "document": "Implied volatility . Another way to look at implied volatility is to think of it as a price, not as a measure of future stock moves. In this view, it simply is a more convenient way to communicate option prices than currency. Prices are different in nature from statistical quantities: one can estimate volatility of future underlying returns using any of a large number of estimation methods; however, the number one gets is not a price. A price requires two counterparties, a buyer, and a seller. Prices are determined by supply and demand. Statistical estimates depend on the time-series and the mathematical structure of the model used.  It is a mistake to confuse a price, which implies a transaction, with the result of a statistical estimation, which is merely what comes out of a calculation. Implied volatilities are prices: they have been derived from actual transactions. Seen in this light, it should not be surprising that implied volatilities might not conform to what a particular statistical model would predict.",
            "score": 89.5581681728363
        },
        {
            "docid": "62329_18",
            "document": "Meta-analysis . Other weaknesses are that it has not been determined if the statistically most accurate method for combining results is the fixed, IVhet, random or quality effect models, though the criticism against the random effects model is mounting because of the perception that the new random effects (used in meta-analysis) are essentially formal devices to facilitate smoothing or shrinkage and prediction may be impossible or ill-advised. The main problem with the random effects approach is that it uses the classic statistical thought of generating a \"compromise estimator\" that makes the weights close to the naturally weighted estimator if heterogeneity across studies is large but close to the inverse variance weighted estimator if the between study heterogeneity is small. However, what has been ignored is the distinction between the model \"we choose\" to analyze a given dataset, and the \"mechanism by which the data came into being\". A random effect can be present in either of these roles, but the two roles are quite distinct. There's no reason to think the analysis model and data-generation mechanism (model) are similar in form, but many sub-fields of statistics have developed the habit of assuming, for theory and simulations, that the data-generation mechanism (model) is identical to the analysis model we choose (or would like others to choose). As a hypothesized mechanisms for producing the data, the random effect model for meta-analysis is silly and it is more appropriate to think of this model as a superficial description and something we choose as an analytical tool \u2013 but this choice for meta-analysis may not work because the study effects are a fixed feature of the respective meta-analysis and the probability distribution is only a descriptive tool.",
            "score": 89.304514169693
        },
        {
            "docid": "306759_8",
            "document": "Gary Kleck . Kleck asserts errors in his critics' claims that his survey's estimates of defensive gun uses linked with specific crime types, or that involved a wounding of the offender, are implausibly large compared to estimates of the total numbers of such crimes. The total number of nonfatal gunshot woundings, whether medically treated or not, is unknown, and no meaningful estimates can be derived from his survey regarding defensive gun uses linked with specific crime types, or that involved wounding the offender, because the sample sizes are too small. The fact that some crime-specific estimates derived from the Kleck survey are implausibly large is at least partly a reflection of the small samples on which they are based - no more than 196 cases. Kleck states that his estimate of total defensive gun uses was based on nearly 5,000 cases. Thus, he argues, the implausible character of some estimates of small \"subsets\" of defensive gun uses is not a valid criticism of whether estimates of the \"total\" number of defensive gun uses are implausible or too high.",
            "score": 72.8656724691391
        },
        {
            "docid": "17765521_7",
            "document": "Software development effort estimation . Most of the research has focused on the construction of formal software effort estimation models. The early models were typically based on regression analysis or mathematically derived from theories from other domains. Since then a high number of model building approaches have been evaluated, such as approaches founded on case-based reasoning, classification and regression trees, simulation, neural networks, Bayesian statistics, lexical analysis of requirement specifications, genetic programming, linear programming, economic production models, soft computing, fuzzy logic modeling, statistical bootstrapping, and combinations of two or more of these models. The perhaps most common estimation methods today are the parametric estimation models COCOMO, SEER-SEM and SLIM. They have their basis in estimation research conducted in the 1970s and 1980s and are since then updated with new calibration data, with the last major release being COCOMO II in the year 2000. The estimation approaches based on functionality-based size measures, e.g., function points, is also based on research conducted in the 1970s and 1980s, but are re-calibrated with modified size measures and different counting approaches, such as the use case points or object points in the 1990s and COSMIC in the 2000s.",
            "score": 57.65065562725067
        },
        {
            "docid": "33802950_12",
            "document": "Thurstonian model . Thurstonian models have been applied to a range of sensory discrimination tasks, including auditory, taste, and olfactory discrimination, to estimate sensory distance between stimuli that range along some sensory continuum.",
            "score": 61.27030348777771
        },
        {
            "docid": "37968451_10",
            "document": "Defensive gun use . Kleck asserts errors in his critics' statements that his survey's estimates of defensive gun uses linked with specific crime types, or that involved a wounding of the offender, are implausibly large compared to estimates of the total numbers of such crimes. The total number of nonfatal gunshot woundings, whether medically treated or not, is unknown, and no meaningful estimates can be derived from his survey regarding defensive gun uses linked with specific crime types, or that involved wounding the offender, because the sample sizes are too small. The fact that some crime-specific estimates derived from the Kleck survey are implausibly large is at least partly a reflection of the small samples on which they are based - no more than 196 cases. Kleck states that his estimate of total defensive gun uses was based on nearly 5,000 cases. Thus, he argues, the implausible character of some estimates of small \"subsets\" of defensive gun uses is not a valid criticism of whether estimates of the \"total\" number of defensive gun uses are implausible or too high.",
            "score": 69.66718327999115
        },
        {
            "docid": "22309853_3",
            "document": "Error correction model . Yule (1936) and Granger and Newbold (1974) were the first to draw attention to the problem of spurious correlation and find solutions on how to address it in time series analysis. Given two completely unrelated but integrated (non-stationary) time series, the regression analysis of one on the other will tend to produce an apparently statistically significant relationship and thus a researcher might falsely believe to have found evidence of a true relationship between these variables. Ordinary least squares will no longer be consistent and commonly used test-statistics will be non-valid. In particular, Monte Carlo simulations show that one will get a very high R squared, very high individual t-statistic and a low Durbin\u2013Watson statistic. Technically speaking, Phillips (1986) proved that parameter estimates will not converge in probability, the intercept will diverge and the slope will have a non-degenerate distribution as the sample size increases. However, there might be a common stochastic trend to both series that a researcher is genuinely interested in because it reflects a long-run relationship between these variables.  Because of the stochastic nature of the trend it is not possible to break up integrated series into a deterministic (predictable) trend and a stationary series containing deviations from trend. Even in deterministically detrended random walks walks spurious correlations will eventually emerge. Thus detrending doesn't solve the estimation problem.  In order to still use the Box\u2013Jenkins approach, one could difference the series and then estimate models such as ARIMA, given that many commonly used time series (e.g. in economics) appear to be stationary in first differences. Forecasts from such a model will still reflect cycles and seasonality that are present in the data. However, any information about long-run adjustments that the data in levels may contain is omitted and longer term forecasts will be unreliable.  This lead Sargan (1964) to develop the ECM methodology, which retains the level information.",
            "score": 89.94849908351898
        },
        {
            "docid": "2889768_11",
            "document": "Image stitching . To estimate a robust model from the data, a common method used is known as RANSAC.<br> The name RANSAC is an abbreviation for \"RANdom SAmple Consensus\". It is an iterative method for robust parameter estimation to fit mathematical models from sets of observed data points which may contain outliers. The algorithm is non-deterministic in the sense that it produces a reasonable result only with a certain probability, with this probability increasing as more iterations are performed. It being a probabilistic method means that different results will be obtained for every time the algorithm is run.  The RANSAC algorithm has found many applications in computer vision, including the simultaneous solving of the correspondence problem and the estimation of the fundamental matrix related to a pair of stereo cameras. The basic assumption of the method is that the data consists of \"inliers\", i.e., data whose distribution can be explained by some mathematical model, and \"outliers\" which are data that do not fit the model. Outliers are considered points which come from noise, erroneous measurements, or simply incorrect data. For the problem of homography estimation, RANSAC works by trying to fit several models using some of the point pairs and then checking if the models were able to relate most of the points. The best model, i.e., the homography which produces the highest number of correct matches, is then chosen as the answer for the problem thus if the ratio of number of outliers to data points is very low the RANSAC outputs a decent model fitting the data.",
            "score": 88.7247462272644
        },
        {
            "docid": "22818988_12",
            "document": "Missing women . As a result of this disparity between countries, American demographer Coale re-estimated the Sen's original numbers of missing women using a different methodology. Using data from his Regional Model Life Tables, Coale found that the natural male-to-female sex ratio, accounting for different country fertility rates and circumstances, had an expected value of 1.059. Using the number, he then arrived at an estimate of 60 million missing women, much lower than Sen's original estimate. However, a few years later, Klasen re-calculated the count of missing women using Coale's methodology with updated data. He found 69.3 million missing women, which was higher than Coale's original estimate. He also noted a problem with the Regional Model Life Tables; they were based on countries with higher female mortality, which would bias Coale's numbers of missing women downwards. Furthermore, Klasen and Wink noted that both Sen's and Coale's methodologies were flawed because Sen and Coale assume that optimal sex ratios are constant across time and space, which they are often not.",
            "score": 85.83836436271667
        },
        {
            "docid": "4152503_13",
            "document": "Credibility theory . Actuarial credibility describes an approach used by actuaries to improve statistical estimates. Although the approach can be formulated in either a frequentist or Bayesian statistical setting, the latter is often preferred because of the ease of recognizing more than one source of randomness through both \"sampling\" and \"prior\" information. In a typical application, the actuary has an estimate X based on a small set of data, and an estimate M based on a larger but less relevant set of data. The credibility estimate is ZX + (1-Z)M, where Z is a number between 0 and 1 (called the \"credibility weight\" or \"credibility factor\") calculated to balance the sampling error of X against the possible lack of relevance (and therefore modeling error) of M.",
            "score": 85.86210942268372
        },
        {
            "docid": "416612_5",
            "document": "Cross-validation (statistics) . Suppose we have a model with one or more unknown parameters, and a data set to which the model can be fit (the training data set). The fitting process optimizes the model parameters to make the model fit the training data as well as possible. If we then take an independent sample of validation data from the same population as the training data, it will generally turn out that the model does not fit the validation data as well as it fits the training data. The size of this difference is likely to be large especially when the size of the training data set is small, or when the number of parameters in the model is large. Cross-validation is a way to estimate the size of this effect.",
            "score": 97.65729117393494
        },
        {
            "docid": "33358913_23",
            "document": "Five-planet Nice model . The bombardment produced by the Nice model may not match the Late Heavy Bombardment. An impactor size distribution similar to the asteroids would result in too many large impact basins relative to smaller craters. The innermost asteroid belt would need a different size distribution, perhaps due to its small asteroids being the result of collisions between a small number of large asteroids, to match this constraint. While the Nice model predicts a bombardment by both asteroids and comets, most evidence (although not all) points toward a bombardment dominated by asteroids. This may reflect the reduced cometary bombardment in the five-planet Nice model and the significant mass loss or the break-up of comets after entering the inner Solar System, potentially allowing the evidence of cometary bombardment to have been lost. However, two recent estimates of the asteroid bombardment find it is also insufficient to explain the Late Heavy Bombardment. Reproducing the lunar craters and impact basins identified with the Late Heavy Bombardment, about 1/6 of the craters larger than 150\u00a0km in diameter, and the craters on Mars may be possible if a different crater-scaling law is used. The remaining lunar craters would then be the result of another population of impactors with a different size distribution, possibly planetesimals left over from the formation of the planets. This crater-scaling law also is more successful at reproducing the more recently formed large craters.",
            "score": 71.58679032325745
        },
        {
            "docid": "1434444_61",
            "document": "Autoregressive model . The question of how to interpret the measured forecasting accuracy arises\u2014for example, what is a \"high\" (bad) or a \"low\" (good) value for the mean squared prediction error? There are two possible points of comparison. First, the forecasting accuracy of an alternative model, estimated under different modeling assumptions or different estimation techniques, can be used for comparison purposes. Second, the out-of-sample accuracy measure can be compared to the same measure computed for the in-sample data points (that were used for parameter estimation) for which enough prior data values are available (that is, dropping the first \"p\" data points, for which \"p\" prior data points are not available). Since the model was estimated specifically to fit the in-sample points as well as possible, it will usually be the case that the out-of-sample predictive performance will be poorer than the in-sample predictive performance. But if the predictive quality deteriorates out-of-sample by \"not very much\" (which is not precisely definable), then the forecaster may be satisfied with the performance.",
            "score": 86.47112345695496
        },
        {
            "docid": "4504135_12",
            "document": "Generalized least squares . In general this estimator has different properties than GLS. For large samples (i.e., asymptotically) all properties are (under appropriate conditions) common with respect to GLS, but for finite samples the properties of FGLS estimators are unknown: they vary dramatically with each particular model, and as a general rule their exact distributions cannot be derived analytically. For finite samples, FGLS may be even less efficient than OLS in some cases. Thus, while GLS can be made feasible, it is not always wise to apply this method when the sample is small. A method sometimes used to improve the accuracy of the estimators in finite samples is to iterate, i.e. taking the residuals from FGLS to update the errors covariance estimator, and then updating the FGLS estimation, applying the same idea iteratively until the estimators vary less than some tolerance. But this method does not necessarily improve the efficiency of the estimator very much if the original sample was small. A reasonable option when samples are not too large is to apply OLS, but throwing away the classical variance estimator (which is inconsistent in this framework) and using a HAC (Heteroskedasticity and Autocorrelation Consistent) estimator. For example, in autocorrelation context we can use the Bartlett estimator (often known as Newey-West estimator since these authors popularized the use of this estimator among econometricians in their 1987 Econometrica article), and in heteroskedastic context we can use the Eicker\u2013White estimator (Eicker\u2013White). This approach is much safer, and it is the appropriate path to take unless the sample is large, and \"large\" is sometimes a slippery issue (e.g. if the errors distribution is asymmetric the required sample would be much larger).",
            "score": 101.34889900684357
        },
        {
            "docid": "2885691_75",
            "document": "Robust statistics . However, using these types of models to predict missing values or outliers in a long time series is difficult and often unreliable, particularly if the number of values to be in-filled is relatively high in comparison with total record length. The accuracy of the estimate depends on how good and representative the model is and how long the period of missing values extends. The in a case of a dynamic process, so any variable is dependent, not just on the historical time series of the same variable but also on several other variables or parameters of the process. In other words, the problem is an exercise in multivariate analysis rather than the univariate approach of most of the traditional methods of estimating missing values and outliers; a multivariate model will therefore be more representative than a univariate one for predicting missing values. The Kohonen self organising map (KSOM) offers a simple and robust multivariate model for data analysis, thus providing good possibilities to estimate missing values, taking into account its relationship or correlation with other pertinent variables in the data record.",
            "score": 96.2338856458664
        },
        {
            "docid": "39761773_9",
            "document": "Central nervous system effects from radiation exposure during spaceflight . At this time, the possible detrimental effects to an astronaut\u2019s CNS from the HZE component of GCR have yet to be identified. This is largely due to the lack of a human epidemiological basis with which to estimate risks and the relatively small number of published experimental studies with animals. RBE factors are combined with human data to estimate cancer risks for low-LET radiation exposure. Since this approach is not possible for CNS risks, new approaches to risk estimation will be needed. Thus, biological research is required to establish risk levels and risk projection models and, if the risk levels are found to be significant, to design countermeasures.",
            "score": 69.21011567115784
        },
        {
            "docid": "62329_42",
            "document": "Meta-analysis . Modern statistical meta-analysis does more than just combine the effect sizes of a set of studies using a weighted average. It can test if the outcomes of studies show more variation than the variation that is expected because of the sampling of different numbers of research participants. Additionally, study characteristics such as measurement instrument used, population sampled, or aspects of the studies' design can be coded and used to reduce variance of the estimator (see statistical models above). Thus some methodological weaknesses in studies can be corrected statistically. Other uses of meta-analytic methods include the development of clinical prediction models, where meta-analysis may be used to combine data from different research centers, or even to aggregate existing prediction models.",
            "score": 131.32981538772583
        },
        {
            "docid": "3705381_62",
            "document": "Tehran Stock Exchange . There are differing estimates of the total capital held by Iranian expatriates. One estimate places the number at $1.3 trillion US dollars. Whatever the actual number, it is clear that these funds are sufficiently large enough to buy significant stakes in all state companies. In Dubai alone, Iranian expatriates are estimated to have invested up to $200 billion. Even a 10 percent repatriation of capital would have a significant impact. In 2000, the Iran Press Service reported that Iranian expatriates had invested between $200 and $400 billion in the United States, Europe, and China, but almost nothing in Iran. FIPPA provisions apply to all foreign investors, and many Iranian expatriates based in the US continue to make substantial investments in Iran.",
            "score": 79.99423336982727
        },
        {
            "docid": "639660_7",
            "document": "Olfactory receptor neuron . A widely publicized study suggested that humans can detect more than one trillion different odors. This finding has however been disputed. Critics argued that the methodology used for the estimation was fundamentally flawed, showing that applying the same argument for better-understood sensory modalities, such as vision or audition, leads to wrong conclusions. Other researchers have also showed that the result is extremely sensitive to the precise details of the calculation, with small variations changing the result over dozens of orders of magnitude, possibly going as low as a few thousand. The authors of the original study have argued that their estimate holds as long as it is assumed that odor space is sufficiently high-dimensional.",
            "score": 142.75861620903015
        },
        {
            "docid": "1761545_40",
            "document": "Minimum mean square error . In many real-time application, observational data is not available in a single batch. Instead the observations are made in a sequence. A naive application of previous formulas would have us discard an old estimate and recompute a new estimate as fresh data is made available. But then we lose all information provided by the old observation. When the observations are scalar quantities, one possible way of avoiding such re-computation is to first concatenate the entire sequence of observations and then apply the standard estimation formula as done in Example 2. But this can be very tedious because as the number of observation increases so does the size of the matrices that need to be inverted and multiplied grow. Also, this method is difficult to extend to the case of vector observations. Another approach to estimation from sequential observations is to simply update an old estimate as additional data becomes available, leading to finer estimates. Thus a recursive method is desired where the new measurements can modify the old estimates. Implicit in these discussions is the assumption that the statistical properties of formula_1 does not change with time. In other words, formula_1 is stationary.",
            "score": 80.56706809997559
        },
        {
            "docid": "367978_75",
            "document": "Panther tank . Shortly before D-Day, Allied intelligence reported that large numbers of Panthers were being used in the panzer divisions, and an attempt was made to investigate Panther production. Using a statistical analysis of the serial numbers on the road wheels on two captured tanks, U.S. intelligence estimated Panther production for February 1944 to be 270 units, much greater than what had been anticipated. This estimate was very accurate, especially compared to previous methods, as German records after the war showed production of Panthers for the month of February 1944 was 276. This indicated that the Panther would be encountered in much larger numbers than had previously been thought. In the planning for the Battle of Normandy, the U.S. Army expected to face a handful of German heavy tanks alongside large numbers of Panzer IVs. At this point, it was too late to prepare to face the Panther. As it turned out, 38% of the German tanks in Normandy were Panthers, whose frontal armour could not be penetrated by the 75\u00a0mm guns of the US M4 Sherman.",
            "score": 75.76957273483276
        },
        {
            "docid": "416612_2",
            "document": "Cross-validation (statistics) . Cross-validation, sometimes called rotation estimation, or out-of-sample testing is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice. In a prediction problem, a model is usually given a dataset of \"known data\" on which training is run (\"training dataset\"), and a dataset of \"unknown data\" (or \"first seen\" data) against which the model is tested (called the validation dataset or \"testing set\"). The goal of cross-validation is to test the model\u2019s ability to predict new data that were not used in estimating it, in order to flag problems like overfitting and to give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem).",
            "score": 57.055928230285645
        },
        {
            "docid": "23924_25",
            "document": "Passenger pigeon . For a 2017 genetic study, the authors sequenced the complete genomes of two passenger pigeons, as well as analyzing the mitochondrial DNA of 41 individuals. This study (based on a Bayesian coalescent analysis) found that the passenger-pigeon population had been stable for the previous 20,000 years. The study also found that the size of the passenger pigeon population over that time period had been much larger than the 2014 genetic study had found. However, the 2017 study\u2019s \u201cconservative\u201d estimate of an \u201ceffective population size\u201d of 13 million birds is still only about 1/300th of the bird\u2019s estimated historic population of approximately 3-5 billion before their \u201c19th century decline and eventual extinction.\u201d A similar study inferring human population size from genetics (published in 2008, and using human mitochondrial DNA and Bayesian coalescent inference methods) showed considerable accuracy in reflecting overall patterns of human population growth as compared to data deduced by other means \u2014 though the study arrived at a human effective population size (as of 1600 AD, for Africa, Eurasia, and the Americas combined) that was roughly 1/1000th of the census population estimate for the same time and area based on anthropological and historical evidence. The 2017 passenger-pigeon genetic study also found that, in spite of the large population size, the genetic diversity was very low in the species, which was beneficial in allowing for faster evolution and eliminating harmful mutations, but also made it more vulnerable to human pressure. The study concluded that earlier suggestions that population instability contributed to the extinction of the species were therefore invalid. Evolutionary biologist A. Townsend Peterson said of the two passenger-pigeon genetic studies (published in 2014 and 2017) that, though the idea of extreme fluctuations in the passenger-pigeon population was \u201cdeeply entrenched,\u201d he was persuaded by the 2017 study\u2019s argument, due to its \u201cin-depth analysis\u201d and \u201cmassive data resources.\u201d A communally roosting species, the passenger pigeon chose roosting sites that could provide shelter and enough food to sustain their large numbers for an indefinite period. The time spent at one roosting site may have depended on the extent of human persecution, weather conditions, or other, unknown factors. Roosts ranged in size and extent, from a few acres to or greater. Some roosting areas would be reused for subsequent years, others would only be used once. The passenger pigeon roosted in such numbers that even thick tree branches would break under the strain. The birds frequently piled on top of each other's backs to roost. They rested in a slumped position that hid their feet. They slept with their bills concealed by the feathers in the middle of the breast while holding their tail at a 45-degree angle. Dung could accumulate under a roosting site to a depth of over .",
            "score": 85.7230087518692
        },
        {
            "docid": "30467_6",
            "document": "Tyrannosaurus . \"Tyrannosaurus rex\" was one of the largest land carnivores of all time; the largest complete specimen, located at the Field Museum of Natural History under the name FMNH PR2081 and nicknamed Sue, measured long, and was tall at the hips, and according to the most recent studies estimated to have weighed between to when alive. Not every adult \"Tyrannosaurus\" specimen recovered is as big. Historically average adult mass estimates have varied widely over the years, from as low as , to more than , with most modern estimates ranging between and . Hutchinson et al. (2011) found that the maximum weight of Sue, the largest complete \"Tyrannosaurus\" specimen, was between , though the authors stated that their upper and lower estimates were based on models with wide error bars and that they \"consider [them] to be too skinny, too fat, or too disproportionate\" and provided a mean estimate at for this specimen. Packard \"et al.\" (2009) tested dinosaur mass estimation procedures on elephants and concluded that those of dinosaurs are flawed and produce over-estimations; thus, the weight of \"Tyrannosaurus\", as well as other dinosaurs, could have been much less. Other estimations have concluded that the largest known \"Tyrannosaurus\" specimens had masses approaching or exceeding 9 tonnes.",
            "score": 52.75493025779724
        },
        {
            "docid": "62329_32",
            "document": "Meta-analysis . Since neither of these factors automatically indicates a faulty larger study or more reliable smaller studies, the re-distribution of weights under this model will not bear a relationship to what these studies actually might offer. Indeed, it has been demonstrated that redistribution of weights is simply in one direction from larger to smaller studies as heterogeneity increases until eventually all studies have equal weight and no more redistribution is possible. Another issue with the random effects model is that the most commonly used confidence intervals generally do not retain their coverage probability above the specified nominal level and thus substantially underestimate the statistical error and are potentially overconfident in their conclusions. Several fixes have been suggested but the debate continues on. A further concern is that the average treatment effect can sometimes be even less conservative compared to the fixed effect model and therefore misleading in practice. One interpretational fix that has been suggested is to create a prediction interval around the random effects estimate to portray the range of possible effects in practice. However, an assumption behind the calculation of such a prediction interval is that trials are considered more or less homogeneous entities and that included patient populations and comparator treatments should be considered exchangeable and this is usually unattainable in practice. The most widely used method to estimate between studies variance (REVC) is the DerSimonian-Laird (DL) approach. Several advanced iterative (and computationally expensive) techniques for computing the between studies variance exist (such as maximum likelihood, profile likelihood and restricted maximum likelihood methods) and random effects models using these methods can be run in Stata with the metaan command. The metaan command must be distinguished from the classic metan (single \"a\") command in Stata that uses the DL estimator. These advanced methods have also been implemented in a free and easy to use Microsoft Excel add-on, MetaEasy. However, a comparison between these advanced methods and the DL method of computing the between studies variance demonstrated that there is little to gain and DL is quite adequate in most scenarios.",
            "score": 70.0338978767395
        },
        {
            "docid": "1507717_19",
            "document": "Mark and recapture . Surprisingly, Chapman's estimate was one conjecture from a range of possible estimators: \"In practice, the whole number immediately less than (K+1)(n+1)/(k+1) or even Kn/(k+1) will be the estimate. The above form is more convenient for mathematical purposes.\"(see footnote, page 144). Chapman also found the estimator could have considerable negative bias for small Kn/N (page 146), but was unconcerned because the estimated standard deviations were large for these cases.",
            "score": 80.39332914352417
        },
        {
            "docid": "54285432_6",
            "document": "Binary response model with latent variable . When the normality assumption doesn\u2019t hold, i.e. \"G\" is not normal distribution any more, then functional form misspecification issue arises. If the model is still estimated as a probit model, the coefficient estimators are inconsistent. For instance, if \"G\" is logistic distribution in the true model, but we estimate it by probit, the estimates will be generally smaller than the true value . However, the inconsistency of the coefficient estimates is practically irrelevant because the estimates for the partial effects, formula_24, estimated by probit model is very close to the estimates given by the true logit model. Actually, to avoid the issue of distribution form misspecification, it is better to adopt a very general distribution assumption for the error term, such that many different types of distribution can be included in the model. The cost is heavier computation and lower accuracy for the increase of the number of parameter. In most of the cases in practice where the distribution form is misspecified, the estimators for the coefficients are inconsistent, but those for the conditional probability and the partial effects are still very good.",
            "score": 81.6194634437561
        }
    ],
    "r": [
        {
            "docid": "21244265_21",
            "document": "Olfaction . Although conventional wisdom and lay literature, based on impressionistic findings in the 1920s, have long presented human olfaction as capable of distinguishing between roughly 10,000 unique odors, recent research has suggested that the average individual is capable of distinguishing over one trillion unique odors. Researchers in the most recent study, which tested the psychophysical responses to combinations of over 128 unique odor molecules with combinations composed of up to 30 different component molecules, noted that this estimate is \"conservative\" and that some subjects of their research might be capable of deciphering between a thousand trillion odorants, adding that their worst performer could probably still distinguish between 80million scents. Authors of the study concluded, \"This is far more than previous estimates of distinguishable olfactory stimuli. It demonstrates that the human olfactory system, with its hundreds of different olfactory receptors, far out performs the other senses in the number of physically different stimuli it can discriminate.\" However, it was also noted by the authors that the ability to distinguish between smells is not analogous to being able to consistently identify them, and that subjects were not typically capable of identifying individual odor stimulants from within the odors the researchers had prepared from multiple odor molecules. In November 2014 the study was strongly criticized by Caltech scientist Markus Meister, who wrote that the study's \"extravagant claims are based on errors of mathematical logic\". The logic of his paper has in turn been criticized by the authors of the original paper.",
            "score": 275.54229736328125
        },
        {
            "docid": "53758770_9",
            "document": "Retronasal smell . In the olfactory bulb, smell molecules are mapped spatially. These spatial representations are known as \u201csmell images.\" Spatial representation permits lateral inhibition, or contrast enhancement and gain compression. Contrast enhancement is sensitive to change and highlights stimuli in the brain that are changing rather than at rest. Gain compression heightens sensitivity to low-intensity stimuli while lessening sensitivity to high-intensity stimuli. The olfactory bulb, while still in the primary stages of its understanding by researchers, distinguishes smell from other senses because it marks a deviation in the sensory pathway from what is characteristic of all other senses. Namely, all non-olfactory sensory information passes through the thalamus after the receptor level, but the fact that odor information instead enters its own specialized area could suggest the primitive history of smell and/or a distinct type of processing of odor information on its way to the cortex. The olfactory bulb houses glomeruli, or cell junctures, on which thousands of receptors of the same type, in addition to mitral cells, converge. This organization allows a vast amount of information to be concisely represented without requiring an equally large number of receptor types. The resulting combination of odor information is dubbed an odor image at the level of the olfactory bulb.",
            "score": 196.1869659423828
        },
        {
            "docid": "665470_10",
            "document": "Olfactory receptor . The reason for the large number of different odor receptors is to provide a system for discriminating between as many different odors as possible. Even so, each odor receptor does not detect a single odor. Rather each individual odor receptor is broadly tuned to be activated by a number of similar odorant structures. Analogous to the immune system, the diversity that exists within the olfactory receptor family allows molecules that have never been encountered before to be characterized. However, unlike the immune system, which generates diversity through \"in-situ\" recombination, every single olfactory receptor is translated from a specific gene; hence the large portion of the genome devoted to encoding OR genes. Furthermore, most odors activate more than one type of odor receptor. Since the number of combinations and permutations of olfactory receptors is very large, the olfactory receptor system is capable of detecting and distinguishing between a very large number of odorant molecules.",
            "score": 183.0480499267578
        },
        {
            "docid": "21244096_11",
            "document": "Odor . Odor perception is a primal sense. The sense of smell enables pleasure, can subconsciously warn of danger, help locate mates, find food, or detect predators. Humans have a surprisingly good sense of smell (even though they only have 350 functional olfactory receptor genes compared to the 1,300 found in mice) correlated to an evolutionary decline in sense of smell. Human's remarkable sense of smell is just as good as many animals and can distinguish a diversity of odors- approximately 10,000 scents. Bushdid et al. reported, however, that humans can distinguish about one trillion odors.",
            "score": 172.7688751220703
        },
        {
            "docid": "21244265_37",
            "document": "Olfaction . Early scientific study of olfaction includes the extensive doctoral dissertation of Eleanor Gamble, published in 1898, which compared olfactory to other stimulus modalities, and implied that smell had a lower intensity discrimination. As the Epicurean and atomistic Roman philosopher Lucretius (1stcentury BCE) speculated, different odors are attributed to different shapes and sizes of \"atoms\" (odor molecules in the modern understanding) that stimulate the olfactory organ . A modern demonstration of that theory was the cloning of olfactory receptor proteins by Linda B. Buck and Richard Axel (who were awarded the Nobel Prize in 2004), and subsequent pairing of odor molecules to specific receptor proteins. Each odor receptor molecule recognizes only a particular molecular feature or class of odor molecules. Mammals have about a thousand genes that code for odor reception. Of the genes that code for odor receptors, only a portion are functional. Humans have far fewer active odor receptor genes than other primates and other mammals. In mammals, each olfactory receptor neuron expresses only one functional odor receptor. Odor receptor nerve cells function like a key\u2013lock system: if the airborne molecules of a certain chemical can fit into the lock, the nerve cell will respond. There are, at present, a number of competing theories regarding the mechanism of odor coding and perception. According to the shape theory, each receptor detects a feature of the odor molecule. The weak-shape theory, known as the odotope theory, suggests that different receptors detect only small pieces of molecules, and these minimal inputs are combined to form a larger olfactory perception (similar to the way visual perception is built up of smaller, information-poor sensations, combined and refined to create a detailed overall perception). According to a new study, researchers have found that a functional relationship exists between molecular volume of odorants and the olfactory neural response.<ref name=\"10.1101/013516\"></ref> An alternative theory, the vibration theory proposed by Luca Turin, posits that odor receptors detect the frequencies of vibrations of odor molecules in the infrared range by quantum tunnelling. However, the behavioral predictions of this theory have been called into question. There is no theory yet that explains olfactory perception completely.",
            "score": 168.82778930664062
        },
        {
            "docid": "53758770_8",
            "document": "Retronasal smell . The first stop in the olfactory system is the olfactory epithelium, or tissue resting on the roof of the nasal cavity which houses smell receptors. Smell receptors are bipolar neurons that bind odorants from the air and congregate at the olfactory nerve before passing axons to the dendrites of mitral cells in the olfactory bulb. Sensory receptors in the mouth and nose are polarized at resting state, and they depolarize in response to some change in environment, such as coming in contact with odor molecules. Odor molecules, consisting of hydrocarbon chains with functional groups, bind to sensory receptors in the nose and mouth. Properties of functional groups include: (1) length of carbon chain, (2) terminal group, which concord with differences associated with different smells, (3) side group, (4) chirality, (5) shape, and (6) size. When odor molecules bind to sensory receptors, they do so in according to these properties. Each olfactory cell has a single type of receptor, but that receptor can be \u201cbroadly tuned\u201d and odor molecules further interact at the receptor level, meaning that, in certain cases, an odor molecule alone may not bind to a receptor, but in the presence of another odor molecule, the original would bind and thus create a sensation of smell only in the presence of the second molecule.",
            "score": 164.35105895996094
        },
        {
            "docid": "23416874_15",
            "document": "Sense . Smell or olfaction (adjectival form: olfactory) is the other \"chemical\" sense. Unlike taste, there are hundreds of olfactory receptors (388 according to one source), each binding to a particular molecular feature. Odor molecules possess a variety of features and, thus, excite specific receptors more or less strongly. This combination of excitatory signals from different receptors makes up what we perceive as the molecule's smell. In the brain, olfaction is processed by the olfactory system. Olfactory receptor neurons in the nose differ from most other neurons in that they die and regenerate on a regular basis. The inability to smell is called anosmia. Some neurons in the nose are specialized to detect pheromones.",
            "score": 163.25608825683594
        },
        {
            "docid": "377397_6",
            "document": "American lobster . The antennae measure about long and split into Y-shaped structures with pointed tips. Each tip exhibits a dense zone of hair tufts staggered in a zigzag arrangement. These hairs are covered with multiple nerve cells that can detect odors. Larger, thicker hairs found along the edges control the flow of water, containing odor molecules, to the inner sensory hairs. The shorter antennules provide a further sense of smell. By having a pair of olfactory organs, a lobster can locate the direction a smell comes from, much the same way humans can hear the direction a sound comes from. In addition to sensing smells, the antennules can judge water speed to improve direction finding.",
            "score": 159.80172729492188
        },
        {
            "docid": "46405867_2",
            "document": "Evolution of olfaction . Odor molecules are detected by the olfactory receptors (hereafter OR) in the olfactory epithelium of the nasal cavity. Each receptor type is expressed within a subset of neurons, from which they directly connect to the olfactory bulb in the brain. Olfaction is essential for survival in most vertebrates; however, the degree to which an animal depends on smell is highly varied. Great variation exists in the number of OR genes among vertebrate species, as shown through bioinformatic analyses. This diversity exists by virtue of the wide-ranging environments that they inhabit. For instance, dolphins that are secondarily adapted to an aquatic niche possess a considerably smaller subset of genes than most mammals. OR gene repertoires have also evolved in relation to other senses, as higher primates with well-developed vision systems tend to have a smaller number of OR genes. As such, investigating the evolutionary changes of OR genes can provide useful information on how genomes respond to environmental changes. Differences in smell sensitivity are also dependent on the anatomy of the olfactory apparatus, such as the size of the olfactory bulb and epithelium. Nonetheless, the general features of the olfactory system are highly conserved among vertebrates, and, similarly to other sensory systems, olfaction has undergone fairly modest changes throughout the evolution of vertebrates. Phylogenetic analyses reveal that at least three distinct olfactory subsystems are broadly consistent in vertebrates, and a fourth accessory system (vomeronasal) solely arose in tetrapods.",
            "score": 154.8748779296875
        },
        {
            "docid": "46405867_3",
            "document": "Evolution of olfaction . Mutations affecting OR genes on the chromosome are primarily responsible for the evolution of smell. OR genes are grouped in clusters along multiple chromosomes and are responsible for coding respective OR proteins. These proteins contain seven transmembrane domains that are responsible for detecting specific sets of odor molecules. OR genes are located on error-prone regions of the chromosome, and consequently, the DNA of the OR gene is periodically duplicated during crossover. After this duplication event, one of the two genes may mutate and disable its function, rendering it as a pseudogene. Alternatively, the duplicated copy may mutate without dysfunctionality, and will continue making the same olfactory receptor but with altered structural changes. This protein adjustment can induce a subtle shift in the range of smells an animal can sense. The diversity of smelling genes present in humans today are attributed to multiple rounds of mutations that have occurred throughout vertebrate evolution.  In particular, repeated rounds of gene duplication, deletion, and pseudogene evolution contribute to the variety of OR gene number. Formally known as \u201cbirth-and-death evolution\u201d, these dynamics are measured by the number of gains and losses from genes in each branch of the phylogenetic tree in question. Statistical methods can be used to estimate the total number of gains and losses, which can be as large as several hundred per branch of the tree. Moreover, the number of gains and losses can be enormous even if two extant species possess the same gene number (for example, humans and macaques).",
            "score": 154.32835388183594
        },
        {
            "docid": "846892_5",
            "document": "Gas chromatography\u2013mass spectrometry . The GC-MS is composed of two major building blocks: the gas chromatograph and the mass spectrometer. The gas chromatograph utilizes a capillary column which depends on the column's dimensions (length, diameter, film thickness) as well as the phase properties (e.g. 5% phenyl polysiloxane). The difference in the chemical properties between different molecules in a mixture and their relative affinity for the stationary phase of the column will promote separation of the molecules as the sample travels the length of the column. The molecules are retained by the column and then elute (come off) from the column at different times (called the retention time), and this allows the mass spectrometer downstream to capture, ionize, accelerate, deflect, and detect the ionized molecules separately. The mass spectrometer does this by breaking each molecule into ionized fragments and detecting these fragments using their mass-to-charge ratio. These two components, used together, allow a much finer degree of substance identification than either unit used separately. It is not possible to make an accurate identification of a particular molecule by gas chromatography or mass spectrometry alone. The mass spectrometry process normally requires a very pure sample while gas chromatography using a traditional detector (e.g. Flame ionization detector) cannot differentiate between multiple molecules that happen to take the same amount of time to travel through the column (\"i.e.\" have the same retention time), which results in two or more molecules that co-elute. Sometimes two different molecules can also have a similar pattern of ionized fragments in a mass spectrometer (mass spectrum). Combining the two processes reduces the possibility of error, as it is extremely unlikely that two different molecules will behave in the same way in both a gas chromatograph and a mass spectrometer. Therefore, when an identifying mass spectrum appears at a characteristic retention time in a GC-MS analysis, it typically increases certainty that the analyte of interest is in the sample.",
            "score": 152.71771240234375
        },
        {
            "docid": "53758770_14",
            "document": "Retronasal smell . The three-layered olfactory cortex, containing pyramidal cells is the next benchmark on the smell pathway. One pyramidal cell receives information from a multiplicity of mitral cells from the olfactory bulb, making the previously organized glomerular pattern distributed in the olfactory cortex. This dispersion of mitral cell information allows for self-excitatory feedback connections, lateral excitation, and self- and lateral-inhibition. These processes contribute to Hebbian learning, named after Donald O. Hebb, and is often simplified by the saying \u201cneurons that fire together wire together.\u201d Long-term potentiation, the neural mechanism for Hebbian learning, allows for memory formation at the pyramidal cell level. Hebbian learning is thus essentially the phenomenon by which the olfactory cortex \u201cremembers\u201d the output of combinations of smell molecules and allows for recognition of previously sensed combinations faster than novel ones by matching them to stored input. The resulting smells that were previously called odor images are stored in the olfactory cortex for recognition are referred to now as odor objects. Experience therefore strengthens signal-to-noise ratio in that a previously sensed odor object can be more easily distinguished against greater background noise.",
            "score": 152.3735809326172
        },
        {
            "docid": "23001_38",
            "document": "Polymer . In general, polymeric mixtures are far less miscible than mixtures of small molecule materials. This effect results from the fact that the driving force for mixing is usually entropy, not interaction energy. In other words, miscible materials usually form a solution not because their interaction with each other is more favorable than their self-interaction, but because of an increase in entropy and hence free energy associated with increasing the amount of volume available to each component. This increase in entropy scales with the number of particles (or moles) being mixed. Since polymeric molecules are much larger and hence generally have much higher specific volumes than small molecules, the number of molecules involved in a polymeric mixture is far smaller than the number in a small molecule mixture of equal volume. The energetics of mixing, on the other hand, is comparable on a per volume basis for polymeric and small molecule mixtures. This tends to increase the free energy of mixing for polymer solutions and thus make solvation less favorable. Thus, concentrated solutions of polymers are far rarer than those of small molecules.",
            "score": 151.23403930664062
        },
        {
            "docid": "3782801_9",
            "document": "Luca Turin . In 2011, Turin and colleagues published a paper in PNAS showing drosophila fruit flies can distinguish between odorants and their deuterated counterparts. Tests on drosophila differ from human experiments by using an animal subject known to have a good sense of smell and free from psychological biases that may complicate human tests. Drosophila were trained to avoid the deuterated odorant in a deuterated/normal pair, indicating a difference in odor. Furthermore, drosophila trained to avoid one deuterated odorant also avoided other deuterated odorants, chemically unrelated, indicating that the deuterated bond itself had a distinct smell. The authors identified a vibrational frequency that could be responsible and found it close to one found in nitriles. When flies trained to avoid deuterated odorants were exposed to the nitrile and its non-nitrile counterpart, the flies also avoided the nitrile, consistent with the theory that fly olfaction detects molecular vibrations.  Two years later, in 2013, Turin and colleagues published a study in PLoS ONE showing that humans easily distinguish gas-chromatography-purified deuterated musk in double-blind tests. The team chose musks due to the high number of carbon-hydrogen bonds available for deuteration. They replicated the earlier results of Vosshall and Keller showing that humans cannot reliably distinguish between acetophenone and its deuterated counterpart, with 8 hydrogens, and showed that humans only begin to detect the isotope odor of the musks beginning at 14 deuteriums, or 50% deuteration. Because Turin's proposed mechanism is a biological method of inelastic electron tunnelling spectroscopy, which exploits a quantum effect, his theory of olfaction mechanism has been described as an example of quantum biology.",
            "score": 148.9029541015625
        },
        {
            "docid": "2384297_6",
            "document": "The Lives of a Cell: Notes of a Biology Watcher . This essay focuses on how connected humanity is to nature and how we must make strides to understand our role. Thomas argues that even our own bodies are not solely ours since the mitochondria and other organelles are descended from other organisms. He creates a metaphor of the Earth as a giant cell itself with humans just as one part of a vast system. Astronauts must be decontaminated before they are allowed to interact on Earth. Thomas states that this is an act of \u201chuman chauvinism.\u201d Most organisms on Earth are symbiotic or, if harmful, have both adapted to warn the other. All organisms on Earth are interdependent and a stray virus or bacteria from the moon will not be adapted to harm us since it is not part of this connection. Bacteria are interconnected to the point where some cannot survive without others and some even live within others. We must recognize how interconnected even the smallest organisms are on Earth; especially if we must interact with life outside our planet. Thomas introduces one of his key metaphors of humans behaving like ants. He suggests that this metaphor is not used because humans do not like to be compared to insects that, as a society, can function as an organism. There are many examples of animals acting as a large organism when in large groups from termites and slime molds to birds and fish. Thomas argues that the communication of results in science puts humans in the same model as these other species. As all scientists communicate and build on each other\u2019s work in order to explore that which we do not know. Humans fear pheromones because we believe we have gone above the basic secretion of chemicals in our communication. However, there are signs that point to humans relying on pheromones as well as our most technological forms of communication. Thomas shows pheromones in the animal world with examples of moths and fish. He then goes on to explain what impact pheromones in humans could have on the future such as in the perfume industry and finding histocompatible donors. Music is the only form of communication that saves us from an overwhelming amount of small talk. This is not only a human phenomenon, but happens throughout the animal world. Thomas makes examples of animals from termites and earthworms to gorillas and alligators that perform some sort of rhythmic noise making that can be interpreted as music if we had full range of hearing. From the vast number of animals that participate in music it is clear that the need to make music is a fundamental characteristic of biology. Thomas proposes that the animal world is continuing a musical memory that has been going since the beginning of time. Thomas argues that even though we have the technological advancements to destroy the Earth that we do not know near enough about the world in which we live. To solve this problem he suggests that we should not be able to fire nuclear weapons without being able to explain one living thing fully. The organism that Thomas proposes is the protozoan Myxotricha paradoxa. There is information known about this protozoan that lives in the digestive tract of Australian termites but with more study it could be a model for how our cells developed. It is seen throughout nature that organisms cooperate and progress into more complex forms. We cannot destroy vast amounts of Earth with nuclear weapons until we understand how interconnected we all are. Thomas presents the three levels of technology in medicine: \u201cnontechnology\u201d that helps patients with diseases that are not well understood but does not help solve the underlying mechanisms of the disease, \u201chalfway technology\u201d that makes up for disease or postpones death for diseases whose courses we cannot do much about, and \u201chigh technology\u201d that from understanding the mechanism of the disease we are now able to cure. When looking at the costs of the three different technologies they are all needed, but once a \u201chigh technology\u201d is found for a disease the benefits outweigh the costs of studying the mechanism of the disease so thoroughly. Thomas suggests that in order to save money in health care, the highest priority in funding should be given to basic research. Humans leave a trace of chemicals in every place they go and on everything they touch. Other animals use signaling mechanisms to leave trails or identify each other. The sense of smell is an important sense in using these mechanisms, but it is still not well understood. Humans, compared to the rest of the animal world, do not have a good olfactory sense though we may be better than we first assume. Johannes Kepler once argued that the Earth is an immense organism itself, with chemical signals spreading across the globe through various organisms in order to keep the world functioning and well informed. Tau Ceti is a nearby sun-like star that we are on the verge of being able to begin making contact with, as well as other celestial bodies, to search for life. We have been attracted to the vast regions of space outside our Earth bubble and what they could hold. If extraterrestrial life is found, it scientifically would make sense, but the social impact of no longer being unique would give humans a new sense of community. The question of what information to send out is answered by Thomas by sending music, specifically Bach. It is timeless and the best language we have to express who we are. If possible Thomas also suggests sending art. However, the questions of what to send will not stop once we receive a reply. As humans we always evade death, despite how it is a natural part of our lives. Unless it is far removed, as in war or on television, then we can discuss it without a problem. It is a subconscious effort that by not thinking about death we may continue to live. Nevertheless, even if we cured all diseases we still would die one day. We must not fear death and research the dying process just as we would any other biological process. Most people who have a near death experience do not recall any pain or fear. It is perhaps the loss of consciousness that people fear more than death itself. Thomas returns to his pondering of the social behaviors of insects in this essay. He discusses the change in behavior of insects in groups and singular insects. We have used insects and their behavior to convey lessons, rules, and virtues and now they have been used in art. Thomas describes an art exhibit with living ants, surrounded by humans who act in a similar manner to the ants themselves. Thomas praises the Marine Biological Laboratory as \u201ca paradigm, a human institution possessed of a life of its own, self-regenerating, touched all around by human meddle but consistently improved, embellished by it.\u201d It attracts the brightest minds and makes great strides in science autonomously. Thomas paints pictures with his description of scientists covering the beach with their diagrams and making \u201cmusic\u201d of discussion after a lecture at the MBL. Humans have to learn how to walk, skip, and ride a bicycle but inside our bodies perform specific manipulations from birth that we do not need to learn. There is new research that suggests humans may be able to change these inner processes with teaching. Thomas reasons that his body has been functioning fine without him trying to control every little process so he will let it continue to do so. He suggests to try the exact opposite and try to disconnect from your body altogether. The biologic revolution is filling in the gaps in understanding about how our cells function. As we begin to understand more about organelles it is clear that they are not originally created from our cells. Mitochondria and chloroplasts most likely have a bacterial ancestry and flagellae and cilia most likely were once spirochetes. It is not necessarily a master-slave relationship that we have with our organelles, but one where their ancestors found an easy way to stay protected and secure. We have brought them along with us as we evolved and yet we do not understand them completely. Organelles and eukaryotic cells are one of the most established symbiotic relationships. We treat bacteria as an ever present enemy even though there are only a small number that actually cause disease, and by accident in most cases. Bacteria normally do not gain anything by causing illness or death in their hosts. Our illness is mostly caused by our immune system doing too great of a job in response to bacteria in our system. The strength of our response is not necessary for most cases, but remains from a primitive time. Health care has become the new name for medicine though this is a misnomer since illness and death cannot be totally eradicated. Thomas argues that to understand how medicine should be used we should look to those internists that are involved in the system. Most things get better in a short while by themselves, so we should no longer be instilling in the public a constant fear of failed health. This will be the best way to solve the problem of funding health care since people will only use it when it is necessary. There are different degrees of social behavior in animals. However, it is not clear where humans fit on the scale. Most signs point that we are above the social behavior of ants and bees that go about a singular task as a whole community. Language is the one trait that brings us to the level of such animals. All humans engage in language and are born with the understanding of language. Language, and perhaps along with art and music, is the core of our social behavior. The human mind comes with the understanding of how to deal with and use language. We store up information as a cell stores energy, though with language, this information can be put to further use. Another main difference between language and other communication systems in biology is the ambiguity that is a necessity in language which would cause the other communication systems to fail. Death is not supposed to happen in the open, along highways and in sight of others. Everything is in the process of dying all around us, though we keep it hidden from our sight and minds. Death is part of the cycle and we need to understand we are part of a larger process. The process of dying is necessary for the birth of the new and we will all experience it together. Thomas explains science as a wild manifestation of human behavior. He explains that science and discovery is a compulsion that scientists seem to have written in their very genes. Science cannot be organized and forced; it must be free to go where the next question leads. It is similar to a bee hive in some sense, but also to animals on a hunt. The activity is never ending and the conglomeration of minds always yearning for the next discovery cannot be kept under control. How humans approach nature has been changing throughout recent years. We used to view nature as ours to control and use to better mankind. Now we have moved away from this view and seen that we are part of the larger system and not the ruler of it. However Thomas argues that we must see ourselves as \u201cindispensable elements of nature\u201d and work for the betterment of the Earth but also be able to protect ourselves. This essay focuses on the tribe of Iks in northern Uganda. Thomas comments on an anthropologist\u2019s report on the Iks that argues that they represent the basic elements of mankind. Thomas instead thinks that each Ik acts as a group and that by observing the whole tribe of Iks you can see how we behave in groups ranging from committees to nations. In order to improve upon our group interactions, we must stay human even when in masses. Computers are approaching humanity, but they will never be able to fully replace us for they will not be able to replicate our collective behavior because we do not understand it ourselves. We are involved in a never ending transfer of information and collective thinking. This is the cause of the unpredictability in our future. The one problem with our information transfer is that we are much better at gaining information than giving output back. Thomas explains in this essay his view on scientific funding and planning. He believes that research should be focused in basic science. Unlike basic science, disease problems do not have the right type of questions to allow for great discoveries. The distinguishing factor of basic science is that there can be an element of surprise that allows for even more discoveries to be made. It is difficult to organize plans for this type of surprise in research even though it may seem a better business model to do so. It is the improbability and maze of puzzles that occur in basic research that Thomas believes will lead us to the most knowledge. Mythical creatures were created by our ancestors but even though we presently have no need for these beasts we continue to use them. The hybridization of animals in mythology is present from multiple ancient people such as the Ganesha, Griffon, Centaur, and Sphinx. Thomas suggests that perhaps we look to replace these mythological creatures which are more biological. He suggests the Myxotricha paradoxa, blepharisma, bacteria, and plant-animal combinations that are either made up of different organisms or set up joint endeavors with more than one organism to survive. From Thomas\u2019s metaphor on how humans behave like ants, he again argues that language is the quality that best resembles social insects. Without any outside direction, humans continually change language. We build language like ants build their hill, without ever knowing what the final result is and how our minuscule changes affect any other part. Thomas explains how some words have changed and developed different meanings. Two words, gene and bheu, are two words that we have derived a great number of current words from. Their descended words: kind, nature, physics are related in the present but also in its ancestry. Thomas compares language to the social behavior of termites in this essay. He thinks of language as an organism that is alive and changing. The genes of language are how words originated when you look into each of their histories. He traces multiple words to their origins to prove his point. He comments that it would be near impossible to keep track of all roots of words back to Indo-European that you use. We should be in awe that we exist and are unique among all the humans on Earth according to probability. Though we are indeed individual organisms, Thomas argues that one\u2019s own self is a myth. He believes we are part of a larger organization of information sharing. Through this system we are adapting and creating. By being more open with communication and less restrictive we will be able to uncover even more surprising discoveries. Thomas compares the Earth to a living cell, one with its own membrane that allows it to keep out disorder. He shows how the evolution of cells was closely tied to the \u201cbreath\u201d of the Earth, the cycling of oxygen concentration in the atmosphere. The atmosphere is \u201cfor sheer size and perfection of function, it is far and away the grandest product of collaboration in all of nature.\u201d It gives us the oxygen we need, protection from UV light, and protection from the millions of meteorites.",
            "score": 148.6350555419922
        },
        {
            "docid": "2051423_50",
            "document": "Cannabis cultivation . Some plants (e.g. cultivars of \"Cannabis sativa\" subsp. \"indica\"), can give off strong odors as they grow, resulting in detection of illegal growing operations. Growers frequently use carbon scrubbers and ventilation to control odors. This typically involves forcing air from the grow room through a device containing activated carbon, then venting it outdoors. Others use an ozone generator. Ozone reacts with odor molecules in the air, permanently eliminating them. However, ozone can build up to levels that may be hazardous both for grower and plant. As a last resort, keeping windows firmly shut and using strong air fresheners can control smells. Checking outside to see if any smells are emanating from indoors is often a necessary precaution, as many growers become acclimated to the smell, and fail to realize just how pervasive the odor may be. Many store plants in more isolated areas such as a basement or attic to prevent smell detection. Another less common solution is to simply grow a strain with a weaker odor.",
            "score": 148.1968231201172
        },
        {
            "docid": "915627_17",
            "document": "Color wheel . There is no straight-line relationship between colors mixed in pigment, which vary from medium to medium. With a psychophysical color circle, however, the resulting hue of any mixture of two colored light sources can be determined simply by the relative brightness and wavelength of the two lights. A similar calculation cannot be performed with two paints. As such, a painter's color wheel is indicative rather than predictive, being used to compare existing colors rather than calculate exact colors of mixtures. Because of differences relating to the medium, different color wheels can be created according to the type of paint or other medium used, and many artists make their own individual color wheels. These often contain only blocks of color rather than the gradation between tones that is characteristic of the color circle.",
            "score": 147.49261474609375
        },
        {
            "docid": "23229753_11",
            "document": "Coconut crab . The coconut crab has a well-developed sense of smell, which it uses to locate its food. The process of smelling works very differently depending on whether the smelled molecules are hydrophilic molecules in water or hydrophobic molecules in air. As most crabs live in the water, they have specialised organs called s on their antennae to determine both the concentration and the direction of a smell. However, as coconut crabs live on the land, the aesthetascs on their antennae are shorter and blunter than those of other crabs and look more like those of insects. While insects and the coconut crab originate from different paths, the same need to detect smells in the air led to the development of remarkably similar organs. Coconut crabs flick their antennae as insects do to enhance their reception. Their sense of smell can detect interesting odours over large distances. The smells of rotting meat, bananas, and coconuts, all potential food sources, catch their attention especially. The olfactory system in the coconut crab's brain is well-developed compared to other areas of the brain.",
            "score": 147.41114807128906
        },
        {
            "docid": "3022936_4",
            "document": "Asphaltene . Asphaltene after heating are subdivided in: Nonvolatile (heterocyclic N and S species), and, volatile (paraffin + olefins), benzenes, naphtalenes, phenanthrenes, several others). Speight reports a simplified representation of the separation of petroleum into the following six major fractions: volatile saturates, volatile aromatics, nonvolatile saturates, nonvolatile aromatics, resins and asphaltenes. He also reports arbitrarily defined physical boundaries for petroleum using carbon-number and boiling point. The molecular structures proposed for the asphaltene (heavy organic) molecules includes carbon, hydrogen, oxygen, nitrogen, sulfur as well as polar and non-polar groups.  The molecular structure of asphaltenes is difficult to determine because the molecules tend to stick together in solution. These materials are extremely complex mixtures containing hundreds or even thousands of individual chemical species. Asphaltenes do not have a specific chemical formula: individual molecules can vary in the number of atoms contained in the structure, and the average chemical formula can depend on the source. Although they have been subjected to modern analytical methods, including the well known SARA analysis, mass spectrometry, and nuclear magnetic resonance, the exact molecular structures are difficult to determine. Given this limitation, asphaltenes are composed mainly of polyaromatic carbon ring units with oxygen, nitrogen, and sulfur heteroatoms, combined with trace amounts of heavy metals, particularly chelated vanadium and nickel, and aliphatic side chains of various lengths. Many asphaltenes from crude oils around the world contain similar ring units, which are linked together to make highly diverse large molecules.",
            "score": 146.97174072265625
        },
        {
            "docid": "1664060_41",
            "document": "Adaptive immune system . For the adaptive response to \"remember\" and eliminate a large number of pathogens the immune system must be able to distinguish between many different antigens, and the receptors that recognize antigens must be produced in a huge variety of configurations, in essence one receptor (at least) for each different pathogen that might ever be encountered. Even in the absence of antigen stimulation, a human can produce more than 1 trillion different antibody molecules. Millions of genes would be required to store the genetic information that produces these receptors, but, the entire human genome contains fewer than 25,000 genes.",
            "score": 146.93406677246094
        },
        {
            "docid": "569650_46",
            "document": "Stimulus modality . Our olfactory ability can vary due to different conditions. For example, our olfactory detection thresholds can change due to molecules with differing lengths of carbon chains. A molecule with a longer carbon chain is easier to detect, and has a lower detection threshold. Additionally, women generally have lower olfactory thresholds than men, and this effect is magnified during a woman's ovulatory period. Interestingly, we can sometimes experience a hallucination of smell, as in the case of phantosmia.",
            "score": 146.7153778076172
        },
        {
            "docid": "40439442_12",
            "document": "Molecular diagnostics . Because molecular diagnostics methods can detect sensitive markers, these tests are less intrusive than a traditional biopsy. For example, because cell-free nucleic acids exist in human plasma, a simple blood sample can be enough to sample genetic information from tumours, transplants or an unborn fetus. Many, but not all, molecular diagnostics methods based on nucleic acids detection use polymerase chain reaction (PCR) to vastly increase the number of nucleic acid molecules, thereby amplifying the target sequence(s) in the patient sample. PCR is a method that a template DNA is amplified using synthetic primers, a DNA polymerase, and dNTPs. The mixture is cycled between at least 2 temperatures: a high temperature for denaturing double-stranded DNA into single-stranded molecules and a low temperature for the primer to hybridize to the template and for the polymerase to extend the primer. Each temperature cycle theoretically doubles the quantity of target sequence.Detection of sequence variations using PCR typically involves the design and use oligonucleotide reagents that amplify the variant of interest more efficiently than wildtype sequence. PCR is currently the most widely used method for detection of DNA sequences. The detection of the marker might use real time PCR, direct sequencing, , microarray chipsprefabricated chips that test many markers at once, or MALDI-TOF The same principle applies to the proteome and the genome. High-throughput protein arrays can use complementary DNA or antibodies to bind and hence can detect many different proteins in parallel. Molecular diagnostic tests vary widely in sensitivity, turn around time, cost, coverage and regulatory approval. They also vary in the level of validation applied in the laboratories using them. Hence, robust local validation in accordance with the regulatory requirements and use of appropriate controls is required especially where the result may be used to inform a patient treatment decision.",
            "score": 146.11720275878906
        },
        {
            "docid": "21244096_9",
            "document": "Odor . Habituation affects the ability to distinguish odors after continuous exposure. The sensitivity and ability to discriminate odors diminishes with exposure, and the brain tends to ignore continuous stimulus and focus on differences and changes in a particular sensation. When odorants are mixed, the conditioned odorant is blocked out because of habituation. This depends on the strength of the odorants in the mixture which can change perception and processing of an odor. This process helps classify similar odors as well as adjust sensitivity to differences in complex stimuli.",
            "score": 145.27423095703125
        },
        {
            "docid": "665470_6",
            "document": "Olfactory receptor . In a recent but highly controversial interpretation, it has also been speculated that olfactory receptors might really sense various vibrational energy-levels of a molecule rather than structural motifs via quantum coherence mechanisms. As evidence it has been shown that flies can differentiate between two odor molecules which only differ in hydrogen isotope (which will drastically change vibrational energy levels of the molecule). Not only could the flies distinguish between the deuterated and non-deuterated forms of an odorant, they could generalise the property of \"deuteratedness\" to other novel molecules. In addition, they generalised the learned avoidance behaviour to molecules which were not deuterated but did share a significant vibration stretch with the deuterated molecules, a fact which the differential physics of deuteration (below) has difficulty in accounting for.",
            "score": 145.2511749267578
        },
        {
            "docid": "51721949_77",
            "document": "Argon compounds . Metal ions can also form with more than one argon atom, in a kind of argon metal cluster. Different sized metal ions at the centre of a cluster can fit different geometries of argons atoms around the ion. Argides with multiple argon atoms have been detected in mass spectrometry. These can have variable numbers of argon attached, but there are magic numbers, where the complex more commonly has a particular number, either four or six argon atoms. These can be studied by time of flight mass spectrometer analysis and by the photodissociation spectrum. Other study methods include Coulomb explosion analysis. Argon-tagging is a technique whereby argon atoms are weakly bound to a molecule under study. It results in a much lower temperature of the tagged molecules, with sharper infra-red absorption lines. The argon-tagged molecules can be disrupted by photons of a particular wavelength.",
            "score": 144.8920440673828
        },
        {
            "docid": "534710_3",
            "document": "Cone cell . Cones are less sensitive to light than the rod cells in the retina (which support vision at low light levels), but allow the perception of color. They are also able to perceive finer detail and more rapid changes in images, because their response times to stimuli are faster than those of rods. Cones are normally one of the three types, each with different pigment, namely: S-cones, M-cones and L-cones. Each cone is therefore sensitive to visible wavelengths of light that correspond to short-wavelength, medium-wavelength and long-wavelength light. Because humans usually have three kinds of cones with different photopsins, which have different response curves and thus respond to variation in color in different ways, we have trichromatic vision. Being color blind can change this, and there have been some verified reports of people with four or more types of cones, giving them tetrachromatic vision. The three pigments responsible for detecting light have been shown to vary in their exact chemical composition due to genetic mutation; different individuals will have cones with different color sensitivity. Destruction of the cone cells from disease would result in color blindness.",
            "score": 144.07098388671875
        },
        {
            "docid": "188103_11",
            "document": "Karl von Frisch . Frisch discovered that bees can distinguish various blossoming plants by their scent, and that each bee is \"flower constant\". Surprisingly, their sensitivity to a \"sweet\" taste is only slightly stronger than in humans. He thought it possible that a bee\u2019s spatial sense of smell arises from the firm coupling of its olfactory sense with its tactile sense. Frisch was the first to demonstrate (in 1914) that honey bees had color vision, which he accomplished by using classical conditioning. He trained bees to feed on a dish of sugar water set on a colored card. He then set the colored card in the middle of a set of gray-toned cards. If the bees see the colored card as a shade of gray, then they will confuse the blue card with at least one of the gray-toned cards; bees arriving to feed will visit more than one card in the array. On the other hand, if they have color vision, then the bees visit only the blue card, as it is visually distinct from the other cards. A bee\u2019s color perception is comparable to that of humans, but with a shift away from the red toward the ultraviolet part of the spectrum. For that reason bees cannot distinguish red from black (colorless), but they can distinguish the colors white, yellow, blue and violet. Color pigments which reflect UV radiation expand the spectrum of colors which can be differentiated. For example, several blossoms which may appear to humans to be of the same yellow color will appear to bees as having different colors (multicolored patterns) because of their different proportions of ultraviolet.",
            "score": 143.6689453125
        },
        {
            "docid": "2174011_16",
            "document": "Avoided crossing . In polyatomic molecules, there are various parameters which determine the Hamiltonian of the system. The mutual distances between the atoms are one set of parameters. If both of the atoms of a diatomic molecule is same, the symmetry suggests that different configurations keeping their mutual distance fixed will result into same electronic states. So it is the relative distance formula_73 which acts as a parameter for the two equations promising level crossing. Hence due to the avoided crossing theorem in general we can not have level crossings between two electronic states of same symmetry. But in polyatomic molecules the number of independent mutual distances of nuclei are more. For a N-atomic molecule the number of independent mutual separation is formula_74 (for formula_75). Each of them acts as a parameter for the total Hamiltonian. Since we always have minimum of three independent parameters, level crossing is not totally avoided in these molecules.",
            "score": 143.12620544433594
        },
        {
            "docid": "639660_7",
            "document": "Olfactory receptor neuron . A widely publicized study suggested that humans can detect more than one trillion different odors. This finding has however been disputed. Critics argued that the methodology used for the estimation was fundamentally flawed, showing that applying the same argument for better-understood sensory modalities, such as vision or audition, leads to wrong conclusions. Other researchers have also showed that the result is extremely sensitive to the precise details of the calculation, with small variations changing the result over dozens of orders of magnitude, possibly going as low as a few thousand. The authors of the original study have argued that their estimate holds as long as it is assumed that odor space is sufficiently high-dimensional.",
            "score": 142.7586212158203
        },
        {
            "docid": "42186219_2",
            "document": "Peanut butter test . The peanut butter test is a diagnostic test which aims to detect Alzheimer's disease by measuring subjects' ability to smell peanut butter through each nostril. The test was originally reported in October 2013 by researchers from the University of Florida's McKnight Brain Institute, led by professor Kenneth Heilman, and involves measuring the ability of patients to smell peanut butter held close to their nose. The researchers concluded that patients with Alzheimer's were not able to smell the peanut butter as well through their left nostril as their right one. The study's lead author, graduate student Jennifer Stamps, got the idea for the study when, while studying under Professor Heilman, she noticed that none of his patients had been tested for their sense of smell. The idea of using peanut butter came to Stamps when she administered it to patients as part of a routine test of cranial nerve function. Their decision to use it was also motivated by the fact that it is a pure odorant (i.e. is only detected by the olfactory nerve), and that Heilman had told Stamps, \"If you can come up with something quick and inexpensive, we can do it.\"",
            "score": 141.77731323242188
        },
        {
            "docid": "34118956_14",
            "document": "Perception of infrasound . In experiments using heart-rate conditioning, Pigeons have been found to be able to detect sounds in the infrasonic range at frequencies as low as 0.5\u00a0Hz. For frequencies below 10\u00a0Hz, the pigeon threshold is at about 55\u00a0dB which is at least 50\u00a0dB more sensitive than humans. Pigeons are able to discriminate small frequency differences in sounds at between 1\u00a0Hz and 20\u00a0Hz, with sensitivity ranging from a 1% shift at 20\u00a0Hz to a 7% shift at 1\u00a0Hz. Sensitivities are measured through a heart-rate conditioning test. In this test, an anesthetized bird is presented with a single sound or a sequence of sounds, followed by an electric shock. The bird\u2019s heart-rate will increase in anticipation of a shock. Therefore, a measure of the heart-rate can determine whether the bird is able to distinguish between stimuli that would be followed by a shock from stimuli that would not. Similar methods have also been used to determine the pigeon\u2019s sensitivity to barometric pressure changes, polarized light, and UV light. These experiments were conducted in sound isolation chambers to avoid the influence of ambient noise. Infrasonic stimuli are hard to produce and are often transmitted through a filter that attenuates higher frequency components. Also, the tone burst stimuli used in these experiments were presented with stimulus onset and offsets ramped on and off gradually in order to prevent initial turn-on and turn-off transients.",
            "score": 141.72732543945312
        },
        {
            "docid": "480465_16",
            "document": "Spectrophotometry . Spectrophotometry is an important technique used in many biochemical experiments that involve DNA, RNA, and protein isolation, enzyme kinetics and biochemical analyses. A brief explanation of the procedure of spectrophotometry includes comparing the absorbency of a blank sample that does not contain a colored compound to a sample that contains a colored compound. This coloring can be accomplished by either a dye such as Coomasie Brilliant Blue G-250 dye measured at 595\u00a0nm or by an enzymatic reaction as seen between \u03b2-galactosidase and ONPG (turns sample yellow) measured at 420\u00a0nm. The spectrophotometer is used to measure colored compounds in the visible region of light (between 350\u00a0nm and 800\u00a0nm), thus it can be used to find more information about the substance being studied. In biochemical experiments, a chemical and/or physical property is chosen and the procedure that is used is specific to that property in order to derive more information about the sample, such as the quantity, purity, enzyme activity, etc. Spectrophotometry can be used for a number of techniques such as determining optimal wavelength absorbance of samples, determining optimal pH for absorbance of samples, determining concentrations of unknown samples, and determining the pKa of various samples. Spectrophotometry is also a helpful process for protein purification and can also be used as a method to create optical assays of a compound. Spectrophotometric data can also be used in conjunction with the Beer-Lambert Equation, A= -logT=\u03b5cl=OD, in order to determine various relationships between transmittance and concentration, and absorbance and concentration. Because a spectrophotometer measures the wavelength of a compound through its color, a dye binding substance can be added so that it can undergo a color change and be measured. It is possible to know the concentrations of a two component mixture using the absorption spectra of the standard solutions of each component. To do this, it is necessary to know the extinction coefficient of this mixture at two wave lengths and the extinction coefficients of solutions that contain the known weights of the two components. Spectrophotometers have been developed and improved over decades and have been widely used among chemists. Additionally, Spectrophotometers are specialized to measure either UV or Visible light wavelength absorbance values. It is considered to be a highly accurate instrument that is also very sensitive and therefore extremely precise, especially in determining color change. This method is also convenient for use in laboratory experiments because it is an inexpensive and relatively simple process.",
            "score": 141.42825317382812
        }
    ]
}