{
    "q": [
        {
            "docid": "53857907_17",
            "document": "Operation of computed tomography . The technique of filtered back projection is one of the most established algorithmic techniques for this problem. It is conceptually simple, tunable and deterministic. It is also computationally undemanding, with modern scanners requiring only a few milliseconds per image. However, this is not the only technique available: the original EMI scanner solved the tomographic reconstruction problem by linear algebra, but this approach was limited by its high computational complexity, especially given the computer technology available at the time. More recently, manufacturers have developed iterative physical model-based maximum likelihood expectation maximization techniques. These techniques are advantageous because they use an internal model of the scanner's physical properties and of the physical laws of X-ray interactions. Earlier methods, such as filtered back projection, assume a perfect scanner and highly simplified physics, which leads to a number of artifacts, high noise and impaired image resolution. Iterative techniques provide images with improved resolution, reduced noise and fewer artifacts, as well as the ability to greatly reduce the radiation dose in certain circumstances. The disadvantage is a very high computational requirement, but advances in computer technology and high-performance computing techniques, such as use of highly parallel GPU algorithms or use of specialized hardware such as FPGAs or ASICs, now allow practical use.",
            "score": 120.81811285018921
        },
        {
            "docid": "328240_14",
            "document": "Marketing strategy . Mintzberg suggests that the top planners spend most of their time engaged in analysis and are concerned with industry or competitive analyses as well as internal studies, including the use of computer models to analyze trends in the organization. Strategic planners use a variety of research tools and analytical techniques, depending on the environment complexity and the firm's goals. Fleitcher and Bensoussan, for instance, have identified some 200 qualitative and quantitative analytical techniques regularly used by strategic analysts while a recent publication suggests that 72 techniques are essential. No optimal technique can be identified as useful across all situations or problems. Determining which technique to use in any given situation rests with the skill of the analyst. The choice of tool depends on a variety of factors including: data availability; the nature of the marketing problem; the objective or purpose, the analyst\u2019s skill level as well as other constraints such as time or motivation.",
            "score": 94.25514543056488
        },
        {
            "docid": "10342677_3",
            "document": "David J. Smith (physicist) . His basic research centers around the development of quantitative High Resolution Transmission Electron Microscopy, aided by computer-controlled microscope operation and image simulation, which enables direct determination of atomic structure in defective materials. His research also involves using electron-microscopy-based methods to characterize advanced materials such as semiconductor heterostructures, magnetic thin films and multilayers, and nanostructures. Semiconductor systems of interest include ternary and quaternary Group III nitride alloys for light-emitting diodes and lasers, and II-VI alloys, such as mercury cadmium telluride for detectors of infra-red radiation. Magnetic materials being studied include shape-memory alloys, as well as magnetic tunnel junctions, which are based on ferromagnet-insulator-ferromagnet combinations, that have promising applications for non-volatile, high-storage-density recording media. Off-axis electron holography is a particularly powerful approach since it permits quantitative visualization of nanoscale electric and magnetic fields, and we are using the technique to investigate the magnetization behavior and fringing fields associated with patterned nanostructures.",
            "score": 126.04891622066498
        },
        {
            "docid": "2536597_3",
            "document": "Nanotribology . Historically, nanotribological research includes direct investigation with microscopy techniques, such as Scanning Tunneling Microscope (STM), Atomic-Force Microscope (AFM) and Surface Forces Apparatus, (SFA) used to analyze surfaces with extremely high resolution, and thanks to the development of computational methods and power surfaces, we can study these phenomena indirectly as well.",
            "score": 104.18174624443054
        },
        {
            "docid": "55172_50",
            "document": "Proteomics . The biomolecular structure forms the 3D configuration of the protein. Understanding the protein's structure aids in identification of the protein's interactions and function. It used to be that the 3D structure of proteins could only be determined using X-ray crystallography and NMR spectroscopy. As of 2017, Cryo-electron microscopy is a leading technique, solving difficulties with crystallization (in X-ray crystallography) and conformational ambiguity (in NMR); resolution was 2.2\u00c5 as of 2015. Now, through bioinformatics, there are computer programs that can in some cases predict and model the structure of proteins. These programs use the chemical properties of amino acids and structural properties of known proteins to predict the 3D model of sample proteins. This also allows scientists to model protein interactions on a larger scale. In addition, biomedical engineers are developing methods to factor in the flexibility of protein structures to make comparisons and predictions.",
            "score": 163.10206472873688
        },
        {
            "docid": "9248684_3",
            "document": "Near-field optics . The limit of optical resolution in a conventional microscope, the so-called diffraction limit, is in the order of half the wavelength of the light used to image. Thus, when imaging at visible wavelengths, the smallest resolvable features are several hundred nanometers in size (although point-like sources, such as quantum dots, can be resolved quite readily). Using near-field optical techniques, researchers currently resolve features in the order of tens of nanometers in size. While other imaging techniques (e.g. atomic force microscopy and electron microscopy) can resolve features of much smaller size, the many advantages of optical microscopy make near-field optics a field of considerable interest.",
            "score": 108.79531455039978
        },
        {
            "docid": "39643325_35",
            "document": "Crowdsourcing software development . Particularly, crowdsourcing is used to develop large and complex software in a virtualized, decentralized manner. Cloud computing is a colloquial expression used to describe a variety of different types of computing concepts that involve a large number of computers connected through a real-time communication network (typically the Internet). Many advantages are to be found when moving crowdsourcing applications to the cloud: focus on project development rather than on the infrastructure that supports this process, foster the collaboration between geographically distributed teams, scale resources to the size of the projects, work in a virtualized, distributed, and collaborative environment. The demands on software crowdsourcing systems are ever evolving as new development philosophies and technologies gain favor. The reference architecture presented above is designed to encompass generality in many dimensions including, for example different software development methodologies, incentive schemes, and competitive/collaborative approaches. There are several clear research directions that could be investigated to enhance the architecture such as data analytics, service based delivery, and framework generalization. As systems grow understanding the use of the platform is an important consideration, data regarding users, projects, and interaction between the two can all be explored to investigate performance. These data may also provide helpful insights when developing tasks or selecting participants. Many of the components designed in the architecture are general purpose and could be delivered as hosted services. By hosting these services the barriers for entry would be significantly reduced. Finally, through deployments of this architecture there is potential to derive a general purpose framework that could be used for different software development crowdsourcing projects or more widely for other crowdsourcing applications. The creation of such frameworks has had transformative effects in other domains for instance the predominant use of BOINC in volunteer computing.",
            "score": 120.08771896362305
        },
        {
            "docid": "36516519_3",
            "document": "Natalie Ahn . Ahn earned her bachelor's degree in chemistry from the University of Washington, Seattle in 1979. Ahn conducted research in Lyle Jensen's lab, focusing on X-ray crystallography. Her participation in this research aided in better understanding of protein folding and visualizing of the 3-D structure of proteins by using computational techniques with X-ray crystallography. Additionally, Ahn worked as an undergraduate research assistant in David Teller's lab, which investigated protein hydrodynamics, the study of the motion of proteins relative to their aqueous environment, which they can either be suspended or dissolved within.",
            "score": 95.02294778823853
        },
        {
            "docid": "13758862_9",
            "document": "Daniel A. Reed (computer scientist) . Reed\u2019s research focuses on the design of very high-speed computers, providing new computing capabilities for scholars in science, medicine, engineering and the humanities, tools and techniques for capturing and analyzing the performance of parallel systems, and collaborative virtual environments for real-time performance analysis. He led the Pablo Research Group, which investigates the interaction of architecture, system software, and applications on large-scale parallel and distributed computer systems. The group created SvPablo, a graphical environment for instrumenting application source code and browsing dynamic performance data. Key research foci of the group included exploration of performance analysis techniques and compiler-aided scalability analysis, scalable parallel file systems, and real-time adaptive systems for resource policy control.",
            "score": 97.55740773677826
        },
        {
            "docid": "28157027_2",
            "document": "Crystallographic image processing . Crystallographic image processing (CIP) is traditionally understood as being a set of key steps in the determination of the atomic structure of crystalline matter from high-resolution electron microscopy (HREM) images obtained in a transmission electron microscope (TEM) that is run in the parallel illumination mode. The term was created in the research group of Sven Hovm\u00f6ller at Stockholm University during the early 1980s and became rapidly a label for the \"3D crystal structure from 2D transmission/projection images\" approach. Since the late 1990s, analogous and complementary image processing techniques that are directed towards the achieving of goals with are either complementary or entirely beyond the scope of the original inception of CIP have been developed independently by members of the computational symmetry/geometry, scanning transmission electron microscopy, scanning probe microscopy communities, and applied crystallography communities.",
            "score": 115.7244143486023
        },
        {
            "docid": "2235991_4",
            "document": "Transmission electron cryomicroscopy . The resolution of CryoTEM maps is improving steadily, and in 2014 some structures at near-atomic resolution had been obtained, including those of viruses, ribosomes, mitochondria, ion channels, and enzyme complexes as small as 170 kDa at a resolution of 4.5 \u00c5. Bridget Carragher and colleagues at the Scripps National Resource for Automated Molecular Microscopy used techniques she and Clint Potter developed to create the first transmission electron cryomicroscopy structural biology image with a resolution finer than 3 \u00c5ngstr\u00f6ms, thereby elevating CryoTEM as a tool comparable to and potentially superior to traditional x-ray crystallography techniques. A 2.2 \u00c5 map of a bacterial enzyme beta-galactosidase was published in June 2015. An application of CryoTEM is electron cryotomography (CryoET), where a 3D reconstruction of a sample is created from tilted 2D images.",
            "score": 97.30194544792175
        },
        {
            "docid": "46877391_18",
            "document": "Precession electron diffraction . The first precession electron diffraction system was developed by Vincent and Midgley in Bristol, UK and published in 1994. Preliminary investigation into the ErGeO crystal structure demonstrated the feasibility of the technique at reducing dynamical effects and providing quasi-kinematical patterns that could be solved through direct methods to determine crystal structure. Over the next ten years, a number of university groups developed their own precession systems and verified the technique by solving complex crystal structures, including the groups of J. Gjonnes (Oslo), Migliori (Bologna), and L. Marks (Northwestern). In 2004, NanoMEGAS developed the first commercial procession system capable of being retrofit to any modern TEM. This hardware solution enabled more widespread implementation of the technique and spurred its more mainstream adoption into the crystallography community. Software methods have also been developed to achieve the necessary scanning and descanning using the built-in electronics of the TEM. HREM Research Inc has developed the QED plug-in for the DigitalMicrograph software. This plug-in enables the widely used software package to collect precession electron diffraction patterns without additional modifications to the microscope.",
            "score": 105.49838352203369
        },
        {
            "docid": "42177410_13",
            "document": "DNA base flipping . X-ray crystallography is a technique that measures the angles and intensities of crystalline atoms in order to determine the atomic and molecular structure of the crystal of interest. Crystallographers are then able to produce and three-dimensional picture where the positions of the atoms, chemical bonds as well as other important characteristics can be determined. Klimasaukas and colleagues used this technique to observe the first base flipping phenomenon, in which their experimental procedure involved several steps:  During purification, Haemophilus haemolyticus methyltransferase was overexpressed and purified using a high salt back-extraction step to selectively solubilize M.HhaI, followed by fast protein liquid chromatography (FPLC) as done previously by Kumar and colleagues. Authors utilized a Mono-Q anion exchange column to remove the small quantity of proteinaceous materials and unwanted DNA prior to the crystallization step. Once M.HhaI was successfully purified, the sample was then grown using a method that mixes the solution containing the complex at a temperature of 16\u00a0\u00b0C and the hanging-drop vapor diffusion technique to obtain the crystals. Authors were then able to collect the x-ray data according to a technique used by Cheng and colleagues in 1993. This technique involved the measurement of the diffraction intensities on a FAST detector, where the exposure times for 0.1\u00b0 rotation were 5 or 10 seconds. For the structure determination and refinement, Klimasaukas and colleagues used the molecular replacement of the refined apo structure described by Cheng and colleagues in 1993 where the search models X-PLOR, MERLOT, and TRNSUM were used to solve the rotation and translation functions. This part of the study involves the use of a variety of software and computer algorithms to solve the structures and characteristics of the crystal of interest.",
            "score": 99.53894639015198
        },
        {
            "docid": "8881792_4",
            "document": "Joachim St\u00f6hr . Prof. St\u00f6hr\u2019s research has focused on the development of novel investigative techniques based on soft x-ray synchrotron radiation for exploring the structure, electronic and magnetic properties of surfaces and thin films. He played a major role in developing the surface extended x-ray absorption fine structure (SEXAFS) technique as a tool for exploring surface structures, especially atoms bonded to surfaces. He also developed the near edge x-ray absorption fine structure (NEXAFS) technique for the study of simple and complex molecules bonded to surfaces and for the study of thin polymer films. The technique is described in his book \u201cNEXAFS Spectroscopy\u201d (Springer, 1992). NEXAFS is widely used today, often in combination with x-ray microscopes, for the study of organic systems like polymers and biological cells.",
            "score": 103.09932076931
        },
        {
            "docid": "19151674_17",
            "document": "Interaction technique . A large part of research in human-computer interaction involves exploring easier-to-learn or more efficient interaction techniques for common computing tasks. This includes inventing new (post-WIMP) interaction techniques, possibly relying on methods from user interface design, and assessing their efficiency with respect to existing techniques using methods from experimental psychology. Examples of scientific venues in these topics are the UIST and the CHI conferences. Other research focuses on the specification of interaction techniques, sometimes using formalisms such as Petri nets for the purposes of formal verification.",
            "score": 70.47087025642395
        },
        {
            "docid": "6782470_8",
            "document": "Jury research . Similarly methodology will differ depending on terms of reference, level of peer review, experience and ability of researchers and attitudes of funding organisations. Today, there are many companies and individuals providing services as jury consultants or trial strategists [See for example , & for links to three randomly selected organisations that provide such services]. Jury consultants use market research techniques in an attempt to bolster the client's chances of a positive outcome by 'gaining an understanding of the current and environment/location specific trends that might impact the attitudes of jurors. Their job is to shape trial team strategy so as to moderate or take advantage of jurors' preexisting beliefs and experiences by way of how the evidence is presented. Regular market research techniques are used in such instances (e.g., phone surveys, focus groups, feedback sessions etc.). Surrogate or mock jurors are selected carefully so that they are statistically representative of the 'general population in the particular region' and they are presented relevant information, visual exhibits, witness statements, legal cases, timelines etc. in an attempt to elicit a variety of responses, thus allowing the lawyers to prepare adequately for any possibility before it causes difficulty to their case. Jury consultants also use pre-trial techniques such as focus groups when preparing for settlement negotiations. Post-trial juror interviews sometimes allow for better understanding of mistakes or good arguments made in a trial, and that knowledge could be used for future trials or for an appeal. A recent innovation in this type of research is using electronic resources where lawyers 'pitch their submissions to online jurors'. More information about this resource can be found by following this link . Also on these pages are papers which outline some of the perceived benefits of this type of research along with the issues which are still topics of debate.",
            "score": 74.09380257129669
        },
        {
            "docid": "1181008_10",
            "document": "Computational science . Exciting new developments in biotechnology are now revolutionizing biology and biomedical research. Examples of these techniques are high-throughput sequencing, high-throughput quantitative PCR, intra-cellular imaging, in-situ hybridization of gene expression, three-dimensional imaging techniques like Light Sheet Fluorescence Microscopy and Optical Projection, (micro)-Computer Tomography. Given the massive amounts of complicated data that is generated by these techniques, their meaningful interpretation, and even their storage, form major challenges calling for new approaches. Going beyond current bioinformatics approaches, computational biology needs to develop new methods to discover meaningful patterns in these large data sets. Model-based reconstruction of gene networks can be used to organize the gene expression data in systematic way and to guide future data collection. A major challenge here is to understand how gene regulation is controlling fundamental biological processes like biomineralisation and embryogenesis. The sub-processes like gene regulation, organic molecules interacting with the mineral deposition process, cellular processes, physiology and other processes at the tissue and environmental levels are linked. Rather than being directed by a central control mechanism, biomineralisation and embryogenesis can be viewed as an emergent behavior resulting from a complex system in which several sub-processes on very different temporal and spatial scales (ranging from nanometer and nanoseconds to meters and years) are connected into a multi-scale system. One of the few available options to understand such systems is by developing a multi-scale model of the system.",
            "score": 126.23307597637177
        },
        {
            "docid": "55633332_5",
            "document": "Junichiro Kono . Prof. Kono's research is currently focused on the physics and applications of semiconductor nanostructures and quantum device structures. They use state-of-the-art spectroscopic techniques to study charge, spin, and vibrational dynamics in a variety of nanostructures. The impact of their research includes: increased understanding of the quantum states and dynamics of interacting, confined, or strongly driven electrons in nanostructures; new spectroscopy techniques; novel device concepts and implementations (especially towards all-optical switches and spin-based devices); establishment of the quantum nature of semiconductor-light interaction; progress towards the solid-state realization of quantum information processing, computation and communications; and provision of a controlled environment in which to address unanswered questions in many-body physics. Click here to read more about the Kono Lab.",
            "score": 77.67976033687592
        },
        {
            "docid": "52922245_3",
            "document": "Correlative light-electron microscopy . This technique is used in order to obtain information at different length scales: the electron microscope provides high-resolution information down to the nano-scale, while the fluorescence microscope highlights the regions of interest. CLEM is used for various disciplines in the life sciences, including neuroscience, tissue research, and protein research.",
            "score": 107.47148513793945
        },
        {
            "docid": "35154335_10",
            "document": "Phase-contrast X-ray imaging . The latest approach discussed here is the so-called grating-based imaging, which makes use of the Talbot effect, discovered by Henry Fox Talbot in 1836. This self-imaging effect creates an interference pattern downstream of a diffraction grating. At a particular distance this pattern resembles exactly the structure of the grating and is recorded by a detector. The position of the interference pattern can be altered by bringing an object in the beam, that induces a phase shift. This displacement of the interference pattern is measured with the help of a second grating, and by certain reconstruction methods, information about the real part of the refractive index is gained. The so-called Talbot\u2013Lau interferometer was initially used in atom interferometry, for instance by John F. Clauser and Shifang Li in 1994. The first X-ray grating interferometers using synchrotron sources were developed by Christian David and colleagues from the Paul Scherrer Institute (PSI) in Villingen, Switzerland and the group of Atsushi Momose from the University of Tokyo. In 2005, independently from each other, both David's and Momose's group incorporated computed tomography into grating interferometry, which can be seen as the next milestone in the development of grating-based imaging. In 2006, another great advancement was the transfer of the grating-based technique to conventional laboratory X-ray tubes by Franz Pfeiffer and co-workers, which fairly enlarged the technique's potential for clinical use. About two years later the group of Franz Pfeiffer also accomplished to extract a supplementary signal from their experiments; the so-called \"dark-field signal\" was caused by scattering due to the porous microstructure of the sample and provided \"complementary and otherwise inaccessible structural information about the specimen at the micrometer and submicrometer length scale\". At the same time, Han Wen and co-workers at the US National Institutes of Health arrived at a much simplified grating technique to obtain the scattering (\u201cdark-field\u201d) image. They used a single projection of a grid and a new approach for signal extraction named \"single-shot Fourier analysis\". Recently, a lot of research was done to improve the grating-based technique: Han Wen and his team analyzed animal bones and found out that the intensity of the dark-field signal depends on the orientation of the grid and this is due to the anisotropy of the bone structure. They made significant progress towards biomedical applications by replacing mechanical scanning of the gratings with electronic scanning of the X-ray source. The grating-based phase-contrast CT field was extended by tomographic images of the dark-field signal and time-resolved phase-contrast CT. Furthermore, the first pre-clinical studies using grating-based phase-contrast X-ray imaging were published. Marco Stampanoni and his group examined native breast tissue with \"differential phase-contrast mammography\", and a team led by Dan Stutman investigated how to use grating-based imaging for the small joints of the hand.",
            "score": 105.65327775478363
        },
        {
            "docid": "45350085_6",
            "document": "Visual computing . At least the following disciplines are sub-fields of visual computing. More detailed descriptions of each of these fields can be found on the linked special pages. Computer graphics is a general term for all techniques that produce images as result with the help of a computer. To transform the description of objects to nice images is called rendering which is always a compromise between image quality and run-time. Techniques that can extract content information from images are called image analysis techniques. Computer vision is the ability of computers (or of robots) to recognize their environment and to interpret it correctly. Visualization is used to produce images that shall communicate messages. Data may be abstract or concrete, often with no a priori geometrical components. Visual analytics describes the discipline of interactive visual analysis of data, also described as \u201cthe science of analytical reasoning supported by the interactive visual interface\u201d. To represent objects for rendering it needs special methods and data structures, which subsumed with the term geometric modeling. In addition to describing and interactive geometric techniques, sensor data are more and more used to reconstruct geometrical models. Algorithms for the efficient control of 3D printers also belong to the field of visual computing. In contrast to image analysis image processing manipulates images to produce better images. \u201cBetter\u201d can have very different meanings subject to the respective application. Also, it has to be discriminated from image editing which describes interactive manipulation of images based on human validation. Techniques that produce the feeling of immersion into a fictive world are called virtual reality (VR). Requirements for VR include head-mounted displays, real-time tracking, and high-quality real-time rendering. Augmented reality enables the user to see the real environment in addition to the virtual objects, which augment this reality. Accuracy requirements on rendering speed and tracking precision are significantly higher here. The planning, design and uses of interfaces between people and computers is not only part of every system involving images. Due to the high bandwidth of the human visual channel (eye), images are also a preferred part of ergonomic user interfaces in any system, so that human-computer interaction is also an integral part of visual computing.",
            "score": 140.38742589950562
        },
        {
            "docid": "45511007_8",
            "document": "Vijay Vaishnavi . The work of Vaishnavi in this area has mainly focused on computational geometry problems and the creation of efficient new data structures for multidimensional and weighted data. In the computational geometry area, Vaishnavi was among early researchers who developed and used techniques for efficiently locating a key in many ordered lists\u2014a problem that frequently arises in computational geometry. In this regard, Mehlhorn and N\u00e4her write (on p.\u00a0215 of their 1990 article) that several researchers including Vaishnavi and Wood \"observed that the na\u00efve strategy of locating the key separately in each list by binary search is far from optimal and that more efficient techniques frequently exist.\" They further write that Chazelle and Guibas \"distilled from these special case solutions a general data structuring technique and called it fractional cascading.\"",
            "score": 95.49527478218079
        },
        {
            "docid": "32816134_16",
            "document": "List of Folding@home cores . The software for this core was developed at D. E. Shaw Research. Desmond performs high-speed molecular dynamics simulations of biological systems on conventional computer clusters. The code uses novel parallel algorithms and numerical techniques to achieve high performance on platforms containing a large number of processors, but may also be executed on a single computer. Desmond and its source code are available without cost for non-commercial use by universities and other not-for-profit research institutions.",
            "score": 134.82250356674194
        },
        {
            "docid": "2695450_13",
            "document": "Virtual museum . There are several types of interactive environments. One is to re-create 3D space with visual representations of the museum by a 3D architectural metaphor, which provides a sense of place using various spatial references. They usually use 3D modelling, VRML (Virtual Reality Modelling Language) and now X3D(successor to VRML) for viewing. There have been introduced various kinds of imaging techniques for building virtual museums, such as infrared reflectography, X-ray imaging, 3D laser scanning, IBMR (Image Based Rendering and Modeling) techniques. In the case of EU-funded projects, the ViHAP3D, a new virtual reality system for scanning museum artifacts, has been developed by EU researchers. Another interactive three-dimensional spatial environment is QTVR. Being a pre-rendered, fixed environment it is more restricted in regards to moving freely around in 3D space but the image quality can be superior to that of real-time rendered environments. This was especially the case in the mid-1990s when computing power and online speeds were limited.",
            "score": 103.58398640155792
        },
        {
            "docid": "9806826_4",
            "document": "John Kuriyan . Kuriyan did postdoctoral research work for one year supervised by Karplus at Harvard before becoming an assistant professor at the Rockefeller University. Kuriyan's laboratory studies the structure and mechanism of enzymes and other proteins that transduce cellular signals and perform DNA replication. The laboratory primarily uses x-ray crystallography to determine 3-D protein structures as well as biochemical, biophysical, and computational techniques to uncover the mechanisms used by these proteins.",
            "score": 91.36799216270447
        },
        {
            "docid": "11220518_2",
            "document": "Search-based software engineering . Search-based software engineering (SBSE) applies metaheuristic search techniques such as genetic algorithms, simulated annealing and tabu search to software engineering problems. Many activities in software engineering can be stated as optimization problems. Optimization techniques of operations research such as linear programming or dynamic programming are often impractical for large scale software engineering problems because of their computational complexity. Researchers and practitioners use metaheuristic search techniques to find near-optimal or \"good-enough\" solutions.",
            "score": 80.61834216117859
        },
        {
            "docid": "29915619_5",
            "document": "Educational data mining . While the analysis of educational data is not itself a new practice, recent advances in educational technology, including the increase in computing power and the ability to log fine-grained data about students' use of a computer-based learning environment, have led to an increased interest in developing techniques for analyzing the large amounts of data generated in educational settings. This interest translated into a series of EDM workshops held from 2000 to 2007 as part of several international research conferences. In 2008, a group of researchers established what has become an annual international research conference on EDM, the first of which took place in Montreal, Quebec, Canada.",
            "score": 81.25138854980469
        },
        {
            "docid": "838449_30",
            "document": "Atheroma . One way to see atheroma is the very invasive and costly IVUS ultrasound technology; it gives us the precise volume of the inside intima plus the central media layers of about of artery length. Unfortunately, it gives no information about the structural strength of the artery. Angiography does not visualize atheroma; it only makes the blood flow within blood vessels visible. Alternative methods that are non or less physically invasive and less expensive per individual test have been used and are continuing to be developed, such as those using computed tomography (CT; led by the electron beam tomography form, given its greater speed) and magnetic resonance imaging (MRI). The most promising since the early 1990s has been EBT, detecting calcification within the atheroma before most individuals start having clinically recognized symptoms and debility. Interestingly, statin therapy (to lower cholesterol) does not slow the speed of calcification as determined by CT scan. MRI coronary vessel wall imaging, although currently limited to research studies, has demonstrated the ability to detect vessel wall thickening in asymptomatic high risk individuals. As a non-invasive, ionising radiation free technique, MRI based techniques could have future uses in monitoring disease progression and regression. Most visualization techniques are used in research, they are not widely available to most patients, have significant technical limitations, have not been widely accepted and generally are not covered by medical insurance carriers.",
            "score": 113.85271406173706
        },
        {
            "docid": "4610407_2",
            "document": "Magnetic resonance force microscopy . Magnetic resonance force microscopy (MRFM) is an imaging technique that acquires magnetic resonance images (MRI) at nanometer scales, and possibly at atomic scales in the future. MRFM is potentially able to observe protein structures which cannot be seen using X-ray crystallography and protein nuclear magnetic resonance spectroscopy. Detection of the magnetic spin of a single electron has been demonstrated using this technique. The sensitivity of a current MRFM microscope is 10 billion times better than a medical MRI used in hospitals.",
            "score": 114.11397886276245
        },
        {
            "docid": "11453612_3",
            "document": "Hemispherical photography . The hemispherical lens (also known as a fisheye or whole-sky lens) was originally designed by Robin Hill (1924) to view the entire sky for meteorological studies of cloud formation. Foresters and ecologists conceived of using photographic techniques to study the light environment in forests by examining the canopy geometry. In particular, Evans and Coombe (1959) estimated sunlight penetration through forest canopy openings by overlaying diagrams of the sun track on hemispherical photographs. Later, Margaret Anderson (1964, 1971) provided a thorough theoretical treatment for calculating the transmission of direct and diffuse components of solar radiation through canopy openings using hemispherical photographs. At that time hemispherical photograph analysis required tedious manual scoring of overlays of sky quadrants and the track of the sun. With the advent of personal computers, researchers developed digital techniques for rapid analysis of hemispherical photographs (Chazdon and Field 1987, Rich 1988, 1989, 1990, Becker et al. 1989). In recent years, researchers have started using digital cameras in favor of film cameras, and algorithms are being developed for automated image classification and analysis. Various commercial software programs have become available for hemispherical photograph analysis, and the technique has been applied for diverse uses in ecology, meteorology, forestry, and agriculture.",
            "score": 86.27481305599213
        },
        {
            "docid": "50982_73",
            "document": "CT scan . Photon counting computed tomography is a CT technique currently under development. Typical CT scanners use energy integrating detectors; photons are measured as a voltage on a capacitor which is proportional to the x-rays detected. However, this technique is susceptible to noise and other factors which can affect the linearity of the voltage to x-ray intensity relationship. Photon counting detectors (PCDs) are still affected by noise but it does not change the measured counts of photons. PCDs have several potential advantages including improving signal (and contrast) to noise ratios, reducing doses, improving spatial resolution and, through use of several energies, distinguishing multiple contrast agents. PCDs have only recently become feasible in CT scanners due to improvements in detector technologies that can cope with the volume and rate of data required. As of February 2016 photon counting CT is in use at three sites. Some early research has found the dose reduction potential of photon counting CT for breast imaging to be very promising.",
            "score": 78.2870762348175
        },
        {
            "docid": "277702_22",
            "document": "Electron diffraction . In a TEM, a single crystal grain or particle may be selected for the diffraction experiments. This means that the diffraction experiments can be performed on single crystals of nanometer size, whereas other diffraction techniques would be limited to studying the diffraction from a multicrystalline or powder sample. Furthermore, electron diffraction in TEM can be combined with direct imaging of the sample, including high resolution imaging of the crystal lattice, and a range of other techniques. These include solving and refining crystal structures by electron crystallography, chemical analysis of the sample composition through energy-dispersive X-ray spectroscopy, investigations of electronic structure and bonding through electron energy loss spectroscopy, and studies of the mean inner potential through electron holography.",
            "score": 91.86481595039368
        }
    ],
    "r": [
        {
            "docid": "26901564_33",
            "document": "DNA nanotechnology . Nucleic acid structures can be directly imaged by atomic force microscopy, which is well suited to extended two-dimensional structures, but less useful for discrete three-dimensional structures because of the microscope tip's interaction with the fragile nucleic acid structure; transmission electron microscopy and cryo-electron microscopy are often used in this case. Extended three-dimensional lattices are analyzed by X-ray crystallography.",
            "score": 168.83807373046875
        },
        {
            "docid": "40618482_4",
            "document": "Henning Stahlberg . Henning Stahlberg studies biological membrane systems from the atomic structure of individual proteins to the cellular context of the system. Employing light and electron microscopy, x- ray crystallography, and electron tomography, he investigates the structure of ion channels and membrane transporters. Further, he develops computational algorithms for the 3D reconstruction of proteins. Using these methods, Stahlberg successfully elucidated among other things the 3D structure of the Type III Secretion System (T3SS) in intact bacteria. In addition he demonstrated that certain proteins at the base of the T3SS needle are elastic. He developed the 2dx software to reconstruct the 3D structure of membrane proteins from cryo-electron microscopy images of 2D crystals.",
            "score": 168.68832397460938
        },
        {
            "docid": "42772736_6",
            "document": "Chaetomium thermophilum . The first observation of a metabolon in fatty acid metabolism at high resolution came from cryo-electron microscopic analysis of cellular homogenates from \"C. thermophilum\". Other protein complexes and higher-order interactions were predicted and validated using a novel integration of experimental and computational methods which include proteomics, cross-linking mass spectrometry, computational structural biology and electron microscopy. The cellular homogenates of \"C. thermophilum\" were used to derive protein complex structures at high resolution, with purity of the native protein complex < 40%.",
            "score": 168.21340942382812
        },
        {
            "docid": "28034_18",
            "document": "Scanning electron microscope . If the SEM is equipped with a cold stage for cryo microscopy, cryofixation may be used and low-temperature scanning electron microscopy performed on the cryogenically fixed specimens. Cryo-fixed specimens may be cryo-fractured under vacuum in a special apparatus to reveal internal structure, sputter-coated and transferred onto the SEM cryo-stage while still frozen. Low-temperature scanning electron microscopy (LT-SEM) is also applicable to the imaging of temperature-sensitive materials such as ice and fats.",
            "score": 167.4259490966797
        },
        {
            "docid": "1823144_20",
            "document": "Scanning transmission electron microscopy . Cryo-electron microscopy in STEM (Cryo-STEM) allows specimens to be held in the microscope at liquid nitrogen or liquid helium temperatures. This is useful for imaging specimens that would be volatile in high vacuum at room temperature. Cryo-STEM has been used to study vitrified biological samples, vitrified solid-liquid interfaces in material specimens, and specimens containing elemental sulfur, which is prone to sublimation in electron microscopes at room temperature.",
            "score": 163.53414916992188
        },
        {
            "docid": "55172_50",
            "document": "Proteomics . The biomolecular structure forms the 3D configuration of the protein. Understanding the protein's structure aids in identification of the protein's interactions and function. It used to be that the 3D structure of proteins could only be determined using X-ray crystallography and NMR spectroscopy. As of 2017, Cryo-electron microscopy is a leading technique, solving difficulties with crystallization (in X-ray crystallography) and conformational ambiguity (in NMR); resolution was 2.2\u00c5 as of 2015. Now, through bioinformatics, there are computer programs that can in some cases predict and model the structure of proteins. These programs use the chemical properties of amino acids and structural properties of known proteins to predict the 3D model of sample proteins. This also allows scientists to model protein interactions on a larger scale. In addition, biomedical engineers are developing methods to factor in the flexibility of protein structures to make comparisons and predictions.",
            "score": 163.10206604003906
        },
        {
            "docid": "10935624_3",
            "document": "Kenneth Roux . Roux was a member of the research team (along with his research associate Ping Zhu) that used negative stain electron microscopy and cryo-electron microscopy coupled with tomography to produce the first detailed 3-D images of the surface of the AIDS viruses, revealing spike proteins. They show that the three gp120 proteins in each spike consist of a lobed head and a three-legged stalk - and use comparisons with atomic structures to gain insight into the mechanism of fusion. The images produced in his research of HIV structure and genome provide important insights for the development of vaccines that will thwart infection by targeting and crippling the sticky HIV-1 spike proteins.",
            "score": 162.3726043701172
        },
        {
            "docid": "303544_21",
            "document": "Lawrence Bragg . X-ray analysis of protein structure flourished in subsequent years, determining the structures of scores of proteins in laboratories around the world. Twenty eight Nobel Prizes have been awarded for work using X-ray analysis. The disadvantage of the method is that it must be done on crystals, which precludes seeing changes in shape when enzymes bind substrates and the like. This problem was solved by the development of another line Bragg had initiated, using modified electron microscopes to image single frozen molecules: cryo-electron microscopy.",
            "score": 160.87149047851562
        },
        {
            "docid": "3690047_9",
            "document": "Electron cryotomography . A major obstacle in CryoET is identifying structures of interest within complicated cellular environments. One solution is to apply correlated cryo-fluorescence light microscopy, and even super-resolution light microscopy (e.g. cryo-PALM), and CryoET. In these techniques, a sample containing a fluorescently-tagged protein of interest is plunge-frozen and first imaged in a light microscope equipped with a special stage to allow the sample to be kept at sub-crystallization temperatures (<-150\u00a0\u00b0C). The location of the fluorescent signal is identified and the sample is transferred to the CryoTEM, where the same location is then imaged at high resolution by CryoET.",
            "score": 160.18963623046875
        },
        {
            "docid": "4487041_8",
            "document": "Chlorosome . To create the mutant, three genes were inactivated that green sulfur bacteria acquired late in their evolution. In this way it was possible to go backward in evolutionary time to an intermediate state with much less variable and better ordered chlorosome organelles than the wild-type. The chlorosomes were isolated from the mutant and the wild-type forms of the bacteria. Cryo-electron microscopy was used to take pictures of the chlorosomes. The images reveal that the chlorophyll molecules inside chlorosomes have a nanotube shape. The team then used MAS NMR spectroscopy to resolve the microscopic arrangement of chlorophyll inside the chlorosome. With distance constraints and DFT ring current analyses the organization was found to consist of unique syn-anti monomer stacking. The combination of NMR, cryo-electron microscopy and modeling enabled the scientists to determine that the chlorophyll molecules in green sulfur bacteria are arranged in helices. In the mutant bacteria, the chlorophyll molecules are positioned at a nearly 90-degree angle in relation to the long axis of the nanotubes, whereas the angle is less steep in the wild-type organism. The structural framework can accommodate disorder to improve the biological light harvesting function, which implies that a less ordered structure has a better performance.",
            "score": 159.10086059570312
        },
        {
            "docid": "8898050_2",
            "document": "Electron tomography . Electron tomography (ET) is a tomography technique for obtaining detailed 3D structures of sub-cellular macro-molecular objects. Electron tomography is an extension of traditional transmission electron microscopy and uses a transmission electron microscope to collect the data. In the process, a beam of electrons is passed through the sample at incremental degrees of rotation around the center of the target sample. This information is collected and used to assemble a three-dimensional image of the target. For biological applications, the typical resolution of ET systems are in the 5\u201320 nm range, suitable for examining supra-molecular multi-protein structures, although not the secondary and tertiary structure of an individual protein or polypeptide.",
            "score": 157.3948516845703
        },
        {
            "docid": "305927_10",
            "document": "Vitrification . Vitrification can also occur when starting with a liquid such as water, usually through very rapid cooling or the introduction of agents that suppress the formation of ice crystals. This is in contrast to ordinary freezing which results in ice crystal formation. Additives used in cryobiology or produced naturally by organisms living in polar regions are called cryoprotectants. Vitrification is used in cryo-electron microscopy to cool samples so quickly that they can be imaged with an electron microscope without damage. In 2017, the Nobel prize for chemistry was awarded for the development of this technology, which can be used to image objects such as proteins or virus particles.",
            "score": 156.1508331298828
        },
        {
            "docid": "55440192_6",
            "document": "Jacques Dubochet . During his career, Jacques Dubochet developed technologies in cryo-electron microscopy, cryo-electron tomography and cryo-electron microscopy of vitreous sections. These technologies are used to image individual biological structures such as protein complexes or virus particles. At Lausanne he took part in initiatives to make scientists more aware of social issues.",
            "score": 155.57693481445312
        },
        {
            "docid": "54895550_3",
            "document": "Liquid-Phase Electron Microscopy . The ability to study liquid samples, particularly those involving water, with electron microscopy has been a wish ever since the early days of electron microscopy but technical difficulties prevented early attempts from achieving high resolution. Two basic approaches exist for imaging liquid specimens: i) closed systems, mostly referred to as liquid cell EM (LC EM), and ii) open systems, often referred to as environmental systems. In closed systems, thin windows made of materials such as silicon nitride or graphene are used to enclose a liquid for placement in the microscope vacuum. Closed cells have found widespread use in the past decade due to the availability of reliable window microfabrication technology. Graphene provides the thinnest possible window. The oldest open system that gained widespread usage was environmental scanning electron microscopy (ESEM) of liquid samples on a cooled stage in a vacuum chamber containing a background pressure of vapor. Low vapor pressure liquids such as ionic liquids can also be studied in open systems. LP-EM systems of both open and closed type have been developed for all three main types of electron microscopy, i.e., transmission electron microscopy (TEM), scanning transmission electron microscopy (STEM), and scanning electron microscope (SEM). Instruments integrating liquid-phase SEM with light microscopy have also been developed. Electron microscopic observation in liquid has been combined with other analytical methods such as electrochemical measurements and energy-dispersive X-ray spectroscopy (EDX).",
            "score": 155.248291015625
        },
        {
            "docid": "57634074_4",
            "document": "Lori Passmore . Passmore worked as a postdoctoral researcher at the MRC-LMB. She worked with Venki Ramakrishnan and Richard Henderson, using cryo-EM to determine the structure of the eukaryotic ribosome bound to initation factors. Passmore became a group leader at the MRC-LMB in 2009. Her group uses in vitro reconstitution, biochemical assays and cryo-EM to understand the function of multiprotein complexes that regulate gene expression at the level of mRNA. Her group has developed new sample grids for cryo-EM that reduce specimen movement during imaging.",
            "score": 155.1385498046875
        },
        {
            "docid": "23890524_11",
            "document": "Eukaryotic ribosome (80S) . Protein synthesis is primarily regulated at the stage of translation initiation. In eukaryotes, the canonical initiation pathway requires at least 12 protein initiation factors, some of which are themselves large complexes. The structures of the 40S:eIF1 and 60S:eIF6 complexes provide first detailed insights into the atomic interactions between the eukaryotic ribosome and regulatory factors. eIF1 is involved in start codon selection, and eIF6 sterically precludes the joining of subunits. However, structural information on the eukaryotic initiation factors and their interactions with the ribosome is limited and largely derived from homology models or low-resolution analyses. Elucidation of the interactions between the eukaryotic ribosome and initiation factors at an atomic level is essential for a mechanistic understanding of the regulatory processes, but represents a significant technical challenge, because of the inherent dynamics and flexibility of the initiation complexes. The first structure of the mammalian pre initiation complex was done by cryo-electron microscopy. Other structures of initiation complexes followed soon, driven by cryo-EM technical improvements. Those structures will help better understand the process of translation initiation in eukaryotes.",
            "score": 152.3829345703125
        },
        {
            "docid": "904792_9",
            "document": "X-ray microscope . The resolution of X-ray microscopy lies between that of the optical microscope and the electron microscope. It has an advantage over conventional electron microscopy in that it can view biological samples in their natural state. Electron microscopy is widely used to obtain images with nanometer to sub-Angstrom level resolution but the relatively thick living cell cannot be observed as the sample has to be chemically fixed, dehydrated, embedded in resin, then sliced ultra thin. However, it should be mentioned that cryo-electron microscopy allows the observation of biological specimens in their hydrated natural state, albeit embedded in water ice. Until now, resolutions of 30 nanometer are possible using the Fresnel zone plate lens which forms the image using the soft x-rays emitted from a synchrotron. Recently, the use of soft x-rays emitted from laser-produced plasmas rather than synchrotron radiation is becoming more popular.",
            "score": 152.35499572753906
        },
        {
            "docid": "33904406_21",
            "document": "Length measurement . Measuring dimensions of localized structures (as opposed to large arrays of atoms like a crystal), as in modern integrated circuits, is done using the scanning electron microscope. This instrument bounces electrons off the object to be measured in a high vacuum enclosure, and the reflected electrons are collected as a photodetector image that is interpreted by a computer. These are not transit-time measurements, but are based upon comparison of Fourier transforms of images with theoretical results from computer modeling. Such elaborate methods are required because the image depends on the three-dimensional geometry of the measured feature, for example, the contour of an edge, and not just upon one- or two-dimensional properties. The underlying limitations are the beam width and the wavelength of the electron beam (determining diffraction), determined, as already discussed, by the electron beam energy.  The calibration of these scanning electron microscope measurements is tricky, as results depend upon the material measured and its geometry. A typical wavelength is and a typical resolution is about",
            "score": 150.67071533203125
        },
        {
            "docid": "4747255_8",
            "document": "Soft x-ray microscopy . The resolution of X-ray microscopy lies between that of the optical microscope and the electron microscope. It has an advantage over conventional electron microscopy in that it can view biological samples in their natural state. Electron microscopy is widely used to obtain images with nanometer level resolution but the relatively thick living cell cannot be observed as the sample has to be chemically fixed, dehydrated, embedded in resin, then sliced ultra thin. However, it should be mentioned that cryo-electron microscopy allows the observation of biological specimens in their hydrated natural state, albeit embedded in water ice. Until now, resolutions of 30 nanometer are possible using the Fresnel zone plate lens which forms the image using the soft x-rays emitted from a synchrotron. Recently, the use of soft x-rays emitted from laser-produced plasmas rather than synchrotron radiation is becoming more popular.",
            "score": 149.24856567382812
        },
        {
            "docid": "36128950_6",
            "document": "Macromolecular assembly . The study of MA structure and function is challenging, in particular because of their megadalton size, but also because of their complex compositions and varying dynamic natures. Most have had standard chemical and biochemical methods applied (methods of protein purification and centrifugation, chemical and electrochemical characterization, etc.). In addition, their methods of study include modern proteomic approaches, computational and atomic-resolution structural methods (e.g., X-ray crystallography), small-angle X-ray scattering (SAXS) and small-angle neutron scattering (SANS), force spectroscopy, and transmission electron microscopy and cryo-electron microscopy. Aaron Klug was recognized with the 1982 Nobel Prize in Chemistry for his work on structural elucidation using electron microscopy, in particular for protein-nucleic acid MAs including the tobacco mosaic virus (a structure containing a 6400 base ssRNA molecule and >2000 coat protein molecules). The crystallization and structure solution for the ribosome, MW ~ 2.5 MDa, an example of part of the protein synthetic 'machinery' of living cells, was object of the 2009 Nobel Prize in Chemistry awarded to Venkatraman Ramakrishnan, Thomas A. Steitz, and Ada E. Yonath.",
            "score": 149.24522399902344
        },
        {
            "docid": "52311495_3",
            "document": "Paul Emsley (crystallographer) . Emsley is the primary author of the model-building software Coot, a tool for building models of proteins whose three dimensional structures are determined via X-ray crystallography or cryo-EM. These protein structures are deposited at the Worldwide Protein Data Bank for collaboration among scientists. Since its introduction in 2004, Coot has remained as free software for use in industrial and academic research groups.",
            "score": 148.81655883789062
        },
        {
            "docid": "1822961_8",
            "document": "Electron crystallography . There was a serious disagreement in the field of electron microscopy of inorganic compounds; while some have claimed that \"the phase information is present in EM images\" others have the opposite view that \"the phase information is lost in EM images\". The reason for these opposite views is that the word \"phase\" has been used with different meanings in the two communities of physicists and crystallographers. The physicists are more concerned about the \"electron wave phase\" - the phase of a wave moving through the sample during exposure by the electrons. This wave has a wavelength of about 0.02-0.03 \u00c5ngstr\u00f6m (depending on the accelerating voltage of the electron microscope). Its phase is related to the phase of the undiffracted direct electron beam. The crystallographers, on the other hand, mean the \"crystallographic structure factor phase\" when they simply say \"phase\". This phase is the phase of standing waves of potential in the crystal (very similar to the electron density measured in X-ray crystallography). Each of these waves have their specific wavelength, called d-value for distance between so-called Bragg planes of low/high potential. These d-values range from the unit cell dimensions to the resolution limit of the electron microscope, i.e. typically from 10 or 20 \u00c5ngstr\u00f6ms down to 1 or 2 \u00c5ngstr\u00f6ms. Their phases are related to a fixed point in the crystal, defined in relation to the symmetry elements of that crystal. The crystallographic phases are a property of the crystal, so they exist also outside the electron microscope. The electron waves vanish if the microscope is switched off. In order to determine a crystal structure, it is necessary to know the crystallographic structure factors, but not to know the electron wave phases. A more detailed discussion how (crystallographic structure factor) phases link with the phases of the electron wave can be found in.",
            "score": 147.4249725341797
        },
        {
            "docid": "7057536_7",
            "document": "Eva Nogales . Nogales was selected for membership in the American Academy of Arts and Sciences, along with three other Berkeley Lab scientists, including Glaeser, who worked with Nogales on cryo-electron microscopy, a technology that uses an electron microscope to see protein molecules in atomic detail.",
            "score": 147.09613037109375
        },
        {
            "docid": "23219544_12",
            "document": "Human genetic clustering . Genetic structure studies are carried out using statistical computer programs designed to find clusters of genetically similar individuals within a sample of individuals. Studies such as those by Risch and Rosenberg use a computer program called STRUCTURE to find human populations (gene clusters). It is a statistical program that works by placing individuals into one of an arbitrary number of clusters based on their overall genetic similarity, many possible pairs of clusters are tested per individual to generate multiple clusters. The basis for these computations are data describing a large number of single nucleotide polymorphisms (SNPs), genetic insertions and deletions (indels), microsatellite markers (or short tandem repeats, STRs) as they appear in each sampled individual. Cluster analysis divides a dataset into any prespecified number of clusters.",
            "score": 146.42388916015625
        },
        {
            "docid": "7731934_7",
            "document": "Gamma secretase . Gamma secretase is an internal protease that cleaves within the membrane-spanning domain of its substrate proteins, including amyloid precursor protein (APP) and Notch. Substrate recognition occurs via nicastrin ectodomain binding to the N-terminus of the target, which is then passed via a poorly understood process between the two presenilin fragments to a water-containing active site where the catalytic aspartate residue is located. The active site must contain water to carry out hydrolysis within a hydrophobic environment in the interior of the cell membrane, although it is not well understood how water and proton exchange is effected, and as yet no X-ray crystallography structure of gamma secretase is available. Low-resolution electron microscopy reconstructions have allowed the visualization of the hypothesized internal pores of about 2 nanometres. In 2014, a three-dimensional structure of an intact human gamma-secretase complex was determined by cryo-electron microscopy single-particle analysis at 4.5 angstrom resolution and in 2015 an atomic-resolution (3.4 angstrom) cryo-EM structure was reported.",
            "score": 145.70700073242188
        },
        {
            "docid": "29467449_17",
            "document": "Protein function prediction . This technique is a computational adaptation of 'wet lab' work from 1996. It was discovered that ascertaining the structure of a protein while it is suspended in different solvents and then superimposing those structures on one another produces data where the organic solvent molecules (that the proteins were suspended in) typically cluster at the protein's active site. This work was carried out as a response to realizing that water molecules are visible in the electron density maps produced by X-ray crystallography. The water molecules are interacting with the protein and tend to cluster at the protein's polar regions. This led to the idea of immersing the purified protein crystal in other solvents (e.g. ethanol, isopropanol, etc.) to determine where these molecules cluster on the protein. The solvents can be chosen based on what they approximate, that is, what molecule this protein may interact with (e.g. ethanol can probe for interactions with the amino acid serine, isopropanol a probe for threonine, etc.). It is vital that the protein crystal maintains its tertiary structure in each solvent. This process is repeated for multiple solvents and then this data can be used to try to determine potential active sites on the protein. Ten years later this technique was developed into an algorithm by Clodfelter et al.",
            "score": 144.4207763671875
        },
        {
            "docid": "28055575_4",
            "document": "Single particle analysis . Images (micrographs) collected on film are digitized using high-quality scanners, although increasingly electron microscopes have built-in CCD detectors coupled to a phosphorescent layer. The image processing is carried out using specialized software programs (for instance ), often run on multi-processor computer clusters. Depending on the sample or the desired results, various steps of two- or three-dimensional processing can be done.",
            "score": 143.75576782226562
        },
        {
            "docid": "12365585_19",
            "document": "Signal recognition particle RNA . X-ray crystallography, nuclear magnetic resonance (NMR), and cryo-electron microscopy (cryo-EM] have been used to determine the molecular structure of portions of the SRP RNAs from various species. The available PDB structures show the RNA molecule either free or when bound to one or more SRP proteins. One or more SRP proteins bind to the SRP RNA to assemble the functional SRP. The SRP proteins are named according to their approximate molecular mass measured in kilodalton. Most bacterial SRPs are composed of SRP RNA and SRP54 (also named Ffh for \"\"F\"ifty-\"f\"our \"h\"omolog\"). The Archaeal SRP contains proteins SRP54 and SRP19. In eukaryotes, the SRP RNA combines with the imported SRP proteins SRP9/14, SRP19, and SRP68/72 in a region of the nucleolus. This pre-SRP is transported to the cytosol where it binds to protein SRP54. The molecular structures of the free or SRP RNA-bound proteins SRP9/14, SRP19, or SRP54 are known at high resolution.",
            "score": 143.5461883544922
        },
        {
            "docid": "21557360_2",
            "document": "Grid-hpa . Grid- History Based Prediction Architecture (Grid-HPA) is a distributed software tool for predicting resource requirements of a job(s) in Grid Computing. Grid-HPA uses similarity measurement based on centralized Historical data of jobs as a knowledge base for deployed Grid infrastructure. Using statistical analysis and similarity measurements, Grid-HPA can predict and provide minimum resource requirement alternatives as a decision support system for newly submitted jobs by clients. Clustering of historical jobs and dynamic updating of clusters improve the prediction accuracy of the tool in comparison to other relevant projects. This tool can be used as an assistant system for decision support in resource allocation and reservation with less knowledge about resource requirements of jobs in order to manage computing with less costs and in a short time with maximum efficiency which could result higher resource utilization of grid and cloud environments. The concept fits very well to the new issues of distributed systems such as cloud computing. Project members will announce soon the cloud adapted version.",
            "score": 143.32908630371094
        },
        {
            "docid": "413102_6",
            "document": "Folding@home . Due to the complexity of proteins' conformation or configuration space (the set of possible shapes a protein can take), and limits in computing power, all-atom molecular dynamics simulations have been severely limited in the timescales which they can study. While most proteins typically fold in the order of milliseconds, before 2010 simulations could only reach nanosecond to microsecond timescales. General-purpose supercomputers have been used to simulate protein folding, but such systems are intrinsically costly and typically shared among many research groups. Further, because the computations in kinetic models occur serially, strong scaling of traditional molecular simulations to these architectures is exceptionally difficult. Moreover, as protein folding is a stochastic process and can statistically vary over time, it is challenging computationally to use long simulations for comprehensive views of the folding process. Protein folding does not occur in one step. Instead, proteins spend most of their folding time, nearly 96% in some cases, \"waiting\" in various intermediate conformational states, each a local thermodynamic free energy minimum in the protein's energy landscape. Through a process known as adaptive sampling, these conformations are used by Folding@home as starting points for a set of simulation trajectories. As the simulations discover more conformations, the trajectories are restarted from them, and a Markov state model (MSM) is gradually created from this cyclic process. MSMs are discrete-time master equation models which describe a biomolecule's conformational and energy landscape as a set of distinct structures and the short transitions between them. The adaptive sampling Markov state model method significantly increases the efficiency of simulation as it avoids computation inside the local energy minimum itself, and is amenable to distributed computing (including on GPUGRID) as it allows for the statistical aggregation of short, independent simulation trajectories. The amount of time it takes to construct a Markov state model is inversely proportional to the number of parallel simulations run, i.e., the number of processors available. In other words, it achieves linear parallelization, leading to an approximately four orders of magnitude reduction in overall serial calculation time. A completed MSM may contain tens of thousands of sample states from the protein's phase space (all the conformations a protein can take on) and the transitions between them. The model illustrates folding events and pathways (i.e., routes) and researchers can later use kinetic clustering to view a coarse-grained representation of the otherwise highly detailed model. They can use these MSMs to reveal how proteins misfold and to quantitatively compare simulations with experiments.",
            "score": 142.77035522460938
        },
        {
            "docid": "19568_28",
            "document": "Microscope . The two major types of electron microscopes are transmission electron microscopes (TEMs) and scanning electron microscopes (SEMs). They both have series of electromagnetic and electrostatic lenses to focus a high energy beam of electrons on a sample. In a TEM the electrons pass through the sample, analogous to basic optical microscopy. This requires careful sample preparation, since electrons are scattered strongly by most materials. The samples must also be very thin (50-100 nm) in order for the electrons to pass through it. Cross-sections of cells stained with osmium and heavy metals reveal clear organelle membranes and proteins such as ribosomes. With a 0.1 nm level of resolution, detailed views of viruses (20-300 nm) and a strand of DNA (2 nm in width) can be obtained. In contrast, the SEM has raster coils to scan the surface of bulk objects with a fine electron beam. Therefore, the specimen do not necessarily need to be sectioned, but require coating with a substance such as a heavy metal. This allows three-dimensional views of the surface of samples.",
            "score": 141.98866271972656
        },
        {
            "docid": "7057536_3",
            "document": "Eva Nogales . Eva Nogales works on structural and functional characterization of macromolecular complexes. Her lab uses electron microscopy, computational image analysis as well as functional biochemical assays to gain insights into function and regulation of the large biological assemblies. Her group used cryo-electron microscopy, a rising star in lab techniques. Her work has uncovered aspects of cellular function that are relevant to the treatment of cancer and other diseases.",
            "score": 141.97714233398438
        }
    ]
}