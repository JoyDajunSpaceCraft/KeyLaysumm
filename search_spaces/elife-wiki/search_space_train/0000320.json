{
    "q": [
        {
            "docid": "4548229_5",
            "document": "Interaural time difference . The duplex theory states that ITDs are used to localise low frequency sounds, in particular, while ILDs are used in the localisation of high frequency sound inputs. However, the frequency ranges for which the auditory system can use ITDs and ILDs significantly overlap, and most natural sounds will have both high and low frequency components, so that the auditory system will in most cases have to combine information from both ITDs and ILDs to judge the location of a sound source.  A consequence of this duplex system is that it is also possible to generate so-called \"cue trading\" or \"time\u2013intensity trading\" stimuli on headphones, where ITDs pointing to the left are offset by ILDs pointing to the right, so the sound is perceived as coming from the midline. A limitation of the duplex theory is that the theory does not completely explain directional hearing, as no explanation is given for the ability to distinguish between a sound source directly in front and behind. Also the theory only relates to localising sounds in the horizontal plane around the head. The theory also does not take into account the use of the pinna in localisation.(Gelfand, 2004)",
            "score": 137.06413793563843
        },
        {
            "docid": "5051081_4",
            "document": "Eric Knudsen . In 1978, Knudsen and Konishi presented the discovery of an auditory map of space in the midbrain of the barn owl. This discovery was groundbreaking because it unearthed the first non-somatotopic space map in the brain. The map was found in the owl\u2019s midbrain, in the lateral and anterior mesencephalicus lateralis dorsalis (MLD), a structure now referred to as the inferior colliculus. Unlike most sound-localization maps, this map was found to be two-dimensional, with units arranged spatially to represent both the vertical and horizontal location of sound. Knudsen and Konishi discovered that units in this structure respond preferentially to sounds originating in a particular region in space. In the 1978 paper, elevation and azimuth (location in the horizontal plane) were shown to be the two coordinates of the map. Using a speaker set on a rotatable hemispherical track, Knudsen and Konishi presented owls with auditory stimulus from various locations in space and recorded the resulting neuronal activity. They found that neurons in this part of the MLD were organized according to the location of their receptive field, with azimuth varying along the horizontal plane of the space map and elevation varying vertically.  Knudsen followed this discovery with research into specific sound localization mechanisms. Two main auditory cues used by the barn owl to localize sound are interaural time difference (ITD) and interaural intensity difference (IID). The owl\u2019s ears are asymmetric, with the right ear\u2019s opening being directed higher than that of the left. This asymmetry allows the barn owl to determine the elevation of a sound by comparing sound levels between its two ears. Interaural time differences provide the owl with information regarding a sound\u2019s azimuth; sound will reach the ear closer to the sound source before reaching the farther ear, and this time difference can be detected and interpreted as an azimuthal direction. At low frequencies, the wavelength of a sound is wider than the owl's facial ruff, and the ruff does not affect detection of azimuth. At high frequencies, the ruff plays a role in reflecting sound for heightened sensitivity to vertical elevation. Therefore, with wide-band noise, containing both high and low frequencies, the owl could use interaural spectrum difference to obtain information about both azimuth and elevation. In 1979, Knudsen and Konishi showed that the barn owl uses interaural spectrum information in sound localization. They presented owls with both wide-bandwidth noise and pure tones. The birds were able to successfully locate pure tones (since they could still gather information from IID and ITD), but their error rate was much lower when localizing wide-bandwidth noise. This indicates that the birds utilize interaural spectrum differences to improve their accuracy.",
            "score": 194.28191614151
        },
        {
            "docid": "39200136_12",
            "document": "Hans Wallach . In a series of papers Wallach explored the ability of humans to locate sounds in the median plane \u2013 that is, to determine whether a sound comes from a source at the same elevation as the ears or from a source that is higher or lower, or even in back of the head. Binaural sound cues, including the phasing or time of the sound\u2019s arrival at each ear and the sound\u2019s relative intensity at the two ears (known respectively as ITD and ILD) enable a listener to determine a sound\u2019s lateral location (whether it is on the left, right, or straight ahead). But two sounds at different elevations can present identical ITD and ILD information to the ears, and so binaural cues to a stationary ear do not suffice to identify a sound\u2019s location in the median plane. Monaural cues that depend on the shape of the head and the structure of the external ear help with vertical localization, but binaural cues also play a part if the head is not stationary.",
            "score": 146.96518325805664
        },
        {
            "docid": "14532984_7",
            "document": "Coincidence detection in neurobiology . Coincidence detection has been shown to be a major factor in sound localization along the azimuth plane in several organisms. In 1948, Lloyd A. Jeffress proposed that some organisms may have a collection of neurons that receive auditory input from each ear. The neural pathways to these neurons are called delay lines. Jeffress claimed that the neurons that the delay lines link act as coincidence detectors by firing maximally when receiving simultaneous inputs from both ears. When a sound is heard, sound waves may reach the ears at different times. This is referred to as the interaural time difference (ITD). Due to differing lengths and a finite conduction speed within the axons of the delay lines, different coincidence detector neurons will fire when sound comes from different positions along the azimuth. Jeffress' model proposes that two signals even from an asynchronous arrival of sound in the cochlea of each ear will converge synchronously on a coincidence detector in the auditory cortex based on the magnitude of the ITD (Fig. 2). Therefore, the ITD should correspond to an anatomical map that can be found within the brain. Masakazu Konishi's study on barn owls shows that this is true. Sensory information from the hair cells of the ears travels to the ipsilateral nucleus magnocellularis. From here, the signals project ipsilaterally and contralaterally to two nucleus laminari. Each nucleus laminaris contains coincidence detectors that receive auditory input from the left and the right ear. Since the ipsilateral axons enter the nucleus laminaris dorsally while the contralateral axons enter ventrally, sounds from various positions along the azimuth correspond directly to stimulation of different depths of the nucleus laminaris. From this information, a neural map of auditory space was formed. The function of the nucleus laminaris parallels that of the medial superior olive in mammals.",
            "score": 180.60654270648956
        },
        {
            "docid": "4301708_10",
            "document": "Cochlear nucleus . The cochlear nuclear complex is the first integrative, or processing, stage in the auditory system. Information is brought to the nuclei from the ipsilateral cochlea via the cochlear nerve. Several tasks are performed in the cochlear nuclei. By distributing acoustic input to multiple types of principal cells, the auditory pathway is subdivided into parallel ascending pathways, which can simultaneously extract different types of information. The cells of the ventral cochlear nucleus extract information that is carried by the auditory nerve in the timing of firing and in the pattern of activation of the population of auditory nerve fibers. The cells of the dorsal cochlear nucleus perform a non-linear spectral analysis and place that spectral analysis into the context of the location of the head, ears and shoulders and that separate expected, self-generated spectral cues from more interesting, unexpected spectral cues using input from the auditory cortex, pontine nuclei, trigeminal ganglion and nucleus, dorsal column nuclei and the second dorsal root ganglion. It is likely that these neurons help mammals to use spectral cues for orienting toward those sounds. The information is used by higher brainstem regions to achieve further computational objectives (such as sound source location or improvement in signal to noise ratio). The inputs from these other areas of the brain probably play a role in sound localization.",
            "score": 119.1215785741806
        },
        {
            "docid": "4548229_7",
            "document": "Interaural time difference . Feddersen et al. (1957) also conducted experiments taking measurements on how ITDs alter with changing the azimuth of the loudspeaker around the head at different frequencies. But unlike the Woodworth experiments human subjects were used rather than a model of the head. The experiment results agreed with the conclusion made by Woodworth about ITDs. The experiments also concluded that is there is no difference in ITDs when sounds are provided from directly in front or behind at 0\u00b0 and 180\u00b0 azimuth. The explanation for this is that the sound is equidistant from both ears. Interaural time differences alter as the loudspeaker is moved around the head. The maximum ITD of 660 \u03bcs occurs when a sound source is positioned at 90\u00b0 azimuth to one ear.",
            "score": 141.21240186691284
        },
        {
            "docid": "39265695_7",
            "document": "Stimulus filtering . Female flies of the genus \"Ormia ochracea\" possess organs in their bodies that can detect frequencies of cricket sounds from meters away. This process is important for the survival of their species because females will lay their first instar larvae into the body of the cricket, where they will feed and molt for approximately seven days. After this period, the larvae grow into flies and the cricket usually perishes. Researchers were puzzled about how precise hearing ability could arise from a small ear structure. Normal animals detect and locate sounds using the interaural time difference (ITD) and the interaural level difference (ILD). The ITD is the difference in the time it takes sound to reach the ear. ILD is the difference in sound intensity measure between both ears. At maximum, the ITD would only reach about 1.5 microseconds and the ILD would be less than one decibel. These small values make it hard to sense the differences. To solve these issues, researchers studied the mechanical aspects of flies\u2019 ears. They found that they have a presternum structure linking both tympanal membranes that is critical in detecting sound and localization. The structure acts as a lever by transferring and amplifying vibrational energy between the membranes. After sound hits the membranes at different amplitudes, the presternum sets up symmetrical vibration modes through bending and rocking. This effect helps the nervous system distinguish which side the sound is coming from. Because the presternum acts as an intertympanal bridge, the ITD is increased from 1.5 us to 55 us and the ILD is increased from less than one decibel to over 10 decibels.",
            "score": 144.82211351394653
        },
        {
            "docid": "5442380_17",
            "document": "Sensory cue . Unless a sound is directly in front of or behind the individual, the sound stimuli will have a slightly different distance to travel to reach each ear. This difference in distance causes a slight delay in the time the signal is perceived by each ear. The magnitude of the interaural time difference is greater the more the signal comes from the side of the head. Thus, this time delay allows humans to accurately predict the location of incoming sound cues. Interaural level difference is caused by the difference in sound pressure level reaching the two ears. This is because the head blocks the sound waves for the further ear, causing less intense sound to reach it. This level difference between the two ears allows humans to accurately predict azimuth of an auditory signal. This effect only occurs at sounds that are high frequency.",
            "score": 158.09137773513794
        },
        {
            "docid": "41087200_8",
            "document": "Perceptual-based 3D sound localization . Interaural level differences (ILD) represents the difference in sound pressure level reaching the two ears. They provide salient cues for localizing high-frequency sounds in space, and populations of neurons that are sensitive to ILD are found at almost every synaptic level from brain stem to cortex. These cells are predominantly excited by stimulation of one ear and predominantly inhibited by stimulation of the other ear, such that the magnitude of their response is determined in large part by the intensities at the 2 ears. This gives rise to the concept of resonant damping. Interaural level difference (ILD) is best for high frequency sounds because low frequency sounds are not attenuated much by the head. ILD (also known as Interaural Intensity Difference) arises when the sound source is not centred, the listener's head partially shadows the ear opposite to the source, diminishing the intensity of the sound in that ear (particularly at higher frequencies). The pinnae filters the sound in a way that is directionally dependent. This is particularly useful in determining if a sound comes from above, below, in front, or behind.",
            "score": 166.89351379871368
        },
        {
            "docid": "32116125_4",
            "document": "Amblyaudia . Amblyaudia is a deficit in binaural integration of environmental information entering the auditory system. It is a disorder related to brain organization and function rather than what is typically considered a \u201chearing loss\u201d (damage to the cochlea). It may be genetic or developmentally acquired or both. When animals are temporarily deprived of hearing from an early age, profound changes occur in the brain. Specifically, cell sizes in brainstem nuclei are reduced, the configuration of brainstem dendrites are altered and neurons respond in different ways to sounds presented to both the deprived and non-deprived ears (in cases of asymmetric deprivation). This last point is particularly important for listening tasks that require inputs from two ears to perform well. There are multiple auditory functions that rely on the computation of well calibrated inputs from the two ears. Chief among these is the ability to localize sound sources and separate what we want to hear from a background of noise. In the brainstem, the auditory system compares the timing and levels of sounds between the two ears to encode the location of sound sources (sounds that originate from our right as opposed to left side are louder and arrive earlier in our right ear). This ability to separate sound sources not only helps us locate the trajectories of moving objects, but also to separate different sound sources in noisy environments.",
            "score": 177.71056723594666
        },
        {
            "docid": "635490_6",
            "document": "Auditory system . The trapezoid body is where most of the cochlear nucleus (CN) fibers decussate (cross left to right and vice versa); this cross aids in sound localization. The CN breaks into ventral (VCN) and dorsal (DCN) regions. The VCN has three nuclei. Bushy cells transmit timing info, their shape averages timing jitters. Stellate (chopper) cells encode sound spectra (peaks and valleys) by spatial neural firing rates based on auditory input strength (rather than frequency). Octopus cells have close to the best temporal precision while firing, they decode the auditory timing code. The DCN has 2 nuclei. DCN also receives info from VCN. Fusiform cells integrate information to determine spectral cues to locations (for example, whether a sound originated from in front or behind). Cochlear nerve fibers (30,000+) each have a most sensitive frequency and respond over a wide range of levels.",
            "score": 100.06391906738281
        },
        {
            "docid": "1021754_6",
            "document": "Sound localization . In vertebrates, inter-aural time differences are known to be calculated in the superior olivary nucleus of the brainstem. According to Jeffress, this calculation relies on delay lines: neurons in the superior olive which accept innervation from each ear with different connecting axon lengths. Some cells are more directly connected to one ear than the other, thus they are specific for a particular inter-aural time difference. This theory is equivalent to the mathematical procedure of cross-correlation. However, because Jeffress' theory is unable to account for the precedence effect, in which only the first of multiple identical sounds is used to determine the sounds' location (thus avoiding confusion caused by echoes), it cannot be entirely used to explain the response. Furthermore, a number of recent physiological observations made in the midbrain and brainstem of small mammals have shed considerable doubt on the validity of Jeffress' original ideas",
            "score": 151.5490791797638
        },
        {
            "docid": "25345530_25",
            "document": "Models of neural computation . According to Jeffress, in order to compute the location of a sound source in space from interaural time differences, an auditory system relies on delay lines: the induced signal from an ipsilateral auditory receptor to a particular neuron is delayed for the same time as it takes for the original sound to go in space from that ear to the other. Each postsynaptic cell is differently delayed and thus specific for a particular inter-aural time difference. This theory is equivalent to the mathematical procedure of cross-correlation.",
            "score": 158.17312908172607
        },
        {
            "docid": "1021754_24",
            "document": "Sound localization . For frequencies above 1600\u00a0Hz the dimensions of the head are greater than the length of the sound waves. An unambiguous determination of the input direction based on interaural phase alone is not possible at these frequencies. However, the interaural level differences become larger, and these level differences are evaluated by the auditory system. Also, group delays between the ears can be evaluated, and is more pronounced at higher frequencies; that is, if there is a sound onset, the delay of this onset between the ears can be used to determine the input direction of the corresponding sound source. This mechanism becomes especially important in reverberant environments. After a sound onset there is a short time frame where the direct sound reaches the ears, but not yet the reflected sound. The auditory system uses this short time frame for evaluating the sound source direction, and keeps this detected direction as long as reflections and reverberation prevent an unambiguous direction estimation. The mechanisms described above cannot be used to differentiate between a sound source ahead of the hearer or behind the hearer; therefore additional cues have to be evaluated.",
            "score": 137.63509619235992
        },
        {
            "docid": "1222458_3",
            "document": "Tonotopy . Tonotopy in the auditory system begins at the cochlea, the small snail-like structure in the inner ear that sends information about sound to the brain. Different regions of the basilar membrane in the organ of Corti, the sound-sensitive portion of the cochlea, vibrate at different sinusoidal frequencies due to variations in thickness and width along the length of the membrane. Nerves that transmit information from different regions of the basilar membrane therefore encode frequency tonotopically. This tonotopy then projects through the vestibulocochlear nerve and associated midbrain structures to the primary auditory cortex via the auditory radiation pathway. Throughout this radiation, organization is linear with relation to placement on the organ of Corti, in accordance to the best frequency response (that is, the frequency at which that neuron is most sensitive) of each neuron. However, binaural fusion in the superior olivary complex onward adds significant amounts of information encoded in the signal strength of each ganglion. Thus, the number of tonotopic maps varies between species and the degree of binaural synthesis and separation of sound intensities; in humans, six tonotopic maps have been identified in the primary auditory cortex. their anatomical locations along the auditory cortex.",
            "score": 133.85400068759918
        },
        {
            "docid": "41087200_9",
            "document": "Perceptual-based 3D sound localization . Interaural time and level differences (ITD, ILD) play a role in azimuth perception but can\u2019t explain vertical localization. According to the duplex theory, ITDs have a greater contribution to the localisation of low frequency sounds (below 1\u00a0kHz),while ILDs are used in the localisation of high frequency sound. The ILD arises from the fact that,a sound coming from a source located to one side of the head will have a higher intensity, or be louder, at the ear nearest the sound source. One can therefore create the illusion of a sound source emanating from one side of the head merely by adjusting the relative level of the sounds that are fed to two separated speakers or headphones. This is the basis of the commonly used pan control.",
            "score": 158.69361472129822
        },
        {
            "docid": "161005_5",
            "document": "Head-related transfer function . Humans estimate the location of a source by taking cues derived from one ear (\"monaural cues\"), and by comparing cues received at both ears (\"difference cues\" or \"binaural cues\"). Among the difference cues are time differences of arrival and intensity differences. The monaural cues come from the interaction between the sound source and the human anatomy, in which the original source sound is modified before it enters the ear canal for processing by the auditory system. These modifications encode the source location, and may be captured via an impulse response which relates the source location and the ear location. This impulse response is termed the \"head-related impulse response\" (HRIR). Convolution of an arbitrary source sound with the HRIR converts the sound to that which would have been heard by the listener if it had been played at the source location, with the listener's ear at the receiver location. HRIRs have been used to produce virtual surround sound.",
            "score": 160.15324640274048
        },
        {
            "docid": "47338295_3",
            "document": "Sound localization in owls . Owls must be able to determine the necessary angle of descent, i.e. the elevation, in addition to azimuth (horizontal angle to the sound). This bi-coordinate sound localization is accomplished through two binaural cues: the interaural time difference (ITD) and the interaural level difference (ILD), also known as the interaural intensity difference (IID). The ability in owls is unusual; in ground-bound mammals such as mice, ITD and ILD are not utilized in the same manner. In these mammals, ITDs tend to be utilized for localization of lower frequency sounds, while ILDs tend to be used for higher frequency sounds.",
            "score": 118.08339762687683
        },
        {
            "docid": "41087200_7",
            "document": "Perceptual-based 3D sound localization . According to the duplex theory, ITDs have a greater contribution to the localisation of low frequency sounds (below 1 kHz), while ILDs are used in the localisation of high frequency sound. These approaches can be applied to selective reconstructions of spatialized signals, where spectrotemporal components believed to be dominated by the desired sound source are identified and isolated through the Short-time Fourier transform (STFT). Modern systems typically compute the STFT of the incoming signal from two or more microphones, and estimate the ITD or each spectrotemporal component by comparing the phases of the STFTs. An advantage to this approach is that it may be generalized to more than two microphones, which can improve accuracy in 3 dimensions and remove the front-back localization ambiguity that occurs with only two ears or microphones. Another advantage is that the ITD is relatively strong and easy to obtain without biomimetic instruments such as dummy heads and artificial pinnae, though these may still be used to enhance amplitude disparities. HRTF phase response is mostly linear and listeners are insensitive to the details of the interaural phase spectrum as long as the interaural time delay (ITD) of the combined low-frequency part of the waveform is maintained.",
            "score": 129.4431608915329
        },
        {
            "docid": "4548229_4",
            "document": "Interaural time difference . The Duplex theory proposed by Lord Rayleigh (1907) provides an explanation for the ability of humans to localise sounds by time differences between the sounds reaching each ear (ITDs) and differences in sound level entering the ears (interaural level differences, ILDs). But there still lies a question whether ITD or ILD is prominent.",
            "score": 141.5758032798767
        },
        {
            "docid": "1021754_25",
            "document": "Sound localization . Duplex theory clearly points out that ITD and IID play significant roles in sound localization but they can only deal with lateral localizing problems. For example, based on duplex theory, if two acoustic sources are symmetrically located on the right front and right back of the human head, they will generate equal ITDs and IIDs, which is called as cone model effect. However, human ears can actually distinguish this set of sources. Besides that, in natural sense of hearing, only one ear, which means no ITD or IID, can distinguish the sources with a high accuracy. Due to the disadvantages of duplex theory, researchers proposed the pinna filtering effect theory. The shape of human pinna is very special. It is concave with complex folds and asymmetrical no matter horizontally or vertically. The reflected waves and the direct waves will generate a frequency spectrum on the eardrum, which is related to the acoustic sources. Then auditory nerves localize the sources by this frequency spectrum. Therefore, a corresponding theory was proposed and called as pinna filtering effect theory.",
            "score": 125.89640951156616
        },
        {
            "docid": "4548229_2",
            "document": "Interaural time difference . The interaural time difference (or ITD) when concerning humans or animals, is the difference in arrival time of a sound between two ears. It is important in the localization of sounds, as it provides a cue to the direction or angle of the sound source from the head. If a signal arrives at the head from one side, the signal has further to travel to reach the far ear than the near ear. This pathlength difference results in a time difference between the sound's arrivals at the ears, which is detected and aids the process of identifying the direction of sound source.",
            "score": 177.3412675857544
        },
        {
            "docid": "323712_10",
            "document": "Async . On \"async\", all of its sounds come together but never create a proper harmony. However, Sakamoto described these sounds as \"significant in their own way because their \"existence has meaning.\" He explained, \"As human beings, we [...] take the liberty to decide which sound is good or bad. [...] I'm suggesting we open our ears and listen to each sound without prejudice.\" Sakamoto did this to symbolize as well as commentate on how the differing viewpoints of humanity worked: \"In this world of myriad viewpoints and unlimited information, every single person is choosing only the information that he or she is interested in, and people with similar interests gather and form a group. Then, groups with similar interests exchange views with one another, accelerating the movement to narrow the conversation down to ever-more specific views bound by a particular concern. And so, groups with different interests barely communicate with each other, or even if they do, they tend to dismiss the views of the other.\" The message of \"async\" is that, like dissonant sounds coming together to create music, humans of all different viewpoints should come together and respect each other.",
            "score": 104.7591267824173
        },
        {
            "docid": "3154127_4",
            "document": "Virtual acoustic space . When one listens to sounds over headphones (in what is known as the \"closed field\") the sound source appears to arise from center of the head. On the other hand, under normal, so-called free-field, listening conditions sounds are perceived as being externalized. The direction of a sound in space (see sound localization) is determined by the brain when it analyses the interaction of incoming sound with head and external ears. A sound arising to one side reaches the near ear before the far ear (creating an interaural time difference, ITD), and will also be louder at the near ear (creating an interaural level difference, ILD \u2013 also known as interaural intensity difference, IID). These binaural cues allow sounds to be lateralized. Although conventional stereo headphone signals make used of ILDs (not ITDs) the sound is not perceived as being externalized.",
            "score": 162.60277438163757
        },
        {
            "docid": "8436042_6",
            "document": "MPEG Surround . MPEG Surround coding uses our capacity to perceive sound in the 3D and captures that perception in a compact set of parameters. Spatial perception is primarily attributed to three parameters, or cues, describing how humans localize sound in the horizontal plane: Interaural level difference (ILD), Interaural time difference (ITD) and Interaural coherence (IC). This three concepts are illustrated in next image. Direct, or first-arrival, waveforms from the source hit the left ear at time, while direct sound received by the right ear is diffracted around the head, with time delay and level attenuation, associated. These two effects result in ITD and ILD are associated with the main source. At last, in a reverberant environment, reflected sound from the source, or sound from diffuse source, or uncorrelated sound can hit both ears, all of them are related with IC.",
            "score": 144.33355021476746
        },
        {
            "docid": "8398605_8",
            "document": "Microphone practice . There are two features of sound that the human brain uses to place objects in the stereo sound-field between the loudspeakers. These are the relative level (or loudness) difference between the two channels \"\u0394\u00a0L\", and the time delay difference in arrival times for the same sound in each channel \"\u0394\u00a0t\". The \"interaural\" signals (binaural \"ILD\" and \"ITD\") at the ears are not the stereo microphone signals which are coming from the loudspeakers, and are called \"interchannel\" signals (\"\u0394\u00a0L\" and \"\u0394\u00a0t\"). These signals are normally not mixed. Loudspeaker signals are different from the sound arriving at the ear. See the article \"Binaural recording for earphones\".",
            "score": 161.2866291999817
        },
        {
            "docid": "233830_28",
            "document": "Ambisonics . At low frequencies, where the wavelength is large compared to the human head, an incoming sound diffracts around it, so that there is virtually no acoustic shadow and hence no level difference between the ears. In this range, the only available information is the phase relationship between the two ear signals, called \"interaural time difference\", or \"ITD\". Evaluating this time difference allows for precise localisation within a \"cone of confusion\": the angle of incidence is unambiguous, but the ITD is the same for sounds from the front or from the back. As long as the sound is not totally unknown to the subject, the confusion can usually be resolved by perceiving the timbral front-back variations caused by the ear flaps (or \"pinnae\").",
            "score": 121.99656414985657
        },
        {
            "docid": "5637355_17",
            "document": "Neural facilitation . In humans, sound localization is primarily accomplished using information about how the intensity and timing of a sound vary between each ear. Neuronal computations involving these interaurual intensity differences (IIDs) and interaural time differences (ITDs) are typically carried out in different pathways in the brain. Short-term plasticity likely assists in differentiating between these two pathways: short-term facilitation dominates in intensity pathways, while short-term depression dominates in temporal pathways. These different types of short-term plasticity allow for different kinds of information filtration, thus contributing to the division of the two kinds of information into distinct processing streams.",
            "score": 145.7409999370575
        },
        {
            "docid": "31352483_6",
            "document": "Soundscape ecology . The function and importance of sound in the environment may not be fully appreciated unless one adopts an organismal perspective on sound perception, and, in this way, soundscape ecology is also informed by sensory ecology. Sensory ecology focuses on understanding the sensory systems of organisms and the biological function of information obtained from these systems. In many cases, humans must acknowledge that sensory modalities and information used by other organisms may not be obvious from an anthropocentric viewpoint. This perspective has already highlighted many instances where organisms rely heavily on sound cues generated within their natural environments to perform important biological functions. For example, a broad range of crustaceans are known to respond to biophony generated around coral reefs. Species that must settle on reefs to complete their developmental cycle are attracted to reef noise while pelagic and nocturnal crustaceans are repelled by the same acoustic signal, presumably as a mechanism to avoid predation (predator densities are high in reef habitats). Similarly, juvenile fish may use biophony as a navigational cue to locate their natal reefs. Other species\u2019 movement patterns are influenced by geophony, as in the case of the reed frog which is known to disperse away from the sound of fire. In addition, a variety of bird and mammal species use auditory cues, such as movement noise, in order to locate prey. Disturbances created by periods of environmental noise may also be exploited by some animals while foraging. For example, insects that prey on spiders concentrate foraging activities during episodes of environmental noise to avoid detection by their prey. These examples demonstrate that many organisms are highly capable of extracting information from soundscapes.",
            "score": 96.407381772995
        },
        {
            "docid": "2263473_7",
            "document": "Volley theory . Pitch is an assigned, perceptual property where a listener orders sound frequencies from low to high. Pitch is hypothesized to be determined by receiving phase-locked input from neuronal axons and combining that information into harmonics. In simple sounds consisting of one frequency, the pitch is equivalent to the frequency. There are two models of pitch perception; a spectral and a temporal. Low frequency sounds evoke the strongest pitches, suggesting that pitch is based on the temporal components of the sound. Historically, there have been many models of pitch perception. (Terhardt, 1974; Goldstein, 1973; Wightman, 1973). Many consisted of a peripheral spectral-analysis stage and a central periodicity-analysis stage. In his model, Terhardt claims that the spectral-analysis output of complex sounds, specifically low frequency ones, is a learned entity which eventually allows easy identification of the virtual pitch. The volley principle is predominantly seen during the pitch perception of lower frequencies where sounds are often resolved. Goldstein proposed that through phase-locking and temporal frequencies encoded in neuron firing rates, the brain has the itemization of frequencies that can then be used to estimate pitch.",
            "score": 123.02977621555328
        },
        {
            "docid": "569650_19",
            "document": "Stimulus modality . The human ear is able to detect differences in pitch through the movement of auditory hair cells found on the basilar membrane. High frequency sounds will stimulate the auditory hair cells at the base of the basilar membrane while medium frequency sounds cause vibrations of auditory hair cells located at the middle of the basilar membrane. For frequencies that are lower than 200\u00a0Hz, the tip of the basilar membrane vibrates in sync with the sound waves. In turn, neurons are fired at the same rate as the vibrations. The brain is able to measure the vibrations and is then aware of any low frequency pitches.",
            "score": 110.22527146339417
        },
        {
            "docid": "6894544_29",
            "document": "Noise-induced hearing loss . NIHL occurs when too much sound intensity is transmitted into and through the auditory system. An acoustic signal from a sound source, such as a radio, enters into the external auditory canal (ear canal), and is funneled through to the tympanic membrane (eardrum), causing it to vibrate. The vibration of the tympanic membrane drives the middle ear ossicles, the malleus, incus, and stapes into motion. The middle ear ossicles transfer mechanical energy to the cochlea by way of the stapes footplate hammering against the oval window of the cochlea. This hammering causes the fluid within the cochlea (perilymph and endolymph) to be displaced. Displacement of the fluid causes movement of the hair cells (sensory cells in the cochlea) and an electrical signal to be sent from the auditory nerve (CN VIII) to the central auditory system within the brain. This is where sound is perceived. Different groups of hair cells are responsive to different frequencies. Hair cells at or near the base of the cochlea are most sensitive to higher frequency sounds while those at the apex are most sensitive to lower frequency sounds. There are two known biological mechanisms of NIHL from excessive sound intensity: damage to the hair cells and damage to the myelination or synaptic regions of auditory nerves.",
            "score": 121.82551205158234
        }
    ],
    "r": [
        {
            "docid": "5051081_4",
            "document": "Eric Knudsen . In 1978, Knudsen and Konishi presented the discovery of an auditory map of space in the midbrain of the barn owl. This discovery was groundbreaking because it unearthed the first non-somatotopic space map in the brain. The map was found in the owl\u2019s midbrain, in the lateral and anterior mesencephalicus lateralis dorsalis (MLD), a structure now referred to as the inferior colliculus. Unlike most sound-localization maps, this map was found to be two-dimensional, with units arranged spatially to represent both the vertical and horizontal location of sound. Knudsen and Konishi discovered that units in this structure respond preferentially to sounds originating in a particular region in space. In the 1978 paper, elevation and azimuth (location in the horizontal plane) were shown to be the two coordinates of the map. Using a speaker set on a rotatable hemispherical track, Knudsen and Konishi presented owls with auditory stimulus from various locations in space and recorded the resulting neuronal activity. They found that neurons in this part of the MLD were organized according to the location of their receptive field, with azimuth varying along the horizontal plane of the space map and elevation varying vertically.  Knudsen followed this discovery with research into specific sound localization mechanisms. Two main auditory cues used by the barn owl to localize sound are interaural time difference (ITD) and interaural intensity difference (IID). The owl\u2019s ears are asymmetric, with the right ear\u2019s opening being directed higher than that of the left. This asymmetry allows the barn owl to determine the elevation of a sound by comparing sound levels between its two ears. Interaural time differences provide the owl with information regarding a sound\u2019s azimuth; sound will reach the ear closer to the sound source before reaching the farther ear, and this time difference can be detected and interpreted as an azimuthal direction. At low frequencies, the wavelength of a sound is wider than the owl's facial ruff, and the ruff does not affect detection of azimuth. At high frequencies, the ruff plays a role in reflecting sound for heightened sensitivity to vertical elevation. Therefore, with wide-band noise, containing both high and low frequencies, the owl could use interaural spectrum difference to obtain information about both azimuth and elevation. In 1979, Knudsen and Konishi showed that the barn owl uses interaural spectrum information in sound localization. They presented owls with both wide-bandwidth noise and pure tones. The birds were able to successfully locate pure tones (since they could still gather information from IID and ITD), but their error rate was much lower when localizing wide-bandwidth noise. This indicates that the birds utilize interaural spectrum differences to improve their accuracy.",
            "score": 194.28192138671875
        },
        {
            "docid": "14532984_7",
            "document": "Coincidence detection in neurobiology . Coincidence detection has been shown to be a major factor in sound localization along the azimuth plane in several organisms. In 1948, Lloyd A. Jeffress proposed that some organisms may have a collection of neurons that receive auditory input from each ear. The neural pathways to these neurons are called delay lines. Jeffress claimed that the neurons that the delay lines link act as coincidence detectors by firing maximally when receiving simultaneous inputs from both ears. When a sound is heard, sound waves may reach the ears at different times. This is referred to as the interaural time difference (ITD). Due to differing lengths and a finite conduction speed within the axons of the delay lines, different coincidence detector neurons will fire when sound comes from different positions along the azimuth. Jeffress' model proposes that two signals even from an asynchronous arrival of sound in the cochlea of each ear will converge synchronously on a coincidence detector in the auditory cortex based on the magnitude of the ITD (Fig. 2). Therefore, the ITD should correspond to an anatomical map that can be found within the brain. Masakazu Konishi's study on barn owls shows that this is true. Sensory information from the hair cells of the ears travels to the ipsilateral nucleus magnocellularis. From here, the signals project ipsilaterally and contralaterally to two nucleus laminari. Each nucleus laminaris contains coincidence detectors that receive auditory input from the left and the right ear. Since the ipsilateral axons enter the nucleus laminaris dorsally while the contralateral axons enter ventrally, sounds from various positions along the azimuth correspond directly to stimulation of different depths of the nucleus laminaris. From this information, a neural map of auditory space was formed. The function of the nucleus laminaris parallels that of the medial superior olive in mammals.",
            "score": 180.60655212402344
        },
        {
            "docid": "32116125_4",
            "document": "Amblyaudia . Amblyaudia is a deficit in binaural integration of environmental information entering the auditory system. It is a disorder related to brain organization and function rather than what is typically considered a \u201chearing loss\u201d (damage to the cochlea). It may be genetic or developmentally acquired or both. When animals are temporarily deprived of hearing from an early age, profound changes occur in the brain. Specifically, cell sizes in brainstem nuclei are reduced, the configuration of brainstem dendrites are altered and neurons respond in different ways to sounds presented to both the deprived and non-deprived ears (in cases of asymmetric deprivation). This last point is particularly important for listening tasks that require inputs from two ears to perform well. There are multiple auditory functions that rely on the computation of well calibrated inputs from the two ears. Chief among these is the ability to localize sound sources and separate what we want to hear from a background of noise. In the brainstem, the auditory system compares the timing and levels of sounds between the two ears to encode the location of sound sources (sounds that originate from our right as opposed to left side are louder and arrive earlier in our right ear). This ability to separate sound sources not only helps us locate the trajectories of moving objects, but also to separate different sound sources in noisy environments.",
            "score": 177.7105712890625
        },
        {
            "docid": "4548229_2",
            "document": "Interaural time difference . The interaural time difference (or ITD) when concerning humans or animals, is the difference in arrival time of a sound between two ears. It is important in the localization of sounds, as it provides a cue to the direction or angle of the sound source from the head. If a signal arrives at the head from one side, the signal has further to travel to reach the far ear than the near ear. This pathlength difference results in a time difference between the sound's arrivals at the ears, which is detected and aids the process of identifying the direction of sound source.",
            "score": 177.3412628173828
        },
        {
            "docid": "1021754_21",
            "document": "Sound localization . In 1907, Lord Rayleigh utilized tuning forks to generate monophonic excitation and studied the lateral sound localization theory on a human head model without auricle. He first presented the interural clue difference based sound localization theory, which is known as Duplex Theory. Human ears are on the different sides of the head, thus they have different coordinates in space. As shown in fig. 2, since the distances between the acoustic source and ears are different, there are time difference and intensity difference between the sound signals of two ears. We call those kinds of differences as Interaural Time Difference (ITD) and Interaural Intensity Difference (IID) respectively.",
            "score": 175.70738220214844
        },
        {
            "docid": "1021754_49",
            "document": "Sound localization . The tiny parasitic fly Ormia ochracea has become a model organism in sound localization experiments because of its unique ear. The animal is too small for the time difference of sound arriving at the two ears to be calculated in the usual way, yet it can determine the direction of sound sources with exquisite precision. The tympanic membranes of opposite ears are directly connected mechanically, allowing resolution of sub-microsecond time differences and requiring a new neural coding strategy. Ho showed that the coupled-eardrum system in frogs can produce increased interaural vibration disparities when only small arrival time and sound level differences were available to the animal's head. Efforts to build directional microphones based on the coupled-eardrum structure are underway.",
            "score": 169.85189819335938
        },
        {
            "docid": "3864383_7",
            "document": "Virtual surround . Perception of direction is greatly affected by the relative time that a sound arrives at each ear and any difference in the amplitude of a sound at each ear. It is possible to create a sound source having an output characteristic which is rapidly varying with direction and frequency of signal. These kinds of sources create sound fields which are rapidly variable around the listeners room. These are often referred to as diffuse sources, this is because their output resembles a diffuse sound field \u2014 a sound field where soundwaves are traveling in all directions with equal probability. In a diffuse field the sound at each of a listeners' ears is so completely different that it is impossible for the brain to work out where the sound has come from. A diffuse source located in front of the listener will be hard to localize and can be used to carry the surround signals.",
            "score": 168.4578094482422
        },
        {
            "docid": "41087200_8",
            "document": "Perceptual-based 3D sound localization . Interaural level differences (ILD) represents the difference in sound pressure level reaching the two ears. They provide salient cues for localizing high-frequency sounds in space, and populations of neurons that are sensitive to ILD are found at almost every synaptic level from brain stem to cortex. These cells are predominantly excited by stimulation of one ear and predominantly inhibited by stimulation of the other ear, such that the magnitude of their response is determined in large part by the intensities at the 2 ears. This gives rise to the concept of resonant damping. Interaural level difference (ILD) is best for high frequency sounds because low frequency sounds are not attenuated much by the head. ILD (also known as Interaural Intensity Difference) arises when the sound source is not centred, the listener's head partially shadows the ear opposite to the source, diminishing the intensity of the sound in that ear (particularly at higher frequencies). The pinnae filters the sound in a way that is directionally dependent. This is particularly useful in determining if a sound comes from above, below, in front, or behind.",
            "score": 166.89352416992188
        },
        {
            "docid": "3154127_4",
            "document": "Virtual acoustic space . When one listens to sounds over headphones (in what is known as the \"closed field\") the sound source appears to arise from center of the head. On the other hand, under normal, so-called free-field, listening conditions sounds are perceived as being externalized. The direction of a sound in space (see sound localization) is determined by the brain when it analyses the interaction of incoming sound with head and external ears. A sound arising to one side reaches the near ear before the far ear (creating an interaural time difference, ITD), and will also be louder at the near ear (creating an interaural level difference, ILD \u2013 also known as interaural intensity difference, IID). These binaural cues allow sounds to be lateralized. Although conventional stereo headphone signals make used of ILDs (not ITDs) the sound is not perceived as being externalized.",
            "score": 162.60276794433594
        },
        {
            "docid": "8398605_8",
            "document": "Microphone practice . There are two features of sound that the human brain uses to place objects in the stereo sound-field between the loudspeakers. These are the relative level (or loudness) difference between the two channels \"\u0394\u00a0L\", and the time delay difference in arrival times for the same sound in each channel \"\u0394\u00a0t\". The \"interaural\" signals (binaural \"ILD\" and \"ITD\") at the ears are not the stereo microphone signals which are coming from the loudspeakers, and are called \"interchannel\" signals (\"\u0394\u00a0L\" and \"\u0394\u00a0t\"). These signals are normally not mixed. Loudspeaker signals are different from the sound arriving at the ear. See the article \"Binaural recording for earphones\".",
            "score": 161.28663635253906
        },
        {
            "docid": "25663206_13",
            "document": "Psychoacoustics . Sound localization is the process of determining the location of a sound source. The brain utilizes subtle differences in loudness, tone and timing between the two ears to allow us to localize sound sources. Localization can be described in terms of three-dimensional position: the azimuth or horizontal angle, the zenith or vertical angle, and the distance (for static sounds) or velocity (for moving sounds). Humans, as most four-legged animals, are adept at detecting direction in the horizontal, but less so in the vertical due to the ears being placed symmetrically. Some species of owls have their ears placed asymmetrically, and can detect sound in all three planes, an adaption to hunt small mammals in the dark.",
            "score": 160.38064575195312
        },
        {
            "docid": "161005_5",
            "document": "Head-related transfer function . Humans estimate the location of a source by taking cues derived from one ear (\"monaural cues\"), and by comparing cues received at both ears (\"difference cues\" or \"binaural cues\"). Among the difference cues are time differences of arrival and intensity differences. The monaural cues come from the interaction between the sound source and the human anatomy, in which the original source sound is modified before it enters the ear canal for processing by the auditory system. These modifications encode the source location, and may be captured via an impulse response which relates the source location and the ear location. This impulse response is termed the \"head-related impulse response\" (HRIR). Convolution of an arbitrary source sound with the HRIR converts the sound to that which would have been heard by the listener if it had been played at the source location, with the listener's ear at the receiver location. HRIRs have been used to produce virtual surround sound.",
            "score": 160.1532440185547
        },
        {
            "docid": "41087200_9",
            "document": "Perceptual-based 3D sound localization . Interaural time and level differences (ITD, ILD) play a role in azimuth perception but can\u2019t explain vertical localization. According to the duplex theory, ITDs have a greater contribution to the localisation of low frequency sounds (below 1\u00a0kHz),while ILDs are used in the localisation of high frequency sound. The ILD arises from the fact that,a sound coming from a source located to one side of the head will have a higher intensity, or be louder, at the ear nearest the sound source. One can therefore create the illusion of a sound source emanating from one side of the head merely by adjusting the relative level of the sounds that are fed to two separated speakers or headphones. This is the basis of the commonly used pan control.",
            "score": 158.69361877441406
        },
        {
            "docid": "25345530_25",
            "document": "Models of neural computation . According to Jeffress, in order to compute the location of a sound source in space from interaural time differences, an auditory system relies on delay lines: the induced signal from an ipsilateral auditory receptor to a particular neuron is delayed for the same time as it takes for the original sound to go in space from that ear to the other. Each postsynaptic cell is differently delayed and thus specific for a particular inter-aural time difference. This theory is equivalent to the mathematical procedure of cross-correlation.",
            "score": 158.17312622070312
        },
        {
            "docid": "5442380_17",
            "document": "Sensory cue . Unless a sound is directly in front of or behind the individual, the sound stimuli will have a slightly different distance to travel to reach each ear. This difference in distance causes a slight delay in the time the signal is perceived by each ear. The magnitude of the interaural time difference is greater the more the signal comes from the side of the head. Thus, this time delay allows humans to accurately predict the location of incoming sound cues. Interaural level difference is caused by the difference in sound pressure level reaching the two ears. This is because the head blocks the sound waves for the further ear, causing less intense sound to reach it. This level difference between the two ears allows humans to accurately predict azimuth of an auditory signal. This effect only occurs at sounds that are high frequency.",
            "score": 158.09136962890625
        },
        {
            "docid": "1021754_13",
            "document": "Sound localization . Sound localization is the process of determining the location of a sound source. Objectively speaking, the major goal of sound localization is to simulate a specific sound field, including the acoustic sources, the listener, the media and environments of sound propagation. The brain utilizes subtle differences in intensity, spectral, and timing cues to allow us to localize sound sources. In this section, to more deeply understand the human auditory mechanism, we will briefly discuss about human ear localization theory.",
            "score": 156.2261505126953
        },
        {
            "docid": "32105732_4",
            "document": "Spatial hearing loss . Sound streams arriving from the left or right (the horizontal plane) are localised primarily by the small time differences of the same sound arriving at the two ears. A sound straight in front of the head is heard at the same time by both ears. A sound to the side of the head is heard approximately 0.0005 seconds later by the ear furthest away. A sound halfway to one side is heard approximately 0.0003 seconds later. This is the interaural time difference (ITD) cue and is measured by signal processing in the two central auditory pathways that begin after the cochlea and pass through the brainstem and mid-brain. Some of those with spatial hearing loss are unable to process ITD (low frequency) cues.",
            "score": 155.41558837890625
        },
        {
            "docid": "8953842_8",
            "document": "Computational auditory scene analysis . Because the ears receive audio signals at different times, the sound source can be determined by using the delays retrieved from the two ears. By cross-correlating the delays from the left and right channels (of the model), the coincided peaks can be categorized as the same localized sound, despite their temporal location in the input signal.  The use of interaural cross-correlation mechanism has been supported through physiological studies, paralleling the arrangement of neurons in the auditory midbrain.",
            "score": 155.3993377685547
        },
        {
            "docid": "50707986_18",
            "document": "Lloyd A. Jeffress . Beginning about 1940, Jeffress' primary research interest was the auditory system, especially the mechanisms underlying sound localization. His most cited article, \"A Place Theory of Sound Localization\", was in the 1948 Journal of Comparative and Physiological Psychology. In the article, he describes a hypothetical neural network capable of cross-correlating the temporal (time) information at the two ears and thereby extracting the small differences that can exist in the time of arrival of a wavefront at the two ears, thus localizing the sound. This neurocomputational model that explains how auditory systems can register and analyze small differences in the arrival time of sounds at the two ears in order to estimate the direction of sound sources became known as the Jeffress model.",
            "score": 154.7079620361328
        },
        {
            "docid": "52287299_10",
            "document": "3D sound synthesis . In synthesizing accurate 3D sound, rather than binaural recordings,attempts to model the human acoustic system by using microphones to record sounds in the ears of real people have been taken. The so-called HRTF(head-related transfer function) is obtained by comparing these recordings to the original sounds.The HRTF is a linear function based on the position of the sound source and considers many other information human also use to localize the sounds like interaural time difference, head shadow, pinna response, shoulder echo, head motion, early echo response, reverberation, and vision. The HRTF is then used to develop pairs of finite impulse response (FIR) filters for specific sound positions; each sound has two filters for left and right.In order to place a sound at a certain position in 3D space, the set of FIR filters that correspond to the position is applied to the incoming sound, yielding spatial sound.",
            "score": 153.5255584716797
        },
        {
            "docid": "1021754_43",
            "document": "Sound localization . If the ears are located at the side of the head, similar lateral localization cues as for the human auditory system can be used. This means: evaluation of interaural time differences (interaural phase differences) for lower frequencies and evaluation of interaural level differences for higher frequencies. The evaluation of interaural phase differences is useful, as long as it gives unambiguous results. This is the case, as long as ear distance is smaller than half the length (maximal one wavelength) of the sound waves. For animals with a larger head than humans the evaluation range for interaural phase differences is shifted towards lower frequencies, for animals with a smaller head, this range is shifted towards higher frequencies.",
            "score": 152.62094116210938
        },
        {
            "docid": "379234_4",
            "document": "Sensory nervous system . Sensory systems code for four aspects of a stimulus; type (modality), intensity, location, and duration. Arrival time of a sound pulse and phase differences of continuous sound are used for sound localization. Certain receptors are sensitive to certain types of stimuli (for example, different mechanoreceptors respond best to different kinds of touch stimuli, like sharp or blunt objects). Receptors send impulses in certain patterns to send information about the intensity of a stimulus (for example, how loud a sound is). The location of the receptor that is stimulated gives the brain information about the location of the stimulus (for example, stimulating a mechanoreceptor in a finger will send information to the brain about that finger). The duration of the stimulus (how long it lasts) is conveyed by firing patterns of receptors. These impulses are transmitted to the brain through afferent neurons.",
            "score": 152.11924743652344
        },
        {
            "docid": "41087200_3",
            "document": "Perceptual-based 3D sound localization . Human listeners combine information from two ears to localize and separate sound sources originating in different locations in a process called binaural hearing. The powerful signal processing methods found in the neural systems and brains of humans and other animals are flexible, environmentally adaptable, and take place rapidly and seemingly without effort. Emulating the mechanisms of binaural hearing can improve recognition accuracy and signal separation in DSP algorithms, especially in noisy environments. Furthermore, by understanding and exploiting biological mechanisms of sound localization, virtual sound scenes may be rendered with more perceptually relevant methods, allowing listeners to accurately perceive the locations of auditory events.",
            "score": 151.80755615234375
        },
        {
            "docid": "1021754_6",
            "document": "Sound localization . In vertebrates, inter-aural time differences are known to be calculated in the superior olivary nucleus of the brainstem. According to Jeffress, this calculation relies on delay lines: neurons in the superior olive which accept innervation from each ear with different connecting axon lengths. Some cells are more directly connected to one ear than the other, thus they are specific for a particular inter-aural time difference. This theory is equivalent to the mathematical procedure of cross-correlation. However, because Jeffress' theory is unable to account for the precedence effect, in which only the first of multiple identical sounds is used to determine the sounds' location (thus avoiding confusion caused by echoes), it cannot be entirely used to explain the response. Furthermore, a number of recent physiological observations made in the midbrain and brainstem of small mammals have shed considerable doubt on the validity of Jeffress' original ideas",
            "score": 151.549072265625
        },
        {
            "docid": "4548229_17",
            "document": "Interaural time difference . The lateral lemniscus (LL) is the main auditory tract in the brainstem connecting SOC to the inferior colliculus. The dorsal nucleus of the lateral lemniscus (DNLL) is a group of neurons separated by lemniscus fibres, these fibres are predominantly destined for the inferior colliculus (IC). In studies using an unanesthetized rabbit the DNLL was shown to alter the sensitivity of the IC neurons and may alter the coding of interaural timing differences (ITDs) in the IC.(Kuwada et al., 2005) The ventral nucleus of the lateral lemniscus (VNLL) is a chief source of input to the inferior colliculus. Research using rabbits shows the discharge patterns, frequency tuning and dynamic ranges of VNLL neurons supply the inferior colliculus with a variety of inputs, each enabling a different function in the analysis of sound.(Batra & Fitzpatrick, 2001)  In the inferior colliculus (IC) all the major ascending pathways from the olivary complex and the central nucleus converge. The IC is situated in the midbrain and consists of a group of nuclei the largest of these is the central nucleus of inferior colliculus (CNIC). The greater part of the ascending axons forming the lateral lemniscus will terminate in the ipsilateral CNIC however a few follow the commissure of Probst and terminate on the contralateral CNIC. The axons of most of the CNIC cells form the brachium of IC and leave the brainstem to travel to the ipsilateral thalamus. Cells in different parts of the IC tend to be monaural, responding to input from one ear, or binaural and therefore respond to bilateral stimulation.",
            "score": 150.96823120117188
        },
        {
            "docid": "1021754_32",
            "document": "Sound localization . When the head is stationary, the binaural cues for lateral sound localization (interaural time difference and interaural level difference) do not give information about the location of a sound in the median plane. Identical ITDs and ILDs can be produced by sounds at eye level or at any elevation, as long as the lateral direction is constant. However, if the head is rotated, the ITD and ILD change dynamically, and those changes are different for sounds at different elevations. For example, if an eye-level sound source is straight ahead and the head turns to the left, the sound becomes louder (and arrives sooner) at the right ear than at the left. But if the sound source is directly overhead, there will be no change in the ITD and ILD as the head turns. Intermediate elevations will produce intermediate degrees of change, and if the presentation of binaural cues to the two ears during head movement is reversed, the sound will be heard behind the listener. Hans Wallach artificially altered a sound\u2019s binaural cues during movements of the head. Although the sound was objectively placed at eye level, the dynamic changes to ITD and ILD as the head rotated were those that would be produced if the sound source had been elevated. In this situation, the sound was heard at the synthesized elevation. The fact that the sound sources objectively remained at eye level prevented monaural cues from specifying the elevation, showing that it was the dynamic change in the binaural cues during head movement that allowed the sound to be correctly localized in the vertical dimension. The head movements need not be actively produced; accurate vertical localization occurred in a similar setup when the head rotation was produced passively, by seating the blindfolded subject in a rotating chair. As long as the dynamic changes in binaural cues accompanied a perceived head rotation, the synthesized elevation was perceived.",
            "score": 149.40240478515625
        },
        {
            "docid": "1986906_5",
            "document": "Ormia ochracea . \"Ormia ochracea\" has become a model organism in sound localization experiments because of its unique \"ears\", which are complex structures inside the fly's prothorax near the bases of their front legs. The animal is too small for the time difference of sound arriving at the two ears to be calculated in the usual way, yet it can determine the direction of sound sources with exquisite precision. The tympanic membranes of opposite ears are directly connected mechanically, allowing resolution of nanosecond time differences and requiring a new neural coding strategy.",
            "score": 148.99586486816406
        },
        {
            "docid": "4302005_7",
            "document": "Superior olivary complex . The medial superior olive is thought to help locate the azimuth of a sound, that is, the angle to the left or right where the sound source is located. Sound elevation cues are not processed in the olivary complex. The fusiform cells of the dorsal cochlear nucleus (DCN), which are thought to contribute to localization in elevation, bypass the SOC and project directly to the inferior colliculus. Only horizontal data is present, but it does come from two different ear sources, which aids in the localizing of sound on the azimuth axis. The way in which the superior olive does this is by measuring the differences in time between two ear signals recording the same stimulus. Traveling around the head takes about 700 \u03bcs, and the medial superior olive is able to distinguish time differences much smaller than this. In fact, it is observed that people can detect interaural differences down to 10 \u03bcs. The nucleus is tonotopically organized, but the azimuthal receptive field projection is \"most likely a complex, nonlinear map\".",
            "score": 148.90985107421875
        },
        {
            "docid": "47338295_9",
            "document": "Sound localization in owls . In the sound level pathway, the posterior lateral lemniscal nucleus (mammalian lateral superior olive) is the site of binaural convergence and where IID is processed. Stimulation of the contralateral ear inhibits and that of the ipsilateral ear excites the neurons of the nuclei in each brain hemisphere independently. The degree of excitation and inhibition depends on sound pressure, and the difference between the strength of the inhibitory input and that of the excitatory input determines the rate at which neurons of the lemniscal nucleus fire. Thus the response of these neurons is a function of the difference in sound pressure between the two ears.",
            "score": 148.88021850585938
        },
        {
            "docid": "47862835_3",
            "document": "Virtual hammock . A network of nerves in the MSO detect the place that a threshold is overcome by sending electrochemical impulses along the auditory nerve pathway to the cortex for higher-level processing. These nerves permit the subconscious localization of a sound source; people can identify the direction from which a sound emanates before they can identify the type of source, which is done when the signals reach the cortex. A sound source produces a propagating sound wave that strikes the ear closer to it before traveling an extra distance to strike the opposite ear. The impulses produced in the auditory nerves which conduct signals to the brain for sound processing pass one another in the MSO at a point on the opposite side relative to the location sound source. If a sound impulse originates from a point equidistant to each ear (e.g. directly behind or in front of the head), the neuronal impulses from each ear will pass at a point in the center of the MSO, allowing us to unconsciously identify where a sound source is located. The Virtual Hammock effect is achieved by intentionally manipulating the passing point by shifting the maximum amplitudes of sound waveforms that are directed into each ear.",
            "score": 148.3774871826172
        },
        {
            "docid": "2263473_17",
            "document": "Volley theory . A fundamental frequency is the lowest frequency of a harmonic. In some cases, sound can have all the frequencies of a harmonic but be missing the fundamental frequency, this is known as missing fundamental. When listening to a sound with a missing fundamental, the human brain still receives information for all frequencies, including the fundamental frequency which does not exist in the sound. This implies that sound is encoded by neurons firing at all frequencies of a harmonic, therefore, the neurons must be locked in some way to result in the hearing of one sound. Congenital deafness or sensorineural hearing loss is an often used model for the study of the inner ear regarding pitch perception and theories of hearing in general. Frequency analysis of these individuals\u2019 hearing has given insight on common deviations from normal tuning curves, excitation patterns, and frequency discrimination ranges. By applying pure or complex tones, information on pitch perception can be obtained. In 1983, it was shown that subjects with low frequency sensorineural hearing loss demonstrated abnormal psychophysical tuning curves. Changes in the spatial responses in these subjects showed similar pitch judgment abilities when compared to subjects with normal spatial responses. This was especially true regarding low frequency stimuli. These results suggest that the place theory of hearing does not explain pitch perception at low frequencies, but that the temporal (frequency) theory is more likely. This conclusion is due to the finding that when deprived of basilar membrane place information, these patients still demonstrated normal pitch perception. Computer models for pitch perception and loudness perception are often used during hearing studies on acoustically impaired subjects. The combination of this modeling and knowledge of natural hearing allows for better development of hearing aids.",
            "score": 147.99737548828125
        },
        {
            "docid": "39200136_12",
            "document": "Hans Wallach . In a series of papers Wallach explored the ability of humans to locate sounds in the median plane \u2013 that is, to determine whether a sound comes from a source at the same elevation as the ears or from a source that is higher or lower, or even in back of the head. Binaural sound cues, including the phasing or time of the sound\u2019s arrival at each ear and the sound\u2019s relative intensity at the two ears (known respectively as ITD and ILD) enable a listener to determine a sound\u2019s lateral location (whether it is on the left, right, or straight ahead). But two sounds at different elevations can present identical ITD and ILD information to the ears, and so binaural cues to a stationary ear do not suffice to identify a sound\u2019s location in the median plane. Monaural cues that depend on the shape of the head and the structure of the external ear help with vertical localization, but binaural cues also play a part if the head is not stationary.",
            "score": 146.96517944335938
        }
    ]
}