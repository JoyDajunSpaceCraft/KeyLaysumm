{
    "q": [
        {
            "docid": "4548229_2",
            "document": "Interaural time difference . The interaural time difference (or ITD) when concerning humans or animals, is the difference in arrival time of a sound between two ears. It is important in the localization of sounds, as it provides a cue to the direction or angle of the sound source from the head. If a signal arrives at the head from one side, the signal has further to travel to reach the far ear than the near ear. This pathlength difference results in a time difference between the sound's arrivals at the ears, which is detected and aids the process of identifying the direction of sound source.",
            "score": 205.679114818573
        },
        {
            "docid": "5051081_4",
            "document": "Eric Knudsen . In 1978, Knudsen and Konishi presented the discovery of an auditory map of space in the midbrain of the barn owl. This discovery was groundbreaking because it unearthed the first non-somatotopic space map in the brain. The map was found in the owl\u2019s midbrain, in the lateral and anterior mesencephalicus lateralis dorsalis (MLD), a structure now referred to as the inferior colliculus. Unlike most sound-localization maps, this map was found to be two-dimensional, with units arranged spatially to represent both the vertical and horizontal location of sound. Knudsen and Konishi discovered that units in this structure respond preferentially to sounds originating in a particular region in space. In the 1978 paper, elevation and azimuth (location in the horizontal plane) were shown to be the two coordinates of the map. Using a speaker set on a rotatable hemispherical track, Knudsen and Konishi presented owls with auditory stimulus from various locations in space and recorded the resulting neuronal activity. They found that neurons in this part of the MLD were organized according to the location of their receptive field, with azimuth varying along the horizontal plane of the space map and elevation varying vertically.  Knudsen followed this discovery with research into specific sound localization mechanisms. Two main auditory cues used by the barn owl to localize sound are interaural time difference (ITD) and interaural intensity difference (IID). The owl\u2019s ears are asymmetric, with the right ear\u2019s opening being directed higher than that of the left. This asymmetry allows the barn owl to determine the elevation of a sound by comparing sound levels between its two ears. Interaural time differences provide the owl with information regarding a sound\u2019s azimuth; sound will reach the ear closer to the sound source before reaching the farther ear, and this time difference can be detected and interpreted as an azimuthal direction. At low frequencies, the wavelength of a sound is wider than the owl's facial ruff, and the ruff does not affect detection of azimuth. At high frequencies, the ruff plays a role in reflecting sound for heightened sensitivity to vertical elevation. Therefore, with wide-band noise, containing both high and low frequencies, the owl could use interaural spectrum difference to obtain information about both azimuth and elevation. In 1979, Knudsen and Konishi showed that the barn owl uses interaural spectrum information in sound localization. They presented owls with both wide-bandwidth noise and pure tones. The birds were able to successfully locate pure tones (since they could still gather information from IID and ITD), but their error rate was much lower when localizing wide-bandwidth noise. This indicates that the birds utilize interaural spectrum differences to improve their accuracy.",
            "score": 203.97733807563782
        },
        {
            "docid": "4548229_7",
            "document": "Interaural time difference . Feddersen et al. (1957) also conducted experiments taking measurements on how ITDs alter with changing the azimuth of the loudspeaker around the head at different frequencies. But unlike the Woodworth experiments human subjects were used rather than a model of the head. The experiment results agreed with the conclusion made by Woodworth about ITDs. The experiments also concluded that is there is no difference in ITDs when sounds are provided from directly in front or behind at 0\u00b0 and 180\u00b0 azimuth. The explanation for this is that the sound is equidistant from both ears. Interaural time differences alter as the loudspeaker is moved around the head. The maximum ITD of 660 \u03bcs occurs when a sound source is positioned at 90\u00b0 azimuth to one ear.",
            "score": 222.92064094543457
        },
        {
            "docid": "39265695_7",
            "document": "Stimulus filtering . Female flies of the genus \"Ormia ochracea\" possess organs in their bodies that can detect frequencies of cricket sounds from meters away. This process is important for the survival of their species because females will lay their first instar larvae into the body of the cricket, where they will feed and molt for approximately seven days. After this period, the larvae grow into flies and the cricket usually perishes. Researchers were puzzled about how precise hearing ability could arise from a small ear structure. Normal animals detect and locate sounds using the interaural time difference (ITD) and the interaural level difference (ILD). The ITD is the difference in the time it takes sound to reach the ear. ILD is the difference in sound intensity measure between both ears. At maximum, the ITD would only reach about 1.5 microseconds and the ILD would be less than one decibel. These small values make it hard to sense the differences. To solve these issues, researchers studied the mechanical aspects of flies\u2019 ears. They found that they have a presternum structure linking both tympanal membranes that is critical in detecting sound and localization. The structure acts as a lever by transferring and amplifying vibrational energy between the membranes. After sound hits the membranes at different amplitudes, the presternum sets up symmetrical vibration modes through bending and rocking. This effect helps the nervous system distinguish which side the sound is coming from. Because the presternum acts as an intertympanal bridge, the ITD is increased from 1.5 us to 55 us and the ILD is increased from less than one decibel to over 10 decibels.",
            "score": 217.00879681110382
        },
        {
            "docid": "25663206_13",
            "document": "Psychoacoustics . Sound localization is the process of determining the location of a sound source. The brain utilizes subtle differences in loudness, tone and timing between the two ears to allow us to localize sound sources. Localization can be described in terms of three-dimensional position: the azimuth or horizontal angle, the zenith or vertical angle, and the distance (for static sounds) or velocity (for moving sounds). Humans, as most four-legged animals, are adept at detecting direction in the horizontal, but less so in the vertical due to the ears being placed symmetrically. Some species of owls have their ears placed asymmetrically, and can detect sound in all three planes, an adaption to hunt small mammals in the dark.",
            "score": 145.56245732307434
        },
        {
            "docid": "1021754_32",
            "document": "Sound localization . When the head is stationary, the binaural cues for lateral sound localization (interaural time difference and interaural level difference) do not give information about the location of a sound in the median plane. Identical ITDs and ILDs can be produced by sounds at eye level or at any elevation, as long as the lateral direction is constant. However, if the head is rotated, the ITD and ILD change dynamically, and those changes are different for sounds at different elevations. For example, if an eye-level sound source is straight ahead and the head turns to the left, the sound becomes louder (and arrives sooner) at the right ear than at the left. But if the sound source is directly overhead, there will be no change in the ITD and ILD as the head turns. Intermediate elevations will produce intermediate degrees of change, and if the presentation of binaural cues to the two ears during head movement is reversed, the sound will be heard behind the listener. Hans Wallach artificially altered a sound\u2019s binaural cues during movements of the head. Although the sound was objectively placed at eye level, the dynamic changes to ITD and ILD as the head rotated were those that would be produced if the sound source had been elevated. In this situation, the sound was heard at the synthesized elevation. The fact that the sound sources objectively remained at eye level prevented monaural cues from specifying the elevation, showing that it was the dynamic change in the binaural cues during head movement that allowed the sound to be correctly localized in the vertical dimension. The head movements need not be actively produced; accurate vertical localization occurred in a similar setup when the head rotation was produced passively, by seating the blindfolded subject in a rotating chair. As long as the dynamic changes in binaural cues accompanied a perceived head rotation, the synthesized elevation was perceived.",
            "score": 194.4641169309616
        },
        {
            "docid": "39200136_12",
            "document": "Hans Wallach . In a series of papers Wallach explored the ability of humans to locate sounds in the median plane \u2013 that is, to determine whether a sound comes from a source at the same elevation as the ears or from a source that is higher or lower, or even in back of the head. Binaural sound cues, including the phasing or time of the sound\u2019s arrival at each ear and the sound\u2019s relative intensity at the two ears (known respectively as ITD and ILD) enable a listener to determine a sound\u2019s lateral location (whether it is on the left, right, or straight ahead). But two sounds at different elevations can present identical ITD and ILD information to the ears, and so binaural cues to a stationary ear do not suffice to identify a sound\u2019s location in the median plane. Monaural cues that depend on the shape of the head and the structure of the external ear help with vertical localization, but binaural cues also play a part if the head is not stationary.",
            "score": 203.8978192806244
        },
        {
            "docid": "1021754_21",
            "document": "Sound localization . In 1907, Lord Rayleigh utilized tuning forks to generate monophonic excitation and studied the lateral sound localization theory on a human head model without auricle. He first presented the interural clue difference based sound localization theory, which is known as Duplex Theory. Human ears are on the different sides of the head, thus they have different coordinates in space. As shown in fig. 2, since the distances between the acoustic source and ears are different, there are time difference and intensity difference between the sound signals of two ears. We call those kinds of differences as Interaural Time Difference (ITD) and Interaural Intensity Difference (IID) respectively.",
            "score": 174.45932531356812
        },
        {
            "docid": "8398605_8",
            "document": "Microphone practice . There are two features of sound that the human brain uses to place objects in the stereo sound-field between the loudspeakers. These are the relative level (or loudness) difference between the two channels \"\u0394\u00a0L\", and the time delay difference in arrival times for the same sound in each channel \"\u0394\u00a0t\". The \"interaural\" signals (binaural \"ILD\" and \"ITD\") at the ears are not the stereo microphone signals which are coming from the loudspeakers, and are called \"interchannel\" signals (\"\u0394\u00a0L\" and \"\u0394\u00a0t\"). These signals are normally not mixed. Loudspeaker signals are different from the sound arriving at the ear. See the article \"Binaural recording for earphones\".",
            "score": 175.266503572464
        },
        {
            "docid": "32105732_4",
            "document": "Spatial hearing loss . Sound streams arriving from the left or right (the horizontal plane) are localised primarily by the small time differences of the same sound arriving at the two ears. A sound straight in front of the head is heard at the same time by both ears. A sound to the side of the head is heard approximately 0.0005 seconds later by the ear furthest away. A sound halfway to one side is heard approximately 0.0003 seconds later. This is the interaural time difference (ITD) cue and is measured by signal processing in the two central auditory pathways that begin after the cochlea and pass through the brainstem and mid-brain. Some of those with spatial hearing loss are unable to process ITD (low frequency) cues.",
            "score": 192.50305461883545
        },
        {
            "docid": "5442380_17",
            "document": "Sensory cue . Unless a sound is directly in front of or behind the individual, the sound stimuli will have a slightly different distance to travel to reach each ear. This difference in distance causes a slight delay in the time the signal is perceived by each ear. The magnitude of the interaural time difference is greater the more the signal comes from the side of the head. Thus, this time delay allows humans to accurately predict the location of incoming sound cues. Interaural level difference is caused by the difference in sound pressure level reaching the two ears. This is because the head blocks the sound waves for the further ear, causing less intense sound to reach it. This level difference between the two ears allows humans to accurately predict azimuth of an auditory signal. This effect only occurs at sounds that are high frequency.",
            "score": 176.64673900604248
        },
        {
            "docid": "14532984_7",
            "document": "Coincidence detection in neurobiology . Coincidence detection has been shown to be a major factor in sound localization along the azimuth plane in several organisms. In 1948, Lloyd A. Jeffress proposed that some organisms may have a collection of neurons that receive auditory input from each ear. The neural pathways to these neurons are called delay lines. Jeffress claimed that the neurons that the delay lines link act as coincidence detectors by firing maximally when receiving simultaneous inputs from both ears. When a sound is heard, sound waves may reach the ears at different times. This is referred to as the interaural time difference (ITD). Due to differing lengths and a finite conduction speed within the axons of the delay lines, different coincidence detector neurons will fire when sound comes from different positions along the azimuth. Jeffress' model proposes that two signals even from an asynchronous arrival of sound in the cochlea of each ear will converge synchronously on a coincidence detector in the auditory cortex based on the magnitude of the ITD (Fig. 2). Therefore, the ITD should correspond to an anatomical map that can be found within the brain. Masakazu Konishi's study on barn owls shows that this is true. Sensory information from the hair cells of the ears travels to the ipsilateral nucleus magnocellularis. From here, the signals project ipsilaterally and contralaterally to two nucleus laminari. Each nucleus laminaris contains coincidence detectors that receive auditory input from the left and the right ear. Since the ipsilateral axons enter the nucleus laminaris dorsally while the contralateral axons enter ventrally, sounds from various positions along the azimuth correspond directly to stimulation of different depths of the nucleus laminaris. From this information, a neural map of auditory space was formed. The function of the nucleus laminaris parallels that of the medial superior olive in mammals.",
            "score": 194.83581614494324
        },
        {
            "docid": "47338295_4",
            "document": "Sound localization in owls . ITD occurs whenever the distance from the source of sound to the two ears is different, resulting in differences in the arrival times of the sound at the two ears. When the sound source is directly in front of the owl, there is no ITD, i.e. the ITD is zero. In sound localization, ITDs are used as cues for location in the azimuth. ITD changes systematically with azimuth. Sounds to the right arrive first at the right ear; sounds to the left arrive first at the left ear.",
            "score": 214.7763569355011
        },
        {
            "docid": "41087200_4",
            "document": "Perceptual-based 3D sound localization . While the relationship between human perception of sound and various attributes of the sound field is not yet well understood, DSP algorithms for sound localization are able to employ several mechanisms found in neural systems, including the interaural time difference (ITD, the difference in arrival time of a sound between two locations), the interaural intensity difference (IID, the difference in intensity of a sound between two locations), artificial pinnae, the precedence effect, and head-related transfer functions (HRTF) . When localizing 3D sound in spatial domain, one could take into account that the incoming sound signal could be reflected, defracted and scattered by the upper torso of the human which consists of shoulders, head and pinnae.Localization also depends on the direction of the sound source.",
            "score": 183.9335914850235
        },
        {
            "docid": "2485942_5",
            "document": "Audiogram . Hearing thresholds of humans and other mammals can be found by using behavioural hearing tests or physiological tests. An audiogram can be obtained using a behavioural hearing test called Audiometry. For humans the test involves different tones being presented at a specific frequency (pitch) and intensity (loudness). When the person hears the sound they raise their hand or press a button so that the tester knows that they have heard it. The lowest intensity sound they can hear is recorded. The test varies for children, their response to the sound can be a head turn or using a toy. The child learns what they can do when they hear the sound, for example they are taught that when they heard the sound they can put a toy man in a boat. This is referred to as conditioned play audiometry. Visual reinforcement audiometry is also used with children. When the child hears the sound, they look in the direction the sound came from and are reinforced with a light and/or animated toy. A similar technique can be used when testing some animals but instead of a toy, food can be used as a reward for responding to the sound.  Physiological tests do not need the patient to respond (Katz 2002). For example when performing the brainstem auditory evoked potentials the patient\u2019s brainstem responses are being measured when a sound is played into their ear, or otoacoustic emissions (OAEs) which are generated by a healthy inner ear either spontaneously or evoked by an outside stimulus.  In the US, the NIOSH recommends that people who are regularly exposed to hazardous noise have their hearing tested once a year, or every three years otherwise.",
            "score": 139.11766362190247
        },
        {
            "docid": "32116125_4",
            "document": "Amblyaudia . Amblyaudia is a deficit in binaural integration of environmental information entering the auditory system. It is a disorder related to brain organization and function rather than what is typically considered a \u201chearing loss\u201d (damage to the cochlea). It may be genetic or developmentally acquired or both. When animals are temporarily deprived of hearing from an early age, profound changes occur in the brain. Specifically, cell sizes in brainstem nuclei are reduced, the configuration of brainstem dendrites are altered and neurons respond in different ways to sounds presented to both the deprived and non-deprived ears (in cases of asymmetric deprivation). This last point is particularly important for listening tasks that require inputs from two ears to perform well. There are multiple auditory functions that rely on the computation of well calibrated inputs from the two ears. Chief among these is the ability to localize sound sources and separate what we want to hear from a background of noise. In the brainstem, the auditory system compares the timing and levels of sounds between the two ears to encode the location of sound sources (sounds that originate from our right as opposed to left side are louder and arrive earlier in our right ear). This ability to separate sound sources not only helps us locate the trajectories of moving objects, but also to separate different sound sources in noisy environments.",
            "score": 185.02668833732605
        },
        {
            "docid": "4302005_7",
            "document": "Superior olivary complex . The medial superior olive is thought to help locate the azimuth of a sound, that is, the angle to the left or right where the sound source is located. Sound elevation cues are not processed in the olivary complex. The fusiform cells of the dorsal cochlear nucleus (DCN), which are thought to contribute to localization in elevation, bypass the SOC and project directly to the inferior colliculus. Only horizontal data is present, but it does come from two different ear sources, which aids in the localizing of sound on the azimuth axis. The way in which the superior olive does this is by measuring the differences in time between two ear signals recording the same stimulus. Traveling around the head takes about 700 \u03bcs, and the medial superior olive is able to distinguish time differences much smaller than this. In fact, it is observed that people can detect interaural differences down to 10 \u03bcs. The nucleus is tonotopically organized, but the azimuthal receptive field projection is \"most likely a complex, nonlinear map\".",
            "score": 161.87523651123047
        },
        {
            "docid": "41087200_8",
            "document": "Perceptual-based 3D sound localization . Interaural level differences (ILD) represents the difference in sound pressure level reaching the two ears. They provide salient cues for localizing high-frequency sounds in space, and populations of neurons that are sensitive to ILD are found at almost every synaptic level from brain stem to cortex. These cells are predominantly excited by stimulation of one ear and predominantly inhibited by stimulation of the other ear, such that the magnitude of their response is determined in large part by the intensities at the 2 ears. This gives rise to the concept of resonant damping. Interaural level difference (ILD) is best for high frequency sounds because low frequency sounds are not attenuated much by the head. ILD (also known as Interaural Intensity Difference) arises when the sound source is not centred, the listener's head partially shadows the ear opposite to the source, diminishing the intensity of the sound in that ear (particularly at higher frequencies). The pinnae filters the sound in a way that is directionally dependent. This is particularly useful in determining if a sound comes from above, below, in front, or behind.",
            "score": 193.01834571361542
        },
        {
            "docid": "1021754_31",
            "document": "Sound localization . The human outer ear, i.e. the structures of the pinna and the external ear canal, form direction-selective filters. Depending on the sound input direction in the median plane, different filter resonances become active. These resonances implant direction-specific patterns into the frequency responses of the ears, which can be evaluated by the auditory system (directional bands) for vertical sound localization. Together with other direction-selective reflections at the head, shoulders and torso, they form the outer ear transfer functions. These patterns in the ear's frequency responses are highly individual, depending on the shape and size of the outer ear. If sound is presented through headphones, and has been recorded via another head with different-shaped outer ear surfaces, the directional patterns differ from the listener's own, and problems will appear when trying to evaluate directions in the median plane with these foreign ears. As a consequence, front\u2013back permutations or inside-the-head-localization can appear when listening to dummy head recordings, or otherwise referred to as binaural recordings. It has been shown that human subjects can monaurally localize high frequency sound but not low frequency sound. Binaural localization, however, was possible with lower frequencies. This is likely due to the pinna being small enough to only interact with sound waves of high frequency. It seems that people can only accurately localize the elevation of sounds that are complex and include frequencies above 7,000\u00a0Hz, and a pinna must be present.",
            "score": 144.8076218366623
        },
        {
            "docid": "3154127_4",
            "document": "Virtual acoustic space . When one listens to sounds over headphones (in what is known as the \"closed field\") the sound source appears to arise from center of the head. On the other hand, under normal, so-called free-field, listening conditions sounds are perceived as being externalized. The direction of a sound in space (see sound localization) is determined by the brain when it analyses the interaction of incoming sound with head and external ears. A sound arising to one side reaches the near ear before the far ear (creating an interaural time difference, ITD), and will also be louder at the near ear (creating an interaural level difference, ILD \u2013 also known as interaural intensity difference, IID). These binaural cues allow sounds to be lateralized. Although conventional stereo headphone signals make used of ILDs (not ITDs) the sound is not perceived as being externalized.",
            "score": 200.44899320602417
        },
        {
            "docid": "4548229_6",
            "document": "Interaural time difference . Experiments conducted by Woodworth (1938) tested the duplex theory by using a solid sphere to model the shape of the head and measuring the ITDs as a function of azimuth for different frequencies. The model used had a distance between the 2 ears of approximately 22\u201323\u00a0cm. Initial measurements found that there was a maximum time delay of approximately 660 \u03bcs when the sound source was placed at directly 90\u00b0 azimuth to one ear. This time delay correlates to the wavelength of a sound input with a frequency of 1500\u00a0Hz. The results concluded that when a sound played had a frequency less than 1500\u00a0Hz the wavelength is greater than this maximum time delay between the ears. Therefore there is a phase difference between the sound waves entering the ears providing acoustic localisation cues. With a sound input with a frequency closer to 1500\u00a0Hz the wavelength of the sound wave is similar to the natural time delay. Therefore due to the size of the head and the distance between the ears there is a reduced phase difference so localisations errors start to be made. When a high frequency sound input is used with a frequency greater than 1500\u00a0Hz, the wavelength is shorter than the distance between the 2 ears, a head shadow is produced and ILD provide cues for the localisation of this sound.",
            "score": 167.311443567276
        },
        {
            "docid": "8436042_6",
            "document": "MPEG Surround . MPEG Surround coding uses our capacity to perceive sound in the 3D and captures that perception in a compact set of parameters. Spatial perception is primarily attributed to three parameters, or cues, describing how humans localize sound in the horizontal plane: Interaural level difference (ILD), Interaural time difference (ITD) and Interaural coherence (IC). This three concepts are illustrated in next image. Direct, or first-arrival, waveforms from the source hit the left ear at time, while direct sound received by the right ear is diffracted around the head, with time delay and level attenuation, associated. These two effects result in ITD and ILD are associated with the main source. At last, in a reverberant environment, reflected sound from the source, or sound from diffuse source, or uncorrelated sound can hit both ears, all of them are related with IC.",
            "score": 209.5969214439392
        },
        {
            "docid": "161005_5",
            "document": "Head-related transfer function . Humans estimate the location of a source by taking cues derived from one ear (\"monaural cues\"), and by comparing cues received at both ears (\"difference cues\" or \"binaural cues\"). Among the difference cues are time differences of arrival and intensity differences. The monaural cues come from the interaction between the sound source and the human anatomy, in which the original source sound is modified before it enters the ear canal for processing by the auditory system. These modifications encode the source location, and may be captured via an impulse response which relates the source location and the ear location. This impulse response is termed the \"head-related impulse response\" (HRIR). Convolution of an arbitrary source sound with the HRIR converts the sound to that which would have been heard by the listener if it had been played at the source location, with the listener's ear at the receiver location. HRIRs have been used to produce virtual surround sound.",
            "score": 164.16526687145233
        },
        {
            "docid": "7527647_17",
            "document": "Binaural fusion . The auditory space of binaural hearing is constructed based on the analysis of differences in two different binaural cues in the horizontal plane: sound level, or ILD, and arrival time at the two ears, or ITD, which allow for the comparison of the sound heard at each eardrum. ITD is processed in the MSO and results from sounds arriving earlier at one ear than the other; this occurs when the sound does not arise from directly in front or directly behind the hearer. ILD is processed in the LSO and results from the shadowing effect that is produced at the ear that is farther from the sound source. Outputs from the SOC are targeted to the dorsal nucleus of the lateral lemniscus as well as the IC.",
            "score": 186.35396766662598
        },
        {
            "docid": "55971_25",
            "document": "Inner ear . Neurons within the ear respond to simple tones, and the brain serves to process other increasingly complex sounds. An average adult is typically able to detect sounds ranging between 20 and 20,000\u00a0Hz. The ability to detect higher pitch sounds decreases in older humans.",
            "score": 121.2690589427948
        },
        {
            "docid": "34118956_15",
            "document": "Perception of infrasound . In order to use infrasound for navigation, it is necessary to be able to localize the source of the sounds. The known mechanisms for sound localizations make use of the time difference cues at the two ears. However, infrasound has such long wavelengths that these mechanisms would not be effective for an animal the size of a pigeon. An alternative method that has been hypothesized is through the use of the Doppler shift. A Doppler shift occurs when there is relative motion between a sound source and a perceiver and slightly shifts the perceived frequency of the sound. When a flying bird is changing direction, the amplitude of the Doppler shift between it and an infrasonic source would change, enabling the bird to locate the source. This kind of mechanism would require the ability to detect very small changes in frequency. A pigeon typically flies at 20\u00a0km/hr, so a turn could cause up to a 12% modulation of an infrasonic stimulus. According to response measurements, pigeons are able to distinguish frequency changes of 1-7% in the infrasonic range, showing that the use of Doppler shifts for infrasound localization may be within the pigeon\u2019s perceptive capabilities.",
            "score": 139.84080386161804
        },
        {
            "docid": "233830_28",
            "document": "Ambisonics . At low frequencies, where the wavelength is large compared to the human head, an incoming sound diffracts around it, so that there is virtually no acoustic shadow and hence no level difference between the ears. In this range, the only available information is the phase relationship between the two ear signals, called \"interaural time difference\", or \"ITD\". Evaluating this time difference allows for precise localisation within a \"cone of confusion\": the angle of incidence is unambiguous, but the ITD is the same for sounds from the front or from the back. As long as the sound is not totally unknown to the subject, the confusion can usually be resolved by perceiving the timbral front-back variations caused by the ear flaps (or \"pinnae\").",
            "score": 176.6757299900055
        },
        {
            "docid": "52287299_10",
            "document": "3D sound synthesis . In synthesizing accurate 3D sound, rather than binaural recordings,attempts to model the human acoustic system by using microphones to record sounds in the ears of real people have been taken. The so-called HRTF(head-related transfer function) is obtained by comparing these recordings to the original sounds.The HRTF is a linear function based on the position of the sound source and considers many other information human also use to localize the sounds like interaural time difference, head shadow, pinna response, shoulder echo, head motion, early echo response, reverberation, and vision. The HRTF is then used to develop pairs of finite impulse response (FIR) filters for specific sound positions; each sound has two filters for left and right.In order to place a sound at a certain position in 3D space, the set of FIR filters that correspond to the position is applied to the incoming sound, yielding spatial sound.",
            "score": 152.6387358903885
        },
        {
            "docid": "34118956_14",
            "document": "Perception of infrasound . In experiments using heart-rate conditioning, Pigeons have been found to be able to detect sounds in the infrasonic range at frequencies as low as 0.5\u00a0Hz. For frequencies below 10\u00a0Hz, the pigeon threshold is at about 55\u00a0dB which is at least 50\u00a0dB more sensitive than humans. Pigeons are able to discriminate small frequency differences in sounds at between 1\u00a0Hz and 20\u00a0Hz, with sensitivity ranging from a 1% shift at 20\u00a0Hz to a 7% shift at 1\u00a0Hz. Sensitivities are measured through a heart-rate conditioning test. In this test, an anesthetized bird is presented with a single sound or a sequence of sounds, followed by an electric shock. The bird\u2019s heart-rate will increase in anticipation of a shock. Therefore, a measure of the heart-rate can determine whether the bird is able to distinguish between stimuli that would be followed by a shock from stimuli that would not. Similar methods have also been used to determine the pigeon\u2019s sensitivity to barometric pressure changes, polarized light, and UV light. These experiments were conducted in sound isolation chambers to avoid the influence of ambient noise. Infrasonic stimuli are hard to produce and are often transmitted through a filter that attenuates higher frequency components. Also, the tone burst stimuli used in these experiments were presented with stimulus onset and offsets ramped on and off gradually in order to prevent initial turn-on and turn-off transients.",
            "score": 115.26237416267395
        },
        {
            "docid": "1021754_24",
            "document": "Sound localization . For frequencies above 1600\u00a0Hz the dimensions of the head are greater than the length of the sound waves. An unambiguous determination of the input direction based on interaural phase alone is not possible at these frequencies. However, the interaural level differences become larger, and these level differences are evaluated by the auditory system. Also, group delays between the ears can be evaluated, and is more pronounced at higher frequencies; that is, if there is a sound onset, the delay of this onset between the ears can be used to determine the input direction of the corresponding sound source. This mechanism becomes especially important in reverberant environments. After a sound onset there is a short time frame where the direct sound reaches the ears, but not yet the reflected sound. The auditory system uses this short time frame for evaluating the sound source direction, and keeps this detected direction as long as reflections and reverberation prevent an unambiguous direction estimation. The mechanisms described above cannot be used to differentiate between a sound source ahead of the hearer or behind the hearer; therefore additional cues have to be evaluated.",
            "score": 159.0618155002594
        },
        {
            "docid": "3154127_5",
            "document": "Virtual acoustic space . The perception of an externalized sound source is due to the frequency and direction-dependent filtering of the pinna which makes up the external ear structure. Unlike ILDs and ITDs, these spectral localization cues are generated monaurally. The same sound presented from different directions will produce at the eardrum a different pattern of peaks and notches across frequency. The pattern of these monaural spectral cues is different for different listeners. Spectral cues are vital for making elevation judgments and distinguishing if a sound arose from in front or behind the listener. They are also vital for creating the illusion of an externalized sound source. Since only ILDs are present in stereo recordings, the lack of spectral cues means that the sound is not perceived as being externalized. The easiest way of re-creating this illusion is to make a recording using two microphones placed inside a dummy human head. Playing back the recording via headphones will create the illusion of an externalized sound source.",
            "score": 178.05192112922668
        },
        {
            "docid": "22751270_6",
            "document": "Illusory conjunctions . Auditory illusory conjunctions occur either when two sounds are presented in different positions in space, but a single sound is heard, or when two different sounds are presented in different positions in space, but some of them are heard in the wrong spatial location. In the octave illusion, the listener is presented via earphones with a 20-second sequence consisting of two alternating tones that are an octave apart, and are repeatedly presented in alternation. The tones are 250 ms in duration. The same sequence is presented to the two ears, but when the right ear receives the high tone the left ear receives the low tone, and vice versa. Most listeners hear this sequence as a single tone that repeatedly changes both in pitch and in location. It has been suggested that time limitations contribute to this auditory illusory conjunction but see other explanations in terms of separate 'what' and 'where' pathways. In the scale illusion, the listener is presented via headphones with a scale with alternating tones switching from ear to ear. The scale is presented in both ascending and descending form, such that when a tone form the ascending scale is in the right ear, a tone from the descending scale is in the left ear. This gives rise to illusory conjunctions of pitch and location, such that all the higher tones are heard in one ear and all the lower tones in the other ear. Similar illusory conjunctions give rise to the chromatic illusion, the glissando illusion, and the cambiata illusion.",
            "score": 122.49107098579407
        }
    ],
    "r": [
        {
            "docid": "4548229_7",
            "document": "Interaural time difference . Feddersen et al. (1957) also conducted experiments taking measurements on how ITDs alter with changing the azimuth of the loudspeaker around the head at different frequencies. But unlike the Woodworth experiments human subjects were used rather than a model of the head. The experiment results agreed with the conclusion made by Woodworth about ITDs. The experiments also concluded that is there is no difference in ITDs when sounds are provided from directly in front or behind at 0\u00b0 and 180\u00b0 azimuth. The explanation for this is that the sound is equidistant from both ears. Interaural time differences alter as the loudspeaker is moved around the head. The maximum ITD of 660 \u03bcs occurs when a sound source is positioned at 90\u00b0 azimuth to one ear.",
            "score": 222.920654296875
        },
        {
            "docid": "39265695_7",
            "document": "Stimulus filtering . Female flies of the genus \"Ormia ochracea\" possess organs in their bodies that can detect frequencies of cricket sounds from meters away. This process is important for the survival of their species because females will lay their first instar larvae into the body of the cricket, where they will feed and molt for approximately seven days. After this period, the larvae grow into flies and the cricket usually perishes. Researchers were puzzled about how precise hearing ability could arise from a small ear structure. Normal animals detect and locate sounds using the interaural time difference (ITD) and the interaural level difference (ILD). The ITD is the difference in the time it takes sound to reach the ear. ILD is the difference in sound intensity measure between both ears. At maximum, the ITD would only reach about 1.5 microseconds and the ILD would be less than one decibel. These small values make it hard to sense the differences. To solve these issues, researchers studied the mechanical aspects of flies\u2019 ears. They found that they have a presternum structure linking both tympanal membranes that is critical in detecting sound and localization. The structure acts as a lever by transferring and amplifying vibrational energy between the membranes. After sound hits the membranes at different amplitudes, the presternum sets up symmetrical vibration modes through bending and rocking. This effect helps the nervous system distinguish which side the sound is coming from. Because the presternum acts as an intertympanal bridge, the ITD is increased from 1.5 us to 55 us and the ILD is increased from less than one decibel to over 10 decibels.",
            "score": 217.00880432128906
        },
        {
            "docid": "47338295_4",
            "document": "Sound localization in owls . ITD occurs whenever the distance from the source of sound to the two ears is different, resulting in differences in the arrival times of the sound at the two ears. When the sound source is directly in front of the owl, there is no ITD, i.e. the ITD is zero. In sound localization, ITDs are used as cues for location in the azimuth. ITD changes systematically with azimuth. Sounds to the right arrive first at the right ear; sounds to the left arrive first at the left ear.",
            "score": 214.7763671875
        },
        {
            "docid": "4548229_5",
            "document": "Interaural time difference . The duplex theory states that ITDs are used to localise low frequency sounds, in particular, while ILDs are used in the localisation of high frequency sound inputs. However, the frequency ranges for which the auditory system can use ITDs and ILDs significantly overlap, and most natural sounds will have both high and low frequency components, so that the auditory system will in most cases have to combine information from both ITDs and ILDs to judge the location of a sound source.  A consequence of this duplex system is that it is also possible to generate so-called \"cue trading\" or \"time\u2013intensity trading\" stimuli on headphones, where ITDs pointing to the left are offset by ILDs pointing to the right, so the sound is perceived as coming from the midline. A limitation of the duplex theory is that the theory does not completely explain directional hearing, as no explanation is given for the ability to distinguish between a sound source directly in front and behind. Also the theory only relates to localising sounds in the horizontal plane around the head. The theory also does not take into account the use of the pinna in localisation.(Gelfand, 2004)",
            "score": 211.2122802734375
        },
        {
            "docid": "8436042_6",
            "document": "MPEG Surround . MPEG Surround coding uses our capacity to perceive sound in the 3D and captures that perception in a compact set of parameters. Spatial perception is primarily attributed to three parameters, or cues, describing how humans localize sound in the horizontal plane: Interaural level difference (ILD), Interaural time difference (ITD) and Interaural coherence (IC). This three concepts are illustrated in next image. Direct, or first-arrival, waveforms from the source hit the left ear at time, while direct sound received by the right ear is diffracted around the head, with time delay and level attenuation, associated. These two effects result in ITD and ILD are associated with the main source. At last, in a reverberant environment, reflected sound from the source, or sound from diffuse source, or uncorrelated sound can hit both ears, all of them are related with IC.",
            "score": 209.596923828125
        },
        {
            "docid": "4548229_2",
            "document": "Interaural time difference . The interaural time difference (or ITD) when concerning humans or animals, is the difference in arrival time of a sound between two ears. It is important in the localization of sounds, as it provides a cue to the direction or angle of the sound source from the head. If a signal arrives at the head from one side, the signal has further to travel to reach the far ear than the near ear. This pathlength difference results in a time difference between the sound's arrivals at the ears, which is detected and aids the process of identifying the direction of sound source.",
            "score": 205.6791229248047
        },
        {
            "docid": "5051081_4",
            "document": "Eric Knudsen . In 1978, Knudsen and Konishi presented the discovery of an auditory map of space in the midbrain of the barn owl. This discovery was groundbreaking because it unearthed the first non-somatotopic space map in the brain. The map was found in the owl\u2019s midbrain, in the lateral and anterior mesencephalicus lateralis dorsalis (MLD), a structure now referred to as the inferior colliculus. Unlike most sound-localization maps, this map was found to be two-dimensional, with units arranged spatially to represent both the vertical and horizontal location of sound. Knudsen and Konishi discovered that units in this structure respond preferentially to sounds originating in a particular region in space. In the 1978 paper, elevation and azimuth (location in the horizontal plane) were shown to be the two coordinates of the map. Using a speaker set on a rotatable hemispherical track, Knudsen and Konishi presented owls with auditory stimulus from various locations in space and recorded the resulting neuronal activity. They found that neurons in this part of the MLD were organized according to the location of their receptive field, with azimuth varying along the horizontal plane of the space map and elevation varying vertically.  Knudsen followed this discovery with research into specific sound localization mechanisms. Two main auditory cues used by the barn owl to localize sound are interaural time difference (ITD) and interaural intensity difference (IID). The owl\u2019s ears are asymmetric, with the right ear\u2019s opening being directed higher than that of the left. This asymmetry allows the barn owl to determine the elevation of a sound by comparing sound levels between its two ears. Interaural time differences provide the owl with information regarding a sound\u2019s azimuth; sound will reach the ear closer to the sound source before reaching the farther ear, and this time difference can be detected and interpreted as an azimuthal direction. At low frequencies, the wavelength of a sound is wider than the owl's facial ruff, and the ruff does not affect detection of azimuth. At high frequencies, the ruff plays a role in reflecting sound for heightened sensitivity to vertical elevation. Therefore, with wide-band noise, containing both high and low frequencies, the owl could use interaural spectrum difference to obtain information about both azimuth and elevation. In 1979, Knudsen and Konishi showed that the barn owl uses interaural spectrum information in sound localization. They presented owls with both wide-bandwidth noise and pure tones. The birds were able to successfully locate pure tones (since they could still gather information from IID and ITD), but their error rate was much lower when localizing wide-bandwidth noise. This indicates that the birds utilize interaural spectrum differences to improve their accuracy.",
            "score": 203.9773406982422
        },
        {
            "docid": "39200136_12",
            "document": "Hans Wallach . In a series of papers Wallach explored the ability of humans to locate sounds in the median plane \u2013 that is, to determine whether a sound comes from a source at the same elevation as the ears or from a source that is higher or lower, or even in back of the head. Binaural sound cues, including the phasing or time of the sound\u2019s arrival at each ear and the sound\u2019s relative intensity at the two ears (known respectively as ITD and ILD) enable a listener to determine a sound\u2019s lateral location (whether it is on the left, right, or straight ahead). But two sounds at different elevations can present identical ITD and ILD information to the ears, and so binaural cues to a stationary ear do not suffice to identify a sound\u2019s location in the median plane. Monaural cues that depend on the shape of the head and the structure of the external ear help with vertical localization, but binaural cues also play a part if the head is not stationary.",
            "score": 203.8978271484375
        },
        {
            "docid": "41087200_9",
            "document": "Perceptual-based 3D sound localization . Interaural time and level differences (ITD, ILD) play a role in azimuth perception but can\u2019t explain vertical localization. According to the duplex theory, ITDs have a greater contribution to the localisation of low frequency sounds (below 1\u00a0kHz),while ILDs are used in the localisation of high frequency sound. The ILD arises from the fact that,a sound coming from a source located to one side of the head will have a higher intensity, or be louder, at the ear nearest the sound source. One can therefore create the illusion of a sound source emanating from one side of the head merely by adjusting the relative level of the sounds that are fed to two separated speakers or headphones. This is the basis of the commonly used pan control.",
            "score": 201.20655822753906
        },
        {
            "docid": "3154127_4",
            "document": "Virtual acoustic space . When one listens to sounds over headphones (in what is known as the \"closed field\") the sound source appears to arise from center of the head. On the other hand, under normal, so-called free-field, listening conditions sounds are perceived as being externalized. The direction of a sound in space (see sound localization) is determined by the brain when it analyses the interaction of incoming sound with head and external ears. A sound arising to one side reaches the near ear before the far ear (creating an interaural time difference, ITD), and will also be louder at the near ear (creating an interaural level difference, ILD \u2013 also known as interaural intensity difference, IID). These binaural cues allow sounds to be lateralized. Although conventional stereo headphone signals make used of ILDs (not ITDs) the sound is not perceived as being externalized.",
            "score": 200.44900512695312
        },
        {
            "docid": "14532984_7",
            "document": "Coincidence detection in neurobiology . Coincidence detection has been shown to be a major factor in sound localization along the azimuth plane in several organisms. In 1948, Lloyd A. Jeffress proposed that some organisms may have a collection of neurons that receive auditory input from each ear. The neural pathways to these neurons are called delay lines. Jeffress claimed that the neurons that the delay lines link act as coincidence detectors by firing maximally when receiving simultaneous inputs from both ears. When a sound is heard, sound waves may reach the ears at different times. This is referred to as the interaural time difference (ITD). Due to differing lengths and a finite conduction speed within the axons of the delay lines, different coincidence detector neurons will fire when sound comes from different positions along the azimuth. Jeffress' model proposes that two signals even from an asynchronous arrival of sound in the cochlea of each ear will converge synchronously on a coincidence detector in the auditory cortex based on the magnitude of the ITD (Fig. 2). Therefore, the ITD should correspond to an anatomical map that can be found within the brain. Masakazu Konishi's study on barn owls shows that this is true. Sensory information from the hair cells of the ears travels to the ipsilateral nucleus magnocellularis. From here, the signals project ipsilaterally and contralaterally to two nucleus laminari. Each nucleus laminaris contains coincidence detectors that receive auditory input from the left and the right ear. Since the ipsilateral axons enter the nucleus laminaris dorsally while the contralateral axons enter ventrally, sounds from various positions along the azimuth correspond directly to stimulation of different depths of the nucleus laminaris. From this information, a neural map of auditory space was formed. The function of the nucleus laminaris parallels that of the medial superior olive in mammals.",
            "score": 194.83583068847656
        },
        {
            "docid": "1021754_32",
            "document": "Sound localization . When the head is stationary, the binaural cues for lateral sound localization (interaural time difference and interaural level difference) do not give information about the location of a sound in the median plane. Identical ITDs and ILDs can be produced by sounds at eye level or at any elevation, as long as the lateral direction is constant. However, if the head is rotated, the ITD and ILD change dynamically, and those changes are different for sounds at different elevations. For example, if an eye-level sound source is straight ahead and the head turns to the left, the sound becomes louder (and arrives sooner) at the right ear than at the left. But if the sound source is directly overhead, there will be no change in the ITD and ILD as the head turns. Intermediate elevations will produce intermediate degrees of change, and if the presentation of binaural cues to the two ears during head movement is reversed, the sound will be heard behind the listener. Hans Wallach artificially altered a sound\u2019s binaural cues during movements of the head. Although the sound was objectively placed at eye level, the dynamic changes to ITD and ILD as the head rotated were those that would be produced if the sound source had been elevated. In this situation, the sound was heard at the synthesized elevation. The fact that the sound sources objectively remained at eye level prevented monaural cues from specifying the elevation, showing that it was the dynamic change in the binaural cues during head movement that allowed the sound to be correctly localized in the vertical dimension. The head movements need not be actively produced; accurate vertical localization occurred in a similar setup when the head rotation was produced passively, by seating the blindfolded subject in a rotating chair. As long as the dynamic changes in binaural cues accompanied a perceived head rotation, the synthesized elevation was perceived.",
            "score": 194.464111328125
        },
        {
            "docid": "41087200_8",
            "document": "Perceptual-based 3D sound localization . Interaural level differences (ILD) represents the difference in sound pressure level reaching the two ears. They provide salient cues for localizing high-frequency sounds in space, and populations of neurons that are sensitive to ILD are found at almost every synaptic level from brain stem to cortex. These cells are predominantly excited by stimulation of one ear and predominantly inhibited by stimulation of the other ear, such that the magnitude of their response is determined in large part by the intensities at the 2 ears. This gives rise to the concept of resonant damping. Interaural level difference (ILD) is best for high frequency sounds because low frequency sounds are not attenuated much by the head. ILD (also known as Interaural Intensity Difference) arises when the sound source is not centred, the listener's head partially shadows the ear opposite to the source, diminishing the intensity of the sound in that ear (particularly at higher frequencies). The pinnae filters the sound in a way that is directionally dependent. This is particularly useful in determining if a sound comes from above, below, in front, or behind.",
            "score": 193.0183563232422
        },
        {
            "docid": "32105732_4",
            "document": "Spatial hearing loss . Sound streams arriving from the left or right (the horizontal plane) are localised primarily by the small time differences of the same sound arriving at the two ears. A sound straight in front of the head is heard at the same time by both ears. A sound to the side of the head is heard approximately 0.0005 seconds later by the ear furthest away. A sound halfway to one side is heard approximately 0.0003 seconds later. This is the interaural time difference (ITD) cue and is measured by signal processing in the two central auditory pathways that begin after the cochlea and pass through the brainstem and mid-brain. Some of those with spatial hearing loss are unable to process ITD (low frequency) cues.",
            "score": 192.5030517578125
        },
        {
            "docid": "7527647_17",
            "document": "Binaural fusion . The auditory space of binaural hearing is constructed based on the analysis of differences in two different binaural cues in the horizontal plane: sound level, or ILD, and arrival time at the two ears, or ITD, which allow for the comparison of the sound heard at each eardrum. ITD is processed in the MSO and results from sounds arriving earlier at one ear than the other; this occurs when the sound does not arise from directly in front or directly behind the hearer. ILD is processed in the LSO and results from the shadowing effect that is produced at the ear that is farther from the sound source. Outputs from the SOC are targeted to the dorsal nucleus of the lateral lemniscus as well as the IC.",
            "score": 186.35397338867188
        },
        {
            "docid": "32116125_4",
            "document": "Amblyaudia . Amblyaudia is a deficit in binaural integration of environmental information entering the auditory system. It is a disorder related to brain organization and function rather than what is typically considered a \u201chearing loss\u201d (damage to the cochlea). It may be genetic or developmentally acquired or both. When animals are temporarily deprived of hearing from an early age, profound changes occur in the brain. Specifically, cell sizes in brainstem nuclei are reduced, the configuration of brainstem dendrites are altered and neurons respond in different ways to sounds presented to both the deprived and non-deprived ears (in cases of asymmetric deprivation). This last point is particularly important for listening tasks that require inputs from two ears to perform well. There are multiple auditory functions that rely on the computation of well calibrated inputs from the two ears. Chief among these is the ability to localize sound sources and separate what we want to hear from a background of noise. In the brainstem, the auditory system compares the timing and levels of sounds between the two ears to encode the location of sound sources (sounds that originate from our right as opposed to left side are louder and arrive earlier in our right ear). This ability to separate sound sources not only helps us locate the trajectories of moving objects, but also to separate different sound sources in noisy environments.",
            "score": 185.0266876220703
        },
        {
            "docid": "41087200_4",
            "document": "Perceptual-based 3D sound localization . While the relationship between human perception of sound and various attributes of the sound field is not yet well understood, DSP algorithms for sound localization are able to employ several mechanisms found in neural systems, including the interaural time difference (ITD, the difference in arrival time of a sound between two locations), the interaural intensity difference (IID, the difference in intensity of a sound between two locations), artificial pinnae, the precedence effect, and head-related transfer functions (HRTF) . When localizing 3D sound in spatial domain, one could take into account that the incoming sound signal could be reflected, defracted and scattered by the upper torso of the human which consists of shoulders, head and pinnae.Localization also depends on the direction of the sound source.",
            "score": 183.93359375
        },
        {
            "docid": "690883_6",
            "document": "Precedence effect . Additionally, Wallach et al. demonstrated that when successive sounds coming from sources at different locations were heard as fused, the apparent location of the perceived sound was dominated by the location of the sound that reached the ears first (i.e. the first-arriving wavefront). The second-arriving sound had only a very small (albeit measurable) effect on the perceived location of the fused sound. They designated this phenomenon as the \"precedence effect\", and noted that it explains why sound localization is possible in the typical situation where sounds reverberate from walls, furniture and the like, thus providing multiple, successive stimuli. They also noted that the precedence effect is an important factor in the perception of stereophonic sound.",
            "score": 182.82293701171875
        },
        {
            "docid": "35988494_6",
            "document": "Selective auditory attention . The prevalence of selective hearing has not been clearly researched yet. However, there are some that have argued that the proportion of selective hearing is particularly higher in males than females. Ida Z\u00fcndorf, Hans-Otto Karnath and J\u00f6rg Lewald carried out a study in 2010 which investigated the advantages and abilities males have in the localization of auditory information. A sound localization task centered on the cocktail party effect was utilized in their study. The male and female participants had to try to pick out sounds from a specific source, on top of other competing sounds from other sources. The results showed that the males had a better performance overall. Female participants found it more difficult to locate target sounds in a multiple-source environment. Z\u00fcndorf et al. suggested that there may be sex differences in the attention processes that helped locate the target sound from a multiple-source auditory field. While men and women do have some differences when it comes to selective auditory hearing, they both struggle when presented with the challenge of multitasking, especially when tasks that are to be attempted concurrently are very similar in nature (Dittrich, and Stahl, 2012, p.\u00a0626).",
            "score": 182.17080688476562
        },
        {
            "docid": "3154127_5",
            "document": "Virtual acoustic space . The perception of an externalized sound source is due to the frequency and direction-dependent filtering of the pinna which makes up the external ear structure. Unlike ILDs and ITDs, these spectral localization cues are generated monaurally. The same sound presented from different directions will produce at the eardrum a different pattern of peaks and notches across frequency. The pattern of these monaural spectral cues is different for different listeners. Spectral cues are vital for making elevation judgments and distinguishing if a sound arose from in front or behind the listener. They are also vital for creating the illusion of an externalized sound source. Since only ILDs are present in stereo recordings, the lack of spectral cues means that the sound is not perceived as being externalized. The easiest way of re-creating this illusion is to make a recording using two microphones placed inside a dummy human head. Playing back the recording via headphones will create the illusion of an externalized sound source.",
            "score": 178.0519256591797
        },
        {
            "docid": "3864383_7",
            "document": "Virtual surround . Perception of direction is greatly affected by the relative time that a sound arrives at each ear and any difference in the amplitude of a sound at each ear. It is possible to create a sound source having an output characteristic which is rapidly varying with direction and frequency of signal. These kinds of sources create sound fields which are rapidly variable around the listeners room. These are often referred to as diffuse sources, this is because their output resembles a diffuse sound field \u2014 a sound field where soundwaves are traveling in all directions with equal probability. In a diffuse field the sound at each of a listeners' ears is so completely different that it is impossible for the brain to work out where the sound has come from. A diffuse source located in front of the listener will be hard to localize and can be used to carry the surround signals.",
            "score": 177.94171142578125
        },
        {
            "docid": "233830_28",
            "document": "Ambisonics . At low frequencies, where the wavelength is large compared to the human head, an incoming sound diffracts around it, so that there is virtually no acoustic shadow and hence no level difference between the ears. In this range, the only available information is the phase relationship between the two ear signals, called \"interaural time difference\", or \"ITD\". Evaluating this time difference allows for precise localisation within a \"cone of confusion\": the angle of incidence is unambiguous, but the ITD is the same for sounds from the front or from the back. As long as the sound is not totally unknown to the subject, the confusion can usually be resolved by perceiving the timbral front-back variations caused by the ear flaps (or \"pinnae\").",
            "score": 176.6757354736328
        },
        {
            "docid": "5442380_17",
            "document": "Sensory cue . Unless a sound is directly in front of or behind the individual, the sound stimuli will have a slightly different distance to travel to reach each ear. This difference in distance causes a slight delay in the time the signal is perceived by each ear. The magnitude of the interaural time difference is greater the more the signal comes from the side of the head. Thus, this time delay allows humans to accurately predict the location of incoming sound cues. Interaural level difference is caused by the difference in sound pressure level reaching the two ears. This is because the head blocks the sound waves for the further ear, causing less intense sound to reach it. This level difference between the two ears allows humans to accurately predict azimuth of an auditory signal. This effect only occurs at sounds that are high frequency.",
            "score": 176.64674377441406
        },
        {
            "docid": "8398605_8",
            "document": "Microphone practice . There are two features of sound that the human brain uses to place objects in the stereo sound-field between the loudspeakers. These are the relative level (or loudness) difference between the two channels \"\u0394\u00a0L\", and the time delay difference in arrival times for the same sound in each channel \"\u0394\u00a0t\". The \"interaural\" signals (binaural \"ILD\" and \"ITD\") at the ears are not the stereo microphone signals which are coming from the loudspeakers, and are called \"interchannel\" signals (\"\u0394\u00a0L\" and \"\u0394\u00a0t\"). These signals are normally not mixed. Loudspeaker signals are different from the sound arriving at the ear. See the article \"Binaural recording for earphones\".",
            "score": 175.26651000976562
        },
        {
            "docid": "41087200_7",
            "document": "Perceptual-based 3D sound localization . According to the duplex theory, ITDs have a greater contribution to the localisation of low frequency sounds (below 1 kHz), while ILDs are used in the localisation of high frequency sound. These approaches can be applied to selective reconstructions of spatialized signals, where spectrotemporal components believed to be dominated by the desired sound source are identified and isolated through the Short-time Fourier transform (STFT). Modern systems typically compute the STFT of the incoming signal from two or more microphones, and estimate the ITD or each spectrotemporal component by comparing the phases of the STFTs. An advantage to this approach is that it may be generalized to more than two microphones, which can improve accuracy in 3 dimensions and remove the front-back localization ambiguity that occurs with only two ears or microphones. Another advantage is that the ITD is relatively strong and easy to obtain without biomimetic instruments such as dummy heads and artificial pinnae, though these may still be used to enhance amplitude disparities. HRTF phase response is mostly linear and listeners are insensitive to the details of the interaural phase spectrum as long as the interaural time delay (ITD) of the combined low-frequency part of the waveform is maintained.",
            "score": 174.6723175048828
        },
        {
            "docid": "1021754_21",
            "document": "Sound localization . In 1907, Lord Rayleigh utilized tuning forks to generate monophonic excitation and studied the lateral sound localization theory on a human head model without auricle. He first presented the interural clue difference based sound localization theory, which is known as Duplex Theory. Human ears are on the different sides of the head, thus they have different coordinates in space. As shown in fig. 2, since the distances between the acoustic source and ears are different, there are time difference and intensity difference between the sound signals of two ears. We call those kinds of differences as Interaural Time Difference (ITD) and Interaural Intensity Difference (IID) respectively.",
            "score": 174.45933532714844
        },
        {
            "docid": "41087200_14",
            "document": "Perceptual-based 3D sound localization . Head-related transfer functions contain all the descriptors of localization cues such as ITD and IID as well as monaural cues. Every HRTF uniquely represents the transfer of sound from a specific position in 3D space to the ears of a listener. The decoding process performed by the auditory system can be imitated using an artificial setup consisting of two microphones, two artificial ears and a HRTF database. To determine the position of an audio source in 3D space, the ear input signals are convolved with the inverses of all possible HRTF pairs, where the correct inverse maximizes cross-correlation between the convolved right and left signals. In the case of multiple simultaneous sound sources, the transmission of sound from source to ears can be considered a multiple-input and multiple-output. Here, the HRTFs the source signals were filtered with en route to the microphones can be found using methods such as convolutive blind source separation, which has the advantage of efficient implementation in real-time systems. Overall, these approaches using HRTFs can be well optimized to localize multiple moving sound sources. The average human has the remarkable ability to locate a sound source with better than 5 accuracy in both azimuth and elevation, in challenging environments.",
            "score": 172.0833740234375
        },
        {
            "docid": "32105732_8",
            "document": "Spatial hearing loss . Those individuals with spatial hearing loss are not able to accurately perceive the directions different sound streams are coming from and their hearing is no longer 3-dimensional (3D). Sound streams from the rear may appear to come from the front instead. Sound streams from the left or right may appear to come from the front. The gain mechanism can not be used to enhance the speech stream of interest from all other sound streams. Those with spatial hearing loss need target speech to be raised by typically more than 10\u00a0dB when listening to speech in a background noise compared to those with no spatial hearing loss.",
            "score": 170.8834228515625
        },
        {
            "docid": "1555553_18",
            "document": "Unilateral hearing loss . When wearing stereo headphones, people with unilateral hearing loss can hear only one channel, hence the panning information (volume and time differences between channels) is lost; some instruments may be heard better than others if they are mixed predominantly to one channel, and in extreme cases of sound production, such as complete stereo separation or stereo-switching, only part of the composition can be heard; in games using 3D audio effects, sound may not be perceived appropriately due to coming to the disabled ear. This can be corrected by using settings in the software or hardware\u2014audio player, OS, amplifier or sound source\u2014to adjust balance to one channel (only if the setting downmixes sound from both channels to one), or there may be an option to outright downmix both channels to mono. Such settings may be available via the device or software's accessibility features. As hardware solutions, stereo-to-mono adapters may be available to receive mono sound in stereo headphones from a stereo sound source, or some monaural headsets for cellphones and VOIP communication may combine stereo sound to mono (though headphones for voice communication typically offer lower audio quality than headphones targeted for listening to music). From the standpoint of sound fidelity, sound information in downmixed mono channel will, in any case, differ from that in either of the source channels or what is perceived by a normal-hearing person, thus technically some audio quality is lost (for example, the same or slightly different sound occurrences in two channels, with time delay between them, will be merged to a sound in the mono channel that unavoidably cannot correspond to the intent of the sound producer); however, such loss is most probably unnoticeable, especially compared to other distortions inherent in sound reproduction, and to the person's problems from hearing loss.",
            "score": 170.58425903320312
        },
        {
            "docid": "352733_26",
            "document": "Glass harmonica . The somewhat disorienting quality of the ethereal sound is due in part to the way that humans perceive and locate ranges of sounds. Above 4 kHz people primarily use the \"loudness\" of the sound to differentiate between left and right ears and thus triangulate, or locate the source. Below 1\u00a0kHz, they use the \"phase differences\" of sound waves arriving at their left and right ears to identify location. The predominant pitch of the armonica is in the range of 1\u20134\u00a0kHz, which coincides with the sound range where the brain is \"not quite sure\", and thus listeners have difficulty locating it in space (where it comes from), and discerning the source of the sound (the materials and techniques used to produce it).",
            "score": 169.5342254638672
        },
        {
            "docid": "47338295_3",
            "document": "Sound localization in owls . Owls must be able to determine the necessary angle of descent, i.e. the elevation, in addition to azimuth (horizontal angle to the sound). This bi-coordinate sound localization is accomplished through two binaural cues: the interaural time difference (ITD) and the interaural level difference (ILD), also known as the interaural intensity difference (IID). The ability in owls is unusual; in ground-bound mammals such as mice, ITD and ILD are not utilized in the same manner. In these mammals, ITDs tend to be utilized for localization of lower frequency sounds, while ILDs tend to be used for higher frequency sounds.",
            "score": 168.66073608398438
        },
        {
            "docid": "4548229_6",
            "document": "Interaural time difference . Experiments conducted by Woodworth (1938) tested the duplex theory by using a solid sphere to model the shape of the head and measuring the ITDs as a function of azimuth for different frequencies. The model used had a distance between the 2 ears of approximately 22\u201323\u00a0cm. Initial measurements found that there was a maximum time delay of approximately 660 \u03bcs when the sound source was placed at directly 90\u00b0 azimuth to one ear. This time delay correlates to the wavelength of a sound input with a frequency of 1500\u00a0Hz. The results concluded that when a sound played had a frequency less than 1500\u00a0Hz the wavelength is greater than this maximum time delay between the ears. Therefore there is a phase difference between the sound waves entering the ears providing acoustic localisation cues. With a sound input with a frequency closer to 1500\u00a0Hz the wavelength of the sound wave is similar to the natural time delay. Therefore due to the size of the head and the distance between the ears there is a reduced phase difference so localisations errors start to be made. When a high frequency sound input is used with a frequency greater than 1500\u00a0Hz, the wavelength is shorter than the distance between the 2 ears, a head shadow is produced and ILD provide cues for the localisation of this sound.",
            "score": 167.3114471435547
        }
    ]
}