{
    "q": [
        {
            "docid": "5051081_4",
            "document": "Eric Knudsen . In 1978, Knudsen and Konishi presented the discovery of an auditory map of space in the midbrain of the barn owl. This discovery was groundbreaking because it unearthed the first non-somatotopic space map in the brain. The map was found in the owl\u2019s midbrain, in the lateral and anterior mesencephalicus lateralis dorsalis (MLD), a structure now referred to as the inferior colliculus. Unlike most sound-localization maps, this map was found to be two-dimensional, with units arranged spatially to represent both the vertical and horizontal location of sound. Knudsen and Konishi discovered that units in this structure respond preferentially to sounds originating in a particular region in space. In the 1978 paper, elevation and azimuth (location in the horizontal plane) were shown to be the two coordinates of the map. Using a speaker set on a rotatable hemispherical track, Knudsen and Konishi presented owls with auditory stimulus from various locations in space and recorded the resulting neuronal activity. They found that neurons in this part of the MLD were organized according to the location of their receptive field, with azimuth varying along the horizontal plane of the space map and elevation varying vertically.  Knudsen followed this discovery with research into specific sound localization mechanisms. Two main auditory cues used by the barn owl to localize sound are interaural time difference (ITD) and interaural intensity difference (IID). The owl\u2019s ears are asymmetric, with the right ear\u2019s opening being directed higher than that of the left. This asymmetry allows the barn owl to determine the elevation of a sound by comparing sound levels between its two ears. Interaural time differences provide the owl with information regarding a sound\u2019s azimuth; sound will reach the ear closer to the sound source before reaching the farther ear, and this time difference can be detected and interpreted as an azimuthal direction. At low frequencies, the wavelength of a sound is wider than the owl's facial ruff, and the ruff does not affect detection of azimuth. At high frequencies, the ruff plays a role in reflecting sound for heightened sensitivity to vertical elevation. Therefore, with wide-band noise, containing both high and low frequencies, the owl could use interaural spectrum difference to obtain information about both azimuth and elevation. In 1979, Knudsen and Konishi showed that the barn owl uses interaural spectrum information in sound localization. They presented owls with both wide-bandwidth noise and pure tones. The birds were able to successfully locate pure tones (since they could still gather information from IID and ITD), but their error rate was much lower when localizing wide-bandwidth noise. This indicates that the birds utilize interaural spectrum differences to improve their accuracy.",
            "score": 214.125302195549
        },
        {
            "docid": "14532984_7",
            "document": "Coincidence detection in neurobiology . Coincidence detection has been shown to be a major factor in sound localization along the azimuth plane in several organisms. In 1948, Lloyd A. Jeffress proposed that some organisms may have a collection of neurons that receive auditory input from each ear. The neural pathways to these neurons are called delay lines. Jeffress claimed that the neurons that the delay lines link act as coincidence detectors by firing maximally when receiving simultaneous inputs from both ears. When a sound is heard, sound waves may reach the ears at different times. This is referred to as the interaural time difference (ITD). Due to differing lengths and a finite conduction speed within the axons of the delay lines, different coincidence detector neurons will fire when sound comes from different positions along the azimuth. Jeffress' model proposes that two signals even from an asynchronous arrival of sound in the cochlea of each ear will converge synchronously on a coincidence detector in the auditory cortex based on the magnitude of the ITD (Fig. 2). Therefore, the ITD should correspond to an anatomical map that can be found within the brain. Masakazu Konishi's study on barn owls shows that this is true. Sensory information from the hair cells of the ears travels to the ipsilateral nucleus magnocellularis. From here, the signals project ipsilaterally and contralaterally to two nucleus laminari. Each nucleus laminaris contains coincidence detectors that receive auditory input from the left and the right ear. Since the ipsilateral axons enter the nucleus laminaris dorsally while the contralateral axons enter ventrally, sounds from various positions along the azimuth correspond directly to stimulation of different depths of the nucleus laminaris. From this information, a neural map of auditory space was formed. The function of the nucleus laminaris parallels that of the medial superior olive in mammals.",
            "score": 202.96296453475952
        },
        {
            "docid": "4548229_5",
            "document": "Interaural time difference . The duplex theory states that ITDs are used to localise low frequency sounds, in particular, while ILDs are used in the localisation of high frequency sound inputs. However, the frequency ranges for which the auditory system can use ITDs and ILDs significantly overlap, and most natural sounds will have both high and low frequency components, so that the auditory system will in most cases have to combine information from both ITDs and ILDs to judge the location of a sound source.  A consequence of this duplex system is that it is also possible to generate so-called \"cue trading\" or \"time\u2013intensity trading\" stimuli on headphones, where ITDs pointing to the left are offset by ILDs pointing to the right, so the sound is perceived as coming from the midline. A limitation of the duplex theory is that the theory does not completely explain directional hearing, as no explanation is given for the ability to distinguish between a sound source directly in front and behind. Also the theory only relates to localising sounds in the horizontal plane around the head. The theory also does not take into account the use of the pinna in localisation.(Gelfand, 2004)",
            "score": 170.95225477218628
        },
        {
            "docid": "4548229_7",
            "document": "Interaural time difference . Feddersen et al. (1957) also conducted experiments taking measurements on how ITDs alter with changing the azimuth of the loudspeaker around the head at different frequencies. But unlike the Woodworth experiments human subjects were used rather than a model of the head. The experiment results agreed with the conclusion made by Woodworth about ITDs. The experiments also concluded that is there is no difference in ITDs when sounds are provided from directly in front or behind at 0\u00b0 and 180\u00b0 azimuth. The explanation for this is that the sound is equidistant from both ears. Interaural time differences alter as the loudspeaker is moved around the head. The maximum ITD of 660 \u03bcs occurs when a sound source is positioned at 90\u00b0 azimuth to one ear.",
            "score": 164.57464277744293
        },
        {
            "docid": "41087200_8",
            "document": "Perceptual-based 3D sound localization . Interaural level differences (ILD) represents the difference in sound pressure level reaching the two ears. They provide salient cues for localizing high-frequency sounds in space, and populations of neurons that are sensitive to ILD are found at almost every synaptic level from brain stem to cortex. These cells are predominantly excited by stimulation of one ear and predominantly inhibited by stimulation of the other ear, such that the magnitude of their response is determined in large part by the intensities at the 2 ears. This gives rise to the concept of resonant damping. Interaural level difference (ILD) is best for high frequency sounds because low frequency sounds are not attenuated much by the head. ILD (also known as Interaural Intensity Difference) arises when the sound source is not centred, the listener's head partially shadows the ear opposite to the source, diminishing the intensity of the sound in that ear (particularly at higher frequencies). The pinnae filters the sound in a way that is directionally dependent. This is particularly useful in determining if a sound comes from above, below, in front, or behind.",
            "score": 167.22179424762726
        },
        {
            "docid": "6147487_29",
            "document": "Neural coding . For very brief stimuli, a neuron's maximum firing rate may not be fast enough to produce more than a single spike. Due to the density of information about the abbreviated stimulus contained in this single spike, it would seem that the timing of the spike itself would have to convey more information than simply the average frequency of action potentials over a given period of time. This model is especially important for sound localization, which occurs within the brain on the order of milliseconds. The brain must obtain a large quantity of information based on a relatively short neural response. Additionally, if low firing rates on the order of ten spikes per second must be distinguished from arbitrarily close rate coding for different stimuli, then a neuron trying to discriminate these two stimuli may need to wait for a second or more to accumulate enough information. This is not consistent with numerous organisms which are able to discriminate between stimuli in the time frame of milliseconds, suggesting that a rate code is not the only model at work.",
            "score": 129.5714111328125
        },
        {
            "docid": "5442380_17",
            "document": "Sensory cue . Unless a sound is directly in front of or behind the individual, the sound stimuli will have a slightly different distance to travel to reach each ear. This difference in distance causes a slight delay in the time the signal is perceived by each ear. The magnitude of the interaural time difference is greater the more the signal comes from the side of the head. Thus, this time delay allows humans to accurately predict the location of incoming sound cues. Interaural level difference is caused by the difference in sound pressure level reaching the two ears. This is because the head blocks the sound waves for the further ear, causing less intense sound to reach it. This level difference between the two ears allows humans to accurately predict azimuth of an auditory signal. This effect only occurs at sounds that are high frequency.",
            "score": 149.5910520553589
        },
        {
            "docid": "4548229_6",
            "document": "Interaural time difference . Experiments conducted by Woodworth (1938) tested the duplex theory by using a solid sphere to model the shape of the head and measuring the ITDs as a function of azimuth for different frequencies. The model used had a distance between the 2 ears of approximately 22\u201323\u00a0cm. Initial measurements found that there was a maximum time delay of approximately 660 \u03bcs when the sound source was placed at directly 90\u00b0 azimuth to one ear. This time delay correlates to the wavelength of a sound input with a frequency of 1500\u00a0Hz. The results concluded that when a sound played had a frequency less than 1500\u00a0Hz the wavelength is greater than this maximum time delay between the ears. Therefore there is a phase difference between the sound waves entering the ears providing acoustic localisation cues. With a sound input with a frequency closer to 1500\u00a0Hz the wavelength of the sound wave is similar to the natural time delay. Therefore due to the size of the head and the distance between the ears there is a reduced phase difference so localisations errors start to be made. When a high frequency sound input is used with a frequency greater than 1500\u00a0Hz, the wavelength is shorter than the distance between the 2 ears, a head shadow is produced and ILD provide cues for the localisation of this sound.",
            "score": 162.24953377246857
        },
        {
            "docid": "2263473_17",
            "document": "Volley theory . A fundamental frequency is the lowest frequency of a harmonic. In some cases, sound can have all the frequencies of a harmonic but be missing the fundamental frequency, this is known as missing fundamental. When listening to a sound with a missing fundamental, the human brain still receives information for all frequencies, including the fundamental frequency which does not exist in the sound. This implies that sound is encoded by neurons firing at all frequencies of a harmonic, therefore, the neurons must be locked in some way to result in the hearing of one sound. Congenital deafness or sensorineural hearing loss is an often used model for the study of the inner ear regarding pitch perception and theories of hearing in general. Frequency analysis of these individuals\u2019 hearing has given insight on common deviations from normal tuning curves, excitation patterns, and frequency discrimination ranges. By applying pure or complex tones, information on pitch perception can be obtained. In 1983, it was shown that subjects with low frequency sensorineural hearing loss demonstrated abnormal psychophysical tuning curves. Changes in the spatial responses in these subjects showed similar pitch judgment abilities when compared to subjects with normal spatial responses. This was especially true regarding low frequency stimuli. These results suggest that the place theory of hearing does not explain pitch perception at low frequencies, but that the temporal (frequency) theory is more likely. This conclusion is due to the finding that when deprived of basilar membrane place information, these patients still demonstrated normal pitch perception. Computer models for pitch perception and loudness perception are often used during hearing studies on acoustically impaired subjects. The combination of this modeling and knowledge of natural hearing allows for better development of hearing aids.",
            "score": 131.1724568605423
        },
        {
            "docid": "3154127_4",
            "document": "Virtual acoustic space . When one listens to sounds over headphones (in what is known as the \"closed field\") the sound source appears to arise from center of the head. On the other hand, under normal, so-called free-field, listening conditions sounds are perceived as being externalized. The direction of a sound in space (see sound localization) is determined by the brain when it analyses the interaction of incoming sound with head and external ears. A sound arising to one side reaches the near ear before the far ear (creating an interaural time difference, ITD), and will also be louder at the near ear (creating an interaural level difference, ILD \u2013 also known as interaural intensity difference, IID). These binaural cues allow sounds to be lateralized. Although conventional stereo headphone signals make used of ILDs (not ITDs) the sound is not perceived as being externalized.",
            "score": 202.20565509796143
        },
        {
            "docid": "8953842_15",
            "document": "Computational auditory scene analysis . While many models consider the audio signal as a complex combination of different frequencies, modeling the auditory system can also require consideration for the neural components. By taking a holistic process, where a stream (of feature-based sounds) correspond to neuronal activity distributed in many brain areas, the perception of the sound could be mapped and modeled. Two different solutions have been proposed to the binding of the audio perception and the area in the brain. Hierarchical coding models many cells to encode all possible combinations of features and objects in the auditory scene. Temporal or oscillatory correlation addressing the binding problem by focusing on the synchrony and desynchrony between neural oscillations to encode the state of binding among the auditory features. These two solutions are very similar to the debacle between place coding and temporal coding. While drawing from modeling neural components, another phenomenon of ASA comes into play with CASA systems: the extent of modeling neural mechanisms. The studies of CASA systems have involved modeling some known mechanisms, such as the bandpass nature of cochlear filtering and random auditory nerve firing patterns, however, these models may not lead to finding new mechanisms, but rather give an understanding of purpose to the known mechanisms.",
            "score": 161.95434534549713
        },
        {
            "docid": "5637355_17",
            "document": "Neural facilitation . In humans, sound localization is primarily accomplished using information about how the intensity and timing of a sound vary between each ear. Neuronal computations involving these interaurual intensity differences (IIDs) and interaural time differences (ITDs) are typically carried out in different pathways in the brain. Short-term plasticity likely assists in differentiating between these two pathways: short-term facilitation dominates in intensity pathways, while short-term depression dominates in temporal pathways. These different types of short-term plasticity allow for different kinds of information filtration, thus contributing to the division of the two kinds of information into distinct processing streams.",
            "score": 155.04861164093018
        },
        {
            "docid": "493399_15",
            "document": "Loudness . A-weighting follows human sensitivity to sound and describes relative perceived loudness for at quiet to moderate speech levels, around 40 phons. However, physiological loudness perception is a much more complex process than can be captured with a single correction curve. Not only do equal-loudness contours vary with intensity, but perceived loudness of a complex sound depends on whether its spectral components are closely or widely spaced in frequency. When generating neural impulses in response to sounds of one frequency, the ear is less sensitive to nearby frequencies, which are said to be in the same critical band. Sounds containing spectral components in many critical bands are perceived as louder, even if the total sound pressure remains constant.",
            "score": 103.6460657119751
        },
        {
            "docid": "1021754_24",
            "document": "Sound localization . For frequencies above 1600\u00a0Hz the dimensions of the head are greater than the length of the sound waves. An unambiguous determination of the input direction based on interaural phase alone is not possible at these frequencies. However, the interaural level differences become larger, and these level differences are evaluated by the auditory system. Also, group delays between the ears can be evaluated, and is more pronounced at higher frequencies; that is, if there is a sound onset, the delay of this onset between the ears can be used to determine the input direction of the corresponding sound source. This mechanism becomes especially important in reverberant environments. After a sound onset there is a short time frame where the direct sound reaches the ears, but not yet the reflected sound. The auditory system uses this short time frame for evaluating the sound source direction, and keeps this detected direction as long as reflections and reverberation prevent an unambiguous direction estimation. The mechanisms described above cannot be used to differentiate between a sound source ahead of the hearer or behind the hearer; therefore additional cues have to be evaluated.",
            "score": 143.44314444065094
        },
        {
            "docid": "233830_28",
            "document": "Ambisonics . At low frequencies, where the wavelength is large compared to the human head, an incoming sound diffracts around it, so that there is virtually no acoustic shadow and hence no level difference between the ears. In this range, the only available information is the phase relationship between the two ear signals, called \"interaural time difference\", or \"ITD\". Evaluating this time difference allows for precise localisation within a \"cone of confusion\": the angle of incidence is unambiguous, but the ITD is the same for sounds from the front or from the back. As long as the sound is not totally unknown to the subject, the confusion can usually be resolved by perceiving the timbral front-back variations caused by the ear flaps (or \"pinnae\").",
            "score": 138.1605167388916
        },
        {
            "docid": "4548229_2",
            "document": "Interaural time difference . The interaural time difference (or ITD) when concerning humans or animals, is the difference in arrival time of a sound between two ears. It is important in the localization of sounds, as it provides a cue to the direction or angle of the sound source from the head. If a signal arrives at the head from one side, the signal has further to travel to reach the far ear than the near ear. This pathlength difference results in a time difference between the sound's arrivals at the ears, which is detected and aids the process of identifying the direction of sound source.",
            "score": 165.84134221076965
        },
        {
            "docid": "569650_19",
            "document": "Stimulus modality . The human ear is able to detect differences in pitch through the movement of auditory hair cells found on the basilar membrane. High frequency sounds will stimulate the auditory hair cells at the base of the basilar membrane while medium frequency sounds cause vibrations of auditory hair cells located at the middle of the basilar membrane. For frequencies that are lower than 200\u00a0Hz, the tip of the basilar membrane vibrates in sync with the sound waves. In turn, neurons are fired at the same rate as the vibrations. The brain is able to measure the vibrations and is then aware of any low frequency pitches.",
            "score": 122.74559473991394
        },
        {
            "docid": "9736652_7",
            "document": "Auditory masking . If two sounds of two different frequencies are played at the same time, two separate sounds can often be heard rather than a combination tone. The ability to hear frequencies separately is known as \"frequency resolution\" or \"frequency selectivity\". When signals are perceived as a combination tone, they are said to reside in the same \"critical bandwidth\". This effect is thought to occur due to filtering within the cochlea, the hearing organ in the inner ear. A complex sound is split into different frequency components and these components cause a peak in the pattern of vibration at a specific place on the cilia inside the basilar membrane within the cochlea. These components are then coded independently on the auditory nerve which transmits sound information to the brain. This individual coding only occurs if the frequency components are different enough in frequency, otherwise they are in the same critical band and are coded at the same place and are perceived as one sound instead of two.",
            "score": 143.45780086517334
        },
        {
            "docid": "32105732_4",
            "document": "Spatial hearing loss . Sound streams arriving from the left or right (the horizontal plane) are localised primarily by the small time differences of the same sound arriving at the two ears. A sound straight in front of the head is heard at the same time by both ears. A sound to the side of the head is heard approximately 0.0005 seconds later by the ear furthest away. A sound halfway to one side is heard approximately 0.0003 seconds later. This is the interaural time difference (ITD) cue and is measured by signal processing in the two central auditory pathways that begin after the cochlea and pass through the brainstem and mid-brain. Some of those with spatial hearing loss are unable to process ITD (low frequency) cues.",
            "score": 178.20465779304504
        },
        {
            "docid": "1021754_21",
            "document": "Sound localization . In 1907, Lord Rayleigh utilized tuning forks to generate monophonic excitation and studied the lateral sound localization theory on a human head model without auricle. He first presented the interural clue difference based sound localization theory, which is known as Duplex Theory. Human ears are on the different sides of the head, thus they have different coordinates in space. As shown in fig. 2, since the distances between the acoustic source and ears are different, there are time difference and intensity difference between the sound signals of two ears. We call those kinds of differences as Interaural Time Difference (ITD) and Interaural Intensity Difference (IID) respectively.",
            "score": 148.9232141971588
        },
        {
            "docid": "8436042_6",
            "document": "MPEG Surround . MPEG Surround coding uses our capacity to perceive sound in the 3D and captures that perception in a compact set of parameters. Spatial perception is primarily attributed to three parameters, or cues, describing how humans localize sound in the horizontal plane: Interaural level difference (ILD), Interaural time difference (ITD) and Interaural coherence (IC). This three concepts are illustrated in next image. Direct, or first-arrival, waveforms from the source hit the left ear at time, while direct sound received by the right ear is diffracted around the head, with time delay and level attenuation, associated. These two effects result in ITD and ILD are associated with the main source. At last, in a reverberant environment, reflected sound from the source, or sound from diffuse source, or uncorrelated sound can hit both ears, all of them are related with IC.",
            "score": 162.41049098968506
        },
        {
            "docid": "25140_41",
            "document": "Perception . Hearing (or \"audition\") is the ability to perceive sound by detecting vibrations. Frequencies capable of being heard by humans are called audio or \"sonic\". The range is typically considered to be between 20\u00a0Hz and 20,000\u00a0Hz. Frequencies higher than audio are referred to as ultrasonic, while frequencies below audio are referred to as infrasonic. The auditory system includes the outer ears which collect and filter sound waves, the middle ear for transforming the sound pressure (impedance matching), and the inner ear which produces neural signals in response to the sound. By the ascending auditory pathway these are led to the primary auditory cortex within the temporal lobe of the human brain, which is where the auditory information arrives in the cerebral cortex and is further processed there.",
            "score": 107.41811466217041
        },
        {
            "docid": "493399_6",
            "document": "Loudness . The sensitivity of the human ear changes as a function of frequency, as shown in the equal-loudness graph. Each line on this graph shows the SPL required for frequencies to be perceived as equally loud, and different curves pertain to different sound pressure levels. It also shows that humans with normal hearing are most sensitive to sounds around 2\u20134\u00a0kHz, with sensitivity declining to either side of this region. A complete model of the perception of loudness will include the integration of SPL by frequency.",
            "score": 90.09410881996155
        },
        {
            "docid": "7527647_14",
            "document": "Binaural fusion . Sound localization is the ability to correctly identify the directional location of sounds. A sound stimulus localized in the horizontal plane is called azimuth; in the vertical plane it is referred to as elevation. The time, intensity, and spectral differences in the sound arriving at the two ears are used in localization. Localization of low frequency sounds is accomplished by analyzing interaural time difference (ITD). Localization of high frequency sounds is accomplished by analyzing interaural level difference (ILD).",
            "score": 160.41944408416748
        },
        {
            "docid": "39265695_7",
            "document": "Stimulus filtering . Female flies of the genus \"Ormia ochracea\" possess organs in their bodies that can detect frequencies of cricket sounds from meters away. This process is important for the survival of their species because females will lay their first instar larvae into the body of the cricket, where they will feed and molt for approximately seven days. After this period, the larvae grow into flies and the cricket usually perishes. Researchers were puzzled about how precise hearing ability could arise from a small ear structure. Normal animals detect and locate sounds using the interaural time difference (ITD) and the interaural level difference (ILD). The ITD is the difference in the time it takes sound to reach the ear. ILD is the difference in sound intensity measure between both ears. At maximum, the ITD would only reach about 1.5 microseconds and the ILD would be less than one decibel. These small values make it hard to sense the differences. To solve these issues, researchers studied the mechanical aspects of flies\u2019 ears. They found that they have a presternum structure linking both tympanal membranes that is critical in detecting sound and localization. The structure acts as a lever by transferring and amplifying vibrational energy between the membranes. After sound hits the membranes at different amplitudes, the presternum sets up symmetrical vibration modes through bending and rocking. This effect helps the nervous system distinguish which side the sound is coming from. Because the presternum acts as an intertympanal bridge, the ITD is increased from 1.5 us to 55 us and the ILD is increased from less than one decibel to over 10 decibels.",
            "score": 180.78800666332245
        },
        {
            "docid": "6147487_24",
            "document": "Neural coding . Neurons exhibit high-frequency fluctuations of firing-rates which could be noise or could carry information. Rate coding models suggest that these irregularities are noise, while temporal coding models suggest that they encode information. If the nervous system only used rate codes to convey information, a more consistent, regular firing rate would have been evolutionarily advantageous, and neurons would have utilized this code over other less robust options. Temporal coding supplies an alternate explanation for the \u201cnoise,\" suggesting that it actually encodes information and affects neural processing. To model this idea, binary symbols can be used to mark the spikes: 1 for a spike, 0 for no spike. Temporal coding allows the sequence 000111000111 to mean something different from 001100110011, even though the mean firing rate is the same for both sequences, at 6 spikes/10\u00a0ms. Until recently, scientists had put the most emphasis on rate encoding as an explanation for post-synaptic potential patterns. However, functions of the brain are more temporally precise than the use of only rate encoding seems to allow. In other words, essential information could be lost due to the inability of the rate code to capture all the available information of the spike train. In addition, responses are different enough between similar (but not identical) stimuli to suggest that the distinct patterns of spikes contain a higher volume of information than is possible to include in a rate code.",
            "score": 101.23613226413727
        },
        {
            "docid": "1021754_43",
            "document": "Sound localization . If the ears are located at the side of the head, similar lateral localization cues as for the human auditory system can be used. This means: evaluation of interaural time differences (interaural phase differences) for lower frequencies and evaluation of interaural level differences for higher frequencies. The evaluation of interaural phase differences is useful, as long as it gives unambiguous results. This is the case, as long as ear distance is smaller than half the length (maximal one wavelength) of the sound waves. For animals with a larger head than humans the evaluation range for interaural phase differences is shifted towards lower frequencies, for animals with a smaller head, this range is shifted towards higher frequencies.",
            "score": 121.15552687644958
        },
        {
            "docid": "4548229_16",
            "document": "Interaural time difference . The MSO is made up of neurons which receive input from the low-frequency fibers of the left and right AVCN. The result of having input from both cochleas is an increase in the firing rate of the MSO units. The neurons in the MSO are sensitive to the difference in the arrival time of sound at each ear, also known as the interaural time difference (ITD). Research shows that if stimulation arrives at one ear before the other, many of the MSO units will have increased discharge rates. The axons from the MSO continue to higher parts of the pathway via the ipsilateral lateral lemniscus tract.(Yost, 2000)",
            "score": 120.48475360870361
        },
        {
            "docid": "1379269_3",
            "document": "Musical acoustics . Whenever two different pitches are played at the same time, their sound waves interact with each other \u2013 the highs and lows in the air pressure reinforce each other to produce a different sound wave. Any repeating sound wave that is not a sine wave can be modeled by many different sine waves of the appropriate frequencies and amplitudes (a frequency spectrum). In humans the hearing apparatus (composed of the ears and brain) can usually isolate these tones and hear them distinctly. When two or more tones are played at once, a variation of air pressure at the ear \"contains\" the pitches of each, and the ear and/or brain isolate and decode them into distinct tones.",
            "score": 138.12086129188538
        },
        {
            "docid": "2263473_7",
            "document": "Volley theory . Pitch is an assigned, perceptual property where a listener orders sound frequencies from low to high. Pitch is hypothesized to be determined by receiving phase-locked input from neuronal axons and combining that information into harmonics. In simple sounds consisting of one frequency, the pitch is equivalent to the frequency. There are two models of pitch perception; a spectral and a temporal. Low frequency sounds evoke the strongest pitches, suggesting that pitch is based on the temporal components of the sound. Historically, there have been many models of pitch perception. (Terhardt, 1974; Goldstein, 1973; Wightman, 1973). Many consisted of a peripheral spectral-analysis stage and a central periodicity-analysis stage. In his model, Terhardt claims that the spectral-analysis output of complex sounds, specifically low frequency ones, is a learned entity which eventually allows easy identification of the virtual pitch. The volley principle is predominantly seen during the pitch perception of lower frequencies where sounds are often resolved. Goldstein proposed that through phase-locking and temporal frequencies encoded in neuron firing rates, the brain has the itemization of frequencies that can then be used to estimate pitch.",
            "score": 125.2793459892273
        },
        {
            "docid": "1222458_3",
            "document": "Tonotopy . Tonotopy in the auditory system begins at the cochlea, the small snail-like structure in the inner ear that sends information about sound to the brain. Different regions of the basilar membrane in the organ of Corti, the sound-sensitive portion of the cochlea, vibrate at different sinusoidal frequencies due to variations in thickness and width along the length of the membrane. Nerves that transmit information from different regions of the basilar membrane therefore encode frequency tonotopically. This tonotopy then projects through the vestibulocochlear nerve and associated midbrain structures to the primary auditory cortex via the auditory radiation pathway. Throughout this radiation, organization is linear with relation to placement on the organ of Corti, in accordance to the best frequency response (that is, the frequency at which that neuron is most sensitive) of each neuron. However, binaural fusion in the superior olivary complex onward adds significant amounts of information encoded in the signal strength of each ganglion. Thus, the number of tonotopic maps varies between species and the degree of binaural synthesis and separation of sound intensities; in humans, six tonotopic maps have been identified in the primary auditory cortex. their anatomical locations along the auditory cortex.",
            "score": 128.94847416877747
        },
        {
            "docid": "11217018_8",
            "document": "A-weighting . A-frequency-weighting is mandated by the international standard IEC 61672 to be fitted to all sound level meters. The old B- and D-frequency-weightings have fallen into disuse, but many sound level meters provide for C frequency-weighting and its fitting is mandated \u2014 at least for testing purposes \u2014 to precision (Class one) sound level meters. D-frequency-weighting was specifically designed for use when measuring high level aircraft noise in accordance with the IEC 537 measurement standard. The large peak in the D-weighting curve is not a feature of the equal-loudness contours, but reflects the fact that humans hear random noise differently from pure tones, an effect that is particularly pronounced around 6\u00a0kHz. This is because individual neurons from different regions of the cochlea in the inner ear respond to narrow bands of frequencies, but the higher frequency neurons integrate a wider band and hence signal a louder sound when presented with noise containing many frequencies than for a single pure tone of the same pressure level. Following changes to the ISO standard, D-frequency-weighting should now only be used for non-bypass engines and as these are not fitted to commercial aircraft \u2014 but only to military ones \u2014 A-frequency-weighting is now mandated for light civilian aircraft measurements, while a more accurate loudness-corrected weighting EPNdB is required for certification of large transport aircraft",
            "score": 100.0101490020752
        }
    ],
    "r": [
        {
            "docid": "5051081_4",
            "document": "Eric Knudsen . In 1978, Knudsen and Konishi presented the discovery of an auditory map of space in the midbrain of the barn owl. This discovery was groundbreaking because it unearthed the first non-somatotopic space map in the brain. The map was found in the owl\u2019s midbrain, in the lateral and anterior mesencephalicus lateralis dorsalis (MLD), a structure now referred to as the inferior colliculus. Unlike most sound-localization maps, this map was found to be two-dimensional, with units arranged spatially to represent both the vertical and horizontal location of sound. Knudsen and Konishi discovered that units in this structure respond preferentially to sounds originating in a particular region in space. In the 1978 paper, elevation and azimuth (location in the horizontal plane) were shown to be the two coordinates of the map. Using a speaker set on a rotatable hemispherical track, Knudsen and Konishi presented owls with auditory stimulus from various locations in space and recorded the resulting neuronal activity. They found that neurons in this part of the MLD were organized according to the location of their receptive field, with azimuth varying along the horizontal plane of the space map and elevation varying vertically.  Knudsen followed this discovery with research into specific sound localization mechanisms. Two main auditory cues used by the barn owl to localize sound are interaural time difference (ITD) and interaural intensity difference (IID). The owl\u2019s ears are asymmetric, with the right ear\u2019s opening being directed higher than that of the left. This asymmetry allows the barn owl to determine the elevation of a sound by comparing sound levels between its two ears. Interaural time differences provide the owl with information regarding a sound\u2019s azimuth; sound will reach the ear closer to the sound source before reaching the farther ear, and this time difference can be detected and interpreted as an azimuthal direction. At low frequencies, the wavelength of a sound is wider than the owl's facial ruff, and the ruff does not affect detection of azimuth. At high frequencies, the ruff plays a role in reflecting sound for heightened sensitivity to vertical elevation. Therefore, with wide-band noise, containing both high and low frequencies, the owl could use interaural spectrum difference to obtain information about both azimuth and elevation. In 1979, Knudsen and Konishi showed that the barn owl uses interaural spectrum information in sound localization. They presented owls with both wide-bandwidth noise and pure tones. The birds were able to successfully locate pure tones (since they could still gather information from IID and ITD), but their error rate was much lower when localizing wide-bandwidth noise. This indicates that the birds utilize interaural spectrum differences to improve their accuracy.",
            "score": 214.12530517578125
        },
        {
            "docid": "14532984_7",
            "document": "Coincidence detection in neurobiology . Coincidence detection has been shown to be a major factor in sound localization along the azimuth plane in several organisms. In 1948, Lloyd A. Jeffress proposed that some organisms may have a collection of neurons that receive auditory input from each ear. The neural pathways to these neurons are called delay lines. Jeffress claimed that the neurons that the delay lines link act as coincidence detectors by firing maximally when receiving simultaneous inputs from both ears. When a sound is heard, sound waves may reach the ears at different times. This is referred to as the interaural time difference (ITD). Due to differing lengths and a finite conduction speed within the axons of the delay lines, different coincidence detector neurons will fire when sound comes from different positions along the azimuth. Jeffress' model proposes that two signals even from an asynchronous arrival of sound in the cochlea of each ear will converge synchronously on a coincidence detector in the auditory cortex based on the magnitude of the ITD (Fig. 2). Therefore, the ITD should correspond to an anatomical map that can be found within the brain. Masakazu Konishi's study on barn owls shows that this is true. Sensory information from the hair cells of the ears travels to the ipsilateral nucleus magnocellularis. From here, the signals project ipsilaterally and contralaterally to two nucleus laminari. Each nucleus laminaris contains coincidence detectors that receive auditory input from the left and the right ear. Since the ipsilateral axons enter the nucleus laminaris dorsally while the contralateral axons enter ventrally, sounds from various positions along the azimuth correspond directly to stimulation of different depths of the nucleus laminaris. From this information, a neural map of auditory space was formed. The function of the nucleus laminaris parallels that of the medial superior olive in mammals.",
            "score": 202.9629669189453
        },
        {
            "docid": "3154127_4",
            "document": "Virtual acoustic space . When one listens to sounds over headphones (in what is known as the \"closed field\") the sound source appears to arise from center of the head. On the other hand, under normal, so-called free-field, listening conditions sounds are perceived as being externalized. The direction of a sound in space (see sound localization) is determined by the brain when it analyses the interaction of incoming sound with head and external ears. A sound arising to one side reaches the near ear before the far ear (creating an interaural time difference, ITD), and will also be louder at the near ear (creating an interaural level difference, ILD \u2013 also known as interaural intensity difference, IID). These binaural cues allow sounds to be lateralized. Although conventional stereo headphone signals make used of ILDs (not ITDs) the sound is not perceived as being externalized.",
            "score": 202.20565795898438
        },
        {
            "docid": "34004373_4",
            "document": "Sensory maps and brain development . The computational map is the \u201ckey building block in the infrastructure of information processing by the nervous system.\u201d Computation defined as the transformation in the representation of information is the essence of brain function. Computational maps are involved in processing sensory information and motor programming, and they contain derived information that is accessible to higher-order processing regions. The first computational map to be proposed was the Jeffress model (1948) which stated that the computation of sound localization was dependent upon timing differences of sensory input. Since the introduction of the Jeffress model, more general guiding principles for relating brain maps to the properties of the computations they perform have been proposed. One of the proposed models is that computations are distributed across parallel processors like computers; with this model, computer processing is a model for computations performed by the brain. More recently, the \u201celastic net\u201d model has been proposed after studying how the primary visual cortex overlaps multiple visual maps, such as visual field position, orientation, direction, ocular dominance, and spatial frequency. The elastic net uses parallel algorithms to analyze the visual field and allows for optimized trade-off between coverage and continuity.",
            "score": 196.77450561523438
        },
        {
            "docid": "1021754_32",
            "document": "Sound localization . When the head is stationary, the binaural cues for lateral sound localization (interaural time difference and interaural level difference) do not give information about the location of a sound in the median plane. Identical ITDs and ILDs can be produced by sounds at eye level or at any elevation, as long as the lateral direction is constant. However, if the head is rotated, the ITD and ILD change dynamically, and those changes are different for sounds at different elevations. For example, if an eye-level sound source is straight ahead and the head turns to the left, the sound becomes louder (and arrives sooner) at the right ear than at the left. But if the sound source is directly overhead, there will be no change in the ITD and ILD as the head turns. Intermediate elevations will produce intermediate degrees of change, and if the presentation of binaural cues to the two ears during head movement is reversed, the sound will be heard behind the listener. Hans Wallach artificially altered a sound\u2019s binaural cues during movements of the head. Although the sound was objectively placed at eye level, the dynamic changes to ITD and ILD as the head rotated were those that would be produced if the sound source had been elevated. In this situation, the sound was heard at the synthesized elevation. The fact that the sound sources objectively remained at eye level prevented monaural cues from specifying the elevation, showing that it was the dynamic change in the binaural cues during head movement that allowed the sound to be correctly localized in the vertical dimension. The head movements need not be actively produced; accurate vertical localization occurred in a similar setup when the head rotation was produced passively, by seating the blindfolded subject in a rotating chair. As long as the dynamic changes in binaural cues accompanied a perceived head rotation, the synthesized elevation was perceived.",
            "score": 182.3660888671875
        },
        {
            "docid": "47338295_4",
            "document": "Sound localization in owls . ITD occurs whenever the distance from the source of sound to the two ears is different, resulting in differences in the arrival times of the sound at the two ears. When the sound source is directly in front of the owl, there is no ITD, i.e. the ITD is zero. In sound localization, ITDs are used as cues for location in the azimuth. ITD changes systematically with azimuth. Sounds to the right arrive first at the right ear; sounds to the left arrive first at the left ear.",
            "score": 180.92672729492188
        },
        {
            "docid": "39265695_7",
            "document": "Stimulus filtering . Female flies of the genus \"Ormia ochracea\" possess organs in their bodies that can detect frequencies of cricket sounds from meters away. This process is important for the survival of their species because females will lay their first instar larvae into the body of the cricket, where they will feed and molt for approximately seven days. After this period, the larvae grow into flies and the cricket usually perishes. Researchers were puzzled about how precise hearing ability could arise from a small ear structure. Normal animals detect and locate sounds using the interaural time difference (ITD) and the interaural level difference (ILD). The ITD is the difference in the time it takes sound to reach the ear. ILD is the difference in sound intensity measure between both ears. At maximum, the ITD would only reach about 1.5 microseconds and the ILD would be less than one decibel. These small values make it hard to sense the differences. To solve these issues, researchers studied the mechanical aspects of flies\u2019 ears. They found that they have a presternum structure linking both tympanal membranes that is critical in detecting sound and localization. The structure acts as a lever by transferring and amplifying vibrational energy between the membranes. After sound hits the membranes at different amplitudes, the presternum sets up symmetrical vibration modes through bending and rocking. This effect helps the nervous system distinguish which side the sound is coming from. Because the presternum acts as an intertympanal bridge, the ITD is increased from 1.5 us to 55 us and the ILD is increased from less than one decibel to over 10 decibels.",
            "score": 180.7880096435547
        },
        {
            "docid": "32105732_4",
            "document": "Spatial hearing loss . Sound streams arriving from the left or right (the horizontal plane) are localised primarily by the small time differences of the same sound arriving at the two ears. A sound straight in front of the head is heard at the same time by both ears. A sound to the side of the head is heard approximately 0.0005 seconds later by the ear furthest away. A sound halfway to one side is heard approximately 0.0003 seconds later. This is the interaural time difference (ITD) cue and is measured by signal processing in the two central auditory pathways that begin after the cochlea and pass through the brainstem and mid-brain. Some of those with spatial hearing loss are unable to process ITD (low frequency) cues.",
            "score": 178.2046661376953
        },
        {
            "docid": "50707986_18",
            "document": "Lloyd A. Jeffress . Beginning about 1940, Jeffress' primary research interest was the auditory system, especially the mechanisms underlying sound localization. His most cited article, \"A Place Theory of Sound Localization\", was in the 1948 Journal of Comparative and Physiological Psychology. In the article, he describes a hypothetical neural network capable of cross-correlating the temporal (time) information at the two ears and thereby extracting the small differences that can exist in the time of arrival of a wavefront at the two ears, thus localizing the sound. This neurocomputational model that explains how auditory systems can register and analyze small differences in the arrival time of sounds at the two ears in order to estimate the direction of sound sources became known as the Jeffress model.",
            "score": 176.36566162109375
        },
        {
            "docid": "32116125_4",
            "document": "Amblyaudia . Amblyaudia is a deficit in binaural integration of environmental information entering the auditory system. It is a disorder related to brain organization and function rather than what is typically considered a \u201chearing loss\u201d (damage to the cochlea). It may be genetic or developmentally acquired or both. When animals are temporarily deprived of hearing from an early age, profound changes occur in the brain. Specifically, cell sizes in brainstem nuclei are reduced, the configuration of brainstem dendrites are altered and neurons respond in different ways to sounds presented to both the deprived and non-deprived ears (in cases of asymmetric deprivation). This last point is particularly important for listening tasks that require inputs from two ears to perform well. There are multiple auditory functions that rely on the computation of well calibrated inputs from the two ears. Chief among these is the ability to localize sound sources and separate what we want to hear from a background of noise. In the brainstem, the auditory system compares the timing and levels of sounds between the two ears to encode the location of sound sources (sounds that originate from our right as opposed to left side are louder and arrive earlier in our right ear). This ability to separate sound sources not only helps us locate the trajectories of moving objects, but also to separate different sound sources in noisy environments.",
            "score": 176.29869079589844
        },
        {
            "docid": "4548229_5",
            "document": "Interaural time difference . The duplex theory states that ITDs are used to localise low frequency sounds, in particular, while ILDs are used in the localisation of high frequency sound inputs. However, the frequency ranges for which the auditory system can use ITDs and ILDs significantly overlap, and most natural sounds will have both high and low frequency components, so that the auditory system will in most cases have to combine information from both ITDs and ILDs to judge the location of a sound source.  A consequence of this duplex system is that it is also possible to generate so-called \"cue trading\" or \"time\u2013intensity trading\" stimuli on headphones, where ITDs pointing to the left are offset by ILDs pointing to the right, so the sound is perceived as coming from the midline. A limitation of the duplex theory is that the theory does not completely explain directional hearing, as no explanation is given for the ability to distinguish between a sound source directly in front and behind. Also the theory only relates to localising sounds in the horizontal plane around the head. The theory also does not take into account the use of the pinna in localisation.(Gelfand, 2004)",
            "score": 170.95225524902344
        },
        {
            "docid": "47338295_3",
            "document": "Sound localization in owls . Owls must be able to determine the necessary angle of descent, i.e. the elevation, in addition to azimuth (horizontal angle to the sound). This bi-coordinate sound localization is accomplished through two binaural cues: the interaural time difference (ITD) and the interaural level difference (ILD), also known as the interaural intensity difference (IID). The ability in owls is unusual; in ground-bound mammals such as mice, ITD and ILD are not utilized in the same manner. In these mammals, ITDs tend to be utilized for localization of lower frequency sounds, while ILDs tend to be used for higher frequency sounds.",
            "score": 169.91854858398438
        },
        {
            "docid": "31251362_5",
            "document": "Beat deafness . When sound waves reach the ears, the energy they contain is converted into electrical signals, which are sent via the auditory nerves to the brain. Sound processing begins when these electrical signals reach the primary auditory receiving area in the core part of the temporal lobe. Signals then travel to the area surrounding the core, known as the belt area, and are then transmitted to the parabelt area, which is located next to the belt. Simple sounds such as pure tones are able to activate the core area of the brain, but both the belt and parabelt areas are activated by only complex sounds, such as those found in speech and music. The auditory cortex in the left hemisphere of the brain is responsible for processing beat and rhythm in music. The right auditory cortex is primarily used in distinguishing between different harmonics, which are simple pure tones that combine to create complex tones.",
            "score": 169.85626220703125
        },
        {
            "docid": "41087200_8",
            "document": "Perceptual-based 3D sound localization . Interaural level differences (ILD) represents the difference in sound pressure level reaching the two ears. They provide salient cues for localizing high-frequency sounds in space, and populations of neurons that are sensitive to ILD are found at almost every synaptic level from brain stem to cortex. These cells are predominantly excited by stimulation of one ear and predominantly inhibited by stimulation of the other ear, such that the magnitude of their response is determined in large part by the intensities at the 2 ears. This gives rise to the concept of resonant damping. Interaural level difference (ILD) is best for high frequency sounds because low frequency sounds are not attenuated much by the head. ILD (also known as Interaural Intensity Difference) arises when the sound source is not centred, the listener's head partially shadows the ear opposite to the source, diminishing the intensity of the sound in that ear (particularly at higher frequencies). The pinnae filters the sound in a way that is directionally dependent. This is particularly useful in determining if a sound comes from above, below, in front, or behind.",
            "score": 167.2218017578125
        },
        {
            "docid": "4548229_2",
            "document": "Interaural time difference . The interaural time difference (or ITD) when concerning humans or animals, is the difference in arrival time of a sound between two ears. It is important in the localization of sounds, as it provides a cue to the direction or angle of the sound source from the head. If a signal arrives at the head from one side, the signal has further to travel to reach the far ear than the near ear. This pathlength difference results in a time difference between the sound's arrivals at the ears, which is detected and aids the process of identifying the direction of sound source.",
            "score": 165.84133911132812
        },
        {
            "docid": "4301708_10",
            "document": "Cochlear nucleus . The cochlear nuclear complex is the first integrative, or processing, stage in the auditory system. Information is brought to the nuclei from the ipsilateral cochlea via the cochlear nerve. Several tasks are performed in the cochlear nuclei. By distributing acoustic input to multiple types of principal cells, the auditory pathway is subdivided into parallel ascending pathways, which can simultaneously extract different types of information. The cells of the ventral cochlear nucleus extract information that is carried by the auditory nerve in the timing of firing and in the pattern of activation of the population of auditory nerve fibers. The cells of the dorsal cochlear nucleus perform a non-linear spectral analysis and place that spectral analysis into the context of the location of the head, ears and shoulders and that separate expected, self-generated spectral cues from more interesting, unexpected spectral cues using input from the auditory cortex, pontine nuclei, trigeminal ganglion and nucleus, dorsal column nuclei and the second dorsal root ganglion. It is likely that these neurons help mammals to use spectral cues for orienting toward those sounds. The information is used by higher brainstem regions to achieve further computational objectives (such as sound source location or improvement in signal to noise ratio). The inputs from these other areas of the brain probably play a role in sound localization.",
            "score": 165.19068908691406
        },
        {
            "docid": "4548229_7",
            "document": "Interaural time difference . Feddersen et al. (1957) also conducted experiments taking measurements on how ITDs alter with changing the azimuth of the loudspeaker around the head at different frequencies. But unlike the Woodworth experiments human subjects were used rather than a model of the head. The experiment results agreed with the conclusion made by Woodworth about ITDs. The experiments also concluded that is there is no difference in ITDs when sounds are provided from directly in front or behind at 0\u00b0 and 180\u00b0 azimuth. The explanation for this is that the sound is equidistant from both ears. Interaural time differences alter as the loudspeaker is moved around the head. The maximum ITD of 660 \u03bcs occurs when a sound source is positioned at 90\u00b0 azimuth to one ear.",
            "score": 164.57464599609375
        },
        {
            "docid": "31075772_15",
            "document": "Thought identification . On 31 January 2012 Brian Pasley and colleagues of University of California Berkeley published their paper in PLoS Biology wherein subjects' internal neural processing of auditory information was decoded and reconstructed as sound on computer by gathering and analyzing electrical signals directly from subjects' brains. The research team conducted their studies on the superior temporal gyrus, a region of the brain that is involved in higher order neural processing to make semantic sense from auditory information. The research team used a computer model to analyze various parts of the brain that might be involved in neural firing while processing auditory signals. Using the computational model, scientists were able to identify the brain activity involved in processing auditory information when subjects were presented with recording of individual words. Later, the computer model of auditory information processing was used to reconstruct some of the words back into sound based on the neural processing of the subjects. However the reconstructed sounds were not of good quality and could be recognized only when the audio wave patterns of the reconstructed sound were visually matched with the audio wave patterns of the original sound that was presented to the subjects. However this research marks a direction towards more precise identification of neural activity in cognition.",
            "score": 163.55752563476562
        },
        {
            "docid": "8436042_6",
            "document": "MPEG Surround . MPEG Surround coding uses our capacity to perceive sound in the 3D and captures that perception in a compact set of parameters. Spatial perception is primarily attributed to three parameters, or cues, describing how humans localize sound in the horizontal plane: Interaural level difference (ILD), Interaural time difference (ITD) and Interaural coherence (IC). This three concepts are illustrated in next image. Direct, or first-arrival, waveforms from the source hit the left ear at time, while direct sound received by the right ear is diffracted around the head, with time delay and level attenuation, associated. These two effects result in ITD and ILD are associated with the main source. At last, in a reverberant environment, reflected sound from the source, or sound from diffuse source, or uncorrelated sound can hit both ears, all of them are related with IC.",
            "score": 162.41049194335938
        },
        {
            "docid": "4548229_6",
            "document": "Interaural time difference . Experiments conducted by Woodworth (1938) tested the duplex theory by using a solid sphere to model the shape of the head and measuring the ITDs as a function of azimuth for different frequencies. The model used had a distance between the 2 ears of approximately 22\u201323\u00a0cm. Initial measurements found that there was a maximum time delay of approximately 660 \u03bcs when the sound source was placed at directly 90\u00b0 azimuth to one ear. This time delay correlates to the wavelength of a sound input with a frequency of 1500\u00a0Hz. The results concluded that when a sound played had a frequency less than 1500\u00a0Hz the wavelength is greater than this maximum time delay between the ears. Therefore there is a phase difference between the sound waves entering the ears providing acoustic localisation cues. With a sound input with a frequency closer to 1500\u00a0Hz the wavelength of the sound wave is similar to the natural time delay. Therefore due to the size of the head and the distance between the ears there is a reduced phase difference so localisations errors start to be made. When a high frequency sound input is used with a frequency greater than 1500\u00a0Hz, the wavelength is shorter than the distance between the 2 ears, a head shadow is produced and ILD provide cues for the localisation of this sound.",
            "score": 162.24952697753906
        },
        {
            "docid": "8953842_15",
            "document": "Computational auditory scene analysis . While many models consider the audio signal as a complex combination of different frequencies, modeling the auditory system can also require consideration for the neural components. By taking a holistic process, where a stream (of feature-based sounds) correspond to neuronal activity distributed in many brain areas, the perception of the sound could be mapped and modeled. Two different solutions have been proposed to the binding of the audio perception and the area in the brain. Hierarchical coding models many cells to encode all possible combinations of features and objects in the auditory scene. Temporal or oscillatory correlation addressing the binding problem by focusing on the synchrony and desynchrony between neural oscillations to encode the state of binding among the auditory features. These two solutions are very similar to the debacle between place coding and temporal coding. While drawing from modeling neural components, another phenomenon of ASA comes into play with CASA systems: the extent of modeling neural mechanisms. The studies of CASA systems have involved modeling some known mechanisms, such as the bandpass nature of cochlear filtering and random auditory nerve firing patterns, however, these models may not lead to finding new mechanisms, but rather give an understanding of purpose to the known mechanisms.",
            "score": 161.954345703125
        },
        {
            "docid": "7527647_14",
            "document": "Binaural fusion . Sound localization is the ability to correctly identify the directional location of sounds. A sound stimulus localized in the horizontal plane is called azimuth; in the vertical plane it is referred to as elevation. The time, intensity, and spectral differences in the sound arriving at the two ears are used in localization. Localization of low frequency sounds is accomplished by analyzing interaural time difference (ITD). Localization of high frequency sounds is accomplished by analyzing interaural level difference (ILD).",
            "score": 160.41944885253906
        },
        {
            "docid": "579799_8",
            "document": "Georg von B\u00e9k\u00e9sy . He concluded that his observations showed how different sound wave frequencies are locally dispersed before exciting different nerve fibers that lead from the cochlea to the brain. He theorized that the placement of each sensory cell (hair cell) along the coil of the cochlea corresponds to a specific frequency of sound (the so-called tonotopy). B\u00e9k\u00e9sy later developed a mechanical model of the cochlea, which confirmed the concept of frequency dispersion by the basilar membrane in the mammalian cochlea. But this model could not provide any information as to a possible function of this frequency dispersion in the process of hearing.",
            "score": 160.2487335205078
        },
        {
            "docid": "685746_11",
            "document": "Laterality . Cerebral dominance or specialization has been studied in relation to a variety of human functions. With speech in particular, many studies have been used as evidence that it is generally localized in the left hemisphere. Research comparing the effects of lesions in the two hemispheres, split-brain patients, and perceptual asymmetries have aided in the knowledge of speech lateralization. In one particular study, the left hemisphere\u2019s sensitivity to differences in rapidly changing sound cues was noted (Annett, 1991). This has real world implication, since very fine acoustic discriminations are needed to comprehend and produce speech signals. In an electrical stimulation demonstration performed by Ojemann and Mateer (1979), the exposed cortex was mapped revealing the same cortical sites were activated in phoneme discrimination and mouth movement sequences (Annett, 1991).",
            "score": 159.92543029785156
        },
        {
            "docid": "47862835_3",
            "document": "Virtual hammock . A network of nerves in the MSO detect the place that a threshold is overcome by sending electrochemical impulses along the auditory nerve pathway to the cortex for higher-level processing. These nerves permit the subconscious localization of a sound source; people can identify the direction from which a sound emanates before they can identify the type of source, which is done when the signals reach the cortex. A sound source produces a propagating sound wave that strikes the ear closer to it before traveling an extra distance to strike the opposite ear. The impulses produced in the auditory nerves which conduct signals to the brain for sound processing pass one another in the MSO at a point on the opposite side relative to the location sound source. If a sound impulse originates from a point equidistant to each ear (e.g. directly behind or in front of the head), the neuronal impulses from each ear will pass at a point in the center of the MSO, allowing us to unconsciously identify where a sound source is located. The Virtual Hammock effect is achieved by intentionally manipulating the passing point by shifting the maximum amplitudes of sound waveforms that are directed into each ear.",
            "score": 159.08021545410156
        },
        {
            "docid": "47338295_12",
            "document": "Sound localization in owls . Ear asymmetry allows for sound originating from below the eye level to sound louder in the left ear, while sound originating from above the eye level to sound louder in the right ear. Asymmetrical ear placement also causes IID for high frequencies (between 4\u00a0kHz and 8\u00a0kHz) to vary systematically with elevation, converting IID into a map of elevation. Thus, it is essential for an owl to have the ability to hear high frequencies. Many birds have the neurophysiological machinery to process both ITD and IID, but because they have small heads and low frequency sensitivity, they use both parameters only for localization in the azimuth. Through evolution, the ability to hear frequencies higher than 3\u00a0kHz, the highest frequency of owl flight noise, enabled owls to exploit elevational IIDs, produced by small ear asymmetries that arose by chance, and began the evolution of more elaborate forms of ear asymmetry.",
            "score": 158.30650329589844
        },
        {
            "docid": "41087200_7",
            "document": "Perceptual-based 3D sound localization . According to the duplex theory, ITDs have a greater contribution to the localisation of low frequency sounds (below 1 kHz), while ILDs are used in the localisation of high frequency sound. These approaches can be applied to selective reconstructions of spatialized signals, where spectrotemporal components believed to be dominated by the desired sound source are identified and isolated through the Short-time Fourier transform (STFT). Modern systems typically compute the STFT of the incoming signal from two or more microphones, and estimate the ITD or each spectrotemporal component by comparing the phases of the STFTs. An advantage to this approach is that it may be generalized to more than two microphones, which can improve accuracy in 3 dimensions and remove the front-back localization ambiguity that occurs with only two ears or microphones. Another advantage is that the ITD is relatively strong and easy to obtain without biomimetic instruments such as dummy heads and artificial pinnae, though these may still be used to enhance amplitude disparities. HRTF phase response is mostly linear and listeners are insensitive to the details of the interaural phase spectrum as long as the interaural time delay (ITD) of the combined low-frequency part of the waveform is maintained.",
            "score": 157.88894653320312
        },
        {
            "docid": "47338295_6",
            "document": "Sound localization in owls . The axons of the auditory nerve originate from the hair cells of the cochlea in the inner ear. Different sound frequencies are encoded by different fibers of the auditory nerve, arranged along the length of the auditory nerve, but codes for the timing and level of the sound are not segregated within the auditory nerve. Instead, the ITD is encoded by phase locking, i.e. firing at or near a particular phase angle of the sinusoidal stimulus sound wave, and the IID is encoded by spike rate. Both parameters are carried by each fiber of the auditory nerve.",
            "score": 157.68702697753906
        },
        {
            "docid": "525667_10",
            "document": "Human echolocation . In a 2014 study by Thaler and colleagues, the researchers first made recordings of the clicks and their very faint echoes using tiny microphones placed in the ears of the blind echolocators as they stood outside and tried to identify different objects such as a car, a flag pole, and a tree. The researchers then played the recorded sounds back to the echolocators while their brain activity was being measured using functional magnetic resonance imaging. Remarkably, when the echolocation recordings were played back to the blind experts, not only did they perceive the objects based on the echoes, but they also showed activity in those areas of their brain that normally process visual information in sighted people, primarily primary visual cortex or V1. This result is surprising, as visual areas, as their names suggest, are only active during visual tasks. The brain areas that process auditory information were no more activated by sound recordings of outdoor scenes containing echoes than they were by sound recordings of outdoor scenes with the echoes removed. Importantly, when the same experiment was carried out with sighted people who did not echolocate, these individuals could not perceive the objects and there was no echo-related activity anywhere in the brain. This suggests that the cortex of blind echolocators is plastic and reorganizes such that primary visual cortex, rather than any auditory area, becomes involved in the computation of echolocation tasks.",
            "score": 157.51368713378906
        },
        {
            "docid": "1021754_19",
            "document": "Sound localization . Helmut Haas discovered that we can discern the sound source despite additional reflections at 10 decibels louder than the original wave front, using the earliest arriving wave front. This principle is known as the Haas effect, a specific version of the precedence effect. Haas measured down to even a 1 millisecond difference in timing between the original sound and reflected sound increased the spaciousness, allowing the brain to discern the true location of the original sound. The nervous system combines all early reflections into a single perceptual whole allowing the brain to process multiple different sounds at once. The nervous system will combine reflections that are within about 35 milliseconds of each other and that have a similar intensity.",
            "score": 156.9569549560547
        },
        {
            "docid": "7527647_15",
            "document": "Binaural fusion . Action potentials originate in the hair cells of the cochlea and propagate to the brainstem; both the timing of these action potentials and the signal they transmit provide information to the SOC about the orientation of sound in space. The processing and propagation of action potentials is rapid, and therefore, information about the timing of the sounds that were heard, which is crucial to binaural processing, is conserved. Each eardrum moves in one dimension, and the auditory brain analyzes and compares the movements of both eardrums in order to synthesize auditory objects. This integration of information from both ears is the essence of binaural fusion. The binaural system of hearing involves sound localization in the horizontal plane, contrasting with the monaural system of hearing, which involves sound localization in the vertical plane.",
            "score": 156.68917846679688
        },
        {
            "docid": "1021754_25",
            "document": "Sound localization . Duplex theory clearly points out that ITD and IID play significant roles in sound localization but they can only deal with lateral localizing problems. For example, based on duplex theory, if two acoustic sources are symmetrically located on the right front and right back of the human head, they will generate equal ITDs and IIDs, which is called as cone model effect. However, human ears can actually distinguish this set of sources. Besides that, in natural sense of hearing, only one ear, which means no ITD or IID, can distinguish the sources with a high accuracy. Due to the disadvantages of duplex theory, researchers proposed the pinna filtering effect theory. The shape of human pinna is very special. It is concave with complex folds and asymmetrical no matter horizontally or vertically. The reflected waves and the direct waves will generate a frequency spectrum on the eardrum, which is related to the acoustic sources. Then auditory nerves localize the sources by this frequency spectrum. Therefore, a corresponding theory was proposed and called as pinna filtering effect theory.",
            "score": 155.61167907714844
        }
    ]
}