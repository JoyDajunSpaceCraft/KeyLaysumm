{
    "q": [
        {
            "docid": "4548229_17",
            "document": "Interaural time difference . The lateral lemniscus (LL) is the main auditory tract in the brainstem connecting SOC to the inferior colliculus. The dorsal nucleus of the lateral lemniscus (DNLL) is a group of neurons separated by lemniscus fibres, these fibres are predominantly destined for the inferior colliculus (IC). In studies using an unanesthetized rabbit the DNLL was shown to alter the sensitivity of the IC neurons and may alter the coding of interaural timing differences (ITDs) in the IC.(Kuwada et al., 2005) The ventral nucleus of the lateral lemniscus (VNLL) is a chief source of input to the inferior colliculus. Research using rabbits shows the discharge patterns, frequency tuning and dynamic ranges of VNLL neurons supply the inferior colliculus with a variety of inputs, each enabling a different function in the analysis of sound.(Batra & Fitzpatrick, 2001)  In the inferior colliculus (IC) all the major ascending pathways from the olivary complex and the central nucleus converge. The IC is situated in the midbrain and consists of a group of nuclei the largest of these is the central nucleus of inferior colliculus (CNIC). The greater part of the ascending axons forming the lateral lemniscus will terminate in the ipsilateral CNIC however a few follow the commissure of Probst and terminate on the contralateral CNIC. The axons of most of the CNIC cells form the brachium of IC and leave the brainstem to travel to the ipsilateral thalamus. Cells in different parts of the IC tend to be monaural, responding to input from one ear, or binaural and therefore respond to bilateral stimulation.",
            "score": 217.5764355659485
        },
        {
            "docid": "5051081_4",
            "document": "Eric Knudsen . In 1978, Knudsen and Konishi presented the discovery of an auditory map of space in the midbrain of the barn owl. This discovery was groundbreaking because it unearthed the first non-somatotopic space map in the brain. The map was found in the owl\u2019s midbrain, in the lateral and anterior mesencephalicus lateralis dorsalis (MLD), a structure now referred to as the inferior colliculus. Unlike most sound-localization maps, this map was found to be two-dimensional, with units arranged spatially to represent both the vertical and horizontal location of sound. Knudsen and Konishi discovered that units in this structure respond preferentially to sounds originating in a particular region in space. In the 1978 paper, elevation and azimuth (location in the horizontal plane) were shown to be the two coordinates of the map. Using a speaker set on a rotatable hemispherical track, Knudsen and Konishi presented owls with auditory stimulus from various locations in space and recorded the resulting neuronal activity. They found that neurons in this part of the MLD were organized according to the location of their receptive field, with azimuth varying along the horizontal plane of the space map and elevation varying vertically.  Knudsen followed this discovery with research into specific sound localization mechanisms. Two main auditory cues used by the barn owl to localize sound are interaural time difference (ITD) and interaural intensity difference (IID). The owl\u2019s ears are asymmetric, with the right ear\u2019s opening being directed higher than that of the left. This asymmetry allows the barn owl to determine the elevation of a sound by comparing sound levels between its two ears. Interaural time differences provide the owl with information regarding a sound\u2019s azimuth; sound will reach the ear closer to the sound source before reaching the farther ear, and this time difference can be detected and interpreted as an azimuthal direction. At low frequencies, the wavelength of a sound is wider than the owl's facial ruff, and the ruff does not affect detection of azimuth. At high frequencies, the ruff plays a role in reflecting sound for heightened sensitivity to vertical elevation. Therefore, with wide-band noise, containing both high and low frequencies, the owl could use interaural spectrum difference to obtain information about both azimuth and elevation. In 1979, Knudsen and Konishi showed that the barn owl uses interaural spectrum information in sound localization. They presented owls with both wide-bandwidth noise and pure tones. The birds were able to successfully locate pure tones (since they could still gather information from IID and ITD), but their error rate was much lower when localizing wide-bandwidth noise. This indicates that the birds utilize interaural spectrum differences to improve their accuracy.",
            "score": 319.8892719745636
        },
        {
            "docid": "47338295_8",
            "document": "Sound localization in owls . In the time pathway, the nucleus laminaris (mammalian medial superior olive) is the first site of binaural convergence. It is here that ITD is detected and encoded using neuronal delay lines and coincidence detection, as in the Jeffress model; when phase-locked impulses coming from the left and right ears coincide at a laminaris neuron, the cell fires most strongly. Thus, the nucleus laminaris acts as a delay-line coincidence detector, converting distance traveled to time delay and generating a map of interaural time difference. Neurons from the nucleus laminaris project to the core of the central nucleus of the inferior colliculus and to the anterior lateral lemniscal nucleus.",
            "score": 208.15930843353271
        },
        {
            "docid": "4548229_16",
            "document": "Interaural time difference . The MSO is made up of neurons which receive input from the low-frequency fibers of the left and right AVCN. The result of having input from both cochleas is an increase in the firing rate of the MSO units. The neurons in the MSO are sensitive to the difference in the arrival time of sound at each ear, also known as the interaural time difference (ITD). Research shows that if stimulation arrives at one ear before the other, many of the MSO units will have increased discharge rates. The axons from the MSO continue to higher parts of the pathway via the ipsilateral lateral lemniscus tract.(Yost, 2000)",
            "score": 266.404744386673
        },
        {
            "docid": "14532984_7",
            "document": "Coincidence detection in neurobiology . Coincidence detection has been shown to be a major factor in sound localization along the azimuth plane in several organisms. In 1948, Lloyd A. Jeffress proposed that some organisms may have a collection of neurons that receive auditory input from each ear. The neural pathways to these neurons are called delay lines. Jeffress claimed that the neurons that the delay lines link act as coincidence detectors by firing maximally when receiving simultaneous inputs from both ears. When a sound is heard, sound waves may reach the ears at different times. This is referred to as the interaural time difference (ITD). Due to differing lengths and a finite conduction speed within the axons of the delay lines, different coincidence detector neurons will fire when sound comes from different positions along the azimuth. Jeffress' model proposes that two signals even from an asynchronous arrival of sound in the cochlea of each ear will converge synchronously on a coincidence detector in the auditory cortex based on the magnitude of the ITD (Fig. 2). Therefore, the ITD should correspond to an anatomical map that can be found within the brain. Masakazu Konishi's study on barn owls shows that this is true. Sensory information from the hair cells of the ears travels to the ipsilateral nucleus magnocellularis. From here, the signals project ipsilaterally and contralaterally to two nucleus laminari. Each nucleus laminaris contains coincidence detectors that receive auditory input from the left and the right ear. Since the ipsilateral axons enter the nucleus laminaris dorsally while the contralateral axons enter ventrally, sounds from various positions along the azimuth correspond directly to stimulation of different depths of the nucleus laminaris. From this information, a neural map of auditory space was formed. The function of the nucleus laminaris parallels that of the medial superior olive in mammals.",
            "score": 239.74064803123474
        },
        {
            "docid": "47338295_10",
            "document": "Sound localization in owls . The time and sound-pressure pathways converge at the lateral shell of the central nucleus of the inferior colliculus. The lateral shell projects to the external nucleus, where each space-specific neuron responds to acoustic stimuli only if the sound originates from a restricted area in space, i.e. the receptive field of that neuron. These neurons respond exclusively to binaural signals containing the same ITD and IID that would be created by a sound source located in the neuron\u2019s receptive field. Thus their receptive fields arise from the neurons\u2019 tuning to particular combinations of ITD and IID, simultaneously in a narrow range. These space-specific neurons can thus form a map of auditory space in which the positions of receptive fields in space are isomorphically projected onto the anatomical sites of the neurons.",
            "score": 161.9012644290924
        },
        {
            "docid": "3915756_3",
            "document": "Electrical tuning . With a combination of voltage-gated calcium channels and calcium sensitive K+ channels, the cells set up an oscillation in voltage and oscillate in response to a depolarizing stimulus. Ca enters the cell and depolarizes it, opening K channels which allow K to leave and hyperpolarize the cell. Since the exit of K from the cell is delayed, the voltage increases, but then with the exit of the K the cell overshoots the membrane potential and hyperpolarizes. Different cells have different delays for K leaving and thus the voltage oscillates at different frequencies. The delays can be as short as 0.7 ms or as long as 150 ms, whereas the Ca entry always occurs within about 1 ms (Fettiplace, 1987). Thus by varying the length of delay for K to leave, cells' ion concentrations can oscillate at specific frequencies. Cells with higher oscillating frequencies respond best to higher-frequency sounds, while those with lower frequencies respond best to lower frequency ones. Which cells resonate best with a given sound tells the brain what the frequency of the sound is. Hair cells that respond to high frequency stimuli send information to specific neurons, and the information remains segregated in the brain. Thus, information about sound frequency is preserved, rather than being lost as it would if all information from different hair cells converged on the same neuron or group of neurons.",
            "score": 208.435227394104
        },
        {
            "docid": "33767830_10",
            "document": "Conspecific song preference . Neurons in both Field L and CM have sophisticated filter properties, selective for both the spectral-temporal modulations and phase relationships of conspecific songs. Furthermore, different neurons are selective for different features of syllables and songs. In Field L, neurons have one of four different tuning strategies\u2014they are either tightly tuned a particular frequency, or they are sensitive to frequency edges, frequency sweeps or combined frequencies. When exposed to natural song as a stimulus, different ensembles of these of neurons respond to different components of sound, and together they demonstrate the ability to perform sensitive discrimination between conspecific and heterospecific syllable types. As in nucleus ovoidalis, the spectral-temporal filter properties of Field L and CM neurons are a function of the particular ion channels and receptor proteins driving their synaptic dynamics. The complex and sophisticated tuning of these higher order processing centers for conspecific sounds may rely on the integrated inputs from the entire ascending auditory pathway, from the hair cells through the thalamus and forebrain, but this challenging synthetic question remains to be investigated.",
            "score": 181.53054761886597
        },
        {
            "docid": "4548229_11",
            "document": "Interaural time difference . Lastly, they went on to further explore the driving forces behind the interaural time difference, specifically whether the process is simply the alignment of inputs that is processed by a coincidence detector, or whether the process is more complicated. Evidence from Franken, et. al. shows that the processing is affected by inputs that precede the binaural signal, which would alter the functioning of voltage-gated sodium and potassium channels to shift the membrane potential of the neuron. Furthermore, the shift is dependent on the frequency tuning of each neuron, ultimately creating a more complex confluence and analysis of sound. Franken, et. al.'s findings provide several pieces of evidence that contradict existing theories about binaural audition.",
            "score": 236.13672637939453
        },
        {
            "docid": "11217018_8",
            "document": "A-weighting . A-frequency-weighting is mandated by the international standard IEC 61672 to be fitted to all sound level meters. The old B- and D-frequency-weightings have fallen into disuse, but many sound level meters provide for C frequency-weighting and its fitting is mandated \u2014 at least for testing purposes \u2014 to precision (Class one) sound level meters. D-frequency-weighting was specifically designed for use when measuring high level aircraft noise in accordance with the IEC 537 measurement standard. The large peak in the D-weighting curve is not a feature of the equal-loudness contours, but reflects the fact that humans hear random noise differently from pure tones, an effect that is particularly pronounced around 6\u00a0kHz. This is because individual neurons from different regions of the cochlea in the inner ear respond to narrow bands of frequencies, but the higher frequency neurons integrate a wider band and hence signal a louder sound when presented with noise containing many frequencies than for a single pure tone of the same pressure level. Following changes to the ISO standard, D-frequency-weighting should now only be used for non-bypass engines and as these are not fitted to commercial aircraft \u2014 but only to military ones \u2014 A-frequency-weighting is now mandated for light civilian aircraft measurements, while a more accurate loudness-corrected weighting EPNdB is required for certification of large transport aircraft",
            "score": 202.36945509910583
        },
        {
            "docid": "25522368_22",
            "document": "Feature detection (nervous system) . In FM-FM regions of the auditory cortex, Suga et al. (1993) identified combination-sensitive neurons which receive inputs from multiple sources. Suga observed that the FM-FM region selectively responded to a FM component (feature) in the call and in the echo. More specifically, an individual FM1-FM2 unit requires an input from a unit tuned to the FM1 frequency range and a second unit tuned to the FM2 frequency range in order to fire. These FM-FM neurons can be considered complex feature detectors because they are sensitive to a particular frequency combination and a specific time delay between the echo and call. An accurate determination of the time delay between the call and echo is critical because it allows the bat to measure the distance between itself and its prey. This FM-FM sensitive region is only one example of a feature detector in the bat auditory cortex. A CF-CF sensitive region also exists in the auditory cortex, which in combination with FM-FM regions allows the bat to create maps for relative target velocity and target distance. Tracing the responses of these combination-sensitive neurons to higher order regions of the auditory pathway reveals that there are neurons with even higher levels of frequency and amplitude selectivity.",
            "score": 128.746666431427
        },
        {
            "docid": "17523336_13",
            "document": "Olivocochlear system . Using acoustic stimuli to activate the MOC reflex pathway, recordings have been made from single efferent fibres in guinea pigs and cats. Both studies confirmed that MOC neurons are sharply tuned to frequency, as previously suggested by Cody and Johnstone (1982), and Robertson (1984). They also showed that the firing rate of MOC neurons increased as the intensity of sound increased from 0 to 100\u00a0dB SPL, and have comparable thresholds (within ~15\u00a0dB) to afferent neurons. Furthermore, both studies showed that most MOC neurons responded to sound presented in the ipsilateral ear, consistent with the majority of mammalian MOC neurons being contralaterally located. No recordings have been made from MOC fibres in humans. because invasive \"in vivo\" experiments are not possible. In other primate species however, it has been shown that about 50-60% of MOC fibres are crossed (Bodian and Gucer, 1980; Thompson and Thompson, 1986).",
            "score": 181.97159016132355
        },
        {
            "docid": "6766895_8",
            "document": "Topographic map (neuroanatomy) . The auditory system is the sensory system for hearing in which the brain interprets information from the frequency of sound waves, yielding the perception of tones. Sound waves enter the ear through the auditory canal. These waves arrive at the eardrum where the properties of the waves are transduced into vibrations. The vibrations travel through the bones of the inner ear to the cochlea. In the cochlea, the vibrations are transduced into electrical information through the firing of hair cells in the organ of Corti. The organ of Corti projects in an orderly fashion to structures in the brainstem (namely, the cochlear nuclei and the inferior colliculus), and from there to the medial geniculate nucleus of the thalamus and the primary auditory cortex. Adjacent sites on the organ of Corti, which are themselves selective for the sound frequency, are represented by adjacent neurons in the aforementioned CNS structures. This projection pattern has been termed tonotopy.",
            "score": 176.26272749900818
        },
        {
            "docid": "4548229_7",
            "document": "Interaural time difference . Feddersen et al. (1957) also conducted experiments taking measurements on how ITDs alter with changing the azimuth of the loudspeaker around the head at different frequencies. But unlike the Woodworth experiments human subjects were used rather than a model of the head. The experiment results agreed with the conclusion made by Woodworth about ITDs. The experiments also concluded that is there is no difference in ITDs when sounds are provided from directly in front or behind at 0\u00b0 and 180\u00b0 azimuth. The explanation for this is that the sound is equidistant from both ears. Interaural time differences alter as the loudspeaker is moved around the head. The maximum ITD of 660 \u03bcs occurs when a sound source is positioned at 90\u00b0 azimuth to one ear.",
            "score": 244.49071598052979
        },
        {
            "docid": "7527647_19",
            "document": "Binaural fusion . MSO neurons are excited bilaterally, meaning that they are excited by inputs from both ears, and they are therefore referred to as EE neurons. Fibers from the left cochlear nucleus terminate on the left of MSO neurons, and fibers from the right cochlear nucleus terminate on the right of MSO neurons. Excitatory inputs to the MSO from spherical bushy cells are mediated by glutamate, and inhibitory inputs to the MSO from globular bushy cells are mediated by glycine. MSO neurons extract ITD information from binaural inputs and resolve small differences in the time of arrival of sounds at each ear. Outputs from the MSO and LSO are sent via the lateral lemniscus to the IC, which integrates the spatial localization of sound. In the IC, acoustic cues have been processed and filtered into separate streams, forming the basis of auditory object recognition.",
            "score": 165.3584554195404
        },
        {
            "docid": "2263473_5",
            "document": "Volley theory . Phase-locking is known as matching amplitude times to a certain phase of another waveform. In the case of auditory neurons, this means firing an action potential at a certain phase of a stimulus sound being delivered. It has been seen that when being played a pure tone, auditory nerve fibers will fire at the same frequency as the tone. Volley theory suggests that groups of auditory neurons use phase-locking to represent subharmonic frequencies of one harmonic sound. This has been shown in guinea pig and cat models.",
            "score": 183.78894686698914
        },
        {
            "docid": "39265695_8",
            "document": "Stimulus filtering . When looking at the nervous systems of flies, researchers found three auditory afferents. Type one fires only one spike to the stimulus onset, has low jitter (variability in timing over stimulus presentations), no spontaneous activity, and is the most common type. Type two fires two to four spikes to the stimulus onset, has increased jitter with subsequent spikes, and has low spontaneous activity. Finally, type three has tonic spiking to the presented stimulus, has low jitter only with the first spikes, has low spontaneous activity, and is the least common type. Researchers discovered that neurons responded the strongest to sound frequencies between 4 and 9\u00a0kHz, which includes the frequencies present in cricket songs. Also, neurons were found to have responded strongest at 4.5\u00a0kHz, which is the frequency of the Gryllus song. Despite the type of auditory afferent, all observed neurons revealed an inverse/latency relationship. The stronger the stimulus, the shorter the time until the neuron begins to respond. The difference in the number of afferents above the threshold on a side of the animal is called population code and can be used to account for sound localization.",
            "score": 187.53837502002716
        },
        {
            "docid": "41087200_8",
            "document": "Perceptual-based 3D sound localization . Interaural level differences (ILD) represents the difference in sound pressure level reaching the two ears. They provide salient cues for localizing high-frequency sounds in space, and populations of neurons that are sensitive to ILD are found at almost every synaptic level from brain stem to cortex. These cells are predominantly excited by stimulation of one ear and predominantly inhibited by stimulation of the other ear, such that the magnitude of their response is determined in large part by the intensities at the 2 ears. This gives rise to the concept of resonant damping. Interaural level difference (ILD) is best for high frequency sounds because low frequency sounds are not attenuated much by the head. ILD (also known as Interaural Intensity Difference) arises when the sound source is not centred, the listener's head partially shadows the ear opposite to the source, diminishing the intensity of the sound in that ear (particularly at higher frequencies). The pinnae filters the sound in a way that is directionally dependent. This is particularly useful in determining if a sound comes from above, below, in front, or behind.",
            "score": 305.7006404399872
        },
        {
            "docid": "1222458_3",
            "document": "Tonotopy . Tonotopy in the auditory system begins at the cochlea, the small snail-like structure in the inner ear that sends information about sound to the brain. Different regions of the basilar membrane in the organ of Corti, the sound-sensitive portion of the cochlea, vibrate at different sinusoidal frequencies due to variations in thickness and width along the length of the membrane. Nerves that transmit information from different regions of the basilar membrane therefore encode frequency tonotopically. This tonotopy then projects through the vestibulocochlear nerve and associated midbrain structures to the primary auditory cortex via the auditory radiation pathway. Throughout this radiation, organization is linear with relation to placement on the organ of Corti, in accordance to the best frequency response (that is, the frequency at which that neuron is most sensitive) of each neuron. However, binaural fusion in the superior olivary complex onward adds significant amounts of information encoded in the signal strength of each ganglion. Thus, the number of tonotopic maps varies between species and the degree of binaural synthesis and separation of sound intensities; in humans, six tonotopic maps have been identified in the primary auditory cortex. their anatomical locations along the auditory cortex.",
            "score": 183.18620812892914
        },
        {
            "docid": "69274_38",
            "document": "Animal echolocation . Suga and his colleagues have shown that the cortex contains a series of \"maps\" of auditory information, each of which is organized systematically based on characteristics of sound such as frequency and amplitude. The neurons in these areas respond only to a specific combination of frequency and timing (sound-echo delay), and are known as combination-sensitive neurons.",
            "score": 171.70633268356323
        },
        {
            "docid": "3106926_2",
            "document": "Temporal theory (hearing) . The temporal theory of hearing states that human perception of sound depends on temporal patterns with which neurons respond to sound in the cochlea. Therefore, in this theory, the pitch of a pure tone is determined by the period of neuron firing patterns\u2014either of single neurons, or groups as described by the volley theory. Temporal or timing theory competes with the place theory of hearing, which instead states that pitch is signaled according to the locations of vibrations along the basilar membrane.",
            "score": 162.10430693626404
        },
        {
            "docid": "2920040_6",
            "document": "Neuronal tuning . Neurons in other systems also become selectively tuned to stimuli. In the auditory system, different neurons may respond selectively to the frequency (pitch), amplitude (loudness), and/or complexity (uniqueness) of sounds. In the olfactory system, neurons may be tuned to certain kinds of smells. In the gustatory system, different neurons may respond selectively to different components of food: sweet, sour, salty, and bitter. In the somatosensory system, neurons may be selectively tuned to different types of pressure, temperature, bodily position, and pain. This tuning in the somatosensory system also provides feedback to the motor system so that it may selectively tune neurons to respond in specific ways to given stimuli. Finally, the encoding and storage of information in both short-term and long-term memory requires the tuning of neurons in complex ways such that information may be later retrieved.",
            "score": 173.65480303764343
        },
        {
            "docid": "69274_36",
            "document": "Animal echolocation . These interneurons are specialized for time sensitivity in several ways. First, when activated, they generally respond with only one or two action potentials. This short duration of response allows their action potentials to give a very specific indication of the exact moment of the time when the stimulus arrived, and to respond accurately to stimuli that occur close in time to one another. In addition, the neurons have a very low threshold of activation \u2013 they respond quickly even to weak stimuli. Finally, for FM signals, each interneuron is tuned to a specific frequency within the sweep, as well as to that same frequency in the following echo. There is specialization for the CF component of the call at this level as well. The high proportion of neurons responding to the frequency of the acoustic fovea actually increases at this level.",
            "score": 144.92537689208984
        },
        {
            "docid": "994097_9",
            "document": "Auditory cortex . Neurons in the auditory cortex are organized according to the frequency of sound to which they respond best. Neurons at one end of the auditory cortex respond best to low frequencies; neurons at the other respond best to high frequencies. There are multiple auditory areas (much like the multiple areas in the visual cortex), which can be distinguished anatomically and on the basis that they contain a complete \"frequency map.\" The purpose of this frequency map (known as a tonotopic map) is unknown, and is likely to reflect the fact that the cochlea is arranged according to sound frequency. The auditory cortex is involved in tasks such as identifying and segregating \"auditory objects\" and identifying the location of a sound in space. For example, it has been shown that A1 encodes complex and abstract aspects of auditory stimuli without encoding their \"raw\" aspects like frequency content, presence of a distinct sound or its echoes.",
            "score": 174.81926131248474
        },
        {
            "docid": "25522368_13",
            "document": "Feature detection (nervous system) . Having already used electrical point stimulation to identify the optic tectum as the region responsible for the prey-catching behaviors, Ewert and colleagues located and recorded from individual prey-selective neurons of the optic tectum in freely moving toads. These T5.2 neurons would increase in discharge frequency prior to a snapping or orienting behavior response. In the case of the snapping behavior, the neurons would stop firing for the duration of the snap. Evidently, these neurons exhibit a preference in responses to the worm configuration of moving bar stimuli and can therefore be considered feature detectors. To get a general idea of their properties, in successive experiments various rectangular dark objects of different edge lengths traverse a toad's visual field against a bright background at constant velocity; then the discharge frequency of a T5.2 neuron towards such an object is correlated with the toad's promptness of responding with prey-capture, expressed by the response latency. Thus, prey feature detection is not an all-or-nothing condition, but rather a matter of degree: the greater an object's releasing value as a prey stimulus, the stronger is prey-selective T5.2 neuron's discharge frequency, the shorter is toad's prey-catching response latency, and the higher is the number of prey-catching responses during a period of time (prey-catching activity)\u2014as well as vice versa.",
            "score": 142.16684985160828
        },
        {
            "docid": "25260196_14",
            "document": "Neuronal encoding of sound . The cochlea of the inner ear, a marvel of physiological engineering, acts as both a frequency analyzer and nonlinear acoustic amplifier. The cochlea has over 32,000 hair cells. Outer hair cells primarily provide amplification of traveling waves that are induced by sound energy, while inner hair cells detect the motion of those waves and excite the (Type I) neurons of the auditory nerve.",
            "score": 149.48731660842896
        },
        {
            "docid": "1187487_59",
            "document": "Sensorineural hearing loss . Research by Kluk and Moore has shown that dead regions may also affect the patient\u2019s perception of frequencies beyond the dead regions. There is an enhancement in the ability to distinguish between tones that differ very slightly in frequency, in regions just beyond the dead regions compared to tones further away. An explanation for this may be that cortical re-mapping has occurred. Whereby, neurons which would normally be stimulated by the dead region, have been reassigned to respond to functioning areas near it. This leads to an over-representation of these areas, resulting in an increased perceptual sensitivity to small frequency differences in tones.",
            "score": 133.48074650764465
        },
        {
            "docid": "2860430_29",
            "document": "Neural oscillation . In response to input, a neuron or neuronal ensemble may change the frequency at which it oscillates, thus changing the rate at which it spikes. Often, a neuron's firing rate depends on the summed activity it receives. Frequency changes are also commonly observed in central pattern generators and directly relate to the speed of motor activities, such as step frequency in walking. However, changes in \"relative\" oscillation frequency between different brain areas is not so common because the frequency of oscillatory activity is often related to the time delays between brain areas.",
            "score": 155.4803867340088
        },
        {
            "docid": "32528_17",
            "document": "Visual cortex . The tuning properties of V1 neurons (what the neurons respond to) differ greatly over time. Early in time (40 ms and further) individual V1 neurons have strong tuning to a small set of stimuli. That is, the neuronal responses can discriminate small changes in visual orientations, spatial frequencies and colors. Furthermore, individual V1 neurons in humans and animals with binocular vision have ocular dominance, namely tuning to one of the two eyes. In V1, and primary sensory cortex in general, neurons with similar tuning properties tend to cluster together as cortical columns. David Hubel and Torsten Wiesel proposed the classic ice-cube organization model of cortical columns for two tuning properties: ocular dominance and orientation. However, this model cannot accommodate the color, spatial frequency and many other features to which neurons are tuned . The exact organization of all these cortical columns within V1 remains a hot topic of current research. The mathematical modeling of this function has been compared to Gabor transforms.",
            "score": 143.0456416606903
        },
        {
            "docid": "4548229_2",
            "document": "Interaural time difference . The interaural time difference (or ITD) when concerning humans or animals, is the difference in arrival time of a sound between two ears. It is important in the localization of sounds, as it provides a cue to the direction or angle of the sound source from the head. If a signal arrives at the head from one side, the signal has further to travel to reach the far ear than the near ear. This pathlength difference results in a time difference between the sound's arrivals at the ears, which is detected and aids the process of identifying the direction of sound source.",
            "score": 243.6650993824005
        },
        {
            "docid": "25522368_21",
            "document": "Feature detection (nervous system) . In the auditory system of bats, like in auditory systems of other vertebrates, primary sensory afferent neurons, which receive inputs from hair cells from a restricted region of the organ of Corti in the cochlea, are the simple feature detectors. These structures are sensitive to a restricted range of frequencies and therefore function as tuned filters. Experimentally, Nobuo Suga and his colleagues (1990) noted that various constant frequency (CF) and frequency modulated (FM) harmonics excited different parts of the basilar membrane because of the frequency difference in the call. Auditory nerve fibers take this slightly-processed sensory information to the cochlear nucleus where information either converges or diverges into parallel pathways. In \"Pteronotus parnellii\", a CF-FM bat, these parallel pathways process CF and FM harmonics separately and contain neurons that exhibit amplitude, frequency, and harmonic selectivity. These pathways converge in the medial geniculate body\u2014giving rise to more complex feature detectors that respond to specific combinations of CF and FM signals.",
            "score": 117.03833711147308
        },
        {
            "docid": "55971_25",
            "document": "Inner ear . Neurons within the ear respond to simple tones, and the brain serves to process other increasingly complex sounds. An average adult is typically able to detect sounds ranging between 20 and 20,000\u00a0Hz. The ability to detect higher pitch sounds decreases in older humans.",
            "score": 165.46043586730957
        }
    ],
    "r": [
        {
            "docid": "5051081_4",
            "document": "Eric Knudsen . In 1978, Knudsen and Konishi presented the discovery of an auditory map of space in the midbrain of the barn owl. This discovery was groundbreaking because it unearthed the first non-somatotopic space map in the brain. The map was found in the owl\u2019s midbrain, in the lateral and anterior mesencephalicus lateralis dorsalis (MLD), a structure now referred to as the inferior colliculus. Unlike most sound-localization maps, this map was found to be two-dimensional, with units arranged spatially to represent both the vertical and horizontal location of sound. Knudsen and Konishi discovered that units in this structure respond preferentially to sounds originating in a particular region in space. In the 1978 paper, elevation and azimuth (location in the horizontal plane) were shown to be the two coordinates of the map. Using a speaker set on a rotatable hemispherical track, Knudsen and Konishi presented owls with auditory stimulus from various locations in space and recorded the resulting neuronal activity. They found that neurons in this part of the MLD were organized according to the location of their receptive field, with azimuth varying along the horizontal plane of the space map and elevation varying vertically.  Knudsen followed this discovery with research into specific sound localization mechanisms. Two main auditory cues used by the barn owl to localize sound are interaural time difference (ITD) and interaural intensity difference (IID). The owl\u2019s ears are asymmetric, with the right ear\u2019s opening being directed higher than that of the left. This asymmetry allows the barn owl to determine the elevation of a sound by comparing sound levels between its two ears. Interaural time differences provide the owl with information regarding a sound\u2019s azimuth; sound will reach the ear closer to the sound source before reaching the farther ear, and this time difference can be detected and interpreted as an azimuthal direction. At low frequencies, the wavelength of a sound is wider than the owl's facial ruff, and the ruff does not affect detection of azimuth. At high frequencies, the ruff plays a role in reflecting sound for heightened sensitivity to vertical elevation. Therefore, with wide-band noise, containing both high and low frequencies, the owl could use interaural spectrum difference to obtain information about both azimuth and elevation. In 1979, Knudsen and Konishi showed that the barn owl uses interaural spectrum information in sound localization. They presented owls with both wide-bandwidth noise and pure tones. The birds were able to successfully locate pure tones (since they could still gather information from IID and ITD), but their error rate was much lower when localizing wide-bandwidth noise. This indicates that the birds utilize interaural spectrum differences to improve their accuracy.",
            "score": 319.8892822265625
        },
        {
            "docid": "41087200_8",
            "document": "Perceptual-based 3D sound localization . Interaural level differences (ILD) represents the difference in sound pressure level reaching the two ears. They provide salient cues for localizing high-frequency sounds in space, and populations of neurons that are sensitive to ILD are found at almost every synaptic level from brain stem to cortex. These cells are predominantly excited by stimulation of one ear and predominantly inhibited by stimulation of the other ear, such that the magnitude of their response is determined in large part by the intensities at the 2 ears. This gives rise to the concept of resonant damping. Interaural level difference (ILD) is best for high frequency sounds because low frequency sounds are not attenuated much by the head. ILD (also known as Interaural Intensity Difference) arises when the sound source is not centred, the listener's head partially shadows the ear opposite to the source, diminishing the intensity of the sound in that ear (particularly at higher frequencies). The pinnae filters the sound in a way that is directionally dependent. This is particularly useful in determining if a sound comes from above, below, in front, or behind.",
            "score": 305.7006530761719
        },
        {
            "docid": "5442380_17",
            "document": "Sensory cue . Unless a sound is directly in front of or behind the individual, the sound stimuli will have a slightly different distance to travel to reach each ear. This difference in distance causes a slight delay in the time the signal is perceived by each ear. The magnitude of the interaural time difference is greater the more the signal comes from the side of the head. Thus, this time delay allows humans to accurately predict the location of incoming sound cues. Interaural level difference is caused by the difference in sound pressure level reaching the two ears. This is because the head blocks the sound waves for the further ear, causing less intense sound to reach it. This level difference between the two ears allows humans to accurately predict azimuth of an auditory signal. This effect only occurs at sounds that are high frequency.",
            "score": 270.60791015625
        },
        {
            "docid": "4548229_16",
            "document": "Interaural time difference . The MSO is made up of neurons which receive input from the low-frequency fibers of the left and right AVCN. The result of having input from both cochleas is an increase in the firing rate of the MSO units. The neurons in the MSO are sensitive to the difference in the arrival time of sound at each ear, also known as the interaural time difference (ITD). Research shows that if stimulation arrives at one ear before the other, many of the MSO units will have increased discharge rates. The axons from the MSO continue to higher parts of the pathway via the ipsilateral lateral lemniscus tract.(Yost, 2000)",
            "score": 266.4047546386719
        },
        {
            "docid": "34118956_11",
            "document": "Perception of infrasound . Tests of the ability to localize sounds also showed the significance of low frequency sound perception in elephants. Localization was tested by observing the successful orienting towards the left or the right source loudspeakers when they were positioned at different angles from the elephant\u2019s head. The elephant could localize sounds best at a frequency below 1\u00a0kHz, with perfect identification of the left or right speaker at angles of 20 degrees or more, and chance level discriminations below 2 degrees. Sound localization ability was measured to be best at 125\u00a0Hz and 250\u00a0Hz, intermediate at 500\u00a0Hz, 1\u00a0kHz, and 2\u00a0kHz, and very poor at frequencies at 4\u00a0kHz and above. A possible reason for this is that elephants are very good at using interaural phase differences which are effective for localizing low frequency sounds, but not as good at using interaural intensity differences which are better for higher frequency sounds. Because of the elephant head size and the large distance between their ears, interaural difference cues become confused when wavelengths are shorter, explaining why sound localization was very poor at frequencies above 4\u00a0kHz. It was observed that the elephant spread the pinna of its ears only during the sound localization tasks, however the precise effect of this behavior is unknown.",
            "score": 258.9651794433594
        },
        {
            "docid": "7527647_14",
            "document": "Binaural fusion . Sound localization is the ability to correctly identify the directional location of sounds. A sound stimulus localized in the horizontal plane is called azimuth; in the vertical plane it is referred to as elevation. The time, intensity, and spectral differences in the sound arriving at the two ears are used in localization. Localization of low frequency sounds is accomplished by analyzing interaural time difference (ITD). Localization of high frequency sounds is accomplished by analyzing interaural level difference (ILD).",
            "score": 256.4309387207031
        },
        {
            "docid": "1021754_43",
            "document": "Sound localization . If the ears are located at the side of the head, similar lateral localization cues as for the human auditory system can be used. This means: evaluation of interaural time differences (interaural phase differences) for lower frequencies and evaluation of interaural level differences for higher frequencies. The evaluation of interaural phase differences is useful, as long as it gives unambiguous results. This is the case, as long as ear distance is smaller than half the length (maximal one wavelength) of the sound waves. For animals with a larger head than humans the evaluation range for interaural phase differences is shifted towards lower frequencies, for animals with a smaller head, this range is shifted towards higher frequencies.",
            "score": 255.365234375
        },
        {
            "docid": "1021754_23",
            "document": "Sound localization . For frequencies below 800\u00a0Hz, the dimensions of the head (ear distance 21.5\u00a0cm, corresponding to an interaural time delay of 625 \u00b5s) are smaller than the half wavelength of the sound waves. So the auditory system can determine phase delays between both ears without confusion. Interaural level differences are very low in this frequency range, especially below about 200\u00a0Hz, so a precise evaluation of the input direction is nearly impossible on the basis of level differences alone. As the frequency drops below 80\u00a0Hz it becomes difficult or impossible to use either time difference or level difference to determine a sound's lateral source, because the phase difference between the ears becomes too small for a directional evaluation.",
            "score": 251.10353088378906
        },
        {
            "docid": "1021754_24",
            "document": "Sound localization . For frequencies above 1600\u00a0Hz the dimensions of the head are greater than the length of the sound waves. An unambiguous determination of the input direction based on interaural phase alone is not possible at these frequencies. However, the interaural level differences become larger, and these level differences are evaluated by the auditory system. Also, group delays between the ears can be evaluated, and is more pronounced at higher frequencies; that is, if there is a sound onset, the delay of this onset between the ears can be used to determine the input direction of the corresponding sound source. This mechanism becomes especially important in reverberant environments. After a sound onset there is a short time frame where the direct sound reaches the ears, but not yet the reflected sound. The auditory system uses this short time frame for evaluating the sound source direction, and keeps this detected direction as long as reflections and reverberation prevent an unambiguous direction estimation. The mechanisms described above cannot be used to differentiate between a sound source ahead of the hearer or behind the hearer; therefore additional cues have to be evaluated.",
            "score": 249.9778594970703
        },
        {
            "docid": "41087200_10",
            "document": "Perceptual-based 3D sound localization . Interaural Phase Difference (IPD) refers to the difference in the phase of a wave that reaches each ear, and is dependent on the frequency of the sound wave and the interaural time differences (ITD).",
            "score": 249.725341796875
        },
        {
            "docid": "3154127_4",
            "document": "Virtual acoustic space . When one listens to sounds over headphones (in what is known as the \"closed field\") the sound source appears to arise from center of the head. On the other hand, under normal, so-called free-field, listening conditions sounds are perceived as being externalized. The direction of a sound in space (see sound localization) is determined by the brain when it analyses the interaction of incoming sound with head and external ears. A sound arising to one side reaches the near ear before the far ear (creating an interaural time difference, ITD), and will also be louder at the near ear (creating an interaural level difference, ILD \u2013 also known as interaural intensity difference, IID). These binaural cues allow sounds to be lateralized. Although conventional stereo headphone signals make used of ILDs (not ITDs) the sound is not perceived as being externalized.",
            "score": 246.93380737304688
        },
        {
            "docid": "4548229_7",
            "document": "Interaural time difference . Feddersen et al. (1957) also conducted experiments taking measurements on how ITDs alter with changing the azimuth of the loudspeaker around the head at different frequencies. But unlike the Woodworth experiments human subjects were used rather than a model of the head. The experiment results agreed with the conclusion made by Woodworth about ITDs. The experiments also concluded that is there is no difference in ITDs when sounds are provided from directly in front or behind at 0\u00b0 and 180\u00b0 azimuth. The explanation for this is that the sound is equidistant from both ears. Interaural time differences alter as the loudspeaker is moved around the head. The maximum ITD of 660 \u03bcs occurs when a sound source is positioned at 90\u00b0 azimuth to one ear.",
            "score": 244.49072265625
        },
        {
            "docid": "4548229_2",
            "document": "Interaural time difference . The interaural time difference (or ITD) when concerning humans or animals, is the difference in arrival time of a sound between two ears. It is important in the localization of sounds, as it provides a cue to the direction or angle of the sound source from the head. If a signal arrives at the head from one side, the signal has further to travel to reach the far ear than the near ear. This pathlength difference results in a time difference between the sound's arrivals at the ears, which is detected and aids the process of identifying the direction of sound source.",
            "score": 243.66510009765625
        },
        {
            "docid": "4548229_6",
            "document": "Interaural time difference . Experiments conducted by Woodworth (1938) tested the duplex theory by using a solid sphere to model the shape of the head and measuring the ITDs as a function of azimuth for different frequencies. The model used had a distance between the 2 ears of approximately 22\u201323\u00a0cm. Initial measurements found that there was a maximum time delay of approximately 660 \u03bcs when the sound source was placed at directly 90\u00b0 azimuth to one ear. This time delay correlates to the wavelength of a sound input with a frequency of 1500\u00a0Hz. The results concluded that when a sound played had a frequency less than 1500\u00a0Hz the wavelength is greater than this maximum time delay between the ears. Therefore there is a phase difference between the sound waves entering the ears providing acoustic localisation cues. With a sound input with a frequency closer to 1500\u00a0Hz the wavelength of the sound wave is similar to the natural time delay. Therefore due to the size of the head and the distance between the ears there is a reduced phase difference so localisations errors start to be made. When a high frequency sound input is used with a frequency greater than 1500\u00a0Hz, the wavelength is shorter than the distance between the 2 ears, a head shadow is produced and ILD provide cues for the localisation of this sound.",
            "score": 243.391357421875
        },
        {
            "docid": "39265695_7",
            "document": "Stimulus filtering . Female flies of the genus \"Ormia ochracea\" possess organs in their bodies that can detect frequencies of cricket sounds from meters away. This process is important for the survival of their species because females will lay their first instar larvae into the body of the cricket, where they will feed and molt for approximately seven days. After this period, the larvae grow into flies and the cricket usually perishes. Researchers were puzzled about how precise hearing ability could arise from a small ear structure. Normal animals detect and locate sounds using the interaural time difference (ITD) and the interaural level difference (ILD). The ITD is the difference in the time it takes sound to reach the ear. ILD is the difference in sound intensity measure between both ears. At maximum, the ITD would only reach about 1.5 microseconds and the ILD would be less than one decibel. These small values make it hard to sense the differences. To solve these issues, researchers studied the mechanical aspects of flies\u2019 ears. They found that they have a presternum structure linking both tympanal membranes that is critical in detecting sound and localization. The structure acts as a lever by transferring and amplifying vibrational energy between the membranes. After sound hits the membranes at different amplitudes, the presternum sets up symmetrical vibration modes through bending and rocking. This effect helps the nervous system distinguish which side the sound is coming from. Because the presternum acts as an intertympanal bridge, the ITD is increased from 1.5 us to 55 us and the ILD is increased from less than one decibel to over 10 decibels.",
            "score": 242.03294372558594
        },
        {
            "docid": "14532984_7",
            "document": "Coincidence detection in neurobiology . Coincidence detection has been shown to be a major factor in sound localization along the azimuth plane in several organisms. In 1948, Lloyd A. Jeffress proposed that some organisms may have a collection of neurons that receive auditory input from each ear. The neural pathways to these neurons are called delay lines. Jeffress claimed that the neurons that the delay lines link act as coincidence detectors by firing maximally when receiving simultaneous inputs from both ears. When a sound is heard, sound waves may reach the ears at different times. This is referred to as the interaural time difference (ITD). Due to differing lengths and a finite conduction speed within the axons of the delay lines, different coincidence detector neurons will fire when sound comes from different positions along the azimuth. Jeffress' model proposes that two signals even from an asynchronous arrival of sound in the cochlea of each ear will converge synchronously on a coincidence detector in the auditory cortex based on the magnitude of the ITD (Fig. 2). Therefore, the ITD should correspond to an anatomical map that can be found within the brain. Masakazu Konishi's study on barn owls shows that this is true. Sensory information from the hair cells of the ears travels to the ipsilateral nucleus magnocellularis. From here, the signals project ipsilaterally and contralaterally to two nucleus laminari. Each nucleus laminaris contains coincidence detectors that receive auditory input from the left and the right ear. Since the ipsilateral axons enter the nucleus laminaris dorsally while the contralateral axons enter ventrally, sounds from various positions along the azimuth correspond directly to stimulation of different depths of the nucleus laminaris. From this information, a neural map of auditory space was formed. The function of the nucleus laminaris parallels that of the medial superior olive in mammals.",
            "score": 239.7406463623047
        },
        {
            "docid": "32105732_4",
            "document": "Spatial hearing loss . Sound streams arriving from the left or right (the horizontal plane) are localised primarily by the small time differences of the same sound arriving at the two ears. A sound straight in front of the head is heard at the same time by both ears. A sound to the side of the head is heard approximately 0.0005 seconds later by the ear furthest away. A sound halfway to one side is heard approximately 0.0003 seconds later. This is the interaural time difference (ITD) cue and is measured by signal processing in the two central auditory pathways that begin after the cochlea and pass through the brainstem and mid-brain. Some of those with spatial hearing loss are unable to process ITD (low frequency) cues.",
            "score": 239.00962829589844
        },
        {
            "docid": "4548229_11",
            "document": "Interaural time difference . Lastly, they went on to further explore the driving forces behind the interaural time difference, specifically whether the process is simply the alignment of inputs that is processed by a coincidence detector, or whether the process is more complicated. Evidence from Franken, et. al. shows that the processing is affected by inputs that precede the binaural signal, which would alter the functioning of voltage-gated sodium and potassium channels to shift the membrane potential of the neuron. Furthermore, the shift is dependent on the frequency tuning of each neuron, ultimately creating a more complex confluence and analysis of sound. Franken, et. al.'s findings provide several pieces of evidence that contradict existing theories about binaural audition.",
            "score": 236.13673400878906
        },
        {
            "docid": "1379269_3",
            "document": "Musical acoustics . Whenever two different pitches are played at the same time, their sound waves interact with each other \u2013 the highs and lows in the air pressure reinforce each other to produce a different sound wave. Any repeating sound wave that is not a sine wave can be modeled by many different sine waves of the appropriate frequencies and amplitudes (a frequency spectrum). In humans the hearing apparatus (composed of the ears and brain) can usually isolate these tones and hear them distinctly. When two or more tones are played at once, a variation of air pressure at the ear \"contains\" the pitches of each, and the ear and/or brain isolate and decode them into distinct tones.",
            "score": 235.2235565185547
        },
        {
            "docid": "25345530_25",
            "document": "Models of neural computation . According to Jeffress, in order to compute the location of a sound source in space from interaural time differences, an auditory system relies on delay lines: the induced signal from an ipsilateral auditory receptor to a particular neuron is delayed for the same time as it takes for the original sound to go in space from that ear to the other. Each postsynaptic cell is differently delayed and thus specific for a particular inter-aural time difference. This theory is equivalent to the mathematical procedure of cross-correlation.",
            "score": 227.8824462890625
        },
        {
            "docid": "47338295_3",
            "document": "Sound localization in owls . Owls must be able to determine the necessary angle of descent, i.e. the elevation, in addition to azimuth (horizontal angle to the sound). This bi-coordinate sound localization is accomplished through two binaural cues: the interaural time difference (ITD) and the interaural level difference (ILD), also known as the interaural intensity difference (IID). The ability in owls is unusual; in ground-bound mammals such as mice, ITD and ILD are not utilized in the same manner. In these mammals, ITDs tend to be utilized for localization of lower frequency sounds, while ILDs tend to be used for higher frequency sounds.",
            "score": 225.17404174804688
        },
        {
            "docid": "4548229_4",
            "document": "Interaural time difference . The Duplex theory proposed by Lord Rayleigh (1907) provides an explanation for the ability of humans to localise sounds by time differences between the sounds reaching each ear (ITDs) and differences in sound level entering the ears (interaural level differences, ILDs). But there still lies a question whether ITD or ILD is prominent.",
            "score": 224.71580505371094
        },
        {
            "docid": "8436042_6",
            "document": "MPEG Surround . MPEG Surround coding uses our capacity to perceive sound in the 3D and captures that perception in a compact set of parameters. Spatial perception is primarily attributed to three parameters, or cues, describing how humans localize sound in the horizontal plane: Interaural level difference (ILD), Interaural time difference (ITD) and Interaural coherence (IC). This three concepts are illustrated in next image. Direct, or first-arrival, waveforms from the source hit the left ear at time, while direct sound received by the right ear is diffracted around the head, with time delay and level attenuation, associated. These two effects result in ITD and ILD are associated with the main source. At last, in a reverberant environment, reflected sound from the source, or sound from diffuse source, or uncorrelated sound can hit both ears, all of them are related with IC.",
            "score": 224.51803588867188
        },
        {
            "docid": "4361430_16",
            "document": "Acoustic location . A well-known example of TDOA is the interaural time difference. The interaural time difference is the difference in arrival time of a sound between two ears. The interaural time difference is given by where",
            "score": 221.71542358398438
        },
        {
            "docid": "41087200_9",
            "document": "Perceptual-based 3D sound localization . Interaural time and level differences (ITD, ILD) play a role in azimuth perception but can\u2019t explain vertical localization. According to the duplex theory, ITDs have a greater contribution to the localisation of low frequency sounds (below 1\u00a0kHz),while ILDs are used in the localisation of high frequency sound. The ILD arises from the fact that,a sound coming from a source located to one side of the head will have a higher intensity, or be louder, at the ear nearest the sound source. One can therefore create the illusion of a sound source emanating from one side of the head merely by adjusting the relative level of the sounds that are fed to two separated speakers or headphones. This is the basis of the commonly used pan control.",
            "score": 221.15655517578125
        },
        {
            "docid": "8953842_8",
            "document": "Computational auditory scene analysis . Because the ears receive audio signals at different times, the sound source can be determined by using the delays retrieved from the two ears. By cross-correlating the delays from the left and right channels (of the model), the coincided peaks can be categorized as the same localized sound, despite their temporal location in the input signal.  The use of interaural cross-correlation mechanism has been supported through physiological studies, paralleling the arrangement of neurons in the auditory midbrain.",
            "score": 220.86363220214844
        },
        {
            "docid": "4548229_5",
            "document": "Interaural time difference . The duplex theory states that ITDs are used to localise low frequency sounds, in particular, while ILDs are used in the localisation of high frequency sound inputs. However, the frequency ranges for which the auditory system can use ITDs and ILDs significantly overlap, and most natural sounds will have both high and low frequency components, so that the auditory system will in most cases have to combine information from both ITDs and ILDs to judge the location of a sound source.  A consequence of this duplex system is that it is also possible to generate so-called \"cue trading\" or \"time\u2013intensity trading\" stimuli on headphones, where ITDs pointing to the left are offset by ILDs pointing to the right, so the sound is perceived as coming from the midline. A limitation of the duplex theory is that the theory does not completely explain directional hearing, as no explanation is given for the ability to distinguish between a sound source directly in front and behind. Also the theory only relates to localising sounds in the horizontal plane around the head. The theory also does not take into account the use of the pinna in localisation.(Gelfand, 2004)",
            "score": 220.1082305908203
        },
        {
            "docid": "233830_28",
            "document": "Ambisonics . At low frequencies, where the wavelength is large compared to the human head, an incoming sound diffracts around it, so that there is virtually no acoustic shadow and hence no level difference between the ears. In this range, the only available information is the phase relationship between the two ear signals, called \"interaural time difference\", or \"ITD\". Evaluating this time difference allows for precise localisation within a \"cone of confusion\": the angle of incidence is unambiguous, but the ITD is the same for sounds from the front or from the back. As long as the sound is not totally unknown to the subject, the confusion can usually be resolved by perceiving the timbral front-back variations caused by the ear flaps (or \"pinnae\").",
            "score": 218.84886169433594
        },
        {
            "docid": "1021754_9",
            "document": "Sound localization . Most mammals are adept at resolving the location of a sound source using interaural time differences and interaural level differences. However, no such time or level differences exist for sounds originating along the circumference of circular conical slices, where the cone's axis lies along the line between the two ears.",
            "score": 217.7413330078125
        },
        {
            "docid": "4548229_17",
            "document": "Interaural time difference . The lateral lemniscus (LL) is the main auditory tract in the brainstem connecting SOC to the inferior colliculus. The dorsal nucleus of the lateral lemniscus (DNLL) is a group of neurons separated by lemniscus fibres, these fibres are predominantly destined for the inferior colliculus (IC). In studies using an unanesthetized rabbit the DNLL was shown to alter the sensitivity of the IC neurons and may alter the coding of interaural timing differences (ITDs) in the IC.(Kuwada et al., 2005) The ventral nucleus of the lateral lemniscus (VNLL) is a chief source of input to the inferior colliculus. Research using rabbits shows the discharge patterns, frequency tuning and dynamic ranges of VNLL neurons supply the inferior colliculus with a variety of inputs, each enabling a different function in the analysis of sound.(Batra & Fitzpatrick, 2001)  In the inferior colliculus (IC) all the major ascending pathways from the olivary complex and the central nucleus converge. The IC is situated in the midbrain and consists of a group of nuclei the largest of these is the central nucleus of inferior colliculus (CNIC). The greater part of the ascending axons forming the lateral lemniscus will terminate in the ipsilateral CNIC however a few follow the commissure of Probst and terminate on the contralateral CNIC. The axons of most of the CNIC cells form the brachium of IC and leave the brainstem to travel to the ipsilateral thalamus. Cells in different parts of the IC tend to be monaural, responding to input from one ear, or binaural and therefore respond to bilateral stimulation.",
            "score": 217.57643127441406
        },
        {
            "docid": "12647848_7",
            "document": "Calyx of Held . Interaural level detection is possible through the calyx system due to the large relative size of the GBCs, the calyx of Held, and the principal neurons. The neurons in the Lateral Superior Olive are especially important in discerning these interaural level changes. The large diameter size of the bushy cell axons allows the inhibitory signal produced by the MNTB neurons to reach the SOC approximately 0.2 ms following the initial cochlear excitation. This ~0.2 second time measurement is important for comparing the contralateral (opposite side) and ipsilateral (same side) stimulation necessary in sound localization in the horizontal plane, and is key in distinguishing the location of low frequency sounds.",
            "score": 214.9549560546875
        },
        {
            "docid": "1021754_42",
            "document": "Sound localization . Since most animals have two ears, many of the effects of the human auditory system can also be found in other animals. Therefore, interaural time differences (interaural phase differences) and interaural level differences play a role for the hearing of many animals. But the influences on localization of these effects are dependent on head sizes, ear distances, the ear positions and the orientation of the ears.",
            "score": 213.1141815185547
        }
    ]
}