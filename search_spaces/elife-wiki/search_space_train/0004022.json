{
    "q": [
        {
            "docid": "668442_45",
            "document": "Lip sync . FPS is a genre that generally places much more emphasis on graphical display, mainly due to the camera almost always being very close to character models. Due to increasingly detailed character models requiring animation, FPS developers assign many resources to create realistic lip synchronization with the many lines of speech used in most FPS games. Early 3D models used basic up-and-down jaw movements to simulate speech. As technology progressed, mouth movements began to closely resemble real human speech movements. \"\" dedicated a development team to lip sync alone, producing the most accurate lip synchronization for games at that time. Since then, games like \"\" and \"Half-Life 2\" have made use of coding that dynamically simulates mouth movements to produce sounds as if they were spoken by a live person, resulting in astoundingly lifelike characters. Gamers who create their own videos using character models with no lip movements, such as the helmeted Master Chief from \"Halo\", improvise lip movements by moving the characters' arms, bodies and making a bobbing movement with the head (see \"Red vs. Blue\").",
            "score": 152.36479496955872
        },
        {
            "docid": "668442_44",
            "document": "Lip sync . Unlike RPGs, strategy video games make extensive use of sound files to create an immersive battle environment. Most games simply played a recorded audio track on cue with some games providing inanimate portraits to accompany the respective voice. \"StarCraft\" used full motion video character portraits with several generic speaking animations that did not synchronize with the lines spoken in the game. The game did, however, make extensive use of recorded speech to convey the game's plot, with the speaking animations providing a good idea of the flow of the conversation. \"Warcraft III\" used fully rendered 3D models to animate speech with generic mouth movements, both as character portraits as well as the in-game units. Like the FMV portraits, the 3D models did not synchronize with actual spoken text, while in-game models tended to simulate speech by moving their heads and arms rather than using actual lip synchronization. Similarly, the game Codename Panzers uses camera angles and hand movements to simulate speech, as the characters have no actual mouth movement. However, \"StarCraft II\" used fully synced unit portraits and cinematic sequences.",
            "score": 141.33675289154053
        },
        {
            "docid": "668442_3",
            "document": "Lip sync . In film production, lip-synching is often part of the postproduction phase. Dubbing foreign-language films and making animated characters appear to speak both require elaborate lip-synching. Many video games make extensive use of lip-synced sound files to create an immersive environment in which on-screen characters appear to be speaking. In the music industry, lip-synching is used by singers for music videos, television and film appearances and some types of live performances. Lip-syncing by singers can be controversial to fans attending concert performances who expect to view a live performance. Lip sync is also referred to as \"lip-sync\" or \"lip-synch\". The proper spelling is sync (when used alone), and synchronize, synchronizing and synchronization. The term \"sync\" or \"synch\" is pronounced the same as \"siNGk\". (Listen).",
            "score": 90.87259483337402
        },
        {
            "docid": "668442_2",
            "document": "Lip sync . Lip sync (short for lip synchronization) is a technical term for matching a speaking or singing person's lip movements with prerecorded sung or spoken vocals that listeners hear, either through the sound reinforcement system in a live performance or via television, computer, cinema speakers, or generally anything with audio output in other cases. The term can refer to any of a number of different techniques and processes, in the context of live performances and audiovisual recordings.",
            "score": 98.0450029373169
        },
        {
            "docid": "28738_11",
            "document": "Synchronization . In groups, synchronization of movement has been shown to increase conformity, cooperation and trust however more research on group synchronization is needed to determine its effects on the group as a whole and on individuals within a group. In dyads, groups of two people, synchronization has been demonstrated to increase affiliation, self-esteem, compassion and altruistic behaviour and increase rapport. During arguments, synchrony between the arguing pair has been noted to decrease, however it is not clear whether this is due to the change in emotion or other factors. There is evidence to show that movement synchronization requires other people to cause its beneficial effects, as the effect on affiliation does not occur when one of the dyad is synchronizing their movements to something outside the dyad. This is known as interpersonal synchrony.",
            "score": 74.67045402526855
        },
        {
            "docid": "2860430_39",
            "document": "Neural oscillation . The functional role of synchronized oscillatory activity in the brain was mainly established in experiments performed on awake kittens with multiple electrodes implanted in the visual cortex. These experiments showed that groups of spatially segregated neurons engage in synchronous oscillatory activity when activated by visual stimuli. The frequency of these oscillations was in the range of 40\u00a0Hz and differed from the periodic activation induced by the grating, suggesting that the oscillations and their synchronization were due to internal neuronal interactions. Similar findings were shown in parallel by the group of Eckhorn, providing further evidence for the functional role of neural synchronization in feature binding. Since then, numerous studies have replicated these findings and extended them to different modalities such as EEG, providing extensive evidence of the functional role of gamma oscillations in visual perception.",
            "score": 95.61534285545349
        },
        {
            "docid": "6202324_17",
            "document": "Haskins Laboratories . Capabilities in the eye movement labs is expanded to include 3 eye trackers, including one with the ability to capture synchronous gaze and EEG data, and another able to capture synchronous gaze and speech signals.",
            "score": 99.616943359375
        },
        {
            "docid": "26603942_3",
            "document": "Silent speech interface . Silent speech interface systems have been created using ultrasound and optical camera input of tongue and lip movements. Electromagnetic devices are another technique for tracking tongue and lip movements. The detection of speech movements by electromyography of speech articulator muscles and the larynx is another technique. Another source of information is the vocal tract resonance signals that get transmitted through bone conduction called non-audible murmurs.  They have also been created as a brain\u2013computer interface using brain activity in the motor cortex obtained from intracortical microelectrodes.",
            "score": 174.60100865364075
        },
        {
            "docid": "9695744_2",
            "document": "Audio synchronizer . An audio synchronizer is a variable audio delay used to correct or maintain audio-video sync or timing also known as lip sync error. See for example the specification for audio to video timing given in ATSC Document IS-191. Modern television systems use large amounts of video signal processing such as MPEG preprocessing, encoding and decoding, video synchronization and resolution conversion in pixelated displays. This video processing can cause delays in the video signal ranging from a few microseconds to tens of seconds. If the television program is displayed to the viewer with this video delay the audio-video synchronization is wrong, and the video will appear to the viewer after the sound is heard. This effect is commonly referred to as A/V sync or lip sync error and can cause serious problems related to the viewer's enjoyment of the program.",
            "score": 78.42718148231506
        },
        {
            "docid": "38734322_6",
            "document": "Music and Aerobic Exercise Performance . Synchronous music is described as the synchronization between tempo and human movement in terms responding to the rhythmical qualities of music. Motivational music is described as music with strong rhythms and are fast tempo (>120 bpm). Type of music has the ability to change arousal levels and may be used as a stimulant or sedative. Music may narrow bodies awareness of fatigue. Synchronous and Asynchronous music has been shown to have significant ergogenic effects on non-professional athletes. In a study by Karageorghis et al. (2009) the effects of motivational synchronous music, oudeterous synchronous music and a no-music control condition was examined for four dependent measures which included time to exhaustion, ratings of perceived exertion (RPE), in-task affect, and exercise-induced feelings. 100 sports science undergraduates participated in the study and went through a thorough music selection procedure where they were able to choose the type of music they wanted and testers were able to create a standardized motivational music track list which would be able to create the desired affects. The subjects then walked briskly on a treadmill at an appropriate velocity that would ensure an exercise intensity of 75% maximal heart rate reserve. Results showed both music conditions had a significant effect on time to exhaustion and in-task affect. However, there were no significant differences in exercise induced feelings or RPE.",
            "score": 77.82873773574829
        },
        {
            "docid": "315084_7",
            "document": "Lip reading . Many factors affect the visibility of a speaking face, including illumination, movement of the head/camera, frame-rate of the moving image and distance from the viewer (see e.g.). Head movement that accompanies normal speech can also improve lip-reading, independently of oral actions. However, when lip-reading connected speech, the viewer's knowledge of the spoken language, familiarity with the speaker and style of speech, and the context of the lip-read material are as important as the visibility of the speaker. While most hearing people are sensitive to seen speech, there is great variability in individual speechreading skill. Good lipreaders are often more accurate than poor lipreaders at identifying phonemes from visual speech.",
            "score": 152.66412687301636
        },
        {
            "docid": "315084_3",
            "document": "Lip reading . Although speech perception is considered to be an auditory skill, it is intrinsically multimodal, since producing speech requires the speaker to make movements of the lips, teeth and tongue which are often visible in face-to-face communication. Information from the lips and face supports aural comprehension and most fluent listeners of a language are sensitive to seen speech actions (see McGurk effect). The extent to which people make use of seen speech actions varies with the visibility of the speech action and the knowledge and skill of the perceiver.",
            "score": 149.26032519340515
        },
        {
            "docid": "2299454_10",
            "document": "Lorentz ether theory . In 1900 Poincar\u00e9 interpreted local time as the result of a synchronization procedure based on light signals. He assumed that 2 observers \"A\" and \"B\" which are moving in the ether, synchronize their clocks by optical signals. Since they believe to be at rest they must consider only the transmission time of the signals and then crossing their observations to examine whether their clocks are synchronous. However, from the point of view of an observer at rest in the ether the clocks are not synchronous and indicate the local time formula_4. But because the moving observers don't know anything about their movement, they don't recognize this. In 1904 he illustrated the same procedure in the following way: \"A\" sends a signal at the time 0 to \"B\", which arrives at the time \"t\". B also sends a signal at the time 0 to \"A\", which arrives at the time \"t\". If in both cases \"t\" has the same value the clocks are synchronous, but only in the system in which the clocks are at rest in the aether. So according to Darrigol Poincar\u00e9 understood local time as a physical effect just like length contraction \u2013 in contrast to Lorentz, who used the same interpretation not before 1906. However, contrary to Einstein, who later used a similar synchronization procedure which was called Einstein synchronisation, Darrigol says that Poincar\u00e9 had the opinion that clocks resting in the aether are showing the true time.",
            "score": 67.53339886665344
        },
        {
            "docid": "29806193_9",
            "document": "Motor unit plasticity . Neural synchronization is the simultaneous firing of motoneurons. Synchronization leads to more efficient motor unit recruitment. Neural synchronization is important for muscle performance because the more motoneurons that are activated at once, the more muscle fibers that contract at once and hence the stronger the total force of contraction. Synchronization is known to slightly increase as a result of resistance training. Motor unit recruitment is frequently associated with synchronization and is defined as the order and number of neurons that are needed to perform a movement. Recruitment is not known to change in response to training or age.",
            "score": 83.70469927787781
        },
        {
            "docid": "9696580_2",
            "document": "Audio-to-video synchronization . Audio-to-video synchronization (also known as lip sync, or by the lack of it: lip sync error, lip flap) refers to the relative timing of audio (sound) and video (image) parts during creation, post-production (mixing), transmission, reception and play-back processing. AV synchronization can be an issue in television, videoconferencing, or film.",
            "score": 84.4702696800232
        },
        {
            "docid": "1507834_3",
            "document": "Fractured Flickers . Host Hans Conried introduced short \"flickers\" pieced together from silent film footage and from other older movies, overdubbed with newly written comic dialogue, music, and sound effects. The voices for these were provided by fellow Ward mainstays Paul Frees, June Foray, and Bill Scott. The earliest episodes have careful dubbing, with the actors and writers taking pains to synchronize the new dialogue with the actors' lip movements. Once the series had deadlines to face, however, the time-consuming dubbing was abandoned, and the later episodes do not bother with exact synchronization.",
            "score": 74.92233610153198
        },
        {
            "docid": "16052460_31",
            "document": "Social information processing (theory) . Burgoon, Chen, and Twitchell (2010) also conducted an experiment to test how synchronicity affects online interactions. They had their participants conduct team-oriented tasks, and used different methods of communication to observe how people perceived their fellow team members. They proposed that synchronicity affects interactivity, and the results of the experiment supported their hypothesis. They observed that synchronous forms of communications allow for increased mental and behavioral engagement between parties, allowing participants to feel a stronger sense of connection, presence, identification, and social awareness in the conversation.",
            "score": 80.35758519172668
        },
        {
            "docid": "28738_10",
            "document": "Synchronization . Synchronization of movement is defined as similar movements between two or more people who are temporally aligned. This is different to mimicry, as these movements occur after a short delay. Muscular bonding is the idea that moving in time evokes particular emotions. This sparked some of the first research into movement synchronization and its effects on human emotion.",
            "score": 78.48339438438416
        },
        {
            "docid": "42799_42",
            "document": "Speech synthesis . A study in the journal \"Speech Communication\" by Amy Drahota and colleagues at the University of Portsmouth, UK, reported that listeners to voice recordings could determine, at better than chance levels, whether or not the speaker was smiling. It was suggested that identification of the vocal features that signal emotional content may be used to help make synthesized speech sound more natural. One of the related issues is modification of the pitch contour of the sentence, depending upon whether it is an affirmative, interrogative or exclamatory sentence. One of the techniques for pitch modification uses discrete cosine transform in the source domain (linear prediction residual). Such pitch synchronous pitch modification techniques need a priori pitch marking of the synthesis speech database using techniques such as epoch extraction using dynamic plosion index applied on the integrated linear prediction residual of the voiced regions of speech.",
            "score": 101.8755156993866
        },
        {
            "docid": "3526311_9",
            "document": "Atlantis: The Lost Tales . The game features a number of proprietary technologies developed by Cryo. One such technology is called OMNI 3D which provides a smooth, panoramic 360-degree first-person view of the game environment. Unfortunately, this view tends to be less crisp looking than the movement clips that are pre-rendered in the game. All the character animations are motion captured and feature another technology called OMNI SYNC to ensure proper lip synchronization with audio speech.",
            "score": 95.43460059165955
        },
        {
            "docid": "2860430_13",
            "document": "Neural oscillation . A group of neurons can also generate oscillatory activity. Through synaptic interactions the firing patterns of different neurons may become synchronized and the rhythmic changes in electric potential caused by their action potentials will add up (constructive interference). That is, synchronized firing patterns result in synchronized input into other cortical areas, which gives rise to large-amplitude oscillations of the local field potential. These large-scale oscillations can also be measured outside the scalp using electroencephalography (EEG) and magnetoencephalography (MEG). The electric potentials generated by single neurons are far too small to be picked up outside the scalp, and EEG or MEG activity always reflects the summation of the synchronous activity of thousands or millions of neurons that have similar spatial orientation. Neurons in a neural ensemble rarely all fire at exactly the same moment, i.e. fully synchronized. Instead, the probability of firing is rhythmically modulated such that neurons are more likely to fire at the same time, which gives rise to oscillations in their mean activity (see figure at top of page). As such, the frequency of large-scale oscillations does not need to match the firing pattern of individual neurons. Isolated cortical neurons fire regularly under certain conditions, but in the intact brain cortical cells are bombarded by highly fluctuating synaptic inputs and typically fire seemingly at random. However, if the probability of a large group of neurons is rhythmically modulated at a common frequency, they will generate oscillations in the mean field (see also figure at top of page). Neural ensembles can generate oscillatory activity endogenously through local interactions between excitatory and inhibitory neurons. In particular, inhibitory interneurons play an important role in producing neural ensemble synchrony by generating a narrow window for effective excitation and rhythmically modulating the firing rate of excitatory neurons.",
            "score": 81.27093052864075
        },
        {
            "docid": "2535324_14",
            "document": "Brainwave entrainment . Even among groups of strangers, the rate of breathing, locomotive and subtle expressive motor movements, and rhythmic speech patterns have been observed to synchronize and entrain, in response to an auditory stimuli, such as a piece of music with a consistent rhythm. Furthermore, motor synchronization to repetitive tactile stimuli occurs in animals, including cats and monkeys as well as humans, with accompanying shifts in electroencephalogram (EEG) readings.",
            "score": 118.00672745704651
        },
        {
            "docid": "52069152_9",
            "document": "The Jazz Singer (play) . Released on October 6, 1927, and also titled \"The Jazz Singer\", the adaptation was the first feature-length motion picture with not only a synchronized recorded music score, but also lip-synchronous singing and speech in several isolated sequences. Its release heralded the commercial ascendance of the \"talkies\" and the decline of the silent film era. Made for $422,000, the movie grossed $3.9 million at the domestic box office. Its release also had the effect of ending the touring production of the play, which could not compete with the lower-priced movie.",
            "score": 91.93534731864929
        },
        {
            "docid": "4220231_8",
            "document": "Evolutionary musicology . The evolutionary switch to bipedalism may have influenced the origins of music. The background is that noise of locomotion and ventilation may mask critical auditory information. Human locomotion is likely to produce more predictable sounds than those of non-human primates. Predictable locomotion sounds may have improved our capacity of entrainment to external rhythms and to feel the beat in music. A sense of rhythm could aid the brain in distinguishing among sounds arising from discrete sources and also help individuals to synchronize their movements with one another. Synchronization of group movement may improve perception by providing periods of relative silence and by facilitating auditory processing. The adaptive value of such skills to early human ancestors may have been keener detection of prey or stalkers and enhanced communication. Thus, bipedal walking may have influenced the development of entrainment in humans and thereby the evolution of rhythmic abilities. Primitive hominids lived and moved around in small groups. The noise generated by the locomotion of two or more individuals can result in a complicated mix of footsteps, breathing, movements against vegetation, echoes, etc. The ability to perceive differences in pitch, rhythm, and harmonies, i.e. \u201cmusicality,\u201d could help the brain to distinguish among sounds arising from discrete sources, and also help the individual to synchronize movements with the group. Endurance and an interest in listening might, for the same reasons, have been associated with survival advantages eventually resulting in adaptive selection for rhythmic and musical abilities and reinforcement of such abilities. Listening to music seems to stimulate release of dopamine. Rhythmic group locomotion combined with attentive listening in nature may have resulted in reinforcement through dopamine release. A primarily survival-based behavior may eventually have attained similarities to dance and music, due to such reinforcement mechanisms . Since music may facilitate social cohesion, improve group effort, reduce conflict, facilitate perceptual and motor skill development, and improve trans-generational communication, music-like behavior may at some stage have become incorporated into human culture.",
            "score": 116.13329029083252
        },
        {
            "docid": "2860430_32",
            "document": "Neural oscillation . Next to evoked activity, neural activity related to stimulus processing may result in induced activity. Induced activity refers to modulation in ongoing brain activity induced by processing of stimuli or movement preparation. Hence, they reflect an indirect response in contrast to evoked responses. A well-studied type of induced activity is amplitude change in oscillatory activity. For instance, gamma activity often increases during increased mental activity such as during object representation. Because induced responses may have different phases across measurements and therefore would cancel out during averaging, they can only be obtained using time-frequency analysis. Induced activity generally reflects the activity of numerous neurons: amplitude changes in oscillatory activity are thought to arise from the synchronization of neural activity, for instance by synchronization of spike timing or membrane potential fluctuations of individual neurons. Increases in oscillatory activity are therefore often referred to as event-related synchronization, while decreases are referred to as event-related desynchronization.",
            "score": 76.57099914550781
        },
        {
            "docid": "47511015_2",
            "document": "Large scale brain networks . Large scale brain networks are collections of widespread brain regions showing functional connectivity by statistical analysis of the fMRI BOLD signal or other signal fluctuations. An emerging paradigm in neuroscience is that cognitive tasks are performed not by individual brain regions working in isolation, but by networks consisting of several discrete brain regions that are said to be \"functionally connected\" due to tightly coupled activity. Functional connectivity may be measured as long-range synchronization of the EEG, MEG, or other dynamic brain signals. Synchronized brain regions may also be identified using spatial independent component analysis. The set of identified brain areas that are linked together in a large-scale network varies with cognitive function. When the cognitive state is not explicit (i.e., the subject is at \"rest\"), the large scale brain network is a resting state network (RSN). As a physical system with graph-like properties, a large scale brain network has both nodes and edges, and cannot be identified simply by the co-activation of brain areas. In recent decades, the analysis of brain networks was made feasible by advances in imaging techniques as well as new tools from graph theory and dynamical systems. Large scale brain networks are identified by their function, and provide a coherent framework for understanding cognition by offering a neural model of how different cognitive functions emerge when different sets of brain regions join together as self-organized coalitions. Disruptions in activity in various networks have been implicated in neuropsychiatric disorders such as depression, Alzheimer's, autism spectrum disorder, schizophrenia and bipolar disorder.",
            "score": 100.81524872779846
        },
        {
            "docid": "68145_2",
            "document": "The Jazz Singer . The Jazz Singer is a 1927 American musical film. As the first feature-length motion picture with not only a synchronized recorded music score, but also lip-synchronous singing and speech in several isolated sequences, its release heralded the commercial ascendance of sound films and ended the silent film era. Directed by Alan Crosland and produced by Warner Bros. with its Vitaphone sound-on-disc system, the film, featuring six songs performed by Al Jolson, is based on a play of the same name by Samson Raphaelson, adapted from one of his short stories, \"The Day of Atonement\".",
            "score": 100.69725275039673
        },
        {
            "docid": "5012810_5",
            "document": "Synchronous learning . Synchronous communication in distance education began long before the advent of the use of computers in synchronous learning. After the very early days of distance education, when students and instructors communicated asynchronously via the post office, synchronous forms of communication in distance education emerged with broadcast radio and television. However, it was not until the 1980s, with video-conferencing and interactive television, that students could ask questions and discuss concepts while seeing participants in a synchronous setting. Manifestations of interactive multimedia, the Internet, access to Web-based resources, to synchronous and asynchronous forms of computer mediated communication followed in the 1990s (Bernard, et al., 2005; Simonson, et al., 2012, p.\u00a037).",
            "score": 68.4849591255188
        },
        {
            "docid": "1408530_25",
            "document": "Circadian clock . A key feature of clocks is their ability to synchronize to external stimuli. The presence of cell autonomous oscillators in almost every cell in the body raises the question of how these oscillators are temporally coordinated. The quest for universal timing cues for peripheral clocks in mammals has yielded principal entrainment signals such as feeding, temperature, and oxygen. Both feeding rhythms and temperature cycles were shown to synchronize peripheral clocks and even uncouple them from the master clock in the brain (e.g., daytime restricted feeding). Recently, oxygen rhythms were found to synchronize clocks in cultured cells.",
            "score": 92.50511312484741
        },
        {
            "docid": "5366050_42",
            "document": "Speech perception . Research into the relationship between music and cognition is an emerging field related to the study of speech perception. Originally it was theorized that the neural signals for music were processed in a specialized \"module\" in the right hemisphere of the brain. Conversely, the neural signals for language were to be processed by a similar \"module\" in the left hemisphere. However, utilizing technologies such as fMRI machines, research has shown that two regions of the brain traditionally considered exclusively to process speech, Broca's and Wernicke's areas, also become active during musical activities such as listening to a sequence of musical chords. Other studies, such as one performed by Marques et al. in 2006 showed that 8-year-olds who were given six months of musical training showed an increase in both their pitch detection performance and their electrophysiological measures when made to listen to an unknown foreign language.",
            "score": 126.5484230518341
        },
        {
            "docid": "2860430_34",
            "document": "Neural oscillation . Neural synchronization can be modulated by task constraints, such as attention, and is thought to play a role in feature binding, neuronal communication, and motor coordination. Neuronal oscillations became a hot topic in neuroscience in the 1990s when the studies of the visual system of the brain by Gray, Singer and others appeared to support the neural binding hypothesis. According to this idea, synchronous oscillations in neuronal ensembles bind neurons representing different features of an object. For example, when a person looks at a tree, visual cortex neurons representing the tree trunk and those representing the branches of the same tree would oscillate in synchrony to form a single representation of the tree. This phenomenon is best seen in local field potentials which reflect the synchronous activity of local groups of neurons, but has also been shown in EEG and MEG recordings providing increasing evidence for a close relation between synchronous oscillatory activity and a variety of cognitive functions such as perceptual grouping.",
            "score": 99.71191740036011
        },
        {
            "docid": "56439577_25",
            "document": "Temporal envelope and fine structure . A computational model of the peripheral auditory system may be used to simulate auditory-nerve fiber responses to complex sounds such as speech, and quantify the transmission (i.e., internal representation) of ENV and TFS cues. In two simulation studies, the mean-rate and spike-timing information was quantified at the output of such a model to characterize, respectively, the short-term rate of neural firing (ENV) and the level of synchronization due to phase locking (TFS) in response to speech sounds degraded by vocoders. The best model predictions of vocoded-speech intelligibility were found when both ENV and TFS cues were included, providing evidence that TFS cues are important for intelligibility when the speech ENV cues are degraded.",
            "score": 111.6076033115387
        }
    ],
    "r": [
        {
            "docid": "26603942_3",
            "document": "Silent speech interface . Silent speech interface systems have been created using ultrasound and optical camera input of tongue and lip movements. Electromagnetic devices are another technique for tracking tongue and lip movements. The detection of speech movements by electromyography of speech articulator muscles and the larynx is another technique. Another source of information is the vocal tract resonance signals that get transmitted through bone conduction called non-audible murmurs.  They have also been created as a brain\u2013computer interface using brain activity in the motor cortex obtained from intracortical microelectrodes.",
            "score": 174.60101318359375
        },
        {
            "docid": "40621603_5",
            "document": "Linguistic intelligence . Speech production is process by which a thought in the brain is converted into an understandable auditory form. This is a multistage mechanism that involves many different areas of the brain. The first stage is planning, where the brain constructs words and sentences that turn the thought into an understandable form. This occurs primarily in the inferior frontal cortex, specifically in an area known as Broca's area. Next, the brain must plan how to physically create the sounds necessary for speech by linking the planned speech with known sounds, or phonemes. While the location of these associations is not known, it is known that the supplementary motor area plays a key role in this step. Finally, the brain must signal for the words to actually be spoken. This is carried out by the premotor cortex and the motor cortex. In most cases, speech production is controlled by the left hemisphere. In a series of studies, Wilder Penfield, among others, probed the brains of both right-handed (generally left-hemisphere dominant) and left-handed (generally right-hemisphere dominant) patients. They discovered that, regardless of handedness, the left hemisphere was almost always the speech controlling side. However, it has been discovered that in cases of neural stress (hemorrhage, stroke, etc.) the right hemisphere has the ability to take control of speech functions.",
            "score": 172.47216796875
        },
        {
            "docid": "2088_20",
            "document": "Aphasia . There have been many instances showing that there is a form of aphasia among deaf individuals. Sign languages are, after all, forms of language that have been shown to use the same areas of the brain as verbal forms of language. Mirror neurons become activated when an animal is acting in a particular way or watching another individual act in the same manner. These mirror neurons are important in giving an individual the ability to mimic movements of hands. Broca's area of speech production has been shown to contain several of these mirror neurons resulting in significant similarities of brain activity between sign language and vocal speech communication. Facial communication is a significant portion of how animals interact with each other. Humans use facial movements to create, what other humans perceive, to be faces of emotions. While combining these facial movements with speech, a more full form of language is created which enables the species to interact with a much more complex and detailed form of communication. Sign language also uses these facial movements and emotions along with the primary hand movement way of communicating. These facial movement forms of communication come from the same areas of the brain. When dealing with damages to certain areas of the brain, vocal forms of communication are in jeopardy of severe forms of aphasia. Since these same areas of the brain are being used for sign language, these same, at least very similar, forms of aphasia can show in the Deaf community. Individuals can show a form of Wernicke's aphasia with sign language and they show deficits in their abilities in being able to produce any form of expressions. Broca's aphasia shows up in some people, as well. These individuals find tremendous difficulty in being able to actually sign the linguistic concepts they are trying to express.",
            "score": 164.77391052246094
        },
        {
            "docid": "40901980_2",
            "document": "Developmental verbal dyspraxia . Developmental verbal dyspraxia (DVD), also known as childhood apraxia of speech (CAS) and developmental apraxia of speech (DAS), is when children have problems saying sounds, syllables, and words. This is not because of muscle weakness or paralysis. The brain has problems planning to move the body parts (e.g., lips, jaw, tongue) needed for speech. The child knows what they want to say, but their brain has difficulty coordinating the muscle movements necessary to say those words. The exact cause of this disorder is unknown. Some observations suggest a genetic cause of DVD, as many with the disorder have a family history of communication disorders. There is no cure for DVD, but with appropriate, intensive intervention, people with this motor speech disorder can improve significantly.",
            "score": 160.6698760986328
        },
        {
            "docid": "315084_2",
            "document": "Lip reading . Lip-reading, also known as lipreading or speechreading, is a technique of understanding speech by visually interpreting the movements of the lips, face and tongue when normal sound is not available. It relies also on information provided by the context, knowledge of the language, and any residual hearing. Although ostensibly used by deaf and hard-of-hearing people, most people with normal hearing process some speech information from sight of the moving mouth.",
            "score": 159.2820281982422
        },
        {
            "docid": "433584_4",
            "document": "McGurk effect . Vision is the primary sense for humans, but speech perception is multimodal, which means that it involves information from more than one sensory modality, in particular, audition and vision. The McGurk effect arises during phonetic processing because the integration of audio and visual information happens early in speech perception. The McGurk effect is very robust; that is, knowledge about it seems to have little effect on one's perception of it. This is different from certain optical illusions, which break down once one 'sees through' them. Some people, including those that have been researching the phenomenon for more than twenty years, experience the effect even when they are aware that it is taking place. With the exception of people who can identify most of what is being said from speech-reading alone, most people are quite limited in their ability to identify speech from visual-only signals. A more extensive phenomenon is the ability of visual speech to increase the intelligibility of heard speech in a noisy environment. Visible speech can also alter the perception of perfectly audible speech sounds when the visual speech stimuli are mismatched with the auditory speech. Normally, speech perception is thought to be an auditory process; however, our use of information is immediate, automatic, and, to a large degree, unconscious and therefore, despite what is widely accepted as true, speech is not only something we hear. Speech is perceived by all of the senses working together (seeing, touching, and listening to a face move). The brain is often unaware of the separate sensory contributions of what it perceives. Therefore, when it comes to recognizing speech the brain cannot differentiate whether it is seeing or hearing the incoming information.",
            "score": 158.48744201660156
        },
        {
            "docid": "315084_7",
            "document": "Lip reading . Many factors affect the visibility of a speaking face, including illumination, movement of the head/camera, frame-rate of the moving image and distance from the viewer (see e.g.). Head movement that accompanies normal speech can also improve lip-reading, independently of oral actions. However, when lip-reading connected speech, the viewer's knowledge of the spoken language, familiarity with the speaker and style of speech, and the context of the lip-read material are as important as the visibility of the speaker. While most hearing people are sensitive to seen speech, there is great variability in individual speechreading skill. Good lipreaders are often more accurate than poor lipreaders at identifying phonemes from visual speech.",
            "score": 152.66412353515625
        },
        {
            "docid": "668442_45",
            "document": "Lip sync . FPS is a genre that generally places much more emphasis on graphical display, mainly due to the camera almost always being very close to character models. Due to increasingly detailed character models requiring animation, FPS developers assign many resources to create realistic lip synchronization with the many lines of speech used in most FPS games. Early 3D models used basic up-and-down jaw movements to simulate speech. As technology progressed, mouth movements began to closely resemble real human speech movements. \"\" dedicated a development team to lip sync alone, producing the most accurate lip synchronization for games at that time. Since then, games like \"\" and \"Half-Life 2\" have made use of coding that dynamically simulates mouth movements to produce sounds as if they were spoken by a live person, resulting in astoundingly lifelike characters. Gamers who create their own videos using character models with no lip movements, such as the helmeted Master Chief from \"Halo\", improvise lip movements by moving the characters' arms, bodies and making a bobbing movement with the head (see \"Red vs. Blue\").",
            "score": 152.3647918701172
        },
        {
            "docid": "2072616_22",
            "document": "Neuroprosthetics . Improved performance on cochlear implant not only depends on understanding the physical and biophysical limitations of implant stimulation but also on an understanding of the brain's pattern processing requirements. Modern signal processing represents the most important speech information while also providing the brain the pattern recognition information that it needs. Pattern recognition in the brain is more effective than algorithmic preprocessing at identifying important features in speech. A combination of engineering, signal processing, biophysics, and cognitive neuroscience was necessary to produce the right balance of technology to maximize the performance of auditory prosthesis.",
            "score": 150.36485290527344
        },
        {
            "docid": "403676_29",
            "document": "Gesture . Because of this connection of co-speech gestures\u2014a form of manual action\u2014in language in the brain, Roel Willems and Peter Hagoort conclude that both gestures and language contribute to the understanding and decoding of a speaker's encoded message. Willems and Hagoort's research suggest that \"processing evoked by gestures is qualitatively similar to that of words at the level of semantic processing.\" This conclusion is supported through findings from experiments by Skipper where the use of gestures led to \"a division of labor between areas related to language or action (Broca's area and premotor/primary motor cortex respectively)\", The use of gestures in combination with speech allowed the brain to decrease the need for \"semantic control\", Because gestures aided in understanding the relayed message, there was not as great a need for semantic selection or control that would otherwise be required of the listener through Broca's area. Gestures are a way to represent the thoughts of an individual, which are prompted in working memory. The results of an experiment revealed that adults have increased accuracy when they used pointing gestures as opposed to simply counting in their heads (without the use of pointing gestures) Furthermore, the results of a study conducted by Marstaller and Burianov\u00e1 suggest that the use of gestures affect working memory. The researchers found that those with low capacity of working memory who were able to use gestures actually recalled more terms than those with low capacity who were not able to use gestures.",
            "score": 149.96690368652344
        },
        {
            "docid": "315084_3",
            "document": "Lip reading . Although speech perception is considered to be an auditory skill, it is intrinsically multimodal, since producing speech requires the speaker to make movements of the lips, teeth and tongue which are often visible in face-to-face communication. Information from the lips and face supports aural comprehension and most fluent listeners of a language are sensitive to seen speech actions (see McGurk effect). The extent to which people make use of seen speech actions varies with the visibility of the speech action and the knowledge and skill of the perceiver.",
            "score": 149.2603302001953
        },
        {
            "docid": "20011748_7",
            "document": "Apraxia of speech . Apraxia of speech can be diagnosed by a speech language pathologist (SLP) through specific exams that measure oral mechanisms of speech. The oral mechanisms exam involves tasks such as pursing lips, blowing, licking lips, elevating the tongue, and also involves an examination of the mouth. A complete exam also involves observation of the patient eating and talking. SLPs do not agree on a specific set of characteristics that make up the apraxia of speech diagnosis, so any of the characteristics from the section above could be used to form a diagnosis. Patients may be asked to perform other daily tasks such as reading, writing, and conversing with others. In situations involving brain damage, an MRI brain scan also helps identify damaged areas of the brain.",
            "score": 148.94583129882812
        },
        {
            "docid": "4833512_9",
            "document": "Mu wave . Brain-computer interfaces (BCIs) are a developing technology that clinicians hope will one day bring more independence and agency to the severely physically disabled. Those the technology has the potential to help include people with near-total or total paralysis, such as those with tetraplegia (quadriplegia) or advanced amyotrophic lateral sclerosis (ALS); BCIs are intended to help them to communicate or even move objects such as motorized wheelchairs, neuroprostheses, or robotic grasping tools. Few of these technologies are currently in regular use by people with disabilities, but a diverse array are in development at an experimental level. One type of BCI uses event-related desynchronization (ERD) of the mu wave in order to control the computer. This method of monitoring brain activity takes advantage of the fact that when a group of neurons is at rest they tend to fire in synchrony with each other. When a participant is cued to imagine movement (an \"event\"), the resulting desynchronization (the group of neurons that was firing in synchronous waves now firing in complex and individualized patterns) can be reliably detected and analyzed by a computer. Users of such an interface are trained in visualizing movements, typically of the foot, hand, and/or tongue, which are each in different locations on the cortical homunculus and thus distinguishable by an electroencephalograph (EEG) or electrocorticograph (ECoG) recording of electrical activity over the motor cortex. In this method, computers monitor for a typical pattern of mu wave ERD contralateral to the visualized movement combined with event-related synchronization (ERS) in the surrounding tissue. This paired pattern intensifies with training, and the training increasingly takes the form of games, some of which utilize virtual reality. Some researchers have found that the feedback from virtual reality games is particularly effective in giving the user tools to improve control of his or her mu wave patterns. The ERD method can be combined with one or more other methods of monitoring the brain's electrical activity to create hybrid BCIs, which often offer more flexibility than a BCI that uses any single monitoring method.",
            "score": 148.57437133789062
        },
        {
            "docid": "33932515_22",
            "document": "Social cue . Benjamin Straube, Antonia Green, Andreas Jansen, Anjan Chatterjee, and Tilo Kircher found that social cues influence the neural processing of speech-gesture utterances. Past studies have focused on mentalizing as being a part of perception of social cues and it is believed that this process relies on the neural system, which consists of: When people focus on things in a social context, the medial prefrontal cortex and precuneus areas of the brain are activated, however when people focus on a non-social context there is no activation of these areas. Straube et al. hypothesized that the areas of the brain involved in mental processes were mainly responsible for social cue processing. It is believed that when iconic gestures are involved, the left temporal and occipital regions would be activated and when emblematic gestures were involved the temporal poles would be activated. When it came to abstract speech and gestures, the left frontal gyrus would be activated according to Straube et al. After conducting an experiment on how body position, speech and gestures affected activation in different areas of the brain Straube et al. came to the following conclusions:",
            "score": 148.31407165527344
        },
        {
            "docid": "17524_35",
            "document": "Language . Early work in neurolinguistics involved the study of language in people with brain lesions, to see how lesions in specific areas affect language and speech. In this way, neuroscientists in the 19th century discovered that two areas in the brain are crucially implicated in language processing. The first area is Wernicke's area, which is in the posterior section of the superior temporal gyrus in the dominant cerebral hemisphere. People with a lesion in this area of the brain develop receptive aphasia, a condition in which there is a major impairment of language comprehension, while speech retains a natural-sounding rhythm and a relatively normal sentence structure. The second area is Broca's area, in the posterior inferior frontal gyrus of the dominant hemisphere. People with a lesion to this area develop expressive aphasia, meaning that they know what they want to say, they just cannot get it out. They are typically able to understand what is being said to them, but unable to speak fluently. Other symptoms that may be present in expressive aphasia include problems with fluency, articulation, word-finding, word repetition, and producing and comprehending complex grammatical sentences, both orally and in writing. Those with this aphasia also exhibit ungrammatical speech and show inability to use syntactic information to determine the meaning of sentences. Both expressive and receptive aphasia also affect the use of sign language, in analogous ways to how they affect speech, with expressive aphasia causing signers to sign slowly and with incorrect grammar, whereas a signer with receptive aphasia will sign fluently, but make little sense to others and have difficulties comprehending others' signs. This shows that the impairment is specific to the ability to use language, not to the physiology used for speech production.",
            "score": 147.76121520996094
        },
        {
            "docid": "4833512_2",
            "document": "Mu wave . Mu waves, also known as mu rhythms, comb or wicket rhythms, arciform rhythms, or sensorimotor rhythms, are synchronized patterns of electrical activity involving large numbers of neurons, probably of the pyramidal type, in the part of the brain that controls voluntary movement. These patterns as measured by electroencephalography (EEG), magnetoencephalography (MEG), or electrocorticography (ECoG), repeat at a frequency of 7.5\u201312.5 (and primarily 9\u201311) Hz, and are most prominent when the body is physically at rest. Unlike the alpha wave, which occurs at a similar frequency over the resting visual cortex at the back of the scalp, the mu wave is found over the motor cortex, in a band approximately from ear to ear. A person suppresses mu wave patterns when he or she performs a motor action or, with practice, when he or she visualizes performing a motor action. This suppression is called desynchronization of the wave because EEG wave forms are caused by large numbers of neurons firing in synchrony. The mu wave is even suppressed when one observes another person performing a motor action or an abstract motion with biological characteristics. Researchers such as V. S. Ramachandran and colleagues have suggested that this is a sign that the mirror neuron system is involved in mu wave suppression, although others disagree.",
            "score": 147.129150390625
        },
        {
            "docid": "43124500_5",
            "document": "Sophie Scott . Scott is head of the Speech Communication Group at UCL's Institute of Cognitive Neuroscience. Her research investigates the neural basis of vocal communication \u2013 how our brains process the information in speech and voices, and how our brains control the production of our voice. Within this, her research covers the roles of streams of processing in auditory cortex, hemispheric asymmetries, and the interaction of speech processing with attentional and working memory factors. Other interests include individual differences in speech perception and plasticity in speech perception, since these are important factors for people with cochlear implants. She is also interested in the expression of emotion in the voice. In particular, research in recent years has focused on the neuroscience of laughter.",
            "score": 145.9515838623047
        },
        {
            "docid": "21715447_27",
            "document": "National Institutes of Health Stroke Scale . Dysarthria is the lack of motor skills required to produce understandable speech. Dysarthria is strictly a motor problem, and is not related to the patient's ability to comprehend speech. Strokes that cause dysarthria typically affect areas such as the anterior opercular, medial prefrontal and premotor, and anterior cingulate regions. These brain regions are vital in coordinating motor control of the tongue, throat, lips, and lungs. To perform this item the patient is asked to read from the list of words provided with the stroke scale while the examiner observes the patient's articulation and clarity of speech.",
            "score": 145.53521728515625
        },
        {
            "docid": "288292_11",
            "document": "Neuropsychology . Inspired by the advances being made in the area of localized function within the brain, Paul Broca committed much of his study to the phenomena of how speech is understood and produced. Through his study, it was discovered and expanded upon that we articulate via the left hemisphere. Broca's observations and methods are widely considered to be where neuropsychology really takes form as a recognizable and respected discipline. Armed with the understanding that specific, independent areas of the brain are responsible for articulation and understanding of speech, the brains abilities were finally being acknowledged as the complex and highly intricate organ that it is. Broca was essentially the first to fully break away from the ideas of phrenology and delve deeper into a more scientific and psychological view of the brain.",
            "score": 144.94639587402344
        },
        {
            "docid": "37691878_17",
            "document": "Phonemic restoration effect . Much like the McGurk Effect, when listeners were also able to see the words being spoken, they were much more likely to correctly identify the missing phonemes. Like every sense, the brain will use every piece of information it deems important to make a judgement about what it is perceiving. Using the visual cues of mouth movements, the brain will you both in top-down processing to make a decision about what phoneme is supposed to be heard. Vision is the primary sense for humans and for the most part assists in speech perception the most.",
            "score": 143.2598114013672
        },
        {
            "docid": "4335971_3",
            "document": "Equipotentiality . In the 1800s brain localization theories were the popular theories on how the brain functioned. The Broca's area of speech was discovered in 1861, in 1870 the cerebral cortex was marked as the motor center of the brain, and the general visual and auditory areas were defined in the cerebral cortex. Behaviorism at the time would also say that learned responses were series of specific connections in the cerebral cortex. Lashley argued that one would then be able to locate these connections in part of the brain and he systematically looked for where learning was localized.",
            "score": 142.7425537109375
        },
        {
            "docid": "620396_42",
            "document": "Origin of language . In humans, functional MRI studies have reported finding areas homologous to the monkey mirror neuron system in the inferior frontal cortex, close to Broca's area, one of the language regions of the brain. This has led to suggestions that human language evolved from a gesture performance/understanding system implemented in mirror neurons. Mirror neurons have been said to have the potential to provide a mechanism for action-understanding, imitation-learning, and the simulation of other people's behavior. This hypothesis is supported by some cytoarchitectonic homologies between monkey premotor area F5 and human Broca's area. Rates of vocabulary expansion link to the ability of children to vocally mirror non-words and so to acquire the new word pronunciations. Such speech repetition occurs automatically, quickly and separately in the brain to speech perception. Moreover, such vocal imitation can occur without comprehension such as in speech shadowing and echolalia. Further evidence for this link comes from a recent study in which the brain activity of two participants was measured using fMRI while they were gesturing words to each other using hand gestures with a game of charades\u2014a modality that some have suggested might represent the evolutionary precursor of human language. Analysis of the data using Granger Causality revealed that the mirror-neuron system of the observer indeed reflects the pattern of activity of in the motor system of the sender, supporting the idea that the motor concept associated with the words is indeed transmitted from one brain to another using the mirror system.",
            "score": 142.60791015625
        },
        {
            "docid": "1035470_44",
            "document": "Stroke recovery . Unlike many effects of stroke, where the clinician is able to judge the particular area of the brain that a stroke has injured by certain signs or symptoms, the causation of apraxia is less clear. A common theory is that the part of the brain that contains information for previously learned skilled motor activities has been either lost or cannot be accessed. The condition is usually due to an insult to the dominant hemisphere of the brain. More often this is located in the frontal lobe of the left hemisphere of the brain. Treatment of acquired apraxia due to stroke usually consists of physical, occupational, and speech therapy. The Copenhagen Stroke Study, which is a large important study published in 2001, showed that out of 618 stroke patients, manual apraxia was found in 7% and oral apraxia was found in 6%. Both manual and oral apraxia were related to increasing severity of stroke. Oral apraxia was related with an increase in age at the time of the stroke. There was no difference in incidence among gender. It was also found that the finding of apraxia has no negative influence on ability to function after rehabilitation is completed. The National Institute of Neurological Disorders and Stroke (NINDS) is currently sponsoring a clinical trial to gain an understanding of how the brain operates while carrying out and controlling voluntary motor movements in normal subjects. The objective is to determine what goes wrong with these processes in the course of acquired apraxia due to stroke or brain injury.",
            "score": 142.11656188964844
        },
        {
            "docid": "668442_44",
            "document": "Lip sync . Unlike RPGs, strategy video games make extensive use of sound files to create an immersive battle environment. Most games simply played a recorded audio track on cue with some games providing inanimate portraits to accompany the respective voice. \"StarCraft\" used full motion video character portraits with several generic speaking animations that did not synchronize with the lines spoken in the game. The game did, however, make extensive use of recorded speech to convey the game's plot, with the speaking animations providing a good idea of the flow of the conversation. \"Warcraft III\" used fully rendered 3D models to animate speech with generic mouth movements, both as character portraits as well as the in-game units. Like the FMV portraits, the 3D models did not synchronize with actual spoken text, while in-game models tended to simulate speech by moving their heads and arms rather than using actual lip synchronization. Similarly, the game Codename Panzers uses camera angles and hand movements to simulate speech, as the characters have no actual mouth movement. However, \"StarCraft II\" used fully synced unit portraits and cinematic sequences.",
            "score": 141.3367462158203
        },
        {
            "docid": "490258_4",
            "document": "Split-brain . When split-brain patients are shown an image only in the left half of each eye's visual field, they cannot vocally name what they have seen. This is because the image seen in the left visual field is sent only to the right side of the brain (see optic tract), and most people's speech-control center is on the left side of the brain. Communication between the two sides is inhibited, so the patient cannot say out loud the name of that which the right side of the brain is seeing. A similar effect occurs if a split-brain patient touches an object with only the left hand while receiving no visual cues in the right visual field; the patient will be unable to name the object, as each cerebral hemisphere of the primary somatosensory cortex only contains a tactile representation of the opposite side of the body. If the speech-control center is on the right side of the brain, the same effect can be achieved by presenting the image or object to only the right visual field or hand.",
            "score": 141.263916015625
        },
        {
            "docid": "1188574_28",
            "document": "Motor cortex . Perhaps the best-known experiments on the human motor map were published by Penfield in 1937. Using a procedure that was common in the 1930s, he examined epileptic patients who were undergoing brain surgery. These patients were given a local anesthetic, their skulls were opened, and their brains exposed. Then, electrical stimulation was applied to the surface of the brain to map out the speech areas. In this way, the surgeon would be able to avoid any damage to speech circuitry. The brain focus of the epilepsy could then be surgically removed. During this procedure, Penfield mapped the effect of electrical stimulation in all parts of the cerebral cortex, including motor cortex.",
            "score": 140.65936279296875
        },
        {
            "docid": "228051_6",
            "document": "Intrapersonal communication . Jones and Fernyhough cite other evidence for this hypothesis that inner speech is essentially like any other action. They mention that schizophrenics suffering auditory verbal hallucinations (AVH) need only open their mouths in order to disrupt the voices in their heads. To try and explain more about how inner speech works, but also what goes wrong with AVH patients, Jones and Fernyhough adapt what is known as the \"forward model\" of motor control, which uses the idea of \"efferent copies\". In a forward model of motor control, the mind generates movement unconsciously. While information is sent to the necessary body parts, the mind basically faxes a copy of that same information to other areas of the brain. This \"efferent\" copy could then be used to make predictions about upcoming movements. If the actual sensations match predictions, we experience the feeling of agency. If there is a mismatch between the body and its predicted position, perhaps due to obstructions or other cognitive disruption, no feeling of agency occurs. Jones and Fernyhough believe that the forward model might explain AVH and inner speech. Perhaps, if inner speech is a normal action, then the malfunction in schizophrenic patients is not the fact that actions (i.e. voices) are occurring at all. Instead, it may be that they are experiencing normal, inner speech, but the \"generation of the predictive efferent copy\" is malfunctioning. Without an efferent copy, motor commands are judged as alien (i.e. one does not feel like they caused the action). This could also explain why an open mouth stops the experience of alien voices: When the patient opens their mouth, the inner speech motor movements are not planned in the first place.",
            "score": 140.55613708496094
        },
        {
            "docid": "5366050_49",
            "document": "Speech perception . Computational modeling has also been used to simulate how speech may be processed by the brain to produce behaviors that are observed. Computer models have been used to address several questions in speech perception, including how the sound signal itself is processed to extract the acoustic cues used in speech, and how speech information is used for higher-level processes, such as word recognition.",
            "score": 140.36557006835938
        },
        {
            "docid": "863241_7",
            "document": "Baby talk . A key visual aspect of CDS is the movement of the lips. One characteristic is the wider opening of the mouth present in those using CDS versus adult-directed speech, particularly in vowels. The horizontal positioning of the lips in CDS does not differ significantly from that used in adult-directed speech. Instead, the observed difference lies in vertical lip positioning: By making the opening of the lips larger, infants are more likely to focus on the face of the speaker. Research suggests that with the larger opening of the lips during CDS, infants are better able to grasp the message being conveyed due to the heightened visual cues.",
            "score": 140.135009765625
        },
        {
            "docid": "403676_28",
            "document": "Gesture . Gestures are processed in the same areas of the brain as speech and sign language such as the left inferior frontal gyrus (Broca's area) and the posterior middle temporal gyrus, posterior superior temporal sulcus and superior temporal gyrus (Wernicke's area). It has been suggested that these parts of the brain originally supported the pairing of gesture and meaning and then were adapted in human evolution \"for the comparable pairing of sound and meaning as voluntary control over the vocal apparatus was established and spoken language evolved\". As a result, it underlies both symbolic gesture and spoken language in the present human brain. Their common neurological basis also supports the idea that symbolic gesture and spoken language are two parts of a single fundamental semiotic system that underlies human discourse. The linkage of hand and body gestures in conjunction with speech is further revealed by the nature of gesture use in blind individuals during conversation. This phenomenon uncovers a function of gesture that goes beyond portraying communicative content of language and extends David McNeill's view of the gesture-speech system. This suggests that gesture and speech work tightly together, and a disruption of one (speech or gesture) will cause a problem in the other. Studies have found strong evidence that speech and gesture are innately linked in the brain and work in an efficiently wired and choreographed system. McNeill's view of this linkage in the brain is just one of three currently up for debate; the others declaring gesture to be a \"support system\" of spoken language or a physical mechanism for lexical retrieval.",
            "score": 139.150146484375
        },
        {
            "docid": "43527201_5",
            "document": "Usha Goswami . Dyslexia is a disorder in which the person affected has difficulty reading due to the reversal of letters in the brain that isn't linked to intelligence. In people with dyslexia, the brain processes certain signals in a specific way making it a very specific learning difficulty. Dr. Goswami's research is concerned with focusing on dyslexia as a language disorder rather than a visual disorder as she has found that the way that children with dyslexia hear language is slightly different than others. When sound waves approach the brain, they vary in pressure depending on the syllables within the words being spoken creating a rhythm. When these signals reach the brain they are lined up with speech rhythms and this process doesn't work properly in those with dyslexia. Goswami is currently researching whether or not reading poetry, nursery rhymes, and singing can be used to help children with dyslexia. The rhythm of the words could allow the child to match the syllable patterns to language before they begin reading as to catch them up to where children without the disability might be.",
            "score": 138.36605834960938
        },
        {
            "docid": "2872287_23",
            "document": "Neural binding . Much of the experimental evidence for neural binding has traditionally revolved around sensory awareness. Sensory awareness is accomplished by integrating things together by cognitively perceiving them and then segmenting them so that, in total, there is an image created. Since there can be an infinite number of possibilities in the perception of an object, this has been a unique area of study. The way the brain then collectively pieces certain things together via networking is important not only in the global way of perceiving but also in segmentation. Much of sensory awareness has to do with the taking of a single piece of an object's makeup and then binding its total characteristics so that the brain perceives the object in its final form. Much of the research for the understanding of segmentation and how the brain perceives an object has been done by studying cats. A major finding of this research has to do with the understanding of gamma waves oscillating at 40\u00a0Hz. The information was extracted from a study using the cat visual cortex. It was shown that the cortical neurons responded differently to spatially different objects. These firings of neurons ranged from 40\u201360\u00a0Hz in measure and when observed showed that they fired synchronously when observing different parts of the object. Such coherent responses point to the fact that the brain is doing a kind of coding where it is piecing certain neurons together in the works of making the form of an object. Since the brain is putting these segmented pieces together unsupervised, a significant consonance is found with many philosophers (like Sigmund Freud) who theorize an underlying subconscious that helps to form every aspect of our conscious thought processes.",
            "score": 138.3005828857422
        }
    ]
}