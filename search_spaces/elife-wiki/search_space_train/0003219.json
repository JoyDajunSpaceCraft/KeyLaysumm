{
    "q": [
        {
            "docid": "32116125_4",
            "document": "Amblyaudia . Amblyaudia is a deficit in binaural integration of environmental information entering the auditory system. It is a disorder related to brain organization and function rather than what is typically considered a \u201chearing loss\u201d (damage to the cochlea). It may be genetic or developmentally acquired or both. When animals are temporarily deprived of hearing from an early age, profound changes occur in the brain. Specifically, cell sizes in brainstem nuclei are reduced, the configuration of brainstem dendrites are altered and neurons respond in different ways to sounds presented to both the deprived and non-deprived ears (in cases of asymmetric deprivation). This last point is particularly important for listening tasks that require inputs from two ears to perform well. There are multiple auditory functions that rely on the computation of well calibrated inputs from the two ears. Chief among these is the ability to localize sound sources and separate what we want to hear from a background of noise. In the brainstem, the auditory system compares the timing and levels of sounds between the two ears to encode the location of sound sources (sounds that originate from our right as opposed to left side are louder and arrive earlier in our right ear). This ability to separate sound sources not only helps us locate the trajectories of moving objects, but also to separate different sound sources in noisy environments.",
            "score": 234.03016591072083
        },
        {
            "docid": "32116125_3",
            "document": "Amblyaudia . Children with amblyaudia experience difficulties in speech perception, particularly in noisy environments, sound localization, and binaural unmasking (using interaural cues to hear better in noise) despite having normal hearing sensitivity (as indexed through pure tone audiometry). These symptoms may lead to difficulty attending to auditory information causing many to speculate that language acquisition and academic achievement may be deleteriously affected in children with amblyaudia. A significant deficit in a child's ability to use and comprehend expressive language may be seen in children who lacked auditory stimulation throughout the critical periods of auditory system development. A child suffering from amblyaudia may have trouble in appropriate vocabulary comprehension and production and the use of past, present and future tenses. Amblyaudia has been diagnosed in many children with reported difficulties understanding and learning from listening and adjudicated adolescents are at a significantly high risk for amblyaudia (Moncrieff, et al., 2013, Seminars in Hearing). Families report the presence of amblyaudia in several individuals, suggesting that it may be genetic in nature. It is possible that abnormal auditory input during the first two years of life may increase a child\u2019s risk for amblyaudia, although the precise relationship between deprivation timing and development of amblyaudia is still unclear. Recurrent ear infections (otitis media) are the leading cause of temporary auditory deprivation in young children. During ear infection bouts, the quality of the signal that reaches the auditory regions of the brains of a subset of children with OM is degraded in both timing and magnitude. When this degradation is asymmetric (worse in one ear than the other) the binaural cues associated with sound localization can also be degraded. Aural atresia (a closed external auditory canal) also causes temporary auditory deprivation in young children. Hearing can be restored to children with ear infections and aural atresia through surgical intervention (although ear infections will also resolve spontaneously). Nevertheless, children with histories of auditory deprivation secondary to these diseases can experience amblyaudia for years after their hearing has been restored.",
            "score": 196.73578584194183
        },
        {
            "docid": "32105732_4",
            "document": "Spatial hearing loss . Sound streams arriving from the left or right (the horizontal plane) are localised primarily by the small time differences of the same sound arriving at the two ears. A sound straight in front of the head is heard at the same time by both ears. A sound to the side of the head is heard approximately 0.0005 seconds later by the ear furthest away. A sound halfway to one side is heard approximately 0.0003 seconds later. This is the interaural time difference (ITD) cue and is measured by signal processing in the two central auditory pathways that begin after the cochlea and pass through the brainstem and mid-brain. Some of those with spatial hearing loss are unable to process ITD (low frequency) cues.",
            "score": 216.17455530166626
        },
        {
            "docid": "57411023_4",
            "document": "Andrew King (neurophysiologist) . King discovered that the mammalian brain contains a spatial map of the auditory world and showed that its development is shaped by sensory experience. His work has also demonstrated that the adult brain represents sound features in a remarkably flexible way, continually adjusting to variations in the statistical distribution of sounds associated with different acoustic environments as well to longer term changes in input resulting from hearing loss. In addition to furthering our understanding of the neural basis for auditory perception, his research is helping to inform better treatment strategies for the hearing impaired.",
            "score": 150.19389605522156
        },
        {
            "docid": "2534964_17",
            "document": "Sensory processing . In the future, research on sensory integration will be used to better understand how different sensory modalities are incorporated within the brain to help us perform even the simplest of tasks. For example, we do not currently have the understanding needed to comprehend how neural circuits transform sensory cues into changes in motor activities. More research done on the sensorimotor system can help understand how these movements are controlled. This understanding can potentially be used to learn more about how to make better prosthetics, and eventually help patients who have lost the use of a limb. Also, by learning more about how different sensory inputs can combine can have profound effects on new engineering approaches using robotics. The robot's sensory devices may take in inputs of different modalities, but if we understand multisensory integration better, we might be able to program these robots to convey these data into a useful output to better serve our purposes.",
            "score": 62.88797104358673
        },
        {
            "docid": "7913402_16",
            "document": "Paul Baltes . Neuronal plasticity, or the capability of the brain to adapt to new requirements, is a prime example of plasticity stressing that the individual\u2019s ability to change is a lifelong process. Recently, researchers have been analyzing how the spared senses compensate for the loss of vision. Without visual input, blind humans have demonstrated that tactile and auditory functions still fully develop. A superiority of the blind has even been observed when they are presented with tactile and auditory tasks. This superiority may suggest that the specific sensory experiences of the blind may influence the development of certain sensory functions, namely tactile and auditory. One experiment was designed by R\u00f6der and colleagues to clarify the auditory localization skills of the blind in comparison to the sighted. They examined both blind human adults\u2019 and sighted human adults\u2019 abilities to locate sounds presented either central or peripheral (lateral) to them. Both congenitally blind adults and sighted adults could locate a sound presented in front of them with precision but the blind were clearly superior in locating sounds presented laterally. Currently, brain-imaging studies have revealed that the sensory cortices in the brain are reorganized after visual deprivation. These findings suggest that when vision is absent in development, the auditory cortices in the brain recruit areas that are normally devoted to vision, thus becoming further refined.",
            "score": 161.73382103443146
        },
        {
            "docid": "32116125_2",
            "document": "Amblyaudia . Amblyaudia (amblyos- blunt; audia-hearing) is a term coined by Dr. Deborah Moncrieff from the University of Pittsburgh to characterize a specific pattern of performance from dichotic listening tests. Dichotic listening tests are widely used to assess individuals for binaural integration, a type of auditory processing skill. During the tests, individuals are asked to identify different words presented simultaneously to the two ears. Normal listeners can identify the words fairly well and show a small difference between the two ears with one ear slightly dominant over the other. For the majority of listeners, this small difference is referred to as a \"right-ear advantage\" because their right ear performs slightly better than their left ear. But some normal individuals produce a \"left-ear advantage\" during dichotic tests and others perform at equal levels in the two ears. Amblyaudia is diagnosed when the scores from the two ears are significantly different with the individual's dominant ear score much higher than the score in the non-dominant ear  Researchers interested in understanding the neurophysiological underpinnings of amblyaudia consider it to be a brain based hearing disorder that may be inherited or that may result from auditory deprivation during critical periods of brain development. Individuals with amblyaudia have normal hearing sensitivity (in other words they hear soft sounds) but have difficulty hearing in noisy environments like restaurants or classrooms. Even in quiet environments, individuals with amblyaudia may fail to understand what they are hearing, especially if the information is new or complicated. Amblyaudia can be conceptualized as the auditory analog of the better known central visual disorder amblyopia. The term \u201clazy ear\u201d has been used to describe amblyaudia although it is currently not known whether it stems from deficits in the auditory periphery (middle ear or cochlea) or from other parts of the auditory system in the brain, or both. A characteristic of amblyaudia is suppression of activity in the non-dominant auditory pathway by activity in the dominant pathway which may be genetically determined and which could also be exacerbated by conditions throughout early development.",
            "score": 201.79769575595856
        },
        {
            "docid": "563743_5",
            "document": "Hearing test . A complete hearing evaluation involves several other tests as well. In order to determine what kind of hearing loss is present, a bone conduction hearing test is administered. In this test, a vibrating tuning fork is placed behind the ear, on the mastoid process. When the patient can no longer feel/hear the vibration, the tuning fork is held in front of the ear; the patient should once more be able to hear a ringing sound. If they cannot, there is conductive hearing loss in that ear. Additionally, the tuning fork is placed on the forehead. The patient is then asked if the sound is localised in the centre of the head or whether it is louder in either ear. If there is conductive hearing loss, it is likely to be louder in the affected ear; if there is sensorineural hearing loss, it will be quieter in the affected ear. This test helps the audiologist determine whether the hearing loss is conductive (caused by problems in the outer or middle ear) or sensorineural (caused by problems in the cochlea, the sensory organ of hearing) or neural - caused by a problem in the auditory nerve or auditory pathways/cortex of the brain.",
            "score": 217.17022013664246
        },
        {
            "docid": "1049228_21",
            "document": "Place cell . Although place cells primarily rely on visuospatial input, some studies suggest that olfactory input may also play a role in generating and recalling place fields. Relatively little is known about the interaction between place cells and non-visual sensory cues, but preliminary studies have shown that non-visual sensory input may have supplementary role in place field formation. A study by Save et al. found that olfactory information can be used to compensate for a loss of visual information. In this study, place fields in subjects exposed to an environment with no light and no olfactory signals were unstable; the position of the place field shifted abruptly and some of the constituent place cells stopped firing entirely. However, place cells in subjects exposed to a dark environment with olfactory signals remained stable despite a lack of visual cues. An additional study by Zhang et al. examined how the hippocampus uses olfactory signals to create and recall place fields. Similar to the Save et al. study, this study exposed subjects to an environment with a series of odors but no visual or auditory information. Place fields remained stable and even adapted to the rotation of the pattern of olfactory signals. Furthermore, the place fields would remap entirely when the odors were moved randomly. This suggests that place cells not only utilize olfactory information to generate place fields, but also use olfactory information to orient place fields during movement.",
            "score": 86.94390296936035
        },
        {
            "docid": "1619306_64",
            "document": "Multisensory integration . Lastly, Nardini et al. (2010) recently hypothesised that young children have optimized their sensory appreciation for speed over accuracy. When information is presented in two forms, children may derive an estimate from the fastest available source, subsequently ignoring the alternate, even if it contains redundant information. Nardini et al. (2010) provides evidence that children's (aged 6 years) response latencies are significantly lower when stimuli are presented in multi-cue over single-cue conditions. Conversely, adults showed no change between these conditions. Indeed, adults display mandatory fusion of signals, therefore they can only ever aim for maximum accuracy. Interestingly, however, the overall mean latencies for children were not faster than adults, which suggests that speed optimization merely enable them to keep up with the mature pace. Considering the haste of real-world events, this strategy may prove necessary to counteract the general slower processing of children and maintain effective vision-action coupling. Ultimately the developing sensory system may preferentially adapt for different goals \u2013 \"speed and detecting sensory conflicts\" \u2013 those typical of objective learning.",
            "score": 103.32314896583557
        },
        {
            "docid": "56439577_35",
            "document": "Temporal envelope and fine structure . For sinusoidal carriers, which have no intrinsic envelope (ENV) fluctuations, the TMTF is roughly flat for AM rates from 10 to 120\u00a0Hz, but increases (i.e. threshold worsens) for higher AM rates, provided that spectral sidebands are not audible. The shape of the TMTF for sinusoidal carriers is similar for young and older people with normal audiometric thresholds, but older people tend to have higher detection thresholds overall, suggesting poorer \u201cdetection efficiency\u201d for ENV cues in older people. Provided that the carrier is fully audible, the ability to detect AM is usually not adversely affected by cochlear hearing loss and may sometimes be better than normal, for both noise carriers and sinusoidal carriers, perhaps because loudness recruitment (an abnormally rapid growth of loudness with increasing sound level) \u201cmagnifies\u201d the perceived amount of AM (i.e., ENV cues). Consistent with this, when the AM is clearly audible, a sound with a fixed AM depth appears to fluctuate more for an impaired ear than for a normal ear. However, the ability to detect changes in AM depth can be impaired by cochlear hearing loss. Additional experiments suggest that age negatively affects the binaural processing of ENV at least at low audio-frequencies.",
            "score": 156.94076132774353
        },
        {
            "docid": "9703_55",
            "document": "Evolutionary psychology . In evolutionary psychology, learning is said to be accomplished through evolved capacities, specifically facultative adaptations. Facultative adaptations express themselves differently depending on input from the environment. Sometimes the input comes during development and helps shape that development. For example, migrating birds learn to orient themselves by the stars during a critical period in their maturation. Evolutionary psychologists believe that humans also learn language along an evolved program, also with critical periods. The input can also come during daily tasks, helping the organism cope with changing environmental conditions. For example, animals evolved Pavlovian conditioning in order to solve problems about causal relationships. Animals accomplish learning tasks most easily when those tasks resemble problems that they faced in their evolutionary past, such as a rat learning where to find food or water. Learning capacities sometimes demonstrate differences between the sexes. In many animal species, for example, males can solve spatial problem faster and more accurately than females, due to the effects of male hormones during development. The same might be true of humans.",
            "score": 72.76007843017578
        },
        {
            "docid": "1021754_32",
            "document": "Sound localization . When the head is stationary, the binaural cues for lateral sound localization (interaural time difference and interaural level difference) do not give information about the location of a sound in the median plane. Identical ITDs and ILDs can be produced by sounds at eye level or at any elevation, as long as the lateral direction is constant. However, if the head is rotated, the ITD and ILD change dynamically, and those changes are different for sounds at different elevations. For example, if an eye-level sound source is straight ahead and the head turns to the left, the sound becomes louder (and arrives sooner) at the right ear than at the left. But if the sound source is directly overhead, there will be no change in the ITD and ILD as the head turns. Intermediate elevations will produce intermediate degrees of change, and if the presentation of binaural cues to the two ears during head movement is reversed, the sound will be heard behind the listener. Hans Wallach artificially altered a sound\u2019s binaural cues during movements of the head. Although the sound was objectively placed at eye level, the dynamic changes to ITD and ILD as the head rotated were those that would be produced if the sound source had been elevated. In this situation, the sound was heard at the synthesized elevation. The fact that the sound sources objectively remained at eye level prevented monaural cues from specifying the elevation, showing that it was the dynamic change in the binaural cues during head movement that allowed the sound to be correctly localized in the vertical dimension. The head movements need not be actively produced; accurate vertical localization occurred in a similar setup when the head rotation was produced passively, by seating the blindfolded subject in a rotating chair. As long as the dynamic changes in binaural cues accompanied a perceived head rotation, the synthesized elevation was perceived.",
            "score": 144.06001555919647
        },
        {
            "docid": "2458955_14",
            "document": "Colin Blakemore . Although initially controversial, the idea that the mammalian brain is 'plastic' and adaptive is now a dominant theme in neuroscience. The plasticity of connections between nerve cells is thought to underlie many different types of learning and memory, as well as sensory development. The changes in organisation can be remarkably rapid, even in adults. Blakemore has shown that the visual parts of the human cortex become responsive to input from the other senses, especially touch, in people who have been blind since shortly after birth. After stroke or other forms of brain injury, reorganisation of this sort can help the process of recovery, as other parts of the brain take over the function of the damaged part.",
            "score": 95.8542833328247
        },
        {
            "docid": "1903855_9",
            "document": "Sensory substitution . \"Brain plasticity\" refers to the brain's ability to adapt to a changing environment, for instance to the absence or deterioration of a sense. It is conceivable that cortical remapping or reorganization in response to the loss of one sense may be an evolutionary mechanism that allows people to adapt and compensate by using other senses better. Functional imaging of congenitally blind patients showed a cross-modal recruitment of the occipital cortex during perceptual tasks such as Braille reading, tactile perception, tactual object recognition, sound localization, and sound discrimination. This may suggest that blind people can use their occipital lobe, generally used for vision, to perceive objects through the use of other sensory modalities. This cross modal plasticity may explain the often described tendency of blind people to show enhanced ability in the other senses.",
            "score": 131.1898546218872
        },
        {
            "docid": "49528276_11",
            "document": "Behavioral plasticity . Recent studies of animals have documented individual differences in virtually all of the different types of behavioral plasticities described above. In addition, behavioral plasticities may themselves be developmentally plastic: individual differences in a type of plasticity that is expressed at a given age may be affected by the conditions to which the subjects were exposed earlier in life. In a variety of species, for instance, social cues during the juvenile period affect the contextual plasticity of responses to cues from potential mates at adulthood. As is the case for many other types of plasticity, researchers studying the development of individual differences in behavioral plasticity have found that genes, prior experiences and interactions between these factors contribute to the individual differences in behavioral plasticity that are expressed at a given age or lifestage. Another question that is currently attracting interest from students of both animal and human behavior is whether different types of behavioral plasticities are correlated with one another across individuals: i.e., whether some individuals are generally more plastic than others. Although there is some evidence that certain types of cognitive traits tend to be positively correlated with one another across individuals (see the g factor in humans), at present there is scant evidence that other types of plasticity (e.g. contextual plasticity and ontogenetic plasticity) are correlated with one another across individuals or genotypes in humans or animals.  Behavioral plasticity can have major impacts on the evolutionary fitness of an individual. Both developmental and contextual plasticity influence the fitness of an animal in a novel environment by increasing the probability that the animal will survive in that environment. Developmental plasticity is particularly important in terms of survival in novel environments, because trial-and-error processes such as learning (which encompass both phenotype sampling and environmental feedback) have the ability to immediately shift an entire population close to a new adaptive norm. As such, the ability to express some level of behavioral plasticity can be very advantageous. In fluctuating environments, animals that can change how they respond to differences in stimuli would have a leg up over animals that were set in a rigid phenotype. However, this would only be the case if the costs of maintaining the ability to change phenotype was lower than the benefit conferred to the individual.",
            "score": 92.04200041294098
        },
        {
            "docid": "5442380_14",
            "document": "Sensory cue . An auditory cue is a sound signal that represents an incoming sign received through the ears, causing the brain to hear. The results of receiving and processing these cues are collectively known as the sense of hearing, and are the subject of research within the fields of psychology, cognitive science, and neurobiology.",
            "score": 150.45667362213135
        },
        {
            "docid": "1049228_19",
            "document": "Place cell . Sensory information received by place cells can be categorized as either metric or contextual information, where metric information corresponds to where place cells should fire and contextual input corresponds to whether or not a place field should fire in a certain environment. Metric sensory information is any kind of spatial input that might indicate a distance between two points. For example, the edges of an environment might signal the size of the overall place field or the distance between two points within a place field. Metric signals can be either linear or directional. Directional inputs provide information about the orientation of a place field, whereas linear inputs essentially form a representational grid. Contextual cues allow established place fields to adapt to minor changes in the environment, such as a change in object color or shape. Metric and contextual inputs are processed together in the entorhinal cortex before reaching the hippocampal place cells. Visuospatial and olfactory inputs are examples of sensory inputs that are utilized by place cells. These types of sensory cues can include both metric and contextual information.",
            "score": 55.974018812179565
        },
        {
            "docid": "10013549_3",
            "document": "Sensory ecology . Sensory ecology is the study of how organisms acquire, process, and respond to information from their environment. All individual organisms interact with their environment (consisting of both animate and inanimate components), and exchange materials, energy, and sensory information. Ecology has generally focused on the exchanges of matter and energy, while sensory interactions have generally been studied as influences on behavior and functions of certain physiological systems (sense organs). The relatively new area of sensory ecology has emerged as more researchers focus on questions concerning information in the environment. This field covers topics ranging from the neurobiological mechanisms of sensory systems to the behavioral patterns employed in the acquisition of sensory information to the role of sensory ecology in larger evolutionary processes such as speciation and reproductive isolation. While human perception is largely visual, other species may rely more heavily on different senses. In fact, how organisms perceive and filter information from their environment varies widely. Organisms experience different perceptual worlds, also known as \u201cumwelten\u201d, as a result of their sensory filters. These senses range from smell (olfaction), taste (gustation), hearing (mechanoreception), and sight (vision) to pheromone detection, pain detection (nociception), electroreception and magnetoreception. Because different species rely on different senses, sensory ecologists seek to understand which environmental and sensory cues are more important in determining the behavioral patterns of certain species. In recent years, this information has been widely applied in conservation and management fields.",
            "score": 76.48510265350342
        },
        {
            "docid": "5051081_7",
            "document": "Eric Knudsen . Knudsen altered owls\u2019 auditory cues by plugging one ear or removing the ruff feathers and preaural flaps. Initially this caused the birds to inaccurately judge sound source location, since the cues normally associated with each location in space had been changed. However, over time, the map shifted to restore a normal auditory sound map, aligned with the visual space map, despite the abnormal cues. New associations were formed between the abnormal cue values and the spatial locations they now represented, adjusting the map to translate the cues the bird was receiving into an accurate representation of its environment. This adjustment happens most rapidly and extensively in young birds. However, the map never perfectly reflects abnormal experience, even when cues are altered so early that the bird never experiences normal cues. This indicates that there is some innate \u201cprogramming\u201d of the map to reflect typical sensory experience.",
            "score": 145.57912147045135
        },
        {
            "docid": "161005_5",
            "document": "Head-related transfer function . Humans estimate the location of a source by taking cues derived from one ear (\"monaural cues\"), and by comparing cues received at both ears (\"difference cues\" or \"binaural cues\"). Among the difference cues are time differences of arrival and intensity differences. The monaural cues come from the interaction between the sound source and the human anatomy, in which the original source sound is modified before it enters the ear canal for processing by the auditory system. These modifications encode the source location, and may be captured via an impulse response which relates the source location and the ear location. This impulse response is termed the \"head-related impulse response\" (HRIR). Convolution of an arbitrary source sound with the HRIR converts the sound to that which would have been heard by the listener if it had been played at the source location, with the listener's ear at the receiver location. HRIRs have been used to produce virtual surround sound.",
            "score": 188.08572816848755
        },
        {
            "docid": "25140_36",
            "document": "Perception . Scientists who study perception and sensation have long understood the human senses as adaptations. Depth perception consists of processing over half a dozen visual cues, each of which is based on a regularity of the physical world. Vision evolved to respond to the narrow range of electromagnetic energy that is plentiful and that does not pass through objects. Sound waves provide useful information about the sources of and distances to objects, with larger animals making and hearing lower-frequency sounds and smaller animals making and hearing higher-frequency sounds. Taste and smell respond to chemicals in the environment that were significant for fitness in the environment of evolutionary adaptedness. The sense of touch is actually many senses, including pressure, heat, cold, tickle, and pain. Pain, while unpleasant, is adaptive. An important adaptation for senses is range shifting, by which the organism becomes temporarily more or less sensitive to sensation. For example, one's eyes automatically adjust to dim or bright ambient light. Sensory abilities of different organisms often coevolve, as is the case with the hearing of echolocating bats and that of the moths that have evolved to respond to the sounds that the bats make.",
            "score": 157.5984481573105
        },
        {
            "docid": "25935238_37",
            "document": "Educational neuroscience . The knowledge of early brain development afforded by neurobiology has been used to support various arguments with regards to education. For example, the idea that any subject can be taught to young children in some intellectually honest form, due to the great adaptability and learning potential of the young brain. Alternatively, the idea that critical periods exist for learning certain skills or knowledge sets appeals to the fact that in animal studies, if the developing brain is deprived of certain sensory inputs, the brain areas responsible for processing those inputs fail to develop fully later in development, and thus \"if you miss the window, you are playing with a handicap\".",
            "score": 71.67556738853455
        },
        {
            "docid": "9703_53",
            "document": "Evolutionary psychology . Scientists who study perception and sensation have long understood the human senses as adaptations. Depth perception consists of processing over half a dozen visual cues, each of which is based on a regularity of the physical world. Vision evolved to respond to the narrow range of electromagnetic energy that is plentiful and that does not pass through objects. Sound waves go around corners and interact with obstacles, creating a complex pattern that includes useful information about the sources of and distances to objects. Larger animals naturally make lower-pitched sounds as a consequence of their size. The range over which an animal hears, on the other hand, is determined by adaptation. Homing pigeons, for example, can hear very low-pitched sound (infrasound) that carries great distances, even though most smaller animals detect higher-pitched sounds. Taste and smell respond to chemicals in the environment that are thought to have been significant for fitness in the environment of evolutionary adaptedness. For example, salt and sugar were apparently both valuable to the human or pre-human inhabitants of the environment of evolutionary adaptedness, so present day humans have an intrinsic hunger for salty and sweet tastes. The sense of touch is actually many senses, including pressure, heat, cold, tickle, and pain. Pain, while unpleasant, is adaptive. An important adaptation for senses is range shifting, by which the organism becomes temporarily more or less sensitive to sensation. For example, one's eyes automatically adjust to dim or bright ambient light. Sensory abilities of different organisms often coevolve, as is the case with the hearing of echolocating bats and that of the moths that have evolved to respond to the sounds that the bats make.",
            "score": 151.3088035583496
        },
        {
            "docid": "17523336_20",
            "document": "Olivocochlear system . In humans, psychophysical experiments conducted in constant BGN have also implicated the olivocochlear bundle (OCB) in selective listening. The research perhaps most relevant to this thesis has been performed by Scharf and his colleagues. In 1993, Scharf et al. presented data from eight patients who had undergone unilateral vestibular neurectomy to treat M\u00e9ni\u00e8re\u2019s disease, a procedure which severs the OCB (presumably both the MOCS and the LOCS). Scharf et al. (1993) did not find any clear differences in subjects\u2019 thresholds to tones in noise before and after surgery. Shortly after this finding, Scharf et al. (1994, 1997) performed a comprehensive set of psychophysical experiments from a total of sixteen patients who had undergone unilateral vestibular neurectomy (including the original eight subjects). They measured performance in the psychophysical listening tasks before and after surgery, and found no significant difference in performance for (i) detection of tones, (ii) intensity discrimination of tones, (iii) frequency discrimination of tones, (iv) loudness adaptation, and (v) detection of tones in notched-noise. Their only positive finding was that most patients detected unexpected sounds in the operated ear better than in the healthy ear, or the same ear before surgery. This result was obtained using a truncated probe-signal procedure which led the patient to expect a certain frequency on each trial. Twelve subjects completed this experiment. Their procedure was similar to that of Greenberg and Larkin (1968), except only 50% of trials (not 77%) contained a target whose frequency matched that of the auditory cue. The other 50% of trials containing a probe whose frequency differed from that of the cue. Also, only two probe frequencies were used, one whose frequency was higher than the target, and one whose frequency was lower than the target. All trials contained an auditory cue (at the target frequency) prior to the first observation interval. The results were used to construct a basic attentional filter, which displayed detection level of the expected (and cued) target frequency and the two unexpected probe frequencies. From the two published reports (Scharf et al., 1994, 1997), ears for which the OCB has been lesioned showed an attentional filter with an average depth of about 15%-correct less than those ears for which the OCB was intact. Although there is no way to empirically convert this value to dB, a rough estimate based on psychometric functions presented by Green and Swets (1966) yields a value of 2-3\u00a0dB. Their results have been summarised in the inset figure.",
            "score": 168.18213510513306
        },
        {
            "docid": "1049228_20",
            "document": "Place cell . Spatial cues such as geometric boundaries or orienting landmarks are important examples of \"metric\" input. Place cells mainly rely on set distal cues rather than cues in the immediate proximal environment. Movement can also be an important spatial cue. The ability of place cells to incorporate new movement information is called path integration, and it is important for keeping track of self-location during movement. Path integration is largely aided by grid cells, which are a type of neuron in the entorhinal cortex that relay information to place cells in the hippocampus. Grid cells establish a grid representation of a location, so that during movement place cells can fire according to their new location while orienting according to the reference grid of their external environment. Visual sensory inputs can also supply important \"contextual\" information. A change in color of a specific object can affect whether or not a place cell fires in a particular field. Thus, visuospatial sensory information is critical to the formation and recollection of place field.",
            "score": 57.222708225250244
        },
        {
            "docid": "56439577_48",
            "document": "Temporal envelope and fine structure . The plasticity of ENV processing has been demonstrated in several ways. For instance, the ability of auditory-cortex neurons to discriminate voice-onset time cues for phonemes is degraded following moderate hearing loss (20-40\u00a0dB HL) induced by acoustic trauma. Interestingly, developmental hearing loss reduces cortical responses to slow, but not fast (100\u00a0Hz) AM stimuli, in parallel with behavioral performance. As a matter of fact, a transient hearing loss (15 days) occurring during the \"critical period\" is sufficient to elevate AM thresholds in adult gerbils. Even non-traumatic noise exposure reduces the phase-locking ability of cortical neurons as well as the animals' behavioral capacity to discriminate between different AM sounds. Behavioral training or pairing protocols involving neuromodulators also alter the ability of cortical neurons to phase lock to AM sounds. In humans, hearing loss may result in an unbalanced representation of speech cues: ENV cues are enhanced at the cost of TFS cues (see: Effects of age and hearing loss on temporal envelope processing). Auditory training may reduce the representation of speech ENV cues for elderly listeners with hearing loss, who may then reach levels comparable to those observed for normal-hearing elderly listeners. Last, intensive musical training induces both behavioral effects such as higher sensitivity to pitch variations (for Mandarin linguistic pitch) and a better synchronization of brainstem responses to the f0-contour of lexical tones for musicians compared with non-musicians.",
            "score": 150.3575897216797
        },
        {
            "docid": "45464854_4",
            "document": "Sensory memory . Echoic memory represents SM for the auditory sense of hearing. Auditory information travels as sound waves which are sensed by hair cells in the ears. Information is sent to and processed in the temporal lobe. The echoic sensory store holds information for 2\u20133 seconds to allow for proper processing. The first studies of echoic memory came shortly after Sperling investigated iconic memory using an adapted partial report paradigm. Today, characteristics of echoic memory have been found mainly using a mismatch negativity (MMN) paradigm which utilizes EEG and MEG recordings. MMN has been used to identify some of the key roles of echoic memory such as change detection and language acquisition. Change detection, or the ability to detect an unusual or possibly dangerous change in the environment independent of attention, is key to the survival of an organism. One study focusing on echoic sensory changes suggested that when a sound is presented to a subject, it is enough to shape an echoic memory trace that can be compared to a physically different sound. Change-related cortical responses were detected in the superior temporal gyrus using EEG . With regards to language, a characteristic of children who begin speaking late in development is reduced duration of echoic memory. In short, \"Echoic memory is a fast-decaying store of auditory information.\" In the case of damage to or lesions developing on the frontal lobe, parietal lobe, or hippocampus, echoic memory will likely be shortened and/or have a slower reaction time.",
            "score": 147.76892232894897
        },
        {
            "docid": "39200136_12",
            "document": "Hans Wallach . In a series of papers Wallach explored the ability of humans to locate sounds in the median plane \u2013 that is, to determine whether a sound comes from a source at the same elevation as the ears or from a source that is higher or lower, or even in back of the head. Binaural sound cues, including the phasing or time of the sound\u2019s arrival at each ear and the sound\u2019s relative intensity at the two ears (known respectively as ITD and ILD) enable a listener to determine a sound\u2019s lateral location (whether it is on the left, right, or straight ahead). But two sounds at different elevations can present identical ITD and ILD information to the ears, and so binaural cues to a stationary ear do not suffice to identify a sound\u2019s location in the median plane. Monaural cues that depend on the shape of the head and the structure of the external ear help with vertical localization, but binaural cues also play a part if the head is not stationary.",
            "score": 200.2918393611908
        },
        {
            "docid": "231030_30",
            "document": "Baleen whale . The mysticete ear is adapted for hearing underwater, where it can hear sound frequencies as low as 7 Hz and as high as 22 kHz. It is largely unknown how sound is received by baleen whales. Unlike in toothed whales, sound does not pass through the lower jaw. The auditory meatus is blocked by connective tissue and an ear plug, which connects to the eardrum. The inner-ear bones are contained in the tympanic bulla, a bony capsule. However, this is attached to the skull, suggesting that vibrations passing through the bone is important. Sinuses may reflect vibrations towards the cochlea. It is known that when the fluid inside the cochlea is disturbed by vibrations, it triggers sensory hairs which send electrical current to the brain, where vibrations are processed into sound.",
            "score": 182.6223602294922
        },
        {
            "docid": "520289_55",
            "document": "Hearing aid . Digital audio, programmable control: Both the audio circuit and the additional control circuits are fully digital. The hearing professional programs the hearing aid with an external computer temporarily connected to the device and can adjust all processing characteristics on an individual basis. Fully digital circuitry allows implementation of many additional features not possible with analog circuitry, can be used in all styles of hearing aids and is the most flexible; for example, digital hearing aids can be programmed to amplify certain frequencies more than others, and can provide better sound quality than analog hearing aids. Fully digital hearing aids can be programmed with multiple programs that can be invoked by the wearer, or that operate automatically and adaptively. These programs reduce acoustic feedback (whistling), reduce background noise, detect and automatically accommodate different listening environments (loud vs soft, speech vs music, quiet vs noisy, etc.), control additional components such as multiple microphones to improve spatial hearing, transpose frequencies (shift high frequencies that a wearer may not hear to lower frequency regions where hearing may be better), and implement many other features. Fully digital circuitry also allows control over wireless transmission capability for both the audio and the control circuitry. Control signals in a hearing aid on one ear can be sent wirelessly to the control circuitry in the hearing aid on the opposite ear to ensure that the audio in both ears is either matched directly or that the audio contains intentional differences that mimic the differences in normal binaural hearing to preserve spatial hearing ability. Audio signals can be sent wirelessly to and from external devices through a separate module, often a small device worn like a pendant and commonly called a \u201cstreamer\u201d, that allows wireless connection to yet other external devices. This capability allows optimal use of mobile telephones, personal music players, remote microphones and other devices. With the addition of speech recognition and internet capability in the mobile phone, the wearer has optimal communication ability in many more situations than with hearing aids alone. This growing list includes voice activated dialing, voice activated software applications either on the phone or on the internet, receipt of audio signals from databases on the phone or on internet, or audio signals from television sets or from global positioning systems. The first practical, wearable, fully digital hearing aid was invented by Maynard Engebretson, Robert E Morley, Jr. and Gerald R Popelka. Their work resulted in US Patent 4,548,082, \"Hearing aids, signal supplying apparatus, systems for compensating hearing deficiencies, and methods\" by A Maynard Engebretson, Robert E Morley, Jr. and Gerald R Popelka, filed in 1984. This patent formed the basis of all subsequent fully digital hearing aids from all manufacturers, including those produced currently.",
            "score": 153.4221190214157
        },
        {
            "docid": "7527647_17",
            "document": "Binaural fusion . The auditory space of binaural hearing is constructed based on the analysis of differences in two different binaural cues in the horizontal plane: sound level, or ILD, and arrival time at the two ears, or ITD, which allow for the comparison of the sound heard at each eardrum. ITD is processed in the MSO and results from sounds arriving earlier at one ear than the other; this occurs when the sound does not arise from directly in front or directly behind the hearer. ILD is processed in the LSO and results from the shadowing effect that is produced at the ear that is farther from the sound source. Outputs from the SOC are targeted to the dorsal nucleus of the lateral lemniscus as well as the IC.",
            "score": 184.40664196014404
        }
    ],
    "r": [
        {
            "docid": "32116125_4",
            "document": "Amblyaudia . Amblyaudia is a deficit in binaural integration of environmental information entering the auditory system. It is a disorder related to brain organization and function rather than what is typically considered a \u201chearing loss\u201d (damage to the cochlea). It may be genetic or developmentally acquired or both. When animals are temporarily deprived of hearing from an early age, profound changes occur in the brain. Specifically, cell sizes in brainstem nuclei are reduced, the configuration of brainstem dendrites are altered and neurons respond in different ways to sounds presented to both the deprived and non-deprived ears (in cases of asymmetric deprivation). This last point is particularly important for listening tasks that require inputs from two ears to perform well. There are multiple auditory functions that rely on the computation of well calibrated inputs from the two ears. Chief among these is the ability to localize sound sources and separate what we want to hear from a background of noise. In the brainstem, the auditory system compares the timing and levels of sounds between the two ears to encode the location of sound sources (sounds that originate from our right as opposed to left side are louder and arrive earlier in our right ear). This ability to separate sound sources not only helps us locate the trajectories of moving objects, but also to separate different sound sources in noisy environments.",
            "score": 234.03016662597656
        },
        {
            "docid": "1247165_4",
            "document": "Weber test . The Weber and the Rinne test ( ) are typically performed together with the results of each combined to determine the location and nature of any hearing losses detected. In the Weber test a vibrating tuning fork (Typically 256\u00a0Hz or 512\u00a0Hz used for Weber vibration test; 512\u00a0Hz used for Rinne hearing test) is placed in the middle of the forehead, above the upper lip under the nose over the teeth, or on top of the head equi-distant from the patient's ears on top of thin skin in contact with the bone. The patient is asked to report in which ear the sound is heard louder. A normal weber test has a patient reporting the sound heard equally in both sides. In an affected patient, if the defective ear hears the Weber tuning fork louder, the finding indicates a conductive hearing loss in the defective ear. In an affected patient, if the normal ear hears the tuning fork sound better, there is sensorineural hearing loss on the other (defective) ear. However, the aforegoing presumes one knows in advance which ear is defective and which is normal (such as the patient telling the clinician that they cannot hear as well in one ear versus the other) and the testing is being done to characterize the type, conductive or sensorineural, of hearing loss that is occurring. In the case where the patient is unaware or has acclimated to their hearing loss, the clinician has to use the Rinne test in conjunction with the Weber to characterize and localize any deficits. That is, an abnormal Weber test is only able to tell the clinician that there is a conductive loss in the ear which hears better or that there is a sensorineural loss in the ear which does not hear as well.",
            "score": 231.04322814941406
        },
        {
            "docid": "6894544_2",
            "document": "Noise-induced hearing loss . Noise-induced hearing loss (NIHL) is hearing impairment resulting from exposure to loud sound. People may have a loss of perception of a narrow range of frequencies, impaired cognitive perception of sound including sensitivity to sound or ringing in the ears. When exposure to hazards such as noise occur at work and is associated with hearing loss, it is referred to as occupational hearing loss.  Hearing may deteriorate gradually from chronic and repeated noise exposure, such as to loud music or background noise, or suddenly, from exposure to impulse noise (a short high intensity noise), such as a gunshot or airhorn. In both types, loud sound overstimulates delicate hearing cells, leading to the permanent injury or death of the cells. Once lost this way, hearing cannot be restored in humans.  There are a variety of prevention strategies available to avoid or reduce hearing loss. Lowering the volume of sound at its source, limiting the time of exposure and physical protection can reduce the impact of excessive noise. If not prevented, hearing loss can be managed through assistive devices and cognitive therapies. The largest burden of NIHL has been through occupational exposures; however, noise-induced hearing loss can also be due to unsafe recreational, residential, social and military service-related noise exposures. It is estimated that 15% of young people are exposed to sufficient leisure noises (i.e. concerts, sporting events, daily activities, personal listening devices, etc.) to cause NIHL. There is not a limited list of noise sources that can cause hearing loss; rather, it is important to understand that exposure to excessively high decibel (dB) levels from any sound source over time, can cause hearing loss. The first symptom of NIHL may be difficulty hearing a conversation against a noisy background. The effect of hearing loss on speech perception has two components. The first component is the loss of audibility, which may be perceived as an overall decrease in volume. Modern hearing aids compensate this loss with amplification. The second component is known as \u201cdistortion\" or \u201cclarity loss\u201d due to selective frequency loss.\u201d Consonants, due to their higher frequency, are typically affected first. For example, the sounds \u201cs\u201d and \u201ct\u201d are often difficult to hear for those with hearing loss, affecting clarity of speech. NIHL can affect either one or both ears. Monaural hearing loss causes problems with directional hearing, affecting the ability to localize sound.",
            "score": 221.77146911621094
        },
        {
            "docid": "563743_5",
            "document": "Hearing test . A complete hearing evaluation involves several other tests as well. In order to determine what kind of hearing loss is present, a bone conduction hearing test is administered. In this test, a vibrating tuning fork is placed behind the ear, on the mastoid process. When the patient can no longer feel/hear the vibration, the tuning fork is held in front of the ear; the patient should once more be able to hear a ringing sound. If they cannot, there is conductive hearing loss in that ear. Additionally, the tuning fork is placed on the forehead. The patient is then asked if the sound is localised in the centre of the head or whether it is louder in either ear. If there is conductive hearing loss, it is likely to be louder in the affected ear; if there is sensorineural hearing loss, it will be quieter in the affected ear. This test helps the audiologist determine whether the hearing loss is conductive (caused by problems in the outer or middle ear) or sensorineural (caused by problems in the cochlea, the sensory organ of hearing) or neural - caused by a problem in the auditory nerve or auditory pathways/cortex of the brain.",
            "score": 217.17022705078125
        },
        {
            "docid": "32105732_4",
            "document": "Spatial hearing loss . Sound streams arriving from the left or right (the horizontal plane) are localised primarily by the small time differences of the same sound arriving at the two ears. A sound straight in front of the head is heard at the same time by both ears. A sound to the side of the head is heard approximately 0.0005 seconds later by the ear furthest away. A sound halfway to one side is heard approximately 0.0003 seconds later. This is the interaural time difference (ITD) cue and is measured by signal processing in the two central auditory pathways that begin after the cochlea and pass through the brainstem and mid-brain. Some of those with spatial hearing loss are unable to process ITD (low frequency) cues.",
            "score": 216.17454528808594
        },
        {
            "docid": "3016334_3",
            "document": "Bone-anchored hearing aid . Two of the causes of hearing loss are lack of function in the inner ear (cochlea) and when the sound has problems in reaching the nerve cells of the inner ear. Example of the first include age-related hearing loss and hearing loss due to noise exposure. A patient born without external ear canals is an example of the latter for which a conventional hearing aid with a mould in the ear canal opening would not be effective. Some with this condition have normal inner ear function, as the external ear canal and the inner ear are developed at different stages during pregnancy. With normal inner anatomy, sound conducted by the skull bone improves hearing.",
            "score": 212.1797637939453
        },
        {
            "docid": "1247165_6",
            "document": "Weber test . In a normal patient, the Weber tuning fork sound is heard equally loudly in both ears, with no one ear hearing the sound louder than the other (lateralization). Similarly, a patient with symmetrical hearing loss will hear the Weber tuning fork sound equally well, with diagnostic utility only in asymmetric (one-sided) hearing losses. In a patient with hearing loss, the Weber tuning fork sound is heard louder in one ear (lateralization) than the other. This clinical finding should be confirmed by repeating the procedure and having the patient occlude one ear with a finger; the sound should be heard best in the occluded ear.",
            "score": 209.2355499267578
        },
        {
            "docid": "1247165_8",
            "document": "Weber test . A patient with a unilateral conductive hearing loss would hear the tuning fork loudest in the affected ear. This is because the ear with the conductive hearing loss is only receiving input from the bone conduction and no air conduction, and the sound is perceived as louder in that ear. This finding is because the conduction problem of the middle ear (incus, malleus, stapes, and eustachian tube) masks the ambient noise of the room, while the well-functioning inner ear (cochlea with its basilar membrane) picks the sound up via the bones of the skull, causing it to be perceived as a louder sound in the affected ear. Another theory, however, is based on the occlusion effect described by Tonndorf et al. in 1966. Lower frequency sounds (as made by the 256\u00a0Hz fork) that are transferred through the bone to the ear canal escape from the canal. If an occlusion is present, the sound cannot escape and appears louder on the ear with the conductive hearing loss.",
            "score": 208.37060546875
        },
        {
            "docid": "1379269_3",
            "document": "Musical acoustics . Whenever two different pitches are played at the same time, their sound waves interact with each other \u2013 the highs and lows in the air pressure reinforce each other to produce a different sound wave. Any repeating sound wave that is not a sine wave can be modeled by many different sine waves of the appropriate frequencies and amplitudes (a frequency spectrum). In humans the hearing apparatus (composed of the ears and brain) can usually isolate these tones and hear them distinctly. When two or more tones are played at once, a variation of air pressure at the ear \"contains\" the pitches of each, and the ear and/or brain isolate and decode them into distinct tones.",
            "score": 207.9938507080078
        },
        {
            "docid": "25663206_13",
            "document": "Psychoacoustics . Sound localization is the process of determining the location of a sound source. The brain utilizes subtle differences in loudness, tone and timing between the two ears to allow us to localize sound sources. Localization can be described in terms of three-dimensional position: the azimuth or horizontal angle, the zenith or vertical angle, and the distance (for static sounds) or velocity (for moving sounds). Humans, as most four-legged animals, are adept at detecting direction in the horizontal, but less so in the vertical due to the ears being placed symmetrically. Some species of owls have their ears placed asymmetrically, and can detect sound in all three planes, an adaption to hunt small mammals in the dark.",
            "score": 205.32037353515625
        },
        {
            "docid": "44267014_6",
            "document": "Real ear measurement . The traditional method of real ear measurement is known as insertion gain, which is the difference between the sound pressure level measured near the ear drum with a hearing aid in place, and the sound pressure level measured in the unaided ear. First a measurement is made with the probe tube in the open ear (Real Ear Unaided Response, or REUR), then a second one is made using the same test signal with the hearing aid in place and turned on (Real Ear Aided Response, or REAR). The difference between these two results is the insertion gain. This gain can be matched to targets produced by various prescriptive formula based on the patient's audiogram or individual hearing loss.",
            "score": 204.84121704101562
        },
        {
            "docid": "3407405_4",
            "document": "Nonsyndromic deafness . Most forms of nonsyndromic deafness are associated with permanent hearing loss caused by damage to structures in the inner ear. The inner ear consists of three parts: a snail-shaped structure called the cochlea that helps process sound, nerves that send information from the cochlea to the brain, and structures involved with balance. Loss of hearing caused by changes in the inner ear is called sensorineural deafness. Hearing loss that results from changes in the middle ear is called conductive hearing loss. The middle ear contains three tiny bones that help transfer sound from the eardrum to the inner ear. Some forms of nonsyndromic deafness involve changes in both the inner ear and the middle ear; this combination is called mixed hearing loss.",
            "score": 203.75184631347656
        },
        {
            "docid": "1374343_19",
            "document": "Audiometry . Apart from testing hearing, part of the function of audiometry is in assessing or evaluating hearing from the test results. The most commonly used assessment of hearing is the determination of the threshold of audibility, i.e. the level of sound required to be just audible. This level can vary for an individual over a range of up to 5 decibels from day to day and from determination to determination, but it provides an additional and useful tool in monitoring the potential ill effects of exposure to noise. Before carrying out a hearing test, it is important to obtain information about the person\u2019s past medical history, not only concerning the ears but also other conditions which may have a bearing on possible hearing loss detected by an audiometric test. Hearing loss may be unilateral or bilateral, and bilateral hearing loss may not be symmetrical. The most common types of hearing loss, due to age and noise exposure, are usually bilateral and symmetrical. Wax in the ear can also cause hearing loss, so the ear should be examined to see if syringing is needed; also to determine if the eardrum has suffered any damage which may reduce the ability of sound to be transmitted to the middle ear.",
            "score": 202.7622528076172
        },
        {
            "docid": "32116125_2",
            "document": "Amblyaudia . Amblyaudia (amblyos- blunt; audia-hearing) is a term coined by Dr. Deborah Moncrieff from the University of Pittsburgh to characterize a specific pattern of performance from dichotic listening tests. Dichotic listening tests are widely used to assess individuals for binaural integration, a type of auditory processing skill. During the tests, individuals are asked to identify different words presented simultaneously to the two ears. Normal listeners can identify the words fairly well and show a small difference between the two ears with one ear slightly dominant over the other. For the majority of listeners, this small difference is referred to as a \"right-ear advantage\" because their right ear performs slightly better than their left ear. But some normal individuals produce a \"left-ear advantage\" during dichotic tests and others perform at equal levels in the two ears. Amblyaudia is diagnosed when the scores from the two ears are significantly different with the individual's dominant ear score much higher than the score in the non-dominant ear  Researchers interested in understanding the neurophysiological underpinnings of amblyaudia consider it to be a brain based hearing disorder that may be inherited or that may result from auditory deprivation during critical periods of brain development. Individuals with amblyaudia have normal hearing sensitivity (in other words they hear soft sounds) but have difficulty hearing in noisy environments like restaurants or classrooms. Even in quiet environments, individuals with amblyaudia may fail to understand what they are hearing, especially if the information is new or complicated. Amblyaudia can be conceptualized as the auditory analog of the better known central visual disorder amblyopia. The term \u201clazy ear\u201d has been used to describe amblyaudia although it is currently not known whether it stems from deficits in the auditory periphery (middle ear or cochlea) or from other parts of the auditory system in the brain, or both. A characteristic of amblyaudia is suppression of activity in the non-dominant auditory pathway by activity in the dominant pathway which may be genetically determined and which could also be exacerbated by conditions throughout early development.",
            "score": 201.79769897460938
        },
        {
            "docid": "3246329_10",
            "document": "Hearing range . Cats have excellent hearing and can detect an extremely broad range of frequencies. They can hear higher-pitched sounds than humans or most dogs, detecting frequencies from 55\u00a0Hz up to 79\u00a0kHz. Cats do not use this ability to hear ultrasound for communication but it is probably important in hunting, since many species of rodents make ultrasonic calls. Cat hearing is also extremely sensitive and is among the best of any mammal, being most acute in the range of 500\u00a0Hz to 32\u00a0kHz. This sensitivity is further enhanced by the cat's large movable outer ears (their \"pinnae\"), which both amplify sounds and help a cat sense the direction from which a noise is coming. The hearing ability of a dog is dependent on breed and age, though the range of hearing is usually around 67\u00a0Hz to 45\u00a0kHz. As with humans, some dog breeds' hearing ranges narrow with age, such as the German shepherd and miniature poodle. When dogs hear a sound, they will move their ears towards it in order to maximise reception. In order to achieve this, the ears of a dog are controlled by at least 18 muscles, which allow the ears to tilt and rotate. The ear's shape also allows the sound to be heard more accurately. Many breeds often have upright and curved ears, which direct and amplify sounds.",
            "score": 201.64447021484375
        },
        {
            "docid": "39200136_12",
            "document": "Hans Wallach . In a series of papers Wallach explored the ability of humans to locate sounds in the median plane \u2013 that is, to determine whether a sound comes from a source at the same elevation as the ears or from a source that is higher or lower, or even in back of the head. Binaural sound cues, including the phasing or time of the sound\u2019s arrival at each ear and the sound\u2019s relative intensity at the two ears (known respectively as ITD and ILD) enable a listener to determine a sound\u2019s lateral location (whether it is on the left, right, or straight ahead). But two sounds at different elevations can present identical ITD and ILD information to the ears, and so binaural cues to a stationary ear do not suffice to identify a sound\u2019s location in the median plane. Monaural cues that depend on the shape of the head and the structure of the external ear help with vertical localization, but binaural cues also play a part if the head is not stationary.",
            "score": 200.29183959960938
        },
        {
            "docid": "2369307_6",
            "document": "Microtia . The hearing loss associated with congenital aural atresia is a conductive hearing loss\u2014hearing loss caused by inefficient conduction of sound to the inner ear. Essentially, children with aural atresia have hearing loss because the sound cannot travel into the (usually) healthy inner ear\u2014there is no ear canal, no eardrum, and the small ear bones (malleus/hammer, incus/anvil, and stapes/stirrup) are underdeveloped. \"Usually\" is in parentheses because rarely, a child with atresia also has a malformation of the inner ear leading to a sensorineural hearing loss (as many as 19% in one study). Sensorineural hearing loss is caused by a problem in the inner ear, the cochlea. Sensorineural hearing loss is not correctable by surgery, but properly fitted and adjusted hearing amplification (hearing aids) generally provide excellent rehabilitation for this hearing loss. If the hearing loss is severe to profound in both ears, the child may be a candidate for a cochlear implant (beyond the scope of this discussion).",
            "score": 199.9512481689453
        },
        {
            "docid": "41087200_8",
            "document": "Perceptual-based 3D sound localization . Interaural level differences (ILD) represents the difference in sound pressure level reaching the two ears. They provide salient cues for localizing high-frequency sounds in space, and populations of neurons that are sensitive to ILD are found at almost every synaptic level from brain stem to cortex. These cells are predominantly excited by stimulation of one ear and predominantly inhibited by stimulation of the other ear, such that the magnitude of their response is determined in large part by the intensities at the 2 ears. This gives rise to the concept of resonant damping. Interaural level difference (ILD) is best for high frequency sounds because low frequency sounds are not attenuated much by the head. ILD (also known as Interaural Intensity Difference) arises when the sound source is not centred, the listener's head partially shadows the ear opposite to the source, diminishing the intensity of the sound in that ear (particularly at higher frequencies). The pinnae filters the sound in a way that is directionally dependent. This is particularly useful in determining if a sound comes from above, below, in front, or behind.",
            "score": 199.45680236816406
        },
        {
            "docid": "39265695_7",
            "document": "Stimulus filtering . Female flies of the genus \"Ormia ochracea\" possess organs in their bodies that can detect frequencies of cricket sounds from meters away. This process is important for the survival of their species because females will lay their first instar larvae into the body of the cricket, where they will feed and molt for approximately seven days. After this period, the larvae grow into flies and the cricket usually perishes. Researchers were puzzled about how precise hearing ability could arise from a small ear structure. Normal animals detect and locate sounds using the interaural time difference (ITD) and the interaural level difference (ILD). The ITD is the difference in the time it takes sound to reach the ear. ILD is the difference in sound intensity measure between both ears. At maximum, the ITD would only reach about 1.5 microseconds and the ILD would be less than one decibel. These small values make it hard to sense the differences. To solve these issues, researchers studied the mechanical aspects of flies\u2019 ears. They found that they have a presternum structure linking both tympanal membranes that is critical in detecting sound and localization. The structure acts as a lever by transferring and amplifying vibrational energy between the membranes. After sound hits the membranes at different amplitudes, the presternum sets up symmetrical vibration modes through bending and rocking. This effect helps the nervous system distinguish which side the sound is coming from. Because the presternum acts as an intertympanal bridge, the ITD is increased from 1.5 us to 55 us and the ILD is increased from less than one decibel to over 10 decibels.",
            "score": 199.35885620117188
        },
        {
            "docid": "24555485_12",
            "document": "Miracle-Ear . In 1990, the Miracle-Ear Children\u2019s Foundation was founded to provide free hearing aids and services to children for families who could not afford hearing aids. The foundation donated more than 6,500 hearing aids to over 4,100 children in the U.S. In 2013, the Miracle-Ear Children\u2019s Foundation was transformed into the Miracle-Ear Foundation, in order to provide free hearing aids to both adults and children in need. In 2014, the Miracle-Ear Foundation initiated and co-sponsored \"One Day Without Sound\", which encourages hearing persons to remove sound from their lives for one day in order to empathize with hearing loss.",
            "score": 198.8804473876953
        },
        {
            "docid": "5442380_17",
            "document": "Sensory cue . Unless a sound is directly in front of or behind the individual, the sound stimuli will have a slightly different distance to travel to reach each ear. This difference in distance causes a slight delay in the time the signal is perceived by each ear. The magnitude of the interaural time difference is greater the more the signal comes from the side of the head. Thus, this time delay allows humans to accurately predict the location of incoming sound cues. Interaural level difference is caused by the difference in sound pressure level reaching the two ears. This is because the head blocks the sound waves for the further ear, causing less intense sound to reach it. This level difference between the two ears allows humans to accurately predict azimuth of an auditory signal. This effect only occurs at sounds that are high frequency.",
            "score": 198.80630493164062
        },
        {
            "docid": "101970_2",
            "document": "Tinnitus . Tinnitus is the hearing of sound when no external sound is present. While often described as a ringing, it may also sound like a clicking, hiss or roaring. Rarely, unclear voices or music are heard. The sound may be soft or loud, low pitched or high pitched and appear to be coming from one ear or both. Most of the time, it comes on gradually. In some people, the sound causes depression or anxiety and can interfere with concentration. Tinnitus is not a disease but a symptom that can result from a number of underlying causes. One of the most common causes is noise-induced hearing loss. Other causes include ear infections, disease of the heart or blood vessels, M\u00e9ni\u00e8re's disease, brain tumors, emotional stress, exposure to certain medications, a previous head injury, and earwax. It is more common in those with depression. The diagnosis of tinnitus is usually based on the person's description. A number of questionnaires exist that may help to assess how much tinnitus is interfering with a person's life. The diagnosis is commonly supported by an audiogram and a neurological examination. If certain problems are found, medical imaging, such as with MRI, may be performed. Other tests are suitable when tinnitus occurs with the same rhythm as the heartbeat. Rarely, the sound may be heard by someone else using a stethoscope, in which case it is known as objective tinnitus. Spontaneous otoacoustic emissions, which are sounds produced normally by the inner ear, may also occasionally result in tinnitus. Prevention involves avoiding loud noise. If there is an underlying cause, treating it may lead to improvements. Otherwise, typically, management involves talk therapy. Sound generators or hearing aids may help some. As of 2013, there were no effective medications. It is common, affecting about 10\u201315% of people. Most, however, tolerate it well, and it is a significant problem in only 1\u20132% of people. The word tinnitus is from the Latin \"tinn\u012bre\" which means \"to ring\". Tinnitus can be perceived in one or both ears or in the head. It is the description of a noise inside a person\u2019s head in the absence of auditory stimulation. The noise can be described in many different ways. It is usually described as a ringing noise but, in some patients, it takes the form of a high-pitched whining, electric buzzing, hissing, humming, tinging or whistling sound or as ticking, clicking, roaring, \"crickets\" or \"tree frogs\" or \"locusts (cicadas)\", tunes, songs, beeping, sizzling, sounds that slightly resemble human voices or even a pure steady tone like that heard during a hearing test. It has also been described as a \"whooshing\" sound because of acute muscle spasms, as of wind or waves. Tinnitus can be intermittent or continuous: in the latter case, it can be the cause of great distress. In some individuals, the intensity can be changed by shoulder, head, tongue, jaw or eye movements. Most people with tinnitus have some degree of hearing loss.",
            "score": 198.76937866210938
        },
        {
            "docid": "14713486_8",
            "document": "Frog hearing and communication . Biologists believed that frogs ears are placed too close together to localize sound accurately. Frogs cannot hear short, high frequency sounds. Sound is localized by the time difference when the sound reaches each ear. The \u201cvibration spot\u201d near the lungs vibrates in response to sound, and may be used as an additional measure to localize from.",
            "score": 198.00047302246094
        },
        {
            "docid": "492229_15",
            "document": "Monochord . In audiology, the device is used to test for hearing loss and other disorders of the ear. The audiometer measures the ability to hear sounds at frequencies normally detectable by the human ear. Several test are usually conducted using the audiometer which will then be used to assess hearing ability. Results typically are recorded on a chart known as an audiogram.",
            "score": 197.72634887695312
        },
        {
            "docid": "41087200_3",
            "document": "Perceptual-based 3D sound localization . Human listeners combine information from two ears to localize and separate sound sources originating in different locations in a process called binaural hearing. The powerful signal processing methods found in the neural systems and brains of humans and other animals are flexible, environmentally adaptable, and take place rapidly and seemingly without effort. Emulating the mechanisms of binaural hearing can improve recognition accuracy and signal separation in DSP algorithms, especially in noisy environments. Furthermore, by understanding and exploiting biological mechanisms of sound localization, virtual sound scenes may be rendered with more perceptually relevant methods, allowing listeners to accurately perceive the locations of auditory events.",
            "score": 197.19808959960938
        },
        {
            "docid": "32116125_3",
            "document": "Amblyaudia . Children with amblyaudia experience difficulties in speech perception, particularly in noisy environments, sound localization, and binaural unmasking (using interaural cues to hear better in noise) despite having normal hearing sensitivity (as indexed through pure tone audiometry). These symptoms may lead to difficulty attending to auditory information causing many to speculate that language acquisition and academic achievement may be deleteriously affected in children with amblyaudia. A significant deficit in a child's ability to use and comprehend expressive language may be seen in children who lacked auditory stimulation throughout the critical periods of auditory system development. A child suffering from amblyaudia may have trouble in appropriate vocabulary comprehension and production and the use of past, present and future tenses. Amblyaudia has been diagnosed in many children with reported difficulties understanding and learning from listening and adjudicated adolescents are at a significantly high risk for amblyaudia (Moncrieff, et al., 2013, Seminars in Hearing). Families report the presence of amblyaudia in several individuals, suggesting that it may be genetic in nature. It is possible that abnormal auditory input during the first two years of life may increase a child\u2019s risk for amblyaudia, although the precise relationship between deprivation timing and development of amblyaudia is still unclear. Recurrent ear infections (otitis media) are the leading cause of temporary auditory deprivation in young children. During ear infection bouts, the quality of the signal that reaches the auditory regions of the brains of a subset of children with OM is degraded in both timing and magnitude. When this degradation is asymmetric (worse in one ear than the other) the binaural cues associated with sound localization can also be degraded. Aural atresia (a closed external auditory canal) also causes temporary auditory deprivation in young children. Hearing can be restored to children with ear infections and aural atresia through surgical intervention (although ear infections will also resolve spontaneously). Nevertheless, children with histories of auditory deprivation secondary to these diseases can experience amblyaudia for years after their hearing has been restored.",
            "score": 196.73577880859375
        },
        {
            "docid": "1555553_18",
            "document": "Unilateral hearing loss . When wearing stereo headphones, people with unilateral hearing loss can hear only one channel, hence the panning information (volume and time differences between channels) is lost; some instruments may be heard better than others if they are mixed predominantly to one channel, and in extreme cases of sound production, such as complete stereo separation or stereo-switching, only part of the composition can be heard; in games using 3D audio effects, sound may not be perceived appropriately due to coming to the disabled ear. This can be corrected by using settings in the software or hardware\u2014audio player, OS, amplifier or sound source\u2014to adjust balance to one channel (only if the setting downmixes sound from both channels to one), or there may be an option to outright downmix both channels to mono. Such settings may be available via the device or software's accessibility features. As hardware solutions, stereo-to-mono adapters may be available to receive mono sound in stereo headphones from a stereo sound source, or some monaural headsets for cellphones and VOIP communication may combine stereo sound to mono (though headphones for voice communication typically offer lower audio quality than headphones targeted for listening to music). From the standpoint of sound fidelity, sound information in downmixed mono channel will, in any case, differ from that in either of the source channels or what is perceived by a normal-hearing person, thus technically some audio quality is lost (for example, the same or slightly different sound occurrences in two channels, with time delay between them, will be merged to a sound in the mono channel that unavoidably cannot correspond to the intent of the sound producer); however, such loss is most probably unnoticeable, especially compared to other distortions inherent in sound reproduction, and to the person's problems from hearing loss.",
            "score": 196.6628875732422
        },
        {
            "docid": "1707270_8",
            "document": "Sensation (psychology) . The frequency, intensity, and complexity of sounds waves in the external world are detected by auditory receptors (cilia or hair cell receptors) in the ear. Different patterns of cilia movement lead to different neural codes, which ultimately lead to hearing different loudness, pitch, and timbre of sounds. Deafness or hearing loss may occur in one or both ears.",
            "score": 195.9422607421875
        },
        {
            "docid": "49604_48",
            "document": "Hearing loss . Conductive hearing loss is present when the sound is not reaching the inner ear, the cochlea. This can be due to external ear canal malformation, dysfunction of the eardrum or malfunction of the bones of the middle ear. The ear drum may show defects from small to total resulting in hearing loss of different degree. Scar tissue after ear infections may also make the ear drum dysfunction as well as when it is retracted and adherent to the medial part of the middle ear.",
            "score": 195.65586853027344
        },
        {
            "docid": "1187432_9",
            "document": "Conductive hearing loss . For basic screening, a conductive hearing loss can be identified using the Rinne test with a 256 Hz tuning fork. The Rinne test, in which a patient is asked to say whether a vibrating tuning fork is heard more loudly adjacent to the ear canal (air conduction) or touching the bine behind the ear (bone conduction), is negative indicating that bone conduction is more effective that air conduction. A normal, or positive, result, is when air conduction is more effective than bone conduction. With a one-sided conductive component the combined use of both the Weber and Rinne tests is useful. If the Weber test is used, in which a vibrating tuning fork is touched to the midline of the forehead, the person will hear the sound more loudly in the affected ear because background noise does not mask the hearing on this side. The following table compares sensorineural hearing loss to conductive:  Tympanometry, or acoustic immitance testing, is a simple objective test of the ability of the middle ear to transmit sound waves across it. This test is usually abnormal with conductive hearing loss.",
            "score": 195.56849670410156
        },
        {
            "docid": "1247165_2",
            "document": "Weber test . The Weber test is a quick screening test for hearing. It can detect unilateral (one-sided) conductive hearing loss (middle ear hearing loss) and unilateral sensorineural hearing loss (inner ear hearing loss). The test is named after Ernst Heinrich Weber (1795\u20131878). Conductive hearing ability is mediated by the middle ear composed of the ossicles: incus, malleus, stapes. Sensorineural hearing ability is mediated by the inner ear composed of the cochlea with its internal basilar membrane and attached cochlear nerve (cranial nerve VIII). The outer ear consisting of the pinna, ear canal, and ear drum or tympanic membrane transmits sounds to the middle ear but does not contribute to the conduction or sensorineural hearing ability save for hearing transmissions limited by cerumen impaction (wax collection in the ear canal).",
            "score": 194.08660888671875
        },
        {
            "docid": "658095_10",
            "document": "Olm . The sensory epithelia of the inner ear are very specifically differentiated, enabling the olm to receive sound waves in the water, as well as vibrations from the ground. The complex functional-morphological orientation of the sensory cells enables the animal to register the sound sources. As this animal stays neotenic throughout its long life span, it is only occasionally exposed to normal adult hearing in air, which is probably also possible for Proteus as in most salamanders. Hence, it would be of adaptive value in caves, with no vision available, to profit from underwater hearing by recognizing particular sounds and eventual localization of prey or other sound sources, i.e. acoustical orientation in general. The ethological experiments indicate that the best hearing sensitivity of \"Proteus\" is between 10\u00a0Hz and up to 15,000\u00a0Hz. The lateral line supplements inner ear sensitivity by registering low-frequency nearby water displacements.",
            "score": 194.01309204101562
        }
    ]
}