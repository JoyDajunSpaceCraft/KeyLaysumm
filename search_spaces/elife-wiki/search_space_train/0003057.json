{
    "q": [
        {
            "docid": "25146378_20",
            "document": "Functional specialization (brain) . Other researchers who provide evidence to support the theory of distributive processing include Anthony McIntosh and William Uttal, who question and debate localization and modality specialization within the brain. McIntosh's research suggests that human cognition involves interactions between the brain regions responsible for processes sensory information, such as vision, audition, and other mediating areas like the prefrontal cortex. McIntosh explains that modularity is mainly observed in sensory and motor systems, however, beyond these very receptors, modularity becomes \"fuzzier\" and you see the cross connections between systems increase. He also illustrates that there is an overlapping of functional characteristics between the sensory and motor systems, where these regions are close to one another. These different neural interactions influence each other, where activity changes in one area influence other connected areas. With this, McIntosh suggest that if you only focus on activity in one area, you may miss the changes in other integrative areas. Neural interactions can be measured using analysis of covariance in neuroimaging. McIntosh used this analysis to convey a clear example of the interaction theory of distributive processing. In this study, subjects learned that an auditory stimulus signalled a visual event. McIntosh found activation (an increase blood flow), in an area of the occipital cortex, a region of the brain involved in visual processing, when the auditory stimulus was presented alone. Correlations between the occipital cortex and different areas of the brain such as the prefrontal cortex, premotor cortex and superior temporal cortex showed a pattern of co-variation and functional connectivity.",
            "score": 97.24331021308899
        },
        {
            "docid": "35982062_6",
            "document": "Biased Competition Theory . There are two major neural pathways that process the information in the visual field; the ventral stream and the dorsal stream. The two pathways run in parallel and are both working simultaneously. The ventral stream is important for object recognition and often referred to as the \u201cwhat\u201d system of the brain; it projects to the inferior temporal cortex. The dorsal stream is important for spatial perception and performance and is referred to as the \u201cwhere\u201d system which projects to the posterior parietal cortex. According to the biased competition theory, an individual\u2019s visual system has limited capacity to process information about multiple objects at any given time. For example, if an individual was presented with two stimuli (objects) and was asked to identify attributes of each object at the same time, the individual\u2019s performance would be worse in comparison to if the objects were presented separately. This suggests multiple objects presented simultaneously in the visual field will compete for neural representation due to limited processing resources. Single cell recording studies conducted by Kastner and Ungerleider examined the neural mechanisms behind the biased competition theory. In their experiment the size of the receptive field's (RF) of neurons within the visual cortex were examined. A single visual stimulus was presented alone in a neuron\u2019s RF, followed with another stimulus presented simultaneously within the same RF. The single \u2018effective\u2019 stimuli produced a low firing rate, whereas the two stimuli presented together produced a high firing rate. The response to the paired stimuli was reduced. This suggests that when two stimuli are presented together within a neuron\u2019s RF, the stimuli are processed in a mutually suppressive manner, rather than being processed independently. This suppression process, according to Kastner and Ungerleider, occurs when two stimuli are presented together because they compete for neural representation, due to limited cognitive processing capacity. The RF experiment suggests that as the number of objects increase, the information available for each object will decrease due to increased neural workload (suppression), and decreased cognitive capacity. In order for an object in the visual field or RF be efficiently processed, there needs to be a way to bias these neurological resources towards the object. Attention prioritizes task relevant objects, biasing this process. For example, this bias can be towards an object which is currently attended to in the visual field or RF, or towards the object that is most relevant to one\u2019s behavior. Functional magnetic resonance imaging (fMRI) has shown that biased competition theory can explain the observed attention effects at a neuronal level. Attention effects bias the internal weight (strengthens connections) of task relevant features toward the attended object. This was shown by Reddy, Kanwisher, and van Rullen who found an increase in oxygenated blood to a specific neuron following a locational cue. Further neurological support comes from neurophysiological studies which have shown that attention results from Top-down biasing, which in turn influences neuronal spiking. In sum, external inputs affect the Top-down guidance of attention, which bias specific neurons in the brain.",
            "score": 101.09620559215546
        },
        {
            "docid": "5212945_3",
            "document": "Visual neuroscience . A recent study using Event-Related Potentials (ERPs) linked an increased neural activity in the occipito-temporal region of the brain to the visual categorization of facial expressions. Results focus on a negative peak in the ERP that occurs 170 milliseconds after the stimulus onset. This action potential, called the N170, was measured using electrodes in the occipito-temporal region, an area already known to be changed by face stimuli. Studying by using the EEG, and ERP methods allow for an extremely high temporal resolution of 4 milliseconds, which makes these kinds of experiments extremely well suited for accurately estimating and comparing the time it takes the brain to perform a certain function. Scientists used classification image techniques, to determine what parts of complex visual stimuli (such as a face) will be relied on when patients are asked to assign them to a category, or emotion. They computed the important features when the stimulus face exhibited one of five different emotions. Stimulus faces exhibiting fear had the distinguishing feature of widening eyes, and stimuli exhibiting happiness exhibited a change in the mouth to make a smile. Regardless of the expression of the stimuli's face, the region near the eyes affected the EEG before the regions near the mouth. This revealed a sequential, and predetermined order to the perception and processing of faces, with the eye being the first, and the mouth, and nose being processed after. This process of downward integration only occurred when the inferior facial features were crucial to the categorization of the stimuli. This is best explained by comparing what happens when participants were shown a face exhibiting fear, versus happiness. The N170 peaked slightly earlier for the fear stimuli at about 175 milliseconds, meaning that it took a participants less time to recognize the facial expression. This is expected because only the eyes need to be processed to recognize the emotion. However, when processing a happy expression, where the mouth is crucial to categorization, downward integration must take place, and thus the N170 peak occurred later at around 185 milliseconds. Eventually visual neuroscience aims to completely explain how the visual system processes all changes in faces as well as objects. This will give a complete view to how the world is constantly visually perceived, and may provide insight into a link between perception and consciousness.",
            "score": 113.47546017169952
        },
        {
            "docid": "35982062_8",
            "document": "Biased Competition Theory . Bottom-up processes are characterized by an absence of higher level direction in sensory processing. It primarily relies on sensory information and incoming sensory information is the starting point for all Bottom-up processing. Bottom-up refers to when a feature stands out in a visual search. This is commonly called the \u201cpop-out\u201d effect. Salient features like bright colors, movement and big objects make the object \u201cpop-out\u201d of the visual search. \u201cPop-out\u201d features can often attract attention without conscious processing. Objects that stand out are often given priority (bias) in processing. Bottom-up processing is data driven, and according to this stimuli are perceived on the basis of the data which is being experienced through the senses. Evidence suggests that simultaneously presented stimuli do in fact compete in order to be represented in the visual cortex, with stimuli mutually suppressing each other to gain this representation. This was examined by Reynolds and colleagues, who looked at the size of neurons\u2019 receptive field\u2019s within the visual cortex. It was found that the presentation of a single stimulus resulted in a low firing rate while two stimuli presented together resulted in a higher firing rate. Reynolds and colleagues also found that when comparing the neural response of an individually presented visual stimulus to responses gathered from simultaneously presented stimuli, the responses of the concurrent presented stimuli were less than the sum of the responses gathered when each stimuli was presented alone. This suggests that two stimuli presented together increase neural work load required for attention. This increased neural load creates suppressive processes and causes the stimuli to compete for neural representation in the brain. Proulx and Egeth predicted that brighter objects would bias attention in favor of that object. Another prediction is that larger objects would bias the attention in favor of that object. The experiment was a computer-based visual search task, where participants searched for a target among distractions. The results of the study suggested that when irrelevant stimuli were large or bright, attention was biased towards the irrelevant objects, prioritizing them for cognitive processing. This research shows the effects of Bottom-up (stimulus-driven) processing on biased competition theory.",
            "score": 84.29515278339386
        },
        {
            "docid": "33246145_4",
            "document": "Neural decoding . When looking at a picture, people's brains are constantly making decisions about what object they are looking at, where they need to move their eyes next, and what they find to be the most salient aspects of the input stimulus. As these images hit the back of the retina, these stimuli are converted from varying wavelengths to a series of neural spikes called action potentials. These pattern of action potentials are different for different objects and different colors; we therefore say that the neurons are encoding objects and colors by varying their spike rates or temporal pattern. Now, if someone were to probe the brain by placing electrodes in the primary visual cortex, they may find what appears to be random electrical activity. These neurons are actually firing in response to the lower level features of visual input, possibly the edges of a picture frame. This highlights the crux of the neural decoding hypothesis: that it is possible to reconstruct a stimulus from the response of the ensemble of neurons that represent it. In other words, it is possible to look at spike train data and say that the person or animal being recorded is looking at a red ball.",
            "score": 132.83154797554016
        },
        {
            "docid": "2213172_5",
            "document": "Minimally conscious state . Some areas of the brain that are correlated with the subjective experience of pain were activated in MCS patients when noxious stimulation was present. Positron emission tomography (PET) scans found increased blood flow to the secondary sensory cortex, posterior parietal cortex, premotor cortex, and the superior temporal cortex. The pattern of activation, however, was with less spatial extent. Some parts of the brain were less activated than normal patients during noxious stimulus processing. These were the posterior cingulate, medial prefrontal cortex, and the occipital cortex. Even though functional brain imaging can objectively measure changes in brain function during noxious stimulation, the role of different areas of the brain in pain processing is only partially understood. Furthermore, there is still the problem of the subjective experience. MCS patients by definition cannot consistently and reliably communicate their experiences. Even if they were able to answer the question \"are you in pain?\", there would not be a reliable response. Further clinical trials are needed to access the appropriateness of the use of analgesia in patients with MCS.",
            "score": 117.95693612098694
        },
        {
            "docid": "24978422_3",
            "document": "Visual adaptation . The aftereffects of exposure to a visual stimulus or pattern causes loss of sensitivity to that pattern and induces stimulus bias. An example of this phenomenon is the \"lilac chaser\", introduced by Jeremy Hinton. The stimulus here are lilac circles, that once removed, leave green circles that then become the most prominent stimulus. The fading of the lilac circles is due to a loss of sensitivity to that stimulus and the adaptation to the new stimulus. To experience the \"lilac chaser\" effect, the subject needs to fixate their eyes on the cross in the middle of the image, and after a while the effect will settle in. Visual coding, a process involved in visual adaptation, is the means by which the brain adapts to certain stimuli, resulting in a biased perception of those stimuli. This phenomenon is referred to as visual plasticity; the brain's ability to change and adapt according to certain, repeated stimuli, altering the way information is perceived and processed. The rate and strength of visual adaptation depends heavily on the number of stimuli presented simultaneously, as well as the amount of time for which the stimulus is present. Visual adaptation was found to be weaker when there were more stimuli present. Moreover, studies have found that stimuli can rival each other, which explains why higher numbers of simultaneous stimuli lead to lower stimulus adaptation. Studies have also found that visual adaptation can have a reversing effect; if the stimulus is absent long enough, the aftereffects of visual adaptation will subside. Studies have also shown that visual adaptation occurs in the early stages of processing.",
            "score": 73.28876197338104
        },
        {
            "docid": "3975854_2",
            "document": "Sensory neuroscience . Sensory neuroscience is a subfield of neuroscience which explores the anatomy and physiology of neurons that are part of sensory systems such as vision, hearing, and olfaction. Neurons in sensory regions of the brain respond to stimuli by firing one or more nerve impulses (action potentials) following stimulus presentation. How is information about the outside world encoded by the rate, timing, and pattern of action potentials? This so-called neural code is currently poorly understood and sensory neuroscience plays an important role in the attempt to decipher it. Looking at early sensory processing is advantageous since brain regions that are \"higher up\" (e.g. those involved in memory or emotion) contain neurons which encode more abstract representations. However, the hope is that there are unifying principles which govern how the brain encodes and processes information. Studying sensory systems is an important stepping stone in our understanding of brain function in general.",
            "score": 102.49124825000763
        },
        {
            "docid": "179092_18",
            "document": "Neurolinguistics . Electrophysiological techniques take advantage of the fact that when a group of neurons in the brain fire together, they create an electric dipole or current. The technique of EEG measures this electric current using sensors on the scalp, while MEG measures the magnetic fields that are generated by these currents. In addition to these non-invasive methods, electrocorticography has also been used to study language processing. These techniques are able to measure brain activity from one millisecond to the next, providing excellent \"temporal resolution\", which is important in studying processes that take place as quickly as language comprehension and production. On the other hand, the location of brain activity can be difficult to identify in EEG; consequently, this technique is used primarily to \"how\" language processes are carried out, rather than \"where\". Research using EEG and MEG generally focuses on event-related potentials (ERPs), which are distinct brain responses (generally realized as negative or positive peaks on a graph of neural activity) elicited in response to a particular stimulus. Studies using ERP may focus on each ERP's \"latency\" (how long after the stimulus the ERP begins or peaks), \"amplitude\" (how high or low the peak is), or \"topography\" (where on the scalp the ERP response is picked up by sensors). Some important and common ERP components include the N400 (a negativity occurring at a latency of about 400 milliseconds), the mismatch negativity, the early left anterior negativity (a negativity occurring at an early latency and a front-left topography), the P600, and the lateralized readiness potential.",
            "score": 101.14058876037598
        },
        {
            "docid": "41118642_7",
            "document": "Dynamic functional connectivity . One of the first methods ever used to analyze DFC was pattern analysis of fMRI images to show that there are patterns of activation in spatially separated brain regions that tend to have synchronous activity. It has become clear that there is a spatial and temporal periodicity in the brain that probably reflects some of the constant processes of the brain. Repeating patterns of network information have been suggested to account for 25\u201350% of the variance in fMRI BOLD data. These patterns of activity have primarily been seen in rats as a propagating wave of synchronized activity along the cortex. These waves have also been shown to be related to underlying neural activity, and has been shown to be present in humans as well as rats.",
            "score": 105.08491373062134
        },
        {
            "docid": "4231622_6",
            "document": "Inferior temporal gyrus . The light energy that comes from the rays bouncing off of an object is converted into chemical energy by the cells in the retina of the eye. This chemical energy is then converted into action potentials that are transferred through the optic nerve and across the optic chiasm, where it is first processed by the lateral geniculate nucleus of the thalamus. From there the information is sent to the primary visual cortex, region V1. It then travels from the visual areas in the occipital lobe to the parietal and temporal lobes via two distinct anatomical streams. These two cortical visual systems were classified by Ungerleider and Mishkin (1982, see two-streams hypothesis). One stream travels ventrally to the inferior temporal cortex (from V1 to V2 then through V4 to ITC) while the other travels dorsally to the posterior parietal cortex. They are labeled the \u201cwhat\u201d and \u201cwhere\u201d streams, respectively. The Inferior Temporal Cortex receives information from the ventral stream, understandably so, as it is known to be a region essential in recognizing patterns, faces, and objects.  The understanding at the single-cell level of the IT cortex and its role of utilizing memory to identify objects and or process the visual field based on color and form visual information is a relatively recent in neuroscience. Early research indicated that the cellular connections of the temporal lobe to other memory associated areas of the brain \u2013 namely the hippocampus, the amygdala, the prefrontal cortex, among others. These cellular connections have recently been found to explain unique elements of memory, suggesting that unique single-cells can be linked to specific unique types and even specific memories. Research into the single-cell understanding of the IT cortex reveals many compelling characteristics of these cells: single-cells with similar selectivity of memory are clustered together across the cortical layers of the IT cortex; the temporal lobe neurons have recently been shown to display learning behaviors and possibly relate to long-term memory; and, cortical memory within the IT cortex is likely to be enhanced over time thanks to the influence of the afferent-neurons of the medial-temporal region. Further research of the single-cells of the IT cortex suggests that these cells not only have a direct link to the visual system pathway but also are deliberate in the visual stimuli they respond to: in certain cases, the single-cell IT cortex neurons do not initiate responses when spots or slits, namely simple visual stimuli, are present in the visual field; however, when complicated objects are put in place, this initiates a response in the single-cell neurons of the IT cortex. This provides evidence that not only are the single-cell neurons of the IT cortex related in having a unique specific response to visual stimuli but rather that each individual single-cell neuron has a specific response to a specific stimuli. The same study also reveals how the magnitude of the response of these single-cell neurons of the IT cortex do not change due to color and size but are only influenced by the shape. This led to even more interesting observations where specific IT neurons have been linked to the recognition of faces and hands. This is very interesting as to the possibility of relating to neurological disorders of prosopagnosia and explaining the complexity and interest in the human hand. Additional research form this study goes into more depth on the role of \"face neurons\" and \"hand neurons\" involved in the IT cortex.  The significance of the single-cell function in the IT cortex is that it is another pathway in addition to the lateral geniculate pathway that processes most visual system: this raises questions about how does it benefit our visual information processing in addition to normal visual pathways and what other functional units are involved in additional visual information processing.",
            "score": 108.0415358543396
        },
        {
            "docid": "25140_10",
            "document": "Perception . The process of perception begins with an object in the real world, termed the \"distal stimulus\" or \"distal object\". By means of light, sound or another physical process, the object stimulates the body's sensory organs. These sensory organs transform the input energy into neural activity\u2014a process called \"transduction\". This raw pattern of neural activity is called the \"proximal stimulus\". These neural signals are transmitted to the brain and processed. The resulting mental re-creation of the distal stimulus is the \"percept\".",
            "score": 102.28331303596497
        },
        {
            "docid": "31075772_15",
            "document": "Thought identification . On 31 January 2012 Brian Pasley and colleagues of University of California Berkeley published their paper in PLoS Biology wherein subjects' internal neural processing of auditory information was decoded and reconstructed as sound on computer by gathering and analyzing electrical signals directly from subjects' brains. The research team conducted their studies on the superior temporal gyrus, a region of the brain that is involved in higher order neural processing to make semantic sense from auditory information. The research team used a computer model to analyze various parts of the brain that might be involved in neural firing while processing auditory signals. Using the computational model, scientists were able to identify the brain activity involved in processing auditory information when subjects were presented with recording of individual words. Later, the computer model of auditory information processing was used to reconstruct some of the words back into sound based on the neural processing of the subjects. However the reconstructed sounds were not of good quality and could be recognized only when the audio wave patterns of the reconstructed sound were visually matched with the audio wave patterns of the original sound that was presented to the subjects. However this research marks a direction towards more precise identification of neural activity in cognition.",
            "score": 97.69184947013855
        },
        {
            "docid": "35982062_9",
            "document": "Biased Competition Theory . A Top-down process is characterized by a high level of direction of sensory processing by more cognition; Top-down processing is based on pre-existing knowledge when interpreting sensory information. Top-down guidance of attention refers to when the properties of an object (i.e. color, shape) are activated and held in working memory to facilitate the visual search for that object. This controls visual search by guiding attention only to objects that could be the target and avoiding attention on irrelevant objects. Top-down processes are not a complete representation of the object but are coarse, which is why objects similar in color, shape or meaning are often attended to in the process of discriminating irrelevant objects. There is evidence that observers have Top-down control over the locations that will benefit from biased competition in spatial selection visual tasks. Evidence supports that observers can make voluntary decision about which locations are selected. or features that capture the attention in a stimulus-driven manner. Neurophysiology studies have showed that the neural mechanisms in Top-down processing are also seen in attention and working memory, suggesting Top-down processes play an important role in those functions as well. Additionally, Top-down processes can modulate Bottom-up processes by suppressing the \u201cpop-out\u201d features of Bottom-up processing from distracting from the visual search. fMRI studies have investigated the Top-down and Bottom-up processes involved in biased competition theory. Results of fMRI suggest that both Bottom-up and Top-down processes work in parallel to bias competition. Multiple studies have shown that stimuli in the visual field suppress each other when presented together, but not when each stimulus is presented alone. Kastner and colleagues also found that directing attention to the specific location of a stimulus reduces the suppressive effect. Increased activity in the visual cortex was also observed; this was the result of Top-down biasing due to the favoring of the attended location.",
            "score": 96.25266909599304
        },
        {
            "docid": "2860430_27",
            "document": "Neural oscillation . Spontaneous activity is brain activity in the absence of an explicit task, such as sensory input or motor output, and hence also referred to as resting-state activity. It is opposed to induced activity, i.e. brain activity that is induced by sensory stimuli or motor responses. The term \"ongoing brain activity\" is used in electroencephalography and magnetoencephalography for those signal components that are not associated with the processing of a stimulus or the occurrence of specific other events, such as moving a body part, i.e. events that do not form evoked potentials/evoked fields, or induced activity. Spontaneous activity is usually considered to be noise if one is interested in stimulus processing; however, spontaneous activity is considered to play a crucial role during brain development, such as in network formation and synaptogenesis. Spontaneous activity may be informative regarding the current mental state of the person (e.g. wakefulness, alertness) and is often used in sleep research. Certain types of oscillatory activity, such as alpha waves, are part of spontaneous activity. Statistical analysis of power fluctuations of alpha activity reveals a bimodal distribution, i.e. a high- and low-amplitude mode, and hence shows that resting-state activity does not just reflect a noise process. In case of fMRI, spontaneous fluctuations in the blood-oxygen-level dependent (BOLD) signal reveal correlation patterns that are linked to resting states networks, such as the default network. The temporal evolution of resting state networks is correlated with fluctuations of oscillatory EEG activity in different frequency bands.",
            "score": 86.6706292629242
        },
        {
            "docid": "34204639_2",
            "document": "Decoded neurofeedback . Decoded Neurofeedback (DecNef) is the process of inducing knowledge in a subject by increasing neural activation in predetermined regions of interest in the brain, such as their visual cortex. This is achieved by measuring neural activity in these regions via functional magnetic resonance imaging (FMRI), comparing this to the ideal pattern of neural activation in these regions (for the intended purpose), and giving subjects feedback on how close their current pattern of neural activity is to the ideal pattern. Without explicit knowledge of what they are supposed to be doing or thinking about, over time participants learn to induce this ideal pattern of neural activation. Corresponding to this, their 'knowledge' or way of thinking has been found to change accordingly.",
            "score": 104.95703315734863
        },
        {
            "docid": "27313901_2",
            "document": "Visual N1 . The visual N1 is a visual evoked potential, a type of event-related electrical potential (ERP), that is produced in the brain and recorded on the scalp. The N1 is so named to reflect the polarity and typical timing of the component. The \"N\" indicates that the polarity of the component is negative with respect to an average mastoid reference. The \"1\" originally indicated that it was the first negative-going component, but it now better indexes the typical peak of this component, which is around 150 to 200 milliseconds post-stimulus. The N1 deflection may be detected at most recording sites, including the occipital, parietal, central, and frontal electrode sites. Although, the visual N1 is widely distributed over the entire scalp, it peaks earlier over frontal than posterior regions of the scalp, suggestive of distinct neural and/or cognitive correlates. The N1 is elicited by visual stimuli, and is part of the visual evoked potential \u2013 a series of voltage deflections observed in response to visual onsets, offsets, and changes. Both the right and left hemispheres generate an N1, but the laterality of the N1 depends on whether a stimulus is presented centrally, laterally, or bilaterally. When a stimulus is presented centrally, the N1 is bilateral. When presented laterally, the N1 is larger, earlier, and contralateral to the visual field of the stimulus. When two visual stimuli are presented, one in each visual field, the N1 is bilateral. In the latter case, the N1's asymmetrical skewedness is modulated by attention. Additionally, its amplitude is influenced by selective attention, and thus it has been used to study a variety of attentional processes.",
            "score": 58.2790801525116
        },
        {
            "docid": "33702464_5",
            "document": "Extrastriate body area . The experiment had subjects view images of different objects, including faces (as a control group), body parts, animals, parts of the face and intimate objects. While viewing the images, the subjects were scanned with an fMRI to see what area of the brain was activated. Through the trials a compilation of the fMRI\u2019s was made. From this compilation image a specific region was determined to have increased activity when shown visual stimuli of body parts and even more activity when viewing whole bodies. There have been no studies involving brain damage to the EBA. Thus far, only scans of brain activity, as well as transcranial magnetic stimulation, have been used to study the EBA. To find the specific functions of the EBA, Comimo Urgesi, Giovanni Berlucchi and Salvatore M. Aglioti used repetitive transcranial magnetic stimulation (rTMS) to disrupt part of the brain, making the brain less responsive in the target area. The study used event-related rTMS to disrupt the EBA, resulting in inactivation of cortical areas. This inactivation caused a slower response time in discriminating body parts. The study used facial features and motorcycle parts as non human parts for control groups. The facial features and motorcycle body parts did not display any change in response time. The neural activity data shows the EBA handles some of the visual processing of human body and parts but is not related to the processing of the face or other objects.",
            "score": 128.32836174964905
        },
        {
            "docid": "35182952_4",
            "document": "Embodied language processing . The overlap between various semantic categories with sensory motor areas suggests that a common mechanism is used by neurons to process action, perception, and semantics. The correlation principle states that neurons that fire together, wire together. Also, neurons out of sync, delink. When an individual pronounces a word, the activation pattern for articulatory motor systems of the speaker leads to activation of auditory and somatosensory systems due to self-perceived sounds and movements. If a word meaning is grounded in the visual shapes of the objects, the word form circuit is active together with neural activity in the ventral-temporal visual stream related to processing of visual object information. Correlation learning links the word and object circuits, resulting in an embodied object-semantic relationship.",
            "score": 100.46707224845886
        },
        {
            "docid": "6147487_3",
            "document": "Neural coding . Neurons are remarkable among the cells of the body in their ability to propagate signals rapidly over large distances. They do this by generating characteristic electrical pulses called action potentials: voltage spikes that can travel down nerve fibers. Sensory neurons change their activities by firing sequences of action potentials in various temporal patterns, with the presence of external sensory stimuli, such as light, sound, taste, smell and touch. It is known that information about the stimulus is encoded in this pattern of action potentials and transmitted into and around the brain.",
            "score": 90.11556959152222
        },
        {
            "docid": "2363287_6",
            "document": "Visual learning . Various areas of the brain work together in a multitude of ways in order to produce the images that we see with our eyes and that are encoded by our brains. The basis of this work takes place in the visual cortex of the brain. The visual cortex is located in the occipital lobe of the brain and harbors many other structures that aid in visual recognition, categorization, and learning. One of the first things the brain must do when acquiring new visual information is recognize the incoming material. Brain areas involved in recognition are the inferior temporal cortex, the superior parietal cortex, and the cerebellum. During tasks of recognition, there is increased activation in the left inferior temporal cortex and decreased activation in the right superior parietal cortex. Recognition is aided by neural plasticity, or the brain's ability to reshape itself based on new information. Next the brain must categorize the material. The three main areas that are used when categorizing new visual information are the orbitofrontal cortex and two dorsolateral prefrontal regions which begin the process of sorting new information into groups and further assimilating that information into things that you might already know. After recognizing and categorizing new material entered into the visual field, the brain is ready to begin the encoding process \u2013 the process which leads to learning. Multiple brain areas are involved in this process such as the frontal lobe, the right extrastriate cortex, the neocortex, and again, the neostriatum. One area in particular, the limbic-diencephalic region, is essential for transforming perceptions into memories. With the coming together of tasks of recognition, categorization and learning; schemas help make the process of encoding new information and relating it to things you already know much easier. One can remember visual images much better when they can apply it to an already known schema. Schemas actually provide enhancement of visual memory and learning.",
            "score": 112.02169585227966
        },
        {
            "docid": "24965027_8",
            "document": "Cognitive neuroscience of visual object recognition . This model, proposed by Marr and Nishihara (1978), states that object recognition is achieved by matching 3-D model representations obtained from the visual object with 3-D model representations stored in memory as veridical shape precepts. Through the use of computer programs and algorithms, Yi Yungfeng (2009) was able to demonstrate the ability for the human brain to mentally construct 3D images using only the 2D images that appear on the retina. Their model also demonstrates a high degree of shape constancy conserved between 2D images, which allow the 3D image to be recognized. The 3-D model representations obtained from the object are formed by first identifying the concavities of the object, which separate the stimulus into individual parts. Recent research suggests that an area of the brain, known as the caudal intraparietal area (CIP), is responsible for storing the slant and tilt of a plan surface that allow for concavity recognition. Rosenburg et al. implanted monkeys with a scleral search coil for monitoring eye position while simultaneously recording single neuron activation from neurons within the CIP. During the experiment, monkeys sat 30\u00a0cm away from an LCD screen that displayed the visual stimuli. Binocular disparity cues were displayed on the screen by rendering stimuli as green-red anaglyphs and the slant-tilt curves ranged from 0 to 330. A single trial consisted of a fixation point and then the presentation of a stimulus for 1 second. Neuron activations were then recorded using the surgically inserted microelectrodes. These single neuron activations for specific concavities of objects lead to the discovery that each axis of an individual part of an object containing concavity are found in memory stores. Identifying the principal axis of the object assists in the normalization process via mental rotation that is required because only the canonical description of the object is stored in memory. Recognition is acquired when the observed object viewpoint is mentally rotated to match the stored canonical description.[11] An extension of Marr and Nishihara's model, the recognition-by-components theory, proposed by Biederman (1987), proposes that the visual information gained from an object is divided into simple geometric components, such as blocks and cylinders, also known as \"geons\" (geometric ions), and are then matched with the most similar object representation that is stored in memory to provide the object's identification (see Figure 1).",
            "score": 118.28580951690674
        },
        {
            "docid": "1764639_17",
            "document": "Levels-of-processing effect . Several brain imaging studies using positron emission tomography and functional magnetic resonance imaging techniques have shown that higher levels of processing correlate with more brain activity and activity in different parts of the brain than lower levels. For example, in a lexical analysis task, subjects showed activity in the left inferior prefrontal cortex only when identifying whether the word represented a living or nonliving object, and not when identifying whether or not the word contained an \"a\". Similarly, an auditory analysis task showed increased activation in the left inferior prefrontal cortex when subjects performed increasingly semantic word manipulations. Synaptic aspects of word recognition have been correlated with the left frontal operculum and the cortex lining the junction of the inferior frontal and inferior precentral sulcus. The self-reference effect also has neural correlates with a region of the medial prefrontal cortex, which was activated in an experiment where subjects analyzed the relevance of data to themselves. Specificity of processing is explained on a neurological basis by studies that show brain activity in the same location when a visual memory is encoded and retrieved, and lexical memory in a different location. Visual memory areas were mostly located within the bilateral extrastriate visual cortex.",
            "score": 96.89819836616516
        },
        {
            "docid": "31682069_2",
            "document": "Steady state topography . Steady State Topography (abbreviated SST) is a methodology for observing and measuring human brain activity that was first described by Richard Silberstein and co-workers in 1990. While SST has been principally used as a cognitive neuroscience research methodology it has also found commercial application in the field of neuromarketing and consumer neuroscience in such areas as brand communication, media research and entertainment. In a typical SST study, brain electrical activity (electroencephalogram or EEG) is recorded while participants view audio visual material and/or perform a psychological task. Simultaneously, a dim sinusoidal visual flicker is presented in the visual periphery. The sinusoidal flicker elicits an oscillatory brain electrical response known as the Steady State Visually Evoked Potential (SSVEP). Task related changes in brain activity in the vicinity of the recording site are then determined from SSVEP measurements at that site. One of the most important features of the SST methodology is the ability to measure variations in the delay (latency) between the stimulus and the SSVEP response over extended periods of time. This offers a unique window into brain function based on neural processing speed as opposed to the more common EEG amplitude indicators of brain activity. Three specific features of the SST methodology make it a useful technique in cognitive neuroscience research as well as neuroscience-based communication research.",
            "score": 73.634104013443
        },
        {
            "docid": "599917_31",
            "document": "Mental image . As cognitive neuroscience approaches to mental imagery continued, research expanded beyond questions of serial versus parallel or topographic processing to questions of the relationship between mental images and perceptual representations. Both brain imaging (fMRI and ERP) and studies of neuropsychological patients have been used to test the hypothesis that a mental image is the reactivation, from memory, of brain representations normally activated during the perception of an external stimulus. In other words, if perceiving an apple activates contour and location and shape and color representations in the brain\u2019s visual system, then imagining an apple activates some or all of these same representations using information stored in memory. Early evidence for this idea came from neuropsychology. Patients with brain damage that impairs perception in specific ways, for example by damaging shape or color representations, seem to generally to have impaired mental imagery in similar ways. Studies of brain function in normal human brains support this same conclusion, showing activity in the brain\u2019s visual areas while subjects imagined visual objects and scenes.",
            "score": 96.93593168258667
        },
        {
            "docid": "491622_7",
            "document": "Damasio's theory of consciousness . Damasio\u2019s definition of emotion is that of an unconscious reaction to any internal or external stimulus which activates neural patterns in the brain. \u2018Feeling\u2019 emerges as a still unconscious state which simply senses the changes affecting the Protoself due to the emotional state. These patterns develop into mental images, which then float into the organism\u2019s awareness. Put simply, consciousness is the feeling of knowing a feeling. When the organism becomes aware of the feeling that its bodily state (Protoself) is being affected by its experiences, or response to emotion, Core Consciousness is born. The brain continues to present nonverbal narrative sequence of images in the mind of the organism, based on its relationship to objects. An object in this context can be anything from a person, to a melody, to a neural image. Core consciousness is concerned only with the present moment, here and now. It does not require language or memory, nor can it reflect on past experiences or project itself into the future.",
            "score": 110.91184115409851
        },
        {
            "docid": "6147487_31",
            "document": "Neural coding . The mammalian gustatory system is useful for studying temporal coding because of its fairly distinct stimuli and the easily discernible responses of the organism. Temporally encoded information may help an organism discriminate between different tastants of the same category (sweet, bitter, sour, salty, umami) that elicit very similar responses in terms of spike count. The temporal component of the pattern elicited by each tastant may be used to determine its identity (e.g., the difference between two bitter tastants, such as quinine and denatonium). In this way, both rate coding and temporal coding may be used in the gustatory system \u2013 rate for basic tastant type, temporal for more specific differentiation. Research on mammalian gustatory system has shown that there is an abundance of information present in temporal patterns across populations of neurons, and this information is different from that which is determined by rate coding schemes. Groups of neurons may synchronize in response to a stimulus. In studies dealing with the front cortical portion of the brain in primates, precise patterns with short time scales only a few milliseconds in length were found across small populations of neurons which correlated with certain information processing behaviors. However, little information could be determined from the patterns; one possible theory is they represented the higher-order processing taking place in the brain.",
            "score": 99.14482927322388
        },
        {
            "docid": "2860430_32",
            "document": "Neural oscillation . Next to evoked activity, neural activity related to stimulus processing may result in induced activity. Induced activity refers to modulation in ongoing brain activity induced by processing of stimuli or movement preparation. Hence, they reflect an indirect response in contrast to evoked responses. A well-studied type of induced activity is amplitude change in oscillatory activity. For instance, gamma activity often increases during increased mental activity such as during object representation. Because induced responses may have different phases across measurements and therefore would cancel out during averaging, they can only be obtained using time-frequency analysis. Induced activity generally reflects the activity of numerous neurons: amplitude changes in oscillatory activity are thought to arise from the synchronization of neural activity, for instance by synchronization of spike timing or membrane potential fluctuations of individual neurons. Increases in oscillatory activity are therefore often referred to as event-related synchronization, while decreases are referred to as event-related desynchronization.",
            "score": 88.47433567047119
        },
        {
            "docid": "4275189_3",
            "document": "Joseph E. LeDoux . As explained in his 1996 book, The Emotional Brain, LeDoux developed an interest in the topic of emotion through his doctoral work with Michael Gazzaniga on split-brain patients in the mid-1970s. Because techniques for studying the human brain were limited at the time, he turned to studies of rodents where the brain could be studied in detail. He chose to focus on a simple behavioral model, Pavlovian fear conditioning. This procedure allowed him to follow the flow of information about a stimulus through the brain as it comes to control behavioral responses by way of sensory pathways to the amygdala, and gave rise to the notion of two sensory roads to the amygdala, with the \u201clow road\u201d being a quick and dirty subcortical pathway for rapid activity behavioral responses to threats and the \u201chigh road\u201d providing slower but highly processed cortical information. His work has shed light on how the brain detects and responds to threats, and how memories about such experiences are formed and stored through cellular, synaptic and molecular changes in the amygdala. A long-standing collaboration with NYU colleague Elizabeth Phelps has shown the validity of the rodent work for understanding threat processing in the human brain. LeDoux\u2019s work on amygdala processing of threats has helped understand exaggerated responses to threats in anxiety disorders in humans. For example, studies with Maria Morgan in the 1990s implicated the medial prefrontal cortex in the extinction of responses to threats and paved the way for understanding how exposure therapy reduces threat reactions in people with anxiety by way of interactions between the medial prefrontal cortex and the amygdala. Work conducted with Karim Nader and Glenn Schafe triggered a wave of interest in the topic of memory reconsolidation, a process by which memories become labile and subject to change after being retrieved. This led to the idea that trauma-related cues might be weakened in humans by blocking reconsolidation. Studies with Marie Mofils, Daniela Schiller and Phelps showed that extinction conducted shortly after triggering reconsolidation is considerably more effective in reducing the threat value of stimuli than conventional extinction, a finding that has proven useful in reducing drug relapse in humans.",
            "score": 103.4300594329834
        },
        {
            "docid": "649382_5",
            "document": "Pareidolia . Pareidolia can cause people to interpret random images, or patterns of light and shadow, as faces. A 2009 magnetoencephalography study found that objects perceived as faces evoke an early (165 ms) activation of the fusiform face area at a time and location similar to that evoked by faces, whereas other common objects do not evoke such activation. This activation is similar to a slightly faster time (130 ms) that is seen for images of real faces. The authors suggest that face perception evoked by face-like objects is a relatively early process, and not a late cognitive reinterpretation phenomenon. A functional magnetic resonance imaging (fMRI) study in 2011 similarly showed that repeated presentation of novel visual shapes that were interpreted as meaningful led to decreased fMRI responses for real objects. These results indicate that the interpretation of ambiguous stimuli depends upon processes similar to those elicited by known objects.",
            "score": 97.38788795471191
        },
        {
            "docid": "832632_5",
            "document": "David H. Hubel . The Hubel and Wiesel experiments greatly expanded the scientific knowledge of sensory processing. The partnership lasted over twenty years and became known as one of the most prominent research pairings in science. In one experiment, done in 1959, they inserted a microelectrode into the primary visual cortex of an anesthetized cat. They then projected patterns of light and dark on a screen in front of the cat. They found that some neurons fired rapidly when presented with lines at one angle, while others responded best to another angle. Some of these neurons responded to light patterns and dark patterns differently. Hubel and Wiesel called these neurons simple cells.\" Still other neurons, which they termed complex cells, detected edges regardless of where they were placed in the receptive field of the neuron and could preferentially detect motion in certain directions. These studies showed how the visual system constructs complex representations of visual information from simple stimulus features.",
            "score": 70.52271389961243
        },
        {
            "docid": "485309_16",
            "document": "Face perception . There are several parts of the brain that play a role in face perception. Rossion, Hanseeuw, and Dricot used BOLD fMRI mapping to identify activation in the brain when subjects viewed both cars and faces. The majority of BOLD fMRI studies use blood oxygen level dependent (BOLD) contrast to determine which areas of the brain are activated by various cognitive functions. They found that the occipital face area, located in the occipital lobe, the fusiform face area, the superior temporal sulcus, the amygdala, and the anterior/inferior cortex of the temporal lobe, all played roles in contrasting the faces from the cars, with the initial face perception beginning in the area and occipital face areas. This entire region links to form a network that acts to distinguish faces. The processing of faces in the brain is known as a \"sum of parts\" perception. However, the individual parts of the face must be processed first in order to put all of the pieces together. In early processing, the occipital face area contributes to face perception by recognizing the eyes, nose, and mouth as individual pieces. Furthermore, Arcurio, Gold, and James used BOLD fMRI mapping to determine the patterns of activation in the brain when parts of the face were presented in combination and when they were presented singly. The occipital face area is activated by the visual perception of single features of the face, for example, the nose and mouth, and preferred combination of two-eyes over other combinations. This research supports that the occipital face area recognizes the parts of the face at the early stages of recognition. On the contrary, the fusiform face area shows no preference for single features, because the fusiform face area is responsible for \"holistic/configural\" information, meaning that it puts all of the processed pieces of the face together in later processing. This theory is supported by the work of Gold et al. who found that regardless of the orientation of a face, subjects were impacted by the configuration of the individual facial features. Subjects were also impacted by the coding of the relationships between those features. This shows that processing is done by a summation of the parts in the later stages of recognition.",
            "score": 97.8298830986023
        }
    ],
    "r": [
        {
            "docid": "7082881_15",
            "document": "Enactivism . Alva No\u00eb in advocating an enactive view of perception sought to resolve how we perceive three-dimensional objects, on the basis of two-dimensional input. He argues that we perceive this solidity (or 'volumetricity') by appealing to patterns of sensorimotor expectations. These arise from our agent-active 'movements and interaction' with objects, or 'object-active' changes in the object itself. The solidity is perceived through our expectations and skills in knowing how the object's appearance would change with changes in how we relate to it. He saw all perception as an active exploration of the world, rather than being a passive process, something which happens to us.",
            "score": 139.2302703857422
        },
        {
            "docid": "1156527_8",
            "document": "Detection theory . Signal detection theory (SDT) is used when psychologists want to measure the way we make decisions under conditions of uncertainty, such as how we would perceive distances in foggy conditions. SDT assumes that the decision maker is not a passive receiver of information, but an active decision-maker who makes difficult perceptual judgments under conditions of uncertainty. In foggy circumstances, we are forced to decide how far away from us an object is, based solely upon visual stimulus which is impaired by the fog. Since the brightness of the object, such as a traffic light, is used by the brain to discriminate the distance of an object, and the fog reduces the brightness of objects, we perceive the object to be much farther away than it actually is (see also decision theory).",
            "score": 134.25942993164062
        },
        {
            "docid": "33246145_4",
            "document": "Neural decoding . When looking at a picture, people's brains are constantly making decisions about what object they are looking at, where they need to move their eyes next, and what they find to be the most salient aspects of the input stimulus. As these images hit the back of the retina, these stimuli are converted from varying wavelengths to a series of neural spikes called action potentials. These pattern of action potentials are different for different objects and different colors; we therefore say that the neurons are encoding objects and colors by varying their spike rates or temporal pattern. Now, if someone were to probe the brain by placing electrodes in the primary visual cortex, they may find what appears to be random electrical activity. These neurons are actually firing in response to the lower level features of visual input, possibly the edges of a picture frame. This highlights the crux of the neural decoding hypothesis: that it is possible to reconstruct a stimulus from the response of the ensemble of neurons that represent it. In other words, it is possible to look at spike train data and say that the person or animal being recorded is looking at a red ball.",
            "score": 132.83154296875
        },
        {
            "docid": "10567836_7",
            "document": "Ordinal numerical competence . Both behavioral research and brain-imaging research show distinct differences in the way \"exact\" arithmetic and \"approximate\" arithmetic are processed. Exact arithmetic is information that is precise and follows specific rules and patterns such as multiplication tables or geometric formulas, and approximate arithmetic is a general comparison between numbers such as the comparisons of greater than or less than. Research shows that exact arithmetic is language-based and processed in the left inferior frontal lobe. Approximate arithmetic is processed much differently in a different part of the brain. Approximate arithmetic is processed in the bilateral areas of the parietal lobes. This part of the brain processes visual information to understand how objects are spatially related to each other, for example, understanding that 10 of something is more than two of something. This difference in brain function can create a difference in how we experience certain types of arithmetic. Approximate arithmetic can be experienced as intuitive and exact arithmetic experienced as recalled knowledge.",
            "score": 130.49490356445312
        },
        {
            "docid": "51462681_3",
            "document": "Objective vision . This is the story of what's happening when you see a picture, even too fast, the brain's visual cortex recognizes what it sees immediately. The visual cortex has a critical job in processing and it's the most complex part of brain. The human brain is much more aware of how it solves complex problems such as playing chess or solving algebra equations, which is why computer programmers have had so much success building machines that emulate this type of activity. but when entities visionary system starts to convert the signals to image(actually the separated shapes and colors) to find a relation between brain's information and those images. The system actually is concentrating on the separable sections, this separation gives the brain a visionary system the excellence processing result, because with this method the system do not waste much time on processing non significant sections and signals. this operation in the Objective Vision project called objective processing and because the O.V. mission is around human visionary simulation, so the developer refers with Objective Vision.",
            "score": 129.81846618652344
        },
        {
            "docid": "1008632_19",
            "document": "Baddeley's model of working memory . There are two different pathways in the brain that control different functions of what is known inclusively as the visuo-spatial sketchpad. The sketchpad consists of the spatial short-term memory and the object memory. The spatial short-term memory is how one is able to learn and thus remember \"where\" they are in comparative representation to other objects. The object memory of the visuo-spatial sketchpad is essential in learning and remembering \"what\" an object is. It should be noted that the differences between these two differing visual abilities is due in large part because of different pathways of each of the abilities in the brain. The visual pathway in the brain that detects spatial representation of a person to and within their environment is the dorsal stream. The visual pathway that determines objects shapes, sizes, colors and other definitive characteristics is called the ventral stream. Each of these two streams runs independent of one another so that the visual system may process one without the other (like in brain damage for instance) or both simultaneously. The two streams do not depend on one another, so if one is functioning manipulatively, the other can still send its information through.",
            "score": 129.636962890625
        },
        {
            "docid": "15559385_11",
            "document": "Tactile discrimination . When a person has become blind, in order to \u201csee\u201d the world, their other senses become heightened. An important sense for the blind is their sense of touch, which becomes more frequently used to help them perceive the world. People that are blind have displayed that their visual cortices become more responsive to auditory and tactile stimulation. Braille allows the blind to be able to use their sense of touch to feel the roughness, and distance of various patterns to be used as a form of language. Within the brain, the activation of the occipital cortex is functionally relevant for tactile braille reading, as well as the somatosensory cortex. These various parts of the brain function in their own way, in which they each contribute to the effectiveness of how braille is read by the blind. People that are blind also rely heavily on Tactile Gnosis, Spatial discrimination, Graphesthesia, and Two-point discrimination. Essentially, the occipital cortex allows one to effectively make judgements on the distance of braille patterns, which is related to spatial discrimination. Meanwhile, the somatosensory cortex allows one to effectively make judgements on the roughness of braille patterns, which is related to two-point discrimination. The various visual areas in the brain are very essential for a blind person to read braille, just as much as it is for a person that has sight. Essentially, whether one is blind or not, the perception of objects that involves tactile discrimination is not impaired if one cannot see. When comparing people that are blind to people that have sight, the amount of activity within the their somatosensory and visual areas of the brain do differ. The activity in the somatosensory and visual areas are not as high in tactile gnosis for people that are not blind, and are more-so active for more visual related stimuli that does not involve touch. Nonetheless, there is a difference in these various areas within the brain when comparing the blind to the sighted, which is that shape discrimination causes a difference in brain activity, as well as tactile gnosis. The visual cortices of blind individuals are active during various vision related tasks including tactile discrimination, and the function of the cortices resemble the activity of adults with sight.",
            "score": 129.1795654296875
        },
        {
            "docid": "1316947_4",
            "document": "Ambiguous image . When we see an image, the first thing we do is attempt to organize all the parts of the scene into different groups. To do this, one of the most basic methods used is finding the edges. Edges can include obvious perceptions such as the edge of a house, and can include other perceptions that the brain needs to process deeper, such as the edges of a person's facial features. When finding edges, the brain's visual system detects a point on the image with a sharp contrast of lighting. Being able to detect the location of the edge of an object aids in recognizing the object. In ambiguous images, detecting edges still seems natural to the person perceiving the image. However, the brain undergoes deeper processing to resolve the ambiguity. For example, consider an image that involves an opposite change in magnitude of luminance between the object and the background (e.g. From the top, the background shifts from black to white, and the object shifts from white to black). The opposing gradients will eventually come to a point where there is an equal degree of luminance of the object and the background. At this point, there is no edge to be perceived. To counter this, the visual system connects the image as a whole rather than a set of edges, allowing one to see an object rather than edges and non-edges. Although there is no complete image to be seen, the brain is able to accomplish this because of its understanding of the physical world and real incidents of ambiguous lighting. In ambiguous images, an illusion is often produced from illusory contours. An illusory contour is a perceived contour without the presence of a physical gradient. In examples where a white shape appears to occlude black objects on a white background, the white shape appears to be brighter than the background, and the edges of this shape produce the illusory contours. These illusory contours are processed by the brain in a similar way as real contours. The visual system accomplishes this by making inferences beyond the information that is presented in much the same way as the luminance gradient.",
            "score": 128.71697998046875
        },
        {
            "docid": "37198050_8",
            "document": "Body part as an object . Tool-use is the manipulation, or use, of an object using the hands. It is one of the many skills that separate humans from animals. There are two factors used to explain tool-use in humans. First of all, part of tool-use knowledge is physical, meaning that it involves the actual manipulation of an object. The other type of knowledge is conceptual: using the physical (or active) knowledge of tool-use to add to the mental representation of the tool. Action schemas explain how we develop motor skills and performance of many complex motor-activities. Essentially, the more an object is manipulated and used, a greater schema is developed in the brain. Therefore, increased object use gives us a greater understanding of the object, which then facilitates our understanding of the object independent of its context. The strength of an action schema is significant in studying apraxia and BPO pantomimes, because there appears to be a disruption in the context of an object: an individual may understand the function of an object, but experiences difficulty using the object out of its context, or when it is not physically present.",
            "score": 128.6253662109375
        },
        {
            "docid": "14158261_23",
            "document": "Temporoparietal junction . Temporal order is the arrangement of events in time. By judging this, one can understand how we process things. Temporal order judgments require an individual to determine the relative timing between two spatially separate events. One study revealed that subjects had to determine the order of appearance of two objects as well as which object fit a certain property better. What was learned from this study was that when identifying the order or appearance, fMRI studies showed that there was bilateral activation of the TPJ. Meanwhile, when it comes to object characterization based on a property, it was noticed that there was only activation of the lTPJ. As such, it is evident that TPJ is involved in the \u201cwhen\u201d pathway of the brain.",
            "score": 128.35702514648438
        },
        {
            "docid": "33702464_5",
            "document": "Extrastriate body area . The experiment had subjects view images of different objects, including faces (as a control group), body parts, animals, parts of the face and intimate objects. While viewing the images, the subjects were scanned with an fMRI to see what area of the brain was activated. Through the trials a compilation of the fMRI\u2019s was made. From this compilation image a specific region was determined to have increased activity when shown visual stimuli of body parts and even more activity when viewing whole bodies. There have been no studies involving brain damage to the EBA. Thus far, only scans of brain activity, as well as transcranial magnetic stimulation, have been used to study the EBA. To find the specific functions of the EBA, Comimo Urgesi, Giovanni Berlucchi and Salvatore M. Aglioti used repetitive transcranial magnetic stimulation (rTMS) to disrupt part of the brain, making the brain less responsive in the target area. The study used event-related rTMS to disrupt the EBA, resulting in inactivation of cortical areas. This inactivation caused a slower response time in discriminating body parts. The study used facial features and motorcycle parts as non human parts for control groups. The facial features and motorcycle body parts did not display any change in response time. The neural activity data shows the EBA handles some of the visual processing of human body and parts but is not related to the processing of the face or other objects.",
            "score": 128.32835388183594
        },
        {
            "docid": "531432_37",
            "document": "Autostereogram . By focusing the lenses on a nearby autostereogram where patterns are repeated and by converging the eyeballs at a distant point behind the autostereogram image, one can trick the brain into seeing 3D images. If the patterns received by the two eyes are similar enough, the brain will consider these two patterns a match and treat them as coming from the same imaginary object. This type of visualization is known as \"wall-eyed viewing\", because the eyeballs adopt a wall-eyed convergence on a distant plane, even though the autostereogram image is actually closer to the eyes. Because the two eyeballs converge on a plane farther away, the perceived location of the imaginary object is behind the autostereogram. The imaginary object also appears bigger than the patterns on the autostereogram because of foreshortening.",
            "score": 128.2168426513672
        },
        {
            "docid": "32197396_2",
            "document": "Form perception . Form perception is the recognition of visual elements of objects, specifically those to do with shapes, patterns and previously identified important characteristics. An object is perceived by the retina as a two-dimensional image, but the image can vary for the same object in terms of the context with which it is viewed, the apparent size of the object, the angle from which it is viewed, how illuminated it is, as well as where it resides in the field of vision.  Despite the fact that each instance of observing an object leads to a unique retinal response pattern, the visual processing in the brain is capable of recognizing these experiences as analogous, allowing invariant object recognition. Visual processing occurs in a hierarchy with the lowest levels recognizing lines and contours, and slightly higher levels performing tasks such as completing boundaries and recognizing contour combinations. The highest levels integrate the perceived information to recognize an entire object. Essentially object recognition is the ability to assign labels to objects in order to categorize and identify them, thus distinguishing one object from another. During visual processing information is not created, but rather reformatted in a way that draws out the most detailed information of the stimulus.",
            "score": 125.88101196289062
        },
        {
            "docid": "2872287_23",
            "document": "Neural binding . Much of the experimental evidence for neural binding has traditionally revolved around sensory awareness. Sensory awareness is accomplished by integrating things together by cognitively perceiving them and then segmenting them so that, in total, there is an image created. Since there can be an infinite number of possibilities in the perception of an object, this has been a unique area of study. The way the brain then collectively pieces certain things together via networking is important not only in the global way of perceiving but also in segmentation. Much of sensory awareness has to do with the taking of a single piece of an object's makeup and then binding its total characteristics so that the brain perceives the object in its final form. Much of the research for the understanding of segmentation and how the brain perceives an object has been done by studying cats. A major finding of this research has to do with the understanding of gamma waves oscillating at 40\u00a0Hz. The information was extracted from a study using the cat visual cortex. It was shown that the cortical neurons responded differently to spatially different objects. These firings of neurons ranged from 40\u201360\u00a0Hz in measure and when observed showed that they fired synchronously when observing different parts of the object. Such coherent responses point to the fact that the brain is doing a kind of coding where it is piecing certain neurons together in the works of making the form of an object. Since the brain is putting these segmented pieces together unsupervised, a significant consonance is found with many philosophers (like Sigmund Freud) who theorize an underlying subconscious that helps to form every aspect of our conscious thought processes.",
            "score": 125.54391479492188
        },
        {
            "docid": "25146378_12",
            "document": "Functional specialization (brain) . One of the most well known examples of functional specialization is the fusiform face area (FFA). Justine Sergent was one of the first researchers that brought forth evidence towards the functional neuroanatomy of face processing. Using positron emission tomography (PET), Sergent found that there were different patterns of activation in response to the two different required tasks, face processing verses object processing. These results can be linked with her studies of brain-damaged patients with lesions in the occipital and temporal lobes. Patients revealed that there was an impairment of face processing but no difficulty recognizing everyday objects, a disorder also known as prosopagnosia. Later research by Nancy Kanwisher using functional magnetic resonance imaging (fMRI), found specifically that the region of the inferior temporal cortex, known as the fusiform gyrus, was significantly more active when subjects viewed, recognized and categorized faces in comparison to other regions of the brain. Lesion studies also supported this finding where patients were able to recognize objects but unable to recognize faces. This provided evidence towards domain specificity in the visual system, as Kanwisher acknowledges the Fusiform Face Area as a module in the brain, specifically the extrastriate cortex, that is specialized for face perception.",
            "score": 124.1028060913086
        },
        {
            "docid": "724626_6",
            "document": "Allan Snyder . Snyder is interested in understanding savants, how the savant brain perceives and interprets the world, the neurological and subjective correlates of the savant brain, and how to activate or at least promote savant level brain functions in non-autistic, healthy individuals. Even something as simple as seeing, he explains, requires phenomenally complex information processing. When a person looks at an object, for example, the brain immediately estimates an object's distance by calculating the subtle differences between the two images on each retina (computers programmed to do this require extreme memory and speed). During the process of face recognition, the brain analyzes countless details, such as the texture of skin and the shape of the eyes, jawbone, and lips. The vast majority of people are simply unaware of these calculations due to the brains' information filtering processes. In savants, says Snyder, the top layer of mental processing \u2014conceptual thinking, making logical deductions\u2014 is somehow deactivated. His working hypothesis is that once this layer is inactivate, one can access a startling capacity for recalling the most minute detail or for performing lightning-quick calculations. Snyder's theory has a conclusion of its own: He believes it may be possible someday to create technologies that will allow any non-autistic person to access these abilities.",
            "score": 123.390625
        },
        {
            "docid": "379234_4",
            "document": "Sensory nervous system . Sensory systems code for four aspects of a stimulus; type (modality), intensity, location, and duration. Arrival time of a sound pulse and phase differences of continuous sound are used for sound localization. Certain receptors are sensitive to certain types of stimuli (for example, different mechanoreceptors respond best to different kinds of touch stimuli, like sharp or blunt objects). Receptors send impulses in certain patterns to send information about the intensity of a stimulus (for example, how loud a sound is). The location of the receptor that is stimulated gives the brain information about the location of the stimulus (for example, stimulating a mechanoreceptor in a finger will send information to the brain about that finger). The duration of the stimulus (how long it lasts) is conveyed by firing patterns of receptors. These impulses are transmitted to the brain through afferent neurons.",
            "score": 122.91333770751953
        },
        {
            "docid": "43777942_4",
            "document": "Face on Moon South Pole . Human brains have the ability to detect ambiguous images displayed upon the moon due to the brain\u2019s structure. On the left hemisphere of the human brain, the fusiform gyrus (an area linked to recognition), detects the accuracy of how \u201cfacelike\u201d an object is. The right fusiform gyrus then uses information from the left fusiform gyrus to conclude whether or not the image is a face. The gyrus's inherent ability to detect faces and patterns in organisms and nature has also led to a phenomenon called Pareidolia, in which the brain detects and recognises faces and patterns in collections of objects where there should be none.",
            "score": 122.43666076660156
        },
        {
            "docid": "20788764_3",
            "document": "Marcel Just . Marcel Just, Tom Mitchell, and colleagues at Carnegie Mellon University are conducting research on \"thought identification\" using fMRI. Using machine learning techniques, they have been able to identify patterns of brain activation that are reliably associated to the concept of different objects. These signature patterns could be generalized across different participants, so that the authors were able to correctly identify which object a participant was thinking of by analyzing the corresponding brain activation.",
            "score": 122.24179077148438
        },
        {
            "docid": "7330954_28",
            "document": "Pattern recognition (psychology) . Music provides deep and emotional experiences for the listener. These experiences become contents in long-term memory, and every time we hear the same tunes, those contents are activated. Recognizing the content by the pattern of the music affects our emotion. The mechanism that forms the pattern recognition of music and the experience has been studied by multiple researchers. The sensation felt when listening to our favorite music is evident by the dilation of the pupils, the increase in pulse and blood pressure, the streaming of blood to the leg muscles, and the activation of the cerebellum, the brain region associated with physical movement.  While retrieving the memory of a tune demonstrates general recognition of musical pattern, pattern recognition also occurs while listening to a tune for the first time. The recurring nature of the metre allows the listener to follow a tune, recognize the metre, expect its upcoming occurrence, and figure the rhythm. The excitement of following a familiar music pattern happens when the pattern breaks and becomes unpredictable. This following and breaking of a pattern creates a problem-solving opportunity for the mind that form the experience. Psychologist Daniel Levitin argues that the repetitions, melodic nature and organization of this music create meaning for the brain. The brain stores information in an arrangement of neurons which retrieve the same information when activated by the environment. By constantly referencing information and additional stimulation from the environment, the brain constructs musical features into a perceptual whole.",
            "score": 122.22005462646484
        },
        {
            "docid": "4142132_6",
            "document": "Rubin vase . Normally the brain classifies images by which object surrounds which\u00a0\u2013 establishing depth and relationships. If one object surrounds another object, the surrounded object is seen as figure, and the presumably further away (and hence background) object is the ground, and vice versa. This makes sense, since if a piece of fruit is lying on the ground, one would want to pay attention to the \"figure\" and not the \"ground\". However, when the contours are not so unequal, ambiguity starts to creep into the previously simple inequality, and the brain must begin \"shaping\" what it sees; it can be shown that this shaping overrides and is at a higher level than feature recognition processes that pull together the face and the vase images\u00a0\u2013 one can think of the lower levels putting together distinct regions of the picture (each region of which makes sense in isolation), but when the brain tries to make sense of it as a whole, contradictions ensue, and patterns must be discarded.",
            "score": 120.90026092529297
        },
        {
            "docid": "37198050_9",
            "document": "Body part as an object . There have been many studies relating activated brain areas to tool-use, in both physical object manipulation and pantomimes. Meta-analyses have found that tool-use is largely lateralized in the left-hemisphere of the brain and independent of handedness. Specifically, the brain region which showed the greatest activity was the left superior parietal lobule. Other areas that showed significant activity was bilaterally in both the ventral and dorsolateral premotor cortex, areas by the inferior parietal lobule, and tissue around the medial temporal gyrus. Furthermore, even when object-use was imagined, activation was found to be largely lateralized in the left hemisphere and was very similar to the brain activation in actual tool-use and pantomiming. The only significant difference was additional activation in the left occipito-parietal region.",
            "score": 120.8136215209961
        },
        {
            "docid": "491622_5",
            "document": "Damasio's theory of consciousness . In this state, emotion begins to manifest itself as second-order neural patterns located in subcortical areas of the brain. Emotion acts as a neural object, from which a physical reaction can be drawn. This reaction causes the organism to become aware of the changes that are affecting it. From this realization, springs Damasio\u2019s notion of \u201cfeeling\u201d. This occurs when the patterns contributing to emotion manifest as mental images, or brain movies. When the body is modified by these neural objects, the second layer of self emerges. This is known as core consciousness.",
            "score": 120.11907958984375
        },
        {
            "docid": "2593441_30",
            "document": "Stephen Grossberg . With Gail Carpenter, Grossberg developed the adaptive resonance theory (ART). ART is a cognitive and neural theory of how the brain can quickly learn, and stably remember and recognize, objects and events in a changing world. ART proposed a solution of the stability-plasticity dilemma; namely, how a brain or machine can learn quickly about new objects and events without just as quickly being forced to forget previously learned, but still useful, memories. ART predicts how learned top-down expectations focus attention on expected combinations of features, leading to a synchronous resonance that can drive fast learning. ART also predicts how large enough mismatches between bottom-up feature patterns and top-down expectations can drive a memory search, or hypothesis testing, for recognition categories with which to better learn to classify the world. ART thus defines a type of self-organizing production system. ART was practically demonstrated through the ART family of classifiers (e.g., ART 1, ART 2, ART 2A, ART 3, ARTMAP, fuzzy ARTMAP, ART eMAP, distributed ARTMAP), developed with Gail Carpenter, which has been used in large-scale applications in engineering and technology where fast, yet stable, incrementally learned classification and prediction are needed.",
            "score": 120.05126190185547
        },
        {
            "docid": "4087208_12",
            "document": "David Marks (psychologist) . Rodway, Gillies and Schepman (2006) found that high vividness participants were significantly more accurate at detecting salient changes to pictures compared to low vividness participants, replicating an earlier study by Gur and Hilgard (1975). Recently Cui et al. (2007) found that reported image vividness correlates with increased activity in the visual cortex. This study shows that the subjective experience of forming a mental image is reflected by increased visual cortical activity. Logie, Pernet, Buonocore and Della Sala (2011) used behavioural and fMRI data for mental rotation from individuals reporting vivid and poor imagery on the VVIQ. Groups differed in brain activation patterns suggesting that the groups performed the same tasks in different ways. These findings help to explain the lack of association previously reported between VVIQ scores and mental rotation performance. Lee, Kravitz and Baker (2012) used fMRI and multi-voxel pattern analysis to investigate the specificity, distribution, and similarity of information for individual seen and imagined objects. Participants either viewed or imagined individual named object images on which they had been trained prior to the scan. Correlation between fMRI and VVIQ scores showed that, in both object-selective and early visual cortex, Lee et al.'s (2012) measure of discrimination across imagery and perception correlated with the vividness of imagery.",
            "score": 120.01963806152344
        },
        {
            "docid": "2727254_7",
            "document": "Mnemonist . The method of loci is \"the use of an orderly arrangement of locations into which one could place the images of things or people that are to be remembered\". The encoding process happens in three steps. First, an architectural area, such as the houses on a street, must be memorized. Second, each item to be remembered must be associated with a separate image. Finally, this set of images can be distributed in a \"locus,\" or place within the architectural area in a pre-determined order. Then, as one tries to recall the information, the mnemonists simply has to \"walk\" down the street, see each symbol, and recall the associated information. An example of mnemonists who used this is Solomon Shereshevsky; he would use Gorky Street, a street he lived on. When he read, each word would form a graphic image. He would then place this image in a place along the street; later, when he needed to recall the information, he would simply \"stroll\" down the street again to recall the necessary information. Neuroimaging studies have shown results that support the method of loci as the retrieval method in world-class memory performers. An fMRI recorded brain activity in memory experts and a control group as they were memorizing selected data. Previous studies have shown that teaching a control group the method of loci leads to changes in brain activation during memorization. Consistent with their use of the method of loci, memory experts had higher activity in the medial parietal cortex, retrospenial cortex, and right posterior hippocampus; these brain areas have been linked to spatial memory and navigation. These differences were observable even when the memory experts were trying to memorize stimuli, such as snowflakes, where they showed no superior ability to the control group.",
            "score": 118.86250305175781
        },
        {
            "docid": "2094955_21",
            "document": "Salience (language) . Guido\u2019s Principle one is Figure-ground, which is the means the perceptual field from which people direct their attention towards something that stands out. Figurality is the brightness, complexity, and energy (movement) of a stimulus. It is thought that these aspects trigger cognitions and thought processes in the brain that lead to salience. Brightness includes the magnitude and the colors of the object. Studies have shown that bright, vibrant colors more easily capture the attention and are easier to remember (Guido, 1998). Complexity builds upon the contextual factors (the number of perceptible qualities about the stimulus object that one can distinguish) and learning (what we perceive as unfamiliar). Complexity is the interaction of the familiarity, unfamiliarity, and the number of aspects of the stimulus object that we can resolve. Complexity is the interaction of these stimuli interact to engage affect and cognitions developed about the object (Guido, 1998). Movement of an object engages sensory receptors, which when sparked, send stimuli to the body and brain. Moving pictures, signs and eyes are used to capture our attention and make us pay attention (Guido, 1998).",
            "score": 118.55626678466797
        },
        {
            "docid": "176997_5",
            "document": "Blindsight . Patients with blindsight have damage to the visual system that allows perception (the visual cortex of the brain and some of the nerve fibers that bring information to it from the eyes) rather than the system that controls eye movements. This phenomenon shows how, after the more complex visual system is damaged, people can use the latter visual system of their brains to guide hand movements towards an object even though they cannot see what they are reaching for. Hence, visual information can control behavior without producing a conscious sensation. This ability of those with blindsight to \"see\" objects that they are unconscious of suggests that consciousness is not a general property of all parts of the brain; yet it suggests that only certain parts of the brain play a special role in consciousness.",
            "score": 118.44617462158203
        },
        {
            "docid": "24965027_8",
            "document": "Cognitive neuroscience of visual object recognition . This model, proposed by Marr and Nishihara (1978), states that object recognition is achieved by matching 3-D model representations obtained from the visual object with 3-D model representations stored in memory as veridical shape precepts. Through the use of computer programs and algorithms, Yi Yungfeng (2009) was able to demonstrate the ability for the human brain to mentally construct 3D images using only the 2D images that appear on the retina. Their model also demonstrates a high degree of shape constancy conserved between 2D images, which allow the 3D image to be recognized. The 3-D model representations obtained from the object are formed by first identifying the concavities of the object, which separate the stimulus into individual parts. Recent research suggests that an area of the brain, known as the caudal intraparietal area (CIP), is responsible for storing the slant and tilt of a plan surface that allow for concavity recognition. Rosenburg et al. implanted monkeys with a scleral search coil for monitoring eye position while simultaneously recording single neuron activation from neurons within the CIP. During the experiment, monkeys sat 30\u00a0cm away from an LCD screen that displayed the visual stimuli. Binocular disparity cues were displayed on the screen by rendering stimuli as green-red anaglyphs and the slant-tilt curves ranged from 0 to 330. A single trial consisted of a fixation point and then the presentation of a stimulus for 1 second. Neuron activations were then recorded using the surgically inserted microelectrodes. These single neuron activations for specific concavities of objects lead to the discovery that each axis of an individual part of an object containing concavity are found in memory stores. Identifying the principal axis of the object assists in the normalization process via mental rotation that is required because only the canonical description of the object is stored in memory. Recognition is acquired when the observed object viewpoint is mentally rotated to match the stored canonical description.[11] An extension of Marr and Nishihara's model, the recognition-by-components theory, proposed by Biederman (1987), proposes that the visual information gained from an object is divided into simple geometric components, such as blocks and cylinders, also known as \"geons\" (geometric ions), and are then matched with the most similar object representation that is stored in memory to provide the object's identification (see Figure 1).",
            "score": 118.28580474853516
        },
        {
            "docid": "2213172_5",
            "document": "Minimally conscious state . Some areas of the brain that are correlated with the subjective experience of pain were activated in MCS patients when noxious stimulation was present. Positron emission tomography (PET) scans found increased blood flow to the secondary sensory cortex, posterior parietal cortex, premotor cortex, and the superior temporal cortex. The pattern of activation, however, was with less spatial extent. Some parts of the brain were less activated than normal patients during noxious stimulus processing. These were the posterior cingulate, medial prefrontal cortex, and the occipital cortex. Even though functional brain imaging can objectively measure changes in brain function during noxious stimulation, the role of different areas of the brain in pain processing is only partially understood. Furthermore, there is still the problem of the subjective experience. MCS patients by definition cannot consistently and reliably communicate their experiences. Even if they were able to answer the question \"are you in pain?\", there would not be a reliable response. Further clinical trials are needed to access the appropriateness of the use of analgesia in patients with MCS.",
            "score": 117.95693969726562
        },
        {
            "docid": "37691351_4",
            "document": "Neuroscience and race . Neurotechnology enables studying the brain and racial interactions, though this study can be difficult because these interactions can be hard to replicate. Face recognition tests are the most commonly used method in studying racial interactions. These tests consist of observing own-race and other-race faces, and studying the brain's response to the faces. There are three major neurological techniques used to measure the brain's response to these simulated racial interactions. Functional magnetic resonance imaging (fMRI) measures the brain activity through measuring the blood oxygen level in the brain. This test gives insight into which regions of the brain are active during a certain event. Event-related potentials (ERPs) measure the brain's activity through measuring electrical impulses by electrodes on the head. This test gives insight in rapid changes in the brain. Transcranial magnetic stimulation (TMS) measures the response of a region of the brain once activated through magnetism. This test gives insight into causality of occurrences and gives specific insight in what the brain regions are doing. Brain-damaged patients have also been used to study racial interactions, by studying how racial interactions are affected when specific brain regions are damaged. These studies give insight into how different brain regions are involved in racial interactions once certain regions have been damaged. An implicit association test (IAC) is often used to measure the racial bias of people in studies by testing what objects, whether positive or negative, people associate with same-race or other-race faces.",
            "score": 117.89078521728516
        },
        {
            "docid": "17357470_4",
            "document": "Photographic lighting . Lighting creates the 2D pattern of contrast the brain interprets to recognize 3D objects in photographs. In an in-person viewing experience the brain relies on stereoscopic vision, parallax, shifting focal in addition to the clues created by the highlight and shadow patterns the light on the object creates. When viewing a photo the brain tries to match the patterns of contrast and color it seen to those other sensory memories.",
            "score": 117.64407348632812
        }
    ]
}