{
    "q": [
        {
            "docid": "1209759_13",
            "document": "Temporal difference learning . Dopamine cells appear to behave in a similar manner. In one experiment measurements of dopamine cells were made while training a monkey to associate a stimulus with the reward of juice. Initially the dopamine cells increased firing rates when the monkey received juice, indicating a difference in expected and actual rewards. Over time this increase in firing back propagated to the earliest reliable stimulus for the reward. Once the monkey was fully trained, there was no increase in firing rate upon presentation of the predicted reward. Continually, the firing rate for the dopamine cells decreased below normal activation when the expected reward was not produced. This mimics closely how the error function in TD is used for reinforcement learning.",
            "score": 226.6566594839096
        },
        {
            "docid": "52324876_8",
            "document": "Distress tolerance . There are several candidate biological neural network mechanisms for distress tolerance. These proposed brain areas are based on the conceptualization of distress tolerance as a function of reward learning. Within this framework, individuals learn to attune to and pursue reward; reduction of tension in escaping from a stressor is similarly framed as a reward and thus can be learned. Individuals differ in how quickly and for how long they display preferences for pursuing reward or in the case of distress tolerance, escaping from a distressful stimulus. Therefore, brain regions that are activated during reward processing and learning are hypothesized to also serve as neurobiological substrates for distress tolerance. For instance, activation intensity of dopamine neurons projecting to the nucleus accumbens, ventral striatum, and prefrontal cortex is associated with an individual's predicted value of an immediate reward during a learning task. As the firing rate for these neurons increases, individuals predict high values of an immediate reward. During instances in which the predicted value is correct, the basal rate of neuronal firing remains the same. When the predicted reward value is below the actual value, neuronal firing rates increase when the reward is received, resulting in a learned response. When the expected reward value is below the actual value, the firing rate of these neurons decreases below baseline levels, resulting in a learned shift that reduces expectancies about reward value. It is posited that these same dopaminergic firing rates are associated with distress tolerance, in that learning the value of escaping a distressing stimulus is analogous to an estimation of an immediate reward There are several potential clinical implications if these posited distress tolerance substrates are corroborated. It may suggest that distress tolerance is malleable among individuals; interventions that change neuronal firing rates may shift predicted values of behaviors intended to escape a distressor and provide relief, thereby increasing distress tolerance.",
            "score": 213.07414877414703
        },
        {
            "docid": "2733733_38",
            "document": "Delayed gratification . When animals are faced with a choice to either wait for a reward, or receive a reward right away, the discounting of the reward is hyperbolic. As the length of time of waiting for a reward increases, the reward is discounted at a gradual rate. Empirical data have suggested that exponential discounting, rewards discounting at a constant rate per unit of waiting time, only occurs when there are random interruptions in foraging. Discounting can also be related to the risk sensitivity of animals. Rather than relating risk to delay, risk sensitivity acts as a function of delay discounting. In a study conducted by Haden and Platt, macaque monkeys were given the choice of a medium reward that they knew they would receive, versus a more risky choice. The riskier choice would reward the monkey with a large reward fifty percent of the time, and a small reward the other fifty percent. The ultimate payoff was the same, but the monkeys preferred the riskier choice. They speculated that the monkeys did not see their action as risky, but rather as a large, delayed reward. They reasoned that the monkeys viewed the large reward as certain: if they did not get the large reward the first time around, they would eventually get it, but at a longer delay. To test for this theory, they gave the same test while varying the time between the opportunities to choose a reward. They found that as the interval increased, the number of times that the monkeys chose the more risky reward decreased. While this occurred in macaque monkeys, the varying interval time did not affect pigeons' choices in another study. This suggests that research looking into varying risk sensitivity of different species is needed. When provided a choice between a small, short delay reward, and a large, long delay reward, there is an impulsive preference for the former. Additionally, as the delay time for the small/short and large/long reward increases, there is a shift in preference toward the larger, delayed reward. This evidence only supports hyperbolic discounting, not exponential.",
            "score": 165.48131358623505
        },
        {
            "docid": "48548_33",
            "document": "Dopamine . Within the brain, dopamine functions partly as a \"global reward signal\", where an initial phasic dopamine response to a rewarding stimulus encodes information about the salience, value, and context of a reward. In the context of reward-related learning, dopamine also functions as a \"reward prediction error\" signal, that is, the degree to which the value of a reward is unexpected. According to this hypothesis of Wolfram Schultz, rewards that are expected do not produce a second phasic dopamine response in certain dopaminergic cells, but rewards that are unexpected, or greater than expected, produce a short-lasting increase in synaptic dopamine, whereas the omission of an expected reward actually causes dopamine release to drop below its background level. The \"prediction error\" hypothesis has drawn particular interest from computational neuroscientists, because an influential computational-learning method known as temporal difference learning makes heavy use of a signal that encodes prediction error. This confluence of theory and data has led to a fertile interaction between neuroscientists and computer scientists interested in machine learning.",
            "score": 224.57078456878662
        },
        {
            "docid": "5212259_6",
            "document": "Pars compacta . \"Dopamine neurons are activated by novel, unexpected stimuli, by primary rewards in the absence of predictive stimuli and during learning\". Dopamine neurons are thought to be involved in learning to predict which behaviours will lead to a reward (for example food or sex). In particular, it is suggested that dopamine neurons fire when a reward is greater than that previously expected; a key component of many reinforcement learning models. This signal can then be used to update the expected value of that action. Many recreational drugs, such as cocaine, mimic this reward response\u2014providing an explanation for their addictive nature.",
            "score": 214.1840159893036
        },
        {
            "docid": "17258308_3",
            "document": "Two-alternative forced choice . There are various manipulations in the design of the task, engineered to test specific behavioral dynamics of choice. In one well known experiment of attention that examines the attentional shift, the Posner Cueing Task uses a 2AFC design to present two stimuli representing two given locations. In this design there is an arrow that cues which stimulus (location) to attend to. The person then has to make a response between the two stimuli (locations) when prompted. In animals, the 2AFC task has been used to test reinforcement probability learning, for example such as choices in pigeons after reinforcement of trials. A 2AFC task has also been designed to test decision making and the interaction of reward and probability learning in monkeys. Monkeys were trained to look at a center stimulus and were then presented with two salient stimuli side by side. A response can then be made in the form of a saccade to the left or to the right stimulus. A juice reward is then administered after each response. The amount of juice reward is then varied to modulate choice.",
            "score": 176.21157836914062
        },
        {
            "docid": "2982535_8",
            "document": "Habenular nuclei . LHb is especially important in understanding the reward and motivation relationship as it relates to addictive behaviors. The LHb inhibits dopaminergic neurons, decreasing the release of dopamine. It was determined by several animal studies that receiving a reward coincided with elevated dopamine levels, but once the learned association was learned by the animal, dopamine levels remain elevated, only decreasing when the reward is removed. Therefore, dopamine levels only increase with unpredicted rewards and with a \"negative prediction error\". Moreover, it was determined that removal of an anticipated award activated LHb, inhibited dopamine levels. This finding helps explain why addictive drugs are associated with elevated dopamine levels.",
            "score": 195.70357084274292
        },
        {
            "docid": "48548_34",
            "document": "Dopamine . Evidence from microelectrode recordings from the brains of animals shows that dopamine neurons in the ventral tegmental area (VTA) and substantia nigra are strongly activated by a wide variety of rewarding events. These reward-responsive dopamine neurons in the VTA and substantia nigra are crucial for reward-related cognition and serve as the central component of the reward system. The function of dopamine varies in each axonal projection from the VTA and substantia nigra; for example, the VTA\u2013nucleus accumbens shell projection assigns incentive salience (\"want\") to rewarding stimuli and its associated cues, the VTA\u2013orbitofrontal cortex projection updates the value of different goals in accordance with their incentive salience, the VTA\u2013amygdala and VTA\u2013hippocampus projections mediate the consolidation of reward-related memories, and both the VTA\u2013nucleus accumbens core and substantia nigra\u2013dorsal striatum pathways are involved in learning motor responses that facilitate the acquisition of rewarding stimuli. Some activity within the VTA dopaminergic projections appears to be associated with reward prediction as well.",
            "score": 212.62912702560425
        },
        {
            "docid": "515094_21",
            "document": "Neuroeconomics . Neuroeconomic research in intertemporal choice is largely aimed at understanding what mediates observed behaviors such as future discounting and impulsively choosing smaller sooner rather than larger later rewards. The process of choosing between immediate and delayed rewards seems to be mediated by an interaction between two brain areas. In choices involving both primary (fruit juice) and secondary rewards (money), the limbic system is highly active when choosing the immediate reward while the lateral prefrontal cortex was equally active when making either choice. Furthermore, the ratio of limbic to cortex activity decreased as a function of the amount of time until reward. This suggests that the limbic system, which forms part of the dopamine reward pathway, is most involved in making impulsive decisions while the cortex is responsible for the more general aspects of the intertemporal decision process.",
            "score": 192.5279175043106
        },
        {
            "docid": "24514_39",
            "document": "Psychosis . Psychosis is associated with ventral striatal hypoactivity during reward anticipation and feedback. Hypoactivity in the left ventral striatum is correlated with the severity of negative symptoms. While anhedonia is a commonly reported symptom in psychosis, hedonic experiences are actually intact in most people with schizophrenia. The impairment that may present itself as anhedonia probably actually lies in the inability to identify goals, and to identify and engage in the behaviors necessary to achieve goals. Studies support a deficiency in the neural representation of goals and goal directed behavior by demonstrating that receipt (not anticipation) of reward is associated with robust response in the ventral striatum; reinforcement learning is intact when contingencies are implicit, but not when they require explicit processing; reward prediction errors (during functional neuroimaging studies), particularly positive PEs are abnormal; ACC response, taken as an indicator of effort allocation, does not increase with reward or reward probability increase, and is associated with negative symptoms; deficits in dlPFC activity and failure to improve performance on cognitive tasks when offered monetary incentives are present; and dopamine mediated functions are abnormal.",
            "score": 147.23750233650208
        },
        {
            "docid": "18345642_14",
            "document": "Behavioral addiction . One of the most important discoveries of addictions has been the drug based reinforcement and, even more important, reward based learning processes. Several structures of the brain are important in the conditioning process of behavioral addiction; these subcortical structures form the brain regions known as the reward system. One of the major areas of study is the amygdala, a brain structure which involves emotional significance and associated learning. Research shows that dopaminergic projections from the ventral tegmental area facilitate a motivational or learned association to a specific behavior.  Dopamine neurons take a role in the learning and sustaining of many acquired behaviors. Research specific to Parkinson\u2019s disease has led to identifying the intracellular signaling pathways that underlie the immediate actions of dopamine. The most common mechanism of dopamine is to create addictive properties along with certain behaviors. There are three stages to the dopamine reward system: bursts of dopamine, triggering of behavior, and further impact to the behavior. Once electronically signaled, possibly through the behavior, dopamine neurons let out a \u2018burst-fire\u2019 of elements to stimulate areas along fast transmitting pathways. The behavior response then perpetuates the striated neurons to further send stimuli. The fast firing of dopamine neurons can be monitored over time by evaluating the amount of extracellular concentrations of dopamine through micro dialysis and brain imaging. This monitoring can lead to a model in which one can see the multiplicity of triggering over a period of time. Once the behavior is triggered, it is hard to work away from the dopamine reward system.",
            "score": 201.23650658130646
        },
        {
            "docid": "8582684_10",
            "document": "Reward system . Two theories exist with regard to the activity of the nucleus accumbens and the generation liking and wanting. The inhibition (or hyperpolarization) hypothesis proposes that the nucleus accumbens exerts tonic inhibitory effects on downstream structures such as the ventral pallidum, hypothalamus or ventral tegmental area, and that in inhibiting in the nucleus accumbens (NAcc), these structures are excited, \"releasing\" reward related behavior. While GABA receptor agonists are capable of eliciting both \"liking\" and \"wanting\" reactions in the nucleus accumbens, glutaminergic inputs from the basolateral amygdala, ventral hippocampus, and medial prefrontal cortex can drive incentive salience. Furthermore, while most studies find that NAcc neurons reduce firing in response to reward, a number of studies find the opposite response. This had lead to the proposal of the disinhibition (or depolarization) hypothesis, that proposes that excitation or NAcc neurons, or at least certain subsets, drives reward related behavior. After nearly 50 years of research on brain-stimulation reward, experts have certified that dozens of sites in the brain will maintain intracranial self-stimulation. Regions include the lateral hypothalamus and medial forebrain bundles, which are especially effective. Stimulation there activates fibers that form the ascending pathways; the ascending pathways include the mesolimbic dopamine pathway, which projects from the ventral tegmental area to the nucleus accumbens. There are several explanations as to why the mesolimbic dopamine pathway is central to circuits mediating reward. First, there is a marked increase in dopamine release from the mesolimbic pathway when animals engage in intracranial self-stimulation. Second, experiments consistently indicate that brain-stimulation reward stimulates the reinforcement of pathways that are normally activated by natural rewards, and drug reward or intracranial self-stimulation can exert more powerful activation of central reward mechanisms because they activate the reward center directly rather than through the peripheral nerves. Third, when animals are administered addictive drugs or engage in naturally rewarding behaviors, such as feeding or sexual activity, there is a marked release of dopamine within the nucleus accumbens. However, dopamine is not the only reward compound in the brain.",
            "score": 170.19679307937622
        },
        {
            "docid": "25225295_12",
            "document": "Consumer neuroscience . Brand loyalty has been shown to be the result of changes in neural activity in the striatum, which is part of the human action reward system. In order to become brand loyal the brain must make a decision of brand A over brand B, a process which relies on the brain to make predictions based upon expected reward and then evaluate the results to learn loyalty. The brain is required to remember both positive and negative outcomes of previous brand choices in order to accurately be able to make predictions regarding the expected outcome of future brand decisions. For example, a helpful salesman or a discount in price may serve as a reward to encourage future customer loyalty. It is thought that the amygdala and striatum are the two most prominent structures for predicting the outcomes of decisions, and that the brain learns to better predict in part by establishing a larger neural network in these structures.",
            "score": 141.2466549873352
        },
        {
            "docid": "40149914_4",
            "document": "Metalearning (neuroscience) . Dopamine is proposed to act as a \"global learning\" signal, critical to prediction of rewards and action reinforcement. In this way, dopamine is involved in a learning algorithm in which Actor, Environment and Critic are bound in a dynamic interplay that ultimately seeks to maximise the sum of future rewards by producing an optimal action selection policy. In this context, Critic and Actor are characterised as independent network edges that also form a single Complex Agent. This Agent collectively influences the information state of the Environment, which is fed back to the Agent for future computations. Through a separate pathway, Environment is also fed back to Critic in the form of the reward gained though the given action, meaning an equilibrium can be reached between the predicted reward of given policy for a given state, and the evolving prospect of future rewards.",
            "score": 150.83789730072021
        },
        {
            "docid": "270792_7",
            "document": "Anhedonia . Anhedonia is commonly listed as one component of negative symptoms in schizophrenia. Although five domains are usually used to classify negative symptoms, factor analysis of questionnaires yield two factors, with one including deficits in pleasure and motivation. People with schizophrenia retrospectively report experiencing fewer positive emotions than healthy individuals. However, \"liking\" or consummatory pleasure, is intact in schizophrenics, as they report experiencing the same degree of positive affect when presented with rewarding stimuli. Neuroimaging studies support this behavioral observation, as most studies report intact responses in the reward system (i.e. ventral striatum, VTA) to simple rewards. However, studies on monetary rewards sometimes report reduced responsiveness. More consistent reductions are observed with regard to emotional response during reward anticipation, which is reflected in a reduced responsiveness of both cortical and subcortical components of the reward system. Schizophrenia is associated with reduced positive prediction errors (a normal pattern of response to an unexpected reward), which a few studies have demonstrated to be correlated with negative symptoms. Schizophrenics demonstrate impairment in reinforcement learnings tasks only when the task requires explicit learning, or is sufficiently complex. Implicit reinforcement learning, on the other hand, is relatively intact. These deficits may be related to dysfunction in the ACC, OFC and dlPFC leading to abnormal representation of reward and goals.",
            "score": 135.81775379180908
        },
        {
            "docid": "41578765_21",
            "document": "Paul Glimcher . Glimcher\u2019s laboratory has conducted extensive research on the brain\u2019s reward system, in particular the dopamine system and reinforcement learning. In 2005, with Hannah Bayer, he published the first quantitative test of the Dopamine Reward Prediction Error Hypothesis based on single neuron recordings from dopamine neurons and a novel kernel-based analysis in \"Neuron\".",
            "score": 173.91139698028564
        },
        {
            "docid": "515094_13",
            "document": "Neuroeconomics . In addition to the importance of specific brain areas to the decision process, there is also evidence that the neurotransmitter dopamine may transmit information about uncertainty throughout the cortex. Dopaminergic neurons are strongly involved in the reward process and become highly active after an unexpected reward occurs. In monkeys, the level of dopaminergic activity is highly correlated with the level of uncertainty such that the activity increases with uncertainty. Furthermore, rats with lesions to the nucleus accumbens, which is an important part of the dopamine reward pathway through the brain, are far more risk averse than normal rats. This suggests that dopamine may be an important mediator of risky behavior.",
            "score": 173.26189303398132
        },
        {
            "docid": "30237309_8",
            "document": "Gray's biopsychological theory of personality . The behavioral activation system (BAS), in contrast to the BIS, is based on a model of appetitive motivation - in this case, an individual's disposition to pursue and achieve goals. The BAS is aroused when it receives cues corresponding to rewards and controls actions that are not related to punishment, rather actions regulating approachment type behaviors. This system has an association with hope. According to Gray's theory, the BAS is sensitive to conditioned appealing stimuli, and is associated with impulsivity. It is also thought to be related to sensitivity to reward as well as approach motivation. The BAS is sensitive to nonpunishment and reward. Individuals with a highly active BAS show higher levels of positive emotions such as elation, happiness, and hope in response to environmental cues consistent with nonpunishment and reward, along with goal-achievement. In terms of personality, these individuals are also more likely to engage in goal-directed efforts and experience these positive emotions when exposed to impending reward. The physiological mechanism for BAS is not known as well as BIS, but is believed to be related to catecholaminergic and dopaminergic pathways in the brain. Dopamine is a neurotransmitter commonly linked with positive emotions, which could explain the susceptibility to elation and happiness upon achieving goals which has been observed. People with a highly active BAS have been shown to learn better by reward than by punishment, inverse to BIS as mentioned above. BAS is considered to include trait impulsivity that is also related to psychopathological disorders such as ADHD, drug abuse, and alcohol abuse. The higher the BAS score, or the higher the impulsive, the more it is likely to be related to psycho-pathological or dis-inhibitory disorders. Certain aspects of the dopaminergic reward system activate when reward cues and reinforcers are presented, including biological rewards such as food and sex. These brain areas, which were highlighted during multiple fMRI studies, are the same areas associated with BAS.",
            "score": 138.80270278453827
        },
        {
            "docid": "26891474_2",
            "document": "PVLV . The primary value learned value (PVLV) model is a possible explanation for the reward-predictive firing properties of dopamine (DA) neurons. It simulates behavioral and neural data on Pavlovian conditioning and the midbrain dopaminergic neurons that fire in proportion to unexpected rewards. It is an alternative to the temporal-differences (TD) algorithm.",
            "score": 160.78243923187256
        },
        {
            "docid": "53918629_5",
            "document": "Evolutionary models of human drug use . Ideas concerning the neural bases of motivation and reinforcement in behavior can be traced back to the 1950s. In 1953, Olds and Milner published findings implicating a brain region, specifically a cluster of dopamine neurons, with reward-based learning. Drugs of abuse were later discovered to increase dopamine in the region of the brain associated with reward-based-learning (see: brain stimulation reward).",
            "score": 154.76273441314697
        },
        {
            "docid": "11233144_3",
            "document": "Read Montague . Montague\u2019s work has long focused on computational neuroscience \u2013 the connection between physical mechanisms present in real neural tissue and the computational functions that these mechanisms embody. His early theoretical work focused on the hypothesis that dopaminergic systems encode a particular kind of computational process, a reward prediction error signal, similar to those used in areas of artificial intelligence like optimal control. This work, carried out in collaboration with Peter Dayan and Terry Sejnowski, focused on prediction as a guiding concept in terms of synaptic learning rules that would underlie learning, valuation, and choice. This work proposed a modification to the then dominant idea of Hebbian or correlational learning. In particular, it was shown that dopamine neurons and homologous octopaminergic neurons in bees display a reward prediction error signal exactly consonant with the temporal difference error signal familiar from models of conditioning proposed by Sutton and Barto during the 1980s.",
            "score": 169.96491825580597
        },
        {
            "docid": "27797792_12",
            "document": "Construal level theory . Time discounting or temporal discounting is a wide range of ideas involving the connection between time and the extent to which an object, situation, or course of action is seen as valuable. The overall theory is that people put more value and worth into immediate events and outcomes, and apply less value to future outcomes or events. According to Trope and Liberman, CLT can provide a framework with which to understand the broad array of phenomena described by temporal discounting research. Different construals may differ in the extent to which they are associated with positive or negative evaluations. An abstract, high-level construal of an activity (e.g., \"learning to speak French\") may lead to a more positive evaluation of that activity than a concrete, low-level construal (e.g., \"learning to conjugate the irregular French verb 'avoir). Thus, CLT predicts that we will think about the value of the low-level construals when evaluating an event in the near future, but think about the value of the high-level construals when evaluating an event in the distant future. Thus CLT predicts that when low-level construals are more valuable, time delay will discount the attractiveness of an option, but when high-level construals are more valuable, time delay will increase its attractiveness. Thus, the discounting rate is affected and measured by the amount of value placed on the event or outcome. If there is a small reward, it is discounted faster than if the reward were larger.",
            "score": 135.39997780323029
        },
        {
            "docid": "8582684_19",
            "document": "Reward system . Rewarding stimuli can drive learning in both the form of classical conditioning (Pavlovian conditioning) and operant conditioning (instrumental conditioning). In classical conditioning, a reward can act as an unconditioned stimulus that, when associated with the conditioned stimulus, causes the conditioned stimulus to elicit both musculoskeletal (in the form of simple approach and avoidance behaviors) and vegetative responses. In operant conditioning, a reward may act as a reinforcing stimulus in that it increases or supports actions that lead to itself. Learned behaviors may or may not be sensitive to the value of the outcomes they lead to; behaviors that are sensitive to the contingency of an outcome on the performance of an action as well as the outcome value are goal-directed, while elicited actions that are insensitive to contingency or value are called habits. This distinction is thought to reflected two forms of learning, model free and model based. Model free learning involves the simple caching and updating of values. In contrast, model based learning involves the storage and construction of an internal model of events that allows inference and flexible prediction. Although pavlovian conditioning is generally assumed to be model free, the incentive salience assigned to a conditioned stimulus is flexible with regard to changes in internal motivational states.",
            "score": 127.55289483070374
        },
        {
            "docid": "8582684_26",
            "document": "Reward system . Addictive drugs and behaviors are rewarding and reinforcing (i.e., are \"addictive\") due to their effects on the dopamine reward pathway. The lateral hypothalamus and medial forebrain bundle has been the most-frequently-studied brain-stimulation reward site, particularly in studies of the effects of drugs on brain stimulation reward. The neurotransmitter system that has been most-clearly identified with the habit-forming actions of drugs-of-abuse is the mesolimbic dopamine system, with its efferent targets in the nucleus accumbens and its local GABAergic afferents. The reward-relevant actions of amphetamine and cocaine are in the dopaminergic synapses of the nucleus accumbens and perhaps the medial prefrontal cortex. Rats also learn to lever-press for cocaine injections into the medial prefrontal cortex, which works by increasing dopamine turnover in the nucleus accumbens. Nicotine infused directly into the nucleus accumbens also enhances local dopamine release, presumably by a presynaptic action on the dopaminergic terminals of this region. Nicotinic receptors localize to dopaminergic cell bodies and local nicotine injections increase dopaminergic cell firing that is critical for nicotinic reward. Some additional habit-forming drugs are also likely to decrease the output of medium spiny neurons as a consequence, despite activating dopaminergic projections. For opiates, the lowest-threshold site for reward effects involves actions on GABAergic neurons in the ventral tegmental area, a secondary site of opiate-rewarding actions on medium spiny output neurons of the nucleus accumbens. Thus GABAergic afferents to the mesolimbic dopamine neurons (primary substrate of opiate reward), the mesolimbic dopamine neurons themselves (primary substrate of psychomotor stimulant reward), and GABAergic efferents to the mesolimbic dopamine neurons (a secondary site of opiate reward) form the core of currently characterized drug-reward circuitry.",
            "score": 165.01509988307953
        },
        {
            "docid": "6226648_5",
            "document": "Brain stimulation reward . Early studies on the motivational effects of brain stimulation addressed two primary questions: 1. Which brain sites can be stimulated to produce the perception of reward? and 2. Which drugs influence the response to stimulation and via what mechanism? Investigation of the brain reward circuitry reveals that it consists of a distributed, multi-synaptic circuit that determines both BSR and natural reward function. The natural drives that motivate and shape behavior reach the reward circuitry trans-synaptically through the peripheral senses of sight, sound, taste, smell, or touch. However, experimentally-induced BSR more directly activates the reward circuitry and bypasses transduction through peripheral sensory pathways. For this reason, electrical brain stimulation provides a tool for identifying the reward circuitry within the central nervous system with some degree of anatomical and neurochemical specificity. Studies involving these two forms of laboratory reward showed stimulation of a broad range of limbic and diencephalic structures could be rewarding as well as implicated the dopamine-containing neurons of the mesolimbic dopamine system in motivational function. The motivational effect of intracranial self-stimulation varies substantially depending on the placement site of the surgically implanted electrode during electrical stimulation, and animals will work to stimulate different neural sites depending on their current state. Often, animals that work to initiate brain stimulation will also work to terminate the stimulation.",
            "score": 157.63204562664032
        },
        {
            "docid": "7506663_3",
            "document": "Behavioral contrast . In 1942, Crespi measured the speed of rats running to various amounts of reward at the end of an alley. He found that the greater the magnitude of reward, the faster the rat would run to get the reward. In the middle of his experiment Crespi shifted some of his animals from a large reward to a small reward. These animals now ran even more slowly than control animals that had been trained on small reward throughout the experiment. This overshoot is an example of successive negative contrast. Likewise, other animals shifted from small to large reward ran faster than those trained on the larger reward throughout (successive positive contrast). Crespi originally called these effects \"depression\" and \"elation\" respectively, but, in 1949, Zeaman suggested changing the names to \"negative contrast\" and \"positive contrast\". In 1981, Bower discovered that positive contrast may be reduced because the response measure hits a ceiling. Thus, if contrast is the subject of an experiment, reward sizes may need to be adjusted to keep the response below such a ceiling. In 1996, Flaherty suggested that negative contrast was related to frustration; that is, the sudden shift to a low reward causes frustration for the person or the animal, and this frustration interferes with the behavior the subject is performing.",
            "score": 102.53195703029633
        },
        {
            "docid": "673153_6",
            "document": "Dopaminergic pathways . The dopaminergic pathways that project from the substantia nigra pars compacta and ventral tegmental area into the striatum (i.e., the nigrostriatal and mesolimbic pathways, respectively) form one component of a sequence of pathways known as the cortico-basal ganglia-thalamo-cortical loop. This method of classification is used in the study of many psychiatric illness. The nigrostriatal component of the loop consists of the SNc, giving rise to both inhibitory and excitatory pathways that run from the striatum into the globus pallidus, before carrying on to the thalamus, or into the subthalamic nucleus before heading into the thalamus. The dopaminergic neurons in this circuit increase the magnitude of phasic firing in response to positive reward error, that is when the reward exceeds the expected reward. These neurons do not decrease phasic firing during a negative reward prediction (less reward than expected), leading to hypothesis that serotonergic, rather than dopaminergic neurons encode reward loss. Dopamine phasic activity also increases during cues that signal negative events, however dopaminergic neuron stimulation still induces place preference, indicating its main role in evaluating a positive stimulus. From these findings, two hypotheses have developed, as to the role of the basal ganglia and nigrostiatal dopamine circuits in action selection. The first model suggests a \"critic\" which encodes value, and an actor which encodes responses to stimuli based on perceived value. However, the second model proposes that the actions do not originate in the basal ganglia, and instead originate in the cortex and are selected by the basal ganglia. This model proposes that the direct pathway controls appropriate behavior and the indirect suppresses actions not suitable for the situation. This model proposes that tonic dopaminergic firing increases the activity of the direct pathway, causing a bias towards executing actions faster.",
            "score": 171.825049161911
        },
        {
            "docid": "48548_67",
            "document": "Dopamine . It had long been believed that arthropods were an exception to this with dopamine being seen as having an adverse effect. Reward was seen to be mediated instead by octopamine, a neurotransmitter closely related to norepinephrine. More recent studies however have shown that dopamine does play a part in reward learning in fruit flies. Also it has been found that the rewarding effect of octopamine is due to its activating a set of dopaminergic neurons not previously accessed in the research.",
            "score": 165.63613367080688
        },
        {
            "docid": "425938_29",
            "document": "Animal cognition . The use of rules has sometimes been considered an ability restricted to humans, but a number of experiments have shown evidence of simple rule learning in primates and also in other animals. Much of the evidence has come from studies of sequence learning in which the \"rule\" consists of the order in which a series of events occurs. Rule use is shown if the animal learns to discriminate different orders of events and transfers this discrimination to new events arranged in the same order. For example, Murphy \"et al.\" (2008) trained rats to discriminate between visual sequences. For one group ABA and BAB were rewarded, where A=\"bright light\" and B=\"dim light\". Other stimulus triplets were not rewarded. The rats learned the visual sequence, although both bright and dim lights were equally associated with reward. More importantly, in a second experiment with auditory stimuli, rats responded correctly to sequences of novel stimuli that were arranged in the same order as those previously learned. Similar sequence learning has been demonstrated in birds and other animals as well.",
            "score": 116.89737915992737
        },
        {
            "docid": "33456209_22",
            "document": "Learned industriousness . The authors found similar results to previous learned industriousness studies: participants in the high difficulty-low reward condition showed more creativity in the circle drawing task than those without a reward while participants in the low difficulty-low reward showed even less creativity. Although most creativity research up until that point suggested that any reward for creative thoughts reduced generalized creativity, this study showed that increases or decreases in generalized creativity depend on whether or not high or low divergent thought is rewarded.",
            "score": 88.78877449035645
        },
        {
            "docid": "32020972_11",
            "document": "Reinforcement sensitivity theory . High BAS is generally associated with high extraversion, low neuroticism, and trait impulsivity, while high BIS is associated with low extraversion, high neuroticism, and trait anxiety. In addition to predicting trait standings, high BAS is associated with higher \"positive\" affect in response to reward, while high BIS is associated with higher \"negative\" affect in response to punishment. Studies in Gray\u2019s laboratory supported his prediction that extraverts, higher in BAS and lower in BIS than introverts, are more sensitive to rewards, experience higher levels of positive affect, and learn faster under rewarding conditions.",
            "score": 104.67821598052979
        },
        {
            "docid": "45312411_12",
            "document": "Cognitive bias in animals . Honeybees (\"Apis mellifera carnica\") were trained to extend their proboscis to a two-component odour mixture (CS+) predicting a reward (e.g., 1.00 or 2.00 M sucrose) and to withhold their proboscis from another mixture (CS\u2212) predicting either punishment or a less valuable reward (e.g., 0.01 M quinine solution or 0.3 M sucrose). Immediately after training, half of the honeybees were subjected to vigorous shaking for 60 s to simulate the state produced by a predatory attack on a concealed colony. This shaking reduced levels of octopamine, dopamine, and serotonin in the hemolymph of a separate group of honeybees at a time point corresponding to when the cognitive bias tests were performed. In honeybees, octopamine is the local neurotransmitter that functions during reward learning, whereas dopamine mediates the ability to learn to associate odours with quinine punishment. If flies are fed serotonin, they are more aggressive; flies depleted of serotonin still exhibit aggression, but they do so much less frequently.",
            "score": 133.85221886634827
        }
    ],
    "r": [
        {
            "docid": "1209759_13",
            "document": "Temporal difference learning . Dopamine cells appear to behave in a similar manner. In one experiment measurements of dopamine cells were made while training a monkey to associate a stimulus with the reward of juice. Initially the dopamine cells increased firing rates when the monkey received juice, indicating a difference in expected and actual rewards. Over time this increase in firing back propagated to the earliest reliable stimulus for the reward. Once the monkey was fully trained, there was no increase in firing rate upon presentation of the predicted reward. Continually, the firing rate for the dopamine cells decreased below normal activation when the expected reward was not produced. This mimics closely how the error function in TD is used for reinforcement learning.",
            "score": 226.65664672851562
        },
        {
            "docid": "48548_33",
            "document": "Dopamine . Within the brain, dopamine functions partly as a \"global reward signal\", where an initial phasic dopamine response to a rewarding stimulus encodes information about the salience, value, and context of a reward. In the context of reward-related learning, dopamine also functions as a \"reward prediction error\" signal, that is, the degree to which the value of a reward is unexpected. According to this hypothesis of Wolfram Schultz, rewards that are expected do not produce a second phasic dopamine response in certain dopaminergic cells, but rewards that are unexpected, or greater than expected, produce a short-lasting increase in synaptic dopamine, whereas the omission of an expected reward actually causes dopamine release to drop below its background level. The \"prediction error\" hypothesis has drawn particular interest from computational neuroscientists, because an influential computational-learning method known as temporal difference learning makes heavy use of a signal that encodes prediction error. This confluence of theory and data has led to a fertile interaction between neuroscientists and computer scientists interested in machine learning.",
            "score": 224.57078552246094
        },
        {
            "docid": "5212259_6",
            "document": "Pars compacta . \"Dopamine neurons are activated by novel, unexpected stimuli, by primary rewards in the absence of predictive stimuli and during learning\". Dopamine neurons are thought to be involved in learning to predict which behaviours will lead to a reward (for example food or sex). In particular, it is suggested that dopamine neurons fire when a reward is greater than that previously expected; a key component of many reinforcement learning models. This signal can then be used to update the expected value of that action. Many recreational drugs, such as cocaine, mimic this reward response\u2014providing an explanation for their addictive nature.",
            "score": 214.18402099609375
        },
        {
            "docid": "52324876_8",
            "document": "Distress tolerance . There are several candidate biological neural network mechanisms for distress tolerance. These proposed brain areas are based on the conceptualization of distress tolerance as a function of reward learning. Within this framework, individuals learn to attune to and pursue reward; reduction of tension in escaping from a stressor is similarly framed as a reward and thus can be learned. Individuals differ in how quickly and for how long they display preferences for pursuing reward or in the case of distress tolerance, escaping from a distressful stimulus. Therefore, brain regions that are activated during reward processing and learning are hypothesized to also serve as neurobiological substrates for distress tolerance. For instance, activation intensity of dopamine neurons projecting to the nucleus accumbens, ventral striatum, and prefrontal cortex is associated with an individual's predicted value of an immediate reward during a learning task. As the firing rate for these neurons increases, individuals predict high values of an immediate reward. During instances in which the predicted value is correct, the basal rate of neuronal firing remains the same. When the predicted reward value is below the actual value, neuronal firing rates increase when the reward is received, resulting in a learned response. When the expected reward value is below the actual value, the firing rate of these neurons decreases below baseline levels, resulting in a learned shift that reduces expectancies about reward value. It is posited that these same dopaminergic firing rates are associated with distress tolerance, in that learning the value of escaping a distressing stimulus is analogous to an estimation of an immediate reward There are several potential clinical implications if these posited distress tolerance substrates are corroborated. It may suggest that distress tolerance is malleable among individuals; interventions that change neuronal firing rates may shift predicted values of behaviors intended to escape a distressor and provide relief, thereby increasing distress tolerance.",
            "score": 213.0741424560547
        },
        {
            "docid": "48548_34",
            "document": "Dopamine . Evidence from microelectrode recordings from the brains of animals shows that dopamine neurons in the ventral tegmental area (VTA) and substantia nigra are strongly activated by a wide variety of rewarding events. These reward-responsive dopamine neurons in the VTA and substantia nigra are crucial for reward-related cognition and serve as the central component of the reward system. The function of dopamine varies in each axonal projection from the VTA and substantia nigra; for example, the VTA\u2013nucleus accumbens shell projection assigns incentive salience (\"want\") to rewarding stimuli and its associated cues, the VTA\u2013orbitofrontal cortex projection updates the value of different goals in accordance with their incentive salience, the VTA\u2013amygdala and VTA\u2013hippocampus projections mediate the consolidation of reward-related memories, and both the VTA\u2013nucleus accumbens core and substantia nigra\u2013dorsal striatum pathways are involved in learning motor responses that facilitate the acquisition of rewarding stimuli. Some activity within the VTA dopaminergic projections appears to be associated with reward prediction as well.",
            "score": 212.62911987304688
        },
        {
            "docid": "18345642_14",
            "document": "Behavioral addiction . One of the most important discoveries of addictions has been the drug based reinforcement and, even more important, reward based learning processes. Several structures of the brain are important in the conditioning process of behavioral addiction; these subcortical structures form the brain regions known as the reward system. One of the major areas of study is the amygdala, a brain structure which involves emotional significance and associated learning. Research shows that dopaminergic projections from the ventral tegmental area facilitate a motivational or learned association to a specific behavior.  Dopamine neurons take a role in the learning and sustaining of many acquired behaviors. Research specific to Parkinson\u2019s disease has led to identifying the intracellular signaling pathways that underlie the immediate actions of dopamine. The most common mechanism of dopamine is to create addictive properties along with certain behaviors. There are three stages to the dopamine reward system: bursts of dopamine, triggering of behavior, and further impact to the behavior. Once electronically signaled, possibly through the behavior, dopamine neurons let out a \u2018burst-fire\u2019 of elements to stimulate areas along fast transmitting pathways. The behavior response then perpetuates the striated neurons to further send stimuli. The fast firing of dopamine neurons can be monitored over time by evaluating the amount of extracellular concentrations of dopamine through micro dialysis and brain imaging. This monitoring can lead to a model in which one can see the multiplicity of triggering over a period of time. Once the behavior is triggered, it is hard to work away from the dopamine reward system.",
            "score": 201.2364959716797
        },
        {
            "docid": "2982535_8",
            "document": "Habenular nuclei . LHb is especially important in understanding the reward and motivation relationship as it relates to addictive behaviors. The LHb inhibits dopaminergic neurons, decreasing the release of dopamine. It was determined by several animal studies that receiving a reward coincided with elevated dopamine levels, but once the learned association was learned by the animal, dopamine levels remain elevated, only decreasing when the reward is removed. Therefore, dopamine levels only increase with unpredicted rewards and with a \"negative prediction error\". Moreover, it was determined that removal of an anticipated award activated LHb, inhibited dopamine levels. This finding helps explain why addictive drugs are associated with elevated dopamine levels.",
            "score": 195.7035675048828
        },
        {
            "docid": "515094_21",
            "document": "Neuroeconomics . Neuroeconomic research in intertemporal choice is largely aimed at understanding what mediates observed behaviors such as future discounting and impulsively choosing smaller sooner rather than larger later rewards. The process of choosing between immediate and delayed rewards seems to be mediated by an interaction between two brain areas. In choices involving both primary (fruit juice) and secondary rewards (money), the limbic system is highly active when choosing the immediate reward while the lateral prefrontal cortex was equally active when making either choice. Furthermore, the ratio of limbic to cortex activity decreased as a function of the amount of time until reward. This suggests that the limbic system, which forms part of the dopamine reward pathway, is most involved in making impulsive decisions while the cortex is responsible for the more general aspects of the intertemporal decision process.",
            "score": 192.52792358398438
        },
        {
            "docid": "1209759_12",
            "document": "Temporal difference learning . The TD algorithm has also received attention in the field of neuroscience. Researchers discovered that the firing rate of dopamine neurons in the ventral tegmental area (VTA) and substantia nigra (SNc) appear to mimic the error function in the algorithm. The error function reports back the difference between the estimated reward at any given state or time step and the actual reward received. The larger the error function, the larger the difference between the expected and actual reward. When this is paired with a stimulus that accurately reflects a future reward, the error can be used to associate the stimulus with the future reward.",
            "score": 176.5283966064453
        },
        {
            "docid": "17258308_3",
            "document": "Two-alternative forced choice . There are various manipulations in the design of the task, engineered to test specific behavioral dynamics of choice. In one well known experiment of attention that examines the attentional shift, the Posner Cueing Task uses a 2AFC design to present two stimuli representing two given locations. In this design there is an arrow that cues which stimulus (location) to attend to. The person then has to make a response between the two stimuli (locations) when prompted. In animals, the 2AFC task has been used to test reinforcement probability learning, for example such as choices in pigeons after reinforcement of trials. A 2AFC task has also been designed to test decision making and the interaction of reward and probability learning in monkeys. Monkeys were trained to look at a center stimulus and were then presented with two salient stimuli side by side. A response can then be made in the form of a saccade to the left or to the right stimulus. A juice reward is then administered after each response. The amount of juice reward is then varied to modulate choice.",
            "score": 176.21157836914062
        },
        {
            "docid": "41578765_21",
            "document": "Paul Glimcher . Glimcher\u2019s laboratory has conducted extensive research on the brain\u2019s reward system, in particular the dopamine system and reinforcement learning. In 2005, with Hannah Bayer, he published the first quantitative test of the Dopamine Reward Prediction Error Hypothesis based on single neuron recordings from dopamine neurons and a novel kernel-based analysis in \"Neuron\".",
            "score": 173.91139221191406
        },
        {
            "docid": "515094_13",
            "document": "Neuroeconomics . In addition to the importance of specific brain areas to the decision process, there is also evidence that the neurotransmitter dopamine may transmit information about uncertainty throughout the cortex. Dopaminergic neurons are strongly involved in the reward process and become highly active after an unexpected reward occurs. In monkeys, the level of dopaminergic activity is highly correlated with the level of uncertainty such that the activity increases with uncertainty. Furthermore, rats with lesions to the nucleus accumbens, which is an important part of the dopamine reward pathway through the brain, are far more risk averse than normal rats. This suggests that dopamine may be an important mediator of risky behavior.",
            "score": 173.2618865966797
        },
        {
            "docid": "673153_6",
            "document": "Dopaminergic pathways . The dopaminergic pathways that project from the substantia nigra pars compacta and ventral tegmental area into the striatum (i.e., the nigrostriatal and mesolimbic pathways, respectively) form one component of a sequence of pathways known as the cortico-basal ganglia-thalamo-cortical loop. This method of classification is used in the study of many psychiatric illness. The nigrostriatal component of the loop consists of the SNc, giving rise to both inhibitory and excitatory pathways that run from the striatum into the globus pallidus, before carrying on to the thalamus, or into the subthalamic nucleus before heading into the thalamus. The dopaminergic neurons in this circuit increase the magnitude of phasic firing in response to positive reward error, that is when the reward exceeds the expected reward. These neurons do not decrease phasic firing during a negative reward prediction (less reward than expected), leading to hypothesis that serotonergic, rather than dopaminergic neurons encode reward loss. Dopamine phasic activity also increases during cues that signal negative events, however dopaminergic neuron stimulation still induces place preference, indicating its main role in evaluating a positive stimulus. From these findings, two hypotheses have developed, as to the role of the basal ganglia and nigrostiatal dopamine circuits in action selection. The first model suggests a \"critic\" which encodes value, and an actor which encodes responses to stimuli based on perceived value. However, the second model proposes that the actions do not originate in the basal ganglia, and instead originate in the cortex and are selected by the basal ganglia. This model proposes that the direct pathway controls appropriate behavior and the indirect suppresses actions not suitable for the situation. This model proposes that tonic dopaminergic firing increases the activity of the direct pathway, causing a bias towards executing actions faster.",
            "score": 171.82504272460938
        },
        {
            "docid": "8582684_10",
            "document": "Reward system . Two theories exist with regard to the activity of the nucleus accumbens and the generation liking and wanting. The inhibition (or hyperpolarization) hypothesis proposes that the nucleus accumbens exerts tonic inhibitory effects on downstream structures such as the ventral pallidum, hypothalamus or ventral tegmental area, and that in inhibiting in the nucleus accumbens (NAcc), these structures are excited, \"releasing\" reward related behavior. While GABA receptor agonists are capable of eliciting both \"liking\" and \"wanting\" reactions in the nucleus accumbens, glutaminergic inputs from the basolateral amygdala, ventral hippocampus, and medial prefrontal cortex can drive incentive salience. Furthermore, while most studies find that NAcc neurons reduce firing in response to reward, a number of studies find the opposite response. This had lead to the proposal of the disinhibition (or depolarization) hypothesis, that proposes that excitation or NAcc neurons, or at least certain subsets, drives reward related behavior. After nearly 50 years of research on brain-stimulation reward, experts have certified that dozens of sites in the brain will maintain intracranial self-stimulation. Regions include the lateral hypothalamus and medial forebrain bundles, which are especially effective. Stimulation there activates fibers that form the ascending pathways; the ascending pathways include the mesolimbic dopamine pathway, which projects from the ventral tegmental area to the nucleus accumbens. There are several explanations as to why the mesolimbic dopamine pathway is central to circuits mediating reward. First, there is a marked increase in dopamine release from the mesolimbic pathway when animals engage in intracranial self-stimulation. Second, experiments consistently indicate that brain-stimulation reward stimulates the reinforcement of pathways that are normally activated by natural rewards, and drug reward or intracranial self-stimulation can exert more powerful activation of central reward mechanisms because they activate the reward center directly rather than through the peripheral nerves. Third, when animals are administered addictive drugs or engage in naturally rewarding behaviors, such as feeding or sexual activity, there is a marked release of dopamine within the nucleus accumbens. However, dopamine is not the only reward compound in the brain.",
            "score": 170.19679260253906
        },
        {
            "docid": "11233144_3",
            "document": "Read Montague . Montague\u2019s work has long focused on computational neuroscience \u2013 the connection between physical mechanisms present in real neural tissue and the computational functions that these mechanisms embody. His early theoretical work focused on the hypothesis that dopaminergic systems encode a particular kind of computational process, a reward prediction error signal, similar to those used in areas of artificial intelligence like optimal control. This work, carried out in collaboration with Peter Dayan and Terry Sejnowski, focused on prediction as a guiding concept in terms of synaptic learning rules that would underlie learning, valuation, and choice. This work proposed a modification to the then dominant idea of Hebbian or correlational learning. In particular, it was shown that dopamine neurons and homologous octopaminergic neurons in bees display a reward prediction error signal exactly consonant with the temporal difference error signal familiar from models of conditioning proposed by Sutton and Barto during the 1980s.",
            "score": 169.9649200439453
        },
        {
            "docid": "1077353_15",
            "document": "Curiosity . Dopamine is linked to the process of curiosity, as it is responsible for assigning and retaining reward values of information gained. Research suggests higher amounts of dopamine is released when the reward is unknown and the stimulus is unfamiliar, compared to activation of dopamine when stimulus is familiar.",
            "score": 169.86456298828125
        },
        {
            "docid": "48548_67",
            "document": "Dopamine . It had long been believed that arthropods were an exception to this with dopamine being seen as having an adverse effect. Reward was seen to be mediated instead by octopamine, a neurotransmitter closely related to norepinephrine. More recent studies however have shown that dopamine does play a part in reward learning in fruit flies. Also it has been found that the rewarding effect of octopamine is due to its activating a set of dopaminergic neurons not previously accessed in the research.",
            "score": 165.63612365722656
        },
        {
            "docid": "2733733_38",
            "document": "Delayed gratification . When animals are faced with a choice to either wait for a reward, or receive a reward right away, the discounting of the reward is hyperbolic. As the length of time of waiting for a reward increases, the reward is discounted at a gradual rate. Empirical data have suggested that exponential discounting, rewards discounting at a constant rate per unit of waiting time, only occurs when there are random interruptions in foraging. Discounting can also be related to the risk sensitivity of animals. Rather than relating risk to delay, risk sensitivity acts as a function of delay discounting. In a study conducted by Haden and Platt, macaque monkeys were given the choice of a medium reward that they knew they would receive, versus a more risky choice. The riskier choice would reward the monkey with a large reward fifty percent of the time, and a small reward the other fifty percent. The ultimate payoff was the same, but the monkeys preferred the riskier choice. They speculated that the monkeys did not see their action as risky, but rather as a large, delayed reward. They reasoned that the monkeys viewed the large reward as certain: if they did not get the large reward the first time around, they would eventually get it, but at a longer delay. To test for this theory, they gave the same test while varying the time between the opportunities to choose a reward. They found that as the interval increased, the number of times that the monkeys chose the more risky reward decreased. While this occurred in macaque monkeys, the varying interval time did not affect pigeons' choices in another study. This suggests that research looking into varying risk sensitivity of different species is needed. When provided a choice between a small, short delay reward, and a large, long delay reward, there is an impulsive preference for the former. Additionally, as the delay time for the small/short and large/long reward increases, there is a shift in preference toward the larger, delayed reward. This evidence only supports hyperbolic discounting, not exponential.",
            "score": 165.48130798339844
        },
        {
            "docid": "8582684_26",
            "document": "Reward system . Addictive drugs and behaviors are rewarding and reinforcing (i.e., are \"addictive\") due to their effects on the dopamine reward pathway. The lateral hypothalamus and medial forebrain bundle has been the most-frequently-studied brain-stimulation reward site, particularly in studies of the effects of drugs on brain stimulation reward. The neurotransmitter system that has been most-clearly identified with the habit-forming actions of drugs-of-abuse is the mesolimbic dopamine system, with its efferent targets in the nucleus accumbens and its local GABAergic afferents. The reward-relevant actions of amphetamine and cocaine are in the dopaminergic synapses of the nucleus accumbens and perhaps the medial prefrontal cortex. Rats also learn to lever-press for cocaine injections into the medial prefrontal cortex, which works by increasing dopamine turnover in the nucleus accumbens. Nicotine infused directly into the nucleus accumbens also enhances local dopamine release, presumably by a presynaptic action on the dopaminergic terminals of this region. Nicotinic receptors localize to dopaminergic cell bodies and local nicotine injections increase dopaminergic cell firing that is critical for nicotinic reward. Some additional habit-forming drugs are also likely to decrease the output of medium spiny neurons as a consequence, despite activating dopaminergic projections. For opiates, the lowest-threshold site for reward effects involves actions on GABAergic neurons in the ventral tegmental area, a secondary site of opiate-rewarding actions on medium spiny output neurons of the nucleus accumbens. Thus GABAergic afferents to the mesolimbic dopamine neurons (primary substrate of opiate reward), the mesolimbic dopamine neurons themselves (primary substrate of psychomotor stimulant reward), and GABAergic efferents to the mesolimbic dopamine neurons (a secondary site of opiate reward) form the core of currently characterized drug-reward circuitry.",
            "score": 165.0150909423828
        },
        {
            "docid": "1077353_16",
            "document": "Curiosity . The nucleus accumbens is a formation of neurons and is important in reward pathway activation. As previously mentioned, the reward pathway is an integral part in the induction of curiosity. The release of dopamine in investigating response to novel or exciting stimuli. The fast dopamine release observed during childhood and adolescence is important in development, as curiosity and exploratory behavior are the largest facilitators of learning during early years.",
            "score": 161.4382781982422
        },
        {
            "docid": "26891474_2",
            "document": "PVLV . The primary value learned value (PVLV) model is a possible explanation for the reward-predictive firing properties of dopamine (DA) neurons. It simulates behavioral and neural data on Pavlovian conditioning and the midbrain dopaminergic neurons that fire in proportion to unexpected rewards. It is an alternative to the temporal-differences (TD) algorithm.",
            "score": 160.78244018554688
        },
        {
            "docid": "7988175_6",
            "document": "Derek van der Kooy . Previous studies have demonstrated that the opiate reward is mediated by a dopamine-independent reward system in nondependent animals, and by a dopamine-dependent reward system in dependent animals. In the present study, infusions of BDNF into the VTA were able to shift opiate reward from a dopamine-independent system to a dopamine-dependent system. This switch is mediated through a specific change in GABA-A receptors in the VTA from inhibitory to excitatory signaling in response to increased BDNF.",
            "score": 159.91030883789062
        },
        {
            "docid": "14511650_50",
            "document": "Impulsivity . Economic theory suggests that optimal discounting involves the exponential discounting of value over time. This model assumes that people and institutions should discount the value of rewards and punishments at a constant rate according to how delayed they are in time. While economically rational, recent evidence suggests that people and animals do not discount exponentially. Many studies suggest that humans and animals discount future values according to a hyperbolic discounting curve where the discount factor decreases with the length of the delay (for example, waiting from today to tomorrow involves more loss of value than waiting from twenty days to twenty-one days). Further evidence for non-constant delay discounting is suggested by the differential involvement of various brain regions in evaluating immediate versus delayed consequences. Specifically, the prefrontal cortex is activated when choosing between rewards at a short delay or a long delay, but regions associated with the dopamine system are additionally activated when the option of an immediate reinforcer is added. Additionally, intertemporal choices differ from economic models because they involve anticipation (which may involve a neurological \"reward\" even if the reinforcer is delayed), self-control (and the breakdown of it when faced with temptations), and representation (how the choice is framed may influence desirability of the reinforcer), none of which are accounted for by a model that assumes economic rationality.",
            "score": 159.61424255371094
        },
        {
            "docid": "19477293_43",
            "document": "Biology of depression . Regions involved in reward are common targets of manipulation in animal models of depression, including the nucleus accumbens (NAc), ventral tegmental area (VTA), ventral pallidum (VP), lateral habenula (LHb) and medial prefrontal cortex (mPFC). Tentative fMRI studies in humans demonstrate elevated LHb activity in depression. The lateral habenula projects to the RMTg to drive inhibition of dopamine neurons in the VTA during omission of reward. In animal models of depression, elevated activity has been reported in LHb neurons that project to the ventral tegmental area(ostensibly reducing dopamine release). The LHb also projects to aversion reactive mPFC neurons, which may provide an indirect mechanism for producing depressive behaviors. Learned helplessness induced potentiation of LHb synapses are reversed by antidepressant treatment, providing predictive validity. A number of inputs to the LHb have been implicated in producing depressive behaviors. Silencing GABAergic projections from the NAc to the LHb reduces conditioned place preference induced in social aggression, and activation of these terminals induces CPP. Ventral pallidum firing is also elevated by stress induced depression, an effect that is pharmacologically valid, and silencing of these neurons alleviates behavioral correlates of depression. Tentative in vivo evidence from patients with major depression suggests abnormalities in dopamine signalling. This led to early studies investigating VTA activity and manipulations in animal models of depression. Massive destruction of VTA neurons enhances depressive behaviors, while VTA neurons reduce firing in response to chronic stress. However, more recent specific manipulations of the VTA produce varying results, with the specific animal model, duration of VTA manipulation, method of VTA manipulation, and subregion of VTA manipulation all potentially leading to differential outcomes. Stress and social defeat induced depressive symptoms, including anhedonia, are associated with potentiation of excitatory inputs to Dopamine D2 receptor expressing medium spiny neurons (D2-MSNs) and depression of excitatory inputs to Dopamine D1 receptor expressing medium spiny neurons (D1-MSNs). Optogenetic excitation of D1-MSNs alleviates depressive symptoms and is rewarding, while the same with D2-MSNs enhances depressive symptoms. Excitation of glutaminergic inputs from the ventral hippocampus reduces social interactions, and enhancing these projections produces susceptibility to stress induced depression. Manipulations of different regions of the mPFC can produce and attenuate depressive behaviors. For example, inhibiting mPFC neurons specifically in the intralimbic cortex attenuates depressive behaviors. The conflicting findings associated with mPFC stimulation, when compared to the relatively specific findings in the infralimbic cortex, suggest that the prelimbic cortex and infralimbic cortex may mediate opposing effects. mPFC projections to the raphe nuclei are largely GABAergic, and inhibit the firing of serotonergic neurons. Specific activation of these regions reduce immobility in the forced swim test, but do not affect open field or forced swim behavior. Inhibition of the raphe shifts the behavioral phenotype of uncontrolled stress to a phenotype closer to that of controlled stress.",
            "score": 158.90463256835938
        },
        {
            "docid": "6226648_5",
            "document": "Brain stimulation reward . Early studies on the motivational effects of brain stimulation addressed two primary questions: 1. Which brain sites can be stimulated to produce the perception of reward? and 2. Which drugs influence the response to stimulation and via what mechanism? Investigation of the brain reward circuitry reveals that it consists of a distributed, multi-synaptic circuit that determines both BSR and natural reward function. The natural drives that motivate and shape behavior reach the reward circuitry trans-synaptically through the peripheral senses of sight, sound, taste, smell, or touch. However, experimentally-induced BSR more directly activates the reward circuitry and bypasses transduction through peripheral sensory pathways. For this reason, electrical brain stimulation provides a tool for identifying the reward circuitry within the central nervous system with some degree of anatomical and neurochemical specificity. Studies involving these two forms of laboratory reward showed stimulation of a broad range of limbic and diencephalic structures could be rewarding as well as implicated the dopamine-containing neurons of the mesolimbic dopamine system in motivational function. The motivational effect of intracranial self-stimulation varies substantially depending on the placement site of the surgically implanted electrode during electrical stimulation, and animals will work to stimulate different neural sites depending on their current state. Often, animals that work to initiate brain stimulation will also work to terminate the stimulation.",
            "score": 157.63204956054688
        },
        {
            "docid": "737439_21",
            "document": "Dopamine receptor . Dopamine is the primary neurotransmitter involved in the reward pathway in the brain. Thus, drugs that increase dopamine signaling may produce euphoric effects. Many recreational drugs, such as cocaine and substituted amphetamines, inhibit the dopamine transporter (DAT), the protein responsible for removing dopamine from the neural synapse. When DAT activity is blocked, the synapse floods with dopamine and increases dopaminergic signaling. When this occurs, particularly in the nucleus accumbens, increased D and decreased D receptor signaling mediates the \"rewarding\" stimulus of drug intake.",
            "score": 156.6355438232422
        },
        {
            "docid": "547827_35",
            "document": "Loss aversion . In a study, adolescents and adults are found to be similarly loss-averse on behavioural level but they demonstrated different underlying neural responses to the process of rejecting gambles. Though adolescents rejected the same proportion of trials as adults, adolescents displayed greater caudate and frontal pole activation than adults to achieve this. These findings suggest a difference in neural development during the avoidance of risk. It is possible that adding affectively arousing factors (e.g. peer influences) may overwhelm the reward-sensitive regions of the adolescent decision making system leading to risk-seeking behaviour. On the other hand, although men and women did not differ on their behavioural task performance, men showed greater neural activation than women in various areas during the task. Loss of striatal dopamine neurons is associated with reduced risk-taking behaviour.\u00a0 Acute administration of D2 dopamine agonists may cause an increase in risky choices in humans. This suggests dopamine acting on stratum and possibly other mesolimbic structures can modulate loss aversion by reducing loss prediction signalling.",
            "score": 156.0811004638672
        },
        {
            "docid": "21312313_32",
            "document": "Procedural memory . Dopamine is one of the more known neuromodulators involved in procedural memory. Evidence suggests that it may influence neural plasticity in memory systems by adapting brain processing when the environment is changing and an individual is then forced to make a behavioural choice or series of rapid decisions. It is very important in the process of \"adaptive navigation\", which serves to help different brain areas respond together during a new situation that has many unknown stimuli and features. Dopamine pathways are dispersed all over the brain and this allows for parallel processing in many structures all at the same time. Currently most research points to the mesocorticolimbic dopamine pathway as the system most related to reward learning and psychological conditioning.",
            "score": 155.60487365722656
        },
        {
            "docid": "48548_2",
            "document": "Dopamine . Dopamine (DA, a contraction of 3,4-dihydroxyphenethylamine) is an organic chemical of the catecholamine and phenethylamine families that plays several important roles in the brain and body. It is an amine synthesized by removing a carboxyl group from a molecule of its precursor chemical L-DOPA, which is synthesized in the brain and kidneys. Dopamine is also synthesized in plants and most animals. In the brain, dopamine functions as a neurotransmitter\u2014a chemical released by neurons (nerve cells) to send signals to other nerve cells. The brain includes several distinct dopamine pathways, one of which plays a major role in the motivational component of reward-motivated behavior. The anticipation of most types of rewards increases the level of dopamine in the brain, and many addictive drugs increase dopamine release or block its reuptake into neurons following release. Other brain dopamine pathways are involved in motor control and in controlling the release of various hormones. These pathways and cell groups form a dopamine system which is neuromodulatory.",
            "score": 155.508544921875
        },
        {
            "docid": "14554121_3",
            "document": "Social inequity aversion . To better understand and breakdown the concept of social inequity aversion would be to use the study done by Sarah Brosnan (as well as with Frans B. M. de Waal), who specializes in social behavior and social cognition. In their experiment, \"Monkeys Reject Unequal Pay\" five female capuchin monkeys were used and given an unequal distribution of rewards by the human experimenter. The female monkeys alternated in pairs under four different conditions with the experimenter. Of the female monkeys, two received the same reward, one female received a superior reward, one female received a superior reward without exchange (for example without work), and a single female observed a superior reward in the absence of a partner. The females were much less likely to complete a trade with the human experimenter when their corresponding partner received a food item of higher value item (a grape; the lower item was a cucumber), and when that partner received the higher food item with no exchange of work of any kind, the likelihood of not completing a trade intensified. All of these refusals of exchange included both passive and active rejections ranging from refusing to take the awards to throwing the reward, respectively. These negative responses of situation made with the monkeys support the early evolutionary origin of inequity aversion and thus helps (in combination with the definitions of inequity and aversion) give an overall idea of what social inequity aversion is: the tendency to reject or avoid situations in which there is social inequality, unfairness, or injustice.",
            "score": 155.47251892089844
        },
        {
            "docid": "53918629_5",
            "document": "Evolutionary models of human drug use . Ideas concerning the neural bases of motivation and reinforcement in behavior can be traced back to the 1950s. In 1953, Olds and Milner published findings implicating a brain region, specifically a cluster of dopamine neurons, with reward-based learning. Drugs of abuse were later discovered to increase dopamine in the region of the brain associated with reward-based-learning (see: brain stimulation reward).",
            "score": 154.76272583007812
        },
        {
            "docid": "8582684_9",
            "document": "Reward system . Most of the dopamine pathways (i.e., neurons that use the neurotransmitter dopamine to communicate with other neurons) that project out of the ventral tegmental area are part of the reward system; in these pathways, dopamine acts on D1-like receptors or D2-like receptors to either stimulate (D1-like) or inhibit (D2-like) the production of cAMP. The GABAergic medium spiny neurons of the striatum are components of the reward system as well. The glutamatergic projection nuclei in the subthalamic nucleus, prefrontal cortex, hippocampus, thalamus, and amygdala connect to other parts of the reward system via glutamate pathways. The medial forebrain bundle, which is a set of many neural pathways that mediate brain stimulation reward (i.e., reward derived from direct electrochemical stimulation of the lateral hypothalamus), is also a component of the reward system.",
            "score": 153.50743103027344
        }
    ]
}