{
    "q": [
        {
            "docid": "62329_18",
            "document": "Meta-analysis . Other weaknesses are that it has not been determined if the statistically most accurate method for combining results is the fixed, IVhet, random or quality effect models, though the criticism against the random effects model is mounting because of the perception that the new random effects (used in meta-analysis) are essentially formal devices to facilitate smoothing or shrinkage and prediction may be impossible or ill-advised. The main problem with the random effects approach is that it uses the classic statistical thought of generating a \"compromise estimator\" that makes the weights close to the naturally weighted estimator if heterogeneity across studies is large but close to the inverse variance weighted estimator if the between study heterogeneity is small. However, what has been ignored is the distinction between the model \"we choose\" to analyze a given dataset, and the \"mechanism by which the data came into being\". A random effect can be present in either of these roles, but the two roles are quite distinct. There's no reason to think the analysis model and data-generation mechanism (model) are similar in form, but many sub-fields of statistics have developed the habit of assuming, for theory and simulations, that the data-generation mechanism (model) is identical to the analysis model we choose (or would like others to choose). As a hypothesized mechanisms for producing the data, the random effect model for meta-analysis is silly and it is more appropriate to think of this model as a superficial description and something we choose as an analytical tool \u2013 but this choice for meta-analysis may not work because the study effects are a fixed feature of the respective meta-analysis and the probability distribution is only a descriptive tool.",
            "score": 62.095340967178345
        },
        {
            "docid": "62329_42",
            "document": "Meta-analysis . Modern statistical meta-analysis does more than just combine the effect sizes of a set of studies using a weighted average. It can test if the outcomes of studies show more variation than the variation that is expected because of the sampling of different numbers of research participants. Additionally, study characteristics such as measurement instrument used, population sampled, or aspects of the studies' design can be coded and used to reduce variance of the estimator (see statistical models above). Thus some methodological weaknesses in studies can be corrected statistically. Other uses of meta-analytic methods include the development of clinical prediction models, where meta-analysis may be used to combine data from different research centers, or even to aggregate existing prediction models.",
            "score": 91.66570925712585
        },
        {
            "docid": "37766195_48",
            "document": "Run-time estimation of system and sub-system level power consumption . This power model technique was developed by collaboration between L. Zhang, B. Tiwana, Z. Qian, Z. Wang, R.P. Dick, Z.Mao from University of Michigan and L. Yang from Google Inc. to accurately estimate power estimation online for Smartphones. PowerBooter is an automated power model that uses built-in battery voltage sensors and behavior of battery during discharge to monitor power consumption of total system. This method doesn\u2019t require any especial external measurement equipment. PowerTutor is also a power measurement tool that uses PowerBooter generated data for online power estimation. There is always a limitation in Smartphone technology battery life span that HW and SW designers need to overcome. Software designers don\u2019t always have the best knowledge of power consumption to design better power optimized applications therefore end users always blame the battery lifespan. Therefore, there is a need for a tool that has the capability to measure power consumption on Smartphones that software designers could use to monitor their applications in real-time. Researchers have developed specific power management models for specific portable embedded systems and it takes a huge effort to reuse those models for a vast variety of modern Smartphone technology. So the solution to this problem is PowerBooter model that can estimate real-time power consumption for individual Smartphone subsystems such as CPU, LCD, GPS, audio, Wi-Fi and cell phone communication components. Along with PowerBooter model an on-line PowerTutor utility can use the generated data to determine the subsystem level power consumption. The model and PowerTutor utility can be used across different platforms and Smartphone technologies.",
            "score": 69.1714893579483
        },
        {
            "docid": "2190913_56",
            "document": "Prenatal development . Growth rate of fetus is linear up to 37 weeks of gestation, after which it plateaus. The growth rate of an embryo and infant can be reflected as the weight per gestational age, and is often given as the weight put in relation to what would be expected by the gestational age. A baby born within the normal range of weight for that gestational age is known as appropriate for gestational age (AGA). An abnormally slow growth rate results in the infant being small for gestational age, and, on the other hand, an abnormally large growth rate results in the infant being large for gestational age. A slow growth rate and preterm birth are the two factors that can cause a low birth weight. Low birth weight (below 2000\u00a0grams) can ultimately increase the likelihood of schizophrenia by almost four times.  The growth rate can be roughly correlated with the fundal height which can be estimated by abdominal palpation. More exact measurements can be performed with obstetric ultrasonography.",
            "score": 160.09866452217102
        },
        {
            "docid": "416612_2",
            "document": "Cross-validation (statistics) . Cross-validation, sometimes called rotation estimation, or out-of-sample testing is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice. In a prediction problem, a model is usually given a dataset of \"known data\" on which training is run (\"training dataset\"), and a dataset of \"unknown data\" (or \"first seen\" data) against which the model is tested (called the validation dataset or \"testing set\"). The goal of cross-validation is to test the model\u2019s ability to predict new data that were not used in estimating it, in order to flag problems like overfitting and to give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem).",
            "score": 94.58472311496735
        },
        {
            "docid": "1434444_61",
            "document": "Autoregressive model . The question of how to interpret the measured forecasting accuracy arises\u2014for example, what is a \"high\" (bad) or a \"low\" (good) value for the mean squared prediction error? There are two possible points of comparison. First, the forecasting accuracy of an alternative model, estimated under different modeling assumptions or different estimation techniques, can be used for comparison purposes. Second, the out-of-sample accuracy measure can be compared to the same measure computed for the in-sample data points (that were used for parameter estimation) for which enough prior data values are available (that is, dropping the first \"p\" data points, for which \"p\" prior data points are not available). Since the model was estimated specifically to fit the in-sample points as well as possible, it will usually be the case that the out-of-sample predictive performance will be poorer than the in-sample predictive performance. But if the predictive quality deteriorates out-of-sample by \"not very much\" (which is not precisely definable), then the forecaster may be satisfied with the performance.",
            "score": 97.68828690052032
        },
        {
            "docid": "36519158_7",
            "document": "Solar power forecasting . \"Short-term\" forecasting provides predictions up to 7 days ahead. This kind of forecast is also valuable for grid operators in order to make decisions of grid operation, as well as, for electric market operators. Under this perspective, the meteorological resources are estimated at a different temporal and spatial resolution. This implies that meteorological variables and phenomena are looked from a more general perspective, not as local as nowcasting services do. In this sense, most of the approaches make use of different numerical weather prediction models (NWP) that provide an initial estimation of weather variables. Currently, several models are available for this purpose, such as Global Forecast System (GFS) or data provided by the European Center for Medium Range Weather Forecasting (ECMWF). These two models are considered the state of the art of global forecast models, which provide meteorological forecasts all over the world. In order to increase spatial and temporal resolution of these models, other models have been developed which are generally called mesoscale models. Among others, HIRLAM, WRF or MM5 are the most representative of these models since they are widely used by different communities. To run these models a wide expertise is needed in order to obtain accurate results, due to the wide variety of parameters that can be configured in the models. In addition, sophisticated techniques such as data assimilation might be used in order to produce more realistic simulations. Finally, some communities argue for the use of post-processing techniques, once the models\u2019 output is obtained, in order to obtain a probabilistic point of view of the accuracy of the output. This is usually done with ensemble techniques that mix different outputs of different models perturbed in strategic meteorological values and finally provide a better estimate of those variables and a degree of uncertainty, like in the model proposed by Bacher et al. (2009)",
            "score": 71.23345971107483
        },
        {
            "docid": "20590_27",
            "document": "Mathematical model . Usually the easiest part of model evaluation is checking whether a model fits experimental measurements or other empirical data. In models with parameters, a common approach to test this fit is to split the data into two disjoint subsets: training data and verification data. The training data are used to estimate the model parameters. An accurate model will closely match the verification data even though these data were not used to set the model's parameters. This practice is referred to as cross-validation in statistics.",
            "score": 59.413002490997314
        },
        {
            "docid": "17765521_7",
            "document": "Software development effort estimation . Most of the research has focused on the construction of formal software effort estimation models. The early models were typically based on regression analysis or mathematically derived from theories from other domains. Since then a high number of model building approaches have been evaluated, such as approaches founded on case-based reasoning, classification and regression trees, simulation, neural networks, Bayesian statistics, lexical analysis of requirement specifications, genetic programming, linear programming, economic production models, soft computing, fuzzy logic modeling, statistical bootstrapping, and combinations of two or more of these models. The perhaps most common estimation methods today are the parametric estimation models COCOMO, SEER-SEM and SLIM. They have their basis in estimation research conducted in the 1970s and 1980s and are since then updated with new calibration data, with the last major release being COCOMO II in the year 2000. The estimation approaches based on functionality-based size measures, e.g., function points, is also based on research conducted in the 1970s and 1980s, but are re-calibrated with modified size measures and different counting approaches, such as the use case points or object points in the 1990s and COSMIC in the 2000s.",
            "score": 70.08044493198395
        },
        {
            "docid": "45340131_18",
            "document": "Blood pressure measurement . Recently, several coefficient-free oscillometric algorithms have developed for estimation of blood pressure. These algorithms do not rely on experimentally obtained coefficients and have been shown to provide more accurate and robust estimation of blood pressure. These algorithms are based on finding the fundamental relationship between the oscillometric waveform and the blood pressure using modeling and learning approaches. Pulse transit time measurements have been also used to improve oscillometric blood pressure estimates.",
            "score": 108.13620710372925
        },
        {
            "docid": "847879_15",
            "document": "Age of the universe . Calculating the age of the universe is accurate only if the assumptions built into the models being used to estimate it are also accurate. This is referred to as strong priors and essentially involves stripping the potential errors in other parts of the model to render the accuracy of actual observational data directly into the concluded result. Although this is not a valid procedure in all contexts (as noted in the accompanying caveat: \"based on the fact we have assumed the underlying model we used is correct\"), the age given is thus accurate to the specified error (since this error represents the error in the instrument used to gather the raw data input into the model).",
            "score": 57.71514940261841
        },
        {
            "docid": "25766500_3",
            "document": "Knemometry . Ignaz Maria Valk developed this technique in 1983 in Nijmegen/Netherlands. It is being practiced since and used for basic research in growth and the treatment of growth disorders. Hermanussen introduced the mini-knemometer for accurate growth measurements of prematures and newborn infants. Mini-knemometry determines the lower leg length with an accuracy of less than 100\u00a0\u00b5m (0.1\u00a0mm). This enables substantiating growth within 24 hours. In an animal model, the technique was used to investigate the effects of steroids and growth hormone on short-term growth. These studies were an important prerequisite for improving growth therapies.",
            "score": 57.654381275177
        },
        {
            "docid": "37822732_3",
            "document": "History of network traffic models . Demands on computer networks are not entirely predictable. Performance modeling is necessary for deciding the quality of service (QoS) level. Performance models in turn, require accurate traffic models that have the ability to capture the statistical characteristics of the actual traffic on the network. Many traffic models have been developed based on traffic measurement data. If the underlying traffic models do not efficiently capture the characteristics of the actual traffic, the result may be the under-estimation or over-estimation of the performance of the network. This impairs the design of the network. Traffic models are hence, a core component of any performance evaluation of networks and they need to be very accurate.",
            "score": 56.15097951889038
        },
        {
            "docid": "30890995_19",
            "document": "History of numerical weather prediction . The first ocean wave models were developed in the 1960s and 1970s. These models had the tendency to overestimate the role of wind in wave development and underplayed wave interactions. A lack of knowledge concerning how waves interacted among each other, assumptions regarding a maximum wave height, and deficiencies in computer power limited the performance of the models. After experiments were performed in 1968, 1969, and 1973, wind input from the Earth's atmosphere was weighted more accurately in the predictions. A second generation of models was developed in the 1980s, but they could not realistically model swell nor depict wind-driven waves (also known as wind waves) caused by rapidly changing wind fields, such as those within tropical cyclones. This caused the development of a third generation of wave models from 1988 onward.",
            "score": 50.379433274269104
        },
        {
            "docid": "1557851_32",
            "document": "Investment management . Portfolio return may be evaluated using factor models. The first model, proposed by Jensen (1968), relies on the CAPM and explains portfolio returns with the market index as the only factor. It quickly becomes clear, however, that one factor is not enough to explain the returns very well and that other factors have to be considered. Multi-factor models were developed as an alternative to the CAPM, allowing a better description of portfolio risks and a more accurate evaluation of a portfolio's performance. For example, Fama and French (1993) have highlighted two important factors that characterize a company's risk in addition to market risk. These factors are the book-to-market ratio and the company's size as measured by its market capitalization. Fama and French therefore proposed three-factor model to describe portfolio normal returns (Fama\u2013French three-factor model). Carhart (1997) proposed to add momentum as a fourth factor to allow the short-term persistence of returns to be taken into account. Also of interest for performance measurement is Sharpe's (1992) style analysis model, in which factors are style indices. This model allows a custom benchmark for each portfolio to be developed, using the linear combination of style indices that best replicate portfolio style allocation, and leads to an accurate evaluation of portfolio alpha.",
            "score": 39.62864005565643
        },
        {
            "docid": "2458875_3",
            "document": "Data assimilation . Data assimilation initially developed in the field of numerical weather prediction. Numerical weather prediction models are equations describing the dynamical behavior of the atmosphere, typically coded into a computer program. In order to use these models to make forecasts, initial conditions are needed for the model that closely resemble the current state of the atmosphere. Simply inserting point-wise measurements into the numerical models did not provide a satisfactory solution. Real world measurements contain errors both due to the quality of the instrument and how accurately the position of the measurement is known. These errors can cause instabilities in the models that eliminate any level of skill in a forecast. Thus, more sophisticated methods were needed in order to initialize a model using all available data while making sure to maintain stability in the numerical model. Such data typically includes the measurements as well as a previous forecast valid at the same time the measurements are made. If applied iteratively, this process begins to accumulate information from past observations into all subsequent forecasts.",
            "score": 58.9297376871109
        },
        {
            "docid": "23179394_8",
            "document": "Ray Hilborn . In their research, Hilborn and Walters investigated the ways that dynamic models can be used to manage fisheries in order to maintain states of optimum equilibrium. In their paper, they examined the effectiveness of using the Ricker and Beverton-Holt models to estimate the potential yield of future generations by using data taken from prior generations. They addressed the problem that, in regards to fisheries, the parameters of the control system are often either varied or uncertain and the use of historical data becomes progressively more unreliable as it gets older. Variables, regarding these issues, include natural mortality and spawning rates as well the effects of human fishing, as a predator-prey relationship. Influenced by control theory, Hilborn and Walters modified the original models with various new formulae to create alternative models, in order to achieve more accurate predictions. They then identified \u201ca series of alternative harvesting experiments\u2026 each of which would be reasonably certain to discriminate between the alternative models\u2026\u201d The development of these alternative models and harvesting methods has been invaluable in assessing the sustainability of the world's fisheries.",
            "score": 61.10671615600586
        },
        {
            "docid": "2889768_11",
            "document": "Image stitching . To estimate a robust model from the data, a common method used is known as RANSAC.<br> The name RANSAC is an abbreviation for \"RANdom SAmple Consensus\". It is an iterative method for robust parameter estimation to fit mathematical models from sets of observed data points which may contain outliers. The algorithm is non-deterministic in the sense that it produces a reasonable result only with a certain probability, with this probability increasing as more iterations are performed. It being a probabilistic method means that different results will be obtained for every time the algorithm is run.  The RANSAC algorithm has found many applications in computer vision, including the simultaneous solving of the correspondence problem and the estimation of the fundamental matrix related to a pair of stereo cameras. The basic assumption of the method is that the data consists of \"inliers\", i.e., data whose distribution can be explained by some mathematical model, and \"outliers\" which are data that do not fit the model. Outliers are considered points which come from noise, erroneous measurements, or simply incorrect data. For the problem of homography estimation, RANSAC works by trying to fit several models using some of the point pairs and then checking if the models were able to relate most of the points. The best model, i.e., the homography which produces the highest number of correct matches, is then chosen as the answer for the problem thus if the ratio of number of outliers to data points is very low the RANSAC outputs a decent model fitting the data.",
            "score": 81.92113542556763
        },
        {
            "docid": "35658939_2",
            "document": "Verification and validation of computer simulation models . Verification and validation of computer simulation models is conducted during the development of a simulation model with the ultimate goal of producing an accurate and credible model. \"Simulation models are increasingly being used to solve problems and to aid in decision-making. The developers and users of these models, the decision makers using information obtained from the results of these models, and the individuals affected by decisions based on such models are all rightly concerned with whether a model and its results are \"correct\". This concern is addressed through verification and validation of the simulation model.",
            "score": 33.802178621292114
        },
        {
            "docid": "3878_62",
            "document": "Biostatistics . In animal and plant breeding, the use of markers in selection aiming for breeding, mainly the molecular ones, collaborated to the development of marker-assisted selection. While QTL mapping is limited due resolution, GWAS does not have enough power when rare variants of small effect that are also influenced by environment. So, the concept of Genomic Selection (GS) arises in order to use all molecular markers in the selection and allow the prediction of the performance of candidates in this selection. The proposal is to genotype and phenotype a training population, develop a model that can obtain the genomic estimated breeding values (GEBVs) of individuals belonging to a genotyped and but not phenotyped population, called testing population. This kind of study could also include a validation population, thinking in the concept of cross-validation, in which the real phenotype results measured in this population are compared with the phenotype results based on the prediction, what used to check the accuracy of the model.",
            "score": 74.38450932502747
        },
        {
            "docid": "39067260_15",
            "document": "Marine weather forecasting . The first ocean wave models were developed in the 1960s and 1970s. These models had the tendency to overestimate the role of wind in wave development and underplayed wave interactions. A lack of knowledge concerning how waves interacted among each other, assumptions regarding a maximum wave height, and deficiencies in computer power limited the performance of the models. After experiments were performed in 1968, 1969, and 1973, wind input from the Earth's atmosphere was weighted more accurately in the predictions. A second generation of models was developed in the 1980s, but they could not realistically model swell nor depict wind-driven waves (also known as wind waves) caused by rapidly changing wind fields, such as those within tropical cyclones. This caused the development of a third generation of wave models from 1988 onward.",
            "score": 49.295246720314026
        },
        {
            "docid": "429542_73",
            "document": "Preterm birth . As weight is easier to determine than gestational age, the World Health Organization tracks rates of low birth weight (< 2,500\u00a0grams), which occurred in 16.5 percent of births in less developed regions in 2000. It is estimated that one-third of these low birth weight deliveries are due to preterm delivery. Weight generally correlates to gestational age, however, infants may be underweight for other reasons than a preterm delivery. Neonates of low birth weight (LBW) have a birth weight of less than 2500 g (5\u00a0lb 8 oz) and are mostly but not exclusively preterm babies as they also include \"small for gestational age\" (SGA) babies. Weight-based classification further recognizes \"Very Low Birth Weight\" (VLBW) which is less than 1500 g, and \"Extremely Low Birth Weight\" (ELBW) which is less than 1000 g. Almost all neonates in these latter two groups are born preterm.",
            "score": 172.34491503238678
        },
        {
            "docid": "416612_39",
            "document": "Cross-validation (statistics) . The goal of cross-validation is to estimate the expected level of fit of a model to a data set that is independent of the data that were used to train the model. It can be used to estimate any quantitative measure of fit that is appropriate for the data and model. For example, for binary classification problems, each case in the validation set is either predicted correctly or incorrectly. In this situation the misclassification error rate can be used to summarize the fit, although other measures like positive predictive value could also be used. When the value being predicted is continuously distributed, the mean squared error, root mean squared error or median absolute deviation could be used to summarize the errors.",
            "score": 56.71777963638306
        },
        {
            "docid": "42831319_4",
            "document": "Dynamic stock modelling . Dynamic stock modelling (DSM) explicitly considers these different roles of in-use stocks. DSM has a long tradition in modelling population and fixed capital; over the last twenty years, applications for product and material stocks have been developed. Age-cohort-based models, state-of-the-art in DSM, are of a descriptive nature: Each age-cohort is assigned an expected lifetime and the cohort\u2019s use phase ends when its lifetime elapses. At any given point in time, in-use stocks are composed of different age-cohorts, each with its specific material content and energy efficiency. In DSM, the assumed total stock size is determined by exogenously specified parameters such as population and per capita service level and the age-cohort lifetime model can be used to adjust the inflows into and the outflows from stocks.",
            "score": 59.93948674201965
        },
        {
            "docid": "1096354_8",
            "document": "Trip distribution . With the development of logit and other discrete choice techniques, new, demographically disaggregate approaches to travel demand were attempted. By including variables other than travel time in determining the probability of making a trip, it is expected to have a better prediction of travel behavior. The logit model and gravity model have been shown by Wilson (1967) to be of essentially the same form as used in statistical mechanics, the entropy maximization model. The application of these models differs in concept in that the gravity model uses impedance by travel time, perhaps stratified by socioeconomic variables, in determining the probability of trip making, while a discrete choice approach brings those variables inside the utility or impedance function. Discrete choice models require more information to estimate and more computational time.",
            "score": 56.951916456222534
        },
        {
            "docid": "22069015_12",
            "document": "Stock assessment . The mathematical and statistical techniques used to complete a stock assessment are referred to as assessment models. Three commonly used models are surplus production models, statistical catch at age models, and virtual population analysis models. Of these models, surplus production models are the least complex and require the least amount of data. This model describes the stock solely in regards of biomass and the only used total catch and effort data. These are most commonly used in situations limited data is available on a stock. Statistical catch at age models are based on the age structure of a fished population. These models use the proportional catch-at-age to predict the relative abundance of each age class. These calculated relative abundances are then used to estimate future abundances of the stock and harvest regulations are set based on the predicted future abundances. In virtual population analysis models, catch-at-age data is used to estimate historical stock abundance. From this analysis, the manager then determines if overfishing is occurring. The type of model used depends on the data that is available. Modern stock assessment methods use statistical approaches to \"integrate\" multiple sources of information to estimate management quantities and their associated uncertainty.",
            "score": 86.43861126899719
        },
        {
            "docid": "61289_40",
            "document": "Forensic entomology . Although physical characteristics and sizes at various instars have been used to estimate fly age, a more recent study has been conducted to determine the age of an egg based on the expression of particular genes. This is particularly useful in determining developmental stages that are not evidenced by change in size; such as the egg or pupa and where only a general time interval can be estimated based on the duration of the particular developmental stage. This is done by breaking the stages down into smaller units separated by predictable changed in gene expression. Three genes were measured in an experiment with \"Drosophila melanogaster\": bicoid (bcd), slalom (sll), and chitin synthase (cs). These three genes were used because they are likely to be in varied levels during different times of the egg development process. These genes all share a linear relationship in regards to age of the egg; that is, the older the egg is the more of the particular gene is expressed. However, all of the genes are expressed in varying amounts. Different genes on different loci would need to be selected for another fly species. The genes expressions are mapped in a control sample to formulate a developmental chart of the gene expression at certain time intervals. This chart can then be compared to the measured values of gene expression to accurately predict the age of an egg to within two hours with a high confidence level. Even though this technique can be used to estimate the age of an egg, the feasibility and legal acceptance of this must be considered for it to be a widely utilized forensic technique. One benefit of this would be that it is like other DNA-based techniques so most labs would be equipped to conduct similar experiments without requiring new capital investment. This style of age determination is in the process of being used to more accurately find the age of the instars and pupa; however, it is much more complicated, as there are more genes being expressed during these stages. The hope is that with this and other similar techniques a more accurate PMI can be obtained.",
            "score": 99.3716322183609
        },
        {
            "docid": "40977477_17",
            "document": "Cross-species transmission . Two methods of measuring genetic variation, variable number tandem repeats (VNTRs) and single nucleotide polymorphisms (SNPs), have been very beneficial to the study of bacterial transmission. VNTRs, due to the low cost and high mutation rates, make them particularly useful to detect genetic differences in recent outbreaks, and while SNPs have a lower mutation rate per locus than VNTRs, they deliver more stable and reliable genetic relationships between isolates. Both methods are used to construct phylogenies for genetic analysis, however, SNPs are more suitable for studies on phylogenies contraction. However, it can be difficult for these methods accurately simulate CSTs everts. Estimates of CST based on phylogenys made using VNTR marker can be biased towards detecting CST events across a wide range of the parameters. SNPs tend to be less biased and variable in estimates of CST when estimations of CST rates are low and low number of SNPs is used. In general, CST rate estimates using these methods are most reliable in systems with more mutations, more markers, and high genetic differences between introduced strains. CST is very complex and models need to account for a lot of parameters to accurately represent the phenomena. Models that oversimplify reality can result in biased data. Multiple parameters such as number of mutations accumulated since introduction, stochasticity, the genetic difference of strains introduced, and the sampling effort can make unbiased estimates of CST difficult even with whole-genome sequences, especially if sampling is limited, mutation rates are low, or if pathogens were recently introduced. More information on the factors that influence CST rates is needed for the contraction of more appropriate models to study these events.",
            "score": 95.22835195064545
        },
        {
            "docid": "3547877_3",
            "document": "Eddy covariance . The technique is also used extensively for verification and tuning of global climate models, mesoscale and weather models, complex biogeochemical and ecological models, and remote sensing estimates from satellites and aircraft. The technique is mathematically complex, and requires significant care in setting up and processing data. To date, there is no uniform terminology or a single methodology for the Eddy Covariance technique, but much effort is being made by flux measurement networks (e.g., FluxNet, Ameriflux, ICOS, CarboEurope, Fluxnet Canada, OzFlux, NEON, and iLEAPS) to unify the various approaches. The technique has additionally proven applicable under water to the benthic zone for measuring oxygen fluxes between seafloor and overlying water. In these environments, the technique is generally known as the eddy correlation technique, or just eddy correlation. Oxygen fluxes are extracted from raw measurements largely following the same principles as used in the atmosphere, and they are typically used as a proxy for carbon exchange, which is important for local and global carbon budgets. For most benthic ecosystems, eddy correlation is the most accurate technique for measuring \"in-situ\" fluxes. The technique's development and its applications under water remains a fruitful area of research.",
            "score": 48.95838224887848
        },
        {
            "docid": "20590_20",
            "document": "Mathematical model . In black-box models one tries to estimate both the functional form of relations between variables and the numerical parameters in those functions. Using a priori information we could end up, for example, with a set of functions that probably could describe the system adequately. If there is no a priori information we would try to use functions as general as possible to cover all different models. An often used approach for black-box models are neural networks which usually do not make assumptions about incoming data. Alternatively the NARMAX (Nonlinear AutoRegressive Moving Average model with eXogenous inputs) algorithms which were developed as part of nonlinear system identification can be used to select the model terms, determine the model structure, and estimate the unknown parameters in the presence of correlated and nonlinear noise. The advantage of NARMAX models compared to neural networks is that NARMAX produces models that can be written down and related to the underlying process, whereas neural networks produce an approximation that is opaque.",
            "score": 68.7975754737854
        },
        {
            "docid": "13803804_6",
            "document": "Calculation of glass properties . Schott and many scientists and engineers afterwards applied the additivity principle to experimental data measured in their own laboratory within sufficiently narrow composition ranges (local glass models). This is most convenient because disagreements between laboratories and non-linear glass component interactions do not need to be considered. In the course of several decades of systematic glass research thousands of glass compositions were studied, resulting in millions of published glass properties, collected in glass databases. This huge pool of experimental data was not investigated as a whole, until Bottinga, Kucuk, Priven, Choudhary, Mazurin, and Fluegel published their global glass models, using various approaches. In contrast to the models by Schott the global models consider many independent data sources, making the model estimates more reliable. In addition, global models can reveal and quantify \"non-additive\" influences of certain glass component combinations on the properties, such as the \"mixed-alkali effect\" as seen in the diagram on the right, or the \"boron anomaly\". Global models also reflect interesting developments of glass property measurement accuracy, e.g., a decreasing accuracy of experimental data in modern scientific literature for some glass properties, shown in the diagram. They can be used for accreditation of new data, experimental procedures, and measurement institutions (glass laboratories). In the following sections (except melting enthalpy) \"empirical\" modeling techniques are presented, which seem to be a successful way for handling huge amounts of experimental data. The resulting models are applied in contemporary engineering and research for the calculation of glass properties.",
            "score": 62.881473541259766
        },
        {
            "docid": "634_2",
            "document": "Analysis of variance . Analysis of variance (ANOVA) is a collection of statistical models and their associated estimation procedures (such as the \"variation\" among and between groups) used to analyze the differences among group means in a sample. ANOVA was developed by statistician and evolutionary biologist Ronald Fisher. In the ANOVA setting, the observed variance in a particular variable is partitioned into components attributable to different sources of variation. In its simplest form, ANOVA provides a statistical test of whether the population means of several groups are equal, and therefore generalizes the \"t\"-test to more than two groups. ANOVA is useful for comparing (testing) three or more group means for statistical significance. It is conceptually similar to multiple two-sample t-tests, but is more conservative (results in less type I error) and is therefore suited to a wide range of practical problems. While the analysis of variance reached fruition in the 20th century, antecedents extend centuries into the past according to Stigler. These include hypothesis testing, the partitioning of sums of squares, experimental techniques and the additive model. Laplace was performing hypothesis testing in the 1770s. The development of least-squares methods by Laplace and Gauss circa 1800 provided an improved method of combining observations (over the existing practices then used in astronomy and geodesy). It also initiated much study of the contributions to sums of squares. Laplace knew how to estimate a variance from a residual (rather than a total) sum of squares. By 1827 Laplace was using least squares methods to address ANOVA problems regarding measurements of atmospheric tides. Before 1800 astronomers had isolated observational errors resulting  from reaction times (the \"personal equation\") and had developed methods of reducing the errors. The experimental methods used in the study of the personal equation were later accepted by the emerging field of psychology which developed strong (full factorial) experimental methods to which randomization and blinding were soon added. An eloquent non-mathematical explanation of the additive effects model was available in 1885.",
            "score": 106.83586776256561
        }
    ],
    "r": [
        {
            "docid": "768605_30",
            "document": "Newborn screening . Newborn screening tests are most commonly done from whole blood samples collected on specially designed filter paper. The filter paper is often attached to a form containing required information about the infant and parents. This includes date and time of birth, date and time of sample collection, the infant's weight and gestational age. The form will also have information about whether the baby has had a blood transfusion and any additional nutrition the baby may have received (total parenteral nutrition). Most newborn screening cards also include contact information for the infant's physician in cases where follow up screening or treatment is needed. The Canadian province of Quebec performs newborn screening on whole blood samples collected as in most other jurisdictions, and also runs a voluntary urine screening program where parents collect a sample at 21 days of age and submit it to a provincial laboratory for an additional panel of conditions.",
            "score": 220.17529296875
        },
        {
            "docid": "429542_2",
            "document": "Preterm birth . Preterm birth, also known as premature birth, is the birth of a baby at fewer than 37 weeks gestational age. These babies are known as preemies or premies. Symptoms of preterm labor include uterine contractions which occur more often than every ten minutes or the leaking of fluid from the vagina. Premature infants are at greater risk for cerebral palsy, delays in development, hearing problems and sight problems. These risks are greater the earlier a baby is born. The cause of preterm birth is often not known. Risk factors include diabetes, high blood pressure, being pregnant with more than one baby, being either obese or underweight, a number of vaginal infections, tobacco smoking and psychological stress, among others. It is recommended that labor not be medically induced before 39 weeks unless required for other medical reasons. The same recommendation applies to cesarean section. Medical reasons for early delivery include preeclampsia. In those at risk, the hormone progesterone, if taken during pregnancy, may prevent preterm birth. Evidence does not support the usefulness of bed rest. It is estimated that at least 75% of preterm infants would survive with appropriate treatment, and the survival rate is highest among the infants born the latest. In women who might deliver between 24 and 37\u00a0weeks, corticosteroids improve outcomes. A number of medications, including nifedipine, may delay delivery so that a mother can be moved to where more medical care is available and the corticosteroids have a greater chance to work. Once the baby is born, care includes keeping the baby warm through skin to skin contact, supporting breastfeeding, treating infections and supporting breathing. Preterm birth is the most common cause of death among infants worldwide. About 15 million babies are preterm each year (5% to 18% of all deliveries). Approximately 0.5% of births are extremely early periviable births, and these account for most of the deaths. In many countries, rates of premature births have increased between the 1990s and 2010s. Complications from preterm births resulted in 0.81 million deaths in 2015 down from 1.57 million in 1990. The chance of survival at 22 weeks is about 6%, while at 23 weeks it is 26%, 24 weeks 55% and 25 weeks about 72%. The chances of survival without any long-term difficulties are lower.",
            "score": 195.7203826904297
        },
        {
            "docid": "697890_2",
            "document": "Gestational diabetes . Gestational diabetes is a condition in which a woman without diabetes develops high blood sugar levels during pregnancy. Gestational diabetes generally results in few symptoms; however, it does increase the risk of pre-eclampsia, depression, and requiring a Caesarean section. Babies born to mothers with poorly treated gestational diabetes are at increased risk of being too large, having low blood sugar after birth, and jaundice. If untreated, it can also result in a stillbirth. Long term, children are at higher risk of being overweight and developing type 2 diabetes. Gestational diabetes is caused by not enough insulin in the setting of insulin resistance. Risk factors include being overweight, previously having gestational diabetes, a family history of type 2 diabetes, and having polycystic ovarian syndrome. Diagnosis is by blood tests. For those at normal risk screening is recommended between 24 and 28 weeks' gestation. For those at high risk testing may occur at the first prenatal visit. Prevention is by maintaining a healthy weight and exercising before pregnancy. Gestational diabetes is a treated with a diabetic diet, exercise, and possibly insulin injections. Most women are able to manage their blood sugar with a diet and exercise. Blood sugar testing among those who are affected is often recommended four times a day. Breastfeeding is recommended as soon as possible after birth. Gestational diabetes affects 3\u20139% of pregnancies, depending on the population studied. It is especially common during the last three months of pregnancy. It affects 1% of those under the age of 20 and 13% of those over the age of 44. A number of ethnic groups including Asians, American Indians, Indigenous Australians, and Pacific Islanders are at higher risk. In 90% of people gestational diabetes will resolve after the baby is born. Women, however, are at an increased risk of developing type 2 diabetes.",
            "score": 193.50140380859375
        },
        {
            "docid": "65909_2",
            "document": "Neonatal heel prick . The neonatal heel prick is a blood collection procedure done on newborns. It consists of making a pinprick puncture in one heel of the newborn to collect their blood. This technique is used frequently as the main way to collect blood from neonates. Other techniques include venous or arterial needle sticks, cord blood sampling, or umbilical line collection. This technique is often utilized for the Guthrie test, where it is used to soaking the blood into pre-printed collection cards known as Guthrie cards.",
            "score": 183.2799835205078
        },
        {
            "docid": "12110212_36",
            "document": "DNA database . Many countries collect newborn blood samples to screen for diseases mainly with a genetic basis. Mainly these are destroyed soon after testing. In some countries the dried blood (and the DNA) is retained for later testing.  In Denmark the Danish Newborn Screening Biobank at Statens Serum Institut keeps a blood sample from people born after 1981. The purpose is to test for phenylketonuria and other diseases. However, it is also used for DNA profiling to identify deceased and suspected criminals. Parents can request that the blood sample of their newborn be destroyed after the result of the test is known.",
            "score": 174.0946044921875
        },
        {
            "docid": "83449_73",
            "document": "Childbirth . Preterm birth is the birth of an infant at fewer than 37 weeks gestational age. It is estimated that 1 in 10 babies are born prematurely. Premature birth is the leading cause of death in children under 5 years of age though many that survive experience disabilities including learning defects and visual and hearing problems. Causes for early birth may be unknown or may be related to certain chronic conditions such as diabetes, infections, and other known causes. The World Health Organization has developed guidelines with recommendations to improve the chances of survival and health outcomes for preterm infants.",
            "score": 173.1724853515625
        },
        {
            "docid": "429542_73",
            "document": "Preterm birth . As weight is easier to determine than gestational age, the World Health Organization tracks rates of low birth weight (< 2,500\u00a0grams), which occurred in 16.5 percent of births in less developed regions in 2000. It is estimated that one-third of these low birth weight deliveries are due to preterm delivery. Weight generally correlates to gestational age, however, infants may be underweight for other reasons than a preterm delivery. Neonates of low birth weight (LBW) have a birth weight of less than 2500 g (5\u00a0lb 8 oz) and are mostly but not exclusively preterm babies as they also include \"small for gestational age\" (SGA) babies. Weight-based classification further recognizes \"Very Low Birth Weight\" (VLBW) which is less than 1500 g, and \"Extremely Low Birth Weight\" (ELBW) which is less than 1000 g. Almost all neonates in these latter two groups are born preterm.",
            "score": 172.34490966796875
        },
        {
            "docid": "36128779_11",
            "document": "High-risk pregnancy . Multiple gestation. Pregnancy with twins, triplets, or more, referred to as a multiple gestation, increases the risk of infants being born prematurely (before 37 weeks of pregnancy). Having infants after age 30 and taking fertility drugs both have been associated with multiple births. Having three or more infants increases the chance that a woman will need to have the infants delivered by cesarean section. Twins and triplets are more likely to be smaller for their size than infants of singleton births. If infants of multiple gestation are born prematurely, they are more likely to have difficulty breathing.16 Gestational diabetes. Gestational diabetes, also known as gestational diabetes mellitus, GDM, or diabetes during pregnancy, is diabetes that first develops when a woman is pregnant. Many women can have healthy pregnancies if they manage their diabetes, following a diet and treatment plan from their health care provider. Uncontrolled gestational diabetes increases the risk for preterm labor and delivery, preeclampsia, and high blood pressure. Preeclampsia and eclampsia. Preeclampsia is a syndrome marked by a sudden increase in the blood pressure of a pregnant woman after the 20th week of pregnancy. It can affect the mother's kidneys, liver, and brain. When left untreated, the condition can be fatal for the mother and/or the fetus and result in long-term health problems. Eclampsia is a more severe form of preeclampsia, marked by seizures and coma in the mother.",
            "score": 171.6634521484375
        },
        {
            "docid": "1771587_2",
            "document": "Pregnancy . Pregnancy, also known as gestation, is the time during which one or more offspring develops inside a woman. A multiple pregnancy involves more than one offspring, such as with twins. Pregnancy can occur by sexual intercourse or assisted reproductive technology. Childbirth typically occurs around 40\u00a0weeks from the last menstrual period (LMP). This is just over nine\u00a0months, where each month averages 29\u00bd days. When measured from conception it is about 38 weeks. An embryo is the developing offspring during the first eight weeks following conception, after which, the term \"fetus\" is used until birth. Symptoms of early pregnancy may include missed periods, tender breasts, nausea and vomiting, hunger, and frequent urination. Pregnancy may be confirmed with a pregnancy test. Pregnancy is typically divided into three trimesters. The first trimester is from week one through 12 and includes conception. Conception is when the sperm fertilizes the egg. The fertilized egg then travels down the fallopian tube and attaches to the inside of the uterus, where it begins to form the embryo and placenta. During the first trimester, the possibility of miscarriage (natural death of embryo or fetus) is at its highest. The second trimester is from week 13 through 28. Around the middle of the second trimester, movement of the fetus may be felt. At 28 weeks, more than 90% of babies can survive outside of the uterus if provided with high-quality medical care. The third trimester is from 29 weeks through 40 weeks. Prenatal care improves pregnancy outcomes. Prenatal care may include taking extra folic acid, avoiding drugs and alcohol, regular exercise, blood tests, and regular physical examinations. Complications of pregnancy may include disorders of high blood pressure, gestational diabetes, iron-deficiency anemia, and severe nausea and vomiting among others. In the ideal childbirth labor begins on its own when a woman is \"at term\". Pregnancy is considered at full term when gestation has lasted 39 to 41 weeks. After 41 weeks, it is known as late term and after 42 weeks post term. Babies born before 39 weeks are considered early term while those before 37 weeks are preterm. Preterm babies are at higher risk of health problems such as cerebral palsy. Delivery before 39 weeks by labor induction or caesarean section is not recommended unless required for other medical reasons. About 213 million pregnancies occurred in 2012, of which, 190 million were in the developing world and 23 million were in the developed world. The number of pregnancies in women ages 15 to 44 is 133 per 1,000 women. About 10% to 15% of recognized pregnancies end in miscarriage. In 2013, complications of pregnancy resulted in 293,000 deaths, down from 377,000 deaths in 1990. Common causes include maternal bleeding, complications of abortion, high blood pressure of pregnancy, maternal sepsis, and obstructed labor. Globally, 40% of pregnancies are unplanned. Half of unplanned pregnancies are aborted. Among unintended pregnancies in the United States, 60% of the women used birth control to some extent during the month pregnancy occurred.",
            "score": 170.41107177734375
        },
        {
            "docid": "6334060_5",
            "document": "Robert Guthrie . Guthrie developed a simple method to screen for elevated phenylalanine levels using a bacterial inhibition assay. He cultured \"Bacillus subtilis\" on agar in the presence of a phenylalanine antagonist, inhibiting the growth. When exposed to blood from patients affected with PKU, the high levels of phenylalanine overcame the inhibition, and bacterial growth was visible. This assay was initially developed to allow monitoring of phenylalanine concentrations in known patients on dietary treatment using serum spotted onto filter paper. Guthrie recognized both the utility of this method as a screening test, and the need to eliminate serum as the sample type to minimize processing. He tested the assay using whole blood collected on filter paper from a heel stick. The collection of whole blood on special filter paper developed by Guthrie is still used in newborn screening programs around the world, allowing babies to be screened shortly after birth for a number of treatable conditions.",
            "score": 170.31895446777344
        },
        {
            "docid": "13421323_3",
            "document": "Percutaneous umbilical cord blood sampling . Percutaneous umbilical cord blood sampling (PUBS), also called cordocentesis, fetal blood sampling, or umbilical vein sampling is a diagnostic genetic test that examines blood from the fetal umbilical cord to detect fetal abnormalities. Fetal and maternal blood supply are typically connected in utero with one vein and two arteries to the fetus. The umbilical vein is responsible for delivering oxygen rich blood to the fetus from the mother; the umbilical arteries are responsible for removing oxygen poor blood from the fetus. This allows for the fetus\u2019 tissues to properly perfuse. PUBS provides a means of rapid chromosome analysis and is useful when information cannot be obtained through amniocentesis, chorionic villus sampling, or ultrasound (or if the results of these tests were inconclusive); this test carries a significant risk of complication and is typically reserved for pregnancies determined to be at high risk for genetic defect. It has been used with mothers with immune thrombocytopenic purpura.",
            "score": 166.94833374023438
        },
        {
            "docid": "65909_7",
            "document": "Neonatal heel prick . The blood spot sample should be taken between 48 and 72 hours of age for all babies regardless of medical condition, milk feeding and prematurity. For the purpose of screening, date of birth is day 0 (some IT systems record date of birth as day 1). False positives and false negatives can sometimes occur when the screening tests are performed before 48 hours.",
            "score": 165.87942504882812
        },
        {
            "docid": "1467374_12",
            "document": "Gestational age . There is no sharp limit of development, gestational age, or weight at which a human fetus automatically becomes viable. According to studies between 2003 and 2005, 20 to 35 percent of babies born at 23 weeks of gestation survive, while 50 to 70 percent of babies born at 24 to 25 weeks, and more than 90 percent born at 26 to 27 weeks, survive. It is rare for a baby weighing less than 500\u00a0g (17.6\u00a0ounces) to survive. A baby's chances for survival increases 3-4% per day between 23 and 24 weeks of gestation and about 2-3% per day between 24 and 26 weeks of gestation. After 26 weeks the rate of survival increases at a much slower rate because survival is high already. Prognosis depends also on medical protocols on whether to resuscitate and aggressively treat a very premature newborn, or whether to provide only palliative care, in view of the high risk of severe disability of very preterm babies.",
            "score": 165.87228393554688
        },
        {
            "docid": "53653356_24",
            "document": "Elective genetic and genomic testing . Newborn screening is a type of testing that assesses risk for certain genetic, endocrine, metabolic disorders, hearing loss and critical congenital heart defects. Each state determines the exact list of conditions that are screened. Early detection, diagnosis, and intervention can prevent death or disability and enable children to reach their full potential. The testing is performed from a few drops of blood collected in the newborn period, often by a heel stick. The exact method of testing may vary but often uses levels of specific analytes present in the blood of the baby. Because this is a screening test, additional testing is often necessary to confirm a diagnosis.",
            "score": 164.16372680664062
        },
        {
            "docid": "310782_12",
            "document": "Genetic testing . Routine newborn screening tests are done on a small blood sample obtained by pricking the baby's heel with a lancet.",
            "score": 163.25796508789062
        },
        {
            "docid": "25936010_3",
            "document": "Single umbilical artery . Most cords have one vein and two arteries. The vein carries oxygenated blood from the placenta to the baby and the arteries carry deoxygenated blood from the baby to the placenta. In approximately 1% of pregnancies there are only two vessels \u2014usually a single vein and single artery. In about 75% of those cases, the baby is entirely normal and healthy and the missing artery isn't missed at all. One artery can support a pregnancy and does not necessarily indicate problems. For the other 25%, a 2-vessel cord is a sign that the baby has other abnormalities\u2014sometimes life-threatening and sometimes not. SUA does increase the risk of the baby having cardiac, skeletal, intestinal or renal problems. Babies with SUA may have a higher likelihood of having other congenital abnormalities, especially of the heart. However, additional testing (high level ultrasound scans) can rule out many of these abnormalities prior to birth and alleviate parental anxiety. Echocardiograms of the fetus may be advised to ensure the heart is functioning properly. Genetic counseling may be useful, too, especially when weighing the pros and cons of more invasive procedures such as chorionic villus sampling and amniocentesis.",
            "score": 163.22230529785156
        },
        {
            "docid": "52711100_8",
            "document": "Lula Lubchenco . In the early 1960s, Lubchenco began to publish her research on the relationship between birth weight and gestational age in newborns. A chart that allowed clinicians to plot a baby's birth weight against its gestational age became informally known as the \"Lulagram\". Before Lubchenco began her work, babies with low birth weights were referred to as premature. Her work led to the popularization of the term \"low birth weight\", which underscored the fact that such babies may or may not have been born early. The descriptors small for gestational age, appropriate for gestational age and large for gestational age originated with Lubchenco's work.",
            "score": 162.7890167236328
        },
        {
            "docid": "13421323_5",
            "document": "Percutaneous umbilical cord blood sampling . If the fetus is viable, the procedure is performed close to an operating room in case an emergency cesarean section is necessary due to complications caused by the procedure. Currently, there is no definite age of viability because this depends on the fetus\u2019 ability to survive outside the womb, which in cases of premature births, can depend on access to medical care and technology needed to keep the fetus alive through the neonatal stage. Fetal viability typically occurs at about 24 to 25 weeks of gestation. When the fetus is in between the ages of 24\u201334 weeks, a glucocorticoid is given to the patient about 24 hours before the procedure to stimulate lung maturity. An ultrasound is performed before the procedure to view the position of the fetus and may be used during the procedure to help guide the needle. The mother\u2019s blood is drawn for comparison against fetal blood, and intravenous access is established in the mother in order to supply medications as needed. To reduce the risk of intraamniotic infection, antibiotics are supplied through the intravenous access about 30\u201360 minutes before the procedure. If movement of the fetus is a risk to the success of the procedure, the fetus may be paralyzed using a fetal paralytic drug.",
            "score": 161.8273468017578
        },
        {
            "docid": "1965183_5",
            "document": "Large for gestational age . LGA and macrosomia cannot be diagnosed until after birth, as it is impossible to accurately estimate the size and weight of a child in the womb. Babies that are large for gestational age throughout the pregnancy may be suspected because of an ultrasound, but fetal weight estimations in pregnancy are quite imprecise. For non-diabetic women, ultrasounds and care providers are equally inaccurate at predicting whether or not a baby will be big. If an ultrasound or a care provider predicts a big baby, they will be wrong half the time.",
            "score": 161.70394897460938
        },
        {
            "docid": "6052485_23",
            "document": "Postpartum bleeding . A Cochrane review suggests that active management (use of uterotonic drugs, cord clamping and controlled cord traction) during the third stage of labour reduces severe bleeding and anemia. However, the review also found that active management increased the mother's blood pressure, nausea, vomiting, and pain. In the active management group more women returned to hospital with bleeding after discharge, and there was also a reduction in birthweight due to infants having a lower blood volume. The effects on the baby of early cord clamping was discussed in another review which found that delayed cord clamping improved iron stores longer term in the infants. Although they were more likely to need phototherapy (light therapy) to treat jaundice, the improved iron stores are expected to be worth increasing the practice of delayed cord clamping in healthy term babies. For preterm babies (babies born before 37 weeks) a review of the research found that delaying cord clamping by 30\u201345 seconds increased the amount of blood flow to the baby. This is important as increased blood volume in the baby made them less likely to develop some serious complications. Much of the research around this subject is poor quality so further, larger research projects are likely to produce more reliable results.",
            "score": 161.24078369140625
        },
        {
            "docid": "2190913_56",
            "document": "Prenatal development . Growth rate of fetus is linear up to 37 weeks of gestation, after which it plateaus. The growth rate of an embryo and infant can be reflected as the weight per gestational age, and is often given as the weight put in relation to what would be expected by the gestational age. A baby born within the normal range of weight for that gestational age is known as appropriate for gestational age (AGA). An abnormally slow growth rate results in the infant being small for gestational age, and, on the other hand, an abnormally large growth rate results in the infant being large for gestational age. A slow growth rate and preterm birth are the two factors that can cause a low birth weight. Low birth weight (below 2000\u00a0grams) can ultimately increase the likelihood of schizophrenia by almost four times.  The growth rate can be roughly correlated with the fundal height which can be estimated by abdominal palpation. More exact measurements can be performed with obstetric ultrasonography.",
            "score": 160.09866333007812
        },
        {
            "docid": "21010263_46",
            "document": "Sickle cell disease . In the United Kingdom, it is thought that between 12,000 and 15,000 people have sickle cell disease with an estimate of 250,000 carriers of the condition in England alone. As the number of carriers is only estimated, all newborn babies in the UK receive a routine blood test to screen for the condition. Due to many adults in high-risk groups not knowing if they are carriers, pregnant women and both partners in a couple are offered screening so they can get counselling if they have the sickle cell trait. In addition blood donors from those in high-risk groups are also screened to confirm whether they are carriers and whether their blood filters properly. Donors who are found to be carriers are then informed and their blood, while often used for those of the same ethnic group, is not used for those with sickle cell disease who require a blood transfusion.",
            "score": 159.50323486328125
        },
        {
            "docid": "45423909_4",
            "document": "Hypertensive disease of pregnancy . Chronic poorly-controlled high blood pressure before and during pregnancy puts a pregnant woman and her baby at risk for problems. It is associated with an increased risk for maternal complications such as\u00a0preeclampsia, placental abruption (when the placenta separates from the wall of the uterus), and gestational diabetes. These women also face a higher risk for poor birth outcomes such as\u00a0preterm delivery, having an infant small for his/her gestational age, and\u00a0infant death.",
            "score": 159.4043426513672
        },
        {
            "docid": "233253_23",
            "document": "Umbilical cord . From 24 to 34 weeks of gestation, when the fetus is typically viable, blood can be taken from the cord in order to test for abnormalities (particularly for hereditary conditions). This diagnostic genetic test procedure is known as percutaneous umbilical cord blood sampling.",
            "score": 159.2694854736328
        },
        {
            "docid": "13214434_22",
            "document": "HIV/AIDS in Australia . In addition, the postnatal care taken to reduce risks of MTCT include avoiding procedures where the baby's skin may be cut or electing to have a cesarean section to reduce the risk of contact with body fluids. Ensuring the baby's eyes and head are cleaned, the umbilical cord is clamped as soon as possible and placing an absorption pack (towel or sponge) over the umbilical cord when cut to prevent blood spurting will also reduce the risk of the baby coming in contact with any contaminated fluids. Bottle feeding the baby also removes any chance of coming in contact with infected body fluids. Along with the mother taking anti-retroviral medication, giving the baby a course of this until it is 4\u20136 weeks of age also drastically reduces its risk of transmission. Medical practitioners also require the infant undergo regular blood tests at 1 week, 6 weeks, 12 weeks, 6 months, 12 months and 18 months to test for any evidence of the HIV virus.",
            "score": 159.08163452148438
        },
        {
            "docid": "9704676_9",
            "document": "Anemia of prematurity . Another treatment used is therapeutic strategies. These strategies are aimed at reducing transfusions have assessed the use of strict blood transfusions guidelines and EPO therapy, but reduction of blood loss is most important. For extremely low birth weight infants, laboratory blood testing using bedside devices offers a unique opportunity to reduce blood transfusions. This practice has been referred to as point-of-care testing. Use of these kind of devices to measure the most common ordered blood tests could significantly decrease phlebotomy loss and lead to a reduction in the need for blood transfusions among critically ill premature neonates. A study was done by Adams, Benitz, Geaghan, Kumar, Madan and Widness (2005) to test this theory by conducting a retrospective chart review on all inborn infants <1000g admitted to the NICU during two separate years. Conventional bench top laboratory analysis during the first year was done using Radiometer Blood Gas and Electrolyte Analyzer. Bedside blood gas analysis during the second year was performed using a point-of-care analyzer. An estimated blood loss in the two groups was determined based on the number of specific blood tests on individual infants. The study found that there was an estimated 30% reduction in the total volume of blood removed for the blood tests. This study concluded that there is modern technology that can be used instead of blood transfusions and r-EPO.",
            "score": 158.8944549560547
        },
        {
            "docid": "9704676_5",
            "document": "Anemia of prematurity . AOP is usually treated by blood transfusion but the indications for this are still unclear. Blood transfusions have the risk of incompatibility and transfusion reactions as well as viral infections. Also, blood transfusions are costly and add to parental anxiety. The best treatment in prevention is minimizing the amount of blood drawn from the infant. It is found that since blood loss attributable to laboratory testing is the primary cause of anemia among preterm infants during the first weeks of life, we quantified blood lost attributable to phlebotomy overdraw, something that might be avoided. A study was done to see when and if overdraw was a problem. They recorded all of the data that could be of influence such as the test performed, the blood collection container used, the infants location (neonatal intensive care unit (NICU) and intermediate intensive care unit), the infant\u2019s weight sampling and the phlebotomist\u2019s level of experience, work shift, and clinical role. Infants were classified by weight into 3 groups: <1\u00a0kg, 1 to 2\u00a0kg, and >2\u00a0kg. The volume of blood removed was calculated by subtracting the weight of the empty collection container from that of the container filled with blood. They found that the mean volume of blood drawn for the 578 tests exceeded that requested by the hospital laboratory by 19.0% \u00b1 1.8% per test. The main factors of overdraw was: collection in blood containers without fill-lines, lighter weight infants and critically ill infants being cared for in the NICU.",
            "score": 157.6597900390625
        },
        {
            "docid": "57449359_2",
            "document": "Neonatal red cell transfusion . Neonates are defined as babies up to 28 days after birth. Most extremely preterm babies (less than 28 weeks) require at least one red cell transfusion, this is partly due to the amount of blood removed with blood samples compared to the baby's total blood volume (iatrogenic anemia) and partly due to anemia of prematurity. Most transfusions are given as small volume top-up transfusions to increase the baby's hemoglobin above a certain pre-defined level, or because the baby is unwell due to the anemia. Possible side-effects of anemia in babies can be poor growth, lethargy and episodes of apnea. Exchange blood transfusion is used to treat a rapidly rising bilirubin that does not respond to treatment with phototherapy or intravenous immunoglobulin. This is usually due to hemolytic disease of the newborn, but may also be due to other causes e.g. G6PD deficiency.",
            "score": 156.2007293701172
        },
        {
            "docid": "29243665_43",
            "document": "Prenatal nutrition . Many factors account for LGA babies, including genetics and excessive nutrient supply. It seems that a common factor for LGA babies is whether or not the mother has diabetes when she is pregnant. An indicator for excessive growth, regardless of gestational age, is the appearance of macrosomia. Many complications are observed for LGA babies and their mothers. A longer delivery time may be expected since it is a difficult birth. The infant would likely suffer hypoglycemia (low glucose level in the blood) after birth. The infant would also have difficulty breathing.",
            "score": 155.45901489257812
        },
        {
            "docid": "3055427_7",
            "document": "Nutrition and pregnancy . Multiple micronutrient supplements taken with iron and folic acid can improve birth outcomes for women in low income countries. These supplements reduce numbers of low birth weight babies, small for gestational age babies and stillbirths in women who may not have many micronutrients in their usual diets. Undernourished women can benefit from having dietary education sessions and, balanced energy and protein supplements. A review showed that dietary education increased the mother\u2019s protein intake and helped the baby grow more inside the womb. The balanced protein and energy supplement lowered risk of stillbirth and small babies and increased weight gain for both the mother and baby. Although more research is needed into the longer term effects on the mothers\u2019 and infants\u2019 health, the short term effects look promising.",
            "score": 155.0422821044922
        },
        {
            "docid": "19391_5",
            "document": "Midwifery . First trimester screening varies by country. Women are typically offered a Pap smear and urine analysis (UA), and blood tests including a complete blood count (CBC), blood typing (including Rh screen), syphilis, hepatitis, HIV, and rubella testing. Additionally, women may have chlamydia testing via a urine sample, and women considered at high risk are screened for Sickle Cell disease and Thalassemia. Women must consent to all tests before they are carried out. The woman's blood pressure, height and weight are measured. Her past pregnancies and family, social, and medical history are discussed. Women may have an ultrasound scan during the first trimester which may be used to help find the estimated due date. Some women may have genetic testing, such as screening for Down's Syndrome. Diet, exercise, and discomforts such as morning sickness are discussed. The mother visits the midwife monthly or more often during the second trimester. The mother's partner and/or the labor coach may accompany her. The midwife will discuss pregnancy issues such as fatigue, heartburn, varicose veins, and other common problems such as back pain. Blood pressure and weight are monitored and the midwife measures the mother's abdomen to see if the baby is growing as expected. Lab tests such as a UA, CBC, and glucose tolerance test are done if the midwife feels they are necessary.",
            "score": 154.86927795410156
        },
        {
            "docid": "10228998_8",
            "document": "Dried blood spot . The technology holds promise for expanding diagnostic services to HIV-infected infants in resource-poor settings due to the samples' longer lifespan with reduced need for refrigeration and the less invasive nature of the test compared with other methods. Unlike ELISA testing for HIV-antibodies in the blood, which may be transmitted to infants in pregnancy independently of the virus itself, dried blood spot testing can be used to detect genetic material of the actual virus, thereby avoiding the likelihood of a false positive result.  Dried blood spot testing for HIV is not considered sensitive enough for diagnostic testing, but may be useful in estimating prevalence of HIV through surveillance.  DBS specimens also pose less of a biohazard risk to handlers, and are easier to transport or store than liquid blood specimens.",
            "score": 153.5319061279297
        }
    ]
}