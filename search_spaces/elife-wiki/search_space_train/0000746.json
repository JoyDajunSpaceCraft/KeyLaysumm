{
    "q": [
        {
            "docid": "7800961_12",
            "document": "Fusiform face area . The FFA is underdeveloped in children and does not fully develop until adolescence. This calls into question the evolutionary purpose of the FFA, as children show the ability to differentiate faces. Two-year-old babies have been shown to prefer the face of their mother. Although the FFA is underdeveloped in two-year-old babies, they have the ability to recognize their mother. Babies as early as three months old have shown the ability to distinguish between faces. During this time, babies exhibit the ability to differentiate between genders, showing a clear preference for female faces. It is theorized that, in terms of evolution, babies focus on women for food, although the preference could simply reflect a bias for the caregivers they experience. Infants do not appear to use this area for the perception of faces. Recent fMRI work has found no face selective area in the brain of infants 4 to 6 months old. However, given that the adult human brain has been studied far more extensively than the infant brain, and that infants are still undergoing major neurodevelopmental processes, it may simply be that the FFA is not located in anatomically familiar area. It may also be that activation for many different percepts and cognitive tasks in infants is diffuse in terms of neural circuitry, as infants are still undergoing periods of neurogenesis and neural pruning; this may make it more difficult to distinguish the signal, or what we would imagine as visual and complex familiar objects (like faces), from the noise, including static firing rates of neurons, and activity that is dedicated to a different task entirely than the activity of face processing. Infant vision involves only light and dark recognition, recognizing only major features of the face, activating the amygdala. These findings question the evolutionary purpose of the FFA.",
            "score": 183.57466995716095
        },
        {
            "docid": "5212945_3",
            "document": "Visual neuroscience . A recent study using Event-Related Potentials (ERPs) linked an increased neural activity in the occipito-temporal region of the brain to the visual categorization of facial expressions. Results focus on a negative peak in the ERP that occurs 170 milliseconds after the stimulus onset. This action potential, called the N170, was measured using electrodes in the occipito-temporal region, an area already known to be changed by face stimuli. Studying by using the EEG, and ERP methods allow for an extremely high temporal resolution of 4 milliseconds, which makes these kinds of experiments extremely well suited for accurately estimating and comparing the time it takes the brain to perform a certain function. Scientists used classification image techniques, to determine what parts of complex visual stimuli (such as a face) will be relied on when patients are asked to assign them to a category, or emotion. They computed the important features when the stimulus face exhibited one of five different emotions. Stimulus faces exhibiting fear had the distinguishing feature of widening eyes, and stimuli exhibiting happiness exhibited a change in the mouth to make a smile. Regardless of the expression of the stimuli's face, the region near the eyes affected the EEG before the regions near the mouth. This revealed a sequential, and predetermined order to the perception and processing of faces, with the eye being the first, and the mouth, and nose being processed after. This process of downward integration only occurred when the inferior facial features were crucial to the categorization of the stimuli. This is best explained by comparing what happens when participants were shown a face exhibiting fear, versus happiness. The N170 peaked slightly earlier for the fear stimuli at about 175 milliseconds, meaning that it took a participants less time to recognize the facial expression. This is expected because only the eyes need to be processed to recognize the emotion. However, when processing a happy expression, where the mouth is crucial to categorization, downward integration must take place, and thus the N170 peak occurred later at around 185 milliseconds. Eventually visual neuroscience aims to completely explain how the visual system processes all changes in faces as well as objects. This will give a complete view to how the world is constantly visually perceived, and may provide insight into a link between perception and consciousness.",
            "score": 151.70216643810272
        },
        {
            "docid": "33702464_5",
            "document": "Extrastriate body area . The experiment had subjects view images of different objects, including faces (as a control group), body parts, animals, parts of the face and intimate objects. While viewing the images, the subjects were scanned with an fMRI to see what area of the brain was activated. Through the trials a compilation of the fMRI\u2019s was made. From this compilation image a specific region was determined to have increased activity when shown visual stimuli of body parts and even more activity when viewing whole bodies. There have been no studies involving brain damage to the EBA. Thus far, only scans of brain activity, as well as transcranial magnetic stimulation, have been used to study the EBA. To find the specific functions of the EBA, Comimo Urgesi, Giovanni Berlucchi and Salvatore M. Aglioti used repetitive transcranial magnetic stimulation (rTMS) to disrupt part of the brain, making the brain less responsive in the target area. The study used event-related rTMS to disrupt the EBA, resulting in inactivation of cortical areas. This inactivation caused a slower response time in discriminating body parts. The study used facial features and motorcycle parts as non human parts for control groups. The facial features and motorcycle body parts did not display any change in response time. The neural activity data shows the EBA handles some of the visual processing of human body and parts but is not related to the processing of the face or other objects.",
            "score": 167.8081773519516
        },
        {
            "docid": "25146378_12",
            "document": "Functional specialization (brain) . One of the most well known examples of functional specialization is the fusiform face area (FFA). Justine Sergent was one of the first researchers that brought forth evidence towards the functional neuroanatomy of face processing. Using positron emission tomography (PET), Sergent found that there were different patterns of activation in response to the two different required tasks, face processing verses object processing. These results can be linked with her studies of brain-damaged patients with lesions in the occipital and temporal lobes. Patients revealed that there was an impairment of face processing but no difficulty recognizing everyday objects, a disorder also known as prosopagnosia. Later research by Nancy Kanwisher using functional magnetic resonance imaging (fMRI), found specifically that the region of the inferior temporal cortex, known as the fusiform gyrus, was significantly more active when subjects viewed, recognized and categorized faces in comparison to other regions of the brain. Lesion studies also supported this finding where patients were able to recognize objects but unable to recognize faces. This provided evidence towards domain specificity in the visual system, as Kanwisher acknowledges the Fusiform Face Area as a module in the brain, specifically the extrastriate cortex, that is specialized for face perception.",
            "score": 170.95846378803253
        },
        {
            "docid": "525667_10",
            "document": "Human echolocation . In a 2014 study by Thaler and colleagues, the researchers first made recordings of the clicks and their very faint echoes using tiny microphones placed in the ears of the blind echolocators as they stood outside and tried to identify different objects such as a car, a flag pole, and a tree. The researchers then played the recorded sounds back to the echolocators while their brain activity was being measured using functional magnetic resonance imaging. Remarkably, when the echolocation recordings were played back to the blind experts, not only did they perceive the objects based on the echoes, but they also showed activity in those areas of their brain that normally process visual information in sighted people, primarily primary visual cortex or V1. This result is surprising, as visual areas, as their names suggest, are only active during visual tasks. The brain areas that process auditory information were no more activated by sound recordings of outdoor scenes containing echoes than they were by sound recordings of outdoor scenes with the echoes removed. Importantly, when the same experiment was carried out with sighted people who did not echolocate, these individuals could not perceive the objects and there was no echo-related activity anywhere in the brain. This suggests that the cortex of blind echolocators is plastic and reorganizes such that primary visual cortex, rather than any auditory area, becomes involved in the computation of echolocation tasks.",
            "score": 88.20841634273529
        },
        {
            "docid": "32197396_4",
            "document": "Form perception . In addition to photoreceptors, the eye requires a properly functioning lens, retina, and an undamaged optic nerve to recognize form. Light travels through the lens, hits the retina, activates the appropriate photoreceptors, depending on available light, which convert the light into an electrical signal that travels along the optic nerve to the lateral geniculate nucleus of the thalamus and then to the primary visual cortex. In the cortex, the adult brain processes information such as lines, orientation, and color. These inputs are integrated in the occipito-temporal cortex where a representation of the object as a whole is created. Visual information continues to be processed in the posterior parietal cortex, also known as the dorsal stream, where the representation of an object\u2019s shape is formed using motion-based cues. It is believed that simultaneously information is processed in the anterior temporal cortex, also known as the ventral stream, where object recognition, identification and naming occur. In the process of recognizing an object, both the dorsal and ventral streams are active, but the ventral stream is more important in discriminating between and recognizing objects. The dorsal stream contributes to object recognition only when two objects have similar shapes and the images are degraded. Observed latency in activation of different parts of the brain supports the idea of hierarchal processing of visual stimuli, with object representations progressing from simple to complex.",
            "score": 99.8950206041336
        },
        {
            "docid": "32197396_5",
            "document": "Form perception . By five months of age infants are capable of using line junction information to perceive three-D images, including depth and shape, like adults are able. However, there are differences between younger infants and adults in the ability to use motion and color cues to discriminate between two objects. Visual information then continues to be processed in the posterior parietal cortex, also known as the dorsal stream, where the representation of an objects shape is formed using motion-based cues. The identification of differences between the infant and adult brain make it clear that there is either functional reorganization of the infant\u2019s cortex or simply age related differences in which the breed impulses have been observed in infants. Although the infant brain is not identical to the adult brain, it is similar with areas of specialization and a hierarchy of processing,[7] however, adult abilities to perceive form from stationary viewing are not fully understood.",
            "score": 100.77613520622253
        },
        {
            "docid": "1764639_17",
            "document": "Levels-of-processing effect . Several brain imaging studies using positron emission tomography and functional magnetic resonance imaging techniques have shown that higher levels of processing correlate with more brain activity and activity in different parts of the brain than lower levels. For example, in a lexical analysis task, subjects showed activity in the left inferior prefrontal cortex only when identifying whether the word represented a living or nonliving object, and not when identifying whether or not the word contained an \"a\". Similarly, an auditory analysis task showed increased activation in the left inferior prefrontal cortex when subjects performed increasingly semantic word manipulations. Synaptic aspects of word recognition have been correlated with the left frontal operculum and the cortex lining the junction of the inferior frontal and inferior precentral sulcus. The self-reference effect also has neural correlates with a region of the medial prefrontal cortex, which was activated in an experiment where subjects analyzed the relevance of data to themselves. Specificity of processing is explained on a neurological basis by studies that show brain activity in the same location when a visual memory is encoded and retrieved, and lexical memory in a different location. Visual memory areas were mostly located within the bilateral extrastriate visual cortex.",
            "score": 94.19584786891937
        },
        {
            "docid": "4231622_5",
            "document": "Inferior temporal gyrus . The temporal lobe is unique to primates. In humans, the IT cortex is more complex than their relative primate counterparts. The human inferior temporal cortex consists of the inferior temporal gyrus, the middle temporal gyrus, and the fusiform gyrus. When looking at the brain laterally \u2013 that is from the side and looking at the surface of the temporal lobe \u2013 the inferior temporal gyrus is along the bottom portion of the temporal lobe, and is separated from the middle temporal gyrus located directly above by the inferior temporal sulcus. Additionally, some processing of the visual field that corresponds to the ventral stream of visual processing occurs in the lower portion of the superior temporal gyrus closest to the superior temporal sulcus. The medial and ventral view of the brain \u2013 meaning looking at the medial surface from below the brain, facing upwards \u2013 reveals that the inferior temporal gyrus is separated from the fusiform gyrus by the occipital-temporal sulcus. This human inferior temporal cortex is much more complex than that of other primates: non-human primates have an inferior temporal cortex that is not divided into unique regions such as humans' inferior temporal gyrus, fusiform gyrus, or middle temporal gyrus.  This region of the brain corresponds to the inferior temporal cortex and is responsible for visual object recognition and receives processed visual information. The inferior temporal cortex in primates has specific regions dedicated to processing different visual stimuli processed and organized by the different layers of the striate cortex and extra-striate cortex. The information from the V1 \u2013V5 regions of the geniculate and tectopulvinar pathways are radiated to the IT cortex via the ventral stream: visual information specifically related to the color and form of the visual stimuli. Through comparative research between primates \u2013 humans and non-human primates \u2013 results indicate that the IT cortex plays a significant role in visual shape processing. This is supported by functional magnetic resonance imaging (fMRI) data collected by researchers comparing this neurological process between humans and macaques.",
            "score": 108.15837597846985
        },
        {
            "docid": "485309_16",
            "document": "Face perception . There are several parts of the brain that play a role in face perception. Rossion, Hanseeuw, and Dricot used BOLD fMRI mapping to identify activation in the brain when subjects viewed both cars and faces. The majority of BOLD fMRI studies use blood oxygen level dependent (BOLD) contrast to determine which areas of the brain are activated by various cognitive functions. They found that the occipital face area, located in the occipital lobe, the fusiform face area, the superior temporal sulcus, the amygdala, and the anterior/inferior cortex of the temporal lobe, all played roles in contrasting the faces from the cars, with the initial face perception beginning in the area and occipital face areas. This entire region links to form a network that acts to distinguish faces. The processing of faces in the brain is known as a \"sum of parts\" perception. However, the individual parts of the face must be processed first in order to put all of the pieces together. In early processing, the occipital face area contributes to face perception by recognizing the eyes, nose, and mouth as individual pieces. Furthermore, Arcurio, Gold, and James used BOLD fMRI mapping to determine the patterns of activation in the brain when parts of the face were presented in combination and when they were presented singly. The occipital face area is activated by the visual perception of single features of the face, for example, the nose and mouth, and preferred combination of two-eyes over other combinations. This research supports that the occipital face area recognizes the parts of the face at the early stages of recognition. On the contrary, the fusiform face area shows no preference for single features, because the fusiform face area is responsible for \"holistic/configural\" information, meaning that it puts all of the processed pieces of the face together in later processing. This theory is supported by the work of Gold et al. who found that regardless of the orientation of a face, subjects were impacted by the configuration of the individual facial features. Subjects were also impacted by the coding of the relationships between those features. This shows that processing is done by a summation of the parts in the later stages of recognition.",
            "score": 186.3813226222992
        },
        {
            "docid": "37691351_6",
            "document": "Neuroscience and race . The first step the brain does to encode a memory is to process the face. The lateral fusiform gyrus is a facial recognition area of the brain. Within this brain region, the fusiform face area (FFA) analyzes the configuration and holistic appearance of the face. The FFA is more activated when viewing same-race faces compared to other-race faces. As time progresses from when the face is first viewed, the differences in FFA activation diminish. It\u2019s believed that the FFA is more activated when viewing a same-race face because the brain individuates (using more analytic power) the same-race faces while simply categorizing other-race faces. The FFA isn\u2019t the only region involved in facial recognition that effects the cross-race effect, but also the whole ventral temporal cortex (VT cortex). Scientists are able to distinguish which race face one is viewing at simply by viewing the VT cortex. Additionally, the fusiform cortex plays a vital role in categorizing race faces. This section is also more activated when viewing same-race faces, as it is studying the face in greater detail. However, these differences in activation of the fusiform complex diminish when a familiar other-race face is shown, like a celebrity.",
            "score": 145.6324918270111
        },
        {
            "docid": "51462681_3",
            "document": "Objective vision . This is the story of what's happening when you see a picture, even too fast, the brain's visual cortex recognizes what it sees immediately. The visual cortex has a critical job in processing and it's the most complex part of brain. The human brain is much more aware of how it solves complex problems such as playing chess or solving algebra equations, which is why computer programmers have had so much success building machines that emulate this type of activity. but when entities visionary system starts to convert the signals to image(actually the separated shapes and colors) to find a relation between brain's information and those images. The system actually is concentrating on the separable sections, this separation gives the brain a visionary system the excellence processing result, because with this method the system do not waste much time on processing non significant sections and signals. this operation in the Objective Vision project called objective processing and because the O.V. mission is around human visionary simulation, so the developer refers with Objective Vision.",
            "score": 84.98225581645966
        },
        {
            "docid": "33826142_4",
            "document": "Brain activity and meditation . Electroencephalography (EEG) has been used in many studies as a primary method for evaluating the meditating brain. Electroencephalography uses electrical leads placed all over the scalp to measure the collective electrical activity of the cerebral cortex. Specifically, EEG measures the electric fields of large groups of neurons. EEG has the benefit of excellent temporal resolution and is able to measure aggregate activity of portions or the entire cortex down to the millisecond scale. Unlike other imaging based methods, EEG does not have good spatial resolution and is more appropriately used to evaluate the running spontaneous activity of the cortex. This spontaneous activity is classified into four main classifications based on the frequency of the activity, ranging from low frequency delta waves (< 4\u00a0Hz) commonly found during sleep to beta waves (13\u201330\u00a0Hz) associated with an awake and alert brain. In between these two extremes are theta waves (4\u20138\u00a0Hz) and alpha waves (8\u201312\u00a0Hz).",
            "score": 65.54263305664062
        },
        {
            "docid": "2566101_4",
            "document": "Long-term video-EEG monitoring . In either case, these EEG measuring techniques allow one to non-invasively measure action potentials of groups of neurons within the brain using transducers called electrodes. The electrical signals from these electrode transducers are then amplified using differential amplifiers to measure potential differences from one area of the scalp or brain to another. The acquired analog signal is then converted to a digital signal to allow processing and storage of the data using an analog-to-digital converter which is then filtered to remove any signal noise not associated with the neuronal activity. The final signal can then be displayed on an external computer screen as a visual representation of the EEG signals measured. Other EEG techniques can sum these cellular responses either temporally or spatially and help determine which areas of the brain are active during specific activities or due to specific stimuli.",
            "score": 78.92799210548401
        },
        {
            "docid": "485309_6",
            "document": "Face perception . Infants are able to comprehend facial expressions as social cues representing the feelings of other people before they are a year old. At seven months, the object of an observed face's apparent emotional reaction is relevant in processing the face. Infants at this age show greater negative central components to angry faces that are looking directly at them than elsewhere, although the direction of fearful faces' gaze produces no difference. In addition, two ERP components in the posterior part of the brain are differently aroused by the two negative expressions tested. These results indicate that infants at this age can at least partially understand the higher level of threat from anger directed at them as compared to anger directed elsewhere. By at least seven months of age, infants are also able to use facial expressions to understand others' behavior. Seven-month-olds will look to facial cues to understand the motives of other people in ambiguous situations, as shown by a study in which they watched an experimenter's face longer if she took a toy from them and maintained a neutral expression than if she made a happy expression. Interest in the social world is increased by interaction with the physical environment. Training three-month-old infants to reach for objects with Velcro-covered \"sticky mitts\" increases the amount of attention that they pay to faces as compared to passively moving objects through their hands and non-trained control groups.",
            "score": 134.8011839389801
        },
        {
            "docid": "57515641_34",
            "document": "Face inversion effect . Prosopagnosia is a condition marked by an inability to recognise faces. When those with prosopagnosia view faces, the fusiform gyrus (a facial recognition area of the brain) activates differently to how it would in someone without the condition. Additionally, non-facial object recognition areas (such as the ventral occipitotemporal extrastriate cortex) are activated when viewing faces, suggesting that faces and objects are processed similarly.",
            "score": 140.04854583740234
        },
        {
            "docid": "21312318_27",
            "document": "Recognition memory . Recognition memory is critically dependent on a hierarchically organized network of brain areas including the visual ventral stream, medial temporal lobe structures, frontal lobe and parietal cortices along with the hippocampus. As mentioned previously, the processes of recollection and familiarity are represented differently in the brain. As such, each of the regions listed above can be further subdivided according to which part is primarily involved in recollection or in familiarity. In the temporal cortex, for instance, the medial region is related to recollection whereas the anterior region is related to familiarity. Similarly, in the parietal cortex, the lateral region is related to recollection whereas the superior region is related to familiarity. An even more specific account divides the medial parietal region, relating the posterior cingulate to recollection and the precuneus to familiarity. The hippocampus plays a prominent role in recollection whereas familiarity depends heavily on the surrounding medial-temporal regions, especially the perirhinal cortex. Finally, it is not yet clear what specific regions of the prefrontal lobes are associated with recollection versus familiarity, although there is evidence that the left prefrontal cortex is correlated more strongly with recollection whereas the right prefrontal cortex is involved more in familiarity. Though left-side activation involved in recollection was originally hypothesized to result from semantic processing of words (many of these earlier studies used written words for stimuli) subsequent studies using nonverbal stimuli produced the same finding\u2014suggesting that prefrontal activation in the left hemisphere results from any kind of detailed remembering.  As previously mentioned, recognition memory is not a stand-alone concept; rather it is a highly interconnected and integrated sub-system of memory. Perhaps misleadingly, the regions of the brain listed above correspond to an abstract and highly generalized understanding of recognition memory, in which the stimuli or items-to-be-recognized are not specified. In reality, however, the location of brain activation involved in recognition is highly dependent on the nature of the stimulus itself. Consider the conceptual differences in recognizing written words compared to recognizing human faces. These are two qualitatively different tasks and as such it is not surprising that they involve additional, distinct regions of the brain. Recognizing words, for example, involves the visual word form area, a region in the left fusiform gyrus, which is believed to specialized in recognizing written words. Similarly, the fusiform face area, located in the right hemisphere, is linked specifically to the recognition of faces.",
            "score": 149.34502506256104
        },
        {
            "docid": "25146378_15",
            "document": "Functional specialization (brain) . During the 1960s, Roger Sperry conducted a natural experiment on epileptic patients who had previously had their corpora callosa cut. The corpus callosum is the area of the brain dedicated to linking both the right and left hemisphere together. Sperry et al.'s experiment was based on flashing images in the right and left visual fields of his participants. Because the participant's corpus callosum was cut, the information processed by each visual field could not be transmitted to the other hemisphere. In one experiment, Sperry flashed images in the right visual field (RVF), which would subsequently be transmitted to the left hemisphere (LH) of the brain. When asked to repeat what they had previously seen, participants were fully capable of remembering the image flashed. However, when the participants were then asked to draw what they had seen, they were unable to. When Sperry et al. flashed images in the left visual field (LVF), the information processed would be sent to the right hemisphere (RH) of the brain. When asked to repeat what they had previously seen, participants were unable to recall the image flashed, but were very successful in drawing the image. Therefore, Sperry concluded that the left hemisphere of the brain was dedicated to language as the participants could clearly speak the image flashed. On the other hand, Sperry concluded that the right hemisphere of the brain was involved in more creative activities such as drawing.",
            "score": 112.33491933345795
        },
        {
            "docid": "179092_18",
            "document": "Neurolinguistics . Electrophysiological techniques take advantage of the fact that when a group of neurons in the brain fire together, they create an electric dipole or current. The technique of EEG measures this electric current using sensors on the scalp, while MEG measures the magnetic fields that are generated by these currents. In addition to these non-invasive methods, electrocorticography has also been used to study language processing. These techniques are able to measure brain activity from one millisecond to the next, providing excellent \"temporal resolution\", which is important in studying processes that take place as quickly as language comprehension and production. On the other hand, the location of brain activity can be difficult to identify in EEG; consequently, this technique is used primarily to \"how\" language processes are carried out, rather than \"where\". Research using EEG and MEG generally focuses on event-related potentials (ERPs), which are distinct brain responses (generally realized as negative or positive peaks on a graph of neural activity) elicited in response to a particular stimulus. Studies using ERP may focus on each ERP's \"latency\" (how long after the stimulus the ERP begins or peaks), \"amplitude\" (how high or low the peak is), or \"topography\" (where on the scalp the ERP response is picked up by sensors). Some important and common ERP components include the N400 (a negativity occurring at a latency of about 400 milliseconds), the mismatch negativity, the early left anterior negativity (a negativity occurring at an early latency and a front-left topography), the P600, and the lateralized readiness potential.",
            "score": 78.43920052051544
        },
        {
            "docid": "37691351_7",
            "document": "Neuroscience and race . Top-down and bottom-up processing are terms used to describe the differences in memory processing when observing same-race and other-race faces. Bottom-up processing puts together pieces of a whole and develops one grand picture. Top-down processing uses more initial cognitive work by breaking down the whole picture into pieces, and then analyzing those pieces. Bottom-up processing is used in processing same-race faces, and requires much less brain activation than top-down processing, which is used while processing other-race faces. When bottom-up brain processing is used while viewing same-race faces, a holistic face in perceived, encoded and remembered. When top-down brain processing is used while viewing other-race faces, only features of the face are perceived and encoded. Once the face is perceived by the VT cortex, the hippocampus is used to encode the memory in the parietal lobe. Overall, same-race faces undergo better memory encoding processes than other-race faces because they are remembered more often, however, other-race faces that are remembered undergo a more effortful memory encoding process. More brain activation is needed to effectively encode an other-race face. Memory encoding isn't the only found cause of the cross-race effect; memory retrieval is also involved. In retrieving a memory, the parietal lobe is reactivated. When retrieving an other-race face, there is more reactivation of the parietal lobe, meaning more effort is needed to retrieve an other-race face memory. The frontal lobe is also activated while observing other-race faces if the parietal lobe is unable to retrieve the memory, acting as a search engine in the brain looking for the location of the memory.",
            "score": 123.8867039680481
        },
        {
            "docid": "7725524_2",
            "document": "Colour centre . The colour centre is a region in the brain primarily responsible for visual perception and cortical processing of colour signals received by the eye, which ultimately results in colour vision. The colour centre in humans is thought to be located in the ventral occipital lobe as part of the visual system, in addition to other areas responsible for recognizing and processing specific visual stimuli, such as faces, words, and objects. Many functional magnetic resonance imaging (fMRI) studies in both humans and macaque monkeys have shown colour stimuli to activate multiple areas in the brain, including the fusiform gyrus and the lingual gyrus. These areas, as well as others identified as having a role in colour vision processing, are collectively labelled visual area 4 (V4). The exact mechanisms, location, and function of V4 are still being investigated.",
            "score": 119.1941351890564
        },
        {
            "docid": "33246145_4",
            "document": "Neural decoding . When looking at a picture, people's brains are constantly making decisions about what object they are looking at, where they need to move their eyes next, and what they find to be the most salient aspects of the input stimulus. As these images hit the back of the retina, these stimuli are converted from varying wavelengths to a series of neural spikes called action potentials. These pattern of action potentials are different for different objects and different colors; we therefore say that the neurons are encoding objects and colors by varying their spike rates or temporal pattern. Now, if someone were to probe the brain by placing electrodes in the primary visual cortex, they may find what appears to be random electrical activity. These neurons are actually firing in response to the lower level features of visual input, possibly the edges of a picture frame. This highlights the crux of the neural decoding hypothesis: that it is possible to reconstruct a stimulus from the response of the ensemble of neurons that represent it. In other words, it is possible to look at spike train data and say that the person or animal being recorded is looking at a red ball.",
            "score": 95.3632904291153
        },
        {
            "docid": "33932515_25",
            "document": "Social cue . Higher level visual regions, such as the fusiform gyrus, extrastriate cortex and superior temporal sulcus (STS) are the areas of the brain that studies have found that link to perceptual processing of social/biological stimuli. Data collected from behavioral studies have found that the right hemisphere is highly connected with the processing of left visual field advantage for face and gaze stimuli. Researchers believe the right STS is also involved in using gaze to understand the intentions of others. While looking at social and nonsocial cues, it has been found that a high level of activity has been found in the bilateral extrastriate cortices in regards to gaze cues versus peripheral cues. There was a study done on two people with split-brain, in order to study each hemisphere to see what their involvement is in gaze cuing. Results suggest that gaze cues show a strong effect with the facial recognition hemisphere of the brain, compared to nonsocial cues. The results of Greene and Zaidel's study suggest that in relation to visual fields, information is processed independently and that the right hemisphere shows greater orienting.",
            "score": 131.03807508945465
        },
        {
            "docid": "486429_2",
            "document": "Prosopagnosia . Prosopagnosia, also called face blindness, is a cognitive disorder of face perception in which the ability to recognize familiar faces, including one's own face (self-recognition), is impaired, while other aspects of visual processing (e.g., object discrimination) and intellectual functioning (e.g., decisionmaking) remain intact. The term originally referred to a condition following acute brain damage (acquired prosopagnosia), but a congenital or developmental form of the disorder also exists, which may affect up to 2.5% of the United States population. The specific brain area usually associated with prosopagnosia is the fusiform gyrus, which activates specifically in response to faces. The functionality of the fusiform gyrus allows most people to recognize faces in more detail than they do similarly complex inanimate objects. For those with prosopagnosia, the new method for recognizing faces depends on the less-sensitive object recognition system. The right hemisphere fusiform gyrus is more often involved in familiar face recognition than the left. It remains unclear whether the fusiform gyrus is only specific for the recognition of human faces or if it is also involved in highly trained visual stimuli.",
            "score": 171.27861833572388
        },
        {
            "docid": "485309_11",
            "document": "Face perception . Recognizing and perceiving faces are vital abilities needed to coexist in society. Faces can tell things such as identity, mood, age, sex, race, and the direction that someone is looking. Studies based on neuropsychology, behavior, electrophysiology, and neuro-imaging have supported the notion of a specialized mechanism for perceiving faces. Prosopagnosia patients demonstrate neuropsychological support for a specialized face perception mechanism as these people, due to brain damage, have deficits in facial perception, but their cognitive perception of objects remains intact. The face inversion effect provides behavioral support of a specialized mechanism as people tend to have greater deficits in task performance when prompted to react to an inverted face than to an inverted object. Electrophysiological support comes from the finding that the N170 and M170 responses tend to be face-specific. Neuro-imaging studies such as PET and fMRI studies have shown support for a specialized facial processing mechanism as they have identified regions of the fusiform gyrus that have higher activation during face perception tasks than other visual perception tasks. Theories about the processes involved in adult face perception have largely come from two sources: research on normal adult face perception and the study of impairments in face perception that are caused by brain injury or neurological illness. Novel optical illusions such as the Flashed Face Distortion Effect, in which scientific phenomenology outpaces neurological theory, also provide areas for research.",
            "score": 158.86556887626648
        },
        {
            "docid": "485309_33",
            "document": "Face perception . This agrees with the object form topology hypothesis proposed by Ishai and colleagues in 1999. However, the relatedness of object and facial perception was process-based, and appears to be associated with their common holistic processing strategy in the right hemisphere. Moreover, when the same men were presented with facial paradigm requiring analytic processing, the left hemisphere was activated. This agrees with the suggestion made by Gauthier in 2000, that the extrastriate cortex contains areas that are best suited for different computations, and described as the process-map model. Therefore, the proposed models are not mutually exclusive, and this underscores the fact that facial processing does not impose any new constraints on the brain other than those used for other stimuli.",
            "score": 124.09292912483215
        },
        {
            "docid": "2213172_5",
            "document": "Minimally conscious state . Some areas of the brain that are correlated with the subjective experience of pain were activated in MCS patients when noxious stimulation was present. Positron emission tomography (PET) scans found increased blood flow to the secondary sensory cortex, posterior parietal cortex, premotor cortex, and the superior temporal cortex. The pattern of activation, however, was with less spatial extent. Some parts of the brain were less activated than normal patients during noxious stimulus processing. These were the posterior cingulate, medial prefrontal cortex, and the occipital cortex. Even though functional brain imaging can objectively measure changes in brain function during noxious stimulation, the role of different areas of the brain in pain processing is only partially understood. Furthermore, there is still the problem of the subjective experience. MCS patients by definition cannot consistently and reliably communicate their experiences. Even if they were able to answer the question \"are you in pain?\", there would not be a reliable response. Further clinical trials are needed to access the appropriateness of the use of analgesia in patients with MCS.",
            "score": 79.15190720558167
        },
        {
            "docid": "1316947_4",
            "document": "Ambiguous image . When we see an image, the first thing we do is attempt to organize all the parts of the scene into different groups. To do this, one of the most basic methods used is finding the edges. Edges can include obvious perceptions such as the edge of a house, and can include other perceptions that the brain needs to process deeper, such as the edges of a person's facial features. When finding edges, the brain's visual system detects a point on the image with a sharp contrast of lighting. Being able to detect the location of the edge of an object aids in recognizing the object. In ambiguous images, detecting edges still seems natural to the person perceiving the image. However, the brain undergoes deeper processing to resolve the ambiguity. For example, consider an image that involves an opposite change in magnitude of luminance between the object and the background (e.g. From the top, the background shifts from black to white, and the object shifts from white to black). The opposing gradients will eventually come to a point where there is an equal degree of luminance of the object and the background. At this point, there is no edge to be perceived. To counter this, the visual system connects the image as a whole rather than a set of edges, allowing one to see an object rather than edges and non-edges. Although there is no complete image to be seen, the brain is able to accomplish this because of its understanding of the physical world and real incidents of ambiguous lighting. In ambiguous images, an illusion is often produced from illusory contours. An illusory contour is a perceived contour without the presence of a physical gradient. In examples where a white shape appears to occlude black objects on a white background, the white shape appears to be brighter than the background, and the edges of this shape produce the illusory contours. These illusory contours are processed by the brain in a similar way as real contours. The visual system accomplishes this by making inferences beyond the information that is presented in much the same way as the luminance gradient.",
            "score": 98.63046681880951
        },
        {
            "docid": "22391885_6",
            "document": "Neural processing for individual categories of objects . It may be that the use of distinct brain regions for processing different object categories results from different processing requirements necessary for each class. Indeed, Malach et al. (2002) detail findings that buildings and faces require processing at different resolutions in order to be recognised - face recognition requires the analysis of fine detail, while buildings can be recognised using larger scale feature integration. As a result, faces are associated with central visual field processing while buildings are processed more peripherally. Malach et al. (2002) report that points on the retina sharing foveal centricity are mapped onto parallel cortical bands and it therefore follows that object classes that are processed differently by retinal cells should be represented distinctly within the brain. Consistently, faces and buildings were found to be processed independently of each other and in discrete cortical regions suggesting that processing is facilitated by assigning object categories to distinct cortical regions according to the level and type of processing that they require.",
            "score": 141.99889969825745
        },
        {
            "docid": "21571449_7",
            "document": "Prosopamnesia . Within the brain, visual stimuli are processed along many different neural circuits. Due to the evolutionary importance of being able to recognize faces and associate information with others based on this recognition, humans have evolved a distinct neural circuit for the processing of facial stimuli. Since the discovery of this distinct circuit, the anatomical structures involved have been studied in depth. The initial processing of visual stimuli occurs in the prefrontal cortex (PFC), postparietal cortex (PPC), and precuneus. The stimuli are then identified as being facial and more refined processing occurs within the fusiform face area (FFA), the occipital face area (OFA), and the face-selective region of the superior temporal sulcus (fSTS). The FFA serves low level tasks, such as distinguishing details between similar well-known objects. The OFA and fSTS serve higher level processing tasks, such as connecting a person's identity to their face and processing emotions based on the arrangement of facial features, respectively. Once facial stimuli have been processed, they are then encoded into memory. This involves many brain structures including the medial temporal lobe (MTL), and the hippocampus. Storage and retrieval of these memories involves the same regions of the FFA, PFA, and PPC that performed the initial processing tasks.",
            "score": 124.26604354381561
        },
        {
            "docid": "1038052_31",
            "document": "Neuroesthetics . Emotions play a large role in aesthetic processing. Experiments designed specifically to force the subjects to view the artwork subjectively (by inquiring of its aesthetic appeal) rather than simply with the visual systems, revealed a higher activation in the brain's emotional circuitry. Results from these experiments revealed high activation in the bilateral insula which can be attributed to the emotional experience of viewing art. This correlates with other known emotional roles of the insula. However, the correlation between the insula's varying states of activation and positive or negative emotions in this context is unknown. The emotional view of art can be contrasted with perception related to object recognition when pragmatically viewing art. The right fusiform gyrus has been revealed to show activation to visual stimuli such as faces and representational art. This holds importance in the field because as Ramachandran also speculated, object recognition and the search for meaning can evoke a pleasant emotional response. The motor cortex was also shown to be involved in aesthetic perception. However, it displayed opposite trends of activation from the OFC. It may be a common correlate for the perception of emotionally charged stimuli despite its previously known roles. Several other areas of the brain were shown to be slightly activated during certain studies such as the anterior cingulate cortex, previously known for its involvement in the feeling of romance, and the left parietal cortex, whose purpose may be to direct spatial attention.",
            "score": 118.74143779277802
        },
        {
            "docid": "37691351_4",
            "document": "Neuroscience and race . Neurotechnology enables studying the brain and racial interactions, though this study can be difficult because these interactions can be hard to replicate. Face recognition tests are the most commonly used method in studying racial interactions. These tests consist of observing own-race and other-race faces, and studying the brain's response to the faces. There are three major neurological techniques used to measure the brain's response to these simulated racial interactions. Functional magnetic resonance imaging (fMRI) measures the brain activity through measuring the blood oxygen level in the brain. This test gives insight into which regions of the brain are active during a certain event. Event-related potentials (ERPs) measure the brain's activity through measuring electrical impulses by electrodes on the head. This test gives insight in rapid changes in the brain. Transcranial magnetic stimulation (TMS) measures the response of a region of the brain once activated through magnetism. This test gives insight into causality of occurrences and gives specific insight in what the brain regions are doing. Brain-damaged patients have also been used to study racial interactions, by studying how racial interactions are affected when specific brain regions are damaged. These studies give insight into how different brain regions are involved in racial interactions once certain regions have been damaged. An implicit association test (IAC) is often used to measure the racial bias of people in studies by testing what objects, whether positive or negative, people associate with same-race or other-race faces.",
            "score": 159.44162392616272
        }
    ],
    "r": [
        {
            "docid": "485309_16",
            "document": "Face perception . There are several parts of the brain that play a role in face perception. Rossion, Hanseeuw, and Dricot used BOLD fMRI mapping to identify activation in the brain when subjects viewed both cars and faces. The majority of BOLD fMRI studies use blood oxygen level dependent (BOLD) contrast to determine which areas of the brain are activated by various cognitive functions. They found that the occipital face area, located in the occipital lobe, the fusiform face area, the superior temporal sulcus, the amygdala, and the anterior/inferior cortex of the temporal lobe, all played roles in contrasting the faces from the cars, with the initial face perception beginning in the area and occipital face areas. This entire region links to form a network that acts to distinguish faces. The processing of faces in the brain is known as a \"sum of parts\" perception. However, the individual parts of the face must be processed first in order to put all of the pieces together. In early processing, the occipital face area contributes to face perception by recognizing the eyes, nose, and mouth as individual pieces. Furthermore, Arcurio, Gold, and James used BOLD fMRI mapping to determine the patterns of activation in the brain when parts of the face were presented in combination and when they were presented singly. The occipital face area is activated by the visual perception of single features of the face, for example, the nose and mouth, and preferred combination of two-eyes over other combinations. This research supports that the occipital face area recognizes the parts of the face at the early stages of recognition. On the contrary, the fusiform face area shows no preference for single features, because the fusiform face area is responsible for \"holistic/configural\" information, meaning that it puts all of the processed pieces of the face together in later processing. This theory is supported by the work of Gold et al. who found that regardless of the orientation of a face, subjects were impacted by the configuration of the individual facial features. Subjects were also impacted by the coding of the relationships between those features. This shows that processing is done by a summation of the parts in the later stages of recognition.",
            "score": 186.38131713867188
        },
        {
            "docid": "7800961_12",
            "document": "Fusiform face area . The FFA is underdeveloped in children and does not fully develop until adolescence. This calls into question the evolutionary purpose of the FFA, as children show the ability to differentiate faces. Two-year-old babies have been shown to prefer the face of their mother. Although the FFA is underdeveloped in two-year-old babies, they have the ability to recognize their mother. Babies as early as three months old have shown the ability to distinguish between faces. During this time, babies exhibit the ability to differentiate between genders, showing a clear preference for female faces. It is theorized that, in terms of evolution, babies focus on women for food, although the preference could simply reflect a bias for the caregivers they experience. Infants do not appear to use this area for the perception of faces. Recent fMRI work has found no face selective area in the brain of infants 4 to 6 months old. However, given that the adult human brain has been studied far more extensively than the infant brain, and that infants are still undergoing major neurodevelopmental processes, it may simply be that the FFA is not located in anatomically familiar area. It may also be that activation for many different percepts and cognitive tasks in infants is diffuse in terms of neural circuitry, as infants are still undergoing periods of neurogenesis and neural pruning; this may make it more difficult to distinguish the signal, or what we would imagine as visual and complex familiar objects (like faces), from the noise, including static firing rates of neurons, and activity that is dedicated to a different task entirely than the activity of face processing. Infant vision involves only light and dark recognition, recognizing only major features of the face, activating the amygdala. These findings question the evolutionary purpose of the FFA.",
            "score": 183.5746612548828
        },
        {
            "docid": "486429_2",
            "document": "Prosopagnosia . Prosopagnosia, also called face blindness, is a cognitive disorder of face perception in which the ability to recognize familiar faces, including one's own face (self-recognition), is impaired, while other aspects of visual processing (e.g., object discrimination) and intellectual functioning (e.g., decisionmaking) remain intact. The term originally referred to a condition following acute brain damage (acquired prosopagnosia), but a congenital or developmental form of the disorder also exists, which may affect up to 2.5% of the United States population. The specific brain area usually associated with prosopagnosia is the fusiform gyrus, which activates specifically in response to faces. The functionality of the fusiform gyrus allows most people to recognize faces in more detail than they do similarly complex inanimate objects. For those with prosopagnosia, the new method for recognizing faces depends on the less-sensitive object recognition system. The right hemisphere fusiform gyrus is more often involved in familiar face recognition than the left. It remains unclear whether the fusiform gyrus is only specific for the recognition of human faces or if it is also involved in highly trained visual stimuli.",
            "score": 171.27862548828125
        },
        {
            "docid": "43777942_4",
            "document": "Face on Moon South Pole . Human brains have the ability to detect ambiguous images displayed upon the moon due to the brain\u2019s structure. On the left hemisphere of the human brain, the fusiform gyrus (an area linked to recognition), detects the accuracy of how \u201cfacelike\u201d an object is. The right fusiform gyrus then uses information from the left fusiform gyrus to conclude whether or not the image is a face. The gyrus's inherent ability to detect faces and patterns in organisms and nature has also led to a phenomenon called Pareidolia, in which the brain detects and recognises faces and patterns in collections of objects where there should be none.",
            "score": 171.19712829589844
        },
        {
            "docid": "25146378_12",
            "document": "Functional specialization (brain) . One of the most well known examples of functional specialization is the fusiform face area (FFA). Justine Sergent was one of the first researchers that brought forth evidence towards the functional neuroanatomy of face processing. Using positron emission tomography (PET), Sergent found that there were different patterns of activation in response to the two different required tasks, face processing verses object processing. These results can be linked with her studies of brain-damaged patients with lesions in the occipital and temporal lobes. Patients revealed that there was an impairment of face processing but no difficulty recognizing everyday objects, a disorder also known as prosopagnosia. Later research by Nancy Kanwisher using functional magnetic resonance imaging (fMRI), found specifically that the region of the inferior temporal cortex, known as the fusiform gyrus, was significantly more active when subjects viewed, recognized and categorized faces in comparison to other regions of the brain. Lesion studies also supported this finding where patients were able to recognize objects but unable to recognize faces. This provided evidence towards domain specificity in the visual system, as Kanwisher acknowledges the Fusiform Face Area as a module in the brain, specifically the extrastriate cortex, that is specialized for face perception.",
            "score": 170.95846557617188
        },
        {
            "docid": "33702464_5",
            "document": "Extrastriate body area . The experiment had subjects view images of different objects, including faces (as a control group), body parts, animals, parts of the face and intimate objects. While viewing the images, the subjects were scanned with an fMRI to see what area of the brain was activated. Through the trials a compilation of the fMRI\u2019s was made. From this compilation image a specific region was determined to have increased activity when shown visual stimuli of body parts and even more activity when viewing whole bodies. There have been no studies involving brain damage to the EBA. Thus far, only scans of brain activity, as well as transcranial magnetic stimulation, have been used to study the EBA. To find the specific functions of the EBA, Comimo Urgesi, Giovanni Berlucchi and Salvatore M. Aglioti used repetitive transcranial magnetic stimulation (rTMS) to disrupt part of the brain, making the brain less responsive in the target area. The study used event-related rTMS to disrupt the EBA, resulting in inactivation of cortical areas. This inactivation caused a slower response time in discriminating body parts. The study used facial features and motorcycle parts as non human parts for control groups. The facial features and motorcycle body parts did not display any change in response time. The neural activity data shows the EBA handles some of the visual processing of human body and parts but is not related to the processing of the face or other objects.",
            "score": 167.8081817626953
        },
        {
            "docid": "37940820_16",
            "document": "Emotion perception . The fusiform face area, part of the fusiform gyrus is an area some believe to specialize in the identification and processing of human faces, although others suspect it is responsible for distinguishing between well known objects such as cars and animals. Neuroimaging studies have found activation in this area in response to participants viewing images of prototypical faces, but not scrambled or inverted faces, suggesting that this region is specialized for processing human faces but not other material. This area has been an area of increasing debate and while some psychologists may approach the fusiform face area in a simplistic manner, in that it specializes in the processing of human faces, more likely this area is implicated in the visual processing of many objects, particularly those familiar and prevalent in the environment. Impairments in the ability to recognize subtle differences in faces would greatly inhibit emotion perception and processing and have significant implications involving social interactions and appropriate biological responses to emotional information.",
            "score": 160.99734497070312
        },
        {
            "docid": "37691351_4",
            "document": "Neuroscience and race . Neurotechnology enables studying the brain and racial interactions, though this study can be difficult because these interactions can be hard to replicate. Face recognition tests are the most commonly used method in studying racial interactions. These tests consist of observing own-race and other-race faces, and studying the brain's response to the faces. There are three major neurological techniques used to measure the brain's response to these simulated racial interactions. Functional magnetic resonance imaging (fMRI) measures the brain activity through measuring the blood oxygen level in the brain. This test gives insight into which regions of the brain are active during a certain event. Event-related potentials (ERPs) measure the brain's activity through measuring electrical impulses by electrodes on the head. This test gives insight in rapid changes in the brain. Transcranial magnetic stimulation (TMS) measures the response of a region of the brain once activated through magnetism. This test gives insight into causality of occurrences and gives specific insight in what the brain regions are doing. Brain-damaged patients have also been used to study racial interactions, by studying how racial interactions are affected when specific brain regions are damaged. These studies give insight into how different brain regions are involved in racial interactions once certain regions have been damaged. An implicit association test (IAC) is often used to measure the racial bias of people in studies by testing what objects, whether positive or negative, people associate with same-race or other-race faces.",
            "score": 159.44161987304688
        },
        {
            "docid": "485309_11",
            "document": "Face perception . Recognizing and perceiving faces are vital abilities needed to coexist in society. Faces can tell things such as identity, mood, age, sex, race, and the direction that someone is looking. Studies based on neuropsychology, behavior, electrophysiology, and neuro-imaging have supported the notion of a specialized mechanism for perceiving faces. Prosopagnosia patients demonstrate neuropsychological support for a specialized face perception mechanism as these people, due to brain damage, have deficits in facial perception, but their cognitive perception of objects remains intact. The face inversion effect provides behavioral support of a specialized mechanism as people tend to have greater deficits in task performance when prompted to react to an inverted face than to an inverted object. Electrophysiological support comes from the finding that the N170 and M170 responses tend to be face-specific. Neuro-imaging studies such as PET and fMRI studies have shown support for a specialized facial processing mechanism as they have identified regions of the fusiform gyrus that have higher activation during face perception tasks than other visual perception tasks. Theories about the processes involved in adult face perception have largely come from two sources: research on normal adult face perception and the study of impairments in face perception that are caused by brain injury or neurological illness. Novel optical illusions such as the Flashed Face Distortion Effect, in which scientific phenomenology outpaces neurological theory, also provide areas for research.",
            "score": 158.86557006835938
        },
        {
            "docid": "17115475_15",
            "document": "Anastasios Venetsanopoulos . For thousands of years, humans have used visually-perceived body characteristics such as face and gait to recognize one another. This remarkable ability of human visual system led Professor Venetsanopoulos to build automated systems to recognize individuals from digitally captured facial images and gait sequences. Face and gait recognition belong to the field of biometrics, a very active area of research in computer science, mainly motivated by government and security-related considerations. Face and gait are two typical physiological and behavioral biometrics. Venetsanopoulos contributed to both areas and his research has been extensively cited. There are two general approaches to the subject: the appearance-based approach and the model-based approach. Appearance-based face recognition processes a 2-D facial image as 2-D holistic patterns. The whole face region is the raw input to a recognition system and each face image is commonly represented by a high-dimensional vector consisting of the pixel intensity values in the image. Thus, face recognition is transformed to a multivariate, statistical pattern recognition problem. In a similar fashion to appearance-based face recognition, an appearance-based gait recognition approach considers gait as a holistic pattern and uses a full-body representation of a human subject as silhouettes or contours. Gait video sequences are naturally three-dimensional objects, formally named tensor objects, and they are very difficult to deal with using traditional vector-based learning algorithms. In order to deal with these tensor objects effectively, Venetsanopoulos and his research team developed a framework of multilinear subspace learning, so that computation and memory demands are reduced, natural structure and correlation in the original data are preserved, and more compact and useful features can be obtained. The Model-based gait recognition approach considers a human subject as an articulated object, represented by various body poses. Professor Venetsanopoulos proposed a full-body, layered deformable model (LDM) inspired by the manually labeled body-part-level silhouettes. The LDM has a layered structure to model self-occlusion between body parts and it is deformable, so simple limb deformation is taken into consideration. In addition, it also models shoulder swing. The LDM parameters can be recovered from automatically extracted silhouettes and then used for recognition.",
            "score": 153.59271240234375
        },
        {
            "docid": "485309_30",
            "document": "Face perception . Studies using electrophysiological techniques have demonstrated gender-related differences during a face recognition memory (FRM) task and a facial affect identification task (FAIT). The male subjects used a right, while the female subjects used a left, hemisphere neural activation system in the processing of faces and facial affect. Moreover, in facial perception there was no association to estimated intelligence, suggesting that face recognition performance in women is unrelated to several basic cognitive processes. Gender-related differences may suggest a role for sex hormones. In females there may be variability for psychological functions related to differences in hormonal levels during different phases of the menstrual cycle.",
            "score": 152.57470703125
        },
        {
            "docid": "49307010_7",
            "document": "Face superiority effect . Prosopagnosia is a \"selective impairment in the ability to recognize individual faces due to brain damage of the visual cortex.\" Essentially, this neurological deficit impairs an individuals ability to recognize faces, even faces of those who should be familiar, such as family members. This is the result of damage to the visual cortex. In terms of the holistic view, the inability to recognize faces stems from a failure to integrate the individual face parts into a whole. A study done by Busigny, Joubert, Felician, Ceccaldi, & Rossion (2010) looked at a prosopagnosia patient, GG, in reference to unimpaired control participants in matching/recognition tasks. Participants were either asked to study a whole face and select a part from the studied face presented in isolation, or study an isolated part and then select the same part when presented in a whole face. The researchers hypothesized that holistic interference would be demonstrated in the \"part-to-whole\" and \"whole-to-part\" conditions relative to the \"part-to-part\" and \"whole-to-whole\" conditions. These results were confirmed in the control participants, however, patient GG performed equally well in both conditions. The researchers suggest this is due to her recognition of face parts is unaffected by surrounding facial features in encoding or retrieving it from memory. Similar studies have also been conducted to show that prosopagnosia results from an individual's inability to form a holistic facial representation.",
            "score": 152.51690673828125
        },
        {
            "docid": "5212945_3",
            "document": "Visual neuroscience . A recent study using Event-Related Potentials (ERPs) linked an increased neural activity in the occipito-temporal region of the brain to the visual categorization of facial expressions. Results focus on a negative peak in the ERP that occurs 170 milliseconds after the stimulus onset. This action potential, called the N170, was measured using electrodes in the occipito-temporal region, an area already known to be changed by face stimuli. Studying by using the EEG, and ERP methods allow for an extremely high temporal resolution of 4 milliseconds, which makes these kinds of experiments extremely well suited for accurately estimating and comparing the time it takes the brain to perform a certain function. Scientists used classification image techniques, to determine what parts of complex visual stimuli (such as a face) will be relied on when patients are asked to assign them to a category, or emotion. They computed the important features when the stimulus face exhibited one of five different emotions. Stimulus faces exhibiting fear had the distinguishing feature of widening eyes, and stimuli exhibiting happiness exhibited a change in the mouth to make a smile. Regardless of the expression of the stimuli's face, the region near the eyes affected the EEG before the regions near the mouth. This revealed a sequential, and predetermined order to the perception and processing of faces, with the eye being the first, and the mouth, and nose being processed after. This process of downward integration only occurred when the inferior facial features were crucial to the categorization of the stimuli. This is best explained by comparing what happens when participants were shown a face exhibiting fear, versus happiness. The N170 peaked slightly earlier for the fear stimuli at about 175 milliseconds, meaning that it took a participants less time to recognize the facial expression. This is expected because only the eyes need to be processed to recognize the emotion. However, when processing a happy expression, where the mouth is crucial to categorization, downward integration must take place, and thus the N170 peak occurred later at around 185 milliseconds. Eventually visual neuroscience aims to completely explain how the visual system processes all changes in faces as well as objects. This will give a complete view to how the world is constantly visually perceived, and may provide insight into a link between perception and consciousness.",
            "score": 151.70217895507812
        },
        {
            "docid": "49307010_5",
            "document": "Face superiority effect . Evidence from neurophysiology studies with humans and monkeys also support face superiority. Neuroimaging and electrophysiological studies in humans shows the effects of holistic face recognition. In particular, when humans are shown normal upright faces, neuroimaging displays higher brain activity and response rates in the middle fusiform gyrus (MFG), and the inferior occipital gyrus (IOG) than when shown scrambled or inverted faces. Additionally, experiments computing event-related scalp potentials (ERPs) reveal higher brain responses 180ms after presenting the normal face, than in inverted/scrambled conditions.",
            "score": 149.6584930419922
        },
        {
            "docid": "5209682_8",
            "document": "Infant visual development . When comparing facial features across species, it was found that infants of six months were better at distinguishing facial information of both humans and monkeys than older infants and adults. They found that both nine-month-olds and adults could discriminate between pictures of human faces; however, neither infants nor adults had the same capabilities when it came to pictures of monkeys. On the other hand, six-month-old infants were able to discriminate both facial features on human faces and on monkey faces. This suggests that there is a narrowing in face processing, as a result of neural network changes in early cognition. Another explanation is that infants likely have no experience with monkey faces and relatively little experience with human faces. This may result in a more broadly tuned face recognition system and, in turn, an advantage in recognizing facial identity in general (i.e., regardless of species). In contrast, healthy adults due to their interaction with people on a frequent basis have fine tuned their sensitivity to facial information of humans \u2013 which has led to cortical specialization.",
            "score": 149.35379028320312
        },
        {
            "docid": "21312318_27",
            "document": "Recognition memory . Recognition memory is critically dependent on a hierarchically organized network of brain areas including the visual ventral stream, medial temporal lobe structures, frontal lobe and parietal cortices along with the hippocampus. As mentioned previously, the processes of recollection and familiarity are represented differently in the brain. As such, each of the regions listed above can be further subdivided according to which part is primarily involved in recollection or in familiarity. In the temporal cortex, for instance, the medial region is related to recollection whereas the anterior region is related to familiarity. Similarly, in the parietal cortex, the lateral region is related to recollection whereas the superior region is related to familiarity. An even more specific account divides the medial parietal region, relating the posterior cingulate to recollection and the precuneus to familiarity. The hippocampus plays a prominent role in recollection whereas familiarity depends heavily on the surrounding medial-temporal regions, especially the perirhinal cortex. Finally, it is not yet clear what specific regions of the prefrontal lobes are associated with recollection versus familiarity, although there is evidence that the left prefrontal cortex is correlated more strongly with recollection whereas the right prefrontal cortex is involved more in familiarity. Though left-side activation involved in recollection was originally hypothesized to result from semantic processing of words (many of these earlier studies used written words for stimuli) subsequent studies using nonverbal stimuli produced the same finding\u2014suggesting that prefrontal activation in the left hemisphere results from any kind of detailed remembering.  As previously mentioned, recognition memory is not a stand-alone concept; rather it is a highly interconnected and integrated sub-system of memory. Perhaps misleadingly, the regions of the brain listed above correspond to an abstract and highly generalized understanding of recognition memory, in which the stimuli or items-to-be-recognized are not specified. In reality, however, the location of brain activation involved in recognition is highly dependent on the nature of the stimulus itself. Consider the conceptual differences in recognizing written words compared to recognizing human faces. These are two qualitatively different tasks and as such it is not surprising that they involve additional, distinct regions of the brain. Recognizing words, for example, involves the visual word form area, a region in the left fusiform gyrus, which is believed to specialized in recognizing written words. Similarly, the fusiform face area, located in the right hemisphere, is linked specifically to the recognition of faces.",
            "score": 149.34503173828125
        },
        {
            "docid": "23836909_39",
            "document": "Hypostatic model of personality . Research using functional magnetic resonance imaging of the brain suggests that cognitive and affective-expressive forms of communication and self-reflection have distinct neural bases. Clinical findings have long suggested that verbalizations are often very incoherent when the individual is trying to put into words something deeply emotional. Identification of words naming emotions (happy, neutral, sad) was found to be faster than identification of corresponding facial expressions. Recognition of face expressions was more difficult to suppress in favor of the recognition of words than vice versa, the two conditions presenting different patterns of brain activation. These experimental results suggest that reading and recognition of face expressions are stimulus-dependent and perhaps hierarchical behaviors, hence recruiting distinct regions of the medial prefrontal cortex.",
            "score": 149.26759338378906
        },
        {
            "docid": "485309_36",
            "document": "Face perception . Studies by Gauthier have shown that an area of the brain known as the fusiform gyrus (sometimes called the fusiform face area because it is active during face recognition) is also active when study participants are asked to discriminate between different types of birds and cars, and even when participants become expert at distinguishing computer generated nonsense shapes known as greebles. This suggests that the fusiform gyrus may have a general role in the recognition of similar visual objects. Yaoda Xu, then a post doctoral fellow with Nancy Kanwisher, replicated the car and bird expertise study using an improved fMRI design that was less susceptible to attentional accounts.",
            "score": 149.00064086914062
        },
        {
            "docid": "485309_4",
            "document": "Face perception . From birth, infants possess rudimentary facial processing capacities and show heightened interest in faces. For example, newborns (1-3 days) have been shown to be able to recognize faces even when they are rotated up to 45 degrees. However, interest in faces is not continuously present in infancy and shows increases and decreases over time as the child grows older. Specifically, while newborns show a preference for faces, this behavior is reduced between one- to four months of age.Around three months of age, a preference for faces re-emerges and interest in faces seems to peak late during the first year but then declines again slowly over the next two years of life. Interestingly, the re-emergence of a preference for faces at three months of age may be influenced by the child's own motor abilities and experiences. Infants as young as two days of age are capable of mimicking the facial expressions of an adult, displaying their capacity to note details like mouth and eye shape as well as to move their own muscles in a way that produces similar patterns in their faces. However, despite this ability, newborns are not yet aware of the emotional content encoded within facial expressions. Five-month-olds, when presented with an image of a person making a fearful expression and a person making a happy expression, pay the same amount of attention to and exhibit similar event-related potentials (ERPs) for both. However, when seven-month-olds are given the same treatment, they focus more on the fearful face, and their event-related potential for the scared face shows a stronger initial negative central component than that for the happy face. This result indicates an increased attentional and cognitive focus toward fear that reflects the threat-salient nature of the emotion. In addition, infants' negative central components were not different for new faces that varied in the intensity of an emotional expression but portrayed the same emotion as a face they had been habituated to but were stronger for different-emotion faces, showing that seven-month-olds regarded happy and sad faces as distinct emotive categories. While seven-month-olds have been found to focus more on fearful faces, another study by Jessen, Altvater-Mackensen, and Grossmann found that \"happy expressions elicit enhanced sympathetic arousal in infants\" both when facial expressions were presented subliminally and when they were presented supraliminally, or in a way that the infants were consciously aware of the stimulus. These results show that conscious awareness of a stimulus is not connected to an infant's reaction to that stimulus.",
            "score": 148.8655242919922
        },
        {
            "docid": "29558261_14",
            "document": "Emotional lateralization . There may be a difference in cortical activation between men and women. Activity in the right hemisphere was greater in women when exposed to unpleasant images than men, though men showed more activation bilaterally while viewing pleasant pictures. Another study found that women but not men, with women had greater activation of their right hemisphere while viewing unpleasant faces and left hemisphere activation while viewing pleasant faces. Yet, another study found contrasting sex difference while recording EEG waves in the parietal and frontal lobes. Negative pictures activated the left hemisphere in women more than in men, and the right hemisphere in men more than in women when shown unpleasant images.",
            "score": 148.61036682128906
        },
        {
            "docid": "649382_5",
            "document": "Pareidolia . Pareidolia can cause people to interpret random images, or patterns of light and shadow, as faces. A 2009 magnetoencephalography study found that objects perceived as faces evoke an early (165 ms) activation of the fusiform face area at a time and location similar to that evoked by faces, whereas other common objects do not evoke such activation. This activation is similar to a slightly faster time (130 ms) that is seen for images of real faces. The authors suggest that face perception evoked by face-like objects is a relatively early process, and not a late cognitive reinterpretation phenomenon. A functional magnetic resonance imaging (fMRI) study in 2011 similarly showed that repeated presentation of novel visual shapes that were interpreted as meaningful led to decreased fMRI responses for real objects. These results indicate that the interpretation of ambiguous stimuli depends upon processes similar to those elicited by known objects.",
            "score": 147.3610382080078
        },
        {
            "docid": "33826251_11",
            "document": "Neural basis of self . Autism is a disorder in which those affected experience impaired social interactions, communication, and behaviors. A new approach to studying autism is to focus on individuals\u2019 perception of self rather than understanding the individual\u2019s social interactions. A common thought is that understanding of the differences between the self and others are impaired. However, the exact biological mechanism of self-understanding in autistic children is currently unknown. It has been found that there are significant differences in brain activation in self and other situations in autistic children when compared to children who do not have autism.  In adults who do not have autism, during self-recognition tasks, the inferior frontal gyrus and the inferior parietal lobule in the right hemisphere are activated. Children who do not have autism show activation in these areas when performing face processing tasks for their own faces and those of others. Children with autism, however, only show activation in these areas when recognizing their own faces. The activation in the inferior frontal gyrus is less in children with autism than in those who do not have autism.",
            "score": 146.9893798828125
        },
        {
            "docid": "968202_30",
            "document": "Dog intelligence . There is evidence that dogs can discriminate the emotional expressions of human faces. In addition, they seem to respond to faces in somewhat the same way as humans. For example, humans tend to gaze at the right side of a person's face, which may be related to the use of right brain hemisphere for facial recognition. Research indicates that dogs also fixate the right side of a human face, but not that of other dogs or other animals. Dogs are the only non-primate species known to do so.",
            "score": 146.6722869873047
        },
        {
            "docid": "21280496_27",
            "document": "Visual perception . There is considerable evidence that face and object recognition are accomplished by distinct systems. For example, prosopagnosic patients show deficits in face, but not object processing, while object agnosic patients (most notably, patient C.K.) show deficits in object processing with spared face processing. Behaviorally, it has been shown that faces, but not objects, are subject to inversion effects, leading to the claim that faces are \"special\". Further, face and object processing recruit distinct neural systems. Notably, some have argued that the apparent specialization of the human brain for face processing does not reflect true domain specificity, but rather a more general process of expert-level discrimination within a given class of stimulus, though this latter claim is the subject of substantial debate. Using fMRI and electrophysiology Doris Tsao and colleagues described brain regions and a mechanism for face recognition in macaque monkeys.",
            "score": 146.4087677001953
        },
        {
            "docid": "43777942_6",
            "document": "Face on Moon South Pole . The researchers found that on the right side of the brain, activity patterns stayed the same for every face except for non-face images, when brain patterns began to change dramatically regardless of whether or not the image resembled a face. On the left side of the brain, activity slowly changed as the images began to resemble a face. This led to the Sinha and the researchers determining that the left side of the brain decides how much an image resembles a face but does not assign them to any classification, while the right side of the brain is the portion that ultimately determines whether or not an image resembles a face.",
            "score": 146.2754669189453
        },
        {
            "docid": "37691351_6",
            "document": "Neuroscience and race . The first step the brain does to encode a memory is to process the face. The lateral fusiform gyrus is a facial recognition area of the brain. Within this brain region, the fusiform face area (FFA) analyzes the configuration and holistic appearance of the face. The FFA is more activated when viewing same-race faces compared to other-race faces. As time progresses from when the face is first viewed, the differences in FFA activation diminish. It\u2019s believed that the FFA is more activated when viewing a same-race face because the brain individuates (using more analytic power) the same-race faces while simply categorizing other-race faces. The FFA isn\u2019t the only region involved in facial recognition that effects the cross-race effect, but also the whole ventral temporal cortex (VT cortex). Scientists are able to distinguish which race face one is viewing at simply by viewing the VT cortex. Additionally, the fusiform cortex plays a vital role in categorizing race faces. This section is also more activated when viewing same-race faces, as it is studying the face in greater detail. However, these differences in activation of the fusiform complex diminish when a familiar other-race face is shown, like a celebrity.",
            "score": 145.6324920654297
        },
        {
            "docid": "24965027_26",
            "document": "Cognitive neuroscience of visual object recognition . Agnosia is a rare occurrence and can be the result of a stroke, dementia, head injury, brain infection, or hereditary. Apperceptive agnosia is a deficit in object perception creating an inability to understand the significance of objects. Similarly, associative visual agnosia is the inability to understand the significance of objects; however, this time the deficit is in semantic memory. Both of these agnosias can affect the pathway to object recognition, like Marr's Theory of Vision. More specifically unlike apperceptive agnosia, associative agnosic patients are more successful at drawing, copying, and matching tasks; however, these patients demonstrate that they can perceive but not recognize. Integrative agnosia(a subtype of associative agnosia) is the inability to integrate separate parts to form a whole image. With these types of agnosias there is damage to the ventral (what) stream of the visual processing pathway. Object orientation agnosia is the inability to extract the orientation of an object despite adequate object recognition. With this type of agnosia there is damage to the dorsal (where) stream of the visual processing pathway. This can affect object recognition in terms of familiarity and even more so in unfamiliar objects and viewpoints. A difficulty in recognizing faces can be explained by prosopagnosia. Someone with prosopagnosia cannot identify the face but is still able to perceive age, gender, and emotional expression. The brain region that specifies in facial recognition is the fusiform face area. Prosopagnosia can also be divided into apperceptive and associative subtypes. Recognition of individual chairs, cars, animals can also be impaired; therefore, these object share similar perceptual features with the face that are recognized in the fusiform face area.",
            "score": 144.71217346191406
        },
        {
            "docid": "21312310_48",
            "document": "Remember versus know judgements . The remember-know paradigm with epilepsy patients to distinguish whether a stimulus (picture of a face) was familiar. Patients that were found to have right temporal lobe epilepsy showed relatively lower face recognition response than those with left temporal lobe epilepsy due to damage of secondary sensory regions (including fusiform gyrus) in the brain's right hemisphere, which is responsible for perception and encoding (esp. face memory).",
            "score": 144.36904907226562
        },
        {
            "docid": "22391885_6",
            "document": "Neural processing for individual categories of objects . It may be that the use of distinct brain regions for processing different object categories results from different processing requirements necessary for each class. Indeed, Malach et al. (2002) detail findings that buildings and faces require processing at different resolutions in order to be recognised - face recognition requires the analysis of fine detail, while buildings can be recognised using larger scale feature integration. As a result, faces are associated with central visual field processing while buildings are processed more peripherally. Malach et al. (2002) report that points on the retina sharing foveal centricity are mapped onto parallel cortical bands and it therefore follows that object classes that are processed differently by retinal cells should be represented distinctly within the brain. Consistently, faces and buildings were found to be processed independently of each other and in discrete cortical regions suggesting that processing is facilitated by assigning object categories to distinct cortical regions according to the level and type of processing that they require.",
            "score": 141.9989013671875
        },
        {
            "docid": "4236583_24",
            "document": "Visual search . Over the past few decades there have been vast amounts of research into face recognition, specifying that faces endure specialized processing within a region called the fusiform face area (FFA) located in the mid fusiform gyrus in the temporal lobe. Debates are ongoing whether both faces and objects are detected and processed in different systems and whether both have category specific regions for recognition and identification. Much research to date focuses on the accuracy of the detection and the time taken to detect the face in a complex visual search array. When faces are displayed in isolation, upright faces are processed faster and more accurately than inverted faces, but this effect was observed in non-face objects as well. When faces are to be detected among inverted or jumbled faces, reaction times for intact and upright faces increase as the number of distractors within the array is increased. Hence, it is argued that the 'pop out' theory defined in feature search is not applicable in the recognition of faces in such visual search paradigm. Conversely, the opposite effect has been argued and within a natural environmental scene, the 'pop out' effect of the face is significantly shown. This could be due to evolutionary developments as the need to be able to identify faces that appear threatening to the individual or group is deemed critical in the survival of the fittest. More recently, it was found that faces can be efficiently detected in a visual search paradigm, if the distracters are non-face objects, however it is debated whether this apparent 'pop out' effect is driven by a high-level mechanism or by low-level confounding features. Furthermore, patients with developmental prosopagnosia, suffering from imparied face identification, generally detect faces normally, suggesting that visual search for faces is facilitated by mechanisms other than the face-identification circuits of the fusiform face area.",
            "score": 141.8575897216797
        },
        {
            "docid": "2970322_9",
            "document": "Visual agnosia . More specifically, the lateral occipital complex appears to respond to many different types of objects. Prosopagnosia (inability to recognize faces) is due to damage of the fusiform face area (FFA). An area in the fusiform gyrus of the temporal lobe that has been strongly associated with a role in facial recognition. However, this area is not exclusive to faces; recognition of other objects of expertise are also processed in this area. The extrastriate body cortex (EBA) was found to be activated by photographs, silhouettes, or stick drawings of human bodies. The parahippocampal place area (PPA) of the limbic cortex has been found to be activated by the sight of scenes and backgrounds. Cerebral achromatopsia (the inability to discriminate between different hues) is caused by damage to the V8 area of the visual association cortex. The left hemisphere seems to play a critical role in recognizing the meaning of common objects.",
            "score": 140.47499084472656
        },
        {
            "docid": "485309_27",
            "document": "Face perception . Just as memory and cognitive function separate the abilities of children and adults to recognize faces, the familiarity of a face may also play a role in the perception of faces. Zheng, Mondloch, and Segalowitz recorded event-related potentials in the brain to determine the timing of recognition of faces in the brain. The results of the study showed that familiar faces are indicated and recognized by a stronger N250, a specific wavelength response that plays a role in the visual memory of faces. Similarly, Moulson et al. found that all faces elicit the N170 response in the brain.",
            "score": 140.35545349121094
        }
    ]
}