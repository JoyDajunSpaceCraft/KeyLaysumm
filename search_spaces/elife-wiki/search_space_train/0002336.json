{
    "q": [
        {
            "docid": "32036278_2",
            "document": "Dichotic listening . Dichotic Listening is a psychological test commonly used to investigate selective attention within the auditory system and is a subtopic of cognitive psychology and neuroscience. Specifically, it is \"used as a behavioral test for hemispheric lateralization of speech sound perception.\" During a standard dichotic listening test, a participant is presented with two different auditory stimuli simultaneously (usually speech). The different stimuli are directed into different ears over headphones. Participants are asked to pay attention to one or both of the stimuli. Later, they are asked about the content of either the message they were asked to attend to or the message that they were not told to listen to.",
            "score": 136.72749209403992
        },
        {
            "docid": "5366050_50",
            "document": "Speech perception . Neurophysiological methods rely on utilizing information stemming from more direct and not necessarily conscious (pre-attentative) processes. Subjects are presented with speech stimuli in different types of tasks and the responses of the brain are measured. The brain itself can be more sensitive than it appears to be through behavioral responses. For example, the subject may not show sensitivity to the difference between two speech sounds in a discrimination test, but brain responses may reveal sensitivity to these differences. Methods used to measure neural responses to speech include event-related potentials, magnetoencephalography, and near infrared spectroscopy. One important response used with event-related potentials is the mismatch negativity, which occurs when speech stimuli are acoustically different from a stimulus that the subject heard previously.",
            "score": 209.51825201511383
        },
        {
            "docid": "22434851_5",
            "document": "Speech shadowing . The speech shadowing technique is used in dichotic listening tests. The first one to apply this technique was E. Colin Cherry in 1953. During dichotic listening tests, subjects are presented with two different messages, one in their right ear and one in their left. The participants are often asked to focus on only one of the different messages and this is where the speech shadowing technique is used. Participants are instructed to shadow the attended message by repeating it out loud with a delay of a few seconds between hearing a word and repeating the word. The speech shadowing technique is significant for these experiments because it ensures that the subjects are attending to the desired message. Various other stimuli are then presented to the other ear, and subjects are afterwards queried on what they can recall from the other message.",
            "score": 131.84767699241638
        },
        {
            "docid": "752723_2",
            "document": "Dichotic listening test . The Dichotic listening test is a psychological test commonly used to investigate selective attention within the auditory system and is a subtopic of cognitive psychology and neuroscience. Specifically, it is \"used as a behavioral test for hemispheric lateralization of speech sound perception.\" During a standard dichotic listening test, a participant is presented with two different auditory stimuli simultaneously (usually speech). The different stimuli are directed into different ears over headphones. Research Participants were instructed to repeat aloud the words they heard in one ear while a different message was presented to the other ear. As a result of focusing to repeat the words, participants noticed little of the message to the other ear, often not even realizing that at some point it changed from English to German. At the same time, participants did notice when the voice in the unattended ear changed from a male\u2019s to a female\u2019s, suggesting that the selectivity of consciousness can work to tune in some information.\"",
            "score": 161.356769323349
        },
        {
            "docid": "5366050_42",
            "document": "Speech perception . Research into the relationship between music and cognition is an emerging field related to the study of speech perception. Originally it was theorized that the neural signals for music were processed in a specialized \"module\" in the right hemisphere of the brain. Conversely, the neural signals for language were to be processed by a similar \"module\" in the left hemisphere. However, utilizing technologies such as fMRI machines, research has shown that two regions of the brain traditionally considered exclusively to process speech, Broca's and Wernicke's areas, also become active during musical activities such as listening to a sequence of musical chords. Other studies, such as one performed by Marques et al. in 2006 showed that 8-year-olds who were given six months of musical training showed an increase in both their pitch detection performance and their electrophysiological measures when made to listen to an unknown foreign language.",
            "score": 180.5974841117859
        },
        {
            "docid": "179092_31",
            "document": "Neurolinguistics . Some experiments give subjects a \"distractor\" task to ensure that subjects are not consciously paying attention to the experimental stimuli; this may be done to test whether a certain computation in the brain is carried out automatically, regardless of whether the subject devotes attentional resources to it. For example, one study had subjects listen to non-linguistic tones (long beeps and buzzes) in one ear and speech in the other ear, and instructed subjects to press a button when they perceived a change in the tone; this supposedly caused subjects not to pay explicit attention to grammatical violations in the speech stimuli. The subjects showed a mismatch response (MMN) anyway, suggesting that the processing of the grammatical errors was happening automatically, regardless of attention\u2014or at least that subjects were unable to consciously separate their attention from the speech stimuli.",
            "score": 167.9246311187744
        },
        {
            "docid": "433584_4",
            "document": "McGurk effect . Vision is the primary sense for humans, but speech perception is multimodal, which means that it involves information from more than one sensory modality, in particular, audition and vision. The McGurk effect arises during phonetic processing because the integration of audio and visual information happens early in speech perception. The McGurk effect is very robust; that is, knowledge about it seems to have little effect on one's perception of it. This is different from certain optical illusions, which break down once one 'sees through' them. Some people, including those that have been researching the phenomenon for more than twenty years, experience the effect even when they are aware that it is taking place. With the exception of people who can identify most of what is being said from speech-reading alone, most people are quite limited in their ability to identify speech from visual-only signals. A more extensive phenomenon is the ability of visual speech to increase the intelligibility of heard speech in a noisy environment. Visible speech can also alter the perception of perfectly audible speech sounds when the visual speech stimuli are mismatched with the auditory speech. Normally, speech perception is thought to be an auditory process; however, our use of information is immediate, automatic, and, to a large degree, unconscious and therefore, despite what is widely accepted as true, speech is not only something we hear. Speech is perceived by all of the senses working together (seeing, touching, and listening to a face move). The brain is often unaware of the separate sensory contributions of what it perceives. Therefore, when it comes to recognizing speech the brain cannot differentiate whether it is seeing or hearing the incoming information.",
            "score": 209.92191457748413
        },
        {
            "docid": "35988494_4",
            "document": "Selective auditory attention . Recently, researchers have attempted to explain mechanisms implicated in selective auditory attention. In 2012, an assistant professor in residence of the Neurological Surgery and Physiology in the University of California San Francisco examined the selective cortical representation of attended speaker in multiple-talker speech perception. Edward Chang and his colleague, Nima Mesgarani undertook a study that recruited three patients affected by severe epilepsy, who were undergoing treatment surgery. All patients were recorded to have normal hearing. The procedure of this study required the surgeons to place a thin sheet of electrodes under the skull on the outer surface of the cortex. The activity of electrodes was recorded in the auditory cortex. The patients were given two speech samples to listen to and they were told to distinguish the words spoken by the speakers. The speech samples were simultaneously played and different speech phrases were spoken by different speakers. Chang and Mesgarani found an increase in neural responses in the auditory cortex when the patients heard words from the target speaker. Chang went on to explain that the method of this experiment was well-conducted as it was able to observe the neural patterns that tells when the patient's auditory attention shifted to the other speaker. This clearly shows the selectivity of auditory attention in humans.",
            "score": 137.13503921031952
        },
        {
            "docid": "32036278_10",
            "document": "Dichotic listening . In selective attention experiments, the participants may be asked to repeat aloud the content of the message they are listening to. This task is known as shadowing. As Colin Cherry (1953) found, people do not recall the shadowed message well, suggesting that most of the processing necessary to shadow the attended to message occurs in working memory and is not preserved in the long-term store. Performance on the unattended message is worse. Participants are generally able to report almost nothing about the content of the unattended message. In fact, a change from English to German in the unattended channel frequently goes unnoticed. However, participants are able to report that the unattended message is speech rather than non-verbal content. In addition to this, if the content of the unattended message contains certain information, such as the listener's name, then the unattended message is more likely to be noticed and remembered. A demonstration of this was done by Conway, Cowen, and Bunting (2001) in which they had subjects shadow words in one ear while ignoring words in the other ear. At some point, the subject's name was spoken in the ignored ear, and the question was whether the subject would report hearing their name. Subjects with a high working memory (WM) span were more capable of blocking out the distracting information. Also if the message contains sexual words then people usually notice them immediately. This suggests that the unattended information is also undergoing analysis and keywords can divert attention to it.",
            "score": 154.55447256565094
        },
        {
            "docid": "38523090_3",
            "document": "Statistical learning in language acquisition . The earliest evidence for these statistics learning abilities comes from a study by Jenny Saffran, Richard Aslin, and Elissa Newport, in which 8-month-old infants were presented with nonsense streams of monotone speech. Each stream was composed of four three-syllable \u201cpseudowords\u201d that were repeated randomly. After exposure to the speech streams for two minutes, infants reacted differently to hearing \u201cpseudowords\u201d as opposed to \u201cnonwords\u201d from the speech stream, where nonwords were composed of the same syllables that the infants had been exposed to, but in a different order. This suggests that infants are able to learn statistical relationships between syllables even with very limited exposure to a language. That is, infants learn which syllables are always paired together and which ones only occur together relatively rarely, suggesting that they are parts of two different units. This method of learning is thought to be one way that children learn which groups of syllables form individual words.",
            "score": 101.47516751289368
        },
        {
            "docid": "20320137_5",
            "document": "Cohort model . The cohort model consists of three stages: access, selection, and integration. Under this model, auditory lexical retrieval begins with the first one or two speech segments, or phonemes, reach the hearer's ear, at which time the mental lexicon activates every possible word that begins with that speech segment. This occurs during the \"access stage\" and all of the possible words are known as the cohort. The words that are activated by the speech signal but are not the intended word are often called \"competitors.\" Identification of the target word is more difficult with more competitors. As more speech segments enter the ear and stimulate more neurons, causing the competitors that no longer match the input to be \"kicked out\" or to decrease in activation. The processes by which words are activated and competitors rejected in the cohort model are frequently called \"activation and selection\" or \"recognition and competition.\" These processes continue until an instant, called the \"recognition point\", at which only one word remains activated and all competitors have been kicked out. The recognition point process is initiated within the first 200 to 250 milliseconds of the onset of the given word. This is also known as the uniqueness point and it is the point where the most processing occurs. Moreover, there is a difference in the way a word is processed before it reaches its recognition point and afterwards. One can look at the process prior to reaching the recognition point as bottom-up, where the phonemes are used to access the lexicon. The post recognition point process is top-down, because the information concerning the chosen word is tested against the word that is presented. The selection stage occurs when only one word is left from the set. Finally, in the integration stage, the semantic and syntactic properties of activated words are incorporated into the high-level utterance representation.",
            "score": 144.72124314308167
        },
        {
            "docid": "9736296_43",
            "document": "Linguistic performance . Unacceptable Sentences are ones which, although are grammatical, are not considered proper utterances. They are considered unacceptable due to the lack of our cognitive systems to process them. Speakers and listeners can be aided in the performance and processing of these sentences by eliminating time and memory constraints, increasing motivation to process these utterances and using pen and paper. In English there are three types of sentences that are grammatical but are considered unacceptable by speakers and listeners. When a speaker makes an utterance they must translate their ideas into words, then syntactically proper phrases with proper pronunciation. The speaker must have prior world knowledge and an understanding of the grammatical rules that their language enforces. When learning a second language or with children acquiring their first language, speakers usually have this knowledge before they are able to produce them. Their speech is usually slow and deliberate, using phrases they have already mastered, and with practice their skills increase. Errors of linguistic performance are judged by the listener giving many interpretations if an utterance is well-formed or ungrammatical depending on the individual. As well the context in which an utterance is used can determine if the error would be considered or not. When comparing \"Who must telephone her?\" and \"Who need telephone her?\" the former would be considered the ungrammatical phrase. However, when comparing it to \"Who want telephone her?\" it would be considered the grammatical phrase. The listener may also be the speaker. When repeating sentences with errors if the error is not comprehended then it is performed. As well if the speaker does notice the error in the sentence they are supposed to repeat they are unaware of the difference between their well-formed sentence and the ungrammatical sentence. An unacceptable utterance can also be performed due to a brain injury. Three types of brain injuries that could cause errors in performance were studied by Fromkin are dysarthria, apraxia and literal paraphasia. Dysarthria is a defect in the neuromuscular connection that involves speech movement. The speech organs involved can be paralyzed or weakened, making it difficult or impossible for the speaker to produce a target utterance. Apraxia is when there is damage to the ability to initiate speech sounds with no paralysis or weakening of the articulators. Literal paraphasia causes disorganization of linguistic properties, resulting in errors of word order of phonemes. Having a brain injury and being unable to perform proper linguistic utterances, some individuals are still able to process complex sentences and formulate syntactically well formed sentences in their mind. Child productions when they are acquiring language are full of errors of linguistic performance. Children must go from imitating adult speech to create new phrases of their own. They will need to use their cognitive operations of the knowledge of their language they are learning to determine the rules and properties of that language. The following are examples of errors in English speaking children's productions.",
            "score": 185.46728241443634
        },
        {
            "docid": "10042066_2",
            "document": "Developmental linguistics . Developmental linguistics is the study of the development of linguistic ability in an individual, particularly the acquisition of language in childhood. It involves research into the different stages in language acquisition, language retention, and language loss in both first and second languages, in addition to the area of bilingualism. Before infants can speak, the neural circuits in their brains are constantly being influenced by exposure to language. The neurobiology of language contains a \"critical period\" in which children are most sensitive to language. The different aspects of language have varying \"critical periods\". Studies show that the critical period for phonetics is toward the end of the first year. At 18 months, a toddler's vocabulary vastly expands. The critical period for syntactic learning is 18-36 months. Infants of different mother languages can be differentiated at the age of 10 months. At 20 weeks they begin vocal imitation. Beginning when babies are about 12 months, they take on computational learning and social learning. Social interactions for infants and toddlers is important because it helps associate \"perception and action\". In-person social interaction rather than audio or video better facilitates learning in babies because they learn from how other people respond to them, especially their mothers. Babies have to learn to mimic certain syllables, which takes practice in manipulating tongue and lip movement. Sensory-motor learning in speech is linked to exposure to speech, which is very sensitive to language. Infants exposed to Spanish exhibit a different vocalization than infants exposed to English. One study took infants that were learning English and made them listen to Spanish in 12 sessions. The result showed consequent alterations in their vocalization, which demonstrated Spanish prosody.  One study used MEG to record activation in the brains of newborns, 6 months olds and 12 months olds while presenting them with syllables, harmonics and non-speech sounds. For the 6 month and 12 month old, the auditory and motor areas responded to speech. The newborn showed auditory activation but not motor activation. Another study presented 3 month olds with sentences and recorded their brain activity via fMRI motor speech areas did activate. These studies suggest that the link between perception and action begins to develop at 3 months. When babies are young, they are actually the most sensitive to distinguishing all phonetic units. During an infant\u2019s 1st year of life, they have to differentiate between about 40 phonetic units. When they are older they have usually been exposed to their native language so much that they lose this ability and can only distinguish phonetic units in their native language. Even at 12 months babies exhibit a deficit in differentiated non-native sounds. However, their ability to distinguish sounds in their native language continues to improve and become more fine-tuned. For example, Japanese learning infants learn that there is no differentiation between /r/ and /l/. However, in English, \"rake\" and \"lake\" are two different words. Japanese babies eventually lose their ability to distinguish between /r/ and /l/. Similarly, a Spanish learning infant cannot form words until they learn the difference between works like \"bano\" and \"pano\", because the /p/ sound is different than the /b/ sound. English learning babies do not learn to differentiate between the two.",
            "score": 164.89982914924622
        },
        {
            "docid": "33106906_46",
            "document": "Eyewitness memory . An accent also has the potential to interfere with the witness's ability to recognize the perpetrator's appearance. It has been found that when witnesses are asked to recall a perpetrator, the perpetrator's physical appearance is remembered less well when they have an accent compared to when they do not. This appears the case with different accents, speech content and how long a listener is exposed to the speaker. One proposed explanation for why accents can negatively affect the recall of visual information and eyewitness memory draws from Wickens' (2002; 2008) multiple resource theory. Wickens' theory suggests that attentional resources are separated into distinct 'pools'. Only visual and auditory tasks have access to visual and auditory attentional resources, respectively. However, when a task arises which requires the use of attentional resources from both modalities, this leads to competition for resources, in turn leading the inability to accomplish one or both tasks or resulting in poorer performance. Therefore, fewer general resources may have been available in order to encode and remember the perpetrator's appearance after witnesses had used attentional resources for the processing of the accented voice and speech content.",
            "score": 149.02187848091125
        },
        {
            "docid": "32036278_8",
            "document": "Dichotic listening . Dichotic listening tests can also be used as lateralized speech assessment task. Neuropsychologists have used this test to explore the role of singular neuroanatomical structures in speech perception and language asymmetry. For example, Hugdahl \"et al.\" (2003), investigated dichotic listening performance and frontal lobe function in left and right lesioned frontal lobe nonaphasiac patients compared to healthy controls. In the study, all groups were exposed to 36 dichotic trials with pairs of CV syllables and each patient was asked to state which syllable he or she heard best. As expected, the right lesioned patients showed a right ear advantage like the healthy control group but the left hemisphere lesioned patients displayed impairment when compared to both the right lesioned patients and control group. From this study, researchers concluded \"dichotic listening as into a neuronal circuitry which also involves the frontal lobes, and that this may be a critical aspect of speech perception.\" Similarly, Westerhausen and Hugdahl (2008) analyzed the role of the corpus callosum in dichotic listening and speech perception. After reviewing many studies, it was concluded that \"...dichotic listening should be considered a test of functional inter-hemispheric interaction and connectivity, besides being a test of lateralized temporal lobe language function\" and \"the corpus callosum is critically involved in the top-down attentional control of dichotic listening performance, thus having a critical role in auditory laterality.\"",
            "score": 116.92365837097168
        },
        {
            "docid": "752723_9",
            "document": "Dichotic listening test . Dichotic listening tests can also be used as lateralized speech assessment task. Neuropsychologists have used this test to explore the role of singular neuroanatomical structures in speech perception and language asymmetry. For example, Hugdahl \"et al.\" (2003), investigated dichotic listening performance and frontal lobe function in left and right lesioned frontal lobe nonaphasiac patients compared to healthy controls. In the study, all groups were exposed to 36 dichotic trials with pairs of CV syllables and each patient was asked to state which syllable he or she heard best. As expected, the right lesioned patients showed a right ear advantage like the healthy control group but the left hemisphere lesioned patients displayed impairment when compared to both the right lesioned patients and control group. From this study, researchers concluded \"dichotic listening as into a neuronal circuitry which also involves the frontal lobes, and that this may be a critical aspect of speech perception.\" Similarly, Westerhausen and Hugdahl (2008) analyzed the role of the corpus callosum in dichotic listening and speech perception. After reviewing many studies, it was concluded that \"...dichotic listening should be considered a test of functional inter-hemispheric interaction and connectivity, besides being a test of lateralized temporal lobe language function\" and \"the corpus callosum is critically involved in the top-down attentional control of dichotic listening performance, thus having a critical role in auditory laterality.\"",
            "score": 116.92365837097168
        },
        {
            "docid": "17523336_22",
            "document": "Olivocochlear system . Although Scharf et al.\u2019s (1993, 1994, 1997) experiments failed to produce any clear differences in the basic psychophysical characteristics of hearing (other than the detection of unexpected sounds), many other studies using both animals and humans have implicated the OCB in listening-in-noise tasks using more complex stimuli. In constant BGN, rhesus monkeys with intact OCBs have been observed to perform better in vowel discrimination tasks than those without (Dewson, 1968). In cats, an intact OCB is associated with better vowel identification (Heinz et al., 1998), sound localisation (May et al., 2004), and intensity discrimination (May and McQuone, 1995). All of these studies were performed in constant BGN. In humans, speech-in-noise discrimination measurements have been performed on individuals who had undergone unilateral vestibular neurectomy (resulting in OCB sectioning). Giraud et al. (1997) observed a small advantage in the healthy ear over the operated ear for phoneme recognition and speech intelligibility in BGN. Scharf et al. (1988) had previously investigated the role of auditory attention during speech perception, and suggested that speech-in-noise discrimination is assisted by attentional focus on frequency regions. In 2000, Zeng et al., reported that vestibular neurectomy did not directly affect pure-tone thresholds or intensity discrimination, confirming earlier findings of Scharf et al. 1994; 1997. For the listening-in-noise tasks, they observed a number of discrepancies between the healthy and operated ear. Consistent with the earlier findings of May and McQuone (1995), intensity discrimination in noise was observed to be slightly worse in the ear without olivocochlear bundle (OCB) input. However, Zeng et al.\u2019s main finding related to the \u201covershoot\u201d effect, which was found to be significantly reduced (~50%) in the operated ears. This effect was first observed by Zwicker (1965), and was characterised as an increased detection threshold of a tone when it is presented at the onset of the noise compared to when it is presented in constant, steady-state noise. Zeng et al. proposed that this finding is consistent with MOCS-evoked antimasking; that is, MOCS-evoked antimasking being absent at the onset of noise however becoming active during steady-state noise. This theory was supported by the time course of MOC activation; being similar to the time course of the overshoot effect (Zwicker, 1965), as well as the overshoot effect being disrupted in subjects with sensorineural hearing loss, for whom the MOCS would be most likely ineffectual (Bacon and Takahashi, 1992).",
            "score": 129.07939040660858
        },
        {
            "docid": "18614_35",
            "document": "Language acquisition . Prosody is the property of speech that conveys an emotional state of the utterance, as well as intended form of speech (whether it be a question, statement or command). Some researchers in the field of developmental neuroscience would argue that fetal auditory learning mechanisms are solely due to discrimination in prosodic elements. Although this would hold merit in an evolutionary psychology perspective (i.e. recognition of mother's voice/familiar group language from emotionally valent stimuli), some theorists argue that there is more than prosodic recognition in elements of fetal learning. Newer evidence shows that fetuses not only react to the native language differently from nonnative, but furthermore that fetuses react differently and can accurately discriminate between native and nonnative vowels (Moon, Lagercrantz, & Kuhl, 2013). Furthermore, a new study in 2016 showed that newborn infants encode the edges of multisyllabic sequences better than the internal components of the sequence (Ferry et al., 2016). Together, these results suggest that newborn infants have learned important properties of syntactic processing in utero, that can be seen in infant knowledge of native language vowels and the sequencing of heard multisyllabic phrases. This ability to sequence specific vowels gives newborn infants some of the fundamental mechanisms needed in order to learn the complex organization of a language.  From a neuroscientific perspective, there are neural correlates have been found that demonstrate human fetal learning of speech-like auditory stimulus that most other studies have been analyzing (Partanen et al., 2013). In a study conducted by Partanen et al. (2013), researchers presented fetuses with certain word variants and saw that these fetuses exhibited higher brain activity to the certain word variants compared to controls. In this same study, there was \"a significant correlation existed between the amount of prenatal exposure and brain activity, with greater activity being associated with a higher amount of prenatal speech exposure,\" pointing to the important learning mechanisms present before birth that is fine-tuned to features in speech (Partanen et al., 2013).",
            "score": 157.36295318603516
        },
        {
            "docid": "8146465_5",
            "document": "Alvin Liberman . Liberman was one of the first to conduct research and experimental studies in the field of speech development and linguistics. Through his research he aimed to gain a thorough understanding of the importance and purpose of speech in the act of reading and the process of learning to read. Some of his profound investigations were made during his time at Haskins Laboratories where he worked as a research scientist trying to investigate the relationships between speech and acoustics. From his research he came up with the idea that we hear spoken words much differently than sounds. It was evident to Liberman that speech, the speed at which someone says something in particular, is connected to the word's amount of syllables, or in other terms its \"acoustic complexity\" (Whalen, 2000).  The difference in the difficulty of speech and reading exists even as alphabetic writing systems provide discrete and invariant signals and nods to vowels, sounds and consonants. When it comes to speech and conversation, it \"does not come to us as a series of individual words; we must extract the words from a stream of speech.\" Liberman and his colleagues were training the blind to read using a reading machine that would replace each letter of the alphabet with a specific sound. However, he and his colleagues found that the replacement of the sounds for each distinct letter of the alphabet did not help with the blind to learn to read or pronounce the letters fluently. After long investigations of why this was, Liberman established that speech was not as simple as an acoustic alphabet. Therefore, speech signals are very distinct from acoustic alphabet (Fowler, 2001). These investigations showed that speech perception is different from perception of other acoustic signals, and convinced Liberman that speech perception is the result of the human biological adaptations to language. Human listeners are able to decode the repetitive variable signal of running speech and to translate it into phonemic components. This is also known as the \"motor theory of speech perception\". Liberman ascribed this to the human biological disposition towards speech as opposed to reading which is not ingrained genetically.",
            "score": 162.78867733478546
        },
        {
            "docid": "540571_8",
            "document": "Wernicke's area . Support for a broad range of speech processing areas was furthered by a recent study done at University of Rochester in which American Sign Language native speakers were subject to MRIs while interpreting sentences that identified a relationship using either syntax (relationship is determined by the word order) or inflection (relationship is determined by physical motion of \"moving hands through space or signing on one side of the body\"). Distinct areas of the brain were activated with the frontal cortex (associated with ability to put information into sequences) being more active in the syntax condition and the temporal lobes (associated with dividing information into its constituent parts) being more active in the inflection condition. However, these areas are not mutually exclusive and show a large amount of overlap. These findings imply that while speech processing is a very complex process, the brain may be using fairly basic, preexisting computational methods.",
            "score": 167.08150470256805
        },
        {
            "docid": "14999344_12",
            "document": "Nominal group (functional grammar) . The experiential pattern in nominal groups opens with the identification of the head in terms of the immediate context of the speech event\u2014the here-and-now\u2014what Halliday calls \"the speaker\u2013now matrix\". Take, for example, the first word of the nominal group exemplified above: \"those\": \"\"those\" apples\", as opposed to \"\"these\" apples\", means \"you know the apples I mean\u2014the ones over there, not close to me\"; distance or proximity to the immediate speech event could also be in temporal terms (the ones we picked last week, not today), or in terms of the surrounding text (the apples mentioned in the previous paragraph in another context, not in the previous sentence in the same context as now) and the assumed background knowledge of the listener/speaker (\"\"the\" apple\" as opposed to \"\"an\" apple\" means \"the one you know about\"). The same function is true of other deictics, such as \"my\", \"all\", \"each\", \"no\", \"some\", and \"either\": they establish the relevance of the head\u2014they \"fix\" it, as it were\u2014in terms of the speech event.",
            "score": 104.80873835086823
        },
        {
            "docid": "38523090_9",
            "document": "Statistical learning in language acquisition . To look more closely at this issue, Saffran Aslin and Newport conducted another study in which infants underwent the same training with the artificial grammar but then were presented with either words or part-words rather than words or nonwords. The part-words were syllable sequences composed of the last syllable from one word and the first two syllables from another (such as \"kupado\"). Because the part-words had been heard during the time when children were listening to the artificial grammar, preferential listening to these part-words would indicate that children were learning not only serial-order information, but also the statistical likelihood of hearing particular syllable sequences. Again, infants showed greater listening times to the novel (part-) words, indicating that 8-month-old infants were able to extract these statistical regularities from a continuous speech stream.",
            "score": 105.81279289722443
        },
        {
            "docid": "23060403_12",
            "document": "Motor theory of speech perception . Using a speech synthesizer, speech sounds can be varied in place of articulation along a continuum from to to , or in voice onset time on a continuum from to (for example). When listeners are asked to discriminate between two different sounds, they perceive sounds as belonging to discrete categories, even though the sounds vary continuously. In other words, 10 sounds (with the sound on one extreme being and the sound on the other extreme being , and the ones in the middle varying on a scale) may all be acoustically different from one another, but the listener will hear all of them as either or . Likewise, the English consonant may vary in its acoustic details across different phonetic contexts (the /d/ in does not technically sound the same as the one in , for example), but all 's as perceived by a listener fall within one category (voiced alveolar plosive) and that is because \"linguistic representations are abstract, canonical, phonetic segments or the gestures that underlie these segments.\" This suggests that humans identify speech using categorical perception, and thus that a specialized module, such as that proposed by the motor theory of speech perception, may be on the right track.",
            "score": 131.9732789993286
        },
        {
            "docid": "3237319_9",
            "document": "Auditory verbal agnosia . Auditory verbal agnosia is the inability to distinguish phonemes. In some patients with unilateral auditory verbal agnosia, there is evidence that the ability to acoustically process speech signals is affected at the prephonemic level, preventing the conversion of these signals into phonemes. There are two predominate hypotheses that address what happens within the language center of the brain in people that have AVA. One of the hypotheses is that an early stage of auditory analysis is impaired. The fact that AVA patients have the ability to read shows that both the semantic system and the speech output lexicon are intact. The second hypotheses suggests that there is either a complete or partial disconnection of the auditory input lexicon from the semantic system. This would suggest that entries in the lexicon can still be activated but they cannot go on to cause subsequent semantic activation. In relation to these two different hypotheses, researchers in one study differentiated between two different types of AVA. According to this study, one form of AVA is a deficit at the prephonemic level and is related to the inability to comprehend rapid changes in sound.This form of AVA is associated with bilateral temporal lobe lesions. Speech perception in patients with this form of AVA has been shown to improve significantly in understanding when the pace of speech is drastically slowed. The second type of AVA that the study discusses is a deficit in linguistic discrimination that does not adhere to a prephonemic pattern. This form is associated with left unilateral temporal lobe lesions and may even be considered a form of Wernicke's aphasia. Often individuals diagnosed with auditory verbal agnosia are also incapable of discriminating between non-verbal sounds as well as speech. The underlying problem seems to be temporal in that understanding speech requires the discrimination between specific sounds which are closely spaced in time. Note that this is not unique to speech; studies using non-speech sounds closely spaced in time (dog bark, phone ring, lightning, etc.) have shown that those with auditory verbal agnosia are unable to discriminate between those sounds in the majority of cases, though a few putative examples of speech-specific impairment have been documented in the literature.",
            "score": 176.39753079414368
        },
        {
            "docid": "9266385_2",
            "document": "Duplex perception . Duplex perception refers to the linguistic phenomenon whereby \"part of the acoustic signal is used for both a speech and a nonspeech percept.\" A listener is presented with two simultaneous, dichotic stimuli. One ear receives an isolated third-formant transition that sounds like a nonspeech chirp. At the same time the other ear receives a base syllable. This base syllable consists of the first two formants, complete with formant transitions, and the third formant without a transition. Normally, there would be peripheral masking in such a binaural listening task but this does not occur. Instead, the listener\u2019s percept is duplex, that is, the completed syllable is perceived and the nonspeech chirp is heard at the same time. This is interpreted as being due to the existence of a special speech module.",
            "score": 109.44148993492126
        },
        {
            "docid": "43124500_5",
            "document": "Sophie Scott . Scott is head of the Speech Communication Group at UCL's Institute of Cognitive Neuroscience. Her research investigates the neural basis of vocal communication \u2013 how our brains process the information in speech and voices, and how our brains control the production of our voice. Within this, her research covers the roles of streams of processing in auditory cortex, hemispheric asymmetries, and the interaction of speech processing with attentional and working memory factors. Other interests include individual differences in speech perception and plasticity in speech perception, since these are important factors for people with cochlear implants. She is also interested in the expression of emotion in the voice. In particular, research in recent years has focused on the neuroscience of laughter.",
            "score": 178.49032640457153
        },
        {
            "docid": "8434151_3",
            "document": "Donald Shankweiler . Donald Shankweiler's research career has spanned a number of areas related speech perception, reading, and cognitive neuroscience. His main interests have been studying the acquisition of reading and writing, understanding disorders of reading, writing, and spoken language, and exploring the representation of spoken and written language in the brain. In the 1960s, Shankweiler and Michael Studdert-Kennedy used a dichotic listening technique (presenting different nonsense syllables simultaneously to opposite ears) to demonstrate the dissociation of phonetic (speech) and auditory (nonspeech) perception by finding that phonetic structure devoid of meaning is an integral part of language, typically processed in the left cerebral hemisphere. Alvin Liberman, Franklin S. Cooper, Shankweiler, and Studdert-Kennedy summarized and interpreted fifteen years of research in a paper \"Perception of the Speech Code,\" that argued for the motor theory of speech perception. This is still among the most cited papers in the speech literature. It set the agenda for many years of research at Haskins and elsewhere by describing speech as a code in which speakers overlap (or coarticulate) segments to form syllables.",
            "score": 161.5157951116562
        },
        {
            "docid": "403676_28",
            "document": "Gesture . Gestures are processed in the same areas of the brain as speech and sign language such as the left inferior frontal gyrus (Broca's area) and the posterior middle temporal gyrus, posterior superior temporal sulcus and superior temporal gyrus (Wernicke's area). It has been suggested that these parts of the brain originally supported the pairing of gesture and meaning and then were adapted in human evolution \"for the comparable pairing of sound and meaning as voluntary control over the vocal apparatus was established and spoken language evolved\". As a result, it underlies both symbolic gesture and spoken language in the present human brain. Their common neurological basis also supports the idea that symbolic gesture and spoken language are two parts of a single fundamental semiotic system that underlies human discourse. The linkage of hand and body gestures in conjunction with speech is further revealed by the nature of gesture use in blind individuals during conversation. This phenomenon uncovers a function of gesture that goes beyond portraying communicative content of language and extends David McNeill's view of the gesture-speech system. This suggests that gesture and speech work tightly together, and a disruption of one (speech or gesture) will cause a problem in the other. Studies have found strong evidence that speech and gesture are innately linked in the brain and work in an efficiently wired and choreographed system. McNeill's view of this linkage in the brain is just one of three currently up for debate; the others declaring gesture to be a \"support system\" of spoken language or a physical mechanism for lexical retrieval.",
            "score": 159.68894481658936
        },
        {
            "docid": "25136770_13",
            "document": "Cognitive musicology . Both music and speech rely on sound processing and require interpretation of several sound features such as timbre, pitch, duration, and their interactions (Elzbieta, 2015). A fMRI study revealed that the Broca's and Wernicke's areas, two areas that are known to activated during speech and language processing, were found activated while the subject was listening to unexpected musical chords (Elzbieta, 2015). This relation between language and music may explain why, it has been found that exposure to music has produced an acceleration in the development of behaviors related to the acquisition of language. The Suzuki music education which is very widely known, emphasizes learning music by ear over reading musical notation and preferably begins with formal lessons between the ages of 3 and 5 years. One fundamental reasoning in favor of this education points to a parallelism between natural speech acquisition and purely auditory based musical training as opposed to musical training due to visual cues. There is evidence that children who take music classes have obtained skills to help them in language acquisition and learning (Oechslin, 2015), an ability that relies heavily on the dorsal pathway. Other studies show an overall enhancement of verbal intelligence in children taking music classes. Since both activities tap into several integrated brain functions and have shared brain pathways it is understandable why strength in music acquisition might also correlate with strength in language acquisition.",
            "score": 183.89861714839935
        },
        {
            "docid": "32116125_2",
            "document": "Amblyaudia . Amblyaudia (amblyos- blunt; audia-hearing) is a term coined by Dr. Deborah Moncrieff from the University of Pittsburgh to characterize a specific pattern of performance from dichotic listening tests. Dichotic listening tests are widely used to assess individuals for binaural integration, a type of auditory processing skill. During the tests, individuals are asked to identify different words presented simultaneously to the two ears. Normal listeners can identify the words fairly well and show a small difference between the two ears with one ear slightly dominant over the other. For the majority of listeners, this small difference is referred to as a \"right-ear advantage\" because their right ear performs slightly better than their left ear. But some normal individuals produce a \"left-ear advantage\" during dichotic tests and others perform at equal levels in the two ears. Amblyaudia is diagnosed when the scores from the two ears are significantly different with the individual's dominant ear score much higher than the score in the non-dominant ear  Researchers interested in understanding the neurophysiological underpinnings of amblyaudia consider it to be a brain based hearing disorder that may be inherited or that may result from auditory deprivation during critical periods of brain development. Individuals with amblyaudia have normal hearing sensitivity (in other words they hear soft sounds) but have difficulty hearing in noisy environments like restaurants or classrooms. Even in quiet environments, individuals with amblyaudia may fail to understand what they are hearing, especially if the information is new or complicated. Amblyaudia can be conceptualized as the auditory analog of the better known central visual disorder amblyopia. The term \u201clazy ear\u201d has been used to describe amblyaudia although it is currently not known whether it stems from deficits in the auditory periphery (middle ear or cochlea) or from other parts of the auditory system in the brain, or both. A characteristic of amblyaudia is suppression of activity in the non-dominant auditory pathway by activity in the dominant pathway which may be genetically determined and which could also be exacerbated by conditions throughout early development.",
            "score": 159.33775663375854
        },
        {
            "docid": "1781678_14",
            "document": "Cocktail party effect . Some of the earliest work in exploring mechanisms of early selective attention was performed by Donald Broadbent, who proposed a theory that came to be known as the \"filter model\". This model was established using the dichotic listening task. His research showed that most participants were accurate in recalling information that they actively attended to, but were far less accurate in recalling information that they had not attended to. This led Broadbent to the conclusion that there must be a \"filter\" mechanism in the brain that could block out information that was not selectively attended to. The filter model was hypothesized to work in the following way: as information enters the brain through sensory organs (in this case, the ears) it is stored in sensory memory, a buffer memory system that hosts an incoming stream of information long enough for us to pay attention to it. Before information is processed further, the filter mechanism allows only attended information to pass through. The selected attention is then passed into working memory, the set of mechanisms that underlies short-term memory and communicates with long-term memory. In this model, auditory information can be selectively attended to on the basis of its physical characteristics, such as location and volume. Others suggest that information can be attended to on the basis of Gestalt features, including continuity and closure. For Broadbent, this explained the mechanism by which people can choose to attend to only one source of information at a time while excluding others. However, Broadbent's model failed to account for the observation that words of semantic importance, for example the individual's own name, can be instantly attended to despite having been in an unattended channel.",
            "score": 176.4834144115448
        },
        {
            "docid": "32036278_6",
            "document": "Dichotic listening . An emotional version of the dichotic listening task was developed. In this version individuals listen to the same word in each ear but they hear it in either a surprised, happy, sad, angry, or neutral tone. Participants are then asked to press a button indicating what tone they heard. Usually dichotic listening tests show a right-ear advantage for speech sounds. Right-ear/left-hemisphere advantage is expected, because of evidence from Broca's area and Wernicke's area, which are both located in the left hemisphere. In contrast, the left ear (and therefore the right hemisphere) is often better at processing nonlinguistic material. The data from the emotional dichotic listening task is consistent with the other studies, because participants tend to have more correct responses to their left ear than to the right. It is important to note that the emotional dichotic listening task is seemingly harder for the participants than the phonemic dichotic listening task. Meaning more incorrect responses were submitted by individuals.",
            "score": 103.95466637611389
        }
    ],
    "r": [
        {
            "docid": "433584_4",
            "document": "McGurk effect . Vision is the primary sense for humans, but speech perception is multimodal, which means that it involves information from more than one sensory modality, in particular, audition and vision. The McGurk effect arises during phonetic processing because the integration of audio and visual information happens early in speech perception. The McGurk effect is very robust; that is, knowledge about it seems to have little effect on one's perception of it. This is different from certain optical illusions, which break down once one 'sees through' them. Some people, including those that have been researching the phenomenon for more than twenty years, experience the effect even when they are aware that it is taking place. With the exception of people who can identify most of what is being said from speech-reading alone, most people are quite limited in their ability to identify speech from visual-only signals. A more extensive phenomenon is the ability of visual speech to increase the intelligibility of heard speech in a noisy environment. Visible speech can also alter the perception of perfectly audible speech sounds when the visual speech stimuli are mismatched with the auditory speech. Normally, speech perception is thought to be an auditory process; however, our use of information is immediate, automatic, and, to a large degree, unconscious and therefore, despite what is widely accepted as true, speech is not only something we hear. Speech is perceived by all of the senses working together (seeing, touching, and listening to a face move). The brain is often unaware of the separate sensory contributions of what it perceives. Therefore, when it comes to recognizing speech the brain cannot differentiate whether it is seeing or hearing the incoming information.",
            "score": 209.9219207763672
        },
        {
            "docid": "5366050_50",
            "document": "Speech perception . Neurophysiological methods rely on utilizing information stemming from more direct and not necessarily conscious (pre-attentative) processes. Subjects are presented with speech stimuli in different types of tasks and the responses of the brain are measured. The brain itself can be more sensitive than it appears to be through behavioral responses. For example, the subject may not show sensitivity to the difference between two speech sounds in a discrimination test, but brain responses may reveal sensitivity to these differences. Methods used to measure neural responses to speech include event-related potentials, magnetoencephalography, and near infrared spectroscopy. One important response used with event-related potentials is the mismatch negativity, which occurs when speech stimuli are acoustically different from a stimulus that the subject heard previously.",
            "score": 209.5182647705078
        },
        {
            "docid": "32018467_7",
            "document": "Christian Keysers . After finishing his master, Christian Keysers decided to concentrate on a subfield of cognitive neuroscience called social neuroscience that uses neuroscience methods to understand how we process the social world. He therefore performed his doctoral studies at the University of St Andrews with David Ian Perrett, one of the founding father of the field, to understand how the brain processes faces and facial expressions. This thesis work led to new insights into how quickly the brain can process the faces of others. During this period, Keysers became fascinated with the question of how the brain can attach meaning to the faces of others. How is it for instance, that we understand that a certain grimace would signal that another person is happy? How do we understand that a certain bodily movement towards a glass indicates that the other person aims to grasp a glass? In 1999, Keysers was exposed to a visit of Vittorio Gallese, who presented his recent discovery of mirror neurons in the Psychology department lecture series. This deeply influenced Keysers who decided to move to the lab of Giacomo Rizzolatti to undertake further studies on how these fascinating neurons could contribute to social perception. In 2000, after finishing his doctorate, Christian Keysers moved to the University of Parma to study mirror neurons. In early work there demonstrated that mirror neurons in the premotor cortex not only respond to the sight of actions, but also when actions can only be deduced or heard, leading to a publication in the journal \"Science\". This work had tremendous impact on the field, as it suggested that the premotor cortex could play a central, modality independent role in perception and may lay the origin for the evolution of speech in humans.  Together this work indicated that brain regions involved in our own actions play a role in how we process the actions of others. Keysers wondered whether a similar principle may underlie how we process the tactile sensations and emotions of others, and became increasingly independent of the research focus on the motor system in Parma. At the time, Keysers had also met his to be wife, Valeria Gazzola, a biologist in the final phases of her studies, and together they decided to explore if the somatosensory system might be involved in perceiving the sensations of others. Via a fruitful collaboration with the French neuroimaging specialist Bruno Wicker, they used functional magnetic resonance imaging, and showed for the first time, that the secondary somatosensory cortex, previously thought only to represent a persons own experiences of touch, is also activated when seeing someone or something else be touched. They also showed that the insula, thought only to respond to the experience of first-hand emotions, was also activated when we see another individual experience similar emotions. Together this indicated a much more general principle than the original mirror neuron theory, in which people process the actions, sensations and emotions of others by vicariously activating owns own actions, sensations and emotions. Jointly, this work laid the foundation of the neuroscientific investigation of empathy.",
            "score": 197.8717803955078
        },
        {
            "docid": "35988494_3",
            "document": "Selective auditory attention . The cocktail party problem was first brought up in 1953 by Colin Cherry. This common problem is how our minds solves the issue of knowing what in the auditory scene is important and combining those in a coherent whole, such as the problem of how we can perceive our friend talking in the midst of a crowded cocktail party. He suggested that the auditory system can filter sounds being heard. Physical characteristics of the auditory information such as speaker's voice or location can improve a person's ability to focus on certain stimuli even if there is other auditory stimuli present. Cherry also did work with shadowing which involves different information being played into both ears and only one ear's information can be processed and remembered (Eysneck, 2012, p.\u00a084). Another psychologist, Albert Bregman, came up with the auditory scene analysis model. The model has three main characteristics: segmentation, integration, and segregation. Segmentation involves the division of auditory messages into segments of importance. The process of combining parts of an auditory message to form a whole is associated with integration. Segregation is the separation of important auditory messages and the unwanted information in the brain. It is important to note that Bregman also makes a link back to the idea of perception. He states that it is essential for one to make a useful representation of the world from sensory inputs around us. Without perception, an individual will not recognize or have the knowledge of what is going on around them. While Begman's seminal work is critical to understanding selective auditory attention, his studies did not focus on the way in which an auditory message is selected, if and when it was correctly segregated from other sounds in a mixture, which is a critical stage of selective auditory attention. Inspired in part by Bregman's work, a number of researchers then set out to link directly work on auditory scene analysis to the processes governing attention, including Maria Chait, Mounya Elhilali, Shihab Shamma, and Barbara Shinn-Cunningham.",
            "score": 191.0166015625
        },
        {
            "docid": "35988494_2",
            "document": "Selective auditory attention . Selective auditory attention or selective hearing is a type of selective attention and involves the auditory system of the nervous system. Selective hearing is characterized as the action in which people focus their attention on a specific source of a sound or spoken words. The sounds and noise in the surrounding environment is heard by the auditory system but only certain parts of the auditory information are processed in the brain. Most often, auditory attention is directed at things people are most interested in hearing. In an article by Krans, Isbell, Giuliano, and Neville (2013), selective auditory attention is defined as the ability to acknowledge some stimuli while ignoring other stimuli that is occurring at the same time. An example of this is a student focusing on a teacher giving a lesson and ignoring the sounds of classmates in a rowdy classroom (p.\u00a053). This is an example of bottlenecking which means that information cannot be processed simultaneously so only some sensory information gets through the \"bottleneck\" and is processed. A brain simply cannot process all sensory information that is occurring in an environment so only that which is most important is thoroughly processed. Selective hearing is not a physiological disorder but rather it is the capability of humans to block out sounds and noise. It is the notion of ignoring certain things in the surrounding environment. Over the years, there has been increased research in the selectivity of auditory attention, namely selective hearing.",
            "score": 190.04779052734375
        },
        {
            "docid": "37759941_5",
            "document": "Crossmodal attention . While research on cross-modal attention has found that deficits in attending often occur, this research has led to a better understanding of attentional processing. Some studies have used positron emission tomography (PET) to examine the neurological basis for how we selectively attend to information using different sensory modalities. Event related potentials (ERPs). have also been used to help researchers measure how humans encode and process attended information in the brain. By increasing our understanding of modality-specific and cross-modal attention, we are better able to understand how we think and direct our attention.",
            "score": 188.30801391601562
        },
        {
            "docid": "14241792_3",
            "document": "TRACE (psycholinguistics) . TRACE was created during the formative period of connectionism, and was included as a chapter in \"Parallel Distributed Processing: Explorations in the Microstructures of Cognition\". The researchers found that certain problems regarding speech perception could be conceptualized in terms of a connectionist interactive activation model. The problems were that (1) speech is extended in time, (2) the sounds of speech (phonemes) overlap with each other, (3) the articulation of a speech sound is affected by the sounds that come before and after it, and (4) there is natural variability in speech (e.g. foreign accent) as well as noise in the environment (e.g. busy restaurant). Each of these causes the speech signal to be complex and often ambiguous, making it difficult for the human mind/brain to decide what words it is really hearing. In very simple terms, an interactive activation model solves this problem by placing different kinds of processing units (phonemes, words) in isolated layers, allowing activated units to pass information between layers, and having units within layers compete with one another, until the \u201cwinner\u201d is considered \u201crecognized\u201d by the model.",
            "score": 186.8614501953125
        },
        {
            "docid": "9736296_43",
            "document": "Linguistic performance . Unacceptable Sentences are ones which, although are grammatical, are not considered proper utterances. They are considered unacceptable due to the lack of our cognitive systems to process them. Speakers and listeners can be aided in the performance and processing of these sentences by eliminating time and memory constraints, increasing motivation to process these utterances and using pen and paper. In English there are three types of sentences that are grammatical but are considered unacceptable by speakers and listeners. When a speaker makes an utterance they must translate their ideas into words, then syntactically proper phrases with proper pronunciation. The speaker must have prior world knowledge and an understanding of the grammatical rules that their language enforces. When learning a second language or with children acquiring their first language, speakers usually have this knowledge before they are able to produce them. Their speech is usually slow and deliberate, using phrases they have already mastered, and with practice their skills increase. Errors of linguistic performance are judged by the listener giving many interpretations if an utterance is well-formed or ungrammatical depending on the individual. As well the context in which an utterance is used can determine if the error would be considered or not. When comparing \"Who must telephone her?\" and \"Who need telephone her?\" the former would be considered the ungrammatical phrase. However, when comparing it to \"Who want telephone her?\" it would be considered the grammatical phrase. The listener may also be the speaker. When repeating sentences with errors if the error is not comprehended then it is performed. As well if the speaker does notice the error in the sentence they are supposed to repeat they are unaware of the difference between their well-formed sentence and the ungrammatical sentence. An unacceptable utterance can also be performed due to a brain injury. Three types of brain injuries that could cause errors in performance were studied by Fromkin are dysarthria, apraxia and literal paraphasia. Dysarthria is a defect in the neuromuscular connection that involves speech movement. The speech organs involved can be paralyzed or weakened, making it difficult or impossible for the speaker to produce a target utterance. Apraxia is when there is damage to the ability to initiate speech sounds with no paralysis or weakening of the articulators. Literal paraphasia causes disorganization of linguistic properties, resulting in errors of word order of phonemes. Having a brain injury and being unable to perform proper linguistic utterances, some individuals are still able to process complex sentences and formulate syntactically well formed sentences in their mind. Child productions when they are acquiring language are full of errors of linguistic performance. Children must go from imitating adult speech to create new phrases of their own. They will need to use their cognitive operations of the knowledge of their language they are learning to determine the rules and properties of that language. The following are examples of errors in English speaking children's productions.",
            "score": 185.46728515625
        },
        {
            "docid": "25136770_13",
            "document": "Cognitive musicology . Both music and speech rely on sound processing and require interpretation of several sound features such as timbre, pitch, duration, and their interactions (Elzbieta, 2015). A fMRI study revealed that the Broca's and Wernicke's areas, two areas that are known to activated during speech and language processing, were found activated while the subject was listening to unexpected musical chords (Elzbieta, 2015). This relation between language and music may explain why, it has been found that exposure to music has produced an acceleration in the development of behaviors related to the acquisition of language. The Suzuki music education which is very widely known, emphasizes learning music by ear over reading musical notation and preferably begins with formal lessons between the ages of 3 and 5 years. One fundamental reasoning in favor of this education points to a parallelism between natural speech acquisition and purely auditory based musical training as opposed to musical training due to visual cues. There is evidence that children who take music classes have obtained skills to help them in language acquisition and learning (Oechslin, 2015), an ability that relies heavily on the dorsal pathway. Other studies show an overall enhancement of verbal intelligence in children taking music classes. Since both activities tap into several integrated brain functions and have shared brain pathways it is understandable why strength in music acquisition might also correlate with strength in language acquisition.",
            "score": 183.89862060546875
        },
        {
            "docid": "355240_5",
            "document": "Cognitive model . A number of key terms are used to describe the processes involved in the perception, storage, and production of speech. Typically, they are used by speech pathologists while treating a child patient. The input signal is the speech signal heard by the child, usually assumed to come from an adult speaker. The output signal is the utterance produced by the child. The unseen psychological events that occur between the arrival of an input signal and the production of speech are the focus of psycholinguistic models. Events that process the input signal are referred to as input processes, whereas events that process the production of speech are referred to as output processes. Some aspects of speech processing are thought to happen online\u2014that is, they occur during the actual perception or production of speech and thus require a share of the attentional resources dedicated to the speech task. Other processes, thought to happen offline, take place as part of the child\u2019s background mental processing rather than during the time dedicated to the speech task. In this sense, online processing is sometimes defined as occurring in real-time, whereas offline processing is said to be time-free (Hewlett, 1990). In box-and-arrow psycholinguistic models, each hypothesized level of representation or processing can be represented in a diagram by a \u201cbox,\u201d and the relationships between them by \u201carrows,\u201d hence the name. Sometimes (as in the models of Smith, 1973, and Menn, 1978, described later in this paper) the arrows represent processes additional to those shown in boxes. Such models make explicit the hypothesized information- processing activities carried out in a particular cognitive function (such as language), in a manner analogous to computer flowcharts that depict the processes and decisions carried out by a computer program. Box-and-arrow models differ widely in the number of unseen psychological processes they describe and thus in the number of boxes they contain. Some have only one or two boxes between the input and output signals (e.g., Menn, 1978; Smith, 1973), whereas others have multiple boxes representing complex relationships between a number of different information-processing events (e.g., Hewlett, 1990; Hewlett, Gibbon, & Cohen- McKenzie,1998; Stackhouse & Wells, 1997). The most important box, however, and the source of much ongoing debate, is that representing the underlying representation (or UR). In essence, an underlying representation captures information stored in a child\u2019s mind about a word he or she knows and uses. As the following description of several models will illustrate, the nature of this information and thus the type(s) of representation present in the child\u2019s knowledge base have captured the attention of researchers for some time. (Elise Baker et al. Psycholinguistic Models of Speech Development and Their Application to Clinical Practice. Journal of Speech, Language, and Hearing Research. June 2001. 44. p 685\u2013702.)",
            "score": 182.9929962158203
        },
        {
            "docid": "5366050_42",
            "document": "Speech perception . Research into the relationship between music and cognition is an emerging field related to the study of speech perception. Originally it was theorized that the neural signals for music were processed in a specialized \"module\" in the right hemisphere of the brain. Conversely, the neural signals for language were to be processed by a similar \"module\" in the left hemisphere. However, utilizing technologies such as fMRI machines, research has shown that two regions of the brain traditionally considered exclusively to process speech, Broca's and Wernicke's areas, also become active during musical activities such as listening to a sequence of musical chords. Other studies, such as one performed by Marques et al. in 2006 showed that 8-year-olds who were given six months of musical training showed an increase in both their pitch detection performance and their electrophysiological measures when made to listen to an unknown foreign language.",
            "score": 180.5974884033203
        },
        {
            "docid": "43124500_5",
            "document": "Sophie Scott . Scott is head of the Speech Communication Group at UCL's Institute of Cognitive Neuroscience. Her research investigates the neural basis of vocal communication \u2013 how our brains process the information in speech and voices, and how our brains control the production of our voice. Within this, her research covers the roles of streams of processing in auditory cortex, hemispheric asymmetries, and the interaction of speech processing with attentional and working memory factors. Other interests include individual differences in speech perception and plasticity in speech perception, since these are important factors for people with cochlear implants. She is also interested in the expression of emotion in the voice. In particular, research in recent years has focused on the neuroscience of laughter.",
            "score": 178.49032592773438
        },
        {
            "docid": "1781678_2",
            "document": "Cocktail party effect . The cocktail party effect is the phenomenon of the brain's ability to focus one's auditory attention (an effect of selective attention in the brain) on a particular stimulus while filtering out a range of other stimuli, as when a partygoer can focus on a single conversation in a noisy room. Listeners have the ability to both segregate different stimuli into different streams, and subsequently decide which streams are most pertinent to them. Thus, it has been proposed that one's sensory memory subconsciously parses all stimuli, identifying discrete pieces of information and classifying them by salience. This effect is what allows most people to \"tune into\" a single voice and \"tune out\" all others. It may also describe a similar phenomenon that occurs when one may immediately detect words of importance originating from unattended stimuli, for instance hearing one's name among a wide range of auditory input.",
            "score": 177.106201171875
        },
        {
            "docid": "1781678_14",
            "document": "Cocktail party effect . Some of the earliest work in exploring mechanisms of early selective attention was performed by Donald Broadbent, who proposed a theory that came to be known as the \"filter model\". This model was established using the dichotic listening task. His research showed that most participants were accurate in recalling information that they actively attended to, but were far less accurate in recalling information that they had not attended to. This led Broadbent to the conclusion that there must be a \"filter\" mechanism in the brain that could block out information that was not selectively attended to. The filter model was hypothesized to work in the following way: as information enters the brain through sensory organs (in this case, the ears) it is stored in sensory memory, a buffer memory system that hosts an incoming stream of information long enough for us to pay attention to it. Before information is processed further, the filter mechanism allows only attended information to pass through. The selected attention is then passed into working memory, the set of mechanisms that underlies short-term memory and communicates with long-term memory. In this model, auditory information can be selectively attended to on the basis of its physical characteristics, such as location and volume. Others suggest that information can be attended to on the basis of Gestalt features, including continuity and closure. For Broadbent, this explained the mechanism by which people can choose to attend to only one source of information at a time while excluding others. However, Broadbent's model failed to account for the observation that words of semantic importance, for example the individual's own name, can be instantly attended to despite having been in an unattended channel.",
            "score": 176.48341369628906
        },
        {
            "docid": "3237319_9",
            "document": "Auditory verbal agnosia . Auditory verbal agnosia is the inability to distinguish phonemes. In some patients with unilateral auditory verbal agnosia, there is evidence that the ability to acoustically process speech signals is affected at the prephonemic level, preventing the conversion of these signals into phonemes. There are two predominate hypotheses that address what happens within the language center of the brain in people that have AVA. One of the hypotheses is that an early stage of auditory analysis is impaired. The fact that AVA patients have the ability to read shows that both the semantic system and the speech output lexicon are intact. The second hypotheses suggests that there is either a complete or partial disconnection of the auditory input lexicon from the semantic system. This would suggest that entries in the lexicon can still be activated but they cannot go on to cause subsequent semantic activation. In relation to these two different hypotheses, researchers in one study differentiated between two different types of AVA. According to this study, one form of AVA is a deficit at the prephonemic level and is related to the inability to comprehend rapid changes in sound.This form of AVA is associated with bilateral temporal lobe lesions. Speech perception in patients with this form of AVA has been shown to improve significantly in understanding when the pace of speech is drastically slowed. The second type of AVA that the study discusses is a deficit in linguistic discrimination that does not adhere to a prephonemic pattern. This form is associated with left unilateral temporal lobe lesions and may even be considered a form of Wernicke's aphasia. Often individuals diagnosed with auditory verbal agnosia are also incapable of discriminating between non-verbal sounds as well as speech. The underlying problem seems to be temporal in that understanding speech requires the discrimination between specific sounds which are closely spaced in time. Note that this is not unique to speech; studies using non-speech sounds closely spaced in time (dog bark, phone ring, lightning, etc.) have shown that those with auditory verbal agnosia are unable to discriminate between those sounds in the majority of cases, though a few putative examples of speech-specific impairment have been documented in the literature.",
            "score": 176.3975372314453
        },
        {
            "docid": "179092_18",
            "document": "Neurolinguistics . Electrophysiological techniques take advantage of the fact that when a group of neurons in the brain fire together, they create an electric dipole or current. The technique of EEG measures this electric current using sensors on the scalp, while MEG measures the magnetic fields that are generated by these currents. In addition to these non-invasive methods, electrocorticography has also been used to study language processing. These techniques are able to measure brain activity from one millisecond to the next, providing excellent \"temporal resolution\", which is important in studying processes that take place as quickly as language comprehension and production. On the other hand, the location of brain activity can be difficult to identify in EEG; consequently, this technique is used primarily to \"how\" language processes are carried out, rather than \"where\". Research using EEG and MEG generally focuses on event-related potentials (ERPs), which are distinct brain responses (generally realized as negative or positive peaks on a graph of neural activity) elicited in response to a particular stimulus. Studies using ERP may focus on each ERP's \"latency\" (how long after the stimulus the ERP begins or peaks), \"amplitude\" (how high or low the peak is), or \"topography\" (where on the scalp the ERP response is picked up by sensors). Some important and common ERP components include the N400 (a negativity occurring at a latency of about 400 milliseconds), the mismatch negativity, the early left anterior negativity (a negativity occurring at an early latency and a front-left topography), the P600, and the lateralized readiness potential.",
            "score": 176.18238830566406
        },
        {
            "docid": "236809_36",
            "document": "Recall (memory) . Recall memory is linked with instincts and mechanisms in order to remember how an event happened to learn from it or avoid the agitator, connections are made with emotions. For instance, if a speaker is very calm and neutral, the effectiveness of encoding memory is very low and listeners get the gist of what the speaker is discussing. On the other hand, if a speaker is shouting and/or using emotionally driven words, listeners tend to remember key phrases and the meaning of the speech. This is full access of the fight or flight mechanism all people have functioning in the brain, but based on what triggers this mechanism will lead to better recall of it. People tend to focus their attention on cues that are loud, very soft, or something unusual. This makes the auditory system pick up the differences in regular speaking and meaningful speech, when something significant is spoken in the discussion people home in on the message at that part of the speech but tend to lose the other part of the discussion. Our brains sense differences in speech and when those differences occur the brain encodes that part of speech into memory and the information can be recalled for future reference.",
            "score": 174.8864288330078
        },
        {
            "docid": "2917649_10",
            "document": "Speech . Speech perception refers to the processes by which humans can interpret and understand the sounds used in language. The study of speech perception is closely linked to the fields of phonetics and phonology in linguistics and cognitive psychology and perception in psychology. Research in speech perception seeks to understand how listeners recognize speech sounds and use this information to understand spoken language. Research into speech perception also has applications in building computer systems that can recognize speech, as well as improving speech recognition for hearing- and language-impaired listeners.",
            "score": 174.3571014404297
        },
        {
            "docid": "6541938_8",
            "document": "Lexical-gustatory synesthesia . SC is a synesthete who automatically experiences smells, tastes, and feelings of textures in her mouth and throat when she reads, speaks, or hears language, music, and certain environmental sounds. In SC\u2019s case study, researchers utilized fMRI to determine the areas of the brain that were activated during her synesthetic experiences. They compared areas of activation in SC\u2019s brain to those found in literature for other synesthetes, speech processing, language, and sound processing. In SC\u2019s scans, two important regions of the brain were largely activated during her taste sensations: the left anterior insula and the left superior parietal lobe. The scans led researchers to speculate that the anterior insula may play a role in SC\u2019s taste experiences while the superior parietal lobe binds together all of the sensory information for processing. Based off the findings of this study and others like it, it could be possible to determine the type of inducer that leads to synesthetic sensations based on the patterns of brain activity.",
            "score": 174.29673767089844
        },
        {
            "docid": "33980253_4",
            "document": "Attenuation theory . Early research came from an era primarily focused upon audition and explaining phenomena such as the cocktail party effect. From this stemmed interest about how we can pick and choose to attend to certain sounds in our surroundings, and at a deeper level, how the processing of attended speech signals differ from those not attended to. Auditory attention is often described as the selection of a channel, message, ear, stimulus, or in the more general phrasing used by Treisman, the \"selection between inputs\". As audition became the preferred way of examining selective attention, so too did the testing procedures of \"dichotic listening\" and \"shadowing\".",
            "score": 172.27227783203125
        },
        {
            "docid": "422247_33",
            "document": "Self-awareness . Autism spectrum disorder (ASD) is a range of neurodevelopmental disabilities that can adversely impact social communication and create behavioral challenges (Understanding Autism, 2003). \"Autism spectrum disorder (ASD) and autism are both general terms for a group of complex disorders of brain development. These disorders are characterized, in varying degrees, by difficulties in social interaction, verbal and nonverbal communication and repetitive behaviors.\" ASDs can also cause imaginative abnormalities and can range from mild to severe, especially in sensory-motor, perceptual and affective dimensions. Children with ASD may struggle with self-awareness and self acceptance. Their different thinking patterns and brain processing functions in the area of social thinking and actions may compromise their ability to understand themselves and social connections to others. About 75% diagnosed autistics are mentally handicapped in some general way and the other 25% diagnosed with Asperger's Syndrome show average to good cognitive functioning. When we compare our own behavior to the morals and values that we were taught, we can focus more attention on ourselves which increases self-awareness. To understand the many effects of autism spectrum disorders on those afflicted have led many scientists to theorize what level of self-awareness occurs and in what degree. Research found that ASD can be associated with intellectual disability and difficulties in motor coordination and attention. It can also result in physical health issues as well, such as sleep and gastrointestinal disturbances. As a result of all those problems, individuals are literally unaware of themselves. It is well known that children suffering from varying degrees of autism struggle in social situations. Scientists at the University of Cambridge have produced evidence that self-awareness is a main problem for people with ASD. Researchers used functional magnetic resonance scans (FMRI) to measure brain activity in volunteers being asked to make judgments about their own thoughts, opinions, preferences, as well as about someone else's. One area of the brain closely examined was the ventromedial pre-frontal cortex (vMPFC) which is known to be active when people think about themselves. A study out of Stanford University has tried to map out brain circuits with understanding self-awareness in Autism Spectrum Disorders. This study suggests that self-awareness is primarily lacking in social situations but when in private they are more self-aware and present. It is in the company of others while engaging in interpersonal interaction that the self-awareness mechanism seems to fail. Higher functioning individuals on the ASD scale have reported that they are more self-aware when alone unless they are in sensory overload or immediately following social exposure. Self-awareness dissipates when an autistic is faced with a demanding social situation. This theory suggests that this happens due to the behavioral inhibitory system which is responsible for self-preservation. This is the system that prevents human from self-harm like jumping out of a speeding bus or putting our hand on a hot stove. Once a dangerous situation is perceived then the behavioral inhibitory system kicks in and restrains our activities. \"For individuals with ASD, this inhibitory mechanism is so powerful, it operates on the least possible trigger and shows an over sensitivity to impending danger and possible threats. Some of these dangers may be perceived as being in the presence of strangers, or a loud noise from a radio. In these situations self-awareness can be compromised due to the desire of self preservation, which trumps social composure and proper interaction. The Hobson hypothesis reports that autism begins in infancy due to the lack of cognitive and linguistic engagement which in turn results in impaired reflective self-awareness. In this study ten children with Asperger's Syndrome were examined using the Self-understanding Interview. This interview was created by Damon and Hart and focuses on seven core areas or schemas that measure the capacity to think in increasingly difficult levels. This interview will estimate the level of self understanding present. \"The study showed that the Asperger group demonstrated impairment in the 'self-as-object' and 'self-as-subject' domains of the Self-understanding Interview, which supported Hobson's concept of an impaired capacity for self-awareness and self-reflection in people with ASD.\". Self-understanding is a self description in an individual's past, present and future. Without self-understanding it is reported that self-awareness is lacking in people with ASD. Joint attention (JA) was developed as a teaching strategy to help increase positive self-awareness in those with autism spectrum disorder. JA strategies were first used to directly teach about reflected mirror images and how they relate to their reflected image. Mirror Self Awareness Development (MSAD) activities were used as a four-step framework to measure increases in self-awareness in those with ASD. Self-awareness and knowledge is not something that can simply be taught through direct instruction. Instead, students acquire this knowledge by interacting with their environment. Mirror understanding and its relation to the development of self leads to measurable increases in self-awareness in those with ASD. It also proves to be a highly engaging and highly preferred tool in understanding the developmental stages of self- awareness. There have been many different theories and studies done on what degree of self-awareness is displayed among people with autism spectrum disorder. Scientists have done research about the various parts of the brain associated with understanding self and self-awareness. Studies have shown evidence of areas of the brain that are impacted by ASD. Other theories suggest that helping an individual learn more about themselves through Joint Activities, such as the Mirror Self Awareness Development may help teach positive self-awareness and growth. In helping to build self-awareness it is also possible to build self-esteem and self acceptance. This in turn can help to allow the individual with ASD to relate better to their environment and have better social interactions with others.",
            "score": 171.8761749267578
        },
        {
            "docid": "5366050_2",
            "document": "Speech perception . Speech perception is the process by which the sounds of language are heard, interpreted and understood. The study of speech perception is closely linked to the fields of phonology and phonetics in linguistics and cognitive psychology and perception in psychology. Research in speech perception seeks to understand how human listeners recognize speech sounds and use this information to understand spoken language. Speech perception research has applications in building computer systems that can recognize speech, in improving speech recognition for hearing- and language-impaired listeners, and in foreign-language teaching.",
            "score": 171.21397399902344
        },
        {
            "docid": "5626_4",
            "document": "Cognitive science . A central tenet of cognitive science is that a complete understanding of the mind/brain cannot be attained by studying only a single level. An example would be the problem of remembering a phone number and recalling it later. One approach to understanding this process would be to study behavior through direct observation, or naturalistic observation. A person could be presented with a phone number and be asked to recall it after some delay of time. Then, the accuracy of the response could be measured. Another approach to measure cognitive ability would be to study the firings of individual neurons while a person is trying to remember the phone number. Neither of these experiments on its own would fully explain how the process of remembering a phone number works. Even if the technology to map out every neuron in the brain in real-time were available, and it were known when each neuron was firing, it would still be impossible to know how a particular firing of neurons translates into the observed behavior. Thus, an understanding of how these two levels relate to each other is imperative. \"The Embodied Mind: Cognitive Science and Human Experience\" says, \u201cthe new sciences of the mind need to enlarge their horizon to encompass both lived human experience and the possibilities for transformation inherent in human experience.\u201d This can be provided by a functional level account of the process. Studying a particular phenomenon from multiple levels creates a better understanding of the processes that occur in the brain to give rise to a particular behavior. Marr gave a famous description of three levels of analysis:",
            "score": 171.178466796875
        },
        {
            "docid": "1781678_9",
            "document": "Cocktail party effect . Selective attention shows up across all ages. Starting with infancy, babies begin to turn their heads toward a sound that is familiar to them, such as their parents' voices. This shows that infants selectively attend to specific stimuli in their environment. Furthermore, reviews of selective attention indicate that infants favor \"baby\" talk over speech with an adult tone. This preference indicates that infants can recognize physical changes in the tone of speech. However, the accuracy in noticing these physical differences, like tone, amid background noise improves over time. Infants may simply ignore stimuli because something like their name, while familiar, holds no higher meaning to them at such a young age. However, research suggests that the more likely scenario is that infants do not understand that the noise being presented to them amidst distracting noise is their own name, and thus do not respond. The ability to filter out unattended stimuli reaches its prime in young adulthood. In reference to the cocktail party phenomenon, older adults have a harder time than younger adults focusing in on one conversation if competing stimuli, like \"subjectively\" important messages, make up the background noise.",
            "score": 170.8370819091797
        },
        {
            "docid": "10567836_7",
            "document": "Ordinal numerical competence . Both behavioral research and brain-imaging research show distinct differences in the way \"exact\" arithmetic and \"approximate\" arithmetic are processed. Exact arithmetic is information that is precise and follows specific rules and patterns such as multiplication tables or geometric formulas, and approximate arithmetic is a general comparison between numbers such as the comparisons of greater than or less than. Research shows that exact arithmetic is language-based and processed in the left inferior frontal lobe. Approximate arithmetic is processed much differently in a different part of the brain. Approximate arithmetic is processed in the bilateral areas of the parietal lobes. This part of the brain processes visual information to understand how objects are spatially related to each other, for example, understanding that 10 of something is more than two of something. This difference in brain function can create a difference in how we experience certain types of arithmetic. Approximate arithmetic can be experienced as intuitive and exact arithmetic experienced as recalled knowledge.",
            "score": 170.69432067871094
        },
        {
            "docid": "37759941_2",
            "document": "Crossmodal attention . Crossmodal attention refers to the distribution of attention to different senses. Attention is the cognitive process of selectively emphasizing and ignoring sensory stimuli. According to the crossmodal attention perspective, attention often occurs simultaneously through multiple sensory modalities. These modalities process information from the different sensory fields, such as: visual, auditory, spatial, and tacitile. While each of these is designed to process a specific type of sensory information, there is considerable overlap between them which has led researchers to question whether attention is modality-specific or the result of shared \"cross-modal\" resources. \"Cross-modal attention\" is considered to be the overlap between modalities that can both enhance and limit attentional processing. The most common example given of crossmodal attention is the Cocktail Party Effect, which is when a person is able to focus and attend to one important stimulus instead of other less important stimuli. This phenomenon allows deeper levels of processing to occur for one stimuli while others are then ignored.",
            "score": 169.93035888671875
        },
        {
            "docid": "12328438_38",
            "document": "Auditory processing disorder . Treating additional issues related to APD can result in success. For example, treatment for phonological disorders (difficulty in speech) can result in success in terms of both the phonological disorder as well as APD. In one study, speech therapy improved auditory evoked potentials (a measure of brain activity in the auditory portions of the brain).",
            "score": 169.8311767578125
        },
        {
            "docid": "11920671_6",
            "document": "Machine perception . Machine hearing, also known as machine listening or computer audition, is the ability of a computer or machine to take in and process sound data such as music or speech. This area has a wide range of application including music recording and compression, speech synthesis, and speech recognition. Moreover, this technology allows the machine to replicate the human brain\u2019s ability to selectively focus in a specific sound against many other competing sounds and background noise. This particular ability is called \u201cauditory scene analysis\u201d. The technology enables the machine to segment several streams occurring at the same time. Many commonly used devices such as a smartphones, voice translators, and cars make use of some form of machine hearing.",
            "score": 168.56314086914062
        },
        {
            "docid": "179092_31",
            "document": "Neurolinguistics . Some experiments give subjects a \"distractor\" task to ensure that subjects are not consciously paying attention to the experimental stimuli; this may be done to test whether a certain computation in the brain is carried out automatically, regardless of whether the subject devotes attentional resources to it. For example, one study had subjects listen to non-linguistic tones (long beeps and buzzes) in one ear and speech in the other ear, and instructed subjects to press a button when they perceived a change in the tone; this supposedly caused subjects not to pay explicit attention to grammatical violations in the speech stimuli. The subjects showed a mismatch response (MMN) anyway, suggesting that the processing of the grammatical errors was happening automatically, regardless of attention\u2014or at least that subjects were unable to consciously separate their attention from the speech stimuli.",
            "score": 167.9246368408203
        },
        {
            "docid": "389579_2",
            "document": "Cognitive neuropsychology . Cognitive neuropsychology is a branch of cognitive psychology that aims to understand how the structure and function of the brain relates to specific psychological processes. Cognitive psychology is the science that looks at how mental processes are responsible for our cognitive abilities to store and produce new memories, produce language, recognize people and objects, as well as our ability to reason and problem solve. Cognitive neuropsychology places a particular emphasis on studying the cognitive effects of brain injury or neurological illness with a view to inferring models of normal cognitive functioning. Evidence is based on case studies of individual brain damaged patients who show deficits in brain areas and from patients who exhibit double dissociations. Double dissociations involve two patients and two tasks. One patient is impaired at one task but normal on the other, while the other patient is normal on the first task and impaired on the other. For example, patient A would be poor at reading printed words while still being normal at understanding spoken words, while the patient B would be normal at understanding written words and be poor at understanding spoken words. Scientists can interpret this information to explain how there is a single cognitive module for word comprehension. From studies like these, researchers infer that different areas of the brain are highly specialised. Cognitive neuropsychology can be distinguished from cognitive neuroscience, which is also interested in brain damaged patients, but is particularly focused on uncovering the neural mechanisms underlying cognitive processes.",
            "score": 167.86053466796875
        },
        {
            "docid": "540571_8",
            "document": "Wernicke's area . Support for a broad range of speech processing areas was furthered by a recent study done at University of Rochester in which American Sign Language native speakers were subject to MRIs while interpreting sentences that identified a relationship using either syntax (relationship is determined by the word order) or inflection (relationship is determined by physical motion of \"moving hands through space or signing on one side of the body\"). Distinct areas of the brain were activated with the frontal cortex (associated with ability to put information into sequences) being more active in the syntax condition and the temporal lobes (associated with dividing information into its constituent parts) being more active in the inflection condition. However, these areas are not mutually exclusive and show a large amount of overlap. These findings imply that while speech processing is a very complex process, the brain may be using fairly basic, preexisting computational methods.",
            "score": 167.08151245117188
        },
        {
            "docid": "2072616_22",
            "document": "Neuroprosthetics . Improved performance on cochlear implant not only depends on understanding the physical and biophysical limitations of implant stimulation but also on an understanding of the brain's pattern processing requirements. Modern signal processing represents the most important speech information while also providing the brain the pattern recognition information that it needs. Pattern recognition in the brain is more effective than algorithmic preprocessing at identifying important features in speech. A combination of engineering, signal processing, biophysics, and cognitive neuroscience was necessary to produce the right balance of technology to maximize the performance of auditory prosthesis.",
            "score": 167.0506134033203
        }
    ]
}