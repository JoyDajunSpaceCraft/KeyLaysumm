{
    "q": [
        {
            "docid": "24955215_11",
            "document": "E-Science librarianship . An example of librarians reconfiguring traditional librarian skills to meet the needs of researchers engaging in e-Science is Witt & Carlson\u2019s adaptation of the traditional reference interview into a \u201cdata interview\u201d in order to provide effective data management and e-Science services. This interview consists of ten practical queries necessary for understanding the provenance and expectations for the preservation of datasets typical of e-Science that also help illustrate some of the educational tools and skills needed by a librarian new to e-Science. \"What is the story of the data? What form and format are the data in? What is the expected lifespan of the dataset? How could the data be used, reused, and repurposed? How large is the dataset, and what is its rate of growth? Who are the potential audiences for the data? Who owns the data? Does the dataset include any sensitive information? What publications or discoveries have resulted from the data? How should the data be made accessible?\"",
            "score": 154.5179327726364
        },
        {
            "docid": "31500839_4",
            "document": "Dryad (repository) . Dryad aims to allow researchers to validate published findings, explore new analysis methodologies, re-purpose data for research questions unanticipated by the original authors, and perform synthetic studies such as formal meta-analyses. For many publications, existing data repositories do not capture the whole data package. As a result, many important datasets are not being preserved and are no longer available, or usable, at the time that they are sought by later investigators.",
            "score": 150.88358855247498
        },
        {
            "docid": "54179726_27",
            "document": "National Pupil Database . Public interest research use of pupil level data through other routes of access to the data, include projects linking individual data together with other education and employment data from citizens' interactions with other government departments and public services. For example, the LEO dataset is made up of information from the National Pupil Database (NPD), the Individualised Learner Record (ILR), the Higher Education Statistics Agency (HESA), Her Majesty\u2019s Revenue and Customs data (HMRC), The National Benefit Database, the Labour Market System and Juvos, the unemployment research database. Further work by DfE compares self-reported salaries from the 2008/09 DLHE survey with earnings data from the LEO dataset coming directly from HMRC tax records.",
            "score": 134.6382234096527
        },
        {
            "docid": "13002240_2",
            "document": "DataAdapter . In ADO.NET, a DataAdapter functions as a bridge between a data source, and a disconnected data class, such as a DataSet. At the simplest level it will specify SQL commands that provide elementary CRUD functionality. At a more advanced level it offers all the functions required in order to create Strongly Typed DataSets, including DataRelations. Data adapters are an integral part of ADO.NET managed providers, which are the set of objects used to communicate between a data source and a dataset. (In addition to adapters, managed providers include connection objects, data reader objects, and command objects.) Adapters are used to exchange data between a data source and a dataset. In many applications, this means reading data from a database into a dataset, and then writing changed data from the dataset back to the database. However, a data adapter can move data between any source and a dataset. For example, there could be an adapter that moves data between a Microsoft Exchange server and a dataset.",
            "score": 144.8041468858719
        },
        {
            "docid": "41722507_6",
            "document": "Enhanced publication . Scientific communities, organizations, and funding agencies supports initiatives, standards and best practices for publishing and citing datasets and publications on the web. Examples are DataCite, EPIC and CrossRef, which establish common best practices to assign metadata information and persistent identifiers to datasets and publications. Data publishing and citation practices are advocated by research communities that believe that datasets should be discoverable and reusable in order to improve the scientific activity. On this respect, several enhanced publication information systems were built to offer the possibility to enrich a publication with links to relevant research data, possibly deposited in data repositories or discipline specific databases. The existence of links between literature and data support the discovery of used and produced scientific data, strengthen data citation, facilitate data re-use, and reward the precious work underlying data management procedures.",
            "score": 149.0756814479828
        },
        {
            "docid": "34017095_6",
            "document": "Privacy for research participants . A privacy attack is the exploitation of an opportunity for someone to identify a study participant based on public research data. The way that this might work is that researchers collect data, including confidential identifying data, from study participants. This produces an identified dataset. Before the data is sent for research processing, it is \"de-identified\", which means that personally identifying data is removed from the dataset. Ideally, this means that the dataset alone could not be used to identify a participant.",
            "score": 174.92238688468933
        },
        {
            "docid": "34017095_7",
            "document": "Privacy for research participants . In some cases, the researchers simply misjudge the information in a de-identified dataset and actually it is identifying, or perhaps the advent of new technology makes the data identifying. In other cases, the published de-identified data can be cross-referenced with other data sets, and by finding matches between an identified dataset and the de-identified data set, participants in the de-identified set may be revealed.",
            "score": 158.80539345741272
        },
        {
            "docid": "25270778_10",
            "document": "Synthetic data . Synthetic data can be generated through the use of random lines, having different orientations and starting positions. Datasets can be get fairly complicated. A more complicated dataset can be generated by using a synthesizer build. To create a synthesizer build, first use the original data to create a model or equation that fits the data the best. This model or equation will be called a synthesizer build. This build can be used to generate more data.",
            "score": 161.54921674728394
        },
        {
            "docid": "47086977_10",
            "document": "Salvatore J. Stolfo . The DARPA IDS evaluation datasets were constructed by Lincoln Labs in 1998 and 1999 for the DARPA Cyber Panel program. These network trace data sets were used to evaluate the performance of different intrusion detection systems; they were the only network trace data with ground truth available to the open research community. The data, however, were difficult to use directly by a wider community of data mining researchers. Stolfo and his associates in the IDS lab including Wenke Lee created the KDD Cup dataset derived from the DARPA IDS datasets. The DARPA network trace data were converted to \"connection records\" making the data more suitable for data mining researchers to test various machine learning algorithms. This data created as a community service is extensively used in IDS research, even today.",
            "score": 148.98265385627747
        },
        {
            "docid": "2545679_6",
            "document": "National Archive of Computerized Data on Aging . NACDA was one of the first organizations to develop and release studies on CD-ROM. NACDA was also one of the first archives to experiment with the idea of offering electronic research data as a public good, free to all interested individuals at no charge. The initial collection of 28 public use datasets first offered on the internet in 1992 has now expanded to over 1,600 datasets that are freely available to any researcher. The entire collection is stored online at the NACDA website, offering immediate access to gerontological researchers.",
            "score": 131.51839900016785
        },
        {
            "docid": "38972404_12",
            "document": "CARTaGENE biobank . Researchers can request access to the CARTaGENE data through the Canadian Partnership for Tomorrow Project Portal. Researchers must submit an application and undergo evaluation by an independent Sample and Data Access Committee (SDAC). eThe dataset is available to researchers in industry and academic institutions, nationally and internationally. Applications detailing their project proposal are a requirement for review by an independent committee, the Sample and Data Access Committee (SDAC). The scientific management of CARTaGENE along with the SDAC determines if data or results should need to be returned to the project. Submission for access to the dataset is done directly online.",
            "score": 131.62664127349854
        },
        {
            "docid": "22101888_7",
            "document": "Oversampling and undersampling in data analysis . There are a number of methods available to oversample a dataset used in a typical classification problem (using a classification algorithm to classify a set of images, given a labelled training set of images). The most common technique is known as SMOTE: Synthetic Minority Over-sampling Technique. To illustrate how this technique works consider some training data which has \"s\" samples, and \"f\" features in the feature space of the data. Note that these features, for simplicity, are continuous. As an example, consider a dataset of birds for clarification. The feature space for the minority class for which we want to oversample could be beak length, wingspan, and weight (all continuous). To then oversample, take a sample from the dataset, and consider its \"k\" nearest neighbors (in feature space). To create a synthetic data point, take the vector between one of those \"k\" neighbors, and the current data point. Multiply this vector by a random number \"x\" which lies between 0, and 1. Add this to the current data point to create the new, synthetic data point.",
            "score": 156.90726673603058
        },
        {
            "docid": "49082762_2",
            "document": "List of datasets for machine learning research . These datasets are used for machine-learning research and have been cited in peer-reviewed academic journals. Datasets are an integral part of the field of machine learning. Major advances in this field can result from advances in learning algorithms (such as deep learning), computer hardware, and, less-intuitively, the availability of high-quality training datasets. High-quality labeled training datasets for supervised and semi-supervised machine learning algorithms are usually difficult and expensive to produce because of the large amount of time needed to label the data. Although they do not need to be labeled, high-quality datasets for unsupervised learning can also be difficult and costly to produce.",
            "score": 139.21522760391235
        },
        {
            "docid": "46228653_7",
            "document": "DisGeNET . The information contained in DisGeNET can also be expanded and complemented using Semantic Web technologies and linked to a variety of resources already present in the Linked Open Data cloud. DisGeNET is distributed as RDF and Nanopublications linked datasets. The DisGeNET-RDF linked dataset is an alternative way to access the DisGeNET data and provides new opportunities for data integration, querying and integrating DisGeNET data to other external RDF datasets. The RDF and Nanopublication distributions of DisGeNET have been developed in the context of the Open PHACTS project to provide disease relevant information to the knowledge base on pharmacological data.",
            "score": 123.82886075973511
        },
        {
            "docid": "42813835_4",
            "document": "Data publishing . There are several distinct ways to make research data available, including:  Publishing data allows researchers to both make their data available to others to use, and enables datasets to be cited similarly to other research publication types (such as articles or books), thereby enabling producers of datasets to gain academic credit for their work.",
            "score": 143.36926674842834
        },
        {
            "docid": "36747956_4",
            "document": "National Addiction and HIV Data Archive Program . NAHDAP is a project of the National Institute on Drug Abuse, part of the National Institutes of Health. The NAHDAP Web site launched in June 2010 and offers a library of electronic data and citations to publications based on those data. NAHDAP provides technical assistance and resources to investigators to facilitate preparing their data for archiving so that they can be easily and effectively archived at project completion. NAHDAP also provides technical assistance to users of the data sets, including workshops and webinars, to train analysts in the unique characteristics of selected datasets, in quantitative methods, and in the utility of analyzing secondary data for the reproduction of original research findings and addressing new research questions on major issues of social and behavioral sciences and public policy.",
            "score": 100.15115189552307
        },
        {
            "docid": "11851994_4",
            "document": "Real Estate Transaction Standard . RETS was originally created to overcome the difficulties presented by the existence of a large number of organizations desiring to share and distribute real estate information with others. Prior to RETS, much of the data exchange was done using the FTP protocol, which did not allow for queries, and required transfer of complete datasets. The inefficiencies of this approach meant that to generate a query such as \"new listings since yesterday\", the entire dataset had to be downloaded again and compared with a local copy. Rather than basing a solution on alternatives used by other industries to allow for such queries, RETS was created from the ground up as a new framework to attempt to address the need for a common and efficient standard for the exchange of real estate data. Most North American multiple listing service (MLS) data exchange service providers use the RETS protocol. Although the implementation of the protocol has offered some standardization, the field names of the underlying datasets still vary widely between markets.",
            "score": 134.89833307266235
        },
        {
            "docid": "42813835_8",
            "document": "Data publishing . Data papers are \u201cscholarly publication of a searchable metadata document describing a particular on-line accessible dataset, or a group of datasets, published in accordance to the standard academic practices\u201d. Their final aim being to provide \u201cinformation on the what, where, why, how and who of the data\u201d. The intent of a data paper is to offer descriptive information on the related dataset(s) focusing on data collection, distinguishing features, access and potential reuse rather than on data processing and analysis. Because data papers are considered academic publications no different than other types of papers they allow scientists sharing data to receive credit in currency recognizable within the academic system, thus \"making data sharing count\". This provides not only an additional incentive to share data, but also through the peer review process, increases the quality of metadata and thus reusability of the shared data.",
            "score": 157.69047617912292
        },
        {
            "docid": "13725847_8",
            "document": "Community Innovation Survey . The resulting micro-datasets can be accessed by researchers via the SAFE Center at the premises of Eurostat in Luxembourg or the anonymised micro-data via CD Rom's; some countries provides also access to their micro-data on similar safe centers. Eurostat also provides access to the EU-wide dataset for selected countries. Some non-EU countries perform very similar surveys according to the same methodology. These include Canada, Australia, New Zealand and South Africa.",
            "score": 117.68296599388123
        },
        {
            "docid": "37060172_3",
            "document": "SeaDataNet . This project aims to provide a web service permitting to retrieve validated datasets (temperature, oxygen, salinity, nutrients, etc.) from 45 different National Data Centers of 35 countries having coasts along European seas. Therefore SeaDataNet is a standardized system for managing the large and diverse data sets collected by the oceanographic fleets and the automatic observation systems. Additional objectives consist in creating product with aggregated data such as climatological descriptions. This European funded project has started in 2004, the project is currently in its second phase with fundings for 2012 to 2016. Most of the datasets are free of access, but some are restricted to institutes.  In term of harmonization SeaDataNet has chosen standards, vocabularies, tools that are used in the different NODC(National Oceanographic Data Center). For example they use Ocean Data View to validate or visualize datasets, they also use DIVA software to perform objective analysis. Datasets are covering the years 1800 up to 2012. In 2012 400 data originators are registered into Seadatanet project. Users of SeaDataNet who want to retrieve datasets coming from multiple Data Centers log to the Common Data Index web-service to define their request. They can provide many details such as the type of platform wanted, the parameter wanted, the rate of sampling, the position, the originator country, etc. Then users send their request, the request is analysed and split into as much request as there are data centers concerned. At the end the user receive an email giving a FTP address where to retrieve all the data ordered in the file format wanted (ASCII, NetCDF or Ocean Data View format).",
            "score": 131.4257390499115
        },
        {
            "docid": "41708438_6",
            "document": "The FracTracker Alliance . FracMapper is the mapping component of FracTracker.org. It offers a variety of maps detailing drilling-related activity. FracMapper's data comes from a variety of sources including state environmental agencies, news reports, freedom of information requests, user reports, collaborations with other groups, and information from other agencies. FracMapper makes its data available for download, and makes clear where the data came from - it includes a variety of metadata along with its data, including information about who created the original content, what is included in the dataset, when the dataset was taken, where the data features were located, and information about any changes from the original dataset. FracTracker.org also offers regular in-person trainings about how to use their mapping tools.",
            "score": 148.93449759483337
        },
        {
            "docid": "43686632_7",
            "document": "Australian Longitudinal Study on Women's Health . ALSWH has approval to anonymously link survey data with a number of national and state-based administrative datasets. Currently these approvals include the National Death Index, Medicare data, state-based cancer registries, perinatal data collections and hospital admission datasets. The aim of linking to these datasets is to provide new insights into the health and health service use of Australian women, taking into account individual level differences in socioeconomic, behavioural, environmental and other risk factors. In the context of a changing health system, it is important to be able to monitor and evaluate the impact and outcomes of health service policy and practice.",
            "score": 117.71296668052673
        },
        {
            "docid": "35951900_22",
            "document": "Data grid . The need for data grids was first recognized by the scientific community concerning climate modeling, where terabyte and petabyte sized data sets were becoming the norm for transport between sites. More recent research requirements for data grids have been driven by the Large Hadron Collider (LHC) at CERN, the Laser Interferometer Gravitational Wave Observatory (LIGO), and the Sloan Digital Sky Survey (SDSS). These examples of scientific instruments produce large amounts of data that need to be accessible by large groups of geographically dispersed researchers. Other uses for data grids involve governments, hospitals, schools and businesses where efforts are taking place to improve services and reduce costs by providing access to dispersed and separate data systems through the use of data grids. From its earliest beginnings, the concept of a Data Grid to support the scientific community was thought of as a specialized extension of the \u201cgrid\u201d which itself was first envisioned as a way to link super computers into meta-computers. However, that was short lived and the grid evolved into meaning the ability to connect computers anywhere on the web to get access to any desired files and resources, similar to the way electricity is delivered over a grid by simply plugging in a device. The device gets electricity through its connection and the connection is not limited to a specific outlet. From this the data grid was proposed as an integrating architecture that would be capable of delivering resources for distributed computations. It would also be able to service numerous to thousands of queries at the same time while delivering gigabytes to terabytes of data for each query. The data grid would include its own management infrastructure capable of managing all aspects of the data grids performance and operation across multiple wide area networks while working within the existing framework known as the web.  The data grid has also been defined more recently in terms of usability; what must a data grid be able to do in order for it to be useful to the scientific community. Proponents of this theory arrived at several criteria. One, users should be able to search and discover applicable resources within the data grid from amongst its many datasets. Two, users should be able to locate datasets within the data grid that are most suitable for their requirement from amongst numerous replicas. Three, users should be able to transfer and move large datasets between points in a short amount of time. Four, the data grid should provide a means to manage multiple copies of datasets within the data grid. And finally, the data grid should provide security with user access controls within the data grid, i.e. which users are allowed to access which data.",
            "score": 122.27040767669678
        },
        {
            "docid": "416612_2",
            "document": "Cross-validation (statistics) . Cross-validation, sometimes called rotation estimation, or out-of-sample testing is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice. In a prediction problem, a model is usually given a dataset of \"known data\" on which training is run (\"training dataset\"), and a dataset of \"unknown data\" (or \"first seen\" data) against which the model is tested (called the validation dataset or \"testing set\"). The goal of cross-validation is to test the model\u2019s ability to predict new data that were not used in estimating it, in order to flag problems like overfitting and to give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem).",
            "score": 140.7482122182846
        },
        {
            "docid": "25968111_16",
            "document": "European Climate Assessment and Dataset . There is, however, no well-established testing procedure for daily climatological data as well as no accepted method to apply the homogeneity results. There is a clear need for additional research on techniques for homogenisation of daily data in order to create high quality, daily datasets for the assessment of extremes without abandoning entire series or tossing out real extremes. This is of particular importance for areas that have a low density of stations with long, daily data series.",
            "score": 128.54211258888245
        },
        {
            "docid": "52637542_2",
            "document": "Inter-university Consortium for Political and Social Research . ICPSR, the Inter-university Consortium for Political and Social Research, was established in 1962. An integral part of the infrastructure of social science research, ICPSR maintains and provides access to a vast archive of social science data for research and instruction (over 8,000 discrete studies/surveys with more than 65,000 datasets). Since 1963, ICPSR has offered training in quantitative methods to facilitate effective data use. The ICPSR Summer Program in Quantitative Methods of Social Research offers a comprehensive curriculum in research design, statistics, data analysis, and methodology. To ensure that data resources are available to future generations of scholars, ICPSR curates and preserves data, migrating them to new storage media and file formats as changes in technology warrant. In addition, ICPSR provides user support to assist researchers in identifying relevant data for analysis and in conducting their research projects.",
            "score": 117.04076623916626
        },
        {
            "docid": "43336209_17",
            "document": "Enigma (company) . Enigma provides access to its datasets through a web-based graphical user interface and an API. Tools are provided in the interface for performing basic statistical analysis, such as finding the minimum, maximum or mean value of any numerical data column. For further analysis, users may either use the interface to export data to a CSV file or make HTTP requests to the provided API. The company also produces interactive data visualizations which provide visual interfaces for particular individual datasets.",
            "score": 117.12443280220032
        },
        {
            "docid": "10221795_7",
            "document": "Integrative bioinformatics . In the data warehousing strategy, the data from different sources are extracted and integrated in a single database. For example, various 'omics' datasets may be integrated to provide biological insights into biological systems. Examples include data from genomics, transcriptomics, proteomics, interactomics, metabolomics. Ideally, changes in these sources are regularly synchronized to the integrated database. The data is presented to the users in a common format. Many programs aimed to aid in the creation of such warehouses are designed to be extremely versatile to allow for them to be implemented in diverse research projects. One advantage of this approach is that data is available for analysis at a single site, using a uniform schema. Some disadvantages are that the datasets are often huge and difficult to keep up to date. Another problem with this method is that it is costly to compile such a warehouse.",
            "score": 124.96531081199646
        },
        {
            "docid": "25270778_12",
            "document": "Synthetic data . David Jensen from the Knowledge Discovery Laboratory explains how to generate synthetic data: \"Researchers frequently need to explore the effects of certain data characteristics on their data model.\" To help construct datasets exhibiting specific properties, such as auto-correlation or degree disparity, proximity can generate synthetic data having one of several types of graph structure: random graphs that are generated by some random process; lattice graphs having a ring structure; lattice graphs having a grid structure, etc. In all cases, the data generation process follows the same process:",
            "score": 143.29773116111755
        },
        {
            "docid": "179088_8",
            "document": "SPSS . SPSS Statistics places constraints on internal file structure, data types, data processing, and matching files, which together considerably simplify programming. SPSS datasets have a two-dimensional table structure, where the rows typically represent cases (such as individuals or households) and the columns represent measurements (such as age, sex, or household income). Only two data types are defined: numeric and text (or \"string\"). All data processing occurs sequentially case-by-case through the file (dataset). Files can be matched one-to-one and one-to-many, but not many-to-many. In addition to that cases-by-variables structure and processing, there is a separate Matrix session where one can process data as matrices using matrix and linear algebra operations.",
            "score": 110.22860908508301
        },
        {
            "docid": "18408210_11",
            "document": "SDTM . The dataset structure for observations is a flat file representing a table with one or more rows and columns. Normally, one dataset is submitted for each domain. Each row of the dataset represents a single observation and each column represents one of the variables. Each dataset or table is accompanied by metadata definitions that provide information about the variables used in the dataset. The metadata are described in a data definition document named 'Define' that is submitted along with the data to regulatory authorities.",
            "score": 126.06385707855225
        },
        {
            "docid": "31757763_10",
            "document": "Development Gateway . AidData maintains a searchable database of development assistance at www.aidddata.org that include data from 95 donor agencies from 1945 to the present. In addition, the AidData website provides a collection of related resources, including unvetted \"raw\" datasets on development assistance and other datasets on international development work. The partner institutions behind AidData engaged in research projects on data collection and augmentation through crowdsourcing and other methods as well as more traditional research on aid allocation and effectiveness.",
            "score": 121.96263813972473
        }
    ],
    "r": [
        {
            "docid": "34017095_6",
            "document": "Privacy for research participants . A privacy attack is the exploitation of an opportunity for someone to identify a study participant based on public research data. The way that this might work is that researchers collect data, including confidential identifying data, from study participants. This produces an identified dataset. Before the data is sent for research processing, it is \"de-identified\", which means that personally identifying data is removed from the dataset. Ideally, this means that the dataset alone could not be used to identify a participant.",
            "score": 174.92237854003906
        },
        {
            "docid": "25270778_10",
            "document": "Synthetic data . Synthetic data can be generated through the use of random lines, having different orientations and starting positions. Datasets can be get fairly complicated. A more complicated dataset can be generated by using a synthesizer build. To create a synthesizer build, first use the original data to create a model or equation that fits the data the best. This model or equation will be called a synthesizer build. This build can be used to generate more data.",
            "score": 161.54922485351562
        },
        {
            "docid": "34017095_7",
            "document": "Privacy for research participants . In some cases, the researchers simply misjudge the information in a de-identified dataset and actually it is identifying, or perhaps the advent of new technology makes the data identifying. In other cases, the published de-identified data can be cross-referenced with other data sets, and by finding matches between an identified dataset and the de-identified data set, participants in the de-identified set may be revealed.",
            "score": 158.80538940429688
        },
        {
            "docid": "42813835_8",
            "document": "Data publishing . Data papers are \u201cscholarly publication of a searchable metadata document describing a particular on-line accessible dataset, or a group of datasets, published in accordance to the standard academic practices\u201d. Their final aim being to provide \u201cinformation on the what, where, why, how and who of the data\u201d. The intent of a data paper is to offer descriptive information on the related dataset(s) focusing on data collection, distinguishing features, access and potential reuse rather than on data processing and analysis. Because data papers are considered academic publications no different than other types of papers they allow scientists sharing data to receive credit in currency recognizable within the academic system, thus \"making data sharing count\". This provides not only an additional incentive to share data, but also through the peer review process, increases the quality of metadata and thus reusability of the shared data.",
            "score": 157.6904754638672
        },
        {
            "docid": "22101888_7",
            "document": "Oversampling and undersampling in data analysis . There are a number of methods available to oversample a dataset used in a typical classification problem (using a classification algorithm to classify a set of images, given a labelled training set of images). The most common technique is known as SMOTE: Synthetic Minority Over-sampling Technique. To illustrate how this technique works consider some training data which has \"s\" samples, and \"f\" features in the feature space of the data. Note that these features, for simplicity, are continuous. As an example, consider a dataset of birds for clarification. The feature space for the minority class for which we want to oversample could be beak length, wingspan, and weight (all continuous). To then oversample, take a sample from the dataset, and consider its \"k\" nearest neighbors (in feature space). To create a synthetic data point, take the vector between one of those \"k\" neighbors, and the current data point. Multiply this vector by a random number \"x\" which lies between 0, and 1. Add this to the current data point to create the new, synthetic data point.",
            "score": 156.9072723388672
        },
        {
            "docid": "8838284_29",
            "document": "Hockey stick controversy . In considering the 1998 Jones et al. reconstruction which went back a thousand years, Mann, Bradley and Hughes reviewed their own research and reexamined 24 proxy records which extended back before 1400. Mann carried out a series of statistical sensitivity tests, removing each proxy in turn to see the effect its removal had on the result. He found that certain proxies were critical to the reliability of the reconstruction, particularly one tree ring dataset collected by Gordon Jacoby and Rosanne D'Arrigo in a part of North America that Bradley's earlier research had identified as a key region. This dataset only extended back to 1400, and though another proxy dataset from the same region (in the International Tree-Ring Data Bank) went further back and should have given reliable proxies for earlier periods, validation tests only supported their reconstruction after 1400. To find out why, Mann compared the two datasets and found that they tracked each other closely from 1400 to 1800, then diverged until around 1900 when they again tracked each other. He found a likely reason in the \"fertilisation effect\" affecting tree rings as identified by Graybill and Idso, with the effect ending once levels had increased to the point where warmth again became the key factor controlling tree growth at high altitude. Mann used comparisons with other tree ring data from the region to produce a corrected version of this dataset. Their reconstruction using this corrected dataset passed the validation tests for the extended period, but they were cautious about the increased uncertainties.",
            "score": 154.53790283203125
        },
        {
            "docid": "5354105_32",
            "document": "Hockey stick graph . In considering the 1998 Jones et al. reconstruction which went back a thousand years, Mann, Bradley and Hughes reviewed their own research and reexamined 24 proxy records which extended back before 1400. Mann carried out a series of statistical sensitivity tests, removing each proxy in turn to see the effect its removal had on the result. He found that certain proxies were critical to the reliability of the reconstruction, particularly one tree ring dataset collected by Gordon Jacoby and Rosanne D'Arrigo in a part of North America Bradley's earlier research had identified as a key region. This dataset only extended back to 1400, and though another proxy dataset from the same region (in the International Tree-Ring Data Bank) went further back and should have given reliable proxies for earlier periods, validation tests only supported their reconstruction after 1400. To find out why, Mann compared the two datasets and found that they tracked each other closely from 1400 to 1800, then diverged until around 1900 when they again tracked each other. He found a likely reason in the \"fertilisation effect\" affecting tree rings as identified by Graybill and Idso, with the effect ending once levels had increased to the point where warmth again became the key factor controlling tree growth at high altitude. Mann used comparisons with other tree ring data from the region to produce a corrected version of this dataset. Their reconstruction using this corrected dataset passed the validation tests for the extended period, but they were cautious about the increased uncertainties.",
            "score": 154.53790283203125
        },
        {
            "docid": "24955215_11",
            "document": "E-Science librarianship . An example of librarians reconfiguring traditional librarian skills to meet the needs of researchers engaging in e-Science is Witt & Carlson\u2019s adaptation of the traditional reference interview into a \u201cdata interview\u201d in order to provide effective data management and e-Science services. This interview consists of ten practical queries necessary for understanding the provenance and expectations for the preservation of datasets typical of e-Science that also help illustrate some of the educational tools and skills needed by a librarian new to e-Science. \"What is the story of the data? What form and format are the data in? What is the expected lifespan of the dataset? How could the data be used, reused, and repurposed? How large is the dataset, and what is its rate of growth? Who are the potential audiences for the data? Who owns the data? Does the dataset include any sensitive information? What publications or discoveries have resulted from the data? How should the data be made accessible?\"",
            "score": 154.51792907714844
        },
        {
            "docid": "31500839_4",
            "document": "Dryad (repository) . Dryad aims to allow researchers to validate published findings, explore new analysis methodologies, re-purpose data for research questions unanticipated by the original authors, and perform synthetic studies such as formal meta-analyses. For many publications, existing data repositories do not capture the whole data package. As a result, many important datasets are not being preserved and are no longer available, or usable, at the time that they are sought by later investigators.",
            "score": 150.8835906982422
        },
        {
            "docid": "41722507_6",
            "document": "Enhanced publication . Scientific communities, organizations, and funding agencies supports initiatives, standards and best practices for publishing and citing datasets and publications on the web. Examples are DataCite, EPIC and CrossRef, which establish common best practices to assign metadata information and persistent identifiers to datasets and publications. Data publishing and citation practices are advocated by research communities that believe that datasets should be discoverable and reusable in order to improve the scientific activity. On this respect, several enhanced publication information systems were built to offer the possibility to enrich a publication with links to relevant research data, possibly deposited in data repositories or discipline specific databases. The existence of links between literature and data support the discovery of used and produced scientific data, strengthen data citation, facilitate data re-use, and reward the precious work underlying data management procedures.",
            "score": 149.07568359375
        },
        {
            "docid": "47086977_10",
            "document": "Salvatore J. Stolfo . The DARPA IDS evaluation datasets were constructed by Lincoln Labs in 1998 and 1999 for the DARPA Cyber Panel program. These network trace data sets were used to evaluate the performance of different intrusion detection systems; they were the only network trace data with ground truth available to the open research community. The data, however, were difficult to use directly by a wider community of data mining researchers. Stolfo and his associates in the IDS lab including Wenke Lee created the KDD Cup dataset derived from the DARPA IDS datasets. The DARPA network trace data were converted to \"connection records\" making the data more suitable for data mining researchers to test various machine learning algorithms. This data created as a community service is extensively used in IDS research, even today.",
            "score": 148.98265075683594
        },
        {
            "docid": "41708438_6",
            "document": "The FracTracker Alliance . FracMapper is the mapping component of FracTracker.org. It offers a variety of maps detailing drilling-related activity. FracMapper's data comes from a variety of sources including state environmental agencies, news reports, freedom of information requests, user reports, collaborations with other groups, and information from other agencies. FracMapper makes its data available for download, and makes clear where the data came from - it includes a variety of metadata along with its data, including information about who created the original content, what is included in the dataset, when the dataset was taken, where the data features were located, and information about any changes from the original dataset. FracTracker.org also offers regular in-person trainings about how to use their mapping tools.",
            "score": 148.9344940185547
        },
        {
            "docid": "22999896_21",
            "document": "Exponential mechanism (differential privacy) . Informally, it means that with high probability the query formula_90 will behave in a similar way on the original dataset formula_4 and on the synthetic dataset formula_92. <br> Let us consider a common problem in Data Mining. Assume there is a database formula_4 with formula_1 entries. Each entry consist of formula_95-tuples of the form formula_96 where formula_97. Now, a user wants to learn a linear halfspace of the form formula_98. In essence the user wants to figure out the values of formula_99 such that maximum number of tuples in the database satisfy the inequality. The algorithm we describe below can generate a synthetic database formula_92 which will allow the user to learn (approximately) the same linear half-space while querying on this synthetic database. The motivation for such an algorithm being that the new database will be generated in a differentially private manner and thus assure privacy to the individual records in the database formula_4.",
            "score": 148.60934448242188
        },
        {
            "docid": "2720954_50",
            "document": "Data analysis . Exploratory data analysis should be interpreted carefully. When testing multiple models at once there is a high chance on finding at least one of them to be significant, but this can be due to a type 1 error. It is important to always adjust the significance level when testing multiple models with, for example, a Bonferroni correction. Also, one should not follow up an exploratory analysis with a confirmatory analysis in the same dataset. An exploratory analysis is used to find ideas for a theory, but not to test that theory as well. When a model is found exploratory in a dataset, then following up that analysis with a confirmatory analysis in the same dataset could simply mean that the results of the confirmatory analysis are due to the same type 1 error that resulted in the exploratory model in the first place. The confirmatory analysis therefore will not be more informative than the original exploratory analysis.",
            "score": 148.3222198486328
        },
        {
            "docid": "1164_38",
            "document": "Artificial intelligence . Much of GOFAI got bogged down on \"ad hoc\" patches to symbolic computation that worked on their own toy models but failed to generalize to real-world results. However, around the 1990s, AI researchers adopted sophisticated mathematical tools, such as hidden Markov models (HMM), information theory, and normative Bayesian decision theory to compare or to unify competing architectures. The shared mathematical language permitted a high level of collaboration with more established fields (like mathematics, economics or operations research). Compared with GOFAI, new \"statistical learning\" techniques such as HMM and neural networks were gaining higher levels of accuracy in many practical domains such as data mining, without necessarily acquiring semantic understanding of the datasets. The increased successes with real-world data led to increasing emphasis on comparing different approaches against shared test data to see which approach performed best in a broader context than that provided by idiosyncratic toy models; AI research was becoming more scientific. Nowadays results of experiments are often rigorously measurable, and are sometimes (with difficulty) reproducible. Different statistical learning techniques have different limitations; for example, basic HMM cannot model the infinite possible combinations of natural language. Critics note that the shift from GOFAI to statistical learning is often also a shift away from Explainable AI. In AGI research, some scholars caution against over-reliance on statistical learning, and argue that continuing research into GOFAI will still be necessary to attain general intelligence.",
            "score": 148.2840118408203
        },
        {
            "docid": "50968824_2",
            "document": "Model organism databases . Model organism databases (MODs) are biological databases, or knowledgebases, dedicated to the provision of in-depth biological data for intensively studied model organisms. MODs allow researchers to easily find background information on large sets of genes, plan experiments efficiently, combine their data with existing knowledge, and construct novel hypotheses. They allow users to analyse results and interpret datasets, and the data they generate are increasingly used to describe less well studied species. Where possible, MODs share common approaches to collect and represent biological information. For example, all MODs use the Gene Ontology to describe functions, processes and cellular locations of specific gene products. Projects also exist to enable software sharing for curation, visualization and querying between different MODs. Organismal diversity and varying user requirements however mean that MODs are often required to customize capture, display, and provision of data.",
            "score": 146.65916442871094
        },
        {
            "docid": "11865154_11",
            "document": "DBpedia . DBpedia has a broad scope of entities covering different areas of human knowledge. This makes it a natural hub for connecting datasets, where external datasets could link to its concepts. The DBpedia dataset is interlinked on the RDF level with various other Open Data datasets on the Web. This enables applications to enrich DBpedia data with data from these datasets. , there are more than 45 million interlinks between DBpedia and external datasets including: Freebase, OpenCyc, UMBEL, GeoNames, MusicBrainz, CIA World Fact Book, DBLP, Project Gutenberg, DBtune Jamendo, Eurostat, UniProt, Bio2RDF, and US Census data. The Thomson Reuters initiative OpenCalais, the Linked Open Data project of the \"New York Times\", the Zemanta API and DBpedia Spotlight also include links to DBpedia. The BBC uses DBpedia to help organize its content. Faviki uses DBpedia for semantic tagging. Samsung also includes DBpedia in its \"Knowledge Sharing Platform\".",
            "score": 145.7910919189453
        },
        {
            "docid": "11091040_3",
            "document": "Protected health information . PHI is often sought out in datasets for de-identification before researchers share the dataset publicly. Researchers remove PHI from a dataset to preserve privacy for research participants.",
            "score": 145.66065979003906
        },
        {
            "docid": "13002240_2",
            "document": "DataAdapter . In ADO.NET, a DataAdapter functions as a bridge between a data source, and a disconnected data class, such as a DataSet. At the simplest level it will specify SQL commands that provide elementary CRUD functionality. At a more advanced level it offers all the functions required in order to create Strongly Typed DataSets, including DataRelations. Data adapters are an integral part of ADO.NET managed providers, which are the set of objects used to communicate between a data source and a dataset. (In addition to adapters, managed providers include connection objects, data reader objects, and command objects.) Adapters are used to exchange data between a data source and a dataset. In many applications, this means reading data from a database into a dataset, and then writing changed data from the dataset back to the database. However, a data adapter can move data between any source and a dataset. For example, there could be an adapter that moves data between a Microsoft Exchange server and a dataset.",
            "score": 144.80413818359375
        },
        {
            "docid": "42813835_4",
            "document": "Data publishing . There are several distinct ways to make research data available, including:  Publishing data allows researchers to both make their data available to others to use, and enables datasets to be cited similarly to other research publication types (such as articles or books), thereby enabling producers of datasets to gain academic credit for their work.",
            "score": 143.3692626953125
        },
        {
            "docid": "25270778_12",
            "document": "Synthetic data . David Jensen from the Knowledge Discovery Laboratory explains how to generate synthetic data: \"Researchers frequently need to explore the effects of certain data characteristics on their data model.\" To help construct datasets exhibiting specific properties, such as auto-correlation or degree disparity, proximity can generate synthetic data having one of several types of graph structure: random graphs that are generated by some random process; lattice graphs having a ring structure; lattice graphs having a grid structure, etc. In all cases, the data generation process follows the same process:",
            "score": 143.2977294921875
        },
        {
            "docid": "56807042_7",
            "document": "EMRBots . \"[EMRBots] are ... pregenerated datasets of synthetic EHR with an insufficient explanation of how the datasets were generated. These datasets exhibit several inconsistencies between health problems, age, and gender.\" An additional criticism is described in a thesis (\"Realism in Synthetic Data Generation\") granted by Massey University.",
            "score": 142.9542999267578
        },
        {
            "docid": "596646_54",
            "document": "Recommender system . To measure the effectiveness of recommender systems, and compare different approaches, three types of evaluations are available: user studies, online evaluations (A/B tests), and offline evaluations. User studies are rather small scale. A few dozens or hundreds of users are presented recommendations created by different recommendation approaches, and then the users judge, which recommendations are best. In A/B tests, recommendations are shown to typically thousands of users of a real product, and the recommender system randomly picks at least two different recommendation approaches to generate recommendations. The effectiveness is measured with implicit measures of effectiveness such as conversion rate or click-through rate. Offline evaluations are based on historic data, e.g. a dataset that contains information about how users previously rated movies. The effectiveness of recommendation approaches is then measured based on how well a recommendation approach can predict the users' ratings in the dataset. While a rating is an explicit expression of whether a user liked a movie, such information is not available in all domains. For instance, in the domain of citation recommender systems, users typically do not rate a citation or recommended article. In such cases, offline evaluations may use implicit measures of effectiveness. For instance, it may be assumed that a recommender system is effective that is able to recommend as many articles as possible that are contained in a research article's reference list. However, this kind of offline evaluations is seen critical by many researchers. For instance, it has been shown that results of offline evaluations have low correlation with results from user studies or A/B tests. A dataset popular for offline evaluation has been shown to contain duplicate data and thus to lead to wrong conclusions in the evaluation of algorithms.",
            "score": 141.79539489746094
        },
        {
            "docid": "34017095_8",
            "document": "Privacy for research participants . The ideal situation from the research perspective is the free sharing of data. Since privacy for research participants is a priority, though, various proposals for protecting participants have been made for different purposes. Replacing the real data with synthetic data allows the researchers to show data which gives a conclusion equivalent to the one drawn by the researchers, but the data may have problems such as being unfit for repurposing for other research. Other strategies include \"noise addition\" by making random value changes or \"data swapping\" by exchanging values across entries. Still another approach is to separate the identifiable variables in the data from the rest, aggregate the identifiable variables and reattach them with the rest of the data. This principle has been used successfully in creating maps of diabetes in Australia and the United Kingdom using confidential General Practice clinic data.",
            "score": 140.79603576660156
        },
        {
            "docid": "416612_2",
            "document": "Cross-validation (statistics) . Cross-validation, sometimes called rotation estimation, or out-of-sample testing is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice. In a prediction problem, a model is usually given a dataset of \"known data\" on which training is run (\"training dataset\"), and a dataset of \"unknown data\" (or \"first seen\" data) against which the model is tested (called the validation dataset or \"testing set\"). The goal of cross-validation is to test the model\u2019s ability to predict new data that were not used in estimating it, in order to flag problems like overfitting and to give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem).",
            "score": 140.7482147216797
        },
        {
            "docid": "49082762_2",
            "document": "List of datasets for machine learning research . These datasets are used for machine-learning research and have been cited in peer-reviewed academic journals. Datasets are an integral part of the field of machine learning. Major advances in this field can result from advances in learning algorithms (such as deep learning), computer hardware, and, less-intuitively, the availability of high-quality training datasets. High-quality labeled training datasets for supervised and semi-supervised machine learning algorithms are usually difficult and expensive to produce because of the large amount of time needed to label the data. Although they do not need to be labeled, high-quality datasets for unsupervised learning can also be difficult and costly to produce.",
            "score": 139.21522521972656
        },
        {
            "docid": "42813835_15",
            "document": "Data publishing . Data citation is the provision of accurate, consistent and standardised referencing for datasets just as bibliographic citations are provided for other published sources like research articles or monographs. Typically the well established Digital Object Identifier (DOI) approach is used with DOIs taking users to a website that contains the metadata on the dataset and the dataset itself.",
            "score": 139.1416015625
        },
        {
            "docid": "25270778_14",
            "document": "Synthetic data . Synthetic data is increasingly being used for machine learning applications: a model is trained on a synthetically generated dataset with the intention of transfer learning to real data. Efforts have been made to construct general-purpose synthetic data generators to enable data science experiments. In general, synthetic data has several natural advantages: This usage of synthetic data has been proposed for computer vision applications, in particular object detection, where the synthetic environment is a 3D model of the object, and learning to navigate environments by visual information.",
            "score": 138.78311157226562
        },
        {
            "docid": "24240177_5",
            "document": "Deb Verhoeven . She is the director of the Humanities Networked Infrastructure (HuNI) project, a national linked data initiative intended to unite and unlock Australia's cultural datasets. The project is funded by NeCTAR (National eResearch Collaboration Tools and Resources). HuNI is a national Virtual Laboratory project developed as part of the Australian government\u2019s NeCTAR (National e-Research Collaboration Tools and Resources) program. HuNI combines information from 31 of Australia\u2019s most significant cultural datasets. These datasets comprise more than a million authoritative records relating to the people, organisations, objects and events that make up Australia's abundant cultural heritage. HuNI also enables researchers to work with and share this large-scale aggregation of cultural information. HuNI was developed as a partnership between 13 public institutions, led by Deakin University.",
            "score": 137.4617919921875
        },
        {
            "docid": "1876018_35",
            "document": "Geocoding . However, this process is not always as straightforward as in this example. Difficulties arise when While there might be 742 Evergreen Terrace in Springfield, there might also be a 742 Evergreen Terrace in Shelbyville. Asking for the city name (and state, province, country, etc. as needed) can solve this problem. Boston, Massachusetts has multiple \"100 Washington Street\" locations because several cities have been annexed without changing street names, thus requiring use of unique postal codes or district names for disambiguation. Geocoding accuracy can be greatly improved by first utilizing good address verification practices. Address verification will confirm the existence of the address and will eliminate ambiguities. Once the valid address is determined, it is very easy to geocode and determine the latitude/longitude coordinates. Finally, several caveats on using interpolation: A very common error is to believe the accuracy ratings of a given map's geocodable attributes. Such accuracy currently touted by most vendors has no bearing on an address being attributed to the correct segment, being attributed to the correct side of the segment, nor resulting in an accurate position along that correct segment. With the geocoding process used for U.S. Census TIGER datasets, 5-7.5% of the addresses may be allocated to a different census tract, while a study of Australia's TIGER-like system found that 50% of the geocoded points were mapped to the wrong property parcel. The accuracy of geocoded data can also have a bearing on the quality of research that can be done using this data. One study by a group of Iowa researchers found that the common method of geocoding using TIGER datasets as described above, can cause a loss of as much as 40% of the power of a statistical analysis. An alternative is to use orthophoto or image coded data such as the Address Point data from Ordnance Survey in the UK, but such datasets are generally expensive. Because of this, it is quite important to avoid using interpolated results except for non-critical applications. Interpolated geocoding is usually not appropriate for making authoritative decisions, for example if life safety will be affected by that decision. Emergency services, for example, do not make an authoritative decision based on their interpolations; an ambulance or fire truck will always be dispatched regardless of what the map says.",
            "score": 136.05093383789062
        },
        {
            "docid": "524392_10",
            "document": "Species diversity . Depending on the purposes of quantifying species diversity, the dataset used for the calculations can be obtained in different ways. Although species diversity can be calculated for any dataset where individuals have been identified to species, meaningful ecological interpretations require that the dataset is appropriate for the questions at hand. In practice, the interest is usually in the species diversity of areas so large that not all individuals in them can be observed and identified to species, but a sample of the relevant individuals has to be obtained. Extrapolation from the sample to the underlying population of interest is not straightforward, because the species diversity of the available sample generally gives an underestimation of the species diversity in the entire population. Applying different sampling methods will lead to different sets of individuals being observed for the same area of interest, and the species diversity of each set may be different. When a new individual is added to a dataset, it may introduce a species that was not yet represented. How much this increases species diversity depends on the value of \"q\": when \"q\" = 0, each new actual species causes species diversity to increase by one effective species, but when \"q\" is large, adding a rare species to a dataset has little effect on its species diversity.",
            "score": 134.97354125976562
        },
        {
            "docid": "11851994_4",
            "document": "Real Estate Transaction Standard . RETS was originally created to overcome the difficulties presented by the existence of a large number of organizations desiring to share and distribute real estate information with others. Prior to RETS, much of the data exchange was done using the FTP protocol, which did not allow for queries, and required transfer of complete datasets. The inefficiencies of this approach meant that to generate a query such as \"new listings since yesterday\", the entire dataset had to be downloaded again and compared with a local copy. Rather than basing a solution on alternatives used by other industries to allow for such queries, RETS was created from the ground up as a new framework to attempt to address the need for a common and efficient standard for the exchange of real estate data. Most North American multiple listing service (MLS) data exchange service providers use the RETS protocol. Although the implementation of the protocol has offered some standardization, the field names of the underlying datasets still vary widely between markets.",
            "score": 134.89833068847656
        }
    ]
}