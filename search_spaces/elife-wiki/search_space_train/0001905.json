{
    "q": [
        {
            "docid": "3474296_4",
            "document": "Neuronal noise . Single neurons demonstrate different responses to specific neuronal input signals. This is commonly referred to as neural response variability. If a specific input signal is initiated in the dendrites of a neuron, then a hypervariability exists in the number of vesicles released from the axon terminal fiber into the synapse. This characteristic is true for fibers without neural input signals, such as pacemaker neurons, as mentioned previously, and cortical pyramidal neurons that have highly-irregular firing pattern. Noise generally hinders neural performance, but recent studies show, in dynamical non-linear neural networks, this statement does not always hold true. Non-linear neural networks are a network of complex neurons that have many connections with one another such as the neuronal systems found within our brains. Comparatively, linear networks are an experimental view of analyzing a neural system by placing neurons in series with each other.",
            "score": 232.41540718078613
        },
        {
            "docid": "941909_26",
            "document": "Receptive field . The term receptive field is also used in the context of artificial neural networks, most often in relation to convolutional neural networks (CNNs). When used in this sense, the term adopts a meaning reminiscent of receptive fields in actual biological nervous systems. CNNs have a distinct architecture, designed to mimic the way in which real animal brains are understood to function; instead of having every neuron in each layer connect to all neurons in the next layer (Multilayer perceptron), the neurons are arranged in a 3-dimensional structure in such a way as to take into account the spatial relationships between different neurons with respect to the original data. Since CNNs are used primarily in the field of computer vision, the data that the neurons represent is typically an image; each input neuron represents one pixel from the original image. The first layer of neurons is composed of all the input neurons; neurons in the next layer will receive connections from some of the input neurons (pixels), but not all, as would be the case in a MLP and in other traditional neural networks. Hence, instead of having each neuron receive connections from all neurons in the previous layer, CNNs use a receptive field-like layout in which each neuron receives connections only from a subset of neurons in the previous (lower) layer. The receptive field of a neuron in one of the lower layers encompasses only a small area of the image, while the receptive field of a neuron in subsequent (higher) layers involves a combination of receptive fields from several (but not all) neurons in the layer before (i. e. a neuron in a higher layer \"looks\" at a larger portion of the image than does a neuron in a lower layer). In this way, each successive layer is capable of learning increasingly abstract features of the original image. The use of receptive fields in this fashion is thought to give CNNs an advantage in recognizing visual patterns when compared to other types of neural networks.",
            "score": 211.19841420650482
        },
        {
            "docid": "6107563_7",
            "document": "Pulse-coupled networks . A PCNN is a two-dimensional neural network. Each neuron in the network corresponds to one pixel in an input image, receiving its corresponding pixel\u2019s color information (e.g. intensity) as an external stimulus. Each neuron also connects with its neighboring neurons, receiving local stimuli from them. The external and local stimuli are combined in an internal activation system, which accumulates the stimuli until it exceeds a dynamic threshold, resulting in a pulse output. Through iterative computation, PCNN neurons produce temporal series of pulse outputs. The temporal series of pulse outputs contain information of input images and can be used for various image processing applications, such as image segmentation and feature generation. Compared with conventional image processing means, PCNNs have several significant merits, including robustness against noise, independence of geometric variations in input patterns, capability of bridging minor intensity variations in input patterns, etc.",
            "score": 208.62479436397552
        },
        {
            "docid": "505717_72",
            "document": "Image segmentation . Pulse-coupled neural networks (PCNNs) are neural models proposed by modeling a cat\u2019s visual cortex and developed for high-performance biomimetic image processing. In 1989, Eckhorn introduced a neural model to emulate the mechanism of a cat\u2019s visual cortex. The Eckhorn model provided a simple and effective tool for studying the visual cortex of small mammals, and was soon recognized as having significant application potential in image processing. In 1994, the Eckhorn model was adapted to be an image processing algorithm by Johnson, who termed this algorithm Pulse-Coupled Neural Network. Over the past decade, PCNNs have been utilized for a variety of image processing applications, including: image segmentation, feature generation, face extraction, motion detection, region growing, noise reduction, and so on. A PCNN is a two-dimensional neural network. Each neuron in the network corresponds to one pixel in an input image, receiving its corresponding pixel\u2019s color information (e.g. intensity) as an external stimulus. Each neuron also connects with its neighboring neurons, receiving local stimuli from them. The external and local stimuli are combined in an internal activation system, which accumulates the stimuli until it exceeds a dynamic threshold, resulting in a pulse output. Through iterative computation, PCNN neurons produce temporal series of pulse outputs. The temporal series of pulse outputs contain information of input images and can be utilized for various image processing applications, such as image segmentation and feature generation. Compared with conventional image processing means, PCNNs have several significant merits, including robustness against noise, independence of geometric variations in input patterns, capability of bridging minor intensity variations in input patterns, etc.",
            "score": 221.35592663288116
        },
        {
            "docid": "1287722_8",
            "document": "Barrel cortex . The whisker barrel cortex contains different types of neurons that receive input from a range of sources that themselves receive and process an array of different types of information. As a result, neurons of the whisker barrel cortex respond to whisker-related input, but in a way that is specific to the neurons type and location. This can manifest in different ways. The simplest way is whether the cortical neuron responds only to the deflection of one whisker, or to the deflection of many whiskers. Neurons in layer 4 barrels tend to strongly or exclusively respond to one whisker, while neurons in other layers are less strongly tuned and can respond to multiple whiskers. Neurons that respond to the deflection of multiple whiskers typically have a primary whisker, to which they respond the most. The difference in response magnitude between deflection of the primary whisker and secondary whiskers can also vary between neurons. Stimulation of multiple whiskers may produce a response that is equal to the sum of the responses if each whisker was stimulated independently, or it may be different. Some neurons show greater responses when multiple neurons are stimulated in sequence, and the sequence may be direction specific.",
            "score": 142.06582152843475
        },
        {
            "docid": "41121206_5",
            "document": "Phase resetting in neurons . Shifts in phase (or behavior of neurons) caused due to a perturbation (an external stimulus) can be quantified within a Phase Response Curve (PRC) to predict synchrony in coupled and oscillating neurons. These effects can be computed, in the case of advances or delays to responses, to observe the changes in the oscillatory behavior of neurons, pending on when a stimulus was applied in the phase cycle of an oscillating neuron. The key to understanding this is in the behavioral patterns of neurons and the routes neural information travels. Neural circuits are able to communicate efficiently and effectively within milliseconds of experiencing a stimulus and lead to the spread of information throughout the neural network. The study of neuron synchrony could provide information on the differences that occur in neural states such as normal and diseased states. Neurons that are involved significantly in diseases such as Alzheimers or Parkinsons diseases are shown to undergo phase resetting before launching into phase locking where clusters of neurons are able to begin firing rapidly to communicate information quickly. A phase response curve can be calculated by noting changes to its period over time depending on where in the cycle the input is applied. The perturbation left by the stimulus moves the stable cycle within the oscillation followed by a return to the stable cycle limit. The curve tracks the amount of advancement or delay due to the input in the oscillating neuron. The PRC assumes certain patterns of behavior in firing pattern as well as the network of oscillating neurons to model the oscillations. Currently, only a few circuits exist which can be modeled using an assumed firing pattern.",
            "score": 197.11972212791443
        },
        {
            "docid": "2918988_5",
            "document": "Neurotransmission . Neurons form elaborate networks through which nerve impulses (action potentials) travel. Each neuron has as many as 15,000 connections with other neurons. Neurons do not touch each other (except in the case of an electrical synapse through a gap junction); instead, neurons interact at close contact points called synapses. A neuron transports its information by way of an action potential. When the nerve impulse arrives at the synapse, it may cause the release of neurotransmitters, which influence another (postsynaptic) neuron. The postsynaptic neuron may receive inputs from many additional neurons, both excitatory and inhibitory. The excitatory and inhibitory influences are summed, and if the net effect is inhibitory, the neuron will be less likely to \"fire\" (i.e., generate an action potential), and if the net effect is excitatory, the neuron will be more likely to fire. How likely a neuron is to fire depends on how far its membrane potential is from the threshold potential, the voltage at which an action potential is triggered because enough voltage-dependent sodium channels are activated so that the net inward sodium current exceeds all outward currents. Excitatory inputs bring a neuron closer to threshold, while inhibitory inputs bring the neuron farther from threshold. An action potential is an \"all-or-none\" event; neurons whose membranes have not reached threshold will not fire, while those that do must fire. Once the action potential is initiated (traditionally at the axon hillock), it will propagate along the axon, leading to release of neurotransmitters at the synaptic bouton to pass along information to yet another adjacent neuron.",
            "score": 160.87249648571014
        },
        {
            "docid": "349771_2",
            "document": "Artificial neuron . An artificial neuron is a mathematical function conceived as a model of biological neurons, a neural network. Artificial neurons are elementary units in an artificial neural network. The artificial neuron receives one or more inputs (representing excitatory postsynaptic potentials and inhibitory postsynaptic potentials at neural dendrites) and sums them to produce an output (or , representing a neuron's action potential which is transmitted along its axon). Usually each input is separately weighted, and the sum is passed through a non-linear function known as an activation function or transfer function. The transfer functions usually have a sigmoid shape, but they may also take the form of other non-linear functions, piecewise linear functions, or step functions. They are also often monotonically increasing, continuous, differentiable and bounded. The thresholding function has inspired building logic gates referred to as threshold logic; applicable to building logic circuits resembling brain processing. For example, new devices such as memristors have been extensively used to develop such logic in recent times.",
            "score": 199.53107511997223
        },
        {
            "docid": "27075922_9",
            "document": "Natural computing . An artificial neural network is a network of artificial neurons.  An artificial neuron \"A\" is equipped with a function formula_1, receives \"n\" real-valued inputs formula_2 with respective weights formula_3, and it outputs formula_4. Some neurons are selected to be the output neurons, and the network function is the vectorial function that associates to the \"n\" input values, the outputs of the \"m\" selected output neurons. Note that different choices of weights produce different network functions for the same inputs. Back-propagation is a supervised learning method by which the weights of the connections in the network are repeatedly adjusted so as to minimize the difference between the vector of actual outputs and that of desired outputs. Learning algorithms based on backwards propagation of errors can be used to find optimal weights for given topology of the network and input-output pairs.",
            "score": 185.4955221414566
        },
        {
            "docid": "20490055_7",
            "document": "Tensor network theory . In 1986, Pellionisz described the geometrization of the \"three-neuron vestibulo-ocular reflex arc\" in a cat using tensor network theory. The \"three-neuron vestibulo-ocular reflex arc\" is named for the three neuron circuit the arc comprises. Sensory input into the vestibular system (angular acceleration of the head) is first received by the primary vestibular neurons which subsequently synapse onto secondary vestibular neurons. These secondary neurons carry out much of the signal processing and produce the efferent signal heading for the oculomotor neurons. Prior to the publishing of this paper, there had been no quantitative model to describe this \"classic example of a basic sensorimotor transformation in the central nervous system\" which is precisely what tensor network theory had been developed to model. <br> <br>  Here, Pellionisz described the analysis of the sensory input into the vestibular canals as the covariant vector component of tensor network theory. Likewise, the synthesized motor response (reflexive eye movement) is described as the contravariant vector component of the theory. By calculating the neuronal network transformations between the sensory input into the vestibular system and the subsequent motor response, a metric tensor representing the neuronal network was calculated.",
            "score": 185.52415144443512
        },
        {
            "docid": "21523_136",
            "document": "Artificial neural network . A fundamental objection is that they do not reflect how real neurons function. Back propagation is a critical part of most artificial neural networks, although no such mechanism exists in biological neural networks. How information is coded by real neurons is not known. Sensor neurons fire action potentials more frequently with sensor activation and muscle cells pull more strongly when their associated motor neurons receive action potentials more frequently. Other than the case of relaying information from a sensor neuron to a motor neuron, almost nothing of the principles of how information is handled by biological neural networks is known.",
            "score": 215.42658138275146
        },
        {
            "docid": "25345530_43",
            "document": "Models of neural computation . This response is then fed as input into other neurons and so on. The goal is to optimize the weights of the neurons to output a desired response at the output layer respective to a set given inputs at the input layer. This optimization of the neuron weights is often performed using the backpropagation algorithm and an optimization method such as gradient descent or Newton's method of optimization. Backpropagation compares the output of the network with the expected output from the training data, then updates the weights of each neuron to minimize the contribution of that individual neuron to the total error of the network.",
            "score": 185.63849639892578
        },
        {
            "docid": "21523_30",
            "document": "Artificial neural network . An \"artificial neural network\" is a network of simple elements called \"artificial neurons\", which receive input, change their internal state (\"activation\") according to that input, and produce output depending on the input and activation. The \"network\" forms by connecting the output of certain neurons to the input of other neurons forming a directed, weighted graph. The weights as well as the functions that compute the activation can be modified by a process called \"learning\" which is governed by a \"learning rule\".",
            "score": 192.2322235107422
        },
        {
            "docid": "8402086_6",
            "document": "Computational neurogenetic modeling . Modeling of genes and proteins allows individual responses of neurons in an artificial neural network that mimic responses in biological nervous systems, such as division (adding new neurons to the artificial neural network), creation of proteins to expand their cell membrane and foster neurite outgrowth (and thus stronger connections with other neurons), up-regulate or down-regulate receptors at synapses (increasing or decreasing the weight (strength) of synaptic inputs), uptake more neurotransmitters, change into different types of neurons, or die due to necrosis or apoptosis. The creation and analysis of these networks can be divided into two sub-areas of research: the  gene up-regulation that is involved in the normal functions of a neuron, such as growth, metabolism, and synapsing; and the effects of mutated genes on neurons and cognitive functions.",
            "score": 185.62103700637817
        },
        {
            "docid": "3474296_6",
            "document": "Neuronal noise . Another theory suggests that stochastic noise in a non-linear network shows a positive relationship between the interconnectivity and noise-like activity. Thus based on this theory, Patrick Wilken and colleagues suggest that neuronal noise is the principal factor that limits the capacity of visual short-term memory. Investigators of neural ensembles and those who especially support the theory of distributed processing, propose that large neuronal populations effectively decrease noise by averaging out the noise in individual neurons. Some investigators have shown in experiments and in models that neuronal noise is a possible mechanism to facilitate neuronal processing. The presence of neuronal noise (or more specifically synaptic noise) confers to neurons more sensitivity to a broader range of inputs, it can equalize the efficacy of synaptic inputs located at different positions on the neuron, and it can also enable finer temporal discrimination. There are many theories of why noise is apparent in the neuronal networks, but many neurologists are unclear of why they exist.",
            "score": 187.15587520599365
        },
        {
            "docid": "2920040_2",
            "document": "Neuronal tuning . Neuronal tuning refers to the hypothesized property of brain cells by which they selectively represent a particular type of sensory, association, motor, or cognitive information. Some neuronal responses have been hypothesized to be optimally tuned to specific patterns through experience. Neuronal tuning can be strong and sharp, as observed in primary visual cortex (area V1) (but see Carandini et al 2005 ), or weak and broad, as observed in neural ensembles. Single neurons are hypothesized to be simultaneously tuned to several modalities, such as visual, auditory, and olfactory. Neurons hypothesized to be tuned to different signals are often hypothesized to integrate information from the different sources. In computational models called neural networks, such integration is the major principle of operation. The best examples of neuronal tuning can be seen in the visual, auditory, olfactory, somatosensory, and memory systems, although due to the small number of stimuli tested the generality of neuronal tuning claims is still an open question.",
            "score": 262.9038647413254
        },
        {
            "docid": "40409788_31",
            "document": "Convolutional neural network . When dealing with high-dimensional inputs such as images, it is impractical to connect neurons to all neurons in the previous volume because such a network architecture does not take the spatial structure of the data into account. Convolutional networks exploit spatially local correlation by enforcing a local connectivity pattern between neurons of adjacent layers: each neuron is connected to only a small region of the input volume. The extent of this connectivity is a hyperparameter called the receptive field of the neuron. The connections are local in space (along width and height), but always extend along the entire depth of the input volume. Such an architecture ensures that the learnt filters produce the strongest response to a spatially local input pattern.",
            "score": 156.01568460464478
        },
        {
            "docid": "33246145_2",
            "document": "Neural decoding . Neural decoding is a neuroscience field concerned with the hypothetical reconstruction of sensory and other stimuli from information that has already been encoded and represented in the brain by networks of neurons. Reconstruction refers to the ability of the researcher to predict what sensory stimuli the subject is receiving based purely on neuron action potentials. Therefore, the main goal of neural decoding is to characterize how the electrical activity of neurons elicit activity and responses in the brain.",
            "score": 308.49615693092346
        },
        {
            "docid": "12142270_3",
            "document": "GENESIS (software) . GENESIS works by creating simulation environments for constructing models of neurons or neural systems. \"Nerve cells are capable of communicating with each other in such a highly structured manner as to form neuronal networks. To understand neural networks, it is necessary to understand the ways in which one neuron communicates with another through synaptic connections and the process called synaptic transmission\". Neurons have a specialized structure for their function, they \"are different from most other cells in the body in that they are polarized and have distinct morphological regions, each with specific functions\". The two important regions of a neuron are the dendrite and the axon. \"Dendrites are the region where one neuron receives connections from other neurons. The cell body or soma contains the nucleus and the other organelles necessary for cellular function. The axon is a key component of nerve cells over which information is transmitted from one part of the neuron (e.g., the cell body) to the terminal regions of the neuron\". The third important piece of a neuron is the synapse. \"The synapse is the terminal region of the axon this is where one neuron forms a connection with another and conveys information through the process of synaptic transmission\".",
            "score": 182.656667470932
        },
        {
            "docid": "6503281_9",
            "document": "Nv network . A basic Nv network is built upon several Nv neurons in a loop. The loop's timing is often varied by input sensors. This difference in timing is often meant to affect the output pattern of the Nv loop. An example of this can be seen in a simple BEAM \"walker\" robot utilizing a bicore network (2 neurons). The neural network is set up to alternate current going to the main motor in a way where under equal input from the main sensors, the neurons oscillate at an equal pace to each other, producing a steady walking gait. When input (e.g. from light sensors) is present, the timing of each neuron in the loop is varied based on the input from the sensors, affecting the pace at which the loop oscillates. This affected pace is often used to alter the walking gait of a robot in order to steer it based on the input from its sensors.",
            "score": 159.28387820720673
        },
        {
            "docid": "21944_35",
            "document": "Nervous system . The basic neuronal function of sending signals to other cells includes a capability for neurons to exchange signals with each other. Networks formed by interconnected groups of neurons are capable of a wide variety of functions, including feature detection, pattern generation and timing, and there are seen to be countless types of information processing possible. Warren McCulloch and Walter Pitts showed in 1943 that even artificial neural networks formed from a greatly simplified mathematical abstraction of a neuron are capable of universal computation.  Historically, for many years the predominant view of the function of the nervous system was as a stimulus-response associator. In this conception, neural processing begins with stimuli that activate sensory neurons, producing signals that propagate through chains of connections in the spinal cord and brain, giving rise eventually to activation of motor neurons and thereby to muscle contraction, i.e., to overt responses. Descartes believed that all of the behaviors of animals, and most of the behaviors of humans, could be explained in terms of stimulus-response circuits, although he also believed that higher cognitive functions such as language were not capable of being explained mechanistically. Charles Sherrington, in his influential 1906 book \"The Integrative Action of the Nervous System\", developed the concept of stimulus-response mechanisms in much more detail, and Behaviorism, the school of thought that dominated Psychology through the middle of the 20th century, attempted to explain every aspect of human behavior in stimulus-response terms.",
            "score": 238.62093353271484
        },
        {
            "docid": "8402086_9",
            "document": "Computational neurogenetic modeling . For the parameters in the gene regulatory network to affect the neurons in the artificial neural network as intended there must be some connection between them. In an organizational context, each node (neuron) in the artificial neural network has its own gene regulatory network associated with it. The weights (and in some networks, frequencies of synaptic transmission to the node), and the resulting membrane potential of the node (including whether an action potential is produced or not), affect the expression of different genes in the gene regulatory network. Factors affecting connections between neurons, such as synaptic plasticity, can be modeled by inputting the values of synaptic activity-associated genes and proteins to a function that re-evaluates the weight of an input from a particular neuron in the artificial neural network.",
            "score": 187.48428583145142
        },
        {
            "docid": "33246145_4",
            "document": "Neural decoding . When looking at a picture, people's brains are constantly making decisions about what object they are looking at, where they need to move their eyes next, and what they find to be the most salient aspects of the input stimulus. As these images hit the back of the retina, these stimuli are converted from varying wavelengths to a series of neural spikes called action potentials. These pattern of action potentials are different for different objects and different colors; we therefore say that the neurons are encoding objects and colors by varying their spike rates or temporal pattern. Now, if someone were to probe the brain by placing electrodes in the primary visual cortex, they may find what appears to be random electrical activity. These neurons are actually firing in response to the lower level features of visual input, possibly the edges of a picture frame. This highlights the crux of the neural decoding hypothesis: that it is possible to reconstruct a stimulus from the response of the ensemble of neurons that represent it. In other words, it is possible to look at spike train data and say that the person or animal being recorded is looking at a red ball.",
            "score": 208.7274409532547
        },
        {
            "docid": "10839226_2",
            "document": "Cultured neuronal network . A cultured neuronal network is a cell culture of neurons that is used as a model to study the central nervous system, especially the brain. Often, cultured neuronal networks are connected to an input/output device such as a multi-electrode array (MEA), thus allowing two-way communication between the researcher and the network. This model has proved to be an invaluable tool to scientists studying the underlying principles behind neuronal learning, memory, plasticity, connectivity, and information processing.",
            "score": 178.1985218524933
        },
        {
            "docid": "21523_4",
            "document": "Artificial neural network . In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called 'edges'. Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times.",
            "score": 175.82124853134155
        },
        {
            "docid": "12154725_5",
            "document": "Neural clique . A theoretical associative memory model with a practical implementation running in real-time on modern hardware was proposed, the Gripon-Berrou Neural Network or Cliques Neural Network, an extension of the Hopfield network. This model suggest that the encoding of memories or information is done in constant O(1) time, by simply creating synapses between the neurons, creating a clique in a subgraph of the network, representing the memory. The decoding is then simple and fast, based on the biological neurons behavior of the all-or-none and winner-takes-all. This model demonstrates the usefulness of cliques, by allowing the reconstruction of a full memory from a partial or corrupted input, even with unreliable synapses and neurons, and providing an explanation for associative train of thoughts when pre-cueing subjects with a familiar sensory stimuli (e.g., Proust's madeleine).",
            "score": 229.3788125514984
        },
        {
            "docid": "21120_3",
            "document": "Neuron . There are many types of specialized neurons. Sensory neurons respond to one particular type of stimulus such as touch, sound, or light and all other stimuli affecting the cells of the sensory organs, and converts it into an electrical signal via transduction, which is then sent to the spinal cord or brain. Motor neurons receive signals from the brain and spinal cord to cause everything from muscle contractions and affect glandular outputs. Interneurons connect neurons to other neurons within the same region of the brain or spinal cord in neural networks.",
            "score": 271.67398500442505
        },
        {
            "docid": "17747058_14",
            "document": "Perceptrons (book) . What the book does prove is that in three-layered feed-forward perceptrons (with a so-called \"hidden\" or \"intermediary\" layer), it is not possible to compute some predicates unless at least one of the neurons in the first layer of neurons (the \"intermediary\" layer) is connected with a non-null weight to each and every input. This was contrary to a hope held by some researchers in relying mostly on networks with a few layers of \"local\" neurons, each one connected only to a small number of inputs. A feed-forward machine with \"local\" neurons is much easier to build and use than a larger, fully connected neural network, so researchers at the time concentrated on these instead of on more complicated models.",
            "score": 158.72928595542908
        },
        {
            "docid": "8402086_18",
            "document": "Computational neurogenetic modeling . Because the amount of data on the interplay of genes and neurons and their effects is not enough to construct a rigorous model,  evolutionary computation is used to optimize artificial neural networks and gene regulatory networks, a common technique being the genetic algorithm. A genetic algorithm is a process that can be used to refine models by mimicking the process of natural selection observed in biological ecosystems. The primary advantages are that, due to not requiring derivative information, it can be applied to black box problems and multimodal optimization. The typical process for using genetic algorithms to refine a gene  regulatory network is: first, create a population; next, to create offspring via a crossover operation and  evaluate their fitness; then, on a group chosen for high fitness, simulate mutation via a mutation operator;  finally, taking the now mutated group, repeat this process until a desired level of fitness is demonstrated. Methods by which artificial neural networks may alter their structure without simulated mutation and fitness selection have been developed. A dynamically evolving neural network is one approach, as the creation of new connections and new neurons can  be modeled as the system adapts to new data. This enables the network to evolve in modeling accuracy without simulated natural selection. One method by which dynamically evolving networks may be optimized, called evolving layer neuron aggregation, combines neurons with sufficiently similar input weights into one neuron. This can take place during the training of the network, referred to as online aggregation, or between periods of training, referred to as offline aggregation. Experiments have suggested that offline aggregation is more efficient.",
            "score": 214.72860157489777
        },
        {
            "docid": "33818014_13",
            "document": "Nervous system network models . The neuron cell has three components\u00a0\u2013 dendrites, soma, and axon as shown in Figure 1. Dendrites, which have the shape of a tree with branches, called arbor, receive the message from other neurons with which the neuron is connected via synapses. The action potential received by each dendrite from the synapse is called the postsynaptic potential. The cumulative sum of the postsynaptic potentials is fed to the soma. The ionic components of the fluid inside and outside maintain the cell membrane at a resting potential of about 65 millivolts. When the cumulative postsynaptic potential exceeds the resting potential, an action potential is generated by the cell body or soma and propagated along the axon. The axon may have one or more terminals and these terminals transmit neurotransmitters to the synapses with which the neuron is connected. Depending on the stimulus received by the dendrites, soma may generate one or more well-separated action potentials or spike train. If the stimulus drives the membrane to a positive potential, it is an excitatory neuron; and if it drives the resting potential further in the negative direction, it is an inhibitory neuron. The generation of the action potential is called the \u201cfiring.\u201d The firing neuron described above is called a spiking neuron. We will model the electrical circuit of the neuron in Section 3.6. There are two types of spiking neurons. If the stimulus remains above the threshold level and the output is a spike train, it is called the Integrate-and-Fire (IF) neuron model. If output is modeled as dependent on the impulse response of the circuit, then it is called the Spike Response Model (SRM) (Gestner, W. (1995)).",
            "score": 153.39913165569305
        },
        {
            "docid": "22000_7",
            "document": "Neural Darwinism . Once the basic variegated anatomical structure of the brain is laid down during early development, it is more or less fixed. But given the numerous and diverse collection of available circuitry, there are bound to be functionally equivalent albeit anatomically non-isomorphic neuronal groups capable of responding to certain sensory input. This creates a competitive environment where circuit groups proficient in their responses to certain inputs are \"chosen\" through the enhancement of the synaptic efficacies of the selected network. This leads to an increased probability that the same network will respond to similar or identical signals at a future time. This occurs through the strengthening of neuron-to-neuron synapses. And these adjustments allow for neural plasticity along a fairly quick timetable.",
            "score": 213.27461051940918
        },
        {
            "docid": "35179233_4",
            "document": "Probabilistic neural network . Each neuron in the input layer represents a predictor variable. In categorical variables, \"N-1\" neurons are used when there are \"N\" number of categories. It standardizes the range of the values by subtracting the median and dividing by the interquartile range. Then the input neurons feed the values to each of the neurons in the hidden layer.",
            "score": 150.88780641555786
        }
    ],
    "r": [
        {
            "docid": "33246145_2",
            "document": "Neural decoding . Neural decoding is a neuroscience field concerned with the hypothetical reconstruction of sensory and other stimuli from information that has already been encoded and represented in the brain by networks of neurons. Reconstruction refers to the ability of the researcher to predict what sensory stimuli the subject is receiving based purely on neuron action potentials. Therefore, the main goal of neural decoding is to characterize how the electrical activity of neurons elicit activity and responses in the brain.",
            "score": 308.49615478515625
        },
        {
            "docid": "321869_55",
            "document": "Coding theory . Neural coding is a neuroscience-related field concerned with how sensory and other information is represented in the brain by networks of neurons. The main goal of studying neural coding is to characterize the relationship between the stimulus and the individual or ensemble neuronal responses and the relationship among electrical activity of the neurons in the ensemble. It is thought that neurons can encode both digital and analog information, and that neurons follow the principles of information theory and compress information, and detect and correct errors in the signals that are sent throughout the brain and wider nervous system.",
            "score": 294.38287353515625
        },
        {
            "docid": "6147487_2",
            "document": "Neural coding . Neural coding is a neuroscience field concerned with characterising the hypothetical relationship between the stimulus and the individual or ensemble neuronal responses and the relationship among the electrical activity of the neurons in the ensemble. Based on the theory that sensory and other information is represented in the brain by networks of neurons, it is thought that neurons can encode both digital and analog information.",
            "score": 277.9509582519531
        },
        {
            "docid": "3975854_2",
            "document": "Sensory neuroscience . Sensory neuroscience is a subfield of neuroscience which explores the anatomy and physiology of neurons that are part of sensory systems such as vision, hearing, and olfaction. Neurons in sensory regions of the brain respond to stimuli by firing one or more nerve impulses (action potentials) following stimulus presentation. How is information about the outside world encoded by the rate, timing, and pattern of action potentials? This so-called neural code is currently poorly understood and sensory neuroscience plays an important role in the attempt to decipher it. Looking at early sensory processing is advantageous since brain regions that are \"higher up\" (e.g. those involved in memory or emotion) contain neurons which encode more abstract representations. However, the hope is that there are unifying principles which govern how the brain encodes and processes information. Studying sensory systems is an important stepping stone in our understanding of brain function in general.",
            "score": 274.396484375
        },
        {
            "docid": "21120_37",
            "document": "Neuron . Neural coding is concerned with how sensory and other information is represented in the brain by neurons. The main goal of studying neural coding is to characterize the relationship between the stimulus and the individual or ensemble neuronal responses, and the relationships amongst the electrical activities of the neurons within the ensemble. It is thought that neurons can encode both digital and analog information.",
            "score": 273.9918212890625
        },
        {
            "docid": "21120_3",
            "document": "Neuron . There are many types of specialized neurons. Sensory neurons respond to one particular type of stimulus such as touch, sound, or light and all other stimuli affecting the cells of the sensory organs, and converts it into an electrical signal via transduction, which is then sent to the spinal cord or brain. Motor neurons receive signals from the brain and spinal cord to cause everything from muscle contractions and affect glandular outputs. Interneurons connect neurons to other neurons within the same region of the brain or spinal cord in neural networks.",
            "score": 271.6739807128906
        },
        {
            "docid": "33246145_6",
            "document": "Neural decoding . Implicit about the decoding hypothesis is the assumption that neural spiking in the brain somehow represents stimuli in the external world. The decoding of neural data would be impossible if the neurons were firing randomly: nothing would be represented. This process of decoding neural data forms a loop with neural encoding. First, the organism must be able to perceive a set of stimuli in the world \u2013 say a picture of a hat. Seeing the stimuli must result in some internal learning: the encoding stage. After varying the range of stimuli that is presented to the observer, we expect the neurons to adapt to the statistical properties of the signals, encoding those that occur most frequently: the efficient-coding hypothesis. Now neural decoding is the process of taking these statistical consistencies, a statistical model of the world, and reproducing the stimuli. This may map to the process of thinking and acting, which in turn guide what stimuli we receive, and thus, completing the loop.",
            "score": 269.30810546875
        },
        {
            "docid": "22396342_9",
            "document": "H1 neuron . Visual information in optical systems is inhibited by the temporal and spatial attributes of the sensory input, and by the biophysical properties of the neuronal circuits. How neural circuits encode behaviorally relevant information is dependent on the computational capacity of the nervous system with relation to the ambient conditions the organisms normally operate in. H1 neurons are proven to be very efficient encoders of information via their high resilience to stimulus noise from external sources. The operational and encoding processes of sensory pathways are often negatively affected by both external noise (relating to the stimulus) and internal noise (imperfect physiological processes); however, the activity of H1 is unaffected by photon noise. Instead, neuronal noise intrinsic to the H1 neural architecture is the limiting factor for accurate responses to stimuli. This dramatically reduces the noise of H1 electrophysiological readings, and provides the reliability necessary for accurate study conclusions.",
            "score": 263.2930908203125
        },
        {
            "docid": "2920040_2",
            "document": "Neuronal tuning . Neuronal tuning refers to the hypothesized property of brain cells by which they selectively represent a particular type of sensory, association, motor, or cognitive information. Some neuronal responses have been hypothesized to be optimally tuned to specific patterns through experience. Neuronal tuning can be strong and sharp, as observed in primary visual cortex (area V1) (but see Carandini et al 2005 ), or weak and broad, as observed in neural ensembles. Single neurons are hypothesized to be simultaneously tuned to several modalities, such as visual, auditory, and olfactory. Neurons hypothesized to be tuned to different signals are often hypothesized to integrate information from the different sources. In computational models called neural networks, such integration is the major principle of operation. The best examples of neuronal tuning can be seen in the visual, auditory, olfactory, somatosensory, and memory systems, although due to the small number of stimuli tested the generality of neuronal tuning claims is still an open question.",
            "score": 262.90386962890625
        },
        {
            "docid": "6147487_3",
            "document": "Neural coding . Neurons are remarkable among the cells of the body in their ability to propagate signals rapidly over large distances. They do this by generating characteristic electrical pulses called action potentials: voltage spikes that can travel down nerve fibers. Sensory neurons change their activities by firing sequences of action potentials in various temporal patterns, with the presence of external sensory stimuli, such as light, sound, taste, smell and touch. It is known that information about the stimulus is encoded in this pattern of action potentials and transmitted into and around the brain.",
            "score": 256.63330078125
        },
        {
            "docid": "37080_15",
            "document": "Thought . A neuron (also known as a neurone or nerve cell) is an excitable cell in the nervous system that processes and transmits information by electrochemical signaling. Neurons are the core components of the brain, the vertebrate spinal cord, the invertebrate ventral nerve cord and the peripheral nerves. A number of specialized types of neurons exist: sensory neurons respond to touch, sound, light and numerous other stimuli affecting cells of the sensory organs that then send signals to the spinal cord and brain. Motor neurons receive signals from the brain and spinal cord that cause muscle contractions and affect glands. Interneurons connect neurons to other neurons within the brain and spinal cord. Neurons respond to stimuli, and communicate the presence of stimuli to the central nervous system, which processes that information and sends responses to other parts of the body for action. Neurons do not go through mitosis and usually cannot be replaced after being destroyed, although astrocytes have been observed to turn into neurons, as they are sometimes pluripotent.",
            "score": 238.86026000976562
        },
        {
            "docid": "21944_35",
            "document": "Nervous system . The basic neuronal function of sending signals to other cells includes a capability for neurons to exchange signals with each other. Networks formed by interconnected groups of neurons are capable of a wide variety of functions, including feature detection, pattern generation and timing, and there are seen to be countless types of information processing possible. Warren McCulloch and Walter Pitts showed in 1943 that even artificial neural networks formed from a greatly simplified mathematical abstraction of a neuron are capable of universal computation.  Historically, for many years the predominant view of the function of the nervous system was as a stimulus-response associator. In this conception, neural processing begins with stimuli that activate sensory neurons, producing signals that propagate through chains of connections in the spinal cord and brain, giving rise eventually to activation of motor neurons and thereby to muscle contraction, i.e., to overt responses. Descartes believed that all of the behaviors of animals, and most of the behaviors of humans, could be explained in terms of stimulus-response circuits, although he also believed that higher cognitive functions such as language were not capable of being explained mechanistically. Charles Sherrington, in his influential 1906 book \"The Integrative Action of the Nervous System\", developed the concept of stimulus-response mechanisms in much more detail, and Behaviorism, the school of thought that dominated Psychology through the middle of the 20th century, attempted to explain every aspect of human behavior in stimulus-response terms.",
            "score": 238.62094116210938
        },
        {
            "docid": "2860430_23",
            "document": "Neural oscillation . A neural network model describes a population of physically interconnected neurons or a group of disparate neurons whose inputs or signalling targets define a recognizable circuit. These models aim to describe how the dynamics of neural circuitry arise from interactions between individual neurons. Local interactions between neurons can result in the synchronization of spiking activity and form the basis of oscillatory activity. In particular, models of interacting pyramidal cells and inhibitory interneurons have been shown to generate brain rhythms such as gamma activity.",
            "score": 238.4751739501953
        },
        {
            "docid": "3717_69",
            "document": "Brain . Theorists have worked to understand these response patterns by constructing mathematical models of neurons and neural networks, which can be simulated using computers. Some useful models are abstract, focusing on the conceptual structure of neural algorithms rather than the details of how they are implemented in the brain; other models attempt to incorporate data about the biophysical properties of real neurons. No model on any level is yet considered to be a fully valid description of brain function, though. The essential difficulty is that sophisticated computation by neural networks requires distributed processing in which hundreds or thousands of neurons work cooperatively\u2014current methods of brain activity recording are only capable of isolating action potentials from a few dozen neurons at a time.",
            "score": 237.0726318359375
        },
        {
            "docid": "21944_9",
            "document": "Nervous system . Even in the nervous system of a single species such as humans, hundreds of different types of neurons exist, with a wide variety of morphologies and functions. These include sensory neurons that transmute physical stimuli such as light and sound into neural signals, and motor neurons that transmute neural signals into activation of muscles or glands; however in many species the great majority of neurons participate in the formation of centralized structures (the brain and ganglia) and they receive all of their input from other neurons and send their output to other neurons.",
            "score": 235.24044799804688
        },
        {
            "docid": "6147487_36",
            "document": "Neural coding . It has been shown that neurons in some cortical sensory areas encode rich naturalistic stimuli in terms of their spike times relative to the phase of ongoing network oscillatory fluctuations, rather than only in terms of their spike count. The local field potential signals reflect population (network) oscillations. The phase-of-firing code is often categorized as a temporal code although the time label used for spikes (i.e. the network oscillation phase) is a low-resolution (coarse-grained) reference for time. As a result, often only four discrete values for the phase are enough to represent all the information content in this kind of code with respect to the phase of oscillations in low frequencies. Phase-of-firing code is loosely based on the phase precession phenomena observed in place cells of the hippocampus. Another feature of this code is that neurons adhere to a preferred order of spiking between a group of sensory neurons, resulting in firing sequence.",
            "score": 234.3387908935547
        },
        {
            "docid": "3474296_4",
            "document": "Neuronal noise . Single neurons demonstrate different responses to specific neuronal input signals. This is commonly referred to as neural response variability. If a specific input signal is initiated in the dendrites of a neuron, then a hypervariability exists in the number of vesicles released from the axon terminal fiber into the synapse. This characteristic is true for fibers without neural input signals, such as pacemaker neurons, as mentioned previously, and cortical pyramidal neurons that have highly-irregular firing pattern. Noise generally hinders neural performance, but recent studies show, in dynamical non-linear neural networks, this statement does not always hold true. Non-linear neural networks are a network of complex neurons that have many connections with one another such as the neuronal systems found within our brains. Comparatively, linear networks are an experimental view of analyzing a neural system by placing neurons in series with each other.",
            "score": 232.4154052734375
        },
        {
            "docid": "15774067_19",
            "document": "Synaptic noise . Noise in neurons is due to intrinsic and extrinsic sources. It can disrupt activity and interfere with how well a neuron can encode a signal. Noise is observed as changes in the membrane potential of a cell. The change in potential causes the accuracy of a neuron to be limited in its transmission. This limited transmission has been coined the signal-to-noise ratio. As noise levels increase, one would assume a lowered ratio and therefore diminished signals. A diminished signal can be detrimental to a cell if neuronal maintenance is disrupted, or more importantly a necessary inhibitory response is lost. Noise limits the fidelity of a neuron's response to a signal or stimulus. The accuracy of the signal will impact how well higher parts of the brain or sensory system process information from the neurons.",
            "score": 231.55982971191406
        },
        {
            "docid": "3975854_3",
            "document": "Sensory neuroscience . A typical experiment in sensory neuroscience involves the presentation of a series of relevant stimuli to an experimental subject while the subject's brain is being monitored. This monitoring can be accomplished by noninvasive means such as functional magnetic resonance imaging (fMRI) or electroencephalography (EEG), or by more invasive means such as electrophysiology, the use of electrodes to record the electrical activity of single neurons or groups of neurons. fMRI measures changes in blood flow which related to the level of neural activity and provides low spatial and temporal resolution, but does provide data from the whole brain. In contrast, Electrophysiology provides very high temporal resolution (the shapes of single spikes can be resolved) and data can be obtained from single cells. This is important since computations are performed within the dendrites of individual neurons.",
            "score": 230.9529266357422
        },
        {
            "docid": "12154725_5",
            "document": "Neural clique . A theoretical associative memory model with a practical implementation running in real-time on modern hardware was proposed, the Gripon-Berrou Neural Network or Cliques Neural Network, an extension of the Hopfield network. This model suggest that the encoding of memories or information is done in constant O(1) time, by simply creating synapses between the neurons, creating a clique in a subgraph of the network, representing the memory. The decoding is then simple and fast, based on the biological neurons behavior of the all-or-none and winner-takes-all. This model demonstrates the usefulness of cliques, by allowing the reconstruction of a full memory from a partial or corrupted input, even with unreliable synapses and neurons, and providing an explanation for associative train of thoughts when pre-cueing subjects with a familiar sensory stimuli (e.g., Proust's madeleine).",
            "score": 229.37881469726562
        },
        {
            "docid": "2860457_6",
            "document": "Neural ensemble . Neuronal ensembles encode information in a way somewhat similar to the principle of Wikipedia operation \u2013 multiple edits by many participants. Neuroscientists have discovered that individual neurons are very noisy. For example, by examining the activity of only a single neuron in the visual cortex, it is very difficult to reconstruct the visual scene that the owner of the brain is looking at. Like a single Wikipedia participant, an individual neuron does not 'know' everything and is likely to make mistakes. This problem is solved by the brain having billions of neurons. Information processing by the brain is population processing, and it is also distributed \u2013 in many cases each neuron knows a little bit about everything, and the more neurons participate in a job, the more precise the information encoding. In the distributed processing scheme, individual neurons may exhibit neuronal noise, but the population as a whole averages this noise out.",
            "score": 228.7043914794922
        },
        {
            "docid": "12142270_4",
            "document": "GENESIS (software) . Neural networks like the ones simulated with GENESIS software can quickly become highly complex and difficult to understand. \"Just a few interconnected neurons (a microcircuit) can perform sophisticated tasks such as mediate reflexes, process sensory information, generate locomotion and mediate learning and memory. Even more complex networks, macrocircuits, consist of multiple embedded microcircuits. Macrocircuits mediate higher brain functions such as object recognition and cognition\". GENESIS endeavors to simulate neural systems as they are found in nature. Often, \"a neuron can receive contacts from up to 10,000 presynaptic neurons, and, in turn, any one neuron can contact up to 10,000 postsynaptic neurons. The combinatorial possibility could give rise to enormously complex neuronal circuits or network topologies, which might be very difficult to understand\".",
            "score": 226.5195770263672
        },
        {
            "docid": "53686950_18",
            "document": "Bi-directional hypothesis of language and action . It has been proposed that the control of movement is organized hierarchically, where movement is not controlled by individually controlling single neurons, but that movements are represented at a gross, more functional level. A similar concept has been applied to the control of cognition, resulting in the theory of cognitive circuits. This theory proposes that there are functional units of neurons in the brain that are strongly connected, and act coherently as a functional unit during cognitive tasks. These functional units of neurons, or \"thought circuits,\" have been referred to as the \"building blocks of cognition\". Thought circuits are believed to have been originally formed from basic anatomical connections, that were strengthened with correlated activity through Hebbian learning and plasticity. Formation of these neural networks has been demonstrated with computational models using known anatomical connections and Hebbian learning principles. For example, sensory stimulation through interaction with an object activates a distributed network of neurons in the cortex. Repeated activation of these neurons, through Hebbian plasticity, may strengthen their connections and form a circuit. This sensory circuit may then be activated during the perception of known objects.",
            "score": 226.3509979248047
        },
        {
            "docid": "2872287_23",
            "document": "Neural binding . Much of the experimental evidence for neural binding has traditionally revolved around sensory awareness. Sensory awareness is accomplished by integrating things together by cognitively perceiving them and then segmenting them so that, in total, there is an image created. Since there can be an infinite number of possibilities in the perception of an object, this has been a unique area of study. The way the brain then collectively pieces certain things together via networking is important not only in the global way of perceiving but also in segmentation. Much of sensory awareness has to do with the taking of a single piece of an object's makeup and then binding its total characteristics so that the brain perceives the object in its final form. Much of the research for the understanding of segmentation and how the brain perceives an object has been done by studying cats. A major finding of this research has to do with the understanding of gamma waves oscillating at 40\u00a0Hz. The information was extracted from a study using the cat visual cortex. It was shown that the cortical neurons responded differently to spatially different objects. These firings of neurons ranged from 40\u201360\u00a0Hz in measure and when observed showed that they fired synchronously when observing different parts of the object. Such coherent responses point to the fact that the brain is doing a kind of coding where it is piecing certain neurons together in the works of making the form of an object. Since the brain is putting these segmented pieces together unsupervised, a significant consonance is found with many philosophers (like Sigmund Freud) who theorize an underlying subconscious that helps to form every aspect of our conscious thought processes.",
            "score": 225.5084228515625
        },
        {
            "docid": "5128182_13",
            "document": "Encoding (memory) . Encoding is achieved using a combination of chemicals and electricity. Neurotransmitters are released when an electrical pulse crosses the synapse which serves as a connection from nerve cells to other cells. The dendrites receive these impulses with their feathery extensions. A phenomenon called long-term potentiation allows a synapse to increase strength with increasing numbers of transmitted signals between the two neurons. For that to happen, NMDA receptor, which influences the flow of information between neurons by controlling the initiation of long-term potentiation in most hippocampal pathways, need to come to the play. For these NMDA receptors to be activated, there must be two conditions. Firstly, glutamate has to be released and bound to the NMDA receptor site on postsynaptic neurons. Secondly, excitation has to take place in postsynaptic neurons. These cells also organise themselves into groups specializing in different kinds of information processing. Thus, with new experiences the brain creates more connections and may 'rewire'. The brain organizes and reorganizes itself in response to one's experiences, creating new memories prompted by experience, education, or training. Therefore, the use of a brain reflects how it is organised. This ability to re-organize is especially important if ever a part of the brain becomes damaged. Scientists are unsure of whether the stimuli of what we do not recall are filtered out at the sensory phase or if they are filtered out after the brain examines their significance.",
            "score": 225.19866943359375
        },
        {
            "docid": "15731985_16",
            "document": "Post-traumatic epilepsy . In addition to chemical changes in cells, structural changes that lead to epilepsy may occur in the brain. Seizures that occur shortly after TBI can reorganize neural networks and cause seizures to occur repeatedly and spontaneously later on. The kindling hypothesis suggests that new neural connections are formed in the brain and cause an increase in excitability. The word \"kindling\" is a metaphor: the way the brain's response to stimuli increases over repeated exposures is similar to the way small burning twigs can produce a large fire. This reorganization of neural networks may make them more excitable. Neurons that are in a hyperexcitable state due to trauma may create an epileptic focus in the brain that leads to seizures. In addition, an increase in neurons' excitability may accompany loss of inhibitory neurons that normally serve to reduce the likelihood that other neurons will fire; these changes may also produce PTE.",
            "score": 224.2969970703125
        },
        {
            "docid": "2671056_11",
            "document": "Metastability in the brain . The so-named HKB model is one of the earliest and well-respected theories to describe coordination dynamics in the brain. In this model, the formation of neural networks can be partly described as self-organization, where individual neurons and small neuronal systems aggregate and coordinate to either adapt or respond to local stimuli or to divide labor and specialize in function.",
            "score": 222.75958251953125
        },
        {
            "docid": "505717_72",
            "document": "Image segmentation . Pulse-coupled neural networks (PCNNs) are neural models proposed by modeling a cat\u2019s visual cortex and developed for high-performance biomimetic image processing. In 1989, Eckhorn introduced a neural model to emulate the mechanism of a cat\u2019s visual cortex. The Eckhorn model provided a simple and effective tool for studying the visual cortex of small mammals, and was soon recognized as having significant application potential in image processing. In 1994, the Eckhorn model was adapted to be an image processing algorithm by Johnson, who termed this algorithm Pulse-Coupled Neural Network. Over the past decade, PCNNs have been utilized for a variety of image processing applications, including: image segmentation, feature generation, face extraction, motion detection, region growing, noise reduction, and so on. A PCNN is a two-dimensional neural network. Each neuron in the network corresponds to one pixel in an input image, receiving its corresponding pixel\u2019s color information (e.g. intensity) as an external stimulus. Each neuron also connects with its neighboring neurons, receiving local stimuli from them. The external and local stimuli are combined in an internal activation system, which accumulates the stimuli until it exceeds a dynamic threshold, resulting in a pulse output. Through iterative computation, PCNN neurons produce temporal series of pulse outputs. The temporal series of pulse outputs contain information of input images and can be utilized for various image processing applications, such as image segmentation and feature generation. Compared with conventional image processing means, PCNNs have several significant merits, including robustness against noise, independence of geometric variations in input patterns, capability of bridging minor intensity variations in input patterns, etc.",
            "score": 221.35592651367188
        },
        {
            "docid": "5198024_2",
            "document": "Efficient coding hypothesis . The efficient coding hypothesis was proposed by Horace Barlow in 1961 as a theoretical model of sensory coding in the brain. Within the brain, neurons often communicate with one another by sending electrical impulses referred to as action potentials or spikes. One goal of sensory neuroscience is to decipher the meaning of these spikes in order to understand how the brain represents and processes information about the outside world. Barlow hypothesized that the spikes in the sensory system formed a neural code for efficiently representing sensory information. By efficient Barlow meant that the code minimized the number of spikes needed to transmit a given signal. This is somewhat analogous to transmitting information across the internet, where different file formats can be used to transmit a given image. Different file formats require different number of bits for representing the same image at given distortion level, and some are better suited for representing certain classes of images than others. According to this model, the brain is thought to use a code which is suited for representing visual and audio information representative of an organism's natural environment.",
            "score": 220.68707275390625
        },
        {
            "docid": "3766002_15",
            "document": "Orbitofrontal cortex . Neurons in the OFC respond both to primary reinforcers, as well as cues that predict rewards across multiple sensory domains. The evidence for responses to visual, gustatory, somatosensory, and olfactory stimuli is robust, but evidence or auditory responses are weaker. In a subset of OFC neurons, neural responses to rewards or reward cues are modulated by individual preference and by internal motivational states such as hunger. A fraction of neurons that respond to sensory cues predicting a reward are selective for reward, and exhibit reversal behavior when cue outcome relationships are swapped. Neurons in the OFC also exhibit responses to the absence of an expected reward, and punishment. Another population of neurons exhibits responses to novel stimuli and can \u201cremember\u201d familiar stimuli for up to a day.",
            "score": 220.5723419189453
        },
        {
            "docid": "35982062_6",
            "document": "Biased Competition Theory . There are two major neural pathways that process the information in the visual field; the ventral stream and the dorsal stream. The two pathways run in parallel and are both working simultaneously. The ventral stream is important for object recognition and often referred to as the \u201cwhat\u201d system of the brain; it projects to the inferior temporal cortex. The dorsal stream is important for spatial perception and performance and is referred to as the \u201cwhere\u201d system which projects to the posterior parietal cortex. According to the biased competition theory, an individual\u2019s visual system has limited capacity to process information about multiple objects at any given time. For example, if an individual was presented with two stimuli (objects) and was asked to identify attributes of each object at the same time, the individual\u2019s performance would be worse in comparison to if the objects were presented separately. This suggests multiple objects presented simultaneously in the visual field will compete for neural representation due to limited processing resources. Single cell recording studies conducted by Kastner and Ungerleider examined the neural mechanisms behind the biased competition theory. In their experiment the size of the receptive field's (RF) of neurons within the visual cortex were examined. A single visual stimulus was presented alone in a neuron\u2019s RF, followed with another stimulus presented simultaneously within the same RF. The single \u2018effective\u2019 stimuli produced a low firing rate, whereas the two stimuli presented together produced a high firing rate. The response to the paired stimuli was reduced. This suggests that when two stimuli are presented together within a neuron\u2019s RF, the stimuli are processed in a mutually suppressive manner, rather than being processed independently. This suppression process, according to Kastner and Ungerleider, occurs when two stimuli are presented together because they compete for neural representation, due to limited cognitive processing capacity. The RF experiment suggests that as the number of objects increase, the information available for each object will decrease due to increased neural workload (suppression), and decreased cognitive capacity. In order for an object in the visual field or RF be efficiently processed, there needs to be a way to bias these neurological resources towards the object. Attention prioritizes task relevant objects, biasing this process. For example, this bias can be towards an object which is currently attended to in the visual field or RF, or towards the object that is most relevant to one\u2019s behavior. Functional magnetic resonance imaging (fMRI) has shown that biased competition theory can explain the observed attention effects at a neuronal level. Attention effects bias the internal weight (strengthens connections) of task relevant features toward the attended object. This was shown by Reddy, Kanwisher, and van Rullen who found an increase in oxygenated blood to a specific neuron following a locational cue. Further neurological support comes from neurophysiological studies which have shown that attention results from Top-down biasing, which in turn influences neuronal spiking. In sum, external inputs affect the Top-down guidance of attention, which bias specific neurons in the brain.",
            "score": 220.08692932128906
        },
        {
            "docid": "2872287_26",
            "document": "Neural binding . Cognitive binding is associated with the different states of human consciousness. Two of the most studied states of consciousness are the wakefulness and REM sleep. There have been multiple studies showing, electrophysiologically, that these two states are quite similar in nature. This has led some neural binding theorists to study the modes of cognitive awareness in each state. Certain observations have even led these scientists to hypothesize that since there is little cognition going on during REM sleep, the increased thalamocortical responses show the action of processing in the waking preconscious. The thalamus and cortex are important anatomical features in cognitive and sensory awareness. The understanding of how these neurons fire and relate to one other in each of these states (REM and Waking) is paramount to understanding awareness and its relation to neural binding. In the waking state, neuronal activity in animals is subject to changes based on the current environment. Changes in environment act as a form of stress on the brain so that when sensory neurons are then fired synchronously, they acclimate to the new state. This new state can then be moved to the hippocampus where it can be stored for later use. In the words of James Newman and Anthony A. Grace in their article, \"Binding Across Time\" this idea is put forth: \"The hippocampus is the primary recipient of inferotemporal outputs and is known to be the substrate for the consolidation of working memories to long term, episodic memories.\" The logging of \"episodes\" is then used for \"streaming\", which can mediate by the selective gating of certain information reentering sensory awareness. Streaming and building of episodic memories would not be possible if neural binding did not unconsciously connect the two synchronous oscillations. The pairing of these oscillations can then help input the correct sensory material. If these paired oscillations are not new, then cognitively these firings will be easily understood. If there are new firings, the brain will have to acclimate to the new understanding. In REM sleep, the only extreme difference from the waking state is that the brain does not have the actual waking amount of sensory firings, so cognitively, there is not as much awareness here, although the activity of the \"brain\u2019s eye\" is still quite significant and very similar to the waking state. Studies have shown that during sleep there are still 40\u00a0Hz Oscillation firings. These firings are due to the perceived stimuli happening in dreams. \"",
            "score": 219.4730987548828
        }
    ]
}