{
    "q": [
        {
            "docid": "8953842_15",
            "document": "Computational auditory scene analysis . While many models consider the audio signal as a complex combination of different frequencies, modeling the auditory system can also require consideration for the neural components. By taking a holistic process, where a stream (of feature-based sounds) correspond to neuronal activity distributed in many brain areas, the perception of the sound could be mapped and modeled. Two different solutions have been proposed to the binding of the audio perception and the area in the brain. Hierarchical coding models many cells to encode all possible combinations of features and objects in the auditory scene. Temporal or oscillatory correlation addressing the binding problem by focusing on the synchrony and desynchrony between neural oscillations to encode the state of binding among the auditory features. These two solutions are very similar to the debacle between place coding and temporal coding. While drawing from modeling neural components, another phenomenon of ASA comes into play with CASA systems: the extent of modeling neural mechanisms. The studies of CASA systems have involved modeling some known mechanisms, such as the bandpass nature of cochlear filtering and random auditory nerve firing patterns, however, these models may not lead to finding new mechanisms, but rather give an understanding of purpose to the known mechanisms.",
            "score": 113.9293681383133
        },
        {
            "docid": "2860430_7",
            "document": "Neural oscillation . Although neural oscillations in human brain activity are mostly investigated using EEG recordings, they are also observed using more invasive recording techniques such as single-unit recordings. Neurons can generate rhythmic patterns of action potentials or spikes. Some types of neurons have the tendency to fire at particular frequencies, so-called \"resonators\". Bursting is another form of rhythmic spiking. Spiking patterns are considered fundamental for information coding in the brain. Oscillatory activity can also be observed in the form of subthreshold membrane potential oscillations (i.e. in the absence of action potentials). If numerous neurons spike in synchrony, they can give rise to oscillations in local field potentials. Quantitative models can estimate the strength of neural oscillations in recorded data.",
            "score": 89.93027877807617
        },
        {
            "docid": "29020457_3",
            "document": "Gordon M. Shepherd . His electrophysiological studies of the olfactory bulb in 1963 produced one of the first examples of a brain microcircuit. Building on this work he collaborated with Wilfrid Rall at NIH to construct the first computational models of brain neurons. This predicted dendrodendritic interactions in the olfactory bulb, subsequently confirmed by electronmicroscopy, hypothesized to mediate lateral inhibition of the sensory input. A collaboration in 1975, using new methods of brain imaging, revealed that odors are encoded by different spatial activity patterns in the olfactory glomeruli of the olfactory bulb. This showed that the neural basis of smell in virtually all vertebrates involves odor representation by glomerular activity patterns (\"odor images\") which are then processed by lateral inhibition mediated by the dendrodendritic circuits. Shepherd's lab continued to use the olfactory bulb as a general model for the integrative actions of neuronal dendrites. This showed that dendrites contain multiple computational units; backpropagating action potentials in dendrites carry out specific functional operations; and dendritic spines can function as semi-independent input-output units. The lab also provided a basic circuit for olfactory cortex. New concepts to replace the classical \"neuron doctrine\" were suggested, and the term \"microcircuit\" for characterizing specific patterns of synaptic interactions in the nervous system.",
            "score": 69.61215317249298
        },
        {
            "docid": "2872287_23",
            "document": "Neural binding . Much of the experimental evidence for neural binding has traditionally revolved around sensory awareness. Sensory awareness is accomplished by integrating things together by cognitively perceiving them and then segmenting them so that, in total, there is an image created. Since there can be an infinite number of possibilities in the perception of an object, this has been a unique area of study. The way the brain then collectively pieces certain things together via networking is important not only in the global way of perceiving but also in segmentation. Much of sensory awareness has to do with the taking of a single piece of an object's makeup and then binding its total characteristics so that the brain perceives the object in its final form. Much of the research for the understanding of segmentation and how the brain perceives an object has been done by studying cats. A major finding of this research has to do with the understanding of gamma waves oscillating at 40\u00a0Hz. The information was extracted from a study using the cat visual cortex. It was shown that the cortical neurons responded differently to spatially different objects. These firings of neurons ranged from 40\u201360\u00a0Hz in measure and when observed showed that they fired synchronously when observing different parts of the object. Such coherent responses point to the fact that the brain is doing a kind of coding where it is piecing certain neurons together in the works of making the form of an object. Since the brain is putting these segmented pieces together unsupervised, a significant consonance is found with many philosophers (like Sigmund Freud) who theorize an underlying subconscious that helps to form every aspect of our conscious thought processes.",
            "score": 94.38896787166595
        },
        {
            "docid": "6147487_31",
            "document": "Neural coding . The mammalian gustatory system is useful for studying temporal coding because of its fairly distinct stimuli and the easily discernible responses of the organism. Temporally encoded information may help an organism discriminate between different tastants of the same category (sweet, bitter, sour, salty, umami) that elicit very similar responses in terms of spike count. The temporal component of the pattern elicited by each tastant may be used to determine its identity (e.g., the difference between two bitter tastants, such as quinine and denatonium). In this way, both rate coding and temporal coding may be used in the gustatory system \u2013 rate for basic tastant type, temporal for more specific differentiation. Research on mammalian gustatory system has shown that there is an abundance of information present in temporal patterns across populations of neurons, and this information is different from that which is determined by rate coding schemes. Groups of neurons may synchronize in response to a stimulus. In studies dealing with the front cortical portion of the brain in primates, precise patterns with short time scales only a few milliseconds in length were found across small populations of neurons which correlated with certain information processing behaviors. However, little information could be determined from the patterns; one possible theory is they represented the higher-order processing taking place in the brain.",
            "score": 73.86069321632385
        },
        {
            "docid": "2872287_26",
            "document": "Neural binding . Cognitive binding is associated with the different states of human consciousness. Two of the most studied states of consciousness are the wakefulness and REM sleep. There have been multiple studies showing, electrophysiologically, that these two states are quite similar in nature. This has led some neural binding theorists to study the modes of cognitive awareness in each state. Certain observations have even led these scientists to hypothesize that since there is little cognition going on during REM sleep, the increased thalamocortical responses show the action of processing in the waking preconscious. The thalamus and cortex are important anatomical features in cognitive and sensory awareness. The understanding of how these neurons fire and relate to one other in each of these states (REM and Waking) is paramount to understanding awareness and its relation to neural binding. In the waking state, neuronal activity in animals is subject to changes based on the current environment. Changes in environment act as a form of stress on the brain so that when sensory neurons are then fired synchronously, they acclimate to the new state. This new state can then be moved to the hippocampus where it can be stored for later use. In the words of James Newman and Anthony A. Grace in their article, \"Binding Across Time\" this idea is put forth: \"The hippocampus is the primary recipient of inferotemporal outputs and is known to be the substrate for the consolidation of working memories to long term, episodic memories.\" The logging of \"episodes\" is then used for \"streaming\", which can mediate by the selective gating of certain information reentering sensory awareness. Streaming and building of episodic memories would not be possible if neural binding did not unconsciously connect the two synchronous oscillations. The pairing of these oscillations can then help input the correct sensory material. If these paired oscillations are not new, then cognitively these firings will be easily understood. If there are new firings, the brain will have to acclimate to the new understanding. In REM sleep, the only extreme difference from the waking state is that the brain does not have the actual waking amount of sensory firings, so cognitively, there is not as much awareness here, although the activity of the \"brain\u2019s eye\" is still quite significant and very similar to the waking state. Studies have shown that during sleep there are still 40\u00a0Hz Oscillation firings. These firings are due to the perceived stimuli happening in dreams. \"",
            "score": 83.81735372543335
        },
        {
            "docid": "2567511_11",
            "document": "Neural engineering . Engineers employ quantitative tools that can be used for understanding and interacting with complex neural systems. Methods of studying and generating chemical, electrical, magnetic, and optical signals responsible for extracellular field potentials and synaptic transmission in neural tissue aid researchers in the modulation of neural system activity (Babb et al. 2008).  To understand properties of neural system activity, engineers use signal processing techniques and computational modeling (Eliasmith & Anderson 2003). To process these signals, neural engineers must translate the voltages across neural membranes into corresponding code, a process known as neural coding. Neural coding uses studies on how the brain encodes simple commands in the form of central pattern generators (CPGs), movement vectors, the cerebellar internal model, and somatotopic maps to understand movement and sensory phenomena. Decoding of these signals in the realm of neuroscience is the process by which neurons understand the voltages that have been transmitted to them. Transformations involve the mechanisms that signals of a certain form get interpreted and then translated into another form. Engineers look to mathematically model these transformations (Eliasmith & Anderson 2003).  There are a variety of methods being used to record these voltage signals. These can be intracellular or extracellular. Extracellular methods involve single-unit recordings, extracellular field potentials, and amperometry; more recently, multielectrode arrays have been used to record and mimic signals.",
            "score": 65.64910864830017
        },
        {
            "docid": "5128182_13",
            "document": "Encoding (memory) . Encoding is achieved using a combination of chemicals and electricity. Neurotransmitters are released when an electrical pulse crosses the synapse which serves as a connection from nerve cells to other cells. The dendrites receive these impulses with their feathery extensions. A phenomenon called long-term potentiation allows a synapse to increase strength with increasing numbers of transmitted signals between the two neurons. For that to happen, NMDA receptor, which influences the flow of information between neurons by controlling the initiation of long-term potentiation in most hippocampal pathways, need to come to the play. For these NMDA receptors to be activated, there must be two conditions. Firstly, glutamate has to be released and bound to the NMDA receptor site on postsynaptic neurons. Secondly, excitation has to take place in postsynaptic neurons. These cells also organise themselves into groups specializing in different kinds of information processing. Thus, with new experiences the brain creates more connections and may 'rewire'. The brain organizes and reorganizes itself in response to one's experiences, creating new memories prompted by experience, education, or training. Therefore, the use of a brain reflects how it is organised. This ability to re-organize is especially important if ever a part of the brain becomes damaged. Scientists are unsure of whether the stimuli of what we do not recall are filtered out at the sensory phase or if they are filtered out after the brain examines their significance.",
            "score": 65.42502880096436
        },
        {
            "docid": "32018467_7",
            "document": "Christian Keysers . After finishing his master, Christian Keysers decided to concentrate on a subfield of cognitive neuroscience called social neuroscience that uses neuroscience methods to understand how we process the social world. He therefore performed his doctoral studies at the University of St Andrews with David Ian Perrett, one of the founding father of the field, to understand how the brain processes faces and facial expressions. This thesis work led to new insights into how quickly the brain can process the faces of others. During this period, Keysers became fascinated with the question of how the brain can attach meaning to the faces of others. How is it for instance, that we understand that a certain grimace would signal that another person is happy? How do we understand that a certain bodily movement towards a glass indicates that the other person aims to grasp a glass? In 1999, Keysers was exposed to a visit of Vittorio Gallese, who presented his recent discovery of mirror neurons in the Psychology department lecture series. This deeply influenced Keysers who decided to move to the lab of Giacomo Rizzolatti to undertake further studies on how these fascinating neurons could contribute to social perception. In 2000, after finishing his doctorate, Christian Keysers moved to the University of Parma to study mirror neurons. In early work there demonstrated that mirror neurons in the premotor cortex not only respond to the sight of actions, but also when actions can only be deduced or heard, leading to a publication in the journal \"Science\". This work had tremendous impact on the field, as it suggested that the premotor cortex could play a central, modality independent role in perception and may lay the origin for the evolution of speech in humans.  Together this work indicated that brain regions involved in our own actions play a role in how we process the actions of others. Keysers wondered whether a similar principle may underlie how we process the tactile sensations and emotions of others, and became increasingly independent of the research focus on the motor system in Parma. At the time, Keysers had also met his to be wife, Valeria Gazzola, a biologist in the final phases of her studies, and together they decided to explore if the somatosensory system might be involved in perceiving the sensations of others. Via a fruitful collaboration with the French neuroimaging specialist Bruno Wicker, they used functional magnetic resonance imaging, and showed for the first time, that the secondary somatosensory cortex, previously thought only to represent a persons own experiences of touch, is also activated when seeing someone or something else be touched. They also showed that the insula, thought only to respond to the experience of first-hand emotions, was also activated when we see another individual experience similar emotions. Together this indicated a much more general principle than the original mirror neuron theory, in which people process the actions, sensations and emotions of others by vicariously activating owns own actions, sensations and emotions. Jointly, this work laid the foundation of the neuroscientific investigation of empathy.",
            "score": 68.5846289396286
        },
        {
            "docid": "20320137_5",
            "document": "Cohort model . The cohort model consists of three stages: access, selection, and integration. Under this model, auditory lexical retrieval begins with the first one or two speech segments, or phonemes, reach the hearer's ear, at which time the mental lexicon activates every possible word that begins with that speech segment. This occurs during the \"access stage\" and all of the possible words are known as the cohort. The words that are activated by the speech signal but are not the intended word are often called \"competitors.\" Identification of the target word is more difficult with more competitors. As more speech segments enter the ear and stimulate more neurons, causing the competitors that no longer match the input to be \"kicked out\" or to decrease in activation. The processes by which words are activated and competitors rejected in the cohort model are frequently called \"activation and selection\" or \"recognition and competition.\" These processes continue until an instant, called the \"recognition point\", at which only one word remains activated and all competitors have been kicked out. The recognition point process is initiated within the first 200 to 250 milliseconds of the onset of the given word. This is also known as the uniqueness point and it is the point where the most processing occurs. Moreover, there is a difference in the way a word is processed before it reaches its recognition point and afterwards. One can look at the process prior to reaching the recognition point as bottom-up, where the phonemes are used to access the lexicon. The post recognition point process is top-down, because the information concerning the chosen word is tested against the word that is presented. The selection stage occurs when only one word is left from the set. Finally, in the integration stage, the semantic and syntactic properties of activated words are incorporated into the high-level utterance representation.",
            "score": 94.9487898349762
        },
        {
            "docid": "37689509_2",
            "document": "Neuroscience of rhythm . The neuroscience of rhythm refers to the various forms of rhythm generated by the central nervous system (CNS). Nerve cells, also known as neurons in the human brain are capable of firing in specific patterns which cause oscillations. The brain possesses many different types of oscillators with different periods. Oscillators are simultaneously outputting frequencies from .02\u00a0Hz to 600\u00a0Hz. It is now well known that a computer is capable of running thousands of processes with just one high frequency clock. Humans have many different clocks as a result of evolution. Prior organisms had no need for a fast responding oscillator. This multi-clock system permits quick response to constantly changing sensory input while still maintaining the autonomic processes that sustain life. This method modulates and controls a great deal of bodily functions.",
            "score": 129.26084971427917
        },
        {
            "docid": "5366050_50",
            "document": "Speech perception . Neurophysiological methods rely on utilizing information stemming from more direct and not necessarily conscious (pre-attentative) processes. Subjects are presented with speech stimuli in different types of tasks and the responses of the brain are measured. The brain itself can be more sensitive than it appears to be through behavioral responses. For example, the subject may not show sensitivity to the difference between two speech sounds in a discrimination test, but brain responses may reveal sensitivity to these differences. Methods used to measure neural responses to speech include event-related potentials, magnetoencephalography, and near infrared spectroscopy. One important response used with event-related potentials is the mismatch negativity, which occurs when speech stimuli are acoustically different from a stimulus that the subject heard previously.",
            "score": 97.25568521022797
        },
        {
            "docid": "2860457_6",
            "document": "Neural ensemble . Neuronal ensembles encode information in a way somewhat similar to the principle of Wikipedia operation \u2013 multiple edits by many participants. Neuroscientists have discovered that individual neurons are very noisy. For example, by examining the activity of only a single neuron in the visual cortex, it is very difficult to reconstruct the visual scene that the owner of the brain is looking at. Like a single Wikipedia participant, an individual neuron does not 'know' everything and is likely to make mistakes. This problem is solved by the brain having billions of neurons. Information processing by the brain is population processing, and it is also distributed \u2013 in many cases each neuron knows a little bit about everything, and the more neurons participate in a job, the more precise the information encoding. In the distributed processing scheme, individual neurons may exhibit neuronal noise, but the population as a whole averages this noise out.",
            "score": 67.2938984632492
        },
        {
            "docid": "620396_42",
            "document": "Origin of language . In humans, functional MRI studies have reported finding areas homologous to the monkey mirror neuron system in the inferior frontal cortex, close to Broca's area, one of the language regions of the brain. This has led to suggestions that human language evolved from a gesture performance/understanding system implemented in mirror neurons. Mirror neurons have been said to have the potential to provide a mechanism for action-understanding, imitation-learning, and the simulation of other people's behavior. This hypothesis is supported by some cytoarchitectonic homologies between monkey premotor area F5 and human Broca's area. Rates of vocabulary expansion link to the ability of children to vocally mirror non-words and so to acquire the new word pronunciations. Such speech repetition occurs automatically, quickly and separately in the brain to speech perception. Moreover, such vocal imitation can occur without comprehension such as in speech shadowing and echolalia. Further evidence for this link comes from a recent study in which the brain activity of two participants was measured using fMRI while they were gesturing words to each other using hand gestures with a game of charades\u2014a modality that some have suggested might represent the evolutionary precursor of human language. Analysis of the data using Granger Causality revealed that the mirror-neuron system of the observer indeed reflects the pattern of activity of in the motor system of the sender, supporting the idea that the motor concept associated with the words is indeed transmitted from one brain to another using the mirror system.",
            "score": 107.97953569889069
        },
        {
            "docid": "2860430_37",
            "document": "Neural oscillation . Neuronal spiking is generally considered the basis for information transfer in the brain. For such a transfer, information needs to be coded in a spiking pattern. Different types of coding schemes have been proposed, such as rate coding and temporal coding. Neural oscillations could create periodic time windows in which input spikes have larger effect on neurons, thereby providing a mechanism for decoding temporal codes.",
            "score": 60.83641314506531
        },
        {
            "docid": "43527201_5",
            "document": "Usha Goswami . Dyslexia is a disorder in which the person affected has difficulty reading due to the reversal of letters in the brain that isn't linked to intelligence. In people with dyslexia, the brain processes certain signals in a specific way making it a very specific learning difficulty. Dr. Goswami's research is concerned with focusing on dyslexia as a language disorder rather than a visual disorder as she has found that the way that children with dyslexia hear language is slightly different than others. When sound waves approach the brain, they vary in pressure depending on the syllables within the words being spoken creating a rhythm. When these signals reach the brain they are lined up with speech rhythms and this process doesn't work properly in those with dyslexia. Goswami is currently researching whether or not reading poetry, nursery rhymes, and singing can be used to help children with dyslexia. The rhythm of the words could allow the child to match the syllable patterns to language before they begin reading as to catch them up to where children without the disability might be.",
            "score": 142.36252450942993
        },
        {
            "docid": "33246145_4",
            "document": "Neural decoding . When looking at a picture, people's brains are constantly making decisions about what object they are looking at, where they need to move their eyes next, and what they find to be the most salient aspects of the input stimulus. As these images hit the back of the retina, these stimuli are converted from varying wavelengths to a series of neural spikes called action potentials. These pattern of action potentials are different for different objects and different colors; we therefore say that the neurons are encoding objects and colors by varying their spike rates or temporal pattern. Now, if someone were to probe the brain by placing electrodes in the primary visual cortex, they may find what appears to be random electrical activity. These neurons are actually firing in response to the lower level features of visual input, possibly the edges of a picture frame. This highlights the crux of the neural decoding hypothesis: that it is possible to reconstruct a stimulus from the response of the ensemble of neurons that represent it. In other words, it is possible to look at spike train data and say that the person or animal being recorded is looking at a red ball.",
            "score": 71.02802300453186
        },
        {
            "docid": "2088_20",
            "document": "Aphasia . There have been many instances showing that there is a form of aphasia among deaf individuals. Sign languages are, after all, forms of language that have been shown to use the same areas of the brain as verbal forms of language. Mirror neurons become activated when an animal is acting in a particular way or watching another individual act in the same manner. These mirror neurons are important in giving an individual the ability to mimic movements of hands. Broca's area of speech production has been shown to contain several of these mirror neurons resulting in significant similarities of brain activity between sign language and vocal speech communication. Facial communication is a significant portion of how animals interact with each other. Humans use facial movements to create, what other humans perceive, to be faces of emotions. While combining these facial movements with speech, a more full form of language is created which enables the species to interact with a much more complex and detailed form of communication. Sign language also uses these facial movements and emotions along with the primary hand movement way of communicating. These facial movement forms of communication come from the same areas of the brain. When dealing with damages to certain areas of the brain, vocal forms of communication are in jeopardy of severe forms of aphasia. Since these same areas of the brain are being used for sign language, these same, at least very similar, forms of aphasia can show in the Deaf community. Individuals can show a form of Wernicke's aphasia with sign language and they show deficits in their abilities in being able to produce any form of expressions. Broca's aphasia shows up in some people, as well. These individuals find tremendous difficulty in being able to actually sign the linguistic concepts they are trying to express.",
            "score": 100.4230694770813
        },
        {
            "docid": "5198024_2",
            "document": "Efficient coding hypothesis . The efficient coding hypothesis was proposed by Horace Barlow in 1961 as a theoretical model of sensory coding in the brain. Within the brain, neurons often communicate with one another by sending electrical impulses referred to as action potentials or spikes. One goal of sensory neuroscience is to decipher the meaning of these spikes in order to understand how the brain represents and processes information about the outside world. Barlow hypothesized that the spikes in the sensory system formed a neural code for efficiently representing sensory information. By efficient Barlow meant that the code minimized the number of spikes needed to transmit a given signal. This is somewhat analogous to transmitting information across the internet, where different file formats can be used to transmit a given image. Different file formats require different number of bits for representing the same image at given distortion level, and some are better suited for representing certain classes of images than others. According to this model, the brain is thought to use a code which is suited for representing visual and audio information representative of an organism's natural environment.",
            "score": 70.54021310806274
        },
        {
            "docid": "9916386_18",
            "document": "Synaptic gating . It has been shown that nucleus accumbens neurons are capable of being gated because they are bistable. Recent evidence has shown that neurons in the cortex are also bistable, and thus also able to be gated. There appear to be three different types of gating circuits \u2013 one that is controlled by the cortex, one that is controlled by the association nuclei in the thalamus, and one that is controlled by circuits spanning the basal ganglia, cortex, and the thalamus. Strong evidence has concluded that gating from thalamus impacts the prefrontal cortex response from the hippocampus. This is seen as either an enhancement or a suppression illustrating the bistability of the process. It has been proven that nucleus accumbens neurons act as a gate yet do the neurons in the cortex act in the same manner? Future research will look at similarities between the two sets of bistable neurons. In addition, the mechanism of shifting the bistable neurons to their \u201cdown\u201d state needs to be expanded upon. This state leads to inhibition and thus are their inhibitory interneurons that modulate this shift and if so are inhibitory neurotransmitters such as GABA involved? Lastly, neurons capable of modulating gates such as hippocampal and thalamic neurons can contact many different areas of the brain. With increasing research saying that neurons in the cortex, nucleus accumbens, and cerebellum are all able to be gated, can the hippocampus modulate the signals for all of these and if so can it connect these different brain areas into a much larger neural network capable of being modulated all at once? These are the questions at the heart of synaptic gating in the future.",
            "score": 76.45500636100769
        },
        {
            "docid": "2860430_23",
            "document": "Neural oscillation . A neural network model describes a population of physically interconnected neurons or a group of disparate neurons whose inputs or signalling targets define a recognizable circuit. These models aim to describe how the dynamics of neural circuitry arise from interactions between individual neurons. Local interactions between neurons can result in the synchronization of spiking activity and form the basis of oscillatory activity. In particular, models of interacting pyramidal cells and inhibitory interneurons have been shown to generate brain rhythms such as gamma activity.",
            "score": 74.38378095626831
        },
        {
            "docid": "2860430_32",
            "document": "Neural oscillation . Next to evoked activity, neural activity related to stimulus processing may result in induced activity. Induced activity refers to modulation in ongoing brain activity induced by processing of stimuli or movement preparation. Hence, they reflect an indirect response in contrast to evoked responses. A well-studied type of induced activity is amplitude change in oscillatory activity. For instance, gamma activity often increases during increased mental activity such as during object representation. Because induced responses may have different phases across measurements and therefore would cancel out during averaging, they can only be obtained using time-frequency analysis. Induced activity generally reflects the activity of numerous neurons: amplitude changes in oscillatory activity are thought to arise from the synchronization of neural activity, for instance by synchronization of spike timing or membrane potential fluctuations of individual neurons. Increases in oscillatory activity are therefore often referred to as event-related synchronization, while decreases are referred to as event-related desynchronization.",
            "score": 84.65902745723724
        },
        {
            "docid": "355240_5",
            "document": "Cognitive model . A number of key terms are used to describe the processes involved in the perception, storage, and production of speech. Typically, they are used by speech pathologists while treating a child patient. The input signal is the speech signal heard by the child, usually assumed to come from an adult speaker. The output signal is the utterance produced by the child. The unseen psychological events that occur between the arrival of an input signal and the production of speech are the focus of psycholinguistic models. Events that process the input signal are referred to as input processes, whereas events that process the production of speech are referred to as output processes. Some aspects of speech processing are thought to happen online\u2014that is, they occur during the actual perception or production of speech and thus require a share of the attentional resources dedicated to the speech task. Other processes, thought to happen offline, take place as part of the child\u2019s background mental processing rather than during the time dedicated to the speech task. In this sense, online processing is sometimes defined as occurring in real-time, whereas offline processing is said to be time-free (Hewlett, 1990). In box-and-arrow psycholinguistic models, each hypothesized level of representation or processing can be represented in a diagram by a \u201cbox,\u201d and the relationships between them by \u201carrows,\u201d hence the name. Sometimes (as in the models of Smith, 1973, and Menn, 1978, described later in this paper) the arrows represent processes additional to those shown in boxes. Such models make explicit the hypothesized information- processing activities carried out in a particular cognitive function (such as language), in a manner analogous to computer flowcharts that depict the processes and decisions carried out by a computer program. Box-and-arrow models differ widely in the number of unseen psychological processes they describe and thus in the number of boxes they contain. Some have only one or two boxes between the input and output signals (e.g., Menn, 1978; Smith, 1973), whereas others have multiple boxes representing complex relationships between a number of different information-processing events (e.g., Hewlett, 1990; Hewlett, Gibbon, & Cohen- McKenzie,1998; Stackhouse & Wells, 1997). The most important box, however, and the source of much ongoing debate, is that representing the underlying representation (or UR). In essence, an underlying representation captures information stored in a child\u2019s mind about a word he or she knows and uses. As the following description of several models will illustrate, the nature of this information and thus the type(s) of representation present in the child\u2019s knowledge base have captured the attention of researchers for some time. (Elise Baker et al. Psycholinguistic Models of Speech Development and Their Application to Clinical Practice. Journal of Speech, Language, and Hearing Research. June 2001. 44. p 685\u2013702.)",
            "score": 90.51970446109772
        },
        {
            "docid": "31075772_15",
            "document": "Thought identification . On 31 January 2012 Brian Pasley and colleagues of University of California Berkeley published their paper in PLoS Biology wherein subjects' internal neural processing of auditory information was decoded and reconstructed as sound on computer by gathering and analyzing electrical signals directly from subjects' brains. The research team conducted their studies on the superior temporal gyrus, a region of the brain that is involved in higher order neural processing to make semantic sense from auditory information. The research team used a computer model to analyze various parts of the brain that might be involved in neural firing while processing auditory signals. Using the computational model, scientists were able to identify the brain activity involved in processing auditory information when subjects were presented with recording of individual words. Later, the computer model of auditory information processing was used to reconstruct some of the words back into sound based on the neural processing of the subjects. However the reconstructed sounds were not of good quality and could be recognized only when the audio wave patterns of the reconstructed sound were visually matched with the audio wave patterns of the original sound that was presented to the subjects. However this research marks a direction towards more precise identification of neural activity in cognition.",
            "score": 98.36402630805969
        },
        {
            "docid": "14241792_3",
            "document": "TRACE (psycholinguistics) . TRACE was created during the formative period of connectionism, and was included as a chapter in \"Parallel Distributed Processing: Explorations in the Microstructures of Cognition\". The researchers found that certain problems regarding speech perception could be conceptualized in terms of a connectionist interactive activation model. The problems were that (1) speech is extended in time, (2) the sounds of speech (phonemes) overlap with each other, (3) the articulation of a speech sound is affected by the sounds that come before and after it, and (4) there is natural variability in speech (e.g. foreign accent) as well as noise in the environment (e.g. busy restaurant). Each of these causes the speech signal to be complex and often ambiguous, making it difficult for the human mind/brain to decide what words it is really hearing. In very simple terms, an interactive activation model solves this problem by placing different kinds of processing units (phonemes, words) in isolated layers, allowing activated units to pass information between layers, and having units within layers compete with one another, until the \u201cwinner\u201d is considered \u201crecognized\u201d by the model.",
            "score": 108.34571266174316
        },
        {
            "docid": "14532984_2",
            "document": "Coincidence detection in neurobiology . Coincidence detection in the context of neurobiology is a process by which a neuron or a neural circuit can encode information by detecting the occurrence of temporally close but spatially distributed input signals. Coincidence detectors influence neuronal information processing by reducing temporal jitter, reducing spontaneous activity, and forming associations between separate neural events. This concept has led to a greater understanding of neural processes and the formation of computational maps in the brain.",
            "score": 53.89132237434387
        },
        {
            "docid": "36058569_5",
            "document": "Frank H. Guenther . Frank Guenther\u2019s research is aimed at uncovering the neural computations underlying the processing of speech by the human brain. He is the originator of the Directions Into Velocities of Articulators (DIVA) model, which is currently the leading model of the neural computations underlying speech production. This model mathematically characterizes the computations performed by each brain region involved in speech production as well as the function of the interconnections between these regions. The model has been supported by a wide range of experimental tests of model predictions, including electromagnetic articulometry studies investigating speech movements, auditory perturbation studies involving modification of a speaker\u2019s feedback of his/her own speech in real time, and functional magnetic resonance imaging studies of brain activity during speech, though some parts of the model remain to be experimentally verified. The DIVA model has been used to investigate the neural underpinnings of a number of communication disorders, including stuttering apraxia of speech, and hearing-impaired speech.",
            "score": 109.84624528884888
        },
        {
            "docid": "18614_35",
            "document": "Language acquisition . Prosody is the property of speech that conveys an emotional state of the utterance, as well as intended form of speech (whether it be a question, statement or command). Some researchers in the field of developmental neuroscience would argue that fetal auditory learning mechanisms are solely due to discrimination in prosodic elements. Although this would hold merit in an evolutionary psychology perspective (i.e. recognition of mother's voice/familiar group language from emotionally valent stimuli), some theorists argue that there is more than prosodic recognition in elements of fetal learning. Newer evidence shows that fetuses not only react to the native language differently from nonnative, but furthermore that fetuses react differently and can accurately discriminate between native and nonnative vowels (Moon, Lagercrantz, & Kuhl, 2013). Furthermore, a new study in 2016 showed that newborn infants encode the edges of multisyllabic sequences better than the internal components of the sequence (Ferry et al., 2016). Together, these results suggest that newborn infants have learned important properties of syntactic processing in utero, that can be seen in infant knowledge of native language vowels and the sequencing of heard multisyllabic phrases. This ability to sequence specific vowels gives newborn infants some of the fundamental mechanisms needed in order to learn the complex organization of a language.  From a neuroscientific perspective, there are neural correlates have been found that demonstrate human fetal learning of speech-like auditory stimulus that most other studies have been analyzing (Partanen et al., 2013). In a study conducted by Partanen et al. (2013), researchers presented fetuses with certain word variants and saw that these fetuses exhibited higher brain activity to the certain word variants compared to controls. In this same study, there was \"a significant correlation existed between the amount of prenatal exposure and brain activity, with greater activity being associated with a higher amount of prenatal speech exposure,\" pointing to the important learning mechanisms present before birth that is fine-tuned to features in speech (Partanen et al., 2013).",
            "score": 119.59172177314758
        },
        {
            "docid": "6147487_24",
            "document": "Neural coding . Neurons exhibit high-frequency fluctuations of firing-rates which could be noise or could carry information. Rate coding models suggest that these irregularities are noise, while temporal coding models suggest that they encode information. If the nervous system only used rate codes to convey information, a more consistent, regular firing rate would have been evolutionarily advantageous, and neurons would have utilized this code over other less robust options. Temporal coding supplies an alternate explanation for the \u201cnoise,\" suggesting that it actually encodes information and affects neural processing. To model this idea, binary symbols can be used to mark the spikes: 1 for a spike, 0 for no spike. Temporal coding allows the sequence 000111000111 to mean something different from 001100110011, even though the mean firing rate is the same for both sequences, at 6 spikes/10\u00a0ms. Until recently, scientists had put the most emphasis on rate encoding as an explanation for post-synaptic potential patterns. However, functions of the brain are more temporally precise than the use of only rate encoding seems to allow. In other words, essential information could be lost due to the inability of the rate code to capture all the available information of the spike train. In addition, responses are different enough between similar (but not identical) stimuli to suggest that the distinct patterns of spikes contain a higher volume of information than is possible to include in a rate code.",
            "score": 90.3445258140564
        },
        {
            "docid": "433584_4",
            "document": "McGurk effect . Vision is the primary sense for humans, but speech perception is multimodal, which means that it involves information from more than one sensory modality, in particular, audition and vision. The McGurk effect arises during phonetic processing because the integration of audio and visual information happens early in speech perception. The McGurk effect is very robust; that is, knowledge about it seems to have little effect on one's perception of it. This is different from certain optical illusions, which break down once one 'sees through' them. Some people, including those that have been researching the phenomenon for more than twenty years, experience the effect even when they are aware that it is taking place. With the exception of people who can identify most of what is being said from speech-reading alone, most people are quite limited in their ability to identify speech from visual-only signals. A more extensive phenomenon is the ability of visual speech to increase the intelligibility of heard speech in a noisy environment. Visible speech can also alter the perception of perfectly audible speech sounds when the visual speech stimuli are mismatched with the auditory speech. Normally, speech perception is thought to be an auditory process; however, our use of information is immediate, automatic, and, to a large degree, unconscious and therefore, despite what is widely accepted as true, speech is not only something we hear. Speech is perceived by all of the senses working together (seeing, touching, and listening to a face move). The brain is often unaware of the separate sensory contributions of what it perceives. Therefore, when it comes to recognizing speech the brain cannot differentiate whether it is seeing or hearing the incoming information.",
            "score": 93.71358895301819
        },
        {
            "docid": "1896271_2",
            "document": "Holonomic brain theory . The holonomic brain theory, developed by neuroscientist Karl Pribram initially in collaboration with physicist David Bohm, is a model of human cognition that describes the brain as a holographic storage network. Pribram suggests these processes involve electric oscillations in the brain's fine-fibered dendritic webs, which are different from the more commonly known action potentials involving axons and synapses. These oscillations are waves and create wave interference patterns in which memory is encoded naturally, and the waves may be analyzed by a Fourier transform. Gabor, Pribram and others noted the similarities between these brain processes and the storage of information in a hologram, which can also be analyzed with a Fourier transform. In a hologram, any part of the hologram with sufficient size contains the whole of the stored information. In this theory, a piece of a long-term memory is similarly distributed over a dendritic arbor so that each part of the dendritic network contains all the information stored over the entire network. This model allows for important aspects of human consciousness, including the fast associative memory that allows for connections between different pieces of stored information and the non-locality of memory storage (a specific memory is not stored in a specific location, i.e. a certain neuron).",
            "score": 102.8133978843689
        },
        {
            "docid": "3975854_2",
            "document": "Sensory neuroscience . Sensory neuroscience is a subfield of neuroscience which explores the anatomy and physiology of neurons that are part of sensory systems such as vision, hearing, and olfaction. Neurons in sensory regions of the brain respond to stimuli by firing one or more nerve impulses (action potentials) following stimulus presentation. How is information about the outside world encoded by the rate, timing, and pattern of action potentials? This so-called neural code is currently poorly understood and sensory neuroscience plays an important role in the attempt to decipher it. Looking at early sensory processing is advantageous since brain regions that are \"higher up\" (e.g. those involved in memory or emotion) contain neurons which encode more abstract representations. However, the hope is that there are unifying principles which govern how the brain encodes and processes information. Studying sensory systems is an important stepping stone in our understanding of brain function in general.",
            "score": 66.09266078472137
        }
    ],
    "r": [
        {
            "docid": "43527201_5",
            "document": "Usha Goswami . Dyslexia is a disorder in which the person affected has difficulty reading due to the reversal of letters in the brain that isn't linked to intelligence. In people with dyslexia, the brain processes certain signals in a specific way making it a very specific learning difficulty. Dr. Goswami's research is concerned with focusing on dyslexia as a language disorder rather than a visual disorder as she has found that the way that children with dyslexia hear language is slightly different than others. When sound waves approach the brain, they vary in pressure depending on the syllables within the words being spoken creating a rhythm. When these signals reach the brain they are lined up with speech rhythms and this process doesn't work properly in those with dyslexia. Goswami is currently researching whether or not reading poetry, nursery rhymes, and singing can be used to help children with dyslexia. The rhythm of the words could allow the child to match the syllable patterns to language before they begin reading as to catch them up to where children without the disability might be.",
            "score": 142.36251831054688
        },
        {
            "docid": "994097_20",
            "document": "Auditory cortex . The auditory cortex has distinct responses to sounds in the gamma band. When subjects are exposed to three or four cycles of a 40 hertz click, an abnormal spike appears in the EEG data, which is not present for other stimuli. The spike in neuronal activity correlating to this frequency is not restrained to the tonotopic organization of the auditory cortex. It has been theorized that gamma frequencies are resonant frequencies of certain areas of the brain, and appear to affect the visual cortex as well. Gamma band activation (25 to 100\u00a0Hz) has been shown to be present during the perception of sensory events and the process of recognition. In a 2000 study by Kneif and colleagues, subjects were presented with eight musical notes to well-known tunes, such as \"Yankee Doodle\" and \"Fr\u00e8re Jacques\". Randomly, the sixth and seventh notes were omitted and an electroencephalogram, as well as a magnetoencephalogram were each employed to measure the neural results. Specifically, the presence of gamma waves, induced by the auditory task at hand, were measured from the temples of the subjects. The OSP response, or omitted stimulus response, was located in a slightly different position; 7\u00a0mm more anterior, 13\u00a0mm more medial and 13\u00a0mm more superior in respect to the complete sets. The OSP recordings were also characteristically lower in gamma waves, as compared to the complete musical set. The evoked responses during the sixth and seventh omitted notes are assumed to be imagined, and were characteristically different, especially in the right hemisphere. The right auditory cortex has long been shown to be more sensitive to tonality, while the left auditory cortex has been shown to be more sensitive to minute sequential differences in sound, such as in speech.",
            "score": 138.34115600585938
        },
        {
            "docid": "2860430_6",
            "document": "Neural oscillation . Neural oscillations have been most widely studied in neural activity generated by large groups of neurons. Large-scale activity can be measured by techniques such as EEG. In general, EEG signals have a broad spectral content similar to pink noise, but also reveal oscillatory activity in specific frequency bands. The first discovered and best-known frequency band is alpha activity (7.5\u201312.5 Hz) that can be detected from the occipital lobe during relaxed wakefulness and which increases when the eyes are closed. Other frequency bands are: delta (1\u20134\u00a0Hz), theta (4\u20138\u00a0Hz), beta (13\u201330\u00a0Hz), low gamma (30\u201370\u00a0Hz), and high gamma (70\u2013150\u00a0Hz) frequency bands, where faster rhythms such as gamma activity have been linked to cognitive processing. Indeed, EEG signals change dramatically during sleep and show a transition from faster frequencies to increasingly slower frequencies such as alpha waves. In fact, different sleep stages are commonly characterized by their spectral content. Consequently, neural oscillations have been linked to cognitive states, such as awareness and consciousness.",
            "score": 133.92315673828125
        },
        {
            "docid": "3071594_3",
            "document": "Theta wave . In rats, the most frequently studied species, theta wave rhythmicity is easily observed in the hippocampus, but can also be detected in numerous other cortical and subcortical brain structures. Hippocampal theta waves, with a frequency range of 6\u201310\u00a0Hz, appear when a rat is engaged in active motor behavior such as walking or exploratory sniffing, and also during REM sleep. Theta waves with a lower frequency range, usually around 6\u20137\u00a0Hz, are sometimes observed when a rat is motionless but alert. When a rat is eating, grooming, or sleeping, the hippocampal EEG usually shows a non-rhythmic pattern known as large irregular activity or \"LIA\". The hippocampal theta rhythm depends critically on projections from the medial septal area, which in turn receives input from the hypothalamus and several brainstem areas. Hippocampal theta rhythms in other species differ in some respects from those in rats. In cats and rabbits, the frequency range is lower (around 4\u20136\u00a0Hz), and theta is less strongly associated with movement than in rats. In bats, theta appears in short bursts associated with echolocation. In humans, hippocampal theta rhythm has been observed and linked to \"memory formation\" and \"navigation\".",
            "score": 133.72975158691406
        },
        {
            "docid": "3071594_2",
            "document": "Theta wave . Theta waves generate the theta rhythm, a neural oscillatory pattern that can be seen on an electroencephalogram (EEG), recorded either from inside the brain or from electrodes attached to the scalp. Two types of theta rhythm have been described. The \"\"hippocampal theta rhythm\"\" is a strong oscillation that can be observed in the hippocampus and other brain structures in numerous species of mammals including rodents, rabbits, dogs, cats, bats, and marsupials. \"\"Cortical theta rhythms\"\" are low-frequency components of scalp EEG, usually recorded from humans. Theta rhythms can be quantified using quantitative electroencephalography (qEEG) using freely available toolboxes, such as, EEGLAB or the Neurophysiological Biomarker Toolbox (NBT).",
            "score": 131.55479431152344
        },
        {
            "docid": "37689509_2",
            "document": "Neuroscience of rhythm . The neuroscience of rhythm refers to the various forms of rhythm generated by the central nervous system (CNS). Nerve cells, also known as neurons in the human brain are capable of firing in specific patterns which cause oscillations. The brain possesses many different types of oscillators with different periods. Oscillators are simultaneously outputting frequencies from .02\u00a0Hz to 600\u00a0Hz. It is now well known that a computer is capable of running thousands of processes with just one high frequency clock. Humans have many different clocks as a result of evolution. Prior organisms had no need for a fast responding oscillator. This multi-clock system permits quick response to constantly changing sensory input while still maintaining the autonomic processes that sustain life. This method modulates and controls a great deal of bodily functions.",
            "score": 129.26084899902344
        },
        {
            "docid": "3071594_9",
            "document": "Theta wave . In the oldest EEG literature dating back to the 1920s, Greek letters such as alpha, beta, theta, and gamma were used to classify EEG waves falling into specific frequency ranges, with \"theta\" generally meaning a range of about 4\u20137 cycles per second (Hz). In the 1930s\u20131950s, a very strong rhythmic oscillation pattern was discovered in the hippocampus of cats and rabbits (Green & Arduini, 1954). In these species, the hippocampal oscillations fell mostly into the 4\u20136\u00a0Hz frequency range, so they were referred to as \"theta\" oscillations. Later, hippocampal oscillations of the same type were observed in rats; however, the frequency of rat hippocampal EEG oscillations averaged about 8\u00a0Hz and rarely fell below 6\u00a0Hz. Thus the rat hippocampal EEG oscillation should not, strictly speaking, have been called a \"theta rhythm\". However the term \"theta\" had already become so strongly associated with hippocampal oscillations that it continued to be used even for rats. Over the years this association has come to be stronger than the original association with a specific frequency range, but the original meaning also persists.",
            "score": 127.03068542480469
        },
        {
            "docid": "47434598_2",
            "document": "Gy\u00f6rgy Buzs\u00e1ki . Gy\u00f6rgy Buzs\u00e1ki (; born November 24, 1949, Kaposv\u00e1r, Hungary) is the Biggs Professor of Neuroscience at New York University School of Medicine. Dr. Buzs\u00e1ki\u2019s primary interests is \u201cneural syntax\u201d, i.e., how segmentation of neural information is organized by the numerous brain rhythms to support cognitive functions. He identified the cellular-synaptic basis of hippocampal theta, gamma oscillations and sharp waves with associated fast oscillations, their relationship to each other and to behavior and sleep. He was the first to demonstrate the role of GABAergic interneurons in network oscillations. Buzsaki\u2019s recognition of the importance hierarchical organization of brain rhythms of different frequencies and their cross-frequency coupling has opened up opportunities for the dissection of cognitive mechanisms in health and disease. His most influential work, the two-stage model of memory trace consolidation, demonstrates how the neocortex-mediated information during learning transiently modifies hippocampal networks, followed by reactivation and consolidation of these memory traces during sharp wave-ripple patterns of sleep. Buzsaki\u2019s demonstration that in the absence of changing environmental signals, cortical circuits continuously generate self-organized cell assembly sequences is an important link to the neuronal assembly basis of cognitive functions. His experiments demonstrated how skewed distribution of firing rates supports robustness, sensitivity, plasticity and stability in neuronal networks. He has pioneered numerous technical innovations, including large-scale recording methods using silicon chips and the NeuroGrid, an organic, comformable electrode system used in both animal and patients.",
            "score": 125.40196990966797
        },
        {
            "docid": "1673438_12",
            "document": "Gamma wave . As mentioned above, gamma waves have been observed in Tibetan Buddhist monks. A 2004 study took eight long-term Tibetan Buddhist practitioners of meditation and, using electrodes, monitored the patterns of electrical activity produced by their brains as they meditated. The researchers compared the brain activity of the monks to a group of novice meditators (the study had these subjects meditate an hour a day for one week prior to empirical observation). In a normal meditative state, both groups were shown to have similar brain activity. However, when the monks were told to generate an objective feeling of compassion during meditation, their brain activity began to fire in a rhythmic, coherent manner, suggesting neuronal structures were firing in harmony. This was observed at a frequency of 25\u201340\u00a0Hz, the rhythm of gamma waves. These gamma-band oscillations in the monk\u2019s brain signals were the largest seen in humans (apart from those in states such as seizures). Conversely, these gamma-band oscillations were scant in novice meditators. Though, a number of rhythmic signals did appear to strengthen in beginner meditators with further experience in the exercise, implying that the aptitude for one to produce gamma-band rhythm is trainable.",
            "score": 124.9523696899414
        },
        {
            "docid": "315084_23",
            "document": "Lip reading . Automated lip-reading has been a topic of interest in computational engineering, as well as in . The computational engineer Steve Omohundro, among others, pioneered its development. In facial animation, the aim is to generate realistic facial actions, especially mouth movements, that simulate human speech actions. Computer algorithms to deform or manipulate images of faces can be driven by heard or written language. Systems may be based on detailed models derived from facial movements (motion capture); on anatomical modelling of actions of the jaw, mouth and tongue; or on mapping of known viseme- phoneme properties. Facial animation has been used in speechreading training (demonstrating how different sounds 'look'). These systems are a subset of speech synthesis modelling which aim to deliver reliable 'text-to-(seen)-speech' outputs. A complementary aim\u2014the reverse of making faces move in speech\u2014is to develop computer algorithms that can deliver realistic interpretations of speech (i.e. a written transcript or audio record) from natural video data of a face in action: this is facial speech recognition. These models too can be sourced from a variety of data. Automatic visual speech recognition from video has been quite successful in distinguishing different languages (from a corpus of spoken language data). Demonstration models, using machine-learning algorithms, have had some success in lipreading speech elements, such as specific words, from video and for identifying hard-to-lipread phonemes from visemically similar seen mouth actions. Machine-based speechreading is now making successful use of neural-net based algorithms which use large databases of speakers and speech material (following the successful model for auditory automatic speech recognition).",
            "score": 123.71845245361328
        },
        {
            "docid": "34112463_5",
            "document": "Speech tempo . Various units of speech have been used as a basis for measurement. The traditional measure of speed in typing and Morse code transmission has been words per minute (wpm). However, in the study of speech the word is not well defined (being primarily a unit of grammar), and speech is not usually temporally stable over a period as long as a minute. Many studies have used the measure of syllables per second, but this is not completely reliable because, although the syllable as a phonological unit of a given language is well-defined, it is not always possible to get agreement on the phonetic syllable. For example, the English word 'particularly' in the form in which it occurs in dictionaries is, phonologically speaking, composed of five syllables /p\u0259.t\u026ak.j\u0259.l\u0259.li/. Phonetic realizations of the word, however, may be heard as comprising five [p\u0259.t\u026ak.j\u0259.l\u0259.li], four [p\u0259.t\u026ak.j\u0259.li], three [p\u0259.t\u026ak.li] or even two syllables [pt\u026ak.li], and listeners are likely to have different opinions about the number of syllables heard.",
            "score": 122.18846893310547
        },
        {
            "docid": "2956315_21",
            "document": "Two-streams hypothesis . Conduction aphasia affects a subject's ability to reproduce speech (typically by repetition), though it has no influence on the subject's ability to comprehend spoken language. This shows that conduction aphasia must reflect not an impairment of the ventral pathway but instead of the dorsal pathway. Hickok and Poeppel found that conduction aphasia can be the result of damage, particularly lesions, to the Spt (Sylvian parietal temporal). This is shown by the Spt's involvement in acquiring new vocabulary, for while experiments have shown that most conduction aphasiacs can repeat high-frequency, simple words, their ability to repeat low-frequency, complex words is impaired. The Spt is responsible for connecting the motor and auditory systems by making auditory code accessible to the motor cortex. It appears that the motor cortex recreates high-frequency, simple words (like \"cup\") in order to more quickly and efficiently access them, while low-frequency, complex words (like \"Sylvian parietal temporal\") require more active, online regulation by the Spt. This explains why conduction aphasiacs have particular difficulty with low-frequency words which requires a more hands-on process for speech production. \"Functionally, conduction aphasia has been characterized as a deficit in the ability to encode phonological information for production,\" namely because of a disruption in the motor-auditory interface. Conduction aphasia has been more specifically related to damage of the arcuate fasciculus, which is vital for both speech and language comprehension, as the arcuate fasiculus makes up the connection between Broca and Wernicke's areas.",
            "score": 121.45573425292969
        },
        {
            "docid": "8434151_3",
            "document": "Donald Shankweiler . Donald Shankweiler's research career has spanned a number of areas related speech perception, reading, and cognitive neuroscience. His main interests have been studying the acquisition of reading and writing, understanding disorders of reading, writing, and spoken language, and exploring the representation of spoken and written language in the brain. In the 1960s, Shankweiler and Michael Studdert-Kennedy used a dichotic listening technique (presenting different nonsense syllables simultaneously to opposite ears) to demonstrate the dissociation of phonetic (speech) and auditory (nonspeech) perception by finding that phonetic structure devoid of meaning is an integral part of language, typically processed in the left cerebral hemisphere. Alvin Liberman, Franklin S. Cooper, Shankweiler, and Studdert-Kennedy summarized and interpreted fifteen years of research in a paper \"Perception of the Speech Code,\" that argued for the motor theory of speech perception. This is still among the most cited papers in the speech literature. It set the agenda for many years of research at Haskins and elsewhere by describing speech as a code in which speakers overlap (or coarticulate) segments to form syllables.",
            "score": 120.99928283691406
        },
        {
            "docid": "49375_26",
            "document": "Larynx . In contrast, though other species have low larynges, their tongues remain anchored in their mouths and their vocal tracts cannot produce the range of speech sounds of humans. The ability to lower the larynx transiently in some species extends the length of their vocal tract, which as Fitch showed creates the acoustic illusion that they are larger. Research at Haskins Laboratories in the 1960s showed that speech allows humans to achieve a vocal communication rate that exceeds the fusion frequency of the auditory system by fusing sounds together into syllables and words. The additional speech sounds that the human tongue enables us to produce, particularly [i], allow humans to unconsciously infer the length of the vocal tract of the person who is talking, a critical element in recovering the phonemes that make up a word.",
            "score": 120.73823547363281
        },
        {
            "docid": "32454456_6",
            "document": "Frequency following response . Due to the lack of specificity at low levels, the FFR has yet to make its way into clinical settings. Only recently has the FFR been evaluated for encoding complex sound and binaural processing. There may be uses for the information the FFR can provide regarding steady state, time-variant, and speech signals for better understanding of individuals with hearing loss and its effects. FFR distortion products (FFR DPs) could supplement low frequency (< 1000\u00a0Hz) DPOAEs. FFRs have the potential to be used to evaluate the neural representation of speech sounds processed by different strategies employed by users of cochlear implants, primarily identification and discrimination of speech. Also, phase-locked neural activity reflected in the FFR has been successfully used to predict auditory thresholds.",
            "score": 119.93895721435547
        },
        {
            "docid": "18614_35",
            "document": "Language acquisition . Prosody is the property of speech that conveys an emotional state of the utterance, as well as intended form of speech (whether it be a question, statement or command). Some researchers in the field of developmental neuroscience would argue that fetal auditory learning mechanisms are solely due to discrimination in prosodic elements. Although this would hold merit in an evolutionary psychology perspective (i.e. recognition of mother's voice/familiar group language from emotionally valent stimuli), some theorists argue that there is more than prosodic recognition in elements of fetal learning. Newer evidence shows that fetuses not only react to the native language differently from nonnative, but furthermore that fetuses react differently and can accurately discriminate between native and nonnative vowels (Moon, Lagercrantz, & Kuhl, 2013). Furthermore, a new study in 2016 showed that newborn infants encode the edges of multisyllabic sequences better than the internal components of the sequence (Ferry et al., 2016). Together, these results suggest that newborn infants have learned important properties of syntactic processing in utero, that can be seen in infant knowledge of native language vowels and the sequencing of heard multisyllabic phrases. This ability to sequence specific vowels gives newborn infants some of the fundamental mechanisms needed in order to learn the complex organization of a language.  From a neuroscientific perspective, there are neural correlates have been found that demonstrate human fetal learning of speech-like auditory stimulus that most other studies have been analyzing (Partanen et al., 2013). In a study conducted by Partanen et al. (2013), researchers presented fetuses with certain word variants and saw that these fetuses exhibited higher brain activity to the certain word variants compared to controls. In this same study, there was \"a significant correlation existed between the amount of prenatal exposure and brain activity, with greater activity being associated with a higher amount of prenatal speech exposure,\" pointing to the important learning mechanisms present before birth that is fine-tuned to features in speech (Partanen et al., 2013).",
            "score": 119.59172058105469
        },
        {
            "docid": "1379269_3",
            "document": "Musical acoustics . Whenever two different pitches are played at the same time, their sound waves interact with each other \u2013 the highs and lows in the air pressure reinforce each other to produce a different sound wave. Any repeating sound wave that is not a sine wave can be modeled by many different sine waves of the appropriate frequencies and amplitudes (a frequency spectrum). In humans the hearing apparatus (composed of the ears and brain) can usually isolate these tones and hear them distinctly. When two or more tones are played at once, a variation of air pressure at the ear \"contains\" the pitches of each, and the ear and/or brain isolate and decode them into distinct tones.",
            "score": 119.30636596679688
        },
        {
            "docid": "1673438_10",
            "document": "Gamma wave . A 2009 study published in Nature successfully induced gamma waves in mouse brains. Researchers performed this study using optogenetics (the method of combining genetic engineering with light to manipulate the activity of individual nerve cells). The protein channelrhodopsin-2 (ChR2), which sensitizes cells to light, was genetically engineered into these mice, specifically to be expressed in a target-group of interneurons. These fast-spiking (FS) interneurons, known for high electrical activity, were then activated with an optical fiber and laser\u2014the second step in optogenetics. In this way, the cell activity of these interneurons was manipulated in the frequency range of 8\u2013200\u00a0Hz. The study produced empirical evidence of gamma wave induction in the approximate interval of 25\u2013100\u00a0Hz. The gamma waves were most apparent at a frequency of 40\u00a0Hz; this indicates that the gamma waves evoked by FS manipulation are a resonating brain circuit property. This is the first study in which it has been shown that a brain state can be induced through the activation of a specific group of cells. Pushed by the need of understanding how gamma might affect disease pathogenesis, a recent study published in Nature demonstrates that entraining oscillations and spiking at 40\u00a0Hz in the hippocampus of a well-established model of Alzheimer's disease (5XFAD mice) reduces A\u03b2 peptides and at the same time activates a microglia response.",
            "score": 118.9810791015625
        },
        {
            "docid": "10042066_2",
            "document": "Developmental linguistics . Developmental linguistics is the study of the development of linguistic ability in an individual, particularly the acquisition of language in childhood. It involves research into the different stages in language acquisition, language retention, and language loss in both first and second languages, in addition to the area of bilingualism. Before infants can speak, the neural circuits in their brains are constantly being influenced by exposure to language. The neurobiology of language contains a \"critical period\" in which children are most sensitive to language. The different aspects of language have varying \"critical periods\". Studies show that the critical period for phonetics is toward the end of the first year. At 18 months, a toddler's vocabulary vastly expands. The critical period for syntactic learning is 18-36 months. Infants of different mother languages can be differentiated at the age of 10 months. At 20 weeks they begin vocal imitation. Beginning when babies are about 12 months, they take on computational learning and social learning. Social interactions for infants and toddlers is important because it helps associate \"perception and action\". In-person social interaction rather than audio or video better facilitates learning in babies because they learn from how other people respond to them, especially their mothers. Babies have to learn to mimic certain syllables, which takes practice in manipulating tongue and lip movement. Sensory-motor learning in speech is linked to exposure to speech, which is very sensitive to language. Infants exposed to Spanish exhibit a different vocalization than infants exposed to English. One study took infants that were learning English and made them listen to Spanish in 12 sessions. The result showed consequent alterations in their vocalization, which demonstrated Spanish prosody.  One study used MEG to record activation in the brains of newborns, 6 months olds and 12 months olds while presenting them with syllables, harmonics and non-speech sounds. For the 6 month and 12 month old, the auditory and motor areas responded to speech. The newborn showed auditory activation but not motor activation. Another study presented 3 month olds with sentences and recorded their brain activity via fMRI motor speech areas did activate. These studies suggest that the link between perception and action begins to develop at 3 months. When babies are young, they are actually the most sensitive to distinguishing all phonetic units. During an infant\u2019s 1st year of life, they have to differentiate between about 40 phonetic units. When they are older they have usually been exposed to their native language so much that they lose this ability and can only distinguish phonetic units in their native language. Even at 12 months babies exhibit a deficit in differentiated non-native sounds. However, their ability to distinguish sounds in their native language continues to improve and become more fine-tuned. For example, Japanese learning infants learn that there is no differentiation between /r/ and /l/. However, in English, \"rake\" and \"lake\" are two different words. Japanese babies eventually lose their ability to distinguish between /r/ and /l/. Similarly, a Spanish learning infant cannot form words until they learn the difference between works like \"bano\" and \"pano\", because the /p/ sound is different than the /b/ sound. English learning babies do not learn to differentiate between the two.",
            "score": 118.69654083251953
        },
        {
            "docid": "3237319_9",
            "document": "Auditory verbal agnosia . Auditory verbal agnosia is the inability to distinguish phonemes. In some patients with unilateral auditory verbal agnosia, there is evidence that the ability to acoustically process speech signals is affected at the prephonemic level, preventing the conversion of these signals into phonemes. There are two predominate hypotheses that address what happens within the language center of the brain in people that have AVA. One of the hypotheses is that an early stage of auditory analysis is impaired. The fact that AVA patients have the ability to read shows that both the semantic system and the speech output lexicon are intact. The second hypotheses suggests that there is either a complete or partial disconnection of the auditory input lexicon from the semantic system. This would suggest that entries in the lexicon can still be activated but they cannot go on to cause subsequent semantic activation. In relation to these two different hypotheses, researchers in one study differentiated between two different types of AVA. According to this study, one form of AVA is a deficit at the prephonemic level and is related to the inability to comprehend rapid changes in sound.This form of AVA is associated with bilateral temporal lobe lesions. Speech perception in patients with this form of AVA has been shown to improve significantly in understanding when the pace of speech is drastically slowed. The second type of AVA that the study discusses is a deficit in linguistic discrimination that does not adhere to a prephonemic pattern. This form is associated with left unilateral temporal lobe lesions and may even be considered a form of Wernicke's aphasia. Often individuals diagnosed with auditory verbal agnosia are also incapable of discriminating between non-verbal sounds as well as speech. The underlying problem seems to be temporal in that understanding speech requires the discrimination between specific sounds which are closely spaced in time. Note that this is not unique to speech; studies using non-speech sounds closely spaced in time (dog bark, phone ring, lightning, etc.) have shown that those with auditory verbal agnosia are unable to discriminate between those sounds in the majority of cases, though a few putative examples of speech-specific impairment have been documented in the literature.",
            "score": 117.94530487060547
        },
        {
            "docid": "34038330_2",
            "document": "Theta model . The theta model, or Ermentrout\u2013Kopell canonical model, is a biological neuron model originally developed to model neurons in the animal Aplysia, and later used in various fields of computational neuroscience. The model is particularly well suited to describe neuron bursting, which are rapid oscillations in the membrane potential of a neuron interrupted by periods of relatively little oscillation. Bursts are often found in neurons responsible for controlling and maintaining steady rhythms. For example, breathing is controlled by a small network of bursting neurons in the brain stem. Of the three main classes of bursting neurons (square wave bursting, parabolic bursting, and elliptic bursting), the theta model describes parabolic bursting. Parabolic bursting is characterized by a series of bursts that are regulated by a slower external oscillation. This slow oscillation changes the frequency of the faster oscillation so that the frequency curve of the burst pattern resembles a parabola.",
            "score": 116.91878509521484
        },
        {
            "docid": "40621603_5",
            "document": "Linguistic intelligence . Speech production is process by which a thought in the brain is converted into an understandable auditory form. This is a multistage mechanism that involves many different areas of the brain. The first stage is planning, where the brain constructs words and sentences that turn the thought into an understandable form. This occurs primarily in the inferior frontal cortex, specifically in an area known as Broca's area. Next, the brain must plan how to physically create the sounds necessary for speech by linking the planned speech with known sounds, or phonemes. While the location of these associations is not known, it is known that the supplementary motor area plays a key role in this step. Finally, the brain must signal for the words to actually be spoken. This is carried out by the premotor cortex and the motor cortex. In most cases, speech production is controlled by the left hemisphere. In a series of studies, Wilder Penfield, among others, probed the brains of both right-handed (generally left-hemisphere dominant) and left-handed (generally right-hemisphere dominant) patients. They discovered that, regardless of handedness, the left hemisphere was almost always the speech controlling side. However, it has been discovered that in cases of neural stress (hemorrhage, stroke, etc.) the right hemisphere has the ability to take control of speech functions.",
            "score": 116.31700897216797
        },
        {
            "docid": "17922_7",
            "document": "Loglan . Brown also intended the language to be completely regular and unambiguous. Each sentence can be parsed in only one way. Furthermore, the syllabic structure of words was designed so that a sequence of syllables can be separated into words in only one way, even if the word separation is not clear from pauses in speech. It has a small number of phonemes, so that regional \"accents\" are less likely to produce unintelligible speech. To make the vocabulary easier to learn, words were constructed to have elements in common with related words in the world's eight most widely spoken languages.",
            "score": 115.21206665039062
        },
        {
            "docid": "4833512_2",
            "document": "Mu wave . Mu waves, also known as mu rhythms, comb or wicket rhythms, arciform rhythms, or sensorimotor rhythms, are synchronized patterns of electrical activity involving large numbers of neurons, probably of the pyramidal type, in the part of the brain that controls voluntary movement. These patterns as measured by electroencephalography (EEG), magnetoencephalography (MEG), or electrocorticography (ECoG), repeat at a frequency of 7.5\u201312.5 (and primarily 9\u201311) Hz, and are most prominent when the body is physically at rest. Unlike the alpha wave, which occurs at a similar frequency over the resting visual cortex at the back of the scalp, the mu wave is found over the motor cortex, in a band approximately from ear to ear. A person suppresses mu wave patterns when he or she performs a motor action or, with practice, when he or she visualizes performing a motor action. This suppression is called desynchronization of the wave because EEG wave forms are caused by large numbers of neurons firing in synchrony. The mu wave is even suppressed when one observes another person performing a motor action or an abstract motion with biological characteristics. Researchers such as V. S. Ramachandran and colleagues have suggested that this is a sign that the mirror neuron system is involved in mu wave suppression, although others disagree.",
            "score": 114.76863861083984
        },
        {
            "docid": "14824490_15",
            "document": "Recurrent thalamo-cortical resonance . Gamma-range oscillations are not the only rhythms associated with conscious thought and activity. Thalamocortical alpha frequency oscillations have been noted in the human occipital-parietal cortex. This activity could be originated by the pyramidal neurons in layer IV. It has been shown that alpha rhythms seem to be related to the focus of one's attention: external focus on visual tasks diminishes alpha activity while internal focus as in heavy working memory tasks show an increase in alpha magnitudes. This is contrary to gamma wave oscillatory frequencies which emerge in selective focus tasks.",
            "score": 114.31134796142578
        },
        {
            "docid": "8953842_15",
            "document": "Computational auditory scene analysis . While many models consider the audio signal as a complex combination of different frequencies, modeling the auditory system can also require consideration for the neural components. By taking a holistic process, where a stream (of feature-based sounds) correspond to neuronal activity distributed in many brain areas, the perception of the sound could be mapped and modeled. Two different solutions have been proposed to the binding of the audio perception and the area in the brain. Hierarchical coding models many cells to encode all possible combinations of features and objects in the auditory scene. Temporal or oscillatory correlation addressing the binding problem by focusing on the synchrony and desynchrony between neural oscillations to encode the state of binding among the auditory features. These two solutions are very similar to the debacle between place coding and temporal coding. While drawing from modeling neural components, another phenomenon of ASA comes into play with CASA systems: the extent of modeling neural mechanisms. The studies of CASA systems have involved modeling some known mechanisms, such as the bandpass nature of cochlear filtering and random auditory nerve firing patterns, however, these models may not lead to finding new mechanisms, but rather give an understanding of purpose to the known mechanisms.",
            "score": 113.92936706542969
        },
        {
            "docid": "33826142_4",
            "document": "Brain activity and meditation . Electroencephalography (EEG) has been used in many studies as a primary method for evaluating the meditating brain. Electroencephalography uses electrical leads placed all over the scalp to measure the collective electrical activity of the cerebral cortex. Specifically, EEG measures the electric fields of large groups of neurons. EEG has the benefit of excellent temporal resolution and is able to measure aggregate activity of portions or the entire cortex down to the millisecond scale. Unlike other imaging based methods, EEG does not have good spatial resolution and is more appropriately used to evaluate the running spontaneous activity of the cortex. This spontaneous activity is classified into four main classifications based on the frequency of the activity, ranging from low frequency delta waves (< 4\u00a0Hz) commonly found during sleep to beta waves (13\u201330\u00a0Hz) associated with an awake and alert brain. In between these two extremes are theta waves (4\u20138\u00a0Hz) and alpha waves (8\u201312\u00a0Hz).",
            "score": 113.75077056884766
        },
        {
            "docid": "42848378_4",
            "document": "Ultrasonic toothbrush . Electric toothbrushes vibrate in either an up/down direction, or in a circular motion, and sometimes in a combination of the two. Typically, the speed of their vibration is measured in movements per minute, where common electric toothbrushes vibrate at a speed of between a few thousand times a minute to approximately 10,000 to 12,000 times per minute. Sonic toothbrushes are called sonic because the speed or frequency of their vibration, as opposed to the sound of the motor, falls within the average range that is used by people in communication. The voiced speech of a typical adult male will have a fundamental frequency from 85 to 180\u00a0Hz (10,200 to 21,000 movements per minute), and that of a typical adult female from 165 to 255\u00a0Hz (19,800 to 30,600 movements per minute). Ultrasonic toothbrushes work by generating an ultrasonic wave usually from an implanted piezo crystal, the frequency of which technically could begin at 20,000\u00a0Hz (2,400,000 movements per minute). The most common frequency however, around which many scientific studies have been conducted, is in the area of approximately 1.6\u00a0MHz, which translates to 96,000,000 waves or 192,000,000 movements per minute.",
            "score": 113.61378479003906
        },
        {
            "docid": "33814258_18",
            "document": "EEG microstates . This study illuminates the complexity of brain activity and EEG dynamics. The data suggest that \"alpha (wave) activity could be indexing different brain information in each arousal state.\" Furthermore, they suggest that the alpha rhythm could be the \"natural resonance frequency of the visual cortex during the waking state, whereas the alpha activity that appears in the drowsiness period at sleep onset could be indexing the hypnagogic imagery self-generated by the sleeping brain, and a phasic event in the case of REM sleep.\" Another claim is that longer periods of stable brain activity may be handling smaller amounts of information processing, and thus few changes in microstates, while shorter, less-stable brain activity may reflect large amounts of different information to process, and thus more microstate changes.",
            "score": 113.46195220947266
        },
        {
            "docid": "20518706_7",
            "document": "PGO waves . Although scientists know they exist, PGO waves have not been detected in healthy humans due to the ethical concerns about accessing these areas where the readings need to be taken from. However, advances in deep brain stimulation has made it possible to put electrodes inside the brains of humans with different pathologies and make EEG recordings of different nuclei. Due to the similarities with the animal models, we can infer that PGO waves are happening at the same frequency in human EEGs. Thus, scientists can infer that PGO waves exist in humans.",
            "score": 113.44947814941406
        },
        {
            "docid": "12469094_5",
            "document": "Andrea Moro . As for the other field, he explored the neurological correlates of artificial languages which do not follow the principles of Universal Grammar providing evidence that Universal Grammar properties cannot be cultural, social or conventional artifacts: in fact, he and the team of people he worked with showed that recursive syntactic rules, that is rules based on recursion selectively activate a neurological network (including Broca's area) whereas non-recursive syntactic rules do not. These discoveries have appeared in a few international Journals, including, for example, Nature Neuroscience (Musso, Moro et al. 2003) or PNAS (Moro 2010): a comprehensive collection of the works in both fields has now become available in the \"Routledge Leading Linguist Series\" as \"The Equilibrium of Human Syntax\" (Routledge 2013). He also explored the correlates between the representation of the world in the brain and the structure of syntax, specifically the relationship between sentential negation and the brain) also available in Moro 2013. In recent papers he took position against the idea that the sequence of human actions can be described as having the same structure as the sequence of words in a well-formed syntactic structure. Furthermore, Moro pursued the study of the relationship between the brain and language by exploiting electrophysiological measure. The core of the experiment - done in a team with neurosurgeons and electric engineers - consists in comparing the shape of the electric waves of non-acoustic language areas (typically, Broca's area) with the shape of the corresponding sound waves. The result was that not only the shape of the two different waves correlate but they do so also in absence of sound production, that is during inner speech activity, opening the possibility to reading linguistic expression from direct measure of the cortex and skipping the actual utterance of the sentence. For a non technical synthesis of these discoveries and a critical discussion see \"Impossible Languages\" which received the honourable mention at the Prose Awards .",
            "score": 112.80838775634766
        },
        {
            "docid": "25140_41",
            "document": "Perception . Hearing (or \"audition\") is the ability to perceive sound by detecting vibrations. Frequencies capable of being heard by humans are called audio or \"sonic\". The range is typically considered to be between 20\u00a0Hz and 20,000\u00a0Hz. Frequencies higher than audio are referred to as ultrasonic, while frequencies below audio are referred to as infrasonic. The auditory system includes the outer ears which collect and filter sound waves, the middle ear for transforming the sound pressure (impedance matching), and the inner ear which produces neural signals in response to the sound. By the ascending auditory pathway these are led to the primary auditory cortex within the temporal lobe of the human brain, which is where the auditory information arrives in the cerebral cortex and is further processed there.",
            "score": 111.9734115600586
        }
    ]
}