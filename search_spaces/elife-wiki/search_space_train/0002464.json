{
    "q": [
        {
            "docid": "25146378_20",
            "document": "Functional specialization (brain) . Other researchers who provide evidence to support the theory of distributive processing include Anthony McIntosh and William Uttal, who question and debate localization and modality specialization within the brain. McIntosh's research suggests that human cognition involves interactions between the brain regions responsible for processes sensory information, such as vision, audition, and other mediating areas like the prefrontal cortex. McIntosh explains that modularity is mainly observed in sensory and motor systems, however, beyond these very receptors, modularity becomes \"fuzzier\" and you see the cross connections between systems increase. He also illustrates that there is an overlapping of functional characteristics between the sensory and motor systems, where these regions are close to one another. These different neural interactions influence each other, where activity changes in one area influence other connected areas. With this, McIntosh suggest that if you only focus on activity in one area, you may miss the changes in other integrative areas. Neural interactions can be measured using analysis of covariance in neuroimaging. McIntosh used this analysis to convey a clear example of the interaction theory of distributive processing. In this study, subjects learned that an auditory stimulus signalled a visual event. McIntosh found activation (an increase blood flow), in an area of the occipital cortex, a region of the brain involved in visual processing, when the auditory stimulus was presented alone. Correlations between the occipital cortex and different areas of the brain such as the prefrontal cortex, premotor cortex and superior temporal cortex showed a pattern of co-variation and functional connectivity.",
            "score": 112.3269659280777
        },
        {
            "docid": "315084_25",
            "document": "Lip reading . Following the discovery that auditory brain regions, including Heschl's gyrus, were activated by seen speech, the neural circuitry for speechreading was shown to include supra-modal processing regions, especially superior temporal sulcus (all parts) as well as posterior inferior occipital-temporal regions including regions specialised for the processing of faces and biological motion. In some but not all studies, activation of Broca's area is reported for speechreading, suggesting that articulatory mechanisms can be activated in speechreading. Studies of the time course of audiovisual speech processing showed that sight of speech can prime auditory processing regions in advance of the acoustic signal. Better lipreading skill is associated with greater activation in (left) superior temporal sulcus and adjacent inferior temporal (visual) regions in hearing people. In deaf people, the circuitry devoted to speechreading appears to be very similar to that in hearing people, with similar associations of (left) superior temporal activation and lipreading skill.",
            "score": 110.97833609580994
        },
        {
            "docid": "17893852_4",
            "document": "Humor research . Cognitive neuroscience has provided insight into how humor is neurologically realized. Brain imaging techniques such as fMRI and PET scans have been implemented in this subfield of humor research.  There are a few main regions of the human brain associated with humor and laughter. The production of laughter involves two primary brain pathways, one for both involuntary and voluntary laughter (cf. Duchenne and non-Duchenne). Involuntary laughter is usually emotionally driven, and includes key emotional brain areas such as the amygdala, thalamic areas, and the brainstem. Voluntary laughter instead begins in the premotor opercular area (in the temporal lobe) and moves to the motor cortex and pyramidal tract before moving to the brainstem. Wild et al. (2003) propose that the generation of laughter is mostly influenced by neural pathways that go from the premotor and motor cortex to the ventral side of the brainstem, through the cerebral peduncles. It is also suggested that real laughter is not produced from the motor cortex, but that the normal inhibition of cortical frontal areas stops during laughter. When the electrical activity of the brain is measured during and after hearing a joke, a prominent response can be seen approximately 300ms after the punchline, followed by a depolarization about 100ms later. The fact that humor response occurs in two separate waves of activity supports the idea that humor processing occurs in two stages. Functional MRI and PET studies further illuminate which parts of the brain are participating in the experience of humor. A study by Ozawa, et al., (2000) found that hearing sentences which participants rated as humorous resulted in activation in Broca's area and the middle frontal gyrus, in addition to Wernicke's area and the transverse temporal gyri, which were activated in control (non-humorous) conditions as well.",
            "score": 93.17686641216278
        },
        {
            "docid": "33932515_22",
            "document": "Social cue . Benjamin Straube, Antonia Green, Andreas Jansen, Anjan Chatterjee, and Tilo Kircher found that social cues influence the neural processing of speech-gesture utterances. Past studies have focused on mentalizing as being a part of perception of social cues and it is believed that this process relies on the neural system, which consists of: When people focus on things in a social context, the medial prefrontal cortex and precuneus areas of the brain are activated, however when people focus on a non-social context there is no activation of these areas. Straube et al. hypothesized that the areas of the brain involved in mental processes were mainly responsible for social cue processing. It is believed that when iconic gestures are involved, the left temporal and occipital regions would be activated and when emblematic gestures were involved the temporal poles would be activated. When it came to abstract speech and gestures, the left frontal gyrus would be activated according to Straube et al. After conducting an experiment on how body position, speech and gestures affected activation in different areas of the brain Straube et al. came to the following conclusions:",
            "score": 110.50014972686768
        },
        {
            "docid": "2363287_6",
            "document": "Visual learning . Various areas of the brain work together in a multitude of ways in order to produce the images that we see with our eyes and that are encoded by our brains. The basis of this work takes place in the visual cortex of the brain. The visual cortex is located in the occipital lobe of the brain and harbors many other structures that aid in visual recognition, categorization, and learning. One of the first things the brain must do when acquiring new visual information is recognize the incoming material. Brain areas involved in recognition are the inferior temporal cortex, the superior parietal cortex, and the cerebellum. During tasks of recognition, there is increased activation in the left inferior temporal cortex and decreased activation in the right superior parietal cortex. Recognition is aided by neural plasticity, or the brain's ability to reshape itself based on new information. Next the brain must categorize the material. The three main areas that are used when categorizing new visual information are the orbitofrontal cortex and two dorsolateral prefrontal regions which begin the process of sorting new information into groups and further assimilating that information into things that you might already know. After recognizing and categorizing new material entered into the visual field, the brain is ready to begin the encoding process \u2013 the process which leads to learning. Multiple brain areas are involved in this process such as the frontal lobe, the right extrastriate cortex, the neocortex, and again, the neostriatum. One area in particular, the limbic-diencephalic region, is essential for transforming perceptions into memories. With the coming together of tasks of recognition, categorization and learning; schemas help make the process of encoding new information and relating it to things you already know much easier. One can remember visual images much better when they can apply it to an already known schema. Schemas actually provide enhancement of visual memory and learning.",
            "score": 130.96033930778503
        },
        {
            "docid": "620396_42",
            "document": "Origin of language . In humans, functional MRI studies have reported finding areas homologous to the monkey mirror neuron system in the inferior frontal cortex, close to Broca's area, one of the language regions of the brain. This has led to suggestions that human language evolved from a gesture performance/understanding system implemented in mirror neurons. Mirror neurons have been said to have the potential to provide a mechanism for action-understanding, imitation-learning, and the simulation of other people's behavior. This hypothesis is supported by some cytoarchitectonic homologies between monkey premotor area F5 and human Broca's area. Rates of vocabulary expansion link to the ability of children to vocally mirror non-words and so to acquire the new word pronunciations. Such speech repetition occurs automatically, quickly and separately in the brain to speech perception. Moreover, such vocal imitation can occur without comprehension such as in speech shadowing and echolalia. Further evidence for this link comes from a recent study in which the brain activity of two participants was measured using fMRI while they were gesturing words to each other using hand gestures with a game of charades\u2014a modality that some have suggested might represent the evolutionary precursor of human language. Analysis of the data using Granger Causality revealed that the mirror-neuron system of the observer indeed reflects the pattern of activity of in the motor system of the sender, supporting the idea that the motor concept associated with the words is indeed transmitted from one brain to another using the mirror system.",
            "score": 117.41660761833191
        },
        {
            "docid": "40621603_5",
            "document": "Linguistic intelligence . Speech production is process by which a thought in the brain is converted into an understandable auditory form. This is a multistage mechanism that involves many different areas of the brain. The first stage is planning, where the brain constructs words and sentences that turn the thought into an understandable form. This occurs primarily in the inferior frontal cortex, specifically in an area known as Broca's area. Next, the brain must plan how to physically create the sounds necessary for speech by linking the planned speech with known sounds, or phonemes. While the location of these associations is not known, it is known that the supplementary motor area plays a key role in this step. Finally, the brain must signal for the words to actually be spoken. This is carried out by the premotor cortex and the motor cortex. In most cases, speech production is controlled by the left hemisphere. In a series of studies, Wilder Penfield, among others, probed the brains of both right-handed (generally left-hemisphere dominant) and left-handed (generally right-hemisphere dominant) patients. They discovered that, regardless of handedness, the left hemisphere was almost always the speech controlling side. However, it has been discovered that in cases of neural stress (hemorrhage, stroke, etc.) the right hemisphere has the ability to take control of speech functions.",
            "score": 137.42573404312134
        },
        {
            "docid": "1731484_21",
            "document": "Amusia . Memory is required in order to process and integrate both melodic and rhythmic aspects of music. Studies suggest that there is a rich interconnection between the right temporal gyrus and frontal cortical areas for working memory in music appreciation. This connection between the temporal and frontal regions of the brain is extremely important since these regions play critical roles in music processing. Changes in the temporal areas of the amusic brain are most likely associated with deficits in pitch perception and other musical characteristics, while changes in the frontal areas are potentially related to deficits in cognitive processing aspects, such as memory, that are needed for musical discrimination tasks. Memory is also concerned with the recognition and internal representation of tunes, which help to identify familiar songs and confer the ability to sing tunes in one's head. The activation of the superior temporal region and left inferior temporal and frontal areas is responsible for the recognition of familiar songs, and the right auditory cortex (a perceptual mechanism) is involved in the internal representation of tunes. These findings suggest that any abnormalities and/or injuries to these regions of the brain could facilitate amusia.",
            "score": 96.5930153131485
        },
        {
            "docid": "1764639_17",
            "document": "Levels-of-processing effect . Several brain imaging studies using positron emission tomography and functional magnetic resonance imaging techniques have shown that higher levels of processing correlate with more brain activity and activity in different parts of the brain than lower levels. For example, in a lexical analysis task, subjects showed activity in the left inferior prefrontal cortex only when identifying whether the word represented a living or nonliving object, and not when identifying whether or not the word contained an \"a\". Similarly, an auditory analysis task showed increased activation in the left inferior prefrontal cortex when subjects performed increasingly semantic word manipulations. Synaptic aspects of word recognition have been correlated with the left frontal operculum and the cortex lining the junction of the inferior frontal and inferior precentral sulcus. The self-reference effect also has neural correlates with a region of the medial prefrontal cortex, which was activated in an experiment where subjects analyzed the relevance of data to themselves. Specificity of processing is explained on a neurological basis by studies that show brain activity in the same location when a visual memory is encoded and retrieved, and lexical memory in a different location. Visual memory areas were mostly located within the bilateral extrastriate visual cortex.",
            "score": 114.44449377059937
        },
        {
            "docid": "1168317_32",
            "document": "Mirror neuron . In humans, functional MRI studies have reported finding areas homologous to the monkey mirror neuron system in the inferior frontal cortex, close to Broca's area, one of the hypothesized language regions of the brain. This has led to suggestions that human language evolved from a gesture performance/understanding system implemented in mirror neurons. Mirror neurons have been said to have the potential to provide a mechanism for action-understanding, imitation-learning, and the simulation of other people's behaviour. This hypothesis is supported by some cytoarchitectonic homologies between monkey premotor area F5 and human Broca's area. Rates of vocabulary expansion link to the ability of children to vocally mirror non-words and so to acquire the new word pronunciations. Such speech repetition occurs automatically, fast and separately in the brain to speech perception. Moreover, such vocal imitation can occur without comprehension such as in speech shadowing and echolalia.",
            "score": 110.79515314102173
        },
        {
            "docid": "5366050_20",
            "document": "Speech perception . The first ever hypothesis of speech perception was used with patients who acquired an auditory comprehension deficit, also known as receptive aphasia. Since then there have been many disabilities that have been classified, which resulted in a true definition of \"speech perception\". The term 'speech perception' describes the process of interest that employs sub lexical contexts to the probe process. It consists of many different language and grammatical functions, such as: features, segments (phonemes), syllabic structure (unit of pronunciation), phonological word forms (how sounds are grouped together), grammatical features, morphemic (prefixes and suffixes), and semantic information (the meaning of the words). In the early years, they were more interested in the acoustics of speech. For instance, they were looking at the differences between /ba/ or /da/, but now research has been directed to the response in the brain from the stimuli. In recent years, there has been a model developed to create a sense of how speech perception works; this model is known as the Dual Stream Model. This model has drastically changed from how psychologists look at perception. The first section of the Dual Stream Model is the ventral pathway. This pathway incorporates middle temporal gyrus, inferior temporal sulcus and perhaps the inferior temporal gyrus. The ventral pathway shows phonological representations to the lexical or conceptual representations, which is the meaning of the words. The second section of the Dual Stream Model is the dorsal pathway. This pathway includes the sylvian parietotemporal, inferior frontal gyrus, anterior insula, and premotor cortex. Its primary function is to take the sensory or phonological stimuli and transfer it into an articulatory-motor representation (formation of speech).",
            "score": 94.67330312728882
        },
        {
            "docid": "599917_19",
            "document": "Mental image . Research has occurred to designate a specific neural correlate of imagery; however, studies show a multitude of results. Most studies published before 2001 suggest neural correlates of visual imagery occur in brodmann area 17. Auditory performance imagery have been observed in the premotor areas, precunes, and medial brodmann area 40. Auditory imagery in general occurs across participants in the temporal voice area (TVA), which allows top-down imaging manipulations, processing, and storage of audition functions. Olfactory imagery research shows activation in the anterior piriform cortex and the posterior piriform cortex; experts in olfactory imagery have larger gray matter associated to olfactory areas. Tactile imagery is found to occur in the dorsolateral prefrontal area, inferior frontal gyrus, frontal gyrus, insula, precentral gyrus, and the medial frontal gyrus with basil ganglia activation in the ventral posteriomedial nucleus and putamen (hemisphere activation corresponds to the location of the imagined tactile stimulus). Research in gustatory imagery reveals activation in the anterior insular cortex, frontal operculum, and prefrontal cortex. Novices of a specific form of mental imagery show less gray matter than experts of mental imagery congruent to that form. A meta-analysis of neuroimagery studies revealed significant activation of the bilateral dorsal parietal, interior insula, and left inferior frontal regions of the brain.",
            "score": 55.300692558288574
        },
        {
            "docid": "25490263_16",
            "document": "Speech repetition . Two cortical processing streams exist: a ventral one which maps sound onto meaning, and a dorsal one, that maps sound onto motor representations. The dorsal stream projects from the posterior Sylvian fissure at the temporoparietal junction, onto frontal motor areas, and is not normally involved in speech perception.  Carl Wernicke identified a pathway between the left posterior superior temporal sulcus (a cerebral cortex region sometimes called the Wernicke's area) as a centre of the sound \"images\" of speech and its syllables that connected through the arcuate fasciculus with part of the inferior frontal gyrus (sometimes called the Broca's area) responsible for their articulation. This pathway is now broadly identified as the dorsal speech pathway, one of the two pathways (together with the ventral pathway) that process speech. The posterior superior temporal gyrus is specialized for the transient representation of the phonetic sequences used for vocal repetition. Part of the auditory cortex also can represent aspects of speech such as its consonantal features.",
            "score": 76.99003911018372
        },
        {
            "docid": "1324735_14",
            "document": "Subvocalization . Silent speech-reading and silent counting are also examined when experimenters look at subvocalization. These tasks show activation in the frontal cortices, hippocampus and the thalamus for silent counting. Silent-reading activates similar areas of the auditory cortex that are involved in listening. Finally, the phonological loop; proposed by Baddeley and Hitch as \u201cbeing responsible for temporary storage of speech-like information\u201d is an active subvocal rehearsal mechanism, activation originating mostly in the left hemispheric speech areas: Broca's, lateral and medial premotor cortices and the cerebellum.",
            "score": 72.59236907958984
        },
        {
            "docid": "21312318_27",
            "document": "Recognition memory . Recognition memory is critically dependent on a hierarchically organized network of brain areas including the visual ventral stream, medial temporal lobe structures, frontal lobe and parietal cortices along with the hippocampus. As mentioned previously, the processes of recollection and familiarity are represented differently in the brain. As such, each of the regions listed above can be further subdivided according to which part is primarily involved in recollection or in familiarity. In the temporal cortex, for instance, the medial region is related to recollection whereas the anterior region is related to familiarity. Similarly, in the parietal cortex, the lateral region is related to recollection whereas the superior region is related to familiarity. An even more specific account divides the medial parietal region, relating the posterior cingulate to recollection and the precuneus to familiarity. The hippocampus plays a prominent role in recollection whereas familiarity depends heavily on the surrounding medial-temporal regions, especially the perirhinal cortex. Finally, it is not yet clear what specific regions of the prefrontal lobes are associated with recollection versus familiarity, although there is evidence that the left prefrontal cortex is correlated more strongly with recollection whereas the right prefrontal cortex is involved more in familiarity. Though left-side activation involved in recollection was originally hypothesized to result from semantic processing of words (many of these earlier studies used written words for stimuli) subsequent studies using nonverbal stimuli produced the same finding\u2014suggesting that prefrontal activation in the left hemisphere results from any kind of detailed remembering.  As previously mentioned, recognition memory is not a stand-alone concept; rather it is a highly interconnected and integrated sub-system of memory. Perhaps misleadingly, the regions of the brain listed above correspond to an abstract and highly generalized understanding of recognition memory, in which the stimuli or items-to-be-recognized are not specified. In reality, however, the location of brain activation involved in recognition is highly dependent on the nature of the stimulus itself. Consider the conceptual differences in recognizing written words compared to recognizing human faces. These are two qualitatively different tasks and as such it is not surprising that they involve additional, distinct regions of the brain. Recognizing words, for example, involves the visual word form area, a region in the left fusiform gyrus, which is believed to specialized in recognizing written words. Similarly, the fusiform face area, located in the right hemisphere, is linked specifically to the recognition of faces.",
            "score": 115.17040693759918
        },
        {
            "docid": "2640086_28",
            "document": "Affective neuroscience . Instead of investigating specific emotions, Kober, et al. 2008 reviewed 162 neuroimaging studies published between 1990-2005 to determine if groups of brain regions show consistent patterns of activation during emotional experience (that is, actively experiencing an emotion first-hand) and during emotion perception (that is, perceiving a given emotion as experienced by another). This meta-analysis used multilevel kernal density analysis (MKDA) to examine fMRI and PET studies, a technique that prevents single studies from dominating the results (particularly if they report multiple nearby peaks) and that enables studies with large sample sizes (those involving more participants) to exert more influence upon the results. MKDA was used to establish a neural reference space that includes the set of regions showing consistent increases across all studies (for further discussion of MDKA see Wager et al. 2007). Next, this neural reference space was partitioned into functional groups of brain regions showing similar activation patterns across studies by first using multivariate techniques to determine co-activation patterns and then using data-reduction techniques to define the functional groupings (resulting in six groups). Consistent with a psychological construction approach to emotion, the authors discuss each functional group in terms more basic psychological operations. The first \u201cCore Limbic\u201d group included the left amygdala, hypothalamus, periaqueductal gray/thalamus regions, and amygdala/ventral striatum/ventral globus pallidus/thalamus regions, which the authors discuss as an integrative emotional center that plays a general role in evaluating affective significance. The second \u201cLateral Paralimbic\u201d group included the ventral anterior insula/frontal operculum/right temporal pole/ posterior orbitofrontal cortex, the anterior insula/ posterior orbitofrontal cortex, the ventral anterior insula/ temporal cortex/ orbitofrontal cortex junction, the midinsula/ dorsal putamen, and the ventral striatum /mid insula/ left hippocampus, which the authors suggest plays a role in motivation, contributing to the general valuation of stimuli and particularly in reward. The third \u201cMedial Prefrontal Cortex\u201d group included the dorsal medial prefrontal cortex, pregenual anterior cingulate cortex, and rostral dorsal anterior cingulate cortex, which the authors discuss as playing a role in both the generation and regulation of emotion. The fourth \u201cCognitive/ Motor Network\u201d group included right frontal operculum, the right interior frontal gyrus, and the pre-supplementray motor area/ left interior frontal gyrus, regions that are not specific to emotion, but instead appear to play a more general role in information processing and cognitive control. The fifth \u201cOccipital/ Visual Association\u201d group included areas V8 and V4 of the primary visual cortex, the medial temporal lobe, and the lateral occipital cortex, and the sixth \u201cMedial Posterior\u201d group included posterior cingulate cortex and area V1 of the primary visual cortex. The authors suggest that these regions play a joint role in visual processing and attention to emotional stimuli.",
            "score": 67.05971193313599
        },
        {
            "docid": "24514_35",
            "document": "Psychosis . Studies during acute experience of hallucinations demonstrate increased activity in primary or secondary sensory cortices. As auditory hallucinations are most common in psychosis, most robust evidence exists for increased activity in the left middle temporal gyrus, left superior temporal gyrus, and left inferior frontal gyrus (i.e. Broca's area). Activity in the ventral striatum, hippocampus, and ACC are related to the lucidity of hallucinations, and indicate that activation or involvement of emotional circuitry are key to the impact of abnormal activity in sensory cortices. Together, these findings indicate abnormal processing of internally generated sensory experiences, coupled with abnormal emotional processing, results in hallucinations. One proposed model involves a failure of feedforward networks from sensory cortices to the inferior frontal cortex, which normal cancel out sensory cortex activity during internally generated speech. The resulting disruption in expected and perceived speech is thought to produce lucid hallucinatory experiences.",
            "score": 52.40616703033447
        },
        {
            "docid": "34776464_4",
            "document": "Bilingual memory . One of the processes involved in analyzing which neural regions of the brain are involved in bilingual memory is a subtraction method. Researchers compare what has been impaired with what is functioning regularly. This contrast between the destroyed and intact regions of the brain, aids researchers in discovering the components of language processing. It has been found that under typical circumstances, when multiple languages are lost at the same time, they are usually regained in the same fashion. It is therefore presumed that areas of the brain, which are responsible for processing language, are potentially the same. There have been examples of cases where languages have been restored prior to one another and to a greater degree, but this is fairly uncommon. The techniques allowing researchers to observe brain activity in multilingual patients are conducted whilst the subject is simultaneously performing and processing a language. Research has proposed that the entire production and comprehension of language is most likely regulated and managed by neural pools, whose stations of communication are in the cortical and subcortical regions. It has been shown that there are no grounds on which to assume the existence of distinct cerebral organization of separate languages in the bilingual brain. That is to say, the cerebral regions that are engaged for both languages are the same. Although neurologists have a basic understanding of the underlying neural components and mechanisms of bilingual language, further research is necessary in order to fully understand or conclude any other findings.  Neuroimaging techniques such as fMRIs have shown that at least four brain areas are involved in bilingual switching: dorsolateral prefrontal cortex, inferior frontal cortex, anterior cingulate, and supramarginal gyrus. It is expected that switching from one language to another should involve different functional processes when compared to the brain of an individual who only speaks one language. However further studies on brain activation during this switching of languages needs to be done.",
            "score": 99.52975606918335
        },
        {
            "docid": "56439577_8",
            "document": "Temporal envelope and fine structure . Responses to the temporal-envelope cues of speech or other complex sounds persist up the auditory pathway, eventually to the various fields of the auditory cortex in many animals. In the Primary Auditory Cortex, responses can encode AM rates by phase-locking up to about 20\u201330\u00a0Hz, while faster rates induce sustained and often tuned responses. A topographical representation of AM rate has been demonstrated in the primary auditory cortex of awake macaques. This representation is approximately perpendicular to the axis of the tonotopic gradient, consistent with an orthogonal organization of spectral and temporal features in the auditory cortex. Combining these temporal responses with the spectral selectivity of A1 neurons gives rise to the spectro-temporal receptive fields that often capture well cortical responses to complex modulated sounds. In secondary auditory cortical fields, responses become temporally more sluggish and spectrally broader, but are still able to phase-lock to the salient features of speech and musical sounds. Tuning to AM rates below about 64\u00a0Hz is also found in the human auditory cortex as revealed by brain-imaging techniques (fMRI) and cortical recordings in epileptic patients (electrocorticography). This is consistent with neuropsychological studies of brain-damaged patients and with the notion that the central auditory system performs some form of spectral decomposition of the ENVp of incoming sounds. Interestingly, the ranges over which cortical responses encode well the temporal-envelope cues of speech have been shown to be predictive of the human ability to understand speech. In the human superior temporal gyrus (STG), an anterior-posterior spatial organization of spectro-temporal modulation tuning has been found in response to speech sounds, the posterior STG being tuned for temporally fast varying speech sounds with low spectral modulations and the anterior STG being tuned for temporally slow varying speech sounds with high spectral modulations.",
            "score": 84.6108021736145
        },
        {
            "docid": "14158261_18",
            "document": "Temporoparietal junction . Theory of mind requires the collaboration of functionally related regions of the brain to form the distinction between self and other mental states and to create a comprehensive understanding of those mental states so that we may recognize, understand, and predict behavior. In general the theory of mind process is mediated by the dopaminergic-serotonergic system, which involves the TPJ as well as other associative regions necessary for mentalizing. Recent studies suggest that both the left TPJ, working in conjunction with the frontal cortex, and the right TPJ are involved in the representation of mental states; furthermore they suggest that the TPJ is particularly active in making the distinction between the mental states of self and others. A study in \"Nature Neuroscience\" from 2004 describes how the TPJ is involved in processing socially relevant cues including gaze direction and goal-directed action and also explains that results from the study show that lesions to this area of the brain result in an impaired ability to detect another persons belief. Moreover, studies have reported an increase in activity in the TPJ when patients are absorbing information through reading or images regarding other peoples' beliefs but not while observing information about physical control stimuli. Some studies, however, have shown that the TPJ, along with the cingulate cortex, is more specifically involved with attributing beliefs, but the process of mentalizing more generally is associated more with the medial prefrontal cortex. Another study in \"Current Biology\" from 2012 identifies the importance of the TPJ in both low-level, such as simple discrimination, and high-level, such as the ability to empathize, sociocognitive operations. In July 2011, a review from \"Neuropsychologia\" presented a model of the mentalizing network that established that mental states are first detected in the TPJ. The TPJ is composed of two discrete anatomical regions, the inferior parietal lobule (IPL) and the caudal parts of the superior temporal sulcus (pSTS), and both are active in the process of distinction between mental states of different individuals; thus, it is probable that this detection is the outcome of the combination and coordination of these two parts. Additionally, the right TPJ is involved in the ventral attention stream and contributes to the ability to focus attention on a particular stimuli or objective. It has also been observed that the interaction and communication between the dorsal and ventral streams involves the TPJ.",
            "score": 93.61904764175415
        },
        {
            "docid": "8305_11",
            "document": "Dyslexia . Modern neuroimaging techniques such as functional magnetic resonance imaging (fMRI) and positron emission tomography (PET) have shown a correlation between both functional and structural differences in the brains of children with reading difficulties. Some dyslexics show less electrical activation in parts of the left hemisphere of the brain involved with reading, such as the inferior frontal gyrus, inferior parietal lobule, and the middle and ventral temporal cortex. Over the past decade, brain activation studies using PET to study language have produced a breakthrough in the understanding of the neural basis of language. Neural bases for the visual lexicon and for auditory verbal short-term memory components have been proposed, with some implication that the observed neural manifestation of developmental dyslexia is task-specific (i.e. functional rather than structural). fMRIs in dyslexics have provided important data which point to the interactive role of the cerebellum and cerebral cortex as well as other brain structures.",
            "score": 68.52806687355042
        },
        {
            "docid": "32018467_7",
            "document": "Christian Keysers . After finishing his master, Christian Keysers decided to concentrate on a subfield of cognitive neuroscience called social neuroscience that uses neuroscience methods to understand how we process the social world. He therefore performed his doctoral studies at the University of St Andrews with David Ian Perrett, one of the founding father of the field, to understand how the brain processes faces and facial expressions. This thesis work led to new insights into how quickly the brain can process the faces of others. During this period, Keysers became fascinated with the question of how the brain can attach meaning to the faces of others. How is it for instance, that we understand that a certain grimace would signal that another person is happy? How do we understand that a certain bodily movement towards a glass indicates that the other person aims to grasp a glass? In 1999, Keysers was exposed to a visit of Vittorio Gallese, who presented his recent discovery of mirror neurons in the Psychology department lecture series. This deeply influenced Keysers who decided to move to the lab of Giacomo Rizzolatti to undertake further studies on how these fascinating neurons could contribute to social perception. In 2000, after finishing his doctorate, Christian Keysers moved to the University of Parma to study mirror neurons. In early work there demonstrated that mirror neurons in the premotor cortex not only respond to the sight of actions, but also when actions can only be deduced or heard, leading to a publication in the journal \"Science\". This work had tremendous impact on the field, as it suggested that the premotor cortex could play a central, modality independent role in perception and may lay the origin for the evolution of speech in humans.  Together this work indicated that brain regions involved in our own actions play a role in how we process the actions of others. Keysers wondered whether a similar principle may underlie how we process the tactile sensations and emotions of others, and became increasingly independent of the research focus on the motor system in Parma. At the time, Keysers had also met his to be wife, Valeria Gazzola, a biologist in the final phases of her studies, and together they decided to explore if the somatosensory system might be involved in perceiving the sensations of others. Via a fruitful collaboration with the French neuroimaging specialist Bruno Wicker, they used functional magnetic resonance imaging, and showed for the first time, that the secondary somatosensory cortex, previously thought only to represent a persons own experiences of touch, is also activated when seeing someone or something else be touched. They also showed that the insula, thought only to respond to the experience of first-hand emotions, was also activated when we see another individual experience similar emotions. Together this indicated a much more general principle than the original mirror neuron theory, in which people process the actions, sensations and emotions of others by vicariously activating owns own actions, sensations and emotions. Jointly, this work laid the foundation of the neuroscientific investigation of empathy.",
            "score": 127.27015197277069
        },
        {
            "docid": "6171525_14",
            "document": "Internal monologue . Studies have revealed the differences in neural activations of inner dialogues versus those of monologues. Functional MRI imaging studies have shown that monologic internal speech involves the activation of the superior temporal gyrus and the left inferior frontal gyrus, which is the standard language system that is activated during any kind of speech. However, dialogical inner speech implicates several additional neural regions. Studies have indicated overlap with regions involved with thinking about other minds.",
            "score": 55.0930495262146
        },
        {
            "docid": "1732213_21",
            "document": "Language processing in the brain . The strongest correlations between language fluency and cortical thicknesses were found in the temporal lobe and temporal\u2013parietal junction. Significant correlations were also found in the auditory cortex, the somatosensory cortex related to the organs responsible for speech (lips, mouth), and frontal and parietal regions related to attention and performance monitoring. The frontal and parietal regions are also evident in the right hemisphere.",
            "score": 93.98943948745728
        },
        {
            "docid": "433584_4",
            "document": "McGurk effect . Vision is the primary sense for humans, but speech perception is multimodal, which means that it involves information from more than one sensory modality, in particular, audition and vision. The McGurk effect arises during phonetic processing because the integration of audio and visual information happens early in speech perception. The McGurk effect is very robust; that is, knowledge about it seems to have little effect on one's perception of it. This is different from certain optical illusions, which break down once one 'sees through' them. Some people, including those that have been researching the phenomenon for more than twenty years, experience the effect even when they are aware that it is taking place. With the exception of people who can identify most of what is being said from speech-reading alone, most people are quite limited in their ability to identify speech from visual-only signals. A more extensive phenomenon is the ability of visual speech to increase the intelligibility of heard speech in a noisy environment. Visible speech can also alter the perception of perfectly audible speech sounds when the visual speech stimuli are mismatched with the auditory speech. Normally, speech perception is thought to be an auditory process; however, our use of information is immediate, automatic, and, to a large degree, unconscious and therefore, despite what is widely accepted as true, speech is not only something we hear. Speech is perceived by all of the senses working together (seeing, touching, and listening to a face move). The brain is often unaware of the separate sensory contributions of what it perceives. Therefore, when it comes to recognizing speech the brain cannot differentiate whether it is seeing or hearing the incoming information.",
            "score": 147.7239738702774
        },
        {
            "docid": "5505463_3",
            "document": "Bimodal bilingualism . Most modern neurological studies of bilingualism employ functional neuroimaging techniques to elucidate the neurological underpinnings of multilingualism and how multilingualism is beneficial to the brain. Neuroimaging and other neurological studies have demonstrated in recent years that multilingualism has a significant impact on the human brain. The mechanisms required by bilinguals to \"code switch\" (a linguistic term used to describe the rapid alternating between multiple languages within a conversation or discourse), not only demonstrate increased connectivity and density of the neural network in multilinguals, but also appear to provide protection against damage due to age and age-related pathologies, such as Alzheimer's. Multilingualism, especially bimodal multilingualism, can help slow to process of cognitive decline in aging. It is thought that this is a result of the increased work load that the executive system, housed mostly in the frontal cortex, must assume in order to successfully control the use of multiple languages at once. This means that the cortex must be more finely tuned, which results in a \"neural reserve\" that then has neuroprotective benefits. Gray matter volume (GMV) has been shown to be significantly preserved in bimodal bilinguals as compared to monolinguals in multiple brain areas, including the hippocampus, amygdala, anterior temporal lobes, and left insula. Similarly, neuroimaging studies that have compared monolinguals, unimodal bilinguals, and bimodal bilinguals provide evidence that deaf signers exhibit brain activation in patterns different than those of hearing signers, especially in regards to the left superior temporal sulcus. In deaf signers, activation of the superior temporal sulcus is highly lateralized to the left side during facial recognition tasks, while this lateralization was not present in hearing, bimodal signers. Bilinguals also require an effective and fast neural control system to allow them to select and control their languages even while code switching rapidly. Evidence indicates that the left caudate nucleus\u2014a centrally located brain feature that is near the thalamus and the basal ganglia\u2014is an important part of this mechanism, as bilinguals tend to have significantly increased GMV and activation in this region as compared to monolinguals, especially during active code switching tasks. As implied by the significant preservation of gray matter in the hippocampi (an area of the brain largely associated with memory consolidation and higher cognitive function, such as decision-making) of bimodal bilinguals, areas of the brain that help control phonological working memory tend to also have higher activation in those individuals who are proficient in two or more languages. There is also evidence that suggests that the age at which an individual acquires a second language may play a significant role in the varying brain functions associated with bilingualism. For example, individuals who acquired their second language early (before the age of 10) tend to have drastically different activation patterns than do late learners. However, late learners who achieve full proficiency in their second language tend to show similar patterns of activation during auditory tasks regardless of which language is being used, whereas early learners tend to activate different brain areas depending upon which language is being used. Along with the neuroprotective benefits that help to prevent onset of age-related cognitive issues such as dementia, bimodal bilinguals also experience a slightly different pattern of organization of language in the brain. While non-hearing-impaired bimodal bilinguals showed less parietal activation than deaf signers when asked to use only sign language, those same bimodal bilinguals demonstrated greater left parietal activation than did monolinguals. Parietal activation is not typically associated with language production bur rather with motor activity. Therefore, it is logical that bimodal bilinguals, when switching between speech- and sign-based language, stimulate their left parietal areas as a result of their increased need to combine both motor action and language production.",
            "score": 106.83001184463501
        },
        {
            "docid": "27414898_4",
            "document": "Body transfer illusion . While the experiment was going on the experimenters also recorded the activity of their brains with a functional MRI scanner. The scans showed increased activity in the parietal lobe and then, right after, as the subjects began to experience the rubber hand as their own, in the premotor cortex, the region of the brain involved in planning movements. On the other hand, when the stroking of the real and rubber hands was uncoordinated and the subjects did not experience the rubber hand as their own, the premotor cortex did not become activated. From this the experimenters concluded that the parietal cortex was involved with visual and touch processing. The premotor cortex, getting transmitted information from the parietal cortex, was involved with the feeling of ownership of the rubber hand.",
            "score": 85.53638482093811
        },
        {
            "docid": "496944_13",
            "document": "Hypergraphia . Several regions of the brain are involved in the act of writing. Primary areas are the superior parietal cortex and the frontal lobe, the region of the brain that plans out movement. An area of the frontal lobe that is especially active is Exner's area, located in the premotor cortex. The physical motion of the hand is controlled by the primary motor cortex, also located in the frontal lobe, and the right cerebellum. Writing creatively and generating ideas, on the other hand, is controlled by the limbic system, specifically involving the activity of the hippocampus, which is important in the retrieval of long-term memories. Words and ideas are cognized and understood by the temporal lobes, and these temporal lobes are connected to the limbic system.  Although hypergraphia cannot be isolated to one specific part of the brain, some areas are known to have more of an effect than others. The hippocampus has been found to play a role in the occurrence of temporal lobe epilepsy and schizophrenia. In one study, rats induced with temporal lobe epilepsy showed an increase in dopamine neural activity in the hippocampus. Because hypergraphia has been linked to temporal lobe epilepsy and schizophrenia, the hippocampus could have an effect on hypergraphia as well. In another study, patients with bilateral hippocampal atrophy (BHA) showed signs of having Geschwind syndrome, including hypergraphia.",
            "score": 95.65217518806458
        },
        {
            "docid": "36058569_5",
            "document": "Frank H. Guenther . Frank Guenther\u2019s research is aimed at uncovering the neural computations underlying the processing of speech by the human brain. He is the originator of the Directions Into Velocities of Articulators (DIVA) model, which is currently the leading model of the neural computations underlying speech production. This model mathematically characterizes the computations performed by each brain region involved in speech production as well as the function of the interconnections between these regions. The model has been supported by a wide range of experimental tests of model predictions, including electromagnetic articulometry studies investigating speech movements, auditory perturbation studies involving modification of a speaker\u2019s feedback of his/her own speech in real time, and functional magnetic resonance imaging studies of brain activity during speech, though some parts of the model remain to be experimentally verified. The DIVA model has been used to investigate the neural underpinnings of a number of communication disorders, including stuttering apraxia of speech, and hearing-impaired speech.",
            "score": 121.97555685043335
        },
        {
            "docid": "403676_28",
            "document": "Gesture . Gestures are processed in the same areas of the brain as speech and sign language such as the left inferior frontal gyrus (Broca's area) and the posterior middle temporal gyrus, posterior superior temporal sulcus and superior temporal gyrus (Wernicke's area). It has been suggested that these parts of the brain originally supported the pairing of gesture and meaning and then were adapted in human evolution \"for the comparable pairing of sound and meaning as voluntary control over the vocal apparatus was established and spoken language evolved\". As a result, it underlies both symbolic gesture and spoken language in the present human brain. Their common neurological basis also supports the idea that symbolic gesture and spoken language are two parts of a single fundamental semiotic system that underlies human discourse. The linkage of hand and body gestures in conjunction with speech is further revealed by the nature of gesture use in blind individuals during conversation. This phenomenon uncovers a function of gesture that goes beyond portraying communicative content of language and extends David McNeill's view of the gesture-speech system. This suggests that gesture and speech work tightly together, and a disruption of one (speech or gesture) will cause a problem in the other. Studies have found strong evidence that speech and gesture are innately linked in the brain and work in an efficiently wired and choreographed system. McNeill's view of this linkage in the brain is just one of three currently up for debate; the others declaring gesture to be a \"support system\" of spoken language or a physical mechanism for lexical retrieval.",
            "score": 109.02473247051239
        },
        {
            "docid": "26685741_28",
            "document": "Sleep and memory . A blood-oxygen-level dependent (BOLD) fMRI was used in a study by Drummond et al. to measure the brain's response to verbal learning following sleep deprivation. An fMRI recorded brain activity during a verbal learning task of participants either having a normal night of sleep or those deprived of 34.7 (\u00b1 1.2) hours of sleep. The task alternated between a baseline condition of determining whether nouns were upper or lower case and an experimental condition of memorizing a list of nouns. The results of the study indicate that performance is significantly worse on free recall of the list of nouns when sleep deprived (an average of 2.8 \u00b1 2 words) compared to having a normal night of sleep (4.7 \u00b1 4 words). In terms of brain regions activated, the left prefrontal cortex, premotor cortex, and temporal lobes were found to be activated during the task in the rested state and discrete regions of the prefrontal cortex were even more activated during the task in the sleep deprived state. As well, the bilateral parietal lobe, left middle frontal gyrus, and right interior frontal gyrus were found to be activated for those sleep deprived. The implication of these findings are that the brain can initially compensate for the effects of sleep deprivation while maintaining partially intact performance, which declines with an increasing time-on-task. This initial compensation may be found in the bilateral regions of both frontal and parietal lobes and the activation of the prefrontal cortex is significantly correlated with sleepiness.",
            "score": 96.45074439048767
        },
        {
            "docid": "37198050_9",
            "document": "Body part as an object . There have been many studies relating activated brain areas to tool-use, in both physical object manipulation and pantomimes. Meta-analyses have found that tool-use is largely lateralized in the left-hemisphere of the brain and independent of handedness. Specifically, the brain region which showed the greatest activity was the left superior parietal lobule. Other areas that showed significant activity was bilaterally in both the ventral and dorsolateral premotor cortex, areas by the inferior parietal lobule, and tissue around the medial temporal gyrus. Furthermore, even when object-use was imagined, activation was found to be largely lateralized in the left hemisphere and was very similar to the brain activation in actual tool-use and pantomiming. The only significant difference was additional activation in the left occipito-parietal region.",
            "score": 76.21034646034241
        }
    ],
    "r": [
        {
            "docid": "433584_4",
            "document": "McGurk effect . Vision is the primary sense for humans, but speech perception is multimodal, which means that it involves information from more than one sensory modality, in particular, audition and vision. The McGurk effect arises during phonetic processing because the integration of audio and visual information happens early in speech perception. The McGurk effect is very robust; that is, knowledge about it seems to have little effect on one's perception of it. This is different from certain optical illusions, which break down once one 'sees through' them. Some people, including those that have been researching the phenomenon for more than twenty years, experience the effect even when they are aware that it is taking place. With the exception of people who can identify most of what is being said from speech-reading alone, most people are quite limited in their ability to identify speech from visual-only signals. A more extensive phenomenon is the ability of visual speech to increase the intelligibility of heard speech in a noisy environment. Visible speech can also alter the perception of perfectly audible speech sounds when the visual speech stimuli are mismatched with the auditory speech. Normally, speech perception is thought to be an auditory process; however, our use of information is immediate, automatic, and, to a large degree, unconscious and therefore, despite what is widely accepted as true, speech is not only something we hear. Speech is perceived by all of the senses working together (seeing, touching, and listening to a face move). The brain is often unaware of the separate sensory contributions of what it perceives. Therefore, when it comes to recognizing speech the brain cannot differentiate whether it is seeing or hearing the incoming information.",
            "score": 147.72396850585938
        },
        {
            "docid": "9736296_43",
            "document": "Linguistic performance . Unacceptable Sentences are ones which, although are grammatical, are not considered proper utterances. They are considered unacceptable due to the lack of our cognitive systems to process them. Speakers and listeners can be aided in the performance and processing of these sentences by eliminating time and memory constraints, increasing motivation to process these utterances and using pen and paper. In English there are three types of sentences that are grammatical but are considered unacceptable by speakers and listeners. When a speaker makes an utterance they must translate their ideas into words, then syntactically proper phrases with proper pronunciation. The speaker must have prior world knowledge and an understanding of the grammatical rules that their language enforces. When learning a second language or with children acquiring their first language, speakers usually have this knowledge before they are able to produce them. Their speech is usually slow and deliberate, using phrases they have already mastered, and with practice their skills increase. Errors of linguistic performance are judged by the listener giving many interpretations if an utterance is well-formed or ungrammatical depending on the individual. As well the context in which an utterance is used can determine if the error would be considered or not. When comparing \"Who must telephone her?\" and \"Who need telephone her?\" the former would be considered the ungrammatical phrase. However, when comparing it to \"Who want telephone her?\" it would be considered the grammatical phrase. The listener may also be the speaker. When repeating sentences with errors if the error is not comprehended then it is performed. As well if the speaker does notice the error in the sentence they are supposed to repeat they are unaware of the difference between their well-formed sentence and the ungrammatical sentence. An unacceptable utterance can also be performed due to a brain injury. Three types of brain injuries that could cause errors in performance were studied by Fromkin are dysarthria, apraxia and literal paraphasia. Dysarthria is a defect in the neuromuscular connection that involves speech movement. The speech organs involved can be paralyzed or weakened, making it difficult or impossible for the speaker to produce a target utterance. Apraxia is when there is damage to the ability to initiate speech sounds with no paralysis or weakening of the articulators. Literal paraphasia causes disorganization of linguistic properties, resulting in errors of word order of phonemes. Having a brain injury and being unable to perform proper linguistic utterances, some individuals are still able to process complex sentences and formulate syntactically well formed sentences in their mind. Child productions when they are acquiring language are full of errors of linguistic performance. Children must go from imitating adult speech to create new phrases of their own. They will need to use their cognitive operations of the knowledge of their language they are learning to determine the rules and properties of that language. The following are examples of errors in English speaking children's productions.",
            "score": 143.56101989746094
        },
        {
            "docid": "236809_36",
            "document": "Recall (memory) . Recall memory is linked with instincts and mechanisms in order to remember how an event happened to learn from it or avoid the agitator, connections are made with emotions. For instance, if a speaker is very calm and neutral, the effectiveness of encoding memory is very low and listeners get the gist of what the speaker is discussing. On the other hand, if a speaker is shouting and/or using emotionally driven words, listeners tend to remember key phrases and the meaning of the speech. This is full access of the fight or flight mechanism all people have functioning in the brain, but based on what triggers this mechanism will lead to better recall of it. People tend to focus their attention on cues that are loud, very soft, or something unusual. This makes the auditory system pick up the differences in regular speaking and meaningful speech, when something significant is spoken in the discussion people home in on the message at that part of the speech but tend to lose the other part of the discussion. Our brains sense differences in speech and when those differences occur the brain encodes that part of speech into memory and the information can be recalled for future reference.",
            "score": 141.34571838378906
        },
        {
            "docid": "315084_3",
            "document": "Lip reading . Although speech perception is considered to be an auditory skill, it is intrinsically multimodal, since producing speech requires the speaker to make movements of the lips, teeth and tongue which are often visible in face-to-face communication. Information from the lips and face supports aural comprehension and most fluent listeners of a language are sensitive to seen speech actions (see McGurk effect). The extent to which people make use of seen speech actions varies with the visibility of the speech action and the knowledge and skill of the perceiver.",
            "score": 140.00318908691406
        },
        {
            "docid": "20011748_7",
            "document": "Apraxia of speech . Apraxia of speech can be diagnosed by a speech language pathologist (SLP) through specific exams that measure oral mechanisms of speech. The oral mechanisms exam involves tasks such as pursing lips, blowing, licking lips, elevating the tongue, and also involves an examination of the mouth. A complete exam also involves observation of the patient eating and talking. SLPs do not agree on a specific set of characteristics that make up the apraxia of speech diagnosis, so any of the characteristics from the section above could be used to form a diagnosis. Patients may be asked to perform other daily tasks such as reading, writing, and conversing with others. In situations involving brain damage, an MRI brain scan also helps identify damaged areas of the brain.",
            "score": 138.0343017578125
        },
        {
            "docid": "40621603_5",
            "document": "Linguistic intelligence . Speech production is process by which a thought in the brain is converted into an understandable auditory form. This is a multistage mechanism that involves many different areas of the brain. The first stage is planning, where the brain constructs words and sentences that turn the thought into an understandable form. This occurs primarily in the inferior frontal cortex, specifically in an area known as Broca's area. Next, the brain must plan how to physically create the sounds necessary for speech by linking the planned speech with known sounds, or phonemes. While the location of these associations is not known, it is known that the supplementary motor area plays a key role in this step. Finally, the brain must signal for the words to actually be spoken. This is carried out by the premotor cortex and the motor cortex. In most cases, speech production is controlled by the left hemisphere. In a series of studies, Wilder Penfield, among others, probed the brains of both right-handed (generally left-hemisphere dominant) and left-handed (generally right-hemisphere dominant) patients. They discovered that, regardless of handedness, the left hemisphere was almost always the speech controlling side. However, it has been discovered that in cases of neural stress (hemorrhage, stroke, etc.) the right hemisphere has the ability to take control of speech functions.",
            "score": 137.4257354736328
        },
        {
            "docid": "40901980_2",
            "document": "Developmental verbal dyspraxia . Developmental verbal dyspraxia (DVD), also known as childhood apraxia of speech (CAS) and developmental apraxia of speech (DAS), is when children have problems saying sounds, syllables, and words. This is not because of muscle weakness or paralysis. The brain has problems planning to move the body parts (e.g., lips, jaw, tongue) needed for speech. The child knows what they want to say, but their brain has difficulty coordinating the muscle movements necessary to say those words. The exact cause of this disorder is unknown. Some observations suggest a genetic cause of DVD, as many with the disorder have a family history of communication disorders. There is no cure for DVD, but with appropriate, intensive intervention, people with this motor speech disorder can improve significantly.",
            "score": 136.75193786621094
        },
        {
            "docid": "37691878_17",
            "document": "Phonemic restoration effect . Much like the McGurk Effect, when listeners were also able to see the words being spoken, they were much more likely to correctly identify the missing phonemes. Like every sense, the brain will use every piece of information it deems important to make a judgement about what it is perceiving. Using the visual cues of mouth movements, the brain will you both in top-down processing to make a decision about what phoneme is supposed to be heard. Vision is the primary sense for humans and for the most part assists in speech perception the most.",
            "score": 132.0401611328125
        },
        {
            "docid": "17524_35",
            "document": "Language . Early work in neurolinguistics involved the study of language in people with brain lesions, to see how lesions in specific areas affect language and speech. In this way, neuroscientists in the 19th century discovered that two areas in the brain are crucially implicated in language processing. The first area is Wernicke's area, which is in the posterior section of the superior temporal gyrus in the dominant cerebral hemisphere. People with a lesion in this area of the brain develop receptive aphasia, a condition in which there is a major impairment of language comprehension, while speech retains a natural-sounding rhythm and a relatively normal sentence structure. The second area is Broca's area, in the posterior inferior frontal gyrus of the dominant hemisphere. People with a lesion to this area develop expressive aphasia, meaning that they know what they want to say, they just cannot get it out. They are typically able to understand what is being said to them, but unable to speak fluently. Other symptoms that may be present in expressive aphasia include problems with fluency, articulation, word-finding, word repetition, and producing and comprehending complex grammatical sentences, both orally and in writing. Those with this aphasia also exhibit ungrammatical speech and show inability to use syntactic information to determine the meaning of sentences. Both expressive and receptive aphasia also affect the use of sign language, in analogous ways to how they affect speech, with expressive aphasia causing signers to sign slowly and with incorrect grammar, whereas a signer with receptive aphasia will sign fluently, but make little sense to others and have difficulties comprehending others' signs. This shows that the impairment is specific to the ability to use language, not to the physiology used for speech production.",
            "score": 131.15086364746094
        },
        {
            "docid": "2363287_6",
            "document": "Visual learning . Various areas of the brain work together in a multitude of ways in order to produce the images that we see with our eyes and that are encoded by our brains. The basis of this work takes place in the visual cortex of the brain. The visual cortex is located in the occipital lobe of the brain and harbors many other structures that aid in visual recognition, categorization, and learning. One of the first things the brain must do when acquiring new visual information is recognize the incoming material. Brain areas involved in recognition are the inferior temporal cortex, the superior parietal cortex, and the cerebellum. During tasks of recognition, there is increased activation in the left inferior temporal cortex and decreased activation in the right superior parietal cortex. Recognition is aided by neural plasticity, or the brain's ability to reshape itself based on new information. Next the brain must categorize the material. The three main areas that are used when categorizing new visual information are the orbitofrontal cortex and two dorsolateral prefrontal regions which begin the process of sorting new information into groups and further assimilating that information into things that you might already know. After recognizing and categorizing new material entered into the visual field, the brain is ready to begin the encoding process \u2013 the process which leads to learning. Multiple brain areas are involved in this process such as the frontal lobe, the right extrastriate cortex, the neocortex, and again, the neostriatum. One area in particular, the limbic-diencephalic region, is essential for transforming perceptions into memories. With the coming together of tasks of recognition, categorization and learning; schemas help make the process of encoding new information and relating it to things you already know much easier. One can remember visual images much better when they can apply it to an already known schema. Schemas actually provide enhancement of visual memory and learning.",
            "score": 130.96034240722656
        },
        {
            "docid": "25140_47",
            "document": "Perception . \"Speech perception\" is the process by which spoken languages are heard, interpreted and understood. Research in speech perception seeks to understand how human listeners recognize speech sounds and use this information to understand spoken language. The sound of a word can vary widely according to words around it and the tempo of the speech, as well as the physical characteristics, accent and mood of the speaker. Listeners manage to perceive words across this wide range of different conditions. Another variation is that reverberation can make a large difference in sound between a word spoken from the far side of a room and the same word spoken up close. Experiments have shown that people automatically compensate for this effect when hearing speech.",
            "score": 130.6620635986328
        },
        {
            "docid": "26603942_3",
            "document": "Silent speech interface . Silent speech interface systems have been created using ultrasound and optical camera input of tongue and lip movements. Electromagnetic devices are another technique for tracking tongue and lip movements. The detection of speech movements by electromyography of speech articulator muscles and the larynx is another technique. Another source of information is the vocal tract resonance signals that get transmitted through bone conduction called non-audible murmurs.  They have also been created as a brain\u2013computer interface using brain activity in the motor cortex obtained from intracortical microelectrodes.",
            "score": 129.9826202392578
        },
        {
            "docid": "10042066_2",
            "document": "Developmental linguistics . Developmental linguistics is the study of the development of linguistic ability in an individual, particularly the acquisition of language in childhood. It involves research into the different stages in language acquisition, language retention, and language loss in both first and second languages, in addition to the area of bilingualism. Before infants can speak, the neural circuits in their brains are constantly being influenced by exposure to language. The neurobiology of language contains a \"critical period\" in which children are most sensitive to language. The different aspects of language have varying \"critical periods\". Studies show that the critical period for phonetics is toward the end of the first year. At 18 months, a toddler's vocabulary vastly expands. The critical period for syntactic learning is 18-36 months. Infants of different mother languages can be differentiated at the age of 10 months. At 20 weeks they begin vocal imitation. Beginning when babies are about 12 months, they take on computational learning and social learning. Social interactions for infants and toddlers is important because it helps associate \"perception and action\". In-person social interaction rather than audio or video better facilitates learning in babies because they learn from how other people respond to them, especially their mothers. Babies have to learn to mimic certain syllables, which takes practice in manipulating tongue and lip movement. Sensory-motor learning in speech is linked to exposure to speech, which is very sensitive to language. Infants exposed to Spanish exhibit a different vocalization than infants exposed to English. One study took infants that were learning English and made them listen to Spanish in 12 sessions. The result showed consequent alterations in their vocalization, which demonstrated Spanish prosody.  One study used MEG to record activation in the brains of newborns, 6 months olds and 12 months olds while presenting them with syllables, harmonics and non-speech sounds. For the 6 month and 12 month old, the auditory and motor areas responded to speech. The newborn showed auditory activation but not motor activation. Another study presented 3 month olds with sentences and recorded their brain activity via fMRI motor speech areas did activate. These studies suggest that the link between perception and action begins to develop at 3 months. When babies are young, they are actually the most sensitive to distinguishing all phonetic units. During an infant\u2019s 1st year of life, they have to differentiate between about 40 phonetic units. When they are older they have usually been exposed to their native language so much that they lose this ability and can only distinguish phonetic units in their native language. Even at 12 months babies exhibit a deficit in differentiated non-native sounds. However, their ability to distinguish sounds in their native language continues to improve and become more fine-tuned. For example, Japanese learning infants learn that there is no differentiation between /r/ and /l/. However, in English, \"rake\" and \"lake\" are two different words. Japanese babies eventually lose their ability to distinguish between /r/ and /l/. Similarly, a Spanish learning infant cannot form words until they learn the difference between works like \"bano\" and \"pano\", because the /p/ sound is different than the /b/ sound. English learning babies do not learn to differentiate between the two.",
            "score": 129.8657989501953
        },
        {
            "docid": "315084_7",
            "document": "Lip reading . Many factors affect the visibility of a speaking face, including illumination, movement of the head/camera, frame-rate of the moving image and distance from the viewer (see e.g.). Head movement that accompanies normal speech can also improve lip-reading, independently of oral actions. However, when lip-reading connected speech, the viewer's knowledge of the spoken language, familiarity with the speaker and style of speech, and the context of the lip-read material are as important as the visibility of the speaker. While most hearing people are sensitive to seen speech, there is great variability in individual speechreading skill. Good lipreaders are often more accurate than poor lipreaders at identifying phonemes from visual speech.",
            "score": 129.81390380859375
        },
        {
            "docid": "12209564_6",
            "document": "Aprosodia . Research into the perisylvan region of the right hemisphere has shown that there are similarly mapped analogues to the speech center in the left hemisphere. This is especially evident in those areas resembling Broca's area and Wernicke's area. The similarity of these regions has led scientists to view aprosodias in a similar manner to how some aphasias are viewed. Because the presence of an aphasia is often more pronounced in an individual than an aprosodia might be, aphasias have traditionally been more heavily studied. Because aphasias are rooted in deficiencies in language modalities rather than affective aspects of language, it has been easier to characterize the underlying impairment caused by brain damage (e.g. inability to choose the right word or inability to speak due to motor control). Combining aphasic research with right-left analogue mapping has allowed for researchers to produce hypotheses on the underlying process behind various aprosodias.  Additionally, in studying the brain regions associated with aprosodia, brain imaging tests were performed to determine if aprosodia is both a lateralized and dominant function of the right hemisphere areas of language production. Aprosodia can be considered a dominant function of the right hemisphere because strong correlation was found between deficits in affective prosody and distribution of lesions in the cortices of those with right brain damage. No correlation was found between the distribution of cortical lesions in patients with left brain damage and the types of aphasic deficits pronounced in those patients. Aprosodia can be considered a lateralized function of the right hemisphere because of the differences in the ability of a patient to respond to affective prosodic information in those with left brain damage when compared to those with right brain damage. Patients with affective-prosodic deficits in the left hemisphere (dysprosodic patients) showed improvement in understanding and repeating prosodic information when other conveyed linguistic information was simplified, i.e. requiring the patient to mainly determine prosodic information contained in an interaction. This improvement in processing affective prosodic information under reduced linguistic processing demands did not occur for patients with right brain damage.",
            "score": 129.0198516845703
        },
        {
            "docid": "17363313_7",
            "document": "Mass Action Principle (neuroscience) . There is evidence supporting both the mass action principle and functional specialization within the brain. Functional specialization is the idea that functions are localized within the brain and can only be carried out by particular area(s) of the brain. Some tasks appear to work on the mass action principle, with lesions causing less drastic effects than would be expected if the tasks were localized within the brain. This was shown in Lashley's rat maze experiments, in which the amount of tissue removed was more important to the rat's performance than where the tissue was removed from within the brain. There are, however, examples of highly specialized areas of the brain in which even small amounts of damage can cause dramatic effects on people's abilities to perform certain tasks. Two such areas effect the comprehension of speech and the ability to produce coherent speech, Wernicke's area and Broca's area, respectively.",
            "score": 128.87188720703125
        },
        {
            "docid": "2088_20",
            "document": "Aphasia . There have been many instances showing that there is a form of aphasia among deaf individuals. Sign languages are, after all, forms of language that have been shown to use the same areas of the brain as verbal forms of language. Mirror neurons become activated when an animal is acting in a particular way or watching another individual act in the same manner. These mirror neurons are important in giving an individual the ability to mimic movements of hands. Broca's area of speech production has been shown to contain several of these mirror neurons resulting in significant similarities of brain activity between sign language and vocal speech communication. Facial communication is a significant portion of how animals interact with each other. Humans use facial movements to create, what other humans perceive, to be faces of emotions. While combining these facial movements with speech, a more full form of language is created which enables the species to interact with a much more complex and detailed form of communication. Sign language also uses these facial movements and emotions along with the primary hand movement way of communicating. These facial movement forms of communication come from the same areas of the brain. When dealing with damages to certain areas of the brain, vocal forms of communication are in jeopardy of severe forms of aphasia. Since these same areas of the brain are being used for sign language, these same, at least very similar, forms of aphasia can show in the Deaf community. Individuals can show a form of Wernicke's aphasia with sign language and they show deficits in their abilities in being able to produce any form of expressions. Broca's aphasia shows up in some people, as well. These individuals find tremendous difficulty in being able to actually sign the linguistic concepts they are trying to express.",
            "score": 128.4528045654297
        },
        {
            "docid": "32018467_7",
            "document": "Christian Keysers . After finishing his master, Christian Keysers decided to concentrate on a subfield of cognitive neuroscience called social neuroscience that uses neuroscience methods to understand how we process the social world. He therefore performed his doctoral studies at the University of St Andrews with David Ian Perrett, one of the founding father of the field, to understand how the brain processes faces and facial expressions. This thesis work led to new insights into how quickly the brain can process the faces of others. During this period, Keysers became fascinated with the question of how the brain can attach meaning to the faces of others. How is it for instance, that we understand that a certain grimace would signal that another person is happy? How do we understand that a certain bodily movement towards a glass indicates that the other person aims to grasp a glass? In 1999, Keysers was exposed to a visit of Vittorio Gallese, who presented his recent discovery of mirror neurons in the Psychology department lecture series. This deeply influenced Keysers who decided to move to the lab of Giacomo Rizzolatti to undertake further studies on how these fascinating neurons could contribute to social perception. In 2000, after finishing his doctorate, Christian Keysers moved to the University of Parma to study mirror neurons. In early work there demonstrated that mirror neurons in the premotor cortex not only respond to the sight of actions, but also when actions can only be deduced or heard, leading to a publication in the journal \"Science\". This work had tremendous impact on the field, as it suggested that the premotor cortex could play a central, modality independent role in perception and may lay the origin for the evolution of speech in humans.  Together this work indicated that brain regions involved in our own actions play a role in how we process the actions of others. Keysers wondered whether a similar principle may underlie how we process the tactile sensations and emotions of others, and became increasingly independent of the research focus on the motor system in Parma. At the time, Keysers had also met his to be wife, Valeria Gazzola, a biologist in the final phases of her studies, and together they decided to explore if the somatosensory system might be involved in perceiving the sensations of others. Via a fruitful collaboration with the French neuroimaging specialist Bruno Wicker, they used functional magnetic resonance imaging, and showed for the first time, that the secondary somatosensory cortex, previously thought only to represent a persons own experiences of touch, is also activated when seeing someone or something else be touched. They also showed that the insula, thought only to respond to the experience of first-hand emotions, was also activated when we see another individual experience similar emotions. Together this indicated a much more general principle than the original mirror neuron theory, in which people process the actions, sensations and emotions of others by vicariously activating owns own actions, sensations and emotions. Jointly, this work laid the foundation of the neuroscientific investigation of empathy.",
            "score": 127.27015686035156
        },
        {
            "docid": "315084_13",
            "document": "Lip reading . While lip-reading silent speech poses a challenge for most hearing people, adding sight of the speaker to heard speech improves speech processing under many conditions. The mechanisms for this, and the precise ways in which lip-reading helps, are topics of current research. Seeing the speaker helps at all levels of speech processing from phonetic feature discrimination to interpretation of pragmatic utterances. The positive effects of adding vision to heard speech are greater in noisy than quiet environments, where by making speech perception easier, seeing the speaker can free up cognitive resources, enabling deeper processing of speech content.",
            "score": 127.0304183959961
        },
        {
            "docid": "2072616_22",
            "document": "Neuroprosthetics . Improved performance on cochlear implant not only depends on understanding the physical and biophysical limitations of implant stimulation but also on an understanding of the brain's pattern processing requirements. Modern signal processing represents the most important speech information while also providing the brain the pattern recognition information that it needs. Pattern recognition in the brain is more effective than algorithmic preprocessing at identifying important features in speech. A combination of engineering, signal processing, biophysics, and cognitive neuroscience was necessary to produce the right balance of technology to maximize the performance of auditory prosthesis.",
            "score": 126.92772674560547
        },
        {
            "docid": "1731484_6",
            "document": "Amusia . Music-specific neural networks exist in the brain for a variety of music-related tasks. It has been shown that Broca's area is involved in the processing of musical syntax. Furthermore, brain damage can disrupt an individual's ability to tell the difference between tonal and atonal music and detect the presence of wrong notes, but can preserve the individual's ability to assess the distance between pitches and the direction of the pitch. The opposite scenario can also occur, in which the individual loses pitch discrimination capabilities, but can sense and appreciate the tonal context of the work. Distinct neural networks also exist for music memories, singing, and music recognition. Neural networks for music recognition are particularly intriguing. A patient can undergo brain damage that renders him/her unable to recognize familiar melodies that are presented without words. However, the patient maintains the ability to recognize spoken lyrics or words, familiar voices, and environmental sounds. The reverse case is also possible, in which the patient cannot recognize spoken words, but can still recognize familiar melodies. These situations overturn previous claims that speech recognition and music recognition share a single processing system. Instead, it is clear that there are at least two distinct processing modules: one for speech and one for music.",
            "score": 126.7992935180664
        },
        {
            "docid": "433584_6",
            "document": "McGurk effect . Both hemispheres of the brain make a contribution to the McGurk effect. They work together to integrate speech information that is received through the auditory and visual senses. A McGurk response is more likely to occur in right-handed individuals for whom the face has privileged access to the right hemisphere and words to the left hemisphere. In people that have had callosotomies done, the McGurk effect is still present but significantly slower. In people with lesions to the left hemisphere of the brain, visual features often play a critical role in speech and language therapy. People with lesions in the left hemisphere of the brain show a greater McGurk effect than normal controls. Visual information strongly influences speech perception in these people. There is a lack of susceptibility to the McGurk illusion if left hemisphere damage resulted in a deficit to visual segmental speech perception. In people with right hemisphere damage, impairment on both visual-only and audio-visual integration tasks is exhibited, although they are still able to integrate the information to produce a McGurk effect. Integration only appears if visual stimuli is used to improve performance when the auditory signal is impoverished but audible. Therefore, there is a McGurk effect exhibited in people with damage to the right hemisphere of the brain but the effect is not as strong as a normal group.",
            "score": 125.03853607177734
        },
        {
            "docid": "724626_6",
            "document": "Allan Snyder . Snyder is interested in understanding savants, how the savant brain perceives and interprets the world, the neurological and subjective correlates of the savant brain, and how to activate or at least promote savant level brain functions in non-autistic, healthy individuals. Even something as simple as seeing, he explains, requires phenomenally complex information processing. When a person looks at an object, for example, the brain immediately estimates an object's distance by calculating the subtle differences between the two images on each retina (computers programmed to do this require extreme memory and speed). During the process of face recognition, the brain analyzes countless details, such as the texture of skin and the shape of the eyes, jawbone, and lips. The vast majority of people are simply unaware of these calculations due to the brains' information filtering processes. In savants, says Snyder, the top layer of mental processing \u2014conceptual thinking, making logical deductions\u2014 is somehow deactivated. His working hypothesis is that once this layer is inactivate, one can access a startling capacity for recalling the most minute detail or for performing lightning-quick calculations. Snyder's theory has a conclusion of its own: He believes it may be possible someday to create technologies that will allow any non-autistic person to access these abilities.",
            "score": 124.38037872314453
        },
        {
            "docid": "5366050_42",
            "document": "Speech perception . Research into the relationship between music and cognition is an emerging field related to the study of speech perception. Originally it was theorized that the neural signals for music were processed in a specialized \"module\" in the right hemisphere of the brain. Conversely, the neural signals for language were to be processed by a similar \"module\" in the left hemisphere. However, utilizing technologies such as fMRI machines, research has shown that two regions of the brain traditionally considered exclusively to process speech, Broca's and Wernicke's areas, also become active during musical activities such as listening to a sequence of musical chords. Other studies, such as one performed by Marques et al. in 2006 showed that 8-year-olds who were given six months of musical training showed an increase in both their pitch detection performance and their electrophysiological measures when made to listen to an unknown foreign language.",
            "score": 124.33361053466797
        },
        {
            "docid": "7330954_30",
            "document": "Pattern recognition (psychology) . To understand music pattern recognition, we need to understand the underlying cognitive systems that each handle a part of this process. Various activities are at work in this recognition of a piece of music and its patterns. Researchers have begun to unveil the reasons behind the stimulated reactions to music. Montreal-based researchers asked ten volunteers who got \"chills\" listening to music to listen to their favorite songs while their brain activity was being monitored. The results show the significant role of the nucleus accumbens (NAcc) region \u2013 involved with cognitive processes such as motivation, reward, addiction, etc. \u2013 creating the neural arrangements that make up the experience. A sense of reward prediction is created by anticipation before the climax of the tune, which comes to a sense of resolution when the climax is reached. The longer the listener is denied the expected pattern, the greater the emotional arousal when the pattern returns. Musicologist Leonard Meyer used fifty measures of Beethoven\u2019s 5th movement of the String Quartet in C-sharp minor, Op. 131 to examine this notion. The stronger this experience is, the more vivid memory it will create and store. This strength affects the speed and accuracy of retrieval and recognition of the musical pattern. The brain not only recognizes specific tunes, it distinguishes standard acoustic features, speech and music.",
            "score": 123.89057159423828
        },
        {
            "docid": "40166_30",
            "document": "Broca's area . \"Hand/mouth goal-directed action representations\" is another way of saying \"gestural communication\", \"gestural language\", or \"communication through body language\". The recent finding that Broca's area is active when people are observing others engaged in meaningful action is evidence in support of this idea. It was hypothesized that a precursor to the modern Broca's area was involved in translating gestures into abstract ideas by interpreting the movements of others as meaningful action with an intelligent purpose. It is argued that over time the ability to predict the intended outcome and purpose of a set of movements eventually gave this area the capability to deal with truly abstract ideas, and therefore (eventually) became capable of associating sounds (words) with abstract meanings. The observation that frontal language areas are activated when people observe Hand Shadows is further evidence that human language may have evolved from existing neural substrates that evolved for the purpose of gesture recognition. The study, therefore, claims that Broca's area is the \"motor center for speech\", which assembles and decodes speech sounds in the same way it interprets body language and gestures. Consistent with this idea is that the neural substrate that regulated motor control in the common ancestor of apes and humans was most likely modified to enhance cognitive and linguistic ability. Studies of speakers of American Sign Language and English suggest that the human brain recruited systems that had evolved to perform more basic functions much earlier; these various brain circuits, according to the authors, were tapped to work together in creating language.",
            "score": 122.3367919921875
        },
        {
            "docid": "53686950_13",
            "document": "Bi-directional hypothesis of language and action . A similar experiment has been performed on the articulatory motor cortex, or the mouth and lip regions of the motor cortex used in the production of words. Two categories of words were used as language stimuli: words that involved the lips for production (e.g. \"pool\") or the tongue (e.g. \"tool). Subjects listened to the words, were shown pairs of pictures, and were asked to indicate which picture matched the word they heard with a button press. TMS was used prior to presentation of the language stimuli to selectively facilitate either the lip or tongue regions of the left motor cortex; these two TMS conditions were compared to a control condition where TMS was not applied. It was found that stimulation of the lip region of the motor cortex lead to a significantly decreased response time for lip words as compared to tongue words. In addition, during recognition of tongue words, reduced reaction times were seen with tongue TMS as compared to lip TMS and no TMS. Although this same effect was not seen with lip words, authors attribute this to the complexity of tongue as opposed to lip movements, and the increase difficulty of tongue words as opposed to lip. Overall, this study demonstrates that the activity in the articulatory motor cortex influences the comprehension of single spoken words, and highlights the importance of the motor cortex in speech comprehension",
            "score": 122.1333999633789
        },
        {
            "docid": "36058569_5",
            "document": "Frank H. Guenther . Frank Guenther\u2019s research is aimed at uncovering the neural computations underlying the processing of speech by the human brain. He is the originator of the Directions Into Velocities of Articulators (DIVA) model, which is currently the leading model of the neural computations underlying speech production. This model mathematically characterizes the computations performed by each brain region involved in speech production as well as the function of the interconnections between these regions. The model has been supported by a wide range of experimental tests of model predictions, including electromagnetic articulometry studies investigating speech movements, auditory perturbation studies involving modification of a speaker\u2019s feedback of his/her own speech in real time, and functional magnetic resonance imaging studies of brain activity during speech, though some parts of the model remain to be experimentally verified. The DIVA model has been used to investigate the neural underpinnings of a number of communication disorders, including stuttering apraxia of speech, and hearing-impaired speech.",
            "score": 121.97555541992188
        },
        {
            "docid": "433584_2",
            "document": "McGurk effect . The McGurk effect is a perceptual phenomenon that demonstrates an interaction between hearing and vision in speech perception. The illusion occurs when the auditory component of one sound is paired with the visual component of another sound, leading to the perception of a third sound. The visual information a person gets from seeing a person speak changes the way they hear the sound. If a person is getting poor quality auditory information but good quality visual information, they may be more likely to experience the McGurk effect. Integration abilities for audio and visual information may also influence whether a person will experience the effect. People who are better at sensory integration have been shown to be more susceptible to the effect. Many people are affected differently by the McGurk effect based on many factors, including brain damage and other disorders. It was first described in 1976 in a paper by Harry McGurk and John MacDonald, titled \"Hearing Lips and Seeing Voices\" in \"Nature\" (23 Dec 1976). This effect was discovered by accident when McGurk and his research assistant, MacDonald, asked a technician to dub a video with a different phoneme from the one spoken while conducting a study on how infants perceive language at different developmental stages. When the video was played back, both researchers heard a third phoneme rather than the one spoken or mouthed in the video.",
            "score": 121.75459289550781
        },
        {
            "docid": "43527201_5",
            "document": "Usha Goswami . Dyslexia is a disorder in which the person affected has difficulty reading due to the reversal of letters in the brain that isn't linked to intelligence. In people with dyslexia, the brain processes certain signals in a specific way making it a very specific learning difficulty. Dr. Goswami's research is concerned with focusing on dyslexia as a language disorder rather than a visual disorder as she has found that the way that children with dyslexia hear language is slightly different than others. When sound waves approach the brain, they vary in pressure depending on the syllables within the words being spoken creating a rhythm. When these signals reach the brain they are lined up with speech rhythms and this process doesn't work properly in those with dyslexia. Goswami is currently researching whether or not reading poetry, nursery rhymes, and singing can be used to help children with dyslexia. The rhythm of the words could allow the child to match the syllable patterns to language before they begin reading as to catch them up to where children without the disability might be.",
            "score": 121.66635131835938
        },
        {
            "docid": "971305_19",
            "document": "Haemodynamic response . If fMRI can be used to detect the regular flow of blood in a healthy brain, it can also be used to detect the problems with a brain that has undergone degenerative diseases. Functional MRI, using haemodynamic response, can help assess the effects of stroke and other degenerative diseases such as Alzheimer\u2019s disease on brain function. Another way fMRI could be used is in the planning of surgery of the brain. Surgeons can use fMRI to detect blood flow of the most active areas of the brain and the areas involved in critical functions like thought, speech, movement, etc. In this way, brain procedures are less dangerous because there is a brain mapping that shows which areas are vital to a person\u2019s life. Haemodynamic response is vital to fMRI and clinical use because through the study of blood flow we are able to examine the anatomy of the brain and effectively plan out procedures of the brain and link together the causes of degenerative brain disease.",
            "score": 120.90306854248047
        },
        {
            "docid": "9736296_27",
            "document": "Linguistic performance . Errors in linguistic performance not only occur in children newly acquiring their native language, second language learners, those with a disability or an acquired brain injury but among competent speakers as well. Types of performance errors that will be of focus here are those that involve errors in syntax, other types of errors can occur in the phonological, semantic features of words, for further information see speech errors. Phonological and semantic errors can be due to the repetition of words, mispronunciations, limitations in verbal working memory, and length of the utterance. Slips of the tongue are most common in spoken languages and occur when the speaker either: says something they did not mean to; produces the incorrect order of sounds or words; or uses the incorrect word. Other instances of errors in linguistic performance are slips of the hand in signed languages, slips of the ear which are errors in comprehension of utterances and slips of the pen which occur while writing. Errors of linguistic performance are perceived by both the speaker and the listener and can therefore have many interpretations depending on the persons judgement and the context in which the sentence was spoken.",
            "score": 120.64042663574219
        }
    ]
}