{
    "q": [
        {
            "docid": "32116125_4",
            "document": "Amblyaudia . Amblyaudia is a deficit in binaural integration of environmental information entering the auditory system. It is a disorder related to brain organization and function rather than what is typically considered a \u201chearing loss\u201d (damage to the cochlea). It may be genetic or developmentally acquired or both. When animals are temporarily deprived of hearing from an early age, profound changes occur in the brain. Specifically, cell sizes in brainstem nuclei are reduced, the configuration of brainstem dendrites are altered and neurons respond in different ways to sounds presented to both the deprived and non-deprived ears (in cases of asymmetric deprivation). This last point is particularly important for listening tasks that require inputs from two ears to perform well. There are multiple auditory functions that rely on the computation of well calibrated inputs from the two ears. Chief among these is the ability to localize sound sources and separate what we want to hear from a background of noise. In the brainstem, the auditory system compares the timing and levels of sounds between the two ears to encode the location of sound sources (sounds that originate from our right as opposed to left side are louder and arrive earlier in our right ear). This ability to separate sound sources not only helps us locate the trajectories of moving objects, but also to separate different sound sources in noisy environments.",
            "score": 101.08555746078491
        },
        {
            "docid": "433584_2",
            "document": "McGurk effect . The McGurk effect is a perceptual phenomenon that demonstrates an interaction between hearing and vision in speech perception. The illusion occurs when the auditory component of one sound is paired with the visual component of another sound, leading to the perception of a third sound. The visual information a person gets from seeing a person speak changes the way they hear the sound. If a person is getting poor quality auditory information but good quality visual information, they may be more likely to experience the McGurk effect. Integration abilities for audio and visual information may also influence whether a person will experience the effect. People who are better at sensory integration have been shown to be more susceptible to the effect. Many people are affected differently by the McGurk effect based on many factors, including brain damage and other disorders. It was first described in 1976 in a paper by Harry McGurk and John MacDonald, titled \"Hearing Lips and Seeing Voices\" in \"Nature\" (23 Dec 1976). This effect was discovered by accident when McGurk and his research assistant, MacDonald, asked a technician to dub a video with a different phoneme from the one spoken while conducting a study on how infants perceive language at different developmental stages. When the video was played back, both researchers heard a third phoneme rather than the one spoken or mouthed in the video.",
            "score": 108.32747554779053
        },
        {
            "docid": "57411023_4",
            "document": "Andrew King (neurophysiologist) . King discovered that the mammalian brain contains a spatial map of the auditory world and showed that its development is shaped by sensory experience. His work has also demonstrated that the adult brain represents sound features in a remarkably flexible way, continually adjusting to variations in the statistical distribution of sounds associated with different acoustic environments as well to longer term changes in input resulting from hearing loss. In addition to furthering our understanding of the neural basis for auditory perception, his research is helping to inform better treatment strategies for the hearing impaired.",
            "score": 86.51758337020874
        },
        {
            "docid": "520289_5",
            "document": "Hearing aid . Hearing aids are incapable of truly correcting a hearing loss; they are an \"aid\" to make sounds more audible. The most common form of hearing loss for which hearing aids are sought is sensorineural, resulting from damage to the hair cells and synapses of the cochlea and auditory nerve. Sensorineural hearing loss reduces the sensitivity to sound, which a hearing aid can partially accommodate by making sound louder. Other decrements in auditory perception caused by sensorineural hearing loss, such as abnormal spectral and temporal processing, and which may negatively affect speech perception, are more difficult to compensate for using digital signal processing and in some cases may be exacerbated by the use of amplification. Conductive hearing losses, which do not involve damage to the cochlea, tend to be better treated by hearing aids; the hearing aid is able to sufficiently amplify sound to account for the attenuation caused by the conductive component. Once the sound is able to reach the cochlea at normal or near-normal levels, the cochlea and auditory nerve are able to transmit signals to the brain normally.",
            "score": 103.7252082824707
        },
        {
            "docid": "10042066_2",
            "document": "Developmental linguistics . Developmental linguistics is the study of the development of linguistic ability in an individual, particularly the acquisition of language in childhood. It involves research into the different stages in language acquisition, language retention, and language loss in both first and second languages, in addition to the area of bilingualism. Before infants can speak, the neural circuits in their brains are constantly being influenced by exposure to language. The neurobiology of language contains a \"critical period\" in which children are most sensitive to language. The different aspects of language have varying \"critical periods\". Studies show that the critical period for phonetics is toward the end of the first year. At 18 months, a toddler's vocabulary vastly expands. The critical period for syntactic learning is 18-36 months. Infants of different mother languages can be differentiated at the age of 10 months. At 20 weeks they begin vocal imitation. Beginning when babies are about 12 months, they take on computational learning and social learning. Social interactions for infants and toddlers is important because it helps associate \"perception and action\". In-person social interaction rather than audio or video better facilitates learning in babies because they learn from how other people respond to them, especially their mothers. Babies have to learn to mimic certain syllables, which takes practice in manipulating tongue and lip movement. Sensory-motor learning in speech is linked to exposure to speech, which is very sensitive to language. Infants exposed to Spanish exhibit a different vocalization than infants exposed to English. One study took infants that were learning English and made them listen to Spanish in 12 sessions. The result showed consequent alterations in their vocalization, which demonstrated Spanish prosody.  One study used MEG to record activation in the brains of newborns, 6 months olds and 12 months olds while presenting them with syllables, harmonics and non-speech sounds. For the 6 month and 12 month old, the auditory and motor areas responded to speech. The newborn showed auditory activation but not motor activation. Another study presented 3 month olds with sentences and recorded their brain activity via fMRI motor speech areas did activate. These studies suggest that the link between perception and action begins to develop at 3 months. When babies are young, they are actually the most sensitive to distinguishing all phonetic units. During an infant\u2019s 1st year of life, they have to differentiate between about 40 phonetic units. When they are older they have usually been exposed to their native language so much that they lose this ability and can only distinguish phonetic units in their native language. Even at 12 months babies exhibit a deficit in differentiated non-native sounds. However, their ability to distinguish sounds in their native language continues to improve and become more fine-tuned. For example, Japanese learning infants learn that there is no differentiation between /r/ and /l/. However, in English, \"rake\" and \"lake\" are two different words. Japanese babies eventually lose their ability to distinguish between /r/ and /l/. Similarly, a Spanish learning infant cannot form words until they learn the difference between works like \"bano\" and \"pano\", because the /p/ sound is different than the /b/ sound. English learning babies do not learn to differentiate between the two.",
            "score": 123.10910606384277
        },
        {
            "docid": "2263473_17",
            "document": "Volley theory . A fundamental frequency is the lowest frequency of a harmonic. In some cases, sound can have all the frequencies of a harmonic but be missing the fundamental frequency, this is known as missing fundamental. When listening to a sound with a missing fundamental, the human brain still receives information for all frequencies, including the fundamental frequency which does not exist in the sound. This implies that sound is encoded by neurons firing at all frequencies of a harmonic, therefore, the neurons must be locked in some way to result in the hearing of one sound. Congenital deafness or sensorineural hearing loss is an often used model for the study of the inner ear regarding pitch perception and theories of hearing in general. Frequency analysis of these individuals\u2019 hearing has given insight on common deviations from normal tuning curves, excitation patterns, and frequency discrimination ranges. By applying pure or complex tones, information on pitch perception can be obtained. In 1983, it was shown that subjects with low frequency sensorineural hearing loss demonstrated abnormal psychophysical tuning curves. Changes in the spatial responses in these subjects showed similar pitch judgment abilities when compared to subjects with normal spatial responses. This was especially true regarding low frequency stimuli. These results suggest that the place theory of hearing does not explain pitch perception at low frequencies, but that the temporal (frequency) theory is more likely. This conclusion is due to the finding that when deprived of basilar membrane place information, these patients still demonstrated normal pitch perception. Computer models for pitch perception and loudness perception are often used during hearing studies on acoustically impaired subjects. The combination of this modeling and knowledge of natural hearing allows for better development of hearing aids.",
            "score": 92.89419889450073
        },
        {
            "docid": "32116125_2",
            "document": "Amblyaudia . Amblyaudia (amblyos- blunt; audia-hearing) is a term coined by Dr. Deborah Moncrieff from the University of Pittsburgh to characterize a specific pattern of performance from dichotic listening tests. Dichotic listening tests are widely used to assess individuals for binaural integration, a type of auditory processing skill. During the tests, individuals are asked to identify different words presented simultaneously to the two ears. Normal listeners can identify the words fairly well and show a small difference between the two ears with one ear slightly dominant over the other. For the majority of listeners, this small difference is referred to as a \"right-ear advantage\" because their right ear performs slightly better than their left ear. But some normal individuals produce a \"left-ear advantage\" during dichotic tests and others perform at equal levels in the two ears. Amblyaudia is diagnosed when the scores from the two ears are significantly different with the individual's dominant ear score much higher than the score in the non-dominant ear  Researchers interested in understanding the neurophysiological underpinnings of amblyaudia consider it to be a brain based hearing disorder that may be inherited or that may result from auditory deprivation during critical periods of brain development. Individuals with amblyaudia have normal hearing sensitivity (in other words they hear soft sounds) but have difficulty hearing in noisy environments like restaurants or classrooms. Even in quiet environments, individuals with amblyaudia may fail to understand what they are hearing, especially if the information is new or complicated. Amblyaudia can be conceptualized as the auditory analog of the better known central visual disorder amblyopia. The term \u201clazy ear\u201d has been used to describe amblyaudia although it is currently not known whether it stems from deficits in the auditory periphery (middle ear or cochlea) or from other parts of the auditory system in the brain, or both. A characteristic of amblyaudia is suppression of activity in the non-dominant auditory pathway by activity in the dominant pathway which may be genetically determined and which could also be exacerbated by conditions throughout early development.",
            "score": 104.80852603912354
        },
        {
            "docid": "35075711_26",
            "document": "Spontaneous recovery . The pathway of recall associated with the retrieval of sound memories is the auditory system. Within the auditory system is the auditory cortex, which can be broken down into the primary auditory cortex and the belt areas. The primary auditory cortex is the main region of the brain that processes sound and is located on the superior temporal gyrus in the temporal lobe where it receives point-to-point input from the medial geniculate nucleus. From this, the primary auditory complex had a topographic map of the cochlea. The belt areas of the auditory complex receive more diffuse input from peripheral areas of the medial geniculate nucleus and therefore are less precise in tonotopic organization compared to the primary visual cortex. A 2001 study by Trama examined how different kinds of brain damage interfere with normal perception of music. One of his studied patients lost most of his auditory cortex to strokes, allowing him to still hear but making it difficult to understand music since he could not recognize harmonic patterns. Detecting a similarity between speech perception and sound perception, spontaneous recovery of lost auditory information is possible in those patients who have experienced a stroke or other major head trauma. Amusia is a disorder manifesting itself as a defect in processing pitch but also affects one's memory and recognition for music.",
            "score": 98.79448235034943
        },
        {
            "docid": "28055624_5",
            "document": "Robert Galambos . Using electrodes implanted in the brains of animals, Galambos was able to use electronic amplifiers to boost the signals of a single nerve to follow the impulses that travel from the ear to the brain in response to auditory stimuli, which allowed him to track how neurons respond to the presence (or absence) of sound at a particular frequency. This research allowed for the development of hearing tests for infants which could be performed by monitoring the brain's response directly to sounds, and could then be used to prescribe hearing aids. The research led to the development of auditory brainstem implants and cochlear implants which could be surgically implanted to allow individuals to regain the ability to hear for the profoundly deaf. With John S. O'Brien, he co-founded the department of neuroscience at the University of California, San Diego, and continued his research there after he was required to retire at age 67.",
            "score": 95.91986131668091
        },
        {
            "docid": "620396_41",
            "document": "Origin of language . Proponents of the motor theory of language evolution have primarily focused on the visual domain and communication through observation of movements. The \"Tool-use sound hypothesis\" suggests that the production and perception of sound, also contributed substantially, particularly \"incidental sound of locomotion\" (\"ISOL\") and \"tool-use sound\" (\"TUS\"). Human bipedalism resulted in rhythmic and more predictable \"ISOL\". That may have stimulated the evolution of musical abilities, auditory working memory, and abilities to produce complex vocalizations, and to mimic natural sounds. Since the human brain proficiently extracts information about objects and events from the sounds they produce, \"TUS\", and mimicry of \"TUS\", might have achieved an iconic function. The prevalence of sound symbolism in many extant languages supports this idea. Self-produced TUS activates multimodal brain processing (motor neurons, hearing, proprioception, touch, vision), and \"TUS\" stimulates primate audiovisual mirror neurons, which is likely to stimulate the development of association chains. Tool use and auditory gestures involve motor-processing of the forelimbs, which is associated with the evolution of vertebrate vocal communication. The production, perception, and mimicry of \"TUS\" may have resulted in a limited number of vocalizations or protowords that were associated with tool use. A new way to communicate about tools, especially when out of sight, would have had selective advantage. A gradual change in acoustic properties and/or meaning could have resulted in arbitrariness and an expanded repertoire of words. Humans have been increasingly exposed to \"TUS\" over millions of years, coinciding with the period during which spoken language evolved.",
            "score": 76.94398820400238
        },
        {
            "docid": "525667_10",
            "document": "Human echolocation . In a 2014 study by Thaler and colleagues, the researchers first made recordings of the clicks and their very faint echoes using tiny microphones placed in the ears of the blind echolocators as they stood outside and tried to identify different objects such as a car, a flag pole, and a tree. The researchers then played the recorded sounds back to the echolocators while their brain activity was being measured using functional magnetic resonance imaging. Remarkably, when the echolocation recordings were played back to the blind experts, not only did they perceive the objects based on the echoes, but they also showed activity in those areas of their brain that normally process visual information in sighted people, primarily primary visual cortex or V1. This result is surprising, as visual areas, as their names suggest, are only active during visual tasks. The brain areas that process auditory information were no more activated by sound recordings of outdoor scenes containing echoes than they were by sound recordings of outdoor scenes with the echoes removed. Importantly, when the same experiment was carried out with sighted people who did not echolocate, these individuals could not perceive the objects and there was no echo-related activity anywhere in the brain. This suggests that the cortex of blind echolocators is plastic and reorganizes such that primary visual cortex, rather than any auditory area, becomes involved in the computation of echolocation tasks.",
            "score": 90.49984121322632
        },
        {
            "docid": "56439577_35",
            "document": "Temporal envelope and fine structure . For sinusoidal carriers, which have no intrinsic envelope (ENV) fluctuations, the TMTF is roughly flat for AM rates from 10 to 120\u00a0Hz, but increases (i.e. threshold worsens) for higher AM rates, provided that spectral sidebands are not audible. The shape of the TMTF for sinusoidal carriers is similar for young and older people with normal audiometric thresholds, but older people tend to have higher detection thresholds overall, suggesting poorer \u201cdetection efficiency\u201d for ENV cues in older people. Provided that the carrier is fully audible, the ability to detect AM is usually not adversely affected by cochlear hearing loss and may sometimes be better than normal, for both noise carriers and sinusoidal carriers, perhaps because loudness recruitment (an abnormally rapid growth of loudness with increasing sound level) \u201cmagnifies\u201d the perceived amount of AM (i.e., ENV cues). Consistent with this, when the AM is clearly audible, a sound with a fixed AM depth appears to fluctuate more for an impaired ear than for a normal ear. However, the ability to detect changes in AM depth can be impaired by cochlear hearing loss. Additional experiments suggest that age negatively affects the binaural processing of ENV at least at low audio-frequencies.",
            "score": 76.1956114768982
        },
        {
            "docid": "36560848_4",
            "document": "Temporal dynamics of music and language . The primary auditory cortex is located on the temporal lobe of the cerebral cortex. This region is important in music processing and plays an important role in determining the pitch and volume of a sound. Brain damage to this region often results in a loss of the ability to hear any sounds at all. The frontal cortex has been found to be involved in processing melodies and harmonies of music. For example, when a patient is asked to tap out a beat or try to reproduce a tone, this region is very active on fMRI and PET scans. The cerebellum is the \"mini\" brain at the rear of the skull. Similar to the frontal cortex, brain imaging studies suggest that the cerebellum is involved in processing melodies and determining tempos. The medial prefrontal cortex along with the primary auditory cortex has also been implicated in tonality, or determining pitch and volume.",
            "score": 73.60287582874298
        },
        {
            "docid": "315084_10",
            "document": "Lip reading . Until around six months of age, most hearing infants are sensitive to a wide range of speech gestures - including ones that can be seen on the mouth - which may or may not later be part of the phonology of their native language. But in the second six months of life, the hearing infant shows perceptual narrowing for the phonetic structure of their own language - and may lose the early sensitivity to mouth patterns that are not useful. The speech sounds /v/ and /b/ which are visemically distinctive in English but not in Castilian Spanish are accurately distinguished in Spanish-exposed and English-exposed babies up to the age of around 6 months. However, older Spanish-exposed infants lose the ability to 'see' this distinction, while it is retained for English-exposed infants. Such studies suggest that rather than hearing and vision developing in independent ways in infancy, multimodal processing is the rule, not the exception, in (language) development of the infant brain.",
            "score": 114.21886157989502
        },
        {
            "docid": "32116125_3",
            "document": "Amblyaudia . Children with amblyaudia experience difficulties in speech perception, particularly in noisy environments, sound localization, and binaural unmasking (using interaural cues to hear better in noise) despite having normal hearing sensitivity (as indexed through pure tone audiometry). These symptoms may lead to difficulty attending to auditory information causing many to speculate that language acquisition and academic achievement may be deleteriously affected in children with amblyaudia. A significant deficit in a child's ability to use and comprehend expressive language may be seen in children who lacked auditory stimulation throughout the critical periods of auditory system development. A child suffering from amblyaudia may have trouble in appropriate vocabulary comprehension and production and the use of past, present and future tenses. Amblyaudia has been diagnosed in many children with reported difficulties understanding and learning from listening and adjudicated adolescents are at a significantly high risk for amblyaudia (Moncrieff, et al., 2013, Seminars in Hearing). Families report the presence of amblyaudia in several individuals, suggesting that it may be genetic in nature. It is possible that abnormal auditory input during the first two years of life may increase a child\u2019s risk for amblyaudia, although the precise relationship between deprivation timing and development of amblyaudia is still unclear. Recurrent ear infections (otitis media) are the leading cause of temporary auditory deprivation in young children. During ear infection bouts, the quality of the signal that reaches the auditory regions of the brains of a subset of children with OM is degraded in both timing and magnitude. When this degradation is asymmetric (worse in one ear than the other) the binaural cues associated with sound localization can also be degraded. Aural atresia (a closed external auditory canal) also causes temporary auditory deprivation in young children. Hearing can be restored to children with ear infections and aural atresia through surgical intervention (although ear infections will also resolve spontaneously). Nevertheless, children with histories of auditory deprivation secondary to these diseases can experience amblyaudia for years after their hearing has been restored.",
            "score": 88.41895580291748
        },
        {
            "docid": "49514057_3",
            "document": "Direct acoustic cochlear implant . A DACI tries to provide an answer for people with hearing problems for which no solution exists today. People with some problems at the level of the cochlea can be helped with a hearing aid. A hearing aid will absorb the incoming sound from a microphone, and offer enhanced through the natural way. For larger reinforcements, this may cause problems with feedback and distortion. A hearing aid also simply provides more loudness, no more resolution. Users will view this often as, \"all sounds louder, but I understand nothing more than before.\" Once a hearing aid offers no solution anymore, one can switch to a cochlear implant. A Cochlear implant captures the sound and sends it electrically, through the cochlea, to the auditory nerve. In this way, completely deaf patients can perceive sounds again. However, As soon as there are problems not only at the level of the cochlea, but also in the middle ear (the so-called conductive losses), then there are more efficient ways to get sound to the partially functioning cochlea. The most obvious solution is a BAHA, which brings the sound to the cochlea via bone conduction. However, patients who have both problems with the cochlea, as with the middle ear (i.e. patients with mixed losses), none of the above solutions is ideal. To this end, the direct acoustic cochlear implant was developed. A DACI brings the sound directly to the cochlea, and provides the most natural way of sound amplification.",
            "score": 62.04517948627472
        },
        {
            "docid": "101970_2",
            "document": "Tinnitus . Tinnitus is the hearing of sound when no external sound is present. While often described as a ringing, it may also sound like a clicking, hiss or roaring. Rarely, unclear voices or music are heard. The sound may be soft or loud, low pitched or high pitched and appear to be coming from one ear or both. Most of the time, it comes on gradually. In some people, the sound causes depression or anxiety and can interfere with concentration. Tinnitus is not a disease but a symptom that can result from a number of underlying causes. One of the most common causes is noise-induced hearing loss. Other causes include ear infections, disease of the heart or blood vessels, M\u00e9ni\u00e8re's disease, brain tumors, emotional stress, exposure to certain medications, a previous head injury, and earwax. It is more common in those with depression. The diagnosis of tinnitus is usually based on the person's description. A number of questionnaires exist that may help to assess how much tinnitus is interfering with a person's life. The diagnosis is commonly supported by an audiogram and a neurological examination. If certain problems are found, medical imaging, such as with MRI, may be performed. Other tests are suitable when tinnitus occurs with the same rhythm as the heartbeat. Rarely, the sound may be heard by someone else using a stethoscope, in which case it is known as objective tinnitus. Spontaneous otoacoustic emissions, which are sounds produced normally by the inner ear, may also occasionally result in tinnitus. Prevention involves avoiding loud noise. If there is an underlying cause, treating it may lead to improvements. Otherwise, typically, management involves talk therapy. Sound generators or hearing aids may help some. As of 2013, there were no effective medications. It is common, affecting about 10\u201315% of people. Most, however, tolerate it well, and it is a significant problem in only 1\u20132% of people. The word tinnitus is from the Latin \"tinn\u012bre\" which means \"to ring\". Tinnitus can be perceived in one or both ears or in the head. It is the description of a noise inside a person\u2019s head in the absence of auditory stimulation. The noise can be described in many different ways. It is usually described as a ringing noise but, in some patients, it takes the form of a high-pitched whining, electric buzzing, hissing, humming, tinging or whistling sound or as ticking, clicking, roaring, \"crickets\" or \"tree frogs\" or \"locusts (cicadas)\", tunes, songs, beeping, sizzling, sounds that slightly resemble human voices or even a pure steady tone like that heard during a hearing test. It has also been described as a \"whooshing\" sound because of acute muscle spasms, as of wind or waves. Tinnitus can be intermittent or continuous: in the latter case, it can be the cause of great distress. In some individuals, the intensity can be changed by shoulder, head, tongue, jaw or eye movements. Most people with tinnitus have some degree of hearing loss.",
            "score": 104.44255089759827
        },
        {
            "docid": "19415143_2",
            "document": "Auditory agnosia . Auditory agnosia is a form of agnosia that manifests itself primarily in the inability to recognize or differentiate between sounds. It is not a defect of the ear or \"hearing\", but a neurological inability of the brain to process sound meaning. It is a disruption of the \"what\" pathway in the brain. Persons with auditory agnosia can physically hear the sounds and describe them using unrelated terms, but are unable to recognize them. They might describe the sound of some environmental sounds, such as a motor starting, as resembling a lion roaring, but would not be able to associate the sound with \"car\" or \"engine\", nor would they say that it \"was\" a lion creating the noise. Auditory agnosia is caused by damage to the secondary and tertiary auditory cortex of the temporal lobe of the brain.",
            "score": 82.56541848182678
        },
        {
            "docid": "33826251_13",
            "document": "Neural basis of self . Sometimes after strokes patients' perception of self changes. Often after a stroke, patients report their perception of self in more negative terms than before their stroke. It has been found that humans\u2019 ideas of themselves are established early in life but that the perception can change as others ideas are combined with their own.  There are differences in the areas activated during self-knowledge retrieval between adults and children. This suggests a difference in self-knowledge neurobiologically due to normal aging. The prefrontal cortex and the medial posterior parietal cortex have been found to be activated when adults perform self-knowledge retrieval processes. Tests consist of presenting subjects with self-description phrases and allowing the subject to respond yes or no depending on whether or not the phrase describes him or herself. During this task, patients brains are fMRI scanned. These results can then be compared to fMRI data of the same patients when they are asked if the same phrases describe another individual, such as a well-known fictional character. The medial prefrontal cortex is activated more strongly for subjects when they are describing themselves than when they are describing others. However, children show greater medial prefrontal cortex activation than adults when performing self-knowledge retrieval tasks. Additionally, children and adults activate different specific regions in the medial prefrontal cortex. Adults activate the posterior precuneus more while children activate the anterior precuneus and the posterior cingulate. The understanding of the areas of the brain most frequently activated in children and adults can also provide information about how children, adolescents, and adults view themselves differently. Older children more significantly activate the medial prefrontal cortex because they deal with introspection much less frequently than adults and adolescents. Children have decreased specificity in skills than adults, so they show greater activation during spatial tasks. This is explained by the idea that with increased expertise in a task, decreased interest in wide spatial parameters occurs. When a person is an expert, he or she is able to be more focused in his or her performance. The difference in performance between adults and children is thought to be attributable to different perceptions of the self whether it is more introspective or more concerned with the surroundings and environment.",
            "score": 114.86013340950012
        },
        {
            "docid": "35988494_6",
            "document": "Selective auditory attention . The prevalence of selective hearing has not been clearly researched yet. However, there are some that have argued that the proportion of selective hearing is particularly higher in males than females. Ida Z\u00fcndorf, Hans-Otto Karnath and J\u00f6rg Lewald carried out a study in 2010 which investigated the advantages and abilities males have in the localization of auditory information. A sound localization task centered on the cocktail party effect was utilized in their study. The male and female participants had to try to pick out sounds from a specific source, on top of other competing sounds from other sources. The results showed that the males had a better performance overall. Female participants found it more difficult to locate target sounds in a multiple-source environment. Z\u00fcndorf et al. suggested that there may be sex differences in the attention processes that helped locate the target sound from a multiple-source auditory field. While men and women do have some differences when it comes to selective auditory hearing, they both struggle when presented with the challenge of multitasking, especially when tasks that are to be attempted concurrently are very similar in nature (Dittrich, and Stahl, 2012, p.\u00a0626).",
            "score": 64.9369101524353
        },
        {
            "docid": "56439577_41",
            "document": "Temporal envelope and fine structure . Several psychophysical studies have shown that older people with normal hearing and people with sensorineural hearing loss often show impaired performance for auditory tasks that are assumed to rely on the ability of the monaural and binaural auditory system to encode and use TFS cues, such as: discrimination of sound frequency, discrimination of the fundamental frequency of harmonic sounds, detection of FM at rates below 5\u00a0Hz, melody recognition for sequences of pure tones and complex sounds, lateralization and localization of pure tones and complex tones, and segregation of concurrent harmonic sounds (such as speech sounds). However, it remains unclear to which extent deficits associated with hearing loss reflect poorer TFS processing or reduced cochlear frequency selectivity.",
            "score": 74.30545353889465
        },
        {
            "docid": "433584_4",
            "document": "McGurk effect . Vision is the primary sense for humans, but speech perception is multimodal, which means that it involves information from more than one sensory modality, in particular, audition and vision. The McGurk effect arises during phonetic processing because the integration of audio and visual information happens early in speech perception. The McGurk effect is very robust; that is, knowledge about it seems to have little effect on one's perception of it. This is different from certain optical illusions, which break down once one 'sees through' them. Some people, including those that have been researching the phenomenon for more than twenty years, experience the effect even when they are aware that it is taking place. With the exception of people who can identify most of what is being said from speech-reading alone, most people are quite limited in their ability to identify speech from visual-only signals. A more extensive phenomenon is the ability of visual speech to increase the intelligibility of heard speech in a noisy environment. Visible speech can also alter the perception of perfectly audible speech sounds when the visual speech stimuli are mismatched with the auditory speech. Normally, speech perception is thought to be an auditory process; however, our use of information is immediate, automatic, and, to a large degree, unconscious and therefore, despite what is widely accepted as true, speech is not only something we hear. Speech is perceived by all of the senses working together (seeing, touching, and listening to a face move). The brain is often unaware of the separate sensory contributions of what it perceives. Therefore, when it comes to recognizing speech the brain cannot differentiate whether it is seeing or hearing the incoming information.",
            "score": 125.6838150024414
        },
        {
            "docid": "4743980_30",
            "document": "Tip of the tongue . Age is an important factor when considering TOT states. There are complaints that problems recalling information increases with age. The frequency of TOTs increases in adulthood and even more so during the elderly years. Compared with young adults, older adults generally report having more TOT states, fewer alternate words, and less phonological information about the target word. The underpinnings of TOT with regard to age have focused on neurological brain differences. Current research uses neuroimaging methods to access the presence of different brain patterns when a younger and older individual is experiencing a TOT state. It is found that older and younger individuals employ a similar network of brain regions during TOT states such as the prefrontal cortex, left insula, and sensorimotor cortex. However, older individuals show differences in activity in some areas compared to younger individuals. TOTs increase with age-related gray matter loss in the left insula for older individuals. This is accompanied by less activity in the left insula and is related to higher frequency of TOTs. Furthermore, it was found that older individuals have over-activation in their prefrontal cortex when experiencing TOT states. This may indicate a continued search when the retrieval process fails and a TOT state is experienced. More specifically, greater activation in the sensorimotor cortex in older individuals and less in younger adults may reflect differences in the knowledge that is used to retrieve the target information. Priming words during word retrieval tests generally reduces the frequency of TOTs and improves the retrieval of the target word and has been shown to have a larger benefit for older adults. This is consistent with the spreading activation model, where neural connections are strengthened when used more. Although older people experience more tip of the tongue states more often than any other category, recent studies have shown that frequent tip of the tongue states are not linked at all to dementia, which is common in the elderly. Despite the association of increased age with lower levels of episodic memory and more frequent TOT states, the two phenomena seem to be largely independent of one another.",
            "score": 122.370432138443
        },
        {
            "docid": "25140_41",
            "document": "Perception . Hearing (or \"audition\") is the ability to perceive sound by detecting vibrations. Frequencies capable of being heard by humans are called audio or \"sonic\". The range is typically considered to be between 20\u00a0Hz and 20,000\u00a0Hz. Frequencies higher than audio are referred to as ultrasonic, while frequencies below audio are referred to as infrasonic. The auditory system includes the outer ears which collect and filter sound waves, the middle ear for transforming the sound pressure (impedance matching), and the inner ear which produces neural signals in response to the sound. By the ascending auditory pathway these are led to the primary auditory cortex within the temporal lobe of the human brain, which is where the auditory information arrives in the cerebral cortex and is further processed there.",
            "score": 46.91308832168579
        },
        {
            "docid": "14405771_9",
            "document": "Speech science . Speech perception refers to the understanding of speech. The beginning of the process towards understanding speech is first hearing the message that is spoken. The auditory system receives sound signals starting at the outer ear. They enter the pinna and continue into the external auditory canal (ear canal) and then to the eardrum. Once in the middle ear, which consists of the malleus, the incus, and the stapes; the sounds are changed into mechanical energy. After being converted into mechanical energy, the message reaches the oval window, which is the beginning of the inner ear. Once inside the inner ear, the message is transferred into hydraulic energy by going through the cochlea, which is filled with fluid, and on to the Organ of Corti. This organ again helps the sound to be transferred into a neural impulse that stimulates the auditory pathway and reaches the brain. Sound is then processed in Heschl's gyrus and associated with meaning in Wernicke's area. As for theories of speech perception, there are a motor and an auditory theory. The motor theory is based upon the premise that speech sounds are encoded in the acoustic signal rather than enciphered in it. The auditory theory puts greater emphasis on the sensory and filtering mechanisms of the listener and suggests that speech knowledge is a minor role that\u2019s only used in hard perceptual conditions.",
            "score": 90.76403903961182
        },
        {
            "docid": "3246329_10",
            "document": "Hearing range . Cats have excellent hearing and can detect an extremely broad range of frequencies. They can hear higher-pitched sounds than humans or most dogs, detecting frequencies from 55\u00a0Hz up to 79\u00a0kHz. Cats do not use this ability to hear ultrasound for communication but it is probably important in hunting, since many species of rodents make ultrasonic calls. Cat hearing is also extremely sensitive and is among the best of any mammal, being most acute in the range of 500\u00a0Hz to 32\u00a0kHz. This sensitivity is further enhanced by the cat's large movable outer ears (their \"pinnae\"), which both amplify sounds and help a cat sense the direction from which a noise is coming. The hearing ability of a dog is dependent on breed and age, though the range of hearing is usually around 67\u00a0Hz to 45\u00a0kHz. As with humans, some dog breeds' hearing ranges narrow with age, such as the German shepherd and miniature poodle. When dogs hear a sound, they will move their ears towards it in order to maximise reception. In order to achieve this, the ears of a dog are controlled by at least 18 muscles, which allow the ears to tilt and rotate. The ear's shape also allows the sound to be heard more accurately. Many breeds often have upright and curved ears, which direct and amplify sounds.",
            "score": 62.84429121017456
        },
        {
            "docid": "54301172_24",
            "document": "Well-being contributing factors . Nobel prize winner Eric Kandel and researcher Cynthia Fu described very accurate diagnoses of depression just by looking at fMRI brain scans. By identifying neural correlates for emotions, scientists may be able to use methods like brain scans to tell us more about the different ways of being \"happy\". Richard Davidson has conducted research to determine which parts of the brain are involved in positive emotions. He found that the left prefrontal cortex is more activated when we are happy and is also associated with greater ability to recover from negative emotions as well as enhanced ability to suppress negative emotions. Interestingly, Davidson found that people can train themselves to increase activation in this area of their brains. It is thought that our brain can change throughout our lives as a result of our experiences; this is known as neuroplasticity.",
            "score": 100.56226074695587
        },
        {
            "docid": "3407405_4",
            "document": "Nonsyndromic deafness . Most forms of nonsyndromic deafness are associated with permanent hearing loss caused by damage to structures in the inner ear. The inner ear consists of three parts: a snail-shaped structure called the cochlea that helps process sound, nerves that send information from the cochlea to the brain, and structures involved with balance. Loss of hearing caused by changes in the inner ear is called sensorineural deafness. Hearing loss that results from changes in the middle ear is called conductive hearing loss. The middle ear contains three tiny bones that help transfer sound from the eardrum to the inner ear. Some forms of nonsyndromic deafness involve changes in both the inner ear and the middle ear; this combination is called mixed hearing loss.",
            "score": 59.56621193885803
        },
        {
            "docid": "5366050_50",
            "document": "Speech perception . Neurophysiological methods rely on utilizing information stemming from more direct and not necessarily conscious (pre-attentative) processes. Subjects are presented with speech stimuli in different types of tasks and the responses of the brain are measured. The brain itself can be more sensitive than it appears to be through behavioral responses. For example, the subject may not show sensitivity to the difference between two speech sounds in a discrimination test, but brain responses may reveal sensitivity to these differences. Methods used to measure neural responses to speech include event-related potentials, magnetoencephalography, and near infrared spectroscopy. One important response used with event-related potentials is the mismatch negativity, which occurs when speech stimuli are acoustically different from a stimulus that the subject heard previously.",
            "score": 112.91021502017975
        },
        {
            "docid": "6894544_2",
            "document": "Noise-induced hearing loss . Noise-induced hearing loss (NIHL) is hearing impairment resulting from exposure to loud sound. People may have a loss of perception of a narrow range of frequencies, impaired cognitive perception of sound including sensitivity to sound or ringing in the ears. When exposure to hazards such as noise occur at work and is associated with hearing loss, it is referred to as occupational hearing loss.  Hearing may deteriorate gradually from chronic and repeated noise exposure, such as to loud music or background noise, or suddenly, from exposure to impulse noise (a short high intensity noise), such as a gunshot or airhorn. In both types, loud sound overstimulates delicate hearing cells, leading to the permanent injury or death of the cells. Once lost this way, hearing cannot be restored in humans.  There are a variety of prevention strategies available to avoid or reduce hearing loss. Lowering the volume of sound at its source, limiting the time of exposure and physical protection can reduce the impact of excessive noise. If not prevented, hearing loss can be managed through assistive devices and cognitive therapies. The largest burden of NIHL has been through occupational exposures; however, noise-induced hearing loss can also be due to unsafe recreational, residential, social and military service-related noise exposures. It is estimated that 15% of young people are exposed to sufficient leisure noises (i.e. concerts, sporting events, daily activities, personal listening devices, etc.) to cause NIHL. There is not a limited list of noise sources that can cause hearing loss; rather, it is important to understand that exposure to excessively high decibel (dB) levels from any sound source over time, can cause hearing loss. The first symptom of NIHL may be difficulty hearing a conversation against a noisy background. The effect of hearing loss on speech perception has two components. The first component is the loss of audibility, which may be perceived as an overall decrease in volume. Modern hearing aids compensate this loss with amplification. The second component is known as \u201cdistortion\" or \u201cclarity loss\u201d due to selective frequency loss.\u201d Consonants, due to their higher frequency, are typically affected first. For example, the sounds \u201cs\u201d and \u201ct\u201d are often difficult to hear for those with hearing loss, affecting clarity of speech. NIHL can affect either one or both ears. Monaural hearing loss causes problems with directional hearing, affecting the ability to localize sound.",
            "score": 94.14921128749847
        },
        {
            "docid": "19008500_13",
            "document": "Near-death experience . NDEs are associated with changes in personality and outlook on life. Kenneth Ring (professor of psychology) has identified a consistent set of value and belief changes associated with people who have had a near-death experience. Among these changes one finds a greater appreciation for life, higher self-esteem, greater compassion for others, less concern for acquiring material wealth, a heightened sense of purpose and self-understanding, desire to learn, elevated spirituality, greater ecological sensitivity and planetary concern, and a feeling of being more intuitive. Changes may also include a need for being alone more often, increased physical sensitivity; diminished tolerance of light, sound, alcohol, or drugs; a feeling that the brain has been \"altered\" to encompass more; and a feeling that one is now using the \"whole brain\" rather than a small part. However, not all after-effects are beneficial and Greyson describes circumstances where changes in attitudes and behavior can lead to psychosocial and psychospiritual problems.",
            "score": 86.55339694023132
        },
        {
            "docid": "18614_31",
            "document": "Language acquisition . Language acquisition has been studied from the perspective of developmental psychology and neuroscience, which looks at learning to use and understand language parallel to a child's brain development. It has been determined, through empirical research on developmentally normal children, as well as through some extreme cases of language deprivation, that there is a \"sensitive period\" of language acquisition in which human infants have the ability to learn any language. Several findings have observed that from birth until the age of six months, infants can discriminate the phonetic contrasts of all languages. Researchers believe that this gives infants the ability to acquire the language spoken around them. After such an age, the child is able to perceive only the phonemes specific to the language learned. The reduced phonemic sensitivity enables children to build phonemic categories and recognize stress patterns and sound combinations specific to the language they are acquiring. As Wilder Penfield noted, \"Before the child begins to speak and to perceive, the uncommitted cortex is a blank slate on which nothing has been written. In the ensuing years much is written, and the writing is normally never erased. After the age of ten or twelve, the general functional connections have been established and fixed for the speech cortex.\" According to the sensitive or critical period models, the age at which a child acquires the ability to use language is a predictor of how well he or she is ultimately able to use language. However, there may be an age at which becoming a fluent and natural user of a language is no longer possible; Penfield and Roberts (1959) cap their sensitive period at 9 years old. Our brains may be automatically wired to learn languages, but the ability does not last into adulthood in the same way that it exists during development. By the onset of puberty (around age 12), language acquisition has typically been solidified and it becomes more difficult to learn a language in the same way a native speaker would. Just like children who speak vocally, deaf children go through the same critical period. Deaf children who acquire their first language later in life show lower performance in complex aspects of grammar. At this point, it is usually a second language that a person is trying to acquire and not a first.",
            "score": 99.5355224609375
        }
    ],
    "r": [
        {
            "docid": "987320_19",
            "document": "Neurotechnology . Magnetic resonance imaging is a vital tool in neurological research in showing activation in the brain as well as providing a comprehensive image of the brain being studied. While MRIs are used clinically for showing brain size, it still has relevance in the study of brains because it can be used to determine extent of injuries or deformation. These can have a significant effect on personality, sense perception, memory, higher order thinking, movement, and spatial understanding. However, current research tends to focus more so on fMRI or real-time functional MRI (rtfMRI). These two methods allow the scientist or the participant, respectively, to view activation in the brain. This is incredibly vital in understanding how a person thinks and how their brain reacts to a person's environment, as well as understanding how the brain works under various stressors or dysfunctions. Real-time functional MRI is a revolutionary tool available to neurologists and neuroscientists because patients can see how their brain reacts to stressors and can perceive visual feedback. CT scans are very similar to MRI in their academic use because they can be used to image the brain upon injury, but they are more limited in perceptual feedback. CTs are generally used in clinical studies far more than in academic studies, and are found far more often in a hospital than a research facility. PET scans are also finding more relevance in academia because they can be used to observe metabolic uptake of neurons, giving researchers a wider perspective about neural activity in the brain for a given condition. Combinations of these methods can provide researchers with knowledge of both physiological and metabolic behaviors of loci in the brain and can be used to explain activation and deactivation of parts of the brain under specific conditions.",
            "score": 138.75096130371094
        },
        {
            "docid": "41421221_2",
            "document": "Elderspeak . Elderspeak is a specialized speech style used by younger adults with older adults, characterized by simpler vocabulary and sentence structure, filler words, lexical fillers, overly-endearing terms, closed-ended questions, using the collective \"we\", repetition, and speaking more slowly. Elderspeak stems from the stereotype that older people have reduced cognitive abilities, such as in language processing and production, and its use may be a result of or contribute to ageism. Although some aspects of elderspeak may be beneficial for some recipients, it is generally seen as inappropriate and a hindrance to intergenerational communication. Individuals and other care providers can use self-monitoring and formal educational programs to overcome elderspeak, and thus promote successful aging for older clients. Communication accommodation theory looks at how we modify our speech for our conversation partners. People can change their speech to be more similar to their conversation partners\u2019 speech, which is known as convergence. In other circumstances, people may change their speech to be more distinct, a process known as divergence. Furthermore, these modifications can promote fluidity of conversation and ease understanding. People tend to draw on stereotypes to infer what types of accommodations need to be made. In terms of intergenerational communication, young people tend to over-accommodate when conversing with older persons. That is, they make more adjustments than necessary. Young people tend to infer that older adults are slower at processing information and more cognitively inflexible. They make these inferences based on the perception of their conversation partner as old, rather than based on information about their conversational ability. This belief leads to more accommodations than necessary.  Ryan and colleagues (1986) assessed several strategies used by younger individuals when accommodating to older adults which include:",
            "score": 134.9091796875
        },
        {
            "docid": "49604_37",
            "document": "Hearing loss . Another method for quantifying hearing loss is a speech-in-noise test. As the name implies, a speech-in-noise test gives an indication of how well one can understand speech in a noisy environment. A person with a hearing loss will often be less able to understand speech, especially in noisy conditions. This is especially true for people who have a sensorineural loss \u2013 which is by far the most common type of hearing loss. As such, speech-in-noise tests can provide valuable information about a person's hearing ability, and can be used to detect the presence of a sensorineural hearing loss. A recently developed digit-triple speech-in-noise test may be a more efficient screening test.",
            "score": 134.7678985595703
        },
        {
            "docid": "3452485_10",
            "document": "Memory disorder . While studies show that there are \u201cnormal\u201d aspects to aging, such as graying hair and changes in vision, there are changes such as forgetting how to do things that are not considered \u201cnormal\u201d. The importance of understanding that the changes most frequently observed and noticed on a daily basis concerning aging loved ones is imperative. While mild cognitive impairment can be considered a normal part of aging, the differences must be noted. In one study by J. Shagam, it was noted that while Diabetes and Hypertension are not considered part of normal aging, they would be classified under mild cognitive impairment. With this being said, it is important to differentiate the differences with what can be potentially harmful and what is not. It is difficult to accurately diagnose dementia due to the fact that most people are unaware of what to be looking for and also because there is no specific test which can be given as a diagnostic tool. What is even more evident is that the symptoms among dementia, Alzheimer's and Parkinson's related dementia tend to go beyond just one ailment. While there are different forms of dementia, Vascular dementia as it would sound is associated with vascular cautions. This form of dementia is not a slow deterioration but rather a sudden and unexpected change due to heart attack or stroke significantly reducing blood to the brain. Research has shown that persistent hypertension can be contributory to the breakdown of the BBB. The blood-brain barrier (BBB) serves as a \u201cgatekeeper\u201d for the brain by keeping out water and other substances. Various studies show that as the brain ages the blood-brain barrier starts to break down and become dysfunctional. There are different ways to measure the thinning of the BBB and one that most are familiar with is imaging, this consists of taking pictures of the brain using CT scans, MRI, or PET scans. Previous research also indicates that with aging and the thinning of the BBB, cognitive changes were also occurring within the section of the brain known as the hippocampus. This shows a relationship between aging and the thinning of the BBB and its effects on the brain. Also indicated by the aging brain are learning and memory impairments. While changes to the BBB are not a cause of impairment to cognitive functions alone research suggests that there is a relationship. Another impairment which is indicative of brain aging and the breakdown of the BBB is the accretion of iron. Too much iron in the body can create free radicals which could influence the degeneration of the blood-brain barrier. One other specific age related factor noted in Popsecu et al. is a decrease in estrogen as one ages could adversely affect the breakdown of the blood-brain barrier and create a sensitivity to neurodegeneration. As pointed out earlier, dementia is a broad category of memory impairments most commonly associated with ageing. Another symptom which should be monitored is Type 2 diabetes, which can lead to vascular dementia. Also linked with vascular dementia issues is high cholesterol; furthermore, this risk factor is related to cognitive decline, stroke, and Alzheimer's disease. It is estimated that within 20 years, worldwide prevalence will increase twofold. By 2050, this number is expected to increase to 115 million. Overall, dementia incidence is similar for men and women. However, after 90 years of age dementia incidence declines in men but not in women.",
            "score": 132.14329528808594
        },
        {
            "docid": "5366050_41",
            "document": "Speech perception . Cochlear implantation restores access to the acoustic signal in individuals with sensorineural hearing loss. The acoustic information conveyed by an implant is usually sufficient for implant users to properly recognize speech of people they know even without visual clues. For cochlear implant users, it is more difficult to understand unknown speakers and sounds. The perceptual abilities of children that received an implant after the age of two are significantly better than of those who were implanted in adulthood. A number of factors have been shown to influence perceptual performance, specifically: duration of deafness prior to implantation, age of onset of deafness, age at implantation (such age effects may be related to the Critical period hypothesis) and the duration of using an implant. There are differences between children with congenital and acquired deafness. Postlingually deaf children have better results than the prelingually deaf and adapt to a cochlear implant faster. In both children with cochlear implants and normal hearing, vowels and voice onset time becomes prevalent in development before the ability to discriminate the place of articulation. Several months following implantation, children with cochlear implants can normalize speech perception. One of the basic problems in the study of speech is how to deal with the noise in the speech signal. This is shown by the difficulty that computer speech recognition systems have with recognizing human speech. These programs can do well at recognizing speech when they have been trained on a specific speaker's voice, and under quiet conditions. However, these systems often do poorly in more realistic listening situations where humans can understand speech without difficulty.",
            "score": 131.75778198242188
        },
        {
            "docid": "30459919_2",
            "document": "Speech Buddies . Speech Buddies are a series of speech therapy tools that are used to remediate articulation and speech sound disorders using the widely accepted teaching method of tactile feedback. Articulation, or speech sound disorders occur when a person has difficulty producing a sound correctly. Sounds may be left off, substituted, added or changed, making it difficult to be understood. Often occurring when children are learning to speak, these errors are considered a disorder if they persist after a certain developmental age. A developmental age at which a child should be able to correctly produce a certain sound is different depending on the difficulty of the sound. The B-sound is one of the first mastered, while the R-sound is more difficult and may not be mastered until several years later. Most articulation disorders are of no known cause, though many can be attributed to other disorders such as autism, or hearing impairment.",
            "score": 130.07595825195312
        },
        {
            "docid": "27016834_23",
            "document": "Cognitive flexibility . The elderly often experience deficits in cognitive flexibility. The aging brain undergoes physical and functional changes including a decline in processing speed, central sensory functioning, white matter integrity, and brain volume. Regions associated with cognitive flexibility such as the PFC and PC atrophy, or shrink, with age, but also show greater task-related activation in older individuals when compared to younger individuals. This increase in blood flow is potentially related to the evidence that atrophy heightens blood flow and metabolism, which is measured as the BOLD response, or blood-oxygen-level dependence, with fMRI. Studies suggest that aerobic exercise and training can have plasticity inducing effects that could potentially serve as an intervention in old age that combat the decline in executive function.",
            "score": 129.58587646484375
        },
        {
            "docid": "45838854_10",
            "document": "Auditory arrhythmia . Those diagnosed with autism tend to have many difficulties processing auditory stimuli. For example, they most often endure language and speech delays, hyperacusis, have difficulties communicating in large social groups, and may experience difficulties hearing certain voices in a noisy environment. These qualities make quality of life difficult, by inhibiting their ability to fully participate in social and educational circumstances in various parts of their lives. As shown in research published in the International Journal of Psychophysiology, efferent pathways throughout the brain help to control various functions throughout the body. For example, in those with autism, pathways running through to the middle ear muscles make it difficult for the person to focus on a single voice when there is a lot of background noise. Raising eyelids was also found to hinder the stapedius muscle by tensing it, which in turn makes it difficult for these individuals to hear other talking when there is background noise present. The laryngeal and pharyngeal muscles located in the throat make prosody and intonation difficult to understand for people with autism. During research, tasks and tests were conducted to see if there is a correlation between cardiac rhythms, respiratory sinus arrhythmias, and auditory processing, or auditory arrhythmia. Because these symptoms tend to go hand in hand, researchers were looking to see if there was a possibility of improving auditory processing. If researchers learn how to effectively improve auditory sensations in people diagnose with autism, then there is a possibility that they can then begin finding the improvement for those only suffering from auditory arrhythmia.",
            "score": 125.84093475341797
        },
        {
            "docid": "433584_4",
            "document": "McGurk effect . Vision is the primary sense for humans, but speech perception is multimodal, which means that it involves information from more than one sensory modality, in particular, audition and vision. The McGurk effect arises during phonetic processing because the integration of audio and visual information happens early in speech perception. The McGurk effect is very robust; that is, knowledge about it seems to have little effect on one's perception of it. This is different from certain optical illusions, which break down once one 'sees through' them. Some people, including those that have been researching the phenomenon for more than twenty years, experience the effect even when they are aware that it is taking place. With the exception of people who can identify most of what is being said from speech-reading alone, most people are quite limited in their ability to identify speech from visual-only signals. A more extensive phenomenon is the ability of visual speech to increase the intelligibility of heard speech in a noisy environment. Visible speech can also alter the perception of perfectly audible speech sounds when the visual speech stimuli are mismatched with the auditory speech. Normally, speech perception is thought to be an auditory process; however, our use of information is immediate, automatic, and, to a large degree, unconscious and therefore, despite what is widely accepted as true, speech is not only something we hear. Speech is perceived by all of the senses working together (seeing, touching, and listening to a face move). The brain is often unaware of the separate sensory contributions of what it perceives. Therefore, when it comes to recognizing speech the brain cannot differentiate whether it is seeing or hearing the incoming information.",
            "score": 125.6838150024414
        },
        {
            "docid": "37691878_2",
            "document": "Phonemic restoration effect . Phonemic restoration effect is a perceptual phenomenon where under certain conditions, sounds actually missing from a speech signal can be restored by the brain and may appear to be heard. The effect occurs when missing phonemes in an auditory signal are replaced with a noise that would have the physical properties to mask those phonemes, creating an ambiguity. In such ambiguity, the brain tends towards filling in absent phonemes. The effect can be so strong that some listeners may not even notice that there are phonemes missing. This effect is commonly observed in a conversation with heavy background noise, making it difficult to properly hear every phoneme being spoken. Different factors can change the strength of the effect, including how rich the context or linguistic cues are in speech, as well as the listener's state, such as their hearing status or age.",
            "score": 125.52037811279297
        },
        {
            "docid": "56439577_8",
            "document": "Temporal envelope and fine structure . Responses to the temporal-envelope cues of speech or other complex sounds persist up the auditory pathway, eventually to the various fields of the auditory cortex in many animals. In the Primary Auditory Cortex, responses can encode AM rates by phase-locking up to about 20\u201330\u00a0Hz, while faster rates induce sustained and often tuned responses. A topographical representation of AM rate has been demonstrated in the primary auditory cortex of awake macaques. This representation is approximately perpendicular to the axis of the tonotopic gradient, consistent with an orthogonal organization of spectral and temporal features in the auditory cortex. Combining these temporal responses with the spectral selectivity of A1 neurons gives rise to the spectro-temporal receptive fields that often capture well cortical responses to complex modulated sounds. In secondary auditory cortical fields, responses become temporally more sluggish and spectrally broader, but are still able to phase-lock to the salient features of speech and musical sounds. Tuning to AM rates below about 64\u00a0Hz is also found in the human auditory cortex as revealed by brain-imaging techniques (fMRI) and cortical recordings in epileptic patients (electrocorticography). This is consistent with neuropsychological studies of brain-damaged patients and with the notion that the central auditory system performs some form of spectral decomposition of the ENVp of incoming sounds. Interestingly, the ranges over which cortical responses encode well the temporal-envelope cues of speech have been shown to be predictive of the human ability to understand speech. In the human superior temporal gyrus (STG), an anterior-posterior spatial organization of spectro-temporal modulation tuning has been found in response to speech sounds, the posterior STG being tuned for temporally fast varying speech sounds with low spectral modulations and the anterior STG being tuned for temporally slow varying speech sounds with high spectral modulations.",
            "score": 123.54658508300781
        },
        {
            "docid": "10042066_2",
            "document": "Developmental linguistics . Developmental linguistics is the study of the development of linguistic ability in an individual, particularly the acquisition of language in childhood. It involves research into the different stages in language acquisition, language retention, and language loss in both first and second languages, in addition to the area of bilingualism. Before infants can speak, the neural circuits in their brains are constantly being influenced by exposure to language. The neurobiology of language contains a \"critical period\" in which children are most sensitive to language. The different aspects of language have varying \"critical periods\". Studies show that the critical period for phonetics is toward the end of the first year. At 18 months, a toddler's vocabulary vastly expands. The critical period for syntactic learning is 18-36 months. Infants of different mother languages can be differentiated at the age of 10 months. At 20 weeks they begin vocal imitation. Beginning when babies are about 12 months, they take on computational learning and social learning. Social interactions for infants and toddlers is important because it helps associate \"perception and action\". In-person social interaction rather than audio or video better facilitates learning in babies because they learn from how other people respond to them, especially their mothers. Babies have to learn to mimic certain syllables, which takes practice in manipulating tongue and lip movement. Sensory-motor learning in speech is linked to exposure to speech, which is very sensitive to language. Infants exposed to Spanish exhibit a different vocalization than infants exposed to English. One study took infants that were learning English and made them listen to Spanish in 12 sessions. The result showed consequent alterations in their vocalization, which demonstrated Spanish prosody.  One study used MEG to record activation in the brains of newborns, 6 months olds and 12 months olds while presenting them with syllables, harmonics and non-speech sounds. For the 6 month and 12 month old, the auditory and motor areas responded to speech. The newborn showed auditory activation but not motor activation. Another study presented 3 month olds with sentences and recorded their brain activity via fMRI motor speech areas did activate. These studies suggest that the link between perception and action begins to develop at 3 months. When babies are young, they are actually the most sensitive to distinguishing all phonetic units. During an infant\u2019s 1st year of life, they have to differentiate between about 40 phonetic units. When they are older they have usually been exposed to their native language so much that they lose this ability and can only distinguish phonetic units in their native language. Even at 12 months babies exhibit a deficit in differentiated non-native sounds. However, their ability to distinguish sounds in their native language continues to improve and become more fine-tuned. For example, Japanese learning infants learn that there is no differentiation between /r/ and /l/. However, in English, \"rake\" and \"lake\" are two different words. Japanese babies eventually lose their ability to distinguish between /r/ and /l/. Similarly, a Spanish learning infant cannot form words until they learn the difference between works like \"bano\" and \"pano\", because the /p/ sound is different than the /b/ sound. English learning babies do not learn to differentiate between the two.",
            "score": 123.10910034179688
        },
        {
            "docid": "4743980_30",
            "document": "Tip of the tongue . Age is an important factor when considering TOT states. There are complaints that problems recalling information increases with age. The frequency of TOTs increases in adulthood and even more so during the elderly years. Compared with young adults, older adults generally report having more TOT states, fewer alternate words, and less phonological information about the target word. The underpinnings of TOT with regard to age have focused on neurological brain differences. Current research uses neuroimaging methods to access the presence of different brain patterns when a younger and older individual is experiencing a TOT state. It is found that older and younger individuals employ a similar network of brain regions during TOT states such as the prefrontal cortex, left insula, and sensorimotor cortex. However, older individuals show differences in activity in some areas compared to younger individuals. TOTs increase with age-related gray matter loss in the left insula for older individuals. This is accompanied by less activity in the left insula and is related to higher frequency of TOTs. Furthermore, it was found that older individuals have over-activation in their prefrontal cortex when experiencing TOT states. This may indicate a continued search when the retrieval process fails and a TOT state is experienced. More specifically, greater activation in the sensorimotor cortex in older individuals and less in younger adults may reflect differences in the knowledge that is used to retrieve the target information. Priming words during word retrieval tests generally reduces the frequency of TOTs and improves the retrieval of the target word and has been shown to have a larger benefit for older adults. This is consistent with the spreading activation model, where neural connections are strengthened when used more. Although older people experience more tip of the tongue states more often than any other category, recent studies have shown that frequent tip of the tongue states are not linked at all to dementia, which is common in the elderly. Despite the association of increased age with lower levels of episodic memory and more frequent TOT states, the two phenomena seem to be largely independent of one another.",
            "score": 122.37043762207031
        },
        {
            "docid": "44742488_3",
            "document": "The Boston process approach . In order to start this procedure, the neuropsychologist first looks at the participant\u2019s behaviors and what kind of complications they may have. Based on the given information, the person performs a standardized test. While the participant is performing the test, the neuropsychologist examines how they are answering each question. The neuropsychologist scrutinizes the process to see if there are any intellectual problems exhibited. Alterations can be made to the test by simply adding, or lessening the time the person has to complete the test. After using the Boston process approach, the clinician looks at the outcomes to make conclusions on what is going on in the brain. These conclusions are based on the errors that occurred during the test. The point of looking at the errors is to see what issues may have occurred when trying to process the information and answer the question in order to evaluate the specific deficit(s). Certain inaccuracies may infer that there\u2019s damage to a definite part of the brain, like speech difficulties implicating the speech area in the left hemisphere.",
            "score": 121.97742462158203
        },
        {
            "docid": "971305_19",
            "document": "Haemodynamic response . If fMRI can be used to detect the regular flow of blood in a healthy brain, it can also be used to detect the problems with a brain that has undergone degenerative diseases. Functional MRI, using haemodynamic response, can help assess the effects of stroke and other degenerative diseases such as Alzheimer\u2019s disease on brain function. Another way fMRI could be used is in the planning of surgery of the brain. Surgeons can use fMRI to detect blood flow of the most active areas of the brain and the areas involved in critical functions like thought, speech, movement, etc. In this way, brain procedures are less dangerous because there is a brain mapping that shows which areas are vital to a person\u2019s life. Haemodynamic response is vital to fMRI and clinical use because through the study of blood flow we are able to examine the anatomy of the brain and effectively plan out procedures of the brain and link together the causes of degenerative brain disease.",
            "score": 120.88224792480469
        },
        {
            "docid": "324918_7",
            "document": "Anomic aphasia . Although the main causes are not specifically known, many researchers have found factors contributing to anomic aphasia. It is known that people with damage to the left hemisphere of the brain are more likely to have anomic aphasia. Broca's area, the speech production center in the brain, was linked to being the source for speech execution problems, with the use of functional magnetic resonance imaging (fMRI), now commonly used to study anomic patients. Other experts believe that damage to Wernicke's area, which is the speech comprehension area of the brain, is connected to anomia because the patients cannot comprehend the words that they are hearing.",
            "score": 120.4673080444336
        },
        {
            "docid": "34357381_10",
            "document": "Speech acquisition . Knowing when a speech sound should be accurately produced helps parents and professionals determine when child may have an articulation disorder. There have been two traditional methods used to compare a child's articulation of speech sounds to chronological age. The first is comparing the number of correct responses on a standardized articulation test with the normative data for a given age on the same test. This allows evaluators to see how well a child is producing sounds compared to their same aged peers. The second method consists of comparing an individual sound a child produces with developmental norms for that individual sound. The second method can be difficult when considering the differing normative data and other factors that affect typical speech development.  Many norms are based on age expectations in which a majority of children of a certain age are accurately producing a sound (75% or 90% depending on the study). Using the results from Sander (1972), Templin (1957), and Wellman, Case, Mengert, & Bradbury, (1931), the American Speech-Language Hearing Association suggests the following: Sounds mastered by age 3 include /p, m, h, n, w, b/; by age 4 /k, g, d, f, y/; by age 6 /t, \u014b, r, l/; by age 7 /t\u0283, \u0283, j, \u03b8/. and by age 8 /s, z, v, \u00f0, \u0292/.",
            "score": 120.10196685791016
        },
        {
            "docid": "5067510_32",
            "document": "Lie detection . Studies using functional magnetic resonance imaging (fMRI) have shown that it has potential to be used as a method of lie detection. While a polygraph detects changes in activity in the peripheral nervous system, fMRI has the potential to catch the lie at the 'source'. To use an MRI as a lie detector, an fMRI should be used by placing a magnetic band as a scanner on a subject's head. However, the neurobiological systems that relate to lying are currently poorly understood. The current consensus is that faced with a forced choice paradigm, in which a subject has the choice of telling the truth or spontaneously generating a lie, lying can be distinguished due to increased prefrontal and parietal lobe activity. More specifically, the superior medial and inferolateral prefrontal cortices show net activation in the process of spontaneous lie generation (which involves suppression of the truthful response as well as generating a conceivable lie). There also is evidence of increased activation in the anterior cingulate cortex when lies are told. The fMRI shows the use of oxygen by the brain, allowing for the identification of which portions of the brain are using more oxygen during a specific task. By studying the brain images, researchers are able to map the systematic procedure the brain went through to produce the action or decision. Subjects are often offered monetary incentive if they can successfully deceive the process in hopes of generating a 'real world' scenario. Using this method, an initial 2005 study on individuals ( not group averages as previous studies) without pattern recognition and automation showed that lies can be distinguished 78% of the time. That statistic has risen, in one study, to 100% when predicting a lie in an individual when baseline lie/truth levels were closely studied with training from pattern recognition technology (machine learning). fMRI does rely upon the individual remaining still and safeguards in the analysis such that the questions can not be gamed by the participant (G. Ganis 2010). Studies have been done on Chinese individuals and their language and cultural differences did not change results. To show the robustness of this fMRI technology, a study (S. Spence 2011) was done that showed fMRI lie detection / truth verification technology worked even in a group of 52 schizophrenic patients, 27 of whom were experiencing delusions at the time of the study.",
            "score": 119.48242950439453
        },
        {
            "docid": "33970693_12",
            "document": "Age discrimination in the United States . There are many reasons why older workers may have a difficult time finding work after being laid-off. Many older workers were employed by the same employer for many years, and in some cases that may have been their only job during their entire career. Many older workers are less likely to be skilled at job-seeking, as they haven\u2019t had to search for employment in many years. Also, as the years have passed, companies have turned to more efficient means by offering applications only online. However, while the current generation relies on technology, many older workers may be accustomed to older methods of performing the same labor. Some may find it difficult to use a computer, whether to find and apply for jobs or in the workplace. Because many of the people in the older generation are less likely to have skills on the computer, their technological inabilities also hold them back from being hired. As the years have passed, many companies have begun to focus using the internet and other programs on the computer, making it more likely they will hire a younger worker that is capable of using technology over an older person that doesn\u2019t know how. This lack of knowledge means that companies would have to provide more training for the elderly person than they would often have to provide for a younger employee. This can be costly and time consuming for companies. Older adults often resist the use of computers for various reasons, such as impaired eyesight and hearing, arthritis and other physical ailments, and reduced cognitive skills, including memory loss and short attention spans, which make surfing the Web more challenging. Learning how to effectively utilize new technology for the elderly can be more demanding due to the fact that learning new skills is stressful both mentally and physically.",
            "score": 119.41049194335938
        },
        {
            "docid": "22648231_9",
            "document": "Ageism . Stereotyping is a tool of cognition which involves categorizing into groups and attributing characteristics to these groups. Stereotypes are necessary for processing huge volumes of information which would otherwise overload a person and are generally accurate descriptors of group characteristics, though some stereotypes are inaccurate. However, they can cause harm when the content of the stereotype is incorrect with respect to most of the group or where a stereotype is so strongly held that it overrides evidence which shows that an individual does not conform to it. For example, age-based stereotypes prime one to draw very different conclusions when one sees an older and a younger adult with, say, back pain or a limp. One might well assume that the younger person's condition is temporary and treatable, following an accident, while the older person's condition is chronic and less susceptible to intervention. On average, this might be true, but plenty of older people have accidents and recover quickly and very young people (such as infants, toddlers and small children) can become permanently disabled in the same situation. This assumption may have no consequence if one makes it in the blink of an eye as one is passing someone in the street, but if it is held by a health professional offering treatment or managers thinking about occupational health, it could inappropriately influence their actions and lead to age-related discrimination. Managers have been accused, by Erdman Palmore, as stereotyping older workers as being resistant to change, not creative, cautious, slow to make judgments, lower in physical capacity, uninterested in technological change, and difficult to train. Another example is when people are rude to children because of their high pitched voice, even if they are kind and courteous. A review of the research literature related to age stereotypes in the workplace was recently published in the Journal of Management.",
            "score": 119.2334213256836
        },
        {
            "docid": "30872390_9",
            "document": "Prelingual deafness . Speech perception can be corrected prior to language acquisition with cochlear implants. After a year and a half experience, researchers found the deaf culture was able to identify words and comprehend movements of others' lips. There is a greater opportunity to hear a sound depending on the location of electrodes compared to the tissue and the number of remaining neurons located in the auditory system. In addition, individual capacities as well as the neural supply to the cochlea play a role in the process of learning with cochlear implantation.  Research has continuously found that early implantation leads to better performance than older implantation. Studies continue to show that children with prelingual deafness are able interact in society comfortably when implantation occurs before the age of five. Speech production is a slower procedure in the beginning since creating words requires more effort. Children who had almost two years experience with cochlear implants were able to generate diphthongs and sound out most vowels. They develop skills to understand more information as well as put together letters.  Cochlear implants give deaf individuals the chance to understand auditory messages. Progress was analyzed after several groups of children were given vocabulary and language tests. After three years of practice, the children with the devices did as well as children that had no previous issues with hearing. Specifically, cochlear implants allow children with prelingual deafness to acquire skills similar to children with minimal or no residual hearing.",
            "score": 119.00341033935547
        },
        {
            "docid": "25136770_13",
            "document": "Cognitive musicology . Both music and speech rely on sound processing and require interpretation of several sound features such as timbre, pitch, duration, and their interactions (Elzbieta, 2015). A fMRI study revealed that the Broca's and Wernicke's areas, two areas that are known to activated during speech and language processing, were found activated while the subject was listening to unexpected musical chords (Elzbieta, 2015). This relation between language and music may explain why, it has been found that exposure to music has produced an acceleration in the development of behaviors related to the acquisition of language. The Suzuki music education which is very widely known, emphasizes learning music by ear over reading musical notation and preferably begins with formal lessons between the ages of 3 and 5 years. One fundamental reasoning in favor of this education points to a parallelism between natural speech acquisition and purely auditory based musical training as opposed to musical training due to visual cues. There is evidence that children who take music classes have obtained skills to help them in language acquisition and learning (Oechslin, 2015), an ability that relies heavily on the dorsal pathway. Other studies show an overall enhancement of verbal intelligence in children taking music classes. Since both activities tap into several integrated brain functions and have shared brain pathways it is understandable why strength in music acquisition might also correlate with strength in language acquisition.",
            "score": 118.93338012695312
        },
        {
            "docid": "17682224_10",
            "document": "Evolution of the brain . With the use of in vivo Magnetic resonance imaging (MRI) and tissue sampling, different cortical samples from members of each hominoid species were analyzed. In each species, specific areas were either relatively enlarged or shrunken, which can detail neural organizations. Different sizes in the corticol areas can show specific adaptations, functional specializations and evolutionary events that were changes in how the hominoid brain is organized. In early prediction it was thought that the frontal lobe, a large part of the brain that is generally devoted to behavior and social interaction, predicted the differences in behavior between hominoid and humans. Discrediting this theory was evidence supporting that damage to the frontal lobe in both humans and hominoids show atypical social and emotional behavior; thus, this similarity means that the frontal lobe was not very likely to be selected for reorganization. Instead, it is now believed that evolution occurred in other parts of the brain that are strictly associated with certain behaviors. The reorganization that took place is thought to have been more organizational than volumetric; whereas the brain volumes were relatively the same but specific landmark position of surface anatomical features, for example, the lunate sulcus suggest that the brains had been through a neurological reorganization. There is also evidence that the early hominin lineage also underwent a quiescent period, which supports the idea of neural reorganization. Dental fossil records for early humans and hominins show that immature hominins, including australopithecines and members of \"Homo\", reveal that these species have a quiescent period (Bown et al. 1987). A quiescent period is a period in which there are no dental eruptions of adult teeth; at this time the child becomes more accustomed to social structure, and development of culture. During this time the child is given an extra advantage over other hominoids, devoting several years into developing speech and learning to cooperate within a community. This period is also discussed in relation to encephalization. It was discovered that chimpanzees do not have this neutral dental period and suggest that a quiescent period occurred in very early hominin evolution. Using the models for neurological reorganization it can be suggested the cause for this period, dubbed middle childhood, is most likely for enhanced foraging abilities in varying seasonal environments. To understand the development of human dentition, taking a look at behavior and biology.",
            "score": 118.519775390625
        },
        {
            "docid": "37691351_4",
            "document": "Neuroscience and race . Neurotechnology enables studying the brain and racial interactions, though this study can be difficult because these interactions can be hard to replicate. Face recognition tests are the most commonly used method in studying racial interactions. These tests consist of observing own-race and other-race faces, and studying the brain's response to the faces. There are three major neurological techniques used to measure the brain's response to these simulated racial interactions. Functional magnetic resonance imaging (fMRI) measures the brain activity through measuring the blood oxygen level in the brain. This test gives insight into which regions of the brain are active during a certain event. Event-related potentials (ERPs) measure the brain's activity through measuring electrical impulses by electrodes on the head. This test gives insight in rapid changes in the brain. Transcranial magnetic stimulation (TMS) measures the response of a region of the brain once activated through magnetism. This test gives insight into causality of occurrences and gives specific insight in what the brain regions are doing. Brain-damaged patients have also been used to study racial interactions, by studying how racial interactions are affected when specific brain regions are damaged. These studies give insight into how different brain regions are involved in racial interactions once certain regions have been damaged. An implicit association test (IAC) is often used to measure the racial bias of people in studies by testing what objects, whether positive or negative, people associate with same-race or other-race faces.",
            "score": 118.34762573242188
        },
        {
            "docid": "3354877_5",
            "document": "Social neuroscience . A number of methods are used in social neuroscience to investigate the confluence of neural and social processes. These methods draw from behavioral techniques developed in social psychology, cognitive psychology, and neuropsychology, and are associated with a variety of neurobiological techniques including functional magnetic resonance imaging (fMRI), magnetoencephalography (MEG), positron emission tomography (PET), facial electromyography (EMG), transcranial magnetic stimulation (TMS), electroencephalography (EEG), event-related potentials (ERPs), electrocardiograms, electromyograms, endocrinology, immunology, galvanic skin response (GSR), single-cell recording, and studies of focal brain lesion patients. Animal models are also important to investigate the putative role of specific brain structures, circuits, or processes (e.g., the reward system and drug addiction). In addition, quantitative meta-analyses are important to move beyond idiosyncrasies of individual studies, and neurodevelopmental investigations can contribute to our understanding of brain-behavior associations. The two most popular forms of methods used in social neuroscience are fMRI and EEG. fMRI are very cost efficient and high in spatial resolution. However, they are low in temporal resolution and therefore, are best to discover pathways in the brain that are used during social experiments. fMRI have low temporal resolution (timing) because they read oxygenated blood levels that pool to the parts of the brain that are activated and need more oxygen. Thus, the blood takes time to travel to the part of the brain being activated and in reverse provides a lower ability to test for exact timing of activation during social experiments. EEG is best used when a researcher is trying to brain map a certain area that correlates to a social construct that is being studied. EEGs provide high temporal resolution but low spatial resolution. In which, the timing of the activation is very accurate but it is hard to pinpoint exact areas on the brain, researchers are to narrow down locations and areas but they also create a lot of \"noise\". Most recently, researchers have been using TMS which is the best way to discover the exact location in the process of brain mapping. This machine can turn on and off parts of the brain which then allows researchers to test what that part of the brain is used for during social events. However, this machine is so expensive that it is rarely used.",
            "score": 118.21871185302734
        },
        {
            "docid": "32105732_3",
            "document": "Spatial hearing loss . People with spatial hearing loss have difficulty processing speech that arrives from one direction while simultaneously filtering out 'noise' arriving from other directions. Research has shown spatial hearing loss to be a leading cause of central auditory processing disorder (CAPD) in children. Children with spatial hearing loss commonly present with difficulties understanding speech in the classroom. Spatial hearing loss is found in most people over 70 years of age, and can sometimes be independent of other types of age related hearing loss. As with presbycusis, spatial hearing loss varies with age. Through childhood and into adulthood it can be viewed as spatial hearing gain (with it becoming easier to hear speech in noise), and then with middle age and beyond the spatial hearing loss begins (with it becoming harder again to hear speech in noise).",
            "score": 117.01665496826172
        },
        {
            "docid": "5505463_3",
            "document": "Bimodal bilingualism . Most modern neurological studies of bilingualism employ functional neuroimaging techniques to elucidate the neurological underpinnings of multilingualism and how multilingualism is beneficial to the brain. Neuroimaging and other neurological studies have demonstrated in recent years that multilingualism has a significant impact on the human brain. The mechanisms required by bilinguals to \"code switch\" (a linguistic term used to describe the rapid alternating between multiple languages within a conversation or discourse), not only demonstrate increased connectivity and density of the neural network in multilinguals, but also appear to provide protection against damage due to age and age-related pathologies, such as Alzheimer's. Multilingualism, especially bimodal multilingualism, can help slow to process of cognitive decline in aging. It is thought that this is a result of the increased work load that the executive system, housed mostly in the frontal cortex, must assume in order to successfully control the use of multiple languages at once. This means that the cortex must be more finely tuned, which results in a \"neural reserve\" that then has neuroprotective benefits. Gray matter volume (GMV) has been shown to be significantly preserved in bimodal bilinguals as compared to monolinguals in multiple brain areas, including the hippocampus, amygdala, anterior temporal lobes, and left insula. Similarly, neuroimaging studies that have compared monolinguals, unimodal bilinguals, and bimodal bilinguals provide evidence that deaf signers exhibit brain activation in patterns different than those of hearing signers, especially in regards to the left superior temporal sulcus. In deaf signers, activation of the superior temporal sulcus is highly lateralized to the left side during facial recognition tasks, while this lateralization was not present in hearing, bimodal signers. Bilinguals also require an effective and fast neural control system to allow them to select and control their languages even while code switching rapidly. Evidence indicates that the left caudate nucleus\u2014a centrally located brain feature that is near the thalamus and the basal ganglia\u2014is an important part of this mechanism, as bilinguals tend to have significantly increased GMV and activation in this region as compared to monolinguals, especially during active code switching tasks. As implied by the significant preservation of gray matter in the hippocampi (an area of the brain largely associated with memory consolidation and higher cognitive function, such as decision-making) of bimodal bilinguals, areas of the brain that help control phonological working memory tend to also have higher activation in those individuals who are proficient in two or more languages. There is also evidence that suggests that the age at which an individual acquires a second language may play a significant role in the varying brain functions associated with bilingualism. For example, individuals who acquired their second language early (before the age of 10) tend to have drastically different activation patterns than do late learners. However, late learners who achieve full proficiency in their second language tend to show similar patterns of activation during auditory tasks regardless of which language is being used, whereas early learners tend to activate different brain areas depending upon which language is being used. Along with the neuroprotective benefits that help to prevent onset of age-related cognitive issues such as dementia, bimodal bilinguals also experience a slightly different pattern of organization of language in the brain. While non-hearing-impaired bimodal bilinguals showed less parietal activation than deaf signers when asked to use only sign language, those same bimodal bilinguals demonstrated greater left parietal activation than did monolinguals. Parietal activation is not typically associated with language production bur rather with motor activity. Therefore, it is logical that bimodal bilinguals, when switching between speech- and sign-based language, stimulate their left parietal areas as a result of their increased need to combine both motor action and language production.",
            "score": 116.87794494628906
        },
        {
            "docid": "422247_33",
            "document": "Self-awareness . Autism spectrum disorder (ASD) is a range of neurodevelopmental disabilities that can adversely impact social communication and create behavioral challenges (Understanding Autism, 2003). \"Autism spectrum disorder (ASD) and autism are both general terms for a group of complex disorders of brain development. These disorders are characterized, in varying degrees, by difficulties in social interaction, verbal and nonverbal communication and repetitive behaviors.\" ASDs can also cause imaginative abnormalities and can range from mild to severe, especially in sensory-motor, perceptual and affective dimensions. Children with ASD may struggle with self-awareness and self acceptance. Their different thinking patterns and brain processing functions in the area of social thinking and actions may compromise their ability to understand themselves and social connections to others. About 75% diagnosed autistics are mentally handicapped in some general way and the other 25% diagnosed with Asperger's Syndrome show average to good cognitive functioning. When we compare our own behavior to the morals and values that we were taught, we can focus more attention on ourselves which increases self-awareness. To understand the many effects of autism spectrum disorders on those afflicted have led many scientists to theorize what level of self-awareness occurs and in what degree. Research found that ASD can be associated with intellectual disability and difficulties in motor coordination and attention. It can also result in physical health issues as well, such as sleep and gastrointestinal disturbances. As a result of all those problems, individuals are literally unaware of themselves. It is well known that children suffering from varying degrees of autism struggle in social situations. Scientists at the University of Cambridge have produced evidence that self-awareness is a main problem for people with ASD. Researchers used functional magnetic resonance scans (FMRI) to measure brain activity in volunteers being asked to make judgments about their own thoughts, opinions, preferences, as well as about someone else's. One area of the brain closely examined was the ventromedial pre-frontal cortex (vMPFC) which is known to be active when people think about themselves. A study out of Stanford University has tried to map out brain circuits with understanding self-awareness in Autism Spectrum Disorders. This study suggests that self-awareness is primarily lacking in social situations but when in private they are more self-aware and present. It is in the company of others while engaging in interpersonal interaction that the self-awareness mechanism seems to fail. Higher functioning individuals on the ASD scale have reported that they are more self-aware when alone unless they are in sensory overload or immediately following social exposure. Self-awareness dissipates when an autistic is faced with a demanding social situation. This theory suggests that this happens due to the behavioral inhibitory system which is responsible for self-preservation. This is the system that prevents human from self-harm like jumping out of a speeding bus or putting our hand on a hot stove. Once a dangerous situation is perceived then the behavioral inhibitory system kicks in and restrains our activities. \"For individuals with ASD, this inhibitory mechanism is so powerful, it operates on the least possible trigger and shows an over sensitivity to impending danger and possible threats. Some of these dangers may be perceived as being in the presence of strangers, or a loud noise from a radio. In these situations self-awareness can be compromised due to the desire of self preservation, which trumps social composure and proper interaction. The Hobson hypothesis reports that autism begins in infancy due to the lack of cognitive and linguistic engagement which in turn results in impaired reflective self-awareness. In this study ten children with Asperger's Syndrome were examined using the Self-understanding Interview. This interview was created by Damon and Hart and focuses on seven core areas or schemas that measure the capacity to think in increasingly difficult levels. This interview will estimate the level of self understanding present. \"The study showed that the Asperger group demonstrated impairment in the 'self-as-object' and 'self-as-subject' domains of the Self-understanding Interview, which supported Hobson's concept of an impaired capacity for self-awareness and self-reflection in people with ASD.\". Self-understanding is a self description in an individual's past, present and future. Without self-understanding it is reported that self-awareness is lacking in people with ASD. Joint attention (JA) was developed as a teaching strategy to help increase positive self-awareness in those with autism spectrum disorder. JA strategies were first used to directly teach about reflected mirror images and how they relate to their reflected image. Mirror Self Awareness Development (MSAD) activities were used as a four-step framework to measure increases in self-awareness in those with ASD. Self-awareness and knowledge is not something that can simply be taught through direct instruction. Instead, students acquire this knowledge by interacting with their environment. Mirror understanding and its relation to the development of self leads to measurable increases in self-awareness in those with ASD. It also proves to be a highly engaging and highly preferred tool in understanding the developmental stages of self- awareness. There have been many different theories and studies done on what degree of self-awareness is displayed among people with autism spectrum disorder. Scientists have done research about the various parts of the brain associated with understanding self and self-awareness. Studies have shown evidence of areas of the brain that are impacted by ASD. Other theories suggest that helping an individual learn more about themselves through Joint Activities, such as the Mirror Self Awareness Development may help teach positive self-awareness and growth. In helping to build self-awareness it is also possible to build self-esteem and self acceptance. This in turn can help to allow the individual with ASD to relate better to their environment and have better social interactions with others.",
            "score": 116.33460235595703
        },
        {
            "docid": "40621603_7",
            "document": "Linguistic intelligence . Hearing plays an important part in both speech generation and comprehension. When speaking, the person can hear their speech, and the brain uses what it hears as a feedback mechanism to fix speech errors. If a single feedback correction occurs multiple times, the brain will begin to incorporate the correction to all future speech, making it a feed forward mechanism. This is apparent in some deaf people. Deafness, as well as other, smaller deficiencies in hearing, can greatly affect one's ability to comprehend spoken language, as well as to speak it. However, if the person loses hearing ability later in life, most can still maintain a normal level of verbal intelligence. This is thought to be because of the brain's feed forward mechanism still helping to fix speech errors, even in the absence of auditory feedback.",
            "score": 115.81671905517578
        },
        {
            "docid": "3452485_18",
            "document": "Memory disorder . It has become clear that aging negatively affects brain function and this can encompass a decrease in locomotor activities and coordination as well as affect in a negative way learning and memory. Certain responses to stress within the hippocampus can have negative effects on learning. In a study done by Mark A. Smith, it is demonstrated that exposure to continuous stress can cause age-related issues to the hippocampus. What then becomes more noticeable is that the aging brain is not as able to recognize growth, this is a symptom of hippocampal damage. If the information is not being encoded properly in the brain then of course there would not be good memory retention without the consideration of outside implications. However, the consideration of anxiety, memory and overall function must be compromised. An emotional memory is capable of being embedded and then reused in a similar scenario at a later time if need be. Also noted within a study relating to age and anxiety and memory it was noted that lesions on the brain can affect spatial learning as well as sex presenting at a disadvantage. Dysfunction within the hippocampus can be a reason behind aging brain changes among the elderly. To sum up anxiety and memory and aging, it is useful to recognize a correlation between what anxiety can cause the body to do and how memories are then formed or not formed, and how the aging brain has enough difficulty on its own trying to perform recall tasks.",
            "score": 115.75704193115234
        },
        {
            "docid": "35073980_10",
            "document": "Self-reference effect . Like children, the continuous development of a self-concept is related to the development of self-referencing in individuals. The relationships formed with intimate others over the lifespan appear to have an effect on self-referencing in relation to memory. The extent to which we include others in our self-concept has been a topic of particular interest for social psychologists. Theories of intimacy and personal relationships might suggest that the self-reference effect is affected by the closeness of a relationship with the other used as a target. Some researchers define closeness as an extension of self into other and suggest that one's cognitive processes about a close other develop in a way so as to include that person as part of the self. Consistent with this idea, it has been demonstrated that the memorial advantage afforded to self-referenced material can be diminished or eliminated when the comparison target is an intimate other such as a parent, friend, or spouse The capacity for utilizing the self-reference effect remains relatively high throughout the lifespan, even well into old age. Normally functioning older adults can benefit from self-referencing. Ageing is marked by cognitive impairments in a number of domains including long-term memory, but older adults' memory performance is malleable. Memory strategies and orientations that engage \"deep\" encoding processes benefit older adults. For example, older adults exhibit increased recall when using self-generated strategies that rely on personally relevant information (e.g., important birthdates) relative to other mnemonic strategies However, research has shown that there are some differences between older adults and younger adults use of the self-reference advantage. Like young adults, older adults exhibit superior recognition for self-referenced items. But the amount of cognitive resources an individual has influence on how much older adults benefit from self referencing. Self-referencing improves older adult's memory, but its benefits are restricted regardless of the social and personally relevant nature of the task. A reason for this change in self referencing may be the change in brain activation that has been observed in older adults when studying self-referencing. Older adults showed more activity in the medial prefrontal cortex and along the cingulate gyrus than young adults. Because these regions often are associated with self-referential processing, these results suggest that older adults' mnemonic boost for positive information may stem from an increased tendency to process this information in relation to themselves. It has been proposed that this \"positivity shift\" may occur because older adults put more emphasis on emotion regulation goals than do young adults, with older adults having a greater motivation to derive emotional meaning from life and to maintain positive affect.",
            "score": 115.58853149414062
        },
        {
            "docid": "2917649_10",
            "document": "Speech . Speech perception refers to the processes by which humans can interpret and understand the sounds used in language. The study of speech perception is closely linked to the fields of phonetics and phonology in linguistics and cognitive psychology and perception in psychology. Research in speech perception seeks to understand how listeners recognize speech sounds and use this information to understand spoken language. Research into speech perception also has applications in building computer systems that can recognize speech, as well as improving speech recognition for hearing- and language-impaired listeners.",
            "score": 115.16173553466797
        }
    ]
}