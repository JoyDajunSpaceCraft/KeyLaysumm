{
    "q": [
        {
            "docid": "8402086_18",
            "document": "Computational neurogenetic modeling . Because the amount of data on the interplay of genes and neurons and their effects is not enough to construct a rigorous model,  evolutionary computation is used to optimize artificial neural networks and gene regulatory networks, a common technique being the genetic algorithm. A genetic algorithm is a process that can be used to refine models by mimicking the process of natural selection observed in biological ecosystems. The primary advantages are that, due to not requiring derivative information, it can be applied to black box problems and multimodal optimization. The typical process for using genetic algorithms to refine a gene  regulatory network is: first, create a population; next, to create offspring via a crossover operation and  evaluate their fitness; then, on a group chosen for high fitness, simulate mutation via a mutation operator;  finally, taking the now mutated group, repeat this process until a desired level of fitness is demonstrated. Methods by which artificial neural networks may alter their structure without simulated mutation and fitness selection have been developed. A dynamically evolving neural network is one approach, as the creation of new connections and new neurons can  be modeled as the system adapts to new data. This enables the network to evolve in modeling accuracy without simulated natural selection. One method by which dynamically evolving networks may be optimized, called evolving layer neuron aggregation, combines neurons with sufficiently similar input weights into one neuron. This can take place during the training of the network, referred to as online aggregation, or between periods of training, referred to as offline aggregation. Experiments have suggested that offline aggregation is more efficient.",
            "score": 147.23933815956116
        },
        {
            "docid": "41077350_3",
            "document": "OpenWorm . The roundworm \"C. elegans\" has one of the simplest nervous systems of any organism, with its hermaphrodite type having only 302 neurons. Furthermore, the structural connectome of these neurons is fully worked out. There are fewer than one thousand cells in the whole body of a \"C. elegans\" worm, each with a unique identifier and comprehensive supporting literature because \"C. elegans\" is a model organism. Being a model organism, the genome is fully known, along with many well characterized mutants readily available, a comprehensive literature of behavioural studies, etc. With so few neurons and new calcium 2 photon microscopy techniques it should soon be possible to record the complete neural activity of a living organism. By manipulating the neurons through optogenetic techniques, combined with the above recording capacities the project is in an unprecedented position to be able to fully characterize the neural dynamics of an entire organism.",
            "score": 107.96596884727478
        },
        {
            "docid": "21944_25",
            "document": "Nervous system . A neuron is called \"identified\" if it has properties that distinguish it from every other neuron in the same animal\u2014properties such as location, neurotransmitter, gene expression pattern, and connectivity\u2014and if every individual organism belonging to the same species has one and only one neuron with the same set of properties. In vertebrate nervous systems very few neurons are \"identified\" in this sense\u2014in humans, there are believed to be none\u2014but in simpler nervous systems, some or all neurons may be thus unique. In the roundworm \"C. elegans\", whose nervous system is the most thoroughly described of any animal's, every neuron in the body is uniquely identifiable, with the same location and the same connections in every individual worm. One notable consequence of this fact is that the form of the \"C. elegans\" nervous system is completely specified by the genome, with no experience-dependent plasticity.",
            "score": 107.50896763801575
        },
        {
            "docid": "25345530_43",
            "document": "Models of neural computation . This response is then fed as input into other neurons and so on. The goal is to optimize the weights of the neurons to output a desired response at the output layer respective to a set given inputs at the input layer. This optimization of the neuron weights is often performed using the backpropagation algorithm and an optimization method such as gradient descent or Newton's method of optimization. Backpropagation compares the output of the network with the expected output from the training data, then updates the weights of each neuron to minimize the contribution of that individual neuron to the total error of the network.",
            "score": 120.97185730934143
        },
        {
            "docid": "3717_69",
            "document": "Brain . Theorists have worked to understand these response patterns by constructing mathematical models of neurons and neural networks, which can be simulated using computers. Some useful models are abstract, focusing on the conceptual structure of neural algorithms rather than the details of how they are implemented in the brain; other models attempt to incorporate data about the biophysical properties of real neurons. No model on any level is yet considered to be a fully valid description of brain function, though. The essential difficulty is that sophisticated computation by neural networks requires distributed processing in which hundreds or thousands of neurons work cooperatively\u2014current methods of brain activity recording are only capable of isolating action potentials from a few dozen neurons at a time.",
            "score": 164.44435238838196
        },
        {
            "docid": "941909_26",
            "document": "Receptive field . The term receptive field is also used in the context of artificial neural networks, most often in relation to convolutional neural networks (CNNs). When used in this sense, the term adopts a meaning reminiscent of receptive fields in actual biological nervous systems. CNNs have a distinct architecture, designed to mimic the way in which real animal brains are understood to function; instead of having every neuron in each layer connect to all neurons in the next layer (Multilayer perceptron), the neurons are arranged in a 3-dimensional structure in such a way as to take into account the spatial relationships between different neurons with respect to the original data. Since CNNs are used primarily in the field of computer vision, the data that the neurons represent is typically an image; each input neuron represents one pixel from the original image. The first layer of neurons is composed of all the input neurons; neurons in the next layer will receive connections from some of the input neurons (pixels), but not all, as would be the case in a MLP and in other traditional neural networks. Hence, instead of having each neuron receive connections from all neurons in the previous layer, CNNs use a receptive field-like layout in which each neuron receives connections only from a subset of neurons in the previous (lower) layer. The receptive field of a neuron in one of the lower layers encompasses only a small area of the image, while the receptive field of a neuron in subsequent (higher) layers involves a combination of receptive fields from several (but not all) neurons in the layer before (i. e. a neuron in a higher layer \"looks\" at a larger portion of the image than does a neuron in a lower layer). In this way, each successive layer is capable of learning increasingly abstract features of the original image. The use of receptive fields in this fashion is thought to give CNNs an advantage in recognizing visual patterns when compared to other types of neural networks.",
            "score": 140.46620917320251
        },
        {
            "docid": "34060396_6",
            "document": "Kwabena Boahen . Boahen's work has demonstrated that neuromorphic computer chips are capable of reproducing many types of brain phenomena across a large range of scales. Examples include ion-channel dynamics (individual molecules), excitable membrane behavior (individual neurons), the orientation tuning of neurons in Visual Cortex (individual cortical columns), and neural synchrony (individual cortical areas). Utilizing these breakthroughs, Boahen's Stanford lab built the first neurmorphic system with one million spiking neurons (and billions of synapses). This system, \"Neurogrid\", emulates networks of cortical neurons in real time, while consuming only a few watts of power. In contrast, simulating one million interconnected cortical neurons in real-time using traditional super-computers requires as much power as several thousand households.",
            "score": 128.84856152534485
        },
        {
            "docid": "14048614_2",
            "document": "Brainbow . Brainbow is a process by which individual neurons in the brain can be distinguished from neighboring neurons using fluorescent proteins. By randomly expressing different ratios of red, green, and blue derivatives of green fluorescent protein in individual neurons, it is possible to flag each neuron with a distinctive color. This process has been a major contribution to the field of connectomics, or the study of neural connections in the brain. The study of neural pathways is also known as hodology by earlier neuroanatomists.",
            "score": 117.66210794448853
        },
        {
            "docid": "14048614_17",
            "document": "Brainbow . The complexity of the Drosophila brain, consisting of about 100,000 neurons, makes it an excellent candidate for implementing neurophysiology and neuroscience techniques like Brainbow. In fact, Stefanie Hampel et al. (2011) combined Brainbow in conjunction with genetic targeting tools to identify individual neurons within the Drosophila brain and various neuronal lineages. One of the genetic targeting tools was a GAL4/UAS binary expression system that controls the expression of UAS-Brainbow and targets the expression to small groups of neurons. Utilizing \u2018Flip Out\u2019 methods increased the cellular resolution of the reporter construct. The expression of fluorescent proteins, as with the original Brainbow, depended on Cre recombination corresponding with matched lox sites. Hampel et al. (2011) also developed their own variation of Brainbow (dBrainbow), based on antibody labeling of epitopes rather than endogenous fluorescence. Two copies of their construct yield six bright, separable colors. This, along with simplifications in color assignment, enabled them to observe the trajectories of each neuron over long distances. Specifically, they traced motor neurons from the antennal lobe to neuromuscular junctions, allowing them to identify the specific muscle targets of individual neurons.",
            "score": 114.30214250087738
        },
        {
            "docid": "42069970_6",
            "document": "Neuronal lineage marker . The study of the nervous system dates back to ancient Egypt but only in the ninetieth century it became more detailed. With the invention of the microscope and a technique of staining developed by Camillo Golgi, it was possible to study individual neurons. This scientist started to impregnate nervous tissue with metal, as silver. The reaction consists in fixing particles of silver chromate to the neurilemma, and resulted in a stark black deposit in the soma, axon and dendrites of the neuron. Thus, it was possible to identify different types of neurons, as Golgi Cell, Golgi I and Golgi II. In 1885 there was a German medical researcher called Franz Nissl who developed another staining technique now known by Nissl staining. This technique is slightly different from Golgi staining since it stains the cell body and the endoplasmic reticulum. In 1887, a Spanish scientist called Santiago Ramon y Cajal learned the staining technique with Golgi and started his famous work of neuroanatomy. With this technique he made an extensive study of several areas of the brain and in different species. He also described very precisely the purkinje cells, the chick cerebellum and the neuronal circuit of the rodent hippocampus. In 1941 Dr. Albert Coons used for the first time a revolutionary technique that uses the principle of antibodies binding specifically to antigens in the tissues. He created an immunoflorescent technique for labelling the antibodies. This technique continues to be widely used in neuroscience studies for identifying different structures. The most important neural markers used nowadays are the GFAP, Nestin, NeuroD antibodies and others. For the past years there are still creating new neural markers for immunocytochemistry or/and immunohistochemistry. In 1953 Heinrich Kl\u00fcver invented a new staining technique called, Luxol Fast Blue stain or LFB, and with this technique it\u2019s possible to detect demyelination in the central nervous system. Myelin sheath will be stained blue, but other structures will be stained as well. The next revolutionary technique was invented in 1969 by an American scientist called Joseph G. Gall. This technique is called in situ Hybridization and it is used in a large variety of studies but mainly used in developmental biology. With this technique it is possible to mark some genes expressed in determined areas of the animal. In neurobiology, it's very useful for understanding the formation of the nervous system.",
            "score": 118.18456816673279
        },
        {
            "docid": "337196_29",
            "document": "Neuroanatomy . The brain is small and simple in some species, such as the nematode worm, where the body plan is quite simple: a tube with a hollow gut cavity running from the mouth to the anus, and a nerve cord with an enlargement (a ganglion) for each body segment, with an especially large ganglion at the front, called the brain. The nematode \"Caenorhabditis elegans\" has been studied because of its importance in genetics. In the early 1970s, Sydney Brenner chose it as a model system for studying the way that genes control development, including neuronal development. One advantage of working with this worm is that the nervous system of the hermaphrodite contains exactly 302 neurons, always in the same places, making identical synaptic connections in every worm. Brenner's team sliced worms into thousands of ultrathin sections and photographed every section under an electron microscope, then visually matched fibers from section to section, to map out every neuron and synapse in the entire body, to give a complete connectome of the nematode. Nothing approaching this level of detail is available for any other organism, and the information has been used to enable a multitude of studies that would not have been possible without it.",
            "score": 107.37500762939453
        },
        {
            "docid": "33826069_2",
            "document": "Viral neuronal tracing . Viral neuronal tracing is the use of a virus to trace neural pathways, providing a self-replicating tracer. Viruses have the advantage of self replication over molecular tracers, but can also spread too quickly and cause degradation of neural tissue. Viruses which can infect the nervous system, called Neurotropic viruses, spread through spatially close assemblies of neurons through synapses, allowing for their use in studying functionally connected neural networks. The use of viruses to label functionally connected neurons stems from work done by Albert Sabin who developed a bioassay which could assess the infection of viruses across neurons. Subsequent research allowed for incorporation of Immunohistochemical techniques to systematically label neuronal connections. To date, viruses have been used to study multiple circuits in the nervous system.",
            "score": 103.15317034721375
        },
        {
            "docid": "1164_53",
            "document": "Artificial intelligence . Neural networks, or neural nets, were inspired by the architecture of neurons in the human brain. A simple \"neuron\" \"N\" accepts input from multiple other neurons, each of which, when activated (or \"fired\"), cast a weighted \"vote\" for or against whether neuron \"N\" should itself activate. Learning requires an algorithm to adjust these weights based on the training data; one simple algorithm (dubbed \"fire together, wire together\") is to increase the weight between two connected neurons when the activation of one triggers the successful activation of another. The net forms \"concepts\" that are distributed among a subnetwork of shared neurons that tend to fire together; a concept meaning \"leg\" might be coupled with a subnetwork meaning \"foot\" that includes the sound for \"foot\". Neurons have a continuous spectrum of activation; in addition, neurons can process inputs in a nonlinear way rather than weighing straightforward votes. Modern neural nets can learn both continuous functions and, surprisingly, digital logical operations. Neural networks' early successes included predicting the stock market and (in 1995) a mostly self-driving car. In the 2010s, advances in neural networks using deep learning thrust AI into widespread public consciousness and contributed to an enormous upshift in corporate AI spending; for example, AI-related M&A in 2017 was over 25 times as large as in 2015.",
            "score": 146.10753691196442
        },
        {
            "docid": "21120_49",
            "document": "Neuron . The number of neurons in the brain varies dramatically from species to species. The adult human brain contains about 85-86 billion neurons, of which 16.3 billion are in the cerebral cortex and 69 billion in the cerebellum. By contrast, the nematode worm \"Caenorhabditis elegans\" has just 302 neurons, making it an ideal experimental subject as scientists have been able to map all of the organism's neurons. The fruit fly \"Drosophila melanogaster\", a common subject in biological experiments, has around 100,000 neurons and exhibits many complex behaviors. Many properties of neurons, from the type of neurotransmitters used to ion channel composition, are maintained across species, allowing scientists to study processes occurring in more complex organisms in much simpler experimental systems.",
            "score": 123.76933455467224
        },
        {
            "docid": "3474296_6",
            "document": "Neuronal noise . Another theory suggests that stochastic noise in a non-linear network shows a positive relationship between the interconnectivity and noise-like activity. Thus based on this theory, Patrick Wilken and colleagues suggest that neuronal noise is the principal factor that limits the capacity of visual short-term memory. Investigators of neural ensembles and those who especially support the theory of distributed processing, propose that large neuronal populations effectively decrease noise by averaging out the noise in individual neurons. Some investigators have shown in experiments and in models that neuronal noise is a possible mechanism to facilitate neuronal processing. The presence of neuronal noise (or more specifically synaptic noise) confers to neurons more sensitivity to a broader range of inputs, it can equalize the efficacy of synaptic inputs located at different positions on the neuron, and it can also enable finer temporal discrimination. There are many theories of why noise is apparent in the neuronal networks, but many neurologists are unclear of why they exist.",
            "score": 114.12047588825226
        },
        {
            "docid": "10459803_13",
            "document": "Neuron (software) . Neuron comes equipped with a slew of simulation tools. Most notably, it includes several \"point processes,\" which are simple functions at a particular segment of a cell. Point processes include simulations of voltage, patch, single electrode and current clamps, as well as several simulated synapses. Synapse point processes are distinct for their ability to model stimulation intensities that vary non-linearly across time. These can be placed on any segment of any section of a built cell, individual or network, and their precise values, including amplitude and duration of stimulation, delay time of activation in a run and time decay parameters (for synapses), can be defined from the point process manager module. When implemented into a network as synapses, point process parameters are defined in the synapse builder for a particular network cell. Graphs describing voltage, conductance, and current axes over time can be used to describe changes in electrical state at the location of any segment on the cell. Neuron allows for graphs of change at both individual points over time, and across an entire section through time. Duration of run can be set. All point processes, including those standing for cells or synapses of artificial neurons, and all graphs reflect the duration.",
            "score": 114.9312961101532
        },
        {
            "docid": "13793747_18",
            "document": "Group method of data handling . For modeling using GMDH, first, the number of inputs for each neuron, polynomial power and input sources of layers after the first layer are decided. Then, the design process begins from the first layer and goes on. All possible combinations of allowable inputs (all possible neurons) are considered. Then polynomial coefficients are determined using one of the available minimizing methods such as singular value decom-position (with training data). Then, neurons that have better external criterion (for testing data) are kept, and others are removed (The input data for development of the model were divided into training and testing groups). If the external criterion for layer\u2019s best neuron surpasses the stopping criterion, network design is completed and the polynomial expression of the best neuron of the last layer is introduced as the math-ematical prediction function; if not, the next layer will be generated, and this process goes on .",
            "score": 95.02302193641663
        },
        {
            "docid": "4833512_8",
            "document": "Mu wave . Based on findings correlating mirror neuron activity and mu wave suppression in individuals with autism as in typically developing individuals, studies have examined both the development of mirror neurons and therapeutic means for stimulating the system. A recent study has found that fMRI activation magnitudes in the inferior frontal gyrus increase with age in people with autism. This finding was not apparent in typically developing individuals. Furthermore, greater activation was associated with greater amounts of eye contact and better social functioning. Scientists believe the inferior frontal gyrus is one of the main neural correlates with the mirror neuron system in humans and is often related to deficits associated with autism. These findings suggest that the mirror neuron system may not be non-functional in individuals with autism, but simply abnormal in its development. This information is significant to the present discussion because mu waves may be integrating different areas of mirror neuron activity in the brain. Other studies have assessed attempts to consciously stimulate the mirror neuron system and suppress mu waves using neurofeedback (a type of biofeedback given through computers that analyze real time recordings of brain activity, in this case EEGs of mu waves). This type of therapy is still in its early phases of implementation for individuals with autism, and has conflicting forecasts for success.",
            "score": 122.90103697776794
        },
        {
            "docid": "2567511_17",
            "document": "Neural engineering . Scientists can use experimental observations of neuronal systems and theoretical and computational models of these systems to create Neural networks with the hopes of modeling neural systems in as realistic a manner as possible. Neural networks can be used for analyses to help design further neurotechnological devices. Specifically, researchers handle analytical or finite element modeling to determine nervous system control of movements and apply these techniques to help patients with brain injuries or disorders. Artificial neural networks can be built from theoretical and computational models and implemented on computers from theoretically devices equations or experimental results of observed behavior of neuronal systems. Models might represent ion concentration dynamics, channel kinetics, synaptic transmission, single neuron computation, oxygen metabolism, or application of dynamic system theory (LaPlaca et al. 2005). Liquid-based template assembly was used to engineer 3D neural networks from neuron-seeded microcarrier beads.",
            "score": 146.40291357040405
        },
        {
            "docid": "2860430_2",
            "document": "Neural oscillation . Neural oscillations, or brainwaves, are rhythmic or repetitive patterns of neural activity in the central nervous system. Neural tissue can generate oscillatory activity in many ways, driven either by mechanisms within individual neurons or by interactions between neurons. In individual neurons, oscillations can appear either as oscillations in membrane potential or as rhythmic patterns of action potentials, which then produce oscillatory activation of post-synaptic neurons. At the level of neural ensembles, synchronized activity of large numbers of neurons can give rise to macroscopic oscillations, which can be observed in an electroencephalogram. Oscillatory activity in groups of neurons generally arises from feedback connections between the neurons that result in the synchronization of their firing patterns. The interaction between neurons can give rise to oscillations at a different frequency than the firing frequency of individual neurons. A well-known example of macroscopic neural oscillations is alpha activity.",
            "score": 113.80282843112946
        },
        {
            "docid": "4778800_27",
            "document": "Gunfire locator . Another method of classifying gunfire uses \"temporal pattern recognition,\" as referred by its developer, that employs artificial neural networks that are trained and then listen for a sound signature in acoustic events. Like other acoustic sensing systems they are fundamentally based on the physics of acoustics, but they analyze the physical acoustic data using a neural network. Information in the network is coded in terms of variation in the sequence of all-or-none (spike) events, or temporal patterns, transmitted between artificial \"neurons\". Identifying the nonlinear input/output properties of neurons involved in forming memories for new patterns and developing mathematical models of those nonlinear properties enable the identification of specific types of sounds. These neural networks can then be trained as \"recognizers\" of a target sound, like a gunshot, even in the presence of high noise.",
            "score": 104.79316186904907
        },
        {
            "docid": "23393127_2",
            "document": "Sholl analysis . Sholl analysis is a method of quantitative analysis commonly used in neuronal studies to characterize the morphological characteristics of an imaged neuron, first used to describe the differences in the visual and motor cortices of cats. Sholl was interested in comparing the morphology of different types of neurons, i.e. stellate and pyramidal, and of different locations in the dendritic field of the same type of neurons, i.e. basal and apical processes of the pyramidal neuron. He looked at dendritic length and diameter (Sholl, p.389, Fig. 1) and also the number of cells per volume (Sholl, p. 401, \"The packing density of perikarya\"). While methods for estimating number of cells have vastly improved since 1953 with the advent of unbiased stereology, the method Sholl uses to record the number of intersections at various distances from the cell body is still in use and is actually named after Sholl. \"In order to study the way in which the number of branches varies with the distance from the perikaryon, it is convenient to use a series of concentric spherical shells as co-ordinates of reference. ... these shells have their common centre in the perikaryon\" (Sholl, p. 392, \"The manner of dendritic branching\"). What Sholl called the 'Concentric Shell Method' is now known as 'Sholl Analysis'. As well as the number of intersections per concentric shell, Sholl also calculated the mean diameter of the dendrites or axons within each concentric shell (Sholl, p. 396, table 2 and 3). Sholl appreciated that his method is good for comparing neurons, for instance in figure 8 the differences in the number of dendritic intersections correlated with distance from the cell body is compared between neurons from the motor and visual cortex. Sholl also realized his method is useful to determine where and how big is the region where synapses are possible, something he called the neuron's 'connective zone' (Sholl, p. 402, \"The connective zone of a neuron\"). In 1953, Sholl was working with projections of 3-D neurons into two-dimensions, but now Sholl analysis can be done on 3-D images (e.g. image stacks or 3-D montages) of neurons, making the concentric circles truly three-dimensional shells. In addition to intersections and diameter: total dendritic length, surface area, and volume of processes per shell; number of nodes, endings, varicosities and spines per shell; and branching order of the dendrites in each shell, can be included in the analysis. For modern examples of the use of Sholl analysis to analyze neurons, please see (O'Neill, et al., 2015) or (Chowhudry, et al., 2015). Curves produced from the 'number of intersections vs. distance from the cell body data' are usually of somewhat irregular shape, and much work has been done to determine appropriate means of analyzing the results. Common methods include Linear Analysis, Semi-log Analysis and Log-Log Analysis",
            "score": 112.3975625038147
        },
        {
            "docid": "38567205_7",
            "document": "BRAIN Initiative . In a 2012 scientific commentary outlining experimental plans for a more limited project, Alivisatos \"et al.\" outlined a variety of specific experimental techniques that might be used to achieve what they termed a \"functional connectome\", as well as new technologies that will have to be developed in the course of the project. They indicated that initial studies might be done in \"Caenorhabditis elegans\", followed by \"Drosophila\", because of their comparatively simple neural circuits. Mid-term studies could be done in zebrafish, mice, and the Etruscan shrew, with studies ultimately to be done in primates and humans. They proposed the development of nanoparticles that could be used as voltage sensors that would detect individual action potentials, as well as nanoprobes that could serve as electrophysiological multielectrode arrays. In particular, they called for the use of wireless, noninvasive methods of neuronal activity detection, either utilizing microelectronic very-large-scale integration, or based on synthetic biology rather than microelectronics. In one such proposed method, enzymatically produced DNA would serve as a \"ticker tape record\" of neuronal activity, based on calcium ion-induced errors in coding by DNA polymerase. Data would be analyzed and modeled by large scale computation. A related technique proposed the use of high-throughput DNA sequencing for rapidly mapping neural connectivity.",
            "score": 129.8017725944519
        },
        {
            "docid": "26421895_2",
            "document": "Retrograde tracing . Retrograde tracing is a research method used in neuroscience to trace neural connections from their point of termination (the synapse) to their source (the cell body). Retrograde tracing techniques allow for detailed assessment of neuronal connections between a target population of neurons and their inputs throughout the nervous system. These techniques allow the \"mapping\" of connections between neurons in a particular structure (e.g. the eye) and the target neurons in the brain. The opposite technique is anterograde tracing, which is used to trace neural connections from their source to their point of termination (i.e. from cell body to synapse). Both the anterograde and retrograde tracing techniques are based on the visualization of axonal transport.",
            "score": 109.70171427726746
        },
        {
            "docid": "2860430_20",
            "document": "Neural oscillation . Computational models adopt a variety of abstractions in order to describe complex oscillatory dynamics observed in brain activity. Many models are used in the field, each defined at a different level of abstraction and trying to model different aspects of neural systems. They range from models of the short-term behaviour of individual neurons, through models of how the dynamics of neural circuitry arise from interactions between individual neurons, to models of how behaviour can arise from abstract neural modules that represent complete subsystems.",
            "score": 112.627286195755
        },
        {
            "docid": "1648224_20",
            "document": "Granger causality . Neural spike train data can be modeled as a point-process. A temporal point process is a stochastic time-series of binary events that occurs in continuous time. It can only take on two values at each point in time, indicating whether or not an event has actually occurred. This type of binary-valued representation of information suits the activity of neural populations because a single neuron\u2019s action potential has a typical waveform. In this way, what carries the actual information being output from a neuron is the occurrence of a \u201cspike\u201d, as well as the time between successive spikes. Using this approach one could abstract the flow of information in a neural-network to be simply the spiking times for each neuron through an observation period. A point-process can be represented either by the timing of the spikes themselves, the waiting times between spikes, using a counting process, or, if time is discretized enough to ensure that in each window only one event has the possibility of occurring, that is to say one time bin can only contain one event, as a set of 1s and 0s, very similar to binary.",
            "score": 96.97587394714355
        },
        {
            "docid": "21523_125",
            "document": "Artificial neural network . Many types of models are used, defined at different levels of abstraction and modeling different aspects of neural systems. They range from models of the short-term behavior of individual neurons, models of how the dynamics of neural circuitry arise from interactions between individual neurons and finally to models of how behavior can arise from abstract neural modules that represent complete subsystems. These include models of the long-term, and short-term plasticity, of neural systems and their relations to learning and memory from the individual neuron to the system level.",
            "score": 116.26965570449829
        },
        {
            "docid": "33644895_11",
            "document": "Evolution of nervous systems . The nervous system of one very small worm, the roundworm \"Caenorhabditis elegans\", has been mapped out down to the synaptic level. Every neuron and its cellular lineage has been recorded and most, if not all, of the neural connections are known. In this species, the nervous system is sexually dimorphic; the nervous systems of the two sexes, males and hermaphrodites, have different numbers of neurons and groups of neurons that perform sex-specific functions. In \"C. elegans\", males have exactly 383 neurons, while hermaphrodites have exactly 302 neurons.",
            "score": 107.41831135749817
        },
        {
            "docid": "27075922_9",
            "document": "Natural computing . An artificial neural network is a network of artificial neurons.  An artificial neuron \"A\" is equipped with a function formula_1, receives \"n\" real-valued inputs formula_2 with respective weights formula_3, and it outputs formula_4. Some neurons are selected to be the output neurons, and the network function is the vectorial function that associates to the \"n\" input values, the outputs of the \"m\" selected output neurons. Note that different choices of weights produce different network functions for the same inputs. Back-propagation is a supervised learning method by which the weights of the connections in the network are repeatedly adjusted so as to minimize the difference between the vector of actual outputs and that of desired outputs. Learning algorithms based on backwards propagation of errors can be used to find optimal weights for given topology of the network and input-output pairs.",
            "score": 133.72468662261963
        },
        {
            "docid": "33246145_9",
            "document": "Neural decoding . Previous recording methods relied on stimulating single neurons over a repeated series of tests in order to generalize this neuron's behavior. New techniques such as high-density multi-electrode array recordings and multi-photon calcium imaging techniques now make it possible to record from upwards of a few hundred neurons. Even with better recording techniques, the focus of these recordings must be on an area of the brain that is both manageable and qualitatively understood. Many studies look at spike train data gathered from the ganglion cells in the retina, since this area has the benefits of being strictly feedforward, retinotopic, and amenable to current recording granularities. The duration, intensity, and location of the stimulus can be controlled to sample, for example, a particular subset of ganglion cells within a structure of the visual system. Other studies use spike trains to evaluate the discriminatory ability of non-visual senses such as rat facial whiskers and the olfactory coding of moth pheromone receptor neurons.",
            "score": 111.31147563457489
        },
        {
            "docid": "271430_14",
            "document": "Computational neuroscience . Modeling the richness of biophysical properties on the single-neuron scale can supply mechanisms that serve as the building blocks for network dynamics. However, detailed neuron descriptions are computationally expensive and this can handicap the pursuit of realistic network investigations, where many neurons need to be simulated. As a result, researchers that study large neural circuits typically represent each neuron and synapse with an artificially simple model, ignoring much of the biological detail. Hence there is a drive to produce simplified neuron models that can retain significant biological fidelity at a low computational overhead. Algorithms have been developed to produce faithful, faster running, simplified surrogate neuron models from computationally expensive, detailed neuron models.",
            "score": 125.27726221084595
        },
        {
            "docid": "3442072_3",
            "document": "Spike sorting . Neurons produce action potentials that are referred to as 'spikes' in laboratory jargon. Frequently this term is used for electrical signals recorded in the vicinity of individual neurons with a microelectrode (exception: 'spikes' in EEG recordings). In these recordings action potentials appear as sharp spikes (deviations from the baseline). These extracellular electrodes pick up all the components constituting the field at the point of its contact. This includes the component due to the synaptic currents and the action potentials. The synaptic currents have slower time course and the spikes have faster time course. They are thus easily separated by filtering: highpass for spikes and low pass for the synaptic mechanisms. The component of the field due to the synaptic mechanism is referred to as the local field potential (LFP). Spike sorting refers to the process of assigning spikes to different neurons. The background to this is that the exact time course of a spike event as recorded by the electrode depends on the size and shape of the neuron, the position of the recording electrode relative to the neuron, etc. These electrodes, positioned outside of the cells in the tissue, however, often 'see' the spikes generated by several neurons in their vicinity. Since the spike shapes are unique and quite reproducible for each neuron it can be used to distinguish spikes produced by different neurons, i.e. to separate the activity produced by each.",
            "score": 98.44616913795471
        }
    ],
    "r": [
        {
            "docid": "3717_69",
            "document": "Brain . Theorists have worked to understand these response patterns by constructing mathematical models of neurons and neural networks, which can be simulated using computers. Some useful models are abstract, focusing on the conceptual structure of neural algorithms rather than the details of how they are implemented in the brain; other models attempt to incorporate data about the biophysical properties of real neurons. No model on any level is yet considered to be a fully valid description of brain function, though. The essential difficulty is that sophisticated computation by neural networks requires distributed processing in which hundreds or thousands of neurons work cooperatively\u2014current methods of brain activity recording are only capable of isolating action potentials from a few dozen neurons at a time.",
            "score": 164.44435119628906
        },
        {
            "docid": "8402086_18",
            "document": "Computational neurogenetic modeling . Because the amount of data on the interplay of genes and neurons and their effects is not enough to construct a rigorous model,  evolutionary computation is used to optimize artificial neural networks and gene regulatory networks, a common technique being the genetic algorithm. A genetic algorithm is a process that can be used to refine models by mimicking the process of natural selection observed in biological ecosystems. The primary advantages are that, due to not requiring derivative information, it can be applied to black box problems and multimodal optimization. The typical process for using genetic algorithms to refine a gene  regulatory network is: first, create a population; next, to create offspring via a crossover operation and  evaluate their fitness; then, on a group chosen for high fitness, simulate mutation via a mutation operator;  finally, taking the now mutated group, repeat this process until a desired level of fitness is demonstrated. Methods by which artificial neural networks may alter their structure without simulated mutation and fitness selection have been developed. A dynamically evolving neural network is one approach, as the creation of new connections and new neurons can  be modeled as the system adapts to new data. This enables the network to evolve in modeling accuracy without simulated natural selection. One method by which dynamically evolving networks may be optimized, called evolving layer neuron aggregation, combines neurons with sufficiently similar input weights into one neuron. This can take place during the training of the network, referred to as online aggregation, or between periods of training, referred to as offline aggregation. Experiments have suggested that offline aggregation is more efficient.",
            "score": 147.2393341064453
        },
        {
            "docid": "2567511_17",
            "document": "Neural engineering . Scientists can use experimental observations of neuronal systems and theoretical and computational models of these systems to create Neural networks with the hopes of modeling neural systems in as realistic a manner as possible. Neural networks can be used for analyses to help design further neurotechnological devices. Specifically, researchers handle analytical or finite element modeling to determine nervous system control of movements and apply these techniques to help patients with brain injuries or disorders. Artificial neural networks can be built from theoretical and computational models and implemented on computers from theoretically devices equations or experimental results of observed behavior of neuronal systems. Models might represent ion concentration dynamics, channel kinetics, synaptic transmission, single neuron computation, oxygen metabolism, or application of dynamic system theory (LaPlaca et al. 2005). Liquid-based template assembly was used to engineer 3D neural networks from neuron-seeded microcarrier beads.",
            "score": 146.4029083251953
        },
        {
            "docid": "1164_53",
            "document": "Artificial intelligence . Neural networks, or neural nets, were inspired by the architecture of neurons in the human brain. A simple \"neuron\" \"N\" accepts input from multiple other neurons, each of which, when activated (or \"fired\"), cast a weighted \"vote\" for or against whether neuron \"N\" should itself activate. Learning requires an algorithm to adjust these weights based on the training data; one simple algorithm (dubbed \"fire together, wire together\") is to increase the weight between two connected neurons when the activation of one triggers the successful activation of another. The net forms \"concepts\" that are distributed among a subnetwork of shared neurons that tend to fire together; a concept meaning \"leg\" might be coupled with a subnetwork meaning \"foot\" that includes the sound for \"foot\". Neurons have a continuous spectrum of activation; in addition, neurons can process inputs in a nonlinear way rather than weighing straightforward votes. Modern neural nets can learn both continuous functions and, surprisingly, digital logical operations. Neural networks' early successes included predicting the stock market and (in 1995) a mostly self-driving car. In the 2010s, advances in neural networks using deep learning thrust AI into widespread public consciousness and contributed to an enormous upshift in corporate AI spending; for example, AI-related M&A in 2017 was over 25 times as large as in 2015.",
            "score": 146.10752868652344
        },
        {
            "docid": "941909_26",
            "document": "Receptive field . The term receptive field is also used in the context of artificial neural networks, most often in relation to convolutional neural networks (CNNs). When used in this sense, the term adopts a meaning reminiscent of receptive fields in actual biological nervous systems. CNNs have a distinct architecture, designed to mimic the way in which real animal brains are understood to function; instead of having every neuron in each layer connect to all neurons in the next layer (Multilayer perceptron), the neurons are arranged in a 3-dimensional structure in such a way as to take into account the spatial relationships between different neurons with respect to the original data. Since CNNs are used primarily in the field of computer vision, the data that the neurons represent is typically an image; each input neuron represents one pixel from the original image. The first layer of neurons is composed of all the input neurons; neurons in the next layer will receive connections from some of the input neurons (pixels), but not all, as would be the case in a MLP and in other traditional neural networks. Hence, instead of having each neuron receive connections from all neurons in the previous layer, CNNs use a receptive field-like layout in which each neuron receives connections only from a subset of neurons in the previous (lower) layer. The receptive field of a neuron in one of the lower layers encompasses only a small area of the image, while the receptive field of a neuron in subsequent (higher) layers involves a combination of receptive fields from several (but not all) neurons in the layer before (i. e. a neuron in a higher layer \"looks\" at a larger portion of the image than does a neuron in a lower layer). In this way, each successive layer is capable of learning increasingly abstract features of the original image. The use of receptive fields in this fashion is thought to give CNNs an advantage in recognizing visual patterns when compared to other types of neural networks.",
            "score": 140.46621704101562
        },
        {
            "docid": "21855574_5",
            "document": "Brain simulation . The connectivity of the neural circuit for touch sensitivity of the simple C. elegans nematode (roundworm) was mapped in 1985 and partly simulated in 1993. Since 2004, many software simulations of the complete neural and muscular system have been developed, including simulation of the worm's physical environment. Some of these models have been made available for download. However, there is still a lack of understanding of how the neurons and the connections between them generate the surprisingly complex range of behaviors that are observed in the relatively simple organism. This contrast between the apparent simplicity of how the mapped neurons interact with their neighbours, and exceeding complexity of the overall brain function, is an example of an emergent property. Interestingly, this kind of emergent property is paralleled within artificial neural networks, the neurons of which are exceedingly simple compared to their often complex, abstract outputs.",
            "score": 140.34893798828125
        },
        {
            "docid": "15978180_16",
            "document": "Connectome . Mapping the connectome at the \"microscale\" (micrometer resolution) means building a complete map of the neural systems, neuron-by-neuron. The challenge of doing this becomes obvious: the number of neurons comprising the brain easily ranges into the billions in more highly evolved organisms. The human cerebral cortex alone contains on the order of 10 neurons linked by 10 synaptic connections. By comparison, the number of base-pairs in a human genome is 3\u00d710. A few of the main challenges of building a human connectome at the microscale today include: (1) data collection would take years given current technology; (2) machine vision tools to annotate the data remain in their infancy, and are inadequate; and (3) neither theory nor algorithms are readily available for the analysis of the resulting \"brain-graphs\". To address the data collection issues, several groups are building high-throughput serial electron microscopes (Kasthuri \"et al.\", 2009; Bock \"et al\". 2011). To address the machine-vision and image-processing issues, the Open Connectome Project is \"alg-sourcing\" (algorithm outsourcing) this hurdle. Finally, statistical graph theory is an emerging discipline which is developing sophisticated pattern recognition and inference tools to parse these brain-graphs (Goldenberg \"et al.\", 2009).",
            "score": 138.7345428466797
        },
        {
            "docid": "3717_60",
            "document": "Brain . Computational neuroscience encompasses two approaches: first, the use of computers to study the brain; second, the study of how brains perform computation. On one hand, it is possible to write a computer program to simulate the operation of a group of neurons by making use of systems of equations that describe their electrochemical activity; such simulations are known as \"biologically realistic neural networks\". On the other hand, it is possible to study algorithms for neural computation by simulating, or mathematically analyzing, the operations of simplified \"units\" that have some of the properties of neurons but abstract out much of their biological complexity. The computational functions of the brain are studied both by computer scientists and neuroscientists.",
            "score": 135.75437927246094
        },
        {
            "docid": "2367309_8",
            "document": "State-dependent memory . At its most basic, state-dependent memory is the product of the strengthening of a particular synaptic pathway in the brain. A neural synapse is the space between brain cells, or neurons, that allows chemical signals to be passed from one neuron to another. Chemicals called neurotransmitters leave one cell, travel across the synapse, and are taken in by the next neuron through a neurotransmitter receptor. This creates a connection between the two neurons called a neural pathway. Memory relies on the strengthening of these neural pathways, associating one neuron with another. When we learn something, new pathways are made between neurons in the brain which then communicate through chemical signals. If these cells have a history of sending out certain signals under specific chemical conditions within the brain, they are then primed to work most effectively under similar circumstances. State-dependent memory happens when a new neural connection is made while the brain is in a specific chemical state - for instance, a child with ADHD learns their multiplication tables while on stimulant medication. Because their brain created these new connections related to multiplication tables while the brain was chemically affected by the stimulant medication, their neurons will be primed in the future to remember these facts best when the same levels of medication are present in the brain.",
            "score": 135.67738342285156
        },
        {
            "docid": "43278016_12",
            "document": "Joe Z. Tsien . In 2015, Tsien developed the \"Theory of Connectivity\" to explain the design principle upon which evolution and development may construct the brain to be capable of generating intelligence. This theory has made six predictions which have received supportive evidence by a recent set of experiments on both the mouse brain and hamster brain. At its core, the \"Theory of Connectivity\" predicts that the cell assemblies in the brain are not random, rather they should conform to the power-of-two-based equation, N = 2 - 1, to form the pre-configured building block termed as the functional connectivity motif (FCM). Instead of using a single neuron as the computational unit in some extremely simple brains, the theory denotes that in most brains, a group of neurons exhibiting similar tuning properties, termed as a neural clique, should serve as the basic computing processing unit (CPU). Defined by the power-of-two-based equation, N = 2 - 1, each FCM consists of principal-projection neuron cliques (N), ranging from those specific cliques receiving specific information inputs (i) to those general and sub-general cliques receiving various combinatorial convergent inputs. As the evolutionarily conserved logic, its validation requires experimental demonstrations of the following three major properties: 1) Anatomical prevalence - FCMs are prevalent across neural circuits, regardless of gross anatomical shapes; 2) Species conservancy - FCMs are conserved across different animal species; and 3) Cognitive universality - FCMs serve as a universal computational logic at the cell-assembly level for processing a variety of cognitive experiences and flexible behaviors. More importantly, this \"Theory of Connectivity\" further predicts that the specific-to-general combinatorial connectivity pattern within FCMs should be pre-configured by evolution, and emerge innately from development as the brain's computational primitives. This proposed design principle can also explain the general purpose and computational algorithm of the neocortex. This proposed design principle of intelligence can be examined via various experiments and also be modeled by neuromorphic engineers and computer scientists. However, Dr. Joe Tsien cautions that artificial general intelligence based on the brain's principles can come with great benefits and, potentially, even greater risks.",
            "score": 134.61151123046875
        },
        {
            "docid": "623686_18",
            "document": "Brain\u2013computer interface . Studies that developed algorithms to reconstruct movements from motor cortex neurons, which control movement, date back to the 1970s. In the 1980s, Apostolos Georgopoulos at Johns Hopkins University found a mathematical relationship between the electrical responses of single motor cortex neurons in rhesus macaque monkeys and the direction in which they moved their arms (based on a cosine function). He also found that dispersed groups of neurons, in different areas of the monkey's brains, collectively controlled motor commands, but was able to record the firings of neurons in only one area at a time, because of the technical limitations imposed by his equipment.",
            "score": 134.27325439453125
        },
        {
            "docid": "27075922_9",
            "document": "Natural computing . An artificial neural network is a network of artificial neurons.  An artificial neuron \"A\" is equipped with a function formula_1, receives \"n\" real-valued inputs formula_2 with respective weights formula_3, and it outputs formula_4. Some neurons are selected to be the output neurons, and the network function is the vectorial function that associates to the \"n\" input values, the outputs of the \"m\" selected output neurons. Note that different choices of weights produce different network functions for the same inputs. Back-propagation is a supervised learning method by which the weights of the connections in the network are repeatedly adjusted so as to minimize the difference between the vector of actual outputs and that of desired outputs. Learning algorithms based on backwards propagation of errors can be used to find optimal weights for given topology of the network and input-output pairs.",
            "score": 133.7246856689453
        },
        {
            "docid": "2920040_2",
            "document": "Neuronal tuning . Neuronal tuning refers to the hypothesized property of brain cells by which they selectively represent a particular type of sensory, association, motor, or cognitive information. Some neuronal responses have been hypothesized to be optimally tuned to specific patterns through experience. Neuronal tuning can be strong and sharp, as observed in primary visual cortex (area V1) (but see Carandini et al 2005 ), or weak and broad, as observed in neural ensembles. Single neurons are hypothesized to be simultaneously tuned to several modalities, such as visual, auditory, and olfactory. Neurons hypothesized to be tuned to different signals are often hypothesized to integrate information from the different sources. In computational models called neural networks, such integration is the major principle of operation. The best examples of neuronal tuning can be seen in the visual, auditory, olfactory, somatosensory, and memory systems, although due to the small number of stimuli tested the generality of neuronal tuning claims is still an open question.",
            "score": 131.67982482910156
        },
        {
            "docid": "21523_139",
            "document": "Artificial neural network . Large and effective neural networks require considerable computing resources. While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a simplified neuron on von Neumann architecture may compel a neural network designer to fill many millions of database rows for its connections which can consume vast amounts of memory and storage. Furthermore, the designer often needs to transmit signals through many of these connections and their associated neurons which must often be matched with enormous CPU processing power and time.",
            "score": 131.54925537109375
        },
        {
            "docid": "38567205_7",
            "document": "BRAIN Initiative . In a 2012 scientific commentary outlining experimental plans for a more limited project, Alivisatos \"et al.\" outlined a variety of specific experimental techniques that might be used to achieve what they termed a \"functional connectome\", as well as new technologies that will have to be developed in the course of the project. They indicated that initial studies might be done in \"Caenorhabditis elegans\", followed by \"Drosophila\", because of their comparatively simple neural circuits. Mid-term studies could be done in zebrafish, mice, and the Etruscan shrew, with studies ultimately to be done in primates and humans. They proposed the development of nanoparticles that could be used as voltage sensors that would detect individual action potentials, as well as nanoprobes that could serve as electrophysiological multielectrode arrays. In particular, they called for the use of wireless, noninvasive methods of neuronal activity detection, either utilizing microelectronic very-large-scale integration, or based on synthetic biology rather than microelectronics. In one such proposed method, enzymatically produced DNA would serve as a \"ticker tape record\" of neuronal activity, based on calcium ion-induced errors in coding by DNA polymerase. Data would be analyzed and modeled by large scale computation. A related technique proposed the use of high-throughput DNA sequencing for rapidly mapping neural connectivity.",
            "score": 129.80177307128906
        },
        {
            "docid": "15978180_15",
            "document": "Connectome . Brain networks can be defined at different levels of scale, corresponding to levels of spatial resolution in brain imaging (K\u00f6tter, 2007, Sporns, 2010). These scales can be roughly categorized as microscale, mesoscale and macroscale. Ultimately, it may be possible to join connectomic maps obtained at different scales into a single hierarchical map of the neural organization of a given species that ranges from single neurons to populations of neurons to larger systems like cortical areas. Given the methodological uncertainties involved in inferring connectivity from the primary experimental data, and given that there are likely to be large differences in the connectomes of different individuals, any unified map will likely rely on \"probabilistic\" representations of connectivity data (Sporns \"et al.\", 2005).",
            "score": 129.75131225585938
        },
        {
            "docid": "586357_18",
            "document": "Artificial general intelligence . The artificial neuron model assumed by Kurzweil and used in many current artificial neural network implementations is simple compared with biological neurons. A brain simulation would likely have to capture the detailed cellular behaviour of biological neurons, presently only understood in the broadest of outlines. The overhead introduced by full modeling of the biological, chemical, and physical details of neural behaviour (especially on a molecular scale) would require computational powers several orders of magnitude larger than Kurzweil's estimate. In addition the estimates do not account for glial cells, which are at least as numerous as neurons, and which may outnumber neurons by as much as 10:1, and are now known to play a role in cognitive processes.",
            "score": 129.31802368164062
        },
        {
            "docid": "31217535_44",
            "document": "Memory . One question that is crucial in cognitive neuroscience is how information and mental experiences are coded and represented in the brain. Scientists have gained much knowledge about the neuronal codes from the studies of plasticity, but most of such research has been focused on simple learning in simple neuronal circuits; it is considerably less clear about the neuronal changes involved in more complex examples of memory, particularly declarative memory that requires the storage of facts and events (Byrne 2007). Convergence-divergence zones might be the neural networks where memories are stored and retrieved. Considering that there are several kinds of memory, depending on types of represented knowledge, underlying mechanisms, processes functions and modes of acquisition, it is likely that different brain areas support different memory systems and that they are in mutual relationships in neuronal networks: \"components of memory representation are distributed widely across different parts of the brain as mediated by multiple neocortical circuits\".",
            "score": 128.8533477783203
        },
        {
            "docid": "34060396_6",
            "document": "Kwabena Boahen . Boahen's work has demonstrated that neuromorphic computer chips are capable of reproducing many types of brain phenomena across a large range of scales. Examples include ion-channel dynamics (individual molecules), excitable membrane behavior (individual neurons), the orientation tuning of neurons in Visual Cortex (individual cortical columns), and neural synchrony (individual cortical areas). Utilizing these breakthroughs, Boahen's Stanford lab built the first neurmorphic system with one million spiking neurons (and billions of synapses). This system, \"Neurogrid\", emulates networks of cortical neurons in real time, while consuming only a few watts of power. In contrast, simulating one million interconnected cortical neurons in real-time using traditional super-computers requires as much power as several thousand households.",
            "score": 128.8485565185547
        },
        {
            "docid": "44554925_5",
            "document": "John Wilson Moore . In the 1980s Moore turned his attention to using the evolving power of computers for two big problems: simulating experimental results, and predicting how action potentials travel in neurons of complex geometry. He hired Michael Hines, a mathematician, to collaborate in developing a neuronal simulator they named NEURON. Using NEURON he pioneered the concept of working back and forth between simulations and actual experiments, using simulations to predict the outcome of experiments on biological preparations, and then carrying out the experiment to test the validity of the parameters entered into the simulations. Hines took over the further evolution of NEURON while Moore collaborated with his wife, neurobiologist Ann Stuart, to make the educational tool Neurons in Action based on NEURON. Neurons in Action has been used widely to convey basic principles of neurophysiology, for example by Tibetan monks and nuns of the Dali Llama in exile in Dharamsala, India, and in a course of the International Brain Research Organization held annually for faculty in different African countries.",
            "score": 128.73875427246094
        },
        {
            "docid": "14408479_2",
            "document": "Biological neuron model . A biological neuron model, also known as a spiking neuron model, is a mathematical description of the properties of certain cells in the nervous system that generate sharp electrical potentials across their cell membrane, roughly one millisecond in duration, as shown in Fig. 1. Spiking neurons are known to be a major signaling unit of the nervous system, and for this reason characterizing their operation is of great importance. It is worth noting that not all the cells of the nervous system produce the type of spike that define the scope of the spiking neuron models. For example, cochlear hair cells, retinal receptor cells, and retinal bipolar cells do not spike. Furthermore, many cells in the nervous system are not classified as neurons but instead are classified as glia. Ultimately, biological neuron models aim to explain the mechanisms underlying the operation of the nervous system for the purpose of restoring lost control capabilities such as perception (e.g. deafness or blindness), motor movement decision making, and continuous limb control. In that sense, biological neuron models differ from artificial neuron models that do not presume to predict the outcomes of experiments involving the biological neural tissue (although artificial neuron models are also concerned with execution of perception and estimation tasks). Accordingly, an important aspect of biological neuron models is experimental validation, and the use of physical units to describe the experimental procedure associated with the model predictions.",
            "score": 128.49996948242188
        },
        {
            "docid": "233488_23",
            "document": "Machine learning . An artificial neural network (ANN) learning algorithm, usually called \"neural network\" (NN), is a learning algorithm that is vaguely inspired by biological neural networks. Computations are structured in terms of an interconnected group of artificial neurons, processing information using a connectionist approach to computation. Modern neural networks are non-linear statistical data modeling tools. They are usually used to model complex relationships between inputs and outputs, to find patterns in data, or to capture the statistical structure in an unknown joint probability distribution between observed variables.",
            "score": 128.2090606689453
        },
        {
            "docid": "3474296_4",
            "document": "Neuronal noise . Single neurons demonstrate different responses to specific neuronal input signals. This is commonly referred to as neural response variability. If a specific input signal is initiated in the dendrites of a neuron, then a hypervariability exists in the number of vesicles released from the axon terminal fiber into the synapse. This characteristic is true for fibers without neural input signals, such as pacemaker neurons, as mentioned previously, and cortical pyramidal neurons that have highly-irregular firing pattern. Noise generally hinders neural performance, but recent studies show, in dynamical non-linear neural networks, this statement does not always hold true. Non-linear neural networks are a network of complex neurons that have many connections with one another such as the neuronal systems found within our brains. Comparatively, linear networks are an experimental view of analyzing a neural system by placing neurons in series with each other.",
            "score": 127.70549774169922
        },
        {
            "docid": "1726672_4",
            "document": "Neural circuit . The connections between neurons in the brain are much more complex than those of the artificial neurons used in the connectionist neural computing models of artificial neural networks. The basic kinds of connections between neurons are synapses, chemical and electrical synapses. One principle by which neurons work is neural summation \u2013 potentials at the postsynaptic membrane will sum up in the cell body. If the depolarization of the neuron at the axon goes above threshold an action potential will occur that travels down the axon to the terminal endings to transmit a signal to other neurons. Excitatory and inhibitory synaptic transmission is realized mostly by inhibitory postsynaptic potentials (IPSPs) and excitatory postsynaptic potentials (EPSPs).",
            "score": 127.34922790527344
        },
        {
            "docid": "53757504_6",
            "document": "Viktor K. Jirsa . The Virtual Brain is a free open source neuroinformatics tool designed to aid in the exploration of network mechanisms of brain function and associated pathologies. TVB provides the possibility to feed computational neuronal network models with information about structural and functional imaging data including population (sEEG/EEG/MEG) activity, spatially highly resolved whole brain metabolic/vascular signals (fMRI) and global measures of neuronal connections (DTI) \u2013 for intact as well as pathologically altered connectivity. TVB is model agnostic and offers a wide range of neural population models to be used as network nodes. The software infrastructure of the Virtual Brain is composed of a functional core running the large-scale brain simulations independently or in batch mode, a web based interface to access the simulator, as well as a command line interface to develop more extensive applications. All simulations may be performed on workstations and labtops, as well as on high-performance clusters (HPCs). Manipulations of network parameters within the Virtual Brain allow researchers and clinicians to test the effects of experimental paradigms, interventions (such as stimulation and surgery) and therapeutic strategies (such as pharmaceutical interventions targeting local areas). The computational environment allows the user to visualise the simulated data in 2D and 3D and perform data analyses in the same way as commonly performed with empirical data.",
            "score": 126.46490478515625
        },
        {
            "docid": "37957946_6",
            "document": "De novo protein synthesis theory of memory formation . A line of research investigates long term potentiation (LTP), a process that describes how a memory can be consolidated between two neurons, or brain cells, ultimately by creating a circuit within the brain that can encode a memory. To initiate a learning circuit between two neurons, one prominent study described using tetanus stimulations to depolarize one neuron by 30mV, which, in turn, activated its NMDA glutamate receptors (Nowak, Bregestovski, Ascher, Herbert, & Prochiantz, 1984). The activation of these receptors resulted in Ca flooding the cell, initiating a cascade of secondary messengers. The cascade of resulting reactions, brought about by secondary messengers, terminates with the activation of cAMP response binding element protein (CREB), which acts as a transcription factor for various genes and initiates their expression (Hawkins, Kandel, & Bailey, 2006). Some proponents argue that the genes stimulate changes in communication between neurons, which underlie the encoding of memory; others suggest that the genes are byproducts of the LTP signaling pathway and are not directly involved in LTP. However, following the cascade of secondary messengers, no one would dispute that more AMPA receptors appear in the postsynaptic terminal (Hayashi et al., 2000). Higher numbers of AMPA receptors, taken together with the aforementioned events, allow for increased firing potential in the postsynaptic cell, which creates an improved learning circuit between these two neurons (Hayashi et al., 2000). Because of the specific, activity-dependent nature of LTP, it is an ideal model for a neural correlate of memory, as postulated by numerous studies; together, these studies show that the abolishment of LTP prevents the formation of memory at the neuronal level (Hawkins, Kandel, & Bailey, 2006).",
            "score": 126.4066390991211
        },
        {
            "docid": "21523_3",
            "document": "Artificial neural network . An ANN is based on a collection of connected units or nodes called artificial neurons which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it.",
            "score": 126.20089721679688
        },
        {
            "docid": "33244792_4",
            "document": "Non-spiking neuron . There are an abundance of neurons that propagate signals via action potentials and the mechanics of this particular kind of transmission is well understood. Spiking neurons exhibit action potentials as a result of a neuron characteristic known as membrane potential. Through studying these complex spiking networks in animals, a neuron that did not exhibit characteristic spiking behavior was discovered. These neurons use a graded potential to transmit data as they lack the membrane potential that spiking neurons possess. This method of transmission has a huge effect on the fidelity, strength, and lifetime of the signal. Non-spiking neurons were identified as a special kind of interneuron and function as an intermediary point of process for sensory-motor systems. Animals have become substantial models for understanding more about non-spiking neural networks and the role they play in an animal\u2019s ability to process information and its overall function. Animal models indicate that the interneurons modulate directional and posture coordinating behaviors. Crustaceans and arthropods such as the crawfish have created many opportunities to learn about the modulatory role that these neurons have in addition to their potential to be modulated regardless of their lack of exhibiting spiking behavior. Most of the known information about nonspiking neurons is derived from animal models. Studies focus on neuromuscular junctions and modulation of abdominal motor cells. Modulatory interneurons are neurons that are physically situated next to muscle fibers and innervate the nerve fibers which allow for some orienting movement. These modulatory interneurons are usually nonspiking neurons. Advances in studying nonspiking neurons included determining new delineations among the different types of interneurons. These discoveries were due to the usage of methods such as protein receptor silencing. Studies have been done on the non-spiking neuron qualities in animals of specific non-spiking neural networks that have a corollary in humans, e.g. retina amacrine cell of the eye.",
            "score": 125.59951782226562
        },
        {
            "docid": "42073672_6",
            "document": "Neuronal self-avoidance . The concept of neuronal self-avoidance emerged about 50\u00a0years ago. The pioneer studies were performed in the leech, focusing on the central nervous system and developing mechanosensory neurons. Leeches from two species: \"Hirudo medicinalis\" and \"Haementeria ghilianii\", remained the main organism for the study of the question of neuronal self-recognition and self-avoidance. In this animal, the repeating segmental pattern of the nervous system along with the fact that neurons are relatively few in number, and many are large enough to be recognized allowed the experimental study of the general problem of neuronal specificity. In 1968, through the mapping of mechanoreceptor axonal receptive fields in \"H. medicilalis\", Nicholls and Baylor revealed distinct types of boundaries between axons from the same or different types of neurons, and also between individual neurons. They observed that receptive fields were subdivided into discrete areas, innervated by the different branches of a single cell. These boundaries, unlike those between adjacent fields of different cells, were abrupt showing nearly no overlap. The authors then suggested a mechanism for the spatial arrangement of axons in which \"\"a fiber might repel other branches more strongly if they arise from the same cell than if they come from a homologue, and not at all if they come from a cell with a different modality\"\". In 1976, Yau confirmed their findings and proposed that the branches of a cell recognized each other, therefore avoiding to grow into the same territory and establishing the discrete areas that Nicholls and Baylor observed. It was then clear that mechanosensory neurons, in leech, show self-avoidance: with the repulsion between branches originating from the same cell, but they did not showed class-avoidance, meaning that branches from the same type of neurons could overlap.",
            "score": 125.4317626953125
        },
        {
            "docid": "6147487_39",
            "document": "Neural coding . From the theoretical point of view, population coding is one of a few mathematically well-formulated problems in neuroscience. It grasps the essential features of neural coding and yet is simple enough for theoretic analysis. Experimental studies have revealed that this coding paradigm is widely used in the sensor and motor areas of the brain. For example, in the visual area medial temporal (MT), neurons are tuned to the moving direction. In response to an object moving in a particular direction, many neurons in MT fire with a noise-corrupted and bell-shaped activity pattern across the population. The moving direction of the object is retrieved from the population activity, to be immune from the fluctuation existing in a single neuron\u2019s signal. In one classic example in the primary motor cortex, Apostolos Georgopoulos and colleagues trained monkeys to move a joystick towards a lit target. They found that a single neuron would fire for multiple target directions. However it would fire fastest for one direction and more slowly depending on how close the target was to the neuron's 'preferred' direction.",
            "score": 125.3404541015625
        },
        {
            "docid": "271430_14",
            "document": "Computational neuroscience . Modeling the richness of biophysical properties on the single-neuron scale can supply mechanisms that serve as the building blocks for network dynamics. However, detailed neuron descriptions are computationally expensive and this can handicap the pursuit of realistic network investigations, where many neurons need to be simulated. As a result, researchers that study large neural circuits typically represent each neuron and synapse with an artificially simple model, ignoring much of the biological detail. Hence there is a drive to produce simplified neuron models that can retain significant biological fidelity at a low computational overhead. Algorithms have been developed to produce faithful, faster running, simplified surrogate neuron models from computationally expensive, detailed neuron models.",
            "score": 125.27725982666016
        },
        {
            "docid": "739262_12",
            "document": "Neural correlate . Using such design, Nikos Logothetis and colleagues discovered perception-reflecting neurons in the temporal lobe. They created an experimental situation in which conflicting images were presented to different eyes (\"i.e.\", binocular rivalry). Under such conditions, human subjects report bistable percepts: they perceive alternatively one or the other image. Logothetis and colleagues trained the monkeys to report with their arm movements which image they perceived. Interestingly, temporal lobe neurons in Logothetis experiments often reflected what the monkeys' perceived. Neurons with such properties were less frequently observed in the primary visual cortex that corresponds to relatively early stages of visual processing. Another set of experiments using binocular rivalry in humans showed that certain layers of the cortex can be excluded as candidates of the neural correlate of consciousness. Logothetis and colleagues switched the images between eyes during the percept of one of the images. Surprisingly the percept stayed stable. This means that the conscious percept stayed stable and at the same time the primary input to layer 4, which is the input layer, in the visual cortex changed. Therefore layer 4 can not be a part of the neural correlate of consciousness. Mikhail Lebedev and their colleagues observed a similar phenomenon in monkey prefrontal cortex. In their experiments monkeys reported the perceived direction of visual stimulus movement (which could be an illusion) by making eye movements. Some prefrontal cortex neurons represented actual and some represented perceived displacements of the stimulus. Observation of perception related neurons in prefrontal cortex is consistent with the theory of Christof Koch and Francis Crick who postulated that neural correlate of consciousness resides in prefrontal cortex. Proponents of distributed neuronal processing may likely dispute the view that consciousness has a precise localization in the brain.",
            "score": 124.65189361572266
        }
    ]
}