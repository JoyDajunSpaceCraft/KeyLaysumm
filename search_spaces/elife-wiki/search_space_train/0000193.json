{
    "q": [
        {
            "docid": "263801_18",
            "document": "Alien hand syndrome . Recent studies have examined the neural correlates of emergence of the sense of agency under normal circumstances. This appears to involve consistent congruence between what is being produced through efferent outflow to the musculature of the body, and what is being sensed as the presumed product in the periphery of this efferent command signal. In alien hand syndrome, the neural mechanisms involved in establishing that this congruence has occurred may be impaired. This may involve an abnormality in the brain mechanism that differentiates between \"re-afference\" (i.e., the return of kinesthetic sensation from the self-generated \"active\" limb movement) and \"ex-afference\" (i.e., kinesthetic sensation generated from an externally produced 'passive' limb movement in which an active self does not participate). This brain mechanism is proposed to involve the production of a parallel \"efference copy\" signal that is sent directly to the somatic sensory regions and is transformed into a \"corollary discharge\", an expected afferent signal from the periphery that would result from the performance driven by the issued efferent signal. The correlation of the corollary discharge signal with the actual afferent signal returned from the periphery can then be used to determine if, in fact, the intended action occurred as expected. When the sensed result of the action is congruent with the predicted result, then the action can be labelled as self-generated and associated with an emergent sense of agency.",
            "score": 116.46268045902252
        },
        {
            "docid": "263801_17",
            "document": "Alien hand syndrome . It is theorized that alien hand syndrome results when disconnection occurs between different parts of the brain that are engaged in different aspects of the control of bodily movement. As a result, different regions of the brain are able to command bodily movements, but cannot generate a conscious feeling of self-control over these movements. As a result, the \"sense of agency\" that is normally associated with voluntary movement is impaired or lost. There is a dissociation between the process associated with the actual execution of the physical movements of the limb and the process that produces an internal sense of voluntary control over the movements, with this latter process thus normally creating the internal conscious sensation that the movements are being internally initiated, controlled and produced by an active self.",
            "score": 80.02448225021362
        },
        {
            "docid": "228051_6",
            "document": "Intrapersonal communication . Jones and Fernyhough cite other evidence for this hypothesis that inner speech is essentially like any other action. They mention that schizophrenics suffering auditory verbal hallucinations (AVH) need only open their mouths in order to disrupt the voices in their heads. To try and explain more about how inner speech works, but also what goes wrong with AVH patients, Jones and Fernyhough adapt what is known as the \"forward model\" of motor control, which uses the idea of \"efferent copies\". In a forward model of motor control, the mind generates movement unconsciously. While information is sent to the necessary body parts, the mind basically faxes a copy of that same information to other areas of the brain. This \"efferent\" copy could then be used to make predictions about upcoming movements. If the actual sensations match predictions, we experience the feeling of agency. If there is a mismatch between the body and its predicted position, perhaps due to obstructions or other cognitive disruption, no feeling of agency occurs. Jones and Fernyhough believe that the forward model might explain AVH and inner speech. Perhaps, if inner speech is a normal action, then the malfunction in schizophrenic patients is not the fact that actions (i.e. voices) are occurring at all. Instead, it may be that they are experiencing normal, inner speech, but the \"generation of the predictive efferent copy\" is malfunctioning. Without an efferent copy, motor commands are judged as alien (i.e. one does not feel like they caused the action). This could also explain why an open mouth stops the experience of alien voices: When the patient opens their mouth, the inner speech motor movements are not planned in the first place.",
            "score": 209.6323275566101
        },
        {
            "docid": "32018467_7",
            "document": "Christian Keysers . After finishing his master, Christian Keysers decided to concentrate on a subfield of cognitive neuroscience called social neuroscience that uses neuroscience methods to understand how we process the social world. He therefore performed his doctoral studies at the University of St Andrews with David Ian Perrett, one of the founding father of the field, to understand how the brain processes faces and facial expressions. This thesis work led to new insights into how quickly the brain can process the faces of others. During this period, Keysers became fascinated with the question of how the brain can attach meaning to the faces of others. How is it for instance, that we understand that a certain grimace would signal that another person is happy? How do we understand that a certain bodily movement towards a glass indicates that the other person aims to grasp a glass? In 1999, Keysers was exposed to a visit of Vittorio Gallese, who presented his recent discovery of mirror neurons in the Psychology department lecture series. This deeply influenced Keysers who decided to move to the lab of Giacomo Rizzolatti to undertake further studies on how these fascinating neurons could contribute to social perception. In 2000, after finishing his doctorate, Christian Keysers moved to the University of Parma to study mirror neurons. In early work there demonstrated that mirror neurons in the premotor cortex not only respond to the sight of actions, but also when actions can only be deduced or heard, leading to a publication in the journal \"Science\". This work had tremendous impact on the field, as it suggested that the premotor cortex could play a central, modality independent role in perception and may lay the origin for the evolution of speech in humans.  Together this work indicated that brain regions involved in our own actions play a role in how we process the actions of others. Keysers wondered whether a similar principle may underlie how we process the tactile sensations and emotions of others, and became increasingly independent of the research focus on the motor system in Parma. At the time, Keysers had also met his to be wife, Valeria Gazzola, a biologist in the final phases of her studies, and together they decided to explore if the somatosensory system might be involved in perceiving the sensations of others. Via a fruitful collaboration with the French neuroimaging specialist Bruno Wicker, they used functional magnetic resonance imaging, and showed for the first time, that the secondary somatosensory cortex, previously thought only to represent a persons own experiences of touch, is also activated when seeing someone or something else be touched. They also showed that the insula, thought only to respond to the experience of first-hand emotions, was also activated when we see another individual experience similar emotions. Together this indicated a much more general principle than the original mirror neuron theory, in which people process the actions, sensations and emotions of others by vicariously activating owns own actions, sensations and emotions. Jointly, this work laid the foundation of the neuroscientific investigation of empathy.",
            "score": 110.83052086830139
        },
        {
            "docid": "263801_15",
            "document": "Alien hand syndrome . As noted above, these ideas can also be linked to the concept of efference copy and \"re-afference\", where efference copy is a signal postulated to be directed from premotor cortex (activated normally in the process associated with emergence of an internally generated movement) over to somatosensory cortex of the parietal region, in advance of the arrival of the \"re-afferent\" input generated from the moving limb, that is, the afferent return from the moving limb associated with the self-generated movement produced. It is generally thought that a movement is recognized as internally generated when the efference copy signal effectively \"cancels out\" the re-afference. The afferent return from the limb is effectively correlated with the efference copy signal so that the re-afference can be recognized as such and distinguished from \"ex-afference\", which would be afferent return from the limb produced by an externally imposed force. When the efference copy is no longer normally generated, then the afferent return from the limb associated with the self-generated movement is mis-perceived as externally produced \"ex-afference\" since it is no longer correlated with or canceled out by the efference copy. As a result, the development of the sense that a movement is not internally generated even though it actually is (i.e. the failure of the sense of agency to emerge in conjunction with the movement), could indicate a failure of the generation of the efference copy signal associated with the normal premotor process through which the movement is prepared for execution.",
            "score": 106.96430850028992
        },
        {
            "docid": "14573357_17",
            "document": "Efference copy . Experiments have been conducted wherein subjects' feet are tickled both by themselves and with a robotic arm controlled by their own arm movements. These experiments have shown that people find a self-produced tickling motion of the foot to be much less \"\u201ctickly\u201d\" than a tickling motion produced by an outside source. They have postulated that this is because when a person sends a motor command to produce the tickling motion, the efference copy anticipates and cancels out the sensory outcome. This idea is further supported by evidence that a delay between the self-produced tickling motor command and the actual execution of this movement (mediated by a robotic arm) causes an increase in the perceived tickliness of the sensation. This shows that when the efference copy is incompatible with the afference, the sensory information is perceived as if it were exafference. Therefore, it is theorized that it is not possible to tickle ourselves because when the predicted sensory feedback (efference copy) matches the actual sensory feedback, the actual feedback will be attenuated. If the predicted sensory feedback does not match the actual sensory feedback, whether caused by a delay (as in the mediation by the robotic arm) or by external influences from the environment, the brain cannot predict the tickling motion on the body and a more intense tickling sensation is perceived. This is the reason why one cannot tickle oneself.",
            "score": 129.46689867973328
        },
        {
            "docid": "14573357_18",
            "document": "Efference copy . It has been argued that motor efference copies play an important role in speech production. Tian and Poeppel propose that a motor efference copy is used to produce a forward model of somatosensory estimation, which entails an estimation of the articulatory movement and position of the articulators as a result of planned motor action. A second (subsequent) auditory efference copy entails the estimation of auditory information as produced by the articulatory system in a second forward model. Both of these forward models can produce respective predictions and corollary discharge, which can in turn be used in comparisons with somatosensory and auditory feedback. Moreover, this system is thought by some to be the basis for inner speech, especially in relation to auditory verbal hallucinations. In the case of inner speech, the efference signal is not sent or is inhibited before action takes place, leaving only the efference copy and leading to the perception of inner speech or inner hearing.. In the case of auditory verbal hallucinations, it is thought that a breakdown along the efference copy and forward model route creates a mismatch between what is expected and what is observed, leading to the experience that speech is not produced by oneself.",
            "score": 178.6505844593048
        },
        {
            "docid": "43349786_9",
            "document": "Lisp . Another popular method for treating a lisp is using specially designed devices that go in the mouth to provide a tactile cue of exactly where the tongue should be positioned when saying the \"S\" sound. This tactile feedback has been shown to correct lisp errors twice as fast as traditional therapy.  Using either or both methods, the repetition of consistent contexts allows the student to align all the necessary processes required to properly produce language; language skills (ability to formulate correct sounds in the brain: What sounds do I need to make?), motor planning (voicing and jaw and tongue movements: How do I produce the sound?), and auditory processing (receptive feedback: Was the sound produced correctly? Do I need to correct?).",
            "score": 114.39751410484314
        },
        {
            "docid": "6989597_3",
            "document": "Premovement neuronal activity . Research of pre-movement neuronal activity generally involves studying two different kinds of movement, movement in natural settings versus movement triggered by a sensory stimulus. These two types of movements are referred to with different nomenclature throughout different studies and literature on the topic of premovement neuronal activity. Voluntary movements are also known as self-timed, self-initiated, self-paced, and non-triggered movements. This type of movement is what generally occurs in natural settings, carried out independently of a sensory cue or external signal which would trigger or cause the movement to be performed  . In contrast, movements that are carried out as a result of a sensory cue or stimulus, or reflex-reactions to external conditions or changes are called reactive movements, but also known as cued movements, stimulated movements, and externally triggered movements depending on the choice of a particular study. In one such study by Lee and Assad (2003), rhesus monkeys were trained to execute arm movement in response to a visual cue versus the same arm movement performed without any correlation to this external (visual) cue. This is one example of reactive movements in contrast to self-initiated movements . Subsequent studies of rates of neuronal firing in the respective types of movements are recorded in different areas of the brain in order to develop a more thorough understanding of premovement neuronal activity.",
            "score": 73.39226639270782
        },
        {
            "docid": "54421246_9",
            "document": "Spinocerebellar ataxia type 1 . SCAs can also be detected before serious atrophy with electrophysiologic techniques, using electrodes on the scalp to detect changes in electric potential within the brain in response to sensations or movements. Individuals with SCA1 often exhibit abnormal brainstem auditory evoked potential, including prolonged latency and absent or poorly defined waveforms, with one study reporting 73.3% of test subjects exhibiting abnormalities. The same study also found abnormalities in visual evoked potential and median somatosensory evoked potential in some SCA1 persons. These results were similar to those exhibited in other SCAs and differences between the SCAs were not statistically significant, so electrophysiologic techniques cannot replace genetic testing for specific diagnoses of SCAs.",
            "score": 51.84974956512451
        },
        {
            "docid": "6541938_8",
            "document": "Lexical-gustatory synesthesia . SC is a synesthete who automatically experiences smells, tastes, and feelings of textures in her mouth and throat when she reads, speaks, or hears language, music, and certain environmental sounds. In SC\u2019s case study, researchers utilized fMRI to determine the areas of the brain that were activated during her synesthetic experiences. They compared areas of activation in SC\u2019s brain to those found in literature for other synesthetes, speech processing, language, and sound processing. In SC\u2019s scans, two important regions of the brain were largely activated during her taste sensations: the left anterior insula and the left superior parietal lobe. The scans led researchers to speculate that the anterior insula may play a role in SC\u2019s taste experiences while the superior parietal lobe binds together all of the sensory information for processing. Based off the findings of this study and others like it, it could be possible to determine the type of inducer that leads to synesthetic sensations based on the patterns of brain activity.",
            "score": 154.94140326976776
        },
        {
            "docid": "14573357_2",
            "document": "Efference copy . An efference copy or efferent copy is an internal copy of an outflowing (\"efferent\"), movement-producing signal generated by the motor system. It can be collated with the (reafferent) sensory input that results from the agent's movement, enabling a comparison of actual movement with desired movement, and a shielding of perception from particular self-induced effects on the sensory input to achieve perceptual stability. Together with internal models, efference copies can serve to enable the brain to predict the effects of an action.",
            "score": 100.79018044471741
        },
        {
            "docid": "620396_41",
            "document": "Origin of language . Proponents of the motor theory of language evolution have primarily focused on the visual domain and communication through observation of movements. The \"Tool-use sound hypothesis\" suggests that the production and perception of sound, also contributed substantially, particularly \"incidental sound of locomotion\" (\"ISOL\") and \"tool-use sound\" (\"TUS\"). Human bipedalism resulted in rhythmic and more predictable \"ISOL\". That may have stimulated the evolution of musical abilities, auditory working memory, and abilities to produce complex vocalizations, and to mimic natural sounds. Since the human brain proficiently extracts information about objects and events from the sounds they produce, \"TUS\", and mimicry of \"TUS\", might have achieved an iconic function. The prevalence of sound symbolism in many extant languages supports this idea. Self-produced TUS activates multimodal brain processing (motor neurons, hearing, proprioception, touch, vision), and \"TUS\" stimulates primate audiovisual mirror neurons, which is likely to stimulate the development of association chains. Tool use and auditory gestures involve motor-processing of the forelimbs, which is associated with the evolution of vertebrate vocal communication. The production, perception, and mimicry of \"TUS\" may have resulted in a limited number of vocalizations or protowords that were associated with tool use. A new way to communicate about tools, especially when out of sight, would have had selective advantage. A gradual change in acoustic properties and/or meaning could have resulted in arbitrariness and an expanded repertoire of words. Humans have been increasingly exposed to \"TUS\" over millions of years, coinciding with the period during which spoken language evolved.",
            "score": 139.45981454849243
        },
        {
            "docid": "35182952_20",
            "document": "Embodied language processing . Neurophysiological evidence has also been presented to prove an ACE. This research used a behavioural paradigm as well as Event-Related Potential (ERP) to record brain activity, allowing the researchers to explore the neural brain markers of the ACE paradigm in semantic processing and motor responses. ERP was particularly beneficial in helping the researchers to investigate the bi-directional hypothesis of action-sentence comprehension, which proposes that language processing facilitates movement and movement also facilitates language comprehension. In the study participants listened to sentences describing an action that involved an open hand, a closed hand or no manual action. They were then required to press a button to indicate their understanding of the sentence. Each participant was assigned a hand-shape, either closed or open, which was required to activate the button. As well as two groups (closed or open hand-shapes), there were three different categories relating to hand-shape: compatible, incompatible and neutral. Behavioural results from the study showed that participants responded quicker when the hand-shape required to press the response-button was compatible with the hand-shape inferred by the sentence. ERP results provided evidence to support the bi-directional hypothesis, showing that cortical markers of motor processes were affected by sentence meaning, therefore providing evidence for a semantics-to-motor effect. ERP results also demonstrated a motor-to-semantics effect as brain markers of comprehension were modified by motor effects.",
            "score": 74.65456295013428
        },
        {
            "docid": "3561546_12",
            "document": "Thought insertion . The comparator-model, also known as the forward model, is an elaboration of theory of misattributed inner speech. This theory relies on a model involved in inner speech known as the forward model. Specifically, the comparator-model of thought insertion describes processing of movement-related sensory feedback involving a parietal-cerebellar network as subject to feedforward inhibition during voluntary movements and this is thought to contribute to the subject feeling as though thoughts are inserted into his or her mind. It has been proposed that the loss of sense of agency results from a disruption of feedforward inhibition of somatosensory processing for self-generated movements. Frith (2012) argues that delusions and hallucination are associated with a failure in the predictive component of the model.  Critics of this model argue that it makes the unsupported claim that a model for movement can be transferred to account for thoughts. These critics argue that this jump cannot be made because it is not known that movements and thoughts are processed in the same way. Support for the comparator-model has also been spotty. In an experiment done by Walsh and colleges (2015), the theory behind the forward model of thought insertion was not supported. They found that thought insertion was not associated with overactivation of somatosensory or other self-monitoring networks that occurs in movement. They argue that this provides evidence that a model for motor agency cannot explain thought agency.",
            "score": 88.46851456165314
        },
        {
            "docid": "13269219_15",
            "document": "Electronic fluency device . The precise reasons for the fluency-inducing effects of AAF in stutterers are unknown. Early investigators suggested that those who stutter had an abnormal speech\u2013auditory feedback loop that was corrected or bypassed while speaking under DAF. Later researchers proposed increased fluency was actually caused by the changes in speech production, including slower speech rates, higher pitches and increased loudness, rather than the AAF per se. However, subsequent studies have noted that increased fluency occurred in some stutterers at normal and fast rates using DAF. Some suggest that stuttering is caused by defective auditory processing, and that AAF helps to correct the misperceived rhythmic structure of speech. It has been shown that some stutterers have noted that have atypical auditory anatomy and that DAF improved fluency in these stutterers but not in those with typical anatomy. However, positron emission tomography studies on choral reading in stutterers suggest that AAF also made changes in motor and speech production areas of the brain, as well as the auditory processing areas. Choral reading reduced the overactivity in motor areas that is found with stuttered reading, and largely reversed the left-hemisphere based auditory-system and speech production system underactivation. Noting that the effects of altered feedback vary from person to person and can wear off over time, distraction has also been proposed as a possible cause of stuttering reduction with AAF.",
            "score": 111.90332746505737
        },
        {
            "docid": "29354346_7",
            "document": "Change deafness . One study used fMRI data to distinguish neural correlates of physical changes in auditory input (independent of conscious change detection), from those of conscious perception of change (independent of an actual physical change). The study made use of a change deafness paradigm in which participants were exposed to complex auditory scenes consisting of six individual auditory streams differing in pitch, rhythm, and sound source location, and received a cue indicating which stream to attend to. Each participant listened to two consecutively presented auditory scenes after which they were prompted to indicate whether both scenes were identical or not. Functional MRI results revealed that physical change in stimulus was correlated with increased BOLD responses in the right auditory cortex, near the lateral portion of Heschl's gyrus, the first cortical structure to process incoming auditory information, but not in hierarchically higher brain regions. Conscious change detection was correlated with increased coupled responses in the ACC and the right insula, consistent with additional evidence that the anterior insula functions to mediate dynamic interactions between other brain networks involved in attention to external stimuli, forming a salience network with the ACC that identifies salient stimulus events and initiates additional processing. In absence of change detection, this salience network was not activated; however increased activity in other cortical areas suggests that undetected changes are still perceived on some level, but fail to trigger conscious change detection, thus producing the change deafness phenomenon.",
            "score": 92.35371315479279
        },
        {
            "docid": "14573357_8",
            "document": "Efference copy . Corollary discharge is characterized as an efference copy of an action command used to inhibit any response to the self generated sensory signal which would interfere with the execution of the motor task. The inhibitory commands originate at the same time as the motor command and target the sensory pathway that would report any reafference to higher levels of the CNS. This is unique from the efference copy, since the corollary discharge is actually fed into the sensory pathway to cancel out the reafferent signals generated by the movement. Alternatively, corollary discharges briefly alters self-generated sensory responses to reduce self-induced desensitization or help distinguish between self-generated and externally generated sensory information.",
            "score": 66.67415308952332
        },
        {
            "docid": "991729_9",
            "document": "Inferior frontal gyrus . The left inferior frontal gyrus (IFG) is also extremely important for language comprehension and production due to the fact that most language processing takes place in the left hemisphere. Commonly known as \"Broca's area\", persons with damage in this region often have a type of non-fluent aphasia known as Broca's aphasia. Broca's area is located on the left hemisphere of the brain and encompasses Brodmann's area 44 and 45. Both overall have contributions to verbal fluency, but each has its own specific contribution. Area 44 is involved in the language production and phonological processing due to its connections with motor areas like the mouth and tongue. Area 45 is a part of the anterior inferior frontal gyrus and is involved in semantic processing. Characteristics of Broca's aphasia include agrammatic speech, relatively good language comprehension, poor repetition, and difficulty speaking. Persons with Broca's aphasia do not have deficits in language comprehension; however, they speak mostly in short utterances of a few words at a time, mostly nouns. Their speech is limited to short sentences, and producing sound is a very difficult task for those affected. The left IFG has also been suggested to play a role in inhibitory processes, including the tendency to inhibit learning from undesirable information. For example, TMS to the left IFG has been shown to release such inhibition, increasing the ability to learn from undesirable information.",
            "score": 117.7114155292511
        },
        {
            "docid": "22509570_10",
            "document": "Lateralized readiness potential . Cueing paradigms may even influence response preparation when the subject is unaware of the cue. In a special type of cuing paradigm the cue can be presented for a very short period of time (e.g., 40 ms) and preceded and followed by other visual stimuli that effectively \"mask\" the cue\u2019s presence. This type of paradigm, called \"masked priming\", has been used with the LRP to see whether a cue someone is unable to identify at all is still able to influence the response system. For example, one study showed that a masked prime that gave conflicting response information compared to the target reliably slowed subjects\u2019 response times, even though the subjects reported never seeing the masked prime. They also showed that the conflicting masked prime induced an LRP such that the brain started preparing a response based on the semantic information in the masked prime. This suggests that a cue with newly learned meaningful implications for the motor system (i.e., arbitrary response-mappings) need not be consciously processed in order for response preparations to begin. Thus since the LRP can pick up signals for responses never actually initiated or perceived of, it can uncover information processing that happens without our awareness but that can still affect our overt behavior.",
            "score": 57.7576459646225
        },
        {
            "docid": "2860430_32",
            "document": "Neural oscillation . Next to evoked activity, neural activity related to stimulus processing may result in induced activity. Induced activity refers to modulation in ongoing brain activity induced by processing of stimuli or movement preparation. Hence, they reflect an indirect response in contrast to evoked responses. A well-studied type of induced activity is amplitude change in oscillatory activity. For instance, gamma activity often increases during increased mental activity such as during object representation. Because induced responses may have different phases across measurements and therefore would cancel out during averaging, they can only be obtained using time-frequency analysis. Induced activity generally reflects the activity of numerous neurons: amplitude changes in oscillatory activity are thought to arise from the synchronization of neural activity, for instance by synchronization of spike timing or membrane potential fluctuations of individual neurons. Increases in oscillatory activity are therefore often referred to as event-related synchronization, while decreases are referred to as event-related desynchronization.",
            "score": 75.31995594501495
        },
        {
            "docid": "40621603_5",
            "document": "Linguistic intelligence . Speech production is process by which a thought in the brain is converted into an understandable auditory form. This is a multistage mechanism that involves many different areas of the brain. The first stage is planning, where the brain constructs words and sentences that turn the thought into an understandable form. This occurs primarily in the inferior frontal cortex, specifically in an area known as Broca's area. Next, the brain must plan how to physically create the sounds necessary for speech by linking the planned speech with known sounds, or phonemes. While the location of these associations is not known, it is known that the supplementary motor area plays a key role in this step. Finally, the brain must signal for the words to actually be spoken. This is carried out by the premotor cortex and the motor cortex. In most cases, speech production is controlled by the left hemisphere. In a series of studies, Wilder Penfield, among others, probed the brains of both right-handed (generally left-hemisphere dominant) and left-handed (generally right-hemisphere dominant) patients. They discovered that, regardless of handedness, the left hemisphere was almost always the speech controlling side. However, it has been discovered that in cases of neural stress (hemorrhage, stroke, etc.) the right hemisphere has the ability to take control of speech functions.",
            "score": 149.98775672912598
        },
        {
            "docid": "35073980_5",
            "document": "Self-reference effect . In recent years, there has been an increase in cognitive neuroscience studies that focus on the concept of the self. These studies were developed in hopes of determining if there are certain brain regions that can account for the encoding advantages involved in the self-reference effect. A great deal of research has been focused on several regions of the brain collectively identified as the cortical midline region. Brain imaging studies have raised the question of whether neural activity in cortical midline regions is self-specific. A quantitative meta-analysis that included 87 studies, representing 1433 participants, was conducted to discuss these questions. The analysis uncovered activity within several cortical midline structures in activities in which participants performed tasks involving the concept of self. Most studies that report such midline activations use tasks that are geared towards uncovering neural processes that are related to social or psychological aspects of the self, such as self-referential judgments, self-appraisal, and judgments of personality traits. Also, in addition to their perceived role in several forms of self-representation, cortical midline structures are also involved in the processing of social relationships and recognizing personally familiar others. Studies that show midline activations during understanding of social interactions between others or ascribing social traits to others (impression formation) typically require subjects to reference the mental state of others.",
            "score": 62.54191017150879
        },
        {
            "docid": "36058569_5",
            "document": "Frank H. Guenther . Frank Guenther\u2019s research is aimed at uncovering the neural computations underlying the processing of speech by the human brain. He is the originator of the Directions Into Velocities of Articulators (DIVA) model, which is currently the leading model of the neural computations underlying speech production. This model mathematically characterizes the computations performed by each brain region involved in speech production as well as the function of the interconnections between these regions. The model has been supported by a wide range of experimental tests of model predictions, including electromagnetic articulometry studies investigating speech movements, auditory perturbation studies involving modification of a speaker\u2019s feedback of his/her own speech in real time, and functional magnetic resonance imaging studies of brain activity during speech, though some parts of the model remain to be experimentally verified. The DIVA model has been used to investigate the neural underpinnings of a number of communication disorders, including stuttering apraxia of speech, and hearing-impaired speech.",
            "score": 131.2084357738495
        },
        {
            "docid": "263801_12",
            "document": "Alien hand syndrome . A 2009 fMRI study looking at the temporal sequence of activation of components of a cortical network associated with voluntary movement in normal individuals demonstrated \"an anterior-to-posterior temporal gradient of activity from supplemental motor area through premotor and motor cortices to the posterior parietal cortex\". Therefore, with normal voluntary movement, the emergent sense of agency appears to be associated with an orderly sequence of activation that develops initially in the anteromedial frontal cortex in the vicinity of the supplementary motor complex on the medial surface of the frontal aspect of the hemisphere (including the supplementary motor area) \"prior\" to activation of the primary motor cortex in the pre-central gyrus on the lateral aspect of the hemisphere, when hand movement is being generated. Activation of the primary motor cortex, presumed to be directly involved in the execution of the action via projections into the corticospinal component of the pyramidal tracts, is then followed by activation of the posterior parietal cortex, possibly related to the receipt of recurrent or \"re-afferent\" somatosensory feedback generated from the periphery by the movement which would normally interact with the efference copy transmitted from primary motor cortex to permit the movement to be recognized as self-generated rather than imposed by an external force. That is, the efference copy allows the recurrent afferent somatosensory flow from the periphery associated with the self-generated movement to be recognized as \"re-afference\" as distinct from \"ex-afference\". Failure of this mechanism may lead to a failure to distinguish between self-generated and externally generated movement of the limb. This anomalous situation in which re-afference from a self-generated movement is mistakenly registered as ex-afference due to a failure to generate and successfully transmit an efference copy to sensory cortex, could readily lead to the interpretation that what is in actuality a self-generated movement has been produced by an external force as a result of the failure to develop a sense of agency in association with emergence of the self-generated movement (see below for a more detailed discussion).",
            "score": 76.26419234275818
        },
        {
            "docid": "56439577_3",
            "document": "Temporal envelope and fine structure . Complex sounds such as speech or music are decomposed by the peripheral auditory system of humans into narrow frequency bands. The resulting narrow-band signals convey information at different time scales ranging from less than one millisecond to hundreds of milliseconds. A dichotomy between slow \"temporal envelope\" cues and faster \"temporal fine structure\" cues has been proposed to study several aspects of auditory perception (e.g., loudness, pitch and timbre perception, auditory scene analysis, sound localization) at two distinct time scales in each frequency band. Over the last decades, a wealth of psychophysical, electrophysiological and computational studies based on this envelope/fine-structure dichotomy have examined the role of these temporal cues in sound identification and communication, how these temporal cues are processed by the peripheral and central auditory system, and the effects of aging and cochlear damage on temporal auditory processing. Although the envelope/fine-structure dichotomy has been debated and questions remain as to how temporal fine structure cues are actually encoded in the auditory system, these studies have led to a range of applications in various fields including speech and audio processing, clinical audiology and rehabilitation of sensorineural hearing loss via hearing aids or cochlear implants.",
            "score": 112.6272406578064
        },
        {
            "docid": "33993614_36",
            "document": "Neurocomputational speech processing . The tuning of the synaptic projections between speech sound map and motor map (i.e. tuning of forward motor commands) is accomplished with the aid of feedback commands, since the projections between sensory error maps and motor map were already tuned during babbling training (see above). Thus the DIVA model tries to \"imitate\" an auditory speech item by attempting to find a proper feedforward motor command. Subsequently the model compares the resulting sensory output (\"current\" sensory state following the articulation of that attempt) with the already learned auditory target region (\"intended\" sensory state) for that speech item. Then the model updates the current feedforward motor command by the current feedback motor command generated from the auditory error map of the auditory feedback system. This process may be repeated several times (several attempts). The DIVA model is capable of producing the speech item with a decreasing auditory difference between current and intended auditory state from attempt to attempt.",
            "score": 104.16201102733612
        },
        {
            "docid": "35073980_17",
            "document": "Self-reference effect . The self-reference effect is a rich and powerful encoding process that can be used multiple ways. The self-reference effect shows better results over the semantic method when processing personal information. Processing personal information can be distinguished and recalled differently with age. The older the subject, the more rich and vivid the memory can be due to the amount of information the brain has processed. The self-reference just as effective as the SQR4 method when study for exams, but the self-reference method is preferred. Defining general and specific memories using objects, verbal cues, etc. can be effective when using the self-reference effect. When using these different method, the same part of the brain is being active resulting in relation and better recall. It was expected that participants would recall the most number of words from the self-referent list rather than from the semantic or structural lists and more words from the semantic list than from the structural list. It was also expected that for the words encoded in the self-referent condition, fewer words would be recalled by participants in the high altruism group than in the low altruism group.",
            "score": 76.81880450248718
        },
        {
            "docid": "1095131_20",
            "document": "Kinesthetic learning . The cerebral cortex is the brain tissue covering the top and sides of the brain in most vertebrates. It is involved in storing and processing of sensory inputs and motor outputs. In the human brain, the cerebral cortex is actually a sheet of neural tissue about 1/8th inch thick. The sheet is folded so that it can fit inside the skull. The neural circuits in this area of the brain expand with practice of an activity, just like the synaptic plasticity grows with practice. Clarification of some of the mechanisms of learning by neuro science has been advanced, in part, by the advent of non-invasive imaging technologies, such as positron emission tomography (PET) and functional magnetic resonance imaging (FMRI). These technologies have allowed researchers to observe human learning processes directly. Through these types of technologies, we are now able to see and study what happens in the process of learning. In different tests performed the brain being imaged showed a greater blood flow and activation to that area of the brain being stimulated through different activities such as finger tapping in a specific sequence. It has been revealed that the process at the beginning of learning a new skill happens quickly, and later on slows down to almost a plateau. This process can also be referred to as The Law of Learning. The slower learning showed in the FMRI that in the cerebral cortex this was when the long term learning was occurring, suggesting that the structural changes in the cortex reflect the enhancement of skill memories during later stages of training. When a person studies a skill for a longer duration of time, but in a shorter amount of time they will learn quickly, but also only retain the information into their short-term memory. Just like studying for an exam; if a student tries to learn everything the night before, it will not stick in the long run. If a person studies a skill for a shorter duration of time, but more frequently and long-term, their brain will retain this information much longer as it is stored in the long-term memory. Functional and structural studies of the brain have revealed a vast interconnectivity between diverse regions of the cerebral cortex. For example, large numbers of axons interconnect the posterior sensory areas serving vision, audition, and touch with anterior motor regions. Constant communication between sensation and movement makes sense, because to execute smooth movement through the environment, movement must be continuously integrated with knowledge about one's surroundings obtained via sensory perception. The cerebral cortex plays a role in allowing humans to do this.",
            "score": 96.90738558769226
        },
        {
            "docid": "1875075_44",
            "document": "Self-control . Years later Dr. Mischel reached out to the participants of his study who were then in their 40's. He found that those who showed less self-control by taking the single marshmallow in the initial study were more likely to develop problems with relationships, stress, and drug abuse later in life. Dr. Mischel carried out the experiment again with the same participants in order to see which parts of the brain were active during the process of self-control. The participants received scans through M.R.I to show brain activity. The results showed that those who exhibited lower levels of self-control had higher brain activity in the ventral striatum, the area that deals with positive rewards.",
            "score": 76.2349259853363
        },
        {
            "docid": "30601657_12",
            "document": "Response priming . Response-priming effects have been demonstrated for a large number of stimuli and discrimination tasks, including geometric stimuli, color stimuli, various types of arrows, natural images (animals vs. objects), vowels and consonants, letters, and digits. In one study, chess configurations were presented as primes and targets, and participants had to decide whether the king was in check. Mattler (2003) could show that response priming can not only influence motor responses, but also works for cognitive operations like a spatial shift of visual attention or a shift between two different response time tasks. Different types of masking have been employed as well. Instead of measuring keypress responses (commonly with two response alternatives), some studies use more than two response alternatives or record speech responses, speeded finger pointing movements, eye movements, or so-called readiness potentials which reflect the degree of motor activation in the brain's motor cortex and can be measured by electro-encephalographic methods. Brain imaging methods like functional magnetic resonance imaging (fMRI) have been employed as well.",
            "score": 84.12818026542664
        },
        {
            "docid": "685746_11",
            "document": "Laterality . Cerebral dominance or specialization has been studied in relation to a variety of human functions. With speech in particular, many studies have been used as evidence that it is generally localized in the left hemisphere. Research comparing the effects of lesions in the two hemispheres, split-brain patients, and perceptual asymmetries have aided in the knowledge of speech lateralization. In one particular study, the left hemisphere\u2019s sensitivity to differences in rapidly changing sound cues was noted (Annett, 1991). This has real world implication, since very fine acoustic discriminations are needed to comprehend and produce speech signals. In an electrical stimulation demonstration performed by Ojemann and Mateer (1979), the exposed cortex was mapped revealing the same cortical sites were activated in phoneme discrimination and mouth movement sequences (Annett, 1991).",
            "score": 112.70013403892517
        }
    ],
    "r": [
        {
            "docid": "228051_6",
            "document": "Intrapersonal communication . Jones and Fernyhough cite other evidence for this hypothesis that inner speech is essentially like any other action. They mention that schizophrenics suffering auditory verbal hallucinations (AVH) need only open their mouths in order to disrupt the voices in their heads. To try and explain more about how inner speech works, but also what goes wrong with AVH patients, Jones and Fernyhough adapt what is known as the \"forward model\" of motor control, which uses the idea of \"efferent copies\". In a forward model of motor control, the mind generates movement unconsciously. While information is sent to the necessary body parts, the mind basically faxes a copy of that same information to other areas of the brain. This \"efferent\" copy could then be used to make predictions about upcoming movements. If the actual sensations match predictions, we experience the feeling of agency. If there is a mismatch between the body and its predicted position, perhaps due to obstructions or other cognitive disruption, no feeling of agency occurs. Jones and Fernyhough believe that the forward model might explain AVH and inner speech. Perhaps, if inner speech is a normal action, then the malfunction in schizophrenic patients is not the fact that actions (i.e. voices) are occurring at all. Instead, it may be that they are experiencing normal, inner speech, but the \"generation of the predictive efferent copy\" is malfunctioning. Without an efferent copy, motor commands are judged as alien (i.e. one does not feel like they caused the action). This could also explain why an open mouth stops the experience of alien voices: When the patient opens their mouth, the inner speech motor movements are not planned in the first place.",
            "score": 209.63232421875
        },
        {
            "docid": "4643899_5",
            "document": "Categorical perception . According to the (now abandoned) motor theory of speech perception, the reason people perceive an abrupt change between /ba/ and /pa/ is that the way we hear speech sounds is influenced by how people produce them when they speak. What is varying along this continuum is voice-onset-time: the \"b\" in /ba/ is voiced and the \"p\" in /pa/ is not. But unlike the synthetic \"morphing\" apparatus, people's natural vocal apparatus is not capable of producing anything in between ba and pa. So when one hears a sound from the voicing continuum, their brain perceives it by trying to match it with what it would have had to do to produce it. Since the only thing they can produce is /ba/ or /pa/, they will perceive any of the synthetic stimuli along the continuum as either /ba/ or /pa/, whichever it is closer to. A similar CP effect is found with ba/da; these too lie along a continuum acoustically, but vocally, /ba/ is formed with the two lips, /da/ with the tip of the tongue and the alveolar ridge, and our anatomy does not allow any intermediates.",
            "score": 198.1061248779297
        },
        {
            "docid": "14405771_9",
            "document": "Speech science . Speech perception refers to the understanding of speech. The beginning of the process towards understanding speech is first hearing the message that is spoken. The auditory system receives sound signals starting at the outer ear. They enter the pinna and continue into the external auditory canal (ear canal) and then to the eardrum. Once in the middle ear, which consists of the malleus, the incus, and the stapes; the sounds are changed into mechanical energy. After being converted into mechanical energy, the message reaches the oval window, which is the beginning of the inner ear. Once inside the inner ear, the message is transferred into hydraulic energy by going through the cochlea, which is filled with fluid, and on to the Organ of Corti. This organ again helps the sound to be transferred into a neural impulse that stimulates the auditory pathway and reaches the brain. Sound is then processed in Heschl's gyrus and associated with meaning in Wernicke's area. As for theories of speech perception, there are a motor and an auditory theory. The motor theory is based upon the premise that speech sounds are encoded in the acoustic signal rather than enciphered in it. The auditory theory puts greater emphasis on the sensory and filtering mechanisms of the listener and suggests that speech knowledge is a minor role that\u2019s only used in hard perceptual conditions.",
            "score": 182.39459228515625
        },
        {
            "docid": "433584_4",
            "document": "McGurk effect . Vision is the primary sense for humans, but speech perception is multimodal, which means that it involves information from more than one sensory modality, in particular, audition and vision. The McGurk effect arises during phonetic processing because the integration of audio and visual information happens early in speech perception. The McGurk effect is very robust; that is, knowledge about it seems to have little effect on one's perception of it. This is different from certain optical illusions, which break down once one 'sees through' them. Some people, including those that have been researching the phenomenon for more than twenty years, experience the effect even when they are aware that it is taking place. With the exception of people who can identify most of what is being said from speech-reading alone, most people are quite limited in their ability to identify speech from visual-only signals. A more extensive phenomenon is the ability of visual speech to increase the intelligibility of heard speech in a noisy environment. Visible speech can also alter the perception of perfectly audible speech sounds when the visual speech stimuli are mismatched with the auditory speech. Normally, speech perception is thought to be an auditory process; however, our use of information is immediate, automatic, and, to a large degree, unconscious and therefore, despite what is widely accepted as true, speech is not only something we hear. Speech is perceived by all of the senses working together (seeing, touching, and listening to a face move). The brain is often unaware of the separate sensory contributions of what it perceives. Therefore, when it comes to recognizing speech the brain cannot differentiate whether it is seeing or hearing the incoming information.",
            "score": 179.45803833007812
        },
        {
            "docid": "14573357_18",
            "document": "Efference copy . It has been argued that motor efference copies play an important role in speech production. Tian and Poeppel propose that a motor efference copy is used to produce a forward model of somatosensory estimation, which entails an estimation of the articulatory movement and position of the articulators as a result of planned motor action. A second (subsequent) auditory efference copy entails the estimation of auditory information as produced by the articulatory system in a second forward model. Both of these forward models can produce respective predictions and corollary discharge, which can in turn be used in comparisons with somatosensory and auditory feedback. Moreover, this system is thought by some to be the basis for inner speech, especially in relation to auditory verbal hallucinations. In the case of inner speech, the efference signal is not sent or is inhibited before action takes place, leaving only the efference copy and leading to the perception of inner speech or inner hearing.. In the case of auditory verbal hallucinations, it is thought that a breakdown along the efference copy and forward model route creates a mismatch between what is expected and what is observed, leading to the experience that speech is not produced by oneself.",
            "score": 178.65057373046875
        },
        {
            "docid": "1187487_6",
            "document": "Sensorineural hearing loss . Frequent symptoms of SNHL are loss of acuity in distinguishing foreground voices against noisy backgrounds, difficulty understanding on the telephone, some kinds of sounds seeming excessively loud or shrill, difficulty understanding some parts of speech (fricatives and sibilants), loss of directionality of sound (especially with high frequency sounds), perception that people mumble when speaking, and difficulty understanding speech. Similar symptoms are also associated with other kinds of hearing loss; audiometry or other diagnostic tests are necessary to distinguish sensorineural hearing loss.",
            "score": 174.97174072265625
        },
        {
            "docid": "23060403_12",
            "document": "Motor theory of speech perception . Using a speech synthesizer, speech sounds can be varied in place of articulation along a continuum from to to , or in voice onset time on a continuum from to (for example). When listeners are asked to discriminate between two different sounds, they perceive sounds as belonging to discrete categories, even though the sounds vary continuously. In other words, 10 sounds (with the sound on one extreme being and the sound on the other extreme being , and the ones in the middle varying on a scale) may all be acoustically different from one another, but the listener will hear all of them as either or . Likewise, the English consonant may vary in its acoustic details across different phonetic contexts (the /d/ in does not technically sound the same as the one in , for example), but all 's as perceived by a listener fall within one category (voiced alveolar plosive) and that is because \"linguistic representations are abstract, canonical, phonetic segments or the gestures that underlie these segments.\" This suggests that humans identify speech using categorical perception, and thus that a specialized module, such as that proposed by the motor theory of speech perception, may be on the right track.",
            "score": 174.59063720703125
        },
        {
            "docid": "23158496_4",
            "document": "Superior temporal sulcus . In individuals without autism, the superior temporal sulcus also activates when hearing human voices. It is thought to be a source of sensory encoding linked to motor output through the superior parietal-temporal areas of the brain inferred from the time course of activation. The conclusion of pertinence to vocal processing can be drawn from data showing that the regions of the STS (superior temporal sulcus) are more active when people are listening to vocal sounds rather than non-vocal environmentally based sounds and corresponding control sounds, which can be scrambled or modulated voices. These experimental results indicate the involvement of the STS in the areas of speech and language recognition.",
            "score": 174.23367309570312
        },
        {
            "docid": "1187487_63",
            "document": "Sensorineural hearing loss . Profound or total hearing loss may be amenable to management by cochlear implants, which stimulate cochlear nerve endings directly. A cochlear implant is surgical implantation of a battery powered electronic medical device in the inner ear. Unlike hearing aids, which make sounds louder, cochlear implants do the work of damaged parts of the inner ear (cochlea) to provide sound signals to the brain. These consist of both internal implanted electrodes and magnets and external components. The quality of sound is different than natural hearing but may enable the recipient to better recognize speech and environmental sounds. Because of risk and expense, such surgery is reserved for cases of severe and disabling hearing impairment",
            "score": 173.98191833496094
        },
        {
            "docid": "11920671_6",
            "document": "Machine perception . Machine hearing, also known as machine listening or computer audition, is the ability of a computer or machine to take in and process sound data such as music or speech. This area has a wide range of application including music recording and compression, speech synthesis, and speech recognition. Moreover, this technology allows the machine to replicate the human brain\u2019s ability to selectively focus in a specific sound against many other competing sounds and background noise. This particular ability is called \u201cauditory scene analysis\u201d. The technology enables the machine to segment several streams occurring at the same time. Many commonly used devices such as a smartphones, voice translators, and cars make use of some form of machine hearing.",
            "score": 169.6311492919922
        },
        {
            "docid": "12563101_29",
            "document": "Speech production . The physical structure of the human nose, throat, and vocal cords allows for the productions of many unique sounds, these areas can be further broken down into places of articulation. Different sounds are produced in different areas, and with different muscles and breathing techniques. Our ability to utilize these skills to create the various sounds needed to communicate effectively is essential to our speech production. Speech is a psychomotor activity. Speech between two people is a conversation - they can be casual, formal, factual, or transactional, and the language structure/ narrative genre employed differs depending upon the context. Affect is a significant factor that controls speech, manifestations that disrupt memory in language use due to affect include feelings of tension, states of apprehension, as well as physical signs like nausea. Language level manifestations that affect brings could be observed with the speaker's hesitations, repetitions, false starts, incompletion, syntactic blends, etc. Difficulties in manner of articulation can contribute to speech difficulties and impediments. It is suggested that infants are capable of making the entire spectrum of possible vowel and consonant sounds. IPA has created a system for understanding and categorizing all possible speech sounds, which includes information about the way in which the sound is produced, and where the sounds is produced. This is extremely useful in the understanding of speech production because speech can be transcribed based on sounds rather than spelling, which may be misleading depending on the language being spoken. Average speaking rates are in the 120 to 150 words per minute (wpm) range, and same is the recommended guidelines for recording audiobooks. As people grow accustomed to a particular language they are prone to lose not only the ability to produce certain speech sounds, but also to distinguish between these sounds.",
            "score": 166.1446533203125
        },
        {
            "docid": "5366050_62",
            "document": "Speech perception . The fuzzy logical theory of speech perception developed by Dominic Massaro proposes that people remember speech sounds in a probabilistic, or graded, way. It suggests that people remember descriptions of the perceptual units of language, called prototypes. Within each prototype various features may combine. However, features are not just binary (true or false), there is a fuzzy value corresponding to how likely it is that a sound belongs to a particular speech category. Thus, when perceiving a speech signal our decision about what we actually hear is based on the relative goodness of the match between the stimulus information and values of particular prototypes. The final decision is based on multiple features or sources of information, even visual information (this explains the McGurk effect). Computer models of the fuzzy logical theory have been used to demonstrate that the theory's predictions of how speech sounds are categorized correspond to the behavior of human listeners.",
            "score": 165.88970947265625
        },
        {
            "docid": "1605494_6",
            "document": "Auditory imagery . The auditory imagery developed from lyrics or words generally is also considered a part of inner speech. When people image their voice or the voices of others it is considered inner speech but some researchers argue that it is a lack of self-monitoring of speech. This generally refers to imagining speech which can occur when trying to remember what someone said or the sound of their voice which can be elicited voluntarily or involuntarily. Auditory verbal imagery is considered useful for practicing and organizing things people would like to say in person. For instance, practicing a speech or getting ready to sing a part in a song.",
            "score": 164.9967041015625
        },
        {
            "docid": "1002770_7",
            "document": "Vestibular schwannoma . The first symptom in 90% of those with an acoustic neuroma is unexplained unilateral sensorineural hearing loss, meaning there is damage to the inner ear (cochlea) or nerve pathways from the inner ear to the brain. It involves a reduction in sound level, speech understanding and hearing clarity. In about 70 percent of cases there is a high frequency pattern of loss. The loss of hearing is usually subtle and worsens slowly, although occasionally a sudden loss of hearing may occur(i.e. sudden deafness). Hearing loss can vary from mild hearing loss to complete deafness.",
            "score": 164.3526611328125
        },
        {
            "docid": "744997_22",
            "document": "Electronic voice phenomenon . Skeptics such as David Federlein, Chris French, Terence Hines and Michael Shermer say that EVP are usually recorded by raising the \"noise floor\" \u2013 the electrical noise created by all electrical devices \u2013 in order to create white noise. When this noise is filtered, it can be made to produce noises which sound like speech. Federlein says that this is no different from using a wah pedal on a guitar, which is a focused sweep filter which moves around the spectrum and creates open vowel sounds. This, according to Federlein, sounds exactly like some EVP. This, in combination with such things as cross modulation of radio stations or faulty ground loops can cause the impression of paranormal voices. The human brain evolved to recognize patterns, and if a person listens to enough noise the brain will detect words, even when there is no intelligent source for them. Expectation also plays an important part in making people believe they are hearing voices in random noise.",
            "score": 163.5381622314453
        },
        {
            "docid": "12689044_5",
            "document": "Speech-to-text reporter . Voice writers enjoy very high accuracy rates, based upon pure physiology. The route taken by a person's words goes from the mouth to the reporter's ear, brain, and \"inner\" voice. This form of repetition is naturally effortless; it is what we all do in our daily conversation, as we listen to a person speak, or when we read a book. So the most natural extension of this process is to psychologically switch the repetition mechanism from \"inner voice\" to the physiological \"spoken voice.\" Therefore, we minimize the introduction of cognitive overhead in our task of routing the spoken word to its permanent destination as printed words. This streamlined process allows voice writers to achieve excellent performance for many continuous hours and greater than 98 percent accuracy at speeds as high as 350 words per minute.",
            "score": 161.55853271484375
        },
        {
            "docid": "418355_25",
            "document": "Babbling . The human mouth moves in distinct ways during speech production. When producing each individual sound out loud, humans use different parts of their mouths, as well as different methods to produce particular sounds. During the beginnings of babbling, infants tend to have greater mouth openings on the right side. This finding suggests that babbling is controlled by the left hemisphere of the brain. The larynx, or voicebox, is originally high in the throat which allows the baby to continue to breathe while swallowing. It descends during the first year of life, allowing the pharynx to develop and facilitates the production of adult-like speech sounds.",
            "score": 161.4484405517578
        },
        {
            "docid": "14339999_6",
            "document": "Virtual pitch . Terhardt rejected the idea of periodicity pitch, because it was not consistent with empirical data on pitch perception, e.g. measurements of the gradual shift of the virtual pitch of a complex tone with a missing fundamental when the partials were gradually shifted. Terhardt instead broke pitch perception into two steps: auditory frequency analysis in the inner ear, and harmonic pitch pattern recognition in the brain. The inner ear effectively performs a running frequency analysis of incoming sounds - otherwise we would not be able to hear out spectral pitches within a complex tone. Physiologically, each spectral pitch depends on both temporal and spectral aspects (i.e. periodicity of the waveform and position of excitation on the basilar membrane), but in Terhardt's approach the spectral pitch itself is a purely experiential parameter, not a physical parameter: it is the outcome of a psychoacoustical experiment in which the conscious listener plays an active role. Psychoacoustic measurements and models can predict which partials are \"perceptually relevant\" in a given complex tone; they are perceptually relevant if you can hear a difference in the whole sound if the frequency or amplitude of a partial is changed). The ear has evolved to separate spectral frequencies, because due to reflection and superposition in everyday environments spectral frequencies are more reliably carriers of environmental information than spectral amplitudies, which in turn are more reliable carriers of environmentally relevant information than phase relationships between partials (when perceived monoaurally). On this basis, Terhardt proposed that spectral pitches - which are what the listener experiences when hearing out partials (as opposed to the physical partials themselves) - are the only information available to the brain for the purpose of extracting virtual pitches. The \"pitch extraction\" process then involves the recognition of incomplete harmonic patterns and happens in neural networks.",
            "score": 160.29824829101562
        },
        {
            "docid": "25140_47",
            "document": "Perception . \"Speech perception\" is the process by which spoken languages are heard, interpreted and understood. Research in speech perception seeks to understand how human listeners recognize speech sounds and use this information to understand spoken language. The sound of a word can vary widely according to words around it and the tempo of the speech, as well as the physical characteristics, accent and mood of the speaker. Listeners manage to perceive words across this wide range of different conditions. Another variation is that reverberation can make a large difference in sound between a word spoken from the far side of a room and the same word spoken up close. Experiments have shown that people automatically compensate for this effect when hearing speech.",
            "score": 160.2519073486328
        },
        {
            "docid": "6273470_8",
            "document": "Zen the Intergalactic Ninja . Zen also communicates through a unique type of telepathy that nobody truly understands. He is able to simply \"talk\" into your head, allowing you to hear him as though he was simply speaking out loud. When you speak back to him, he \"hears\" you though a combination of sound waves and thought patterns that he takes in as language and sound. Zen has an amazing awareness of his surroundings thanks to the fact that he \"hears\" more with his \"mind\" more than we could take in with just our ears. This ability does not mean he can \"read your thoughts\" on a whim though. You have to be projecting or transmitting your thoughts or intentions in order for him to be able to receive them\u2014so if you can clear your mind, you \"might\" be able to keep him in the dark...maybe.",
            "score": 159.2701873779297
        },
        {
            "docid": "32607746_2",
            "document": "Imagined speech . Imagined speech (silent speech or covert speech) is thinking in the form of sound \u2013 \u201chearing\u201d one\u2019s own voice silently to oneself, without the intentional movement of any extremities such as the lips, tongue, or hands. Logically, imagined speech has been possible since the emergence of language, however, the phenomenon is most associated with the signal processing and detection within electroencephalograph (EEG) data as well as data obtained using alternative non-invasive, brain\u2013computer interface (BCI) devices.",
            "score": 159.06053161621094
        },
        {
            "docid": "9736296_43",
            "document": "Linguistic performance . Unacceptable Sentences are ones which, although are grammatical, are not considered proper utterances. They are considered unacceptable due to the lack of our cognitive systems to process them. Speakers and listeners can be aided in the performance and processing of these sentences by eliminating time and memory constraints, increasing motivation to process these utterances and using pen and paper. In English there are three types of sentences that are grammatical but are considered unacceptable by speakers and listeners. When a speaker makes an utterance they must translate their ideas into words, then syntactically proper phrases with proper pronunciation. The speaker must have prior world knowledge and an understanding of the grammatical rules that their language enforces. When learning a second language or with children acquiring their first language, speakers usually have this knowledge before they are able to produce them. Their speech is usually slow and deliberate, using phrases they have already mastered, and with practice their skills increase. Errors of linguistic performance are judged by the listener giving many interpretations if an utterance is well-formed or ungrammatical depending on the individual. As well the context in which an utterance is used can determine if the error would be considered or not. When comparing \"Who must telephone her?\" and \"Who need telephone her?\" the former would be considered the ungrammatical phrase. However, when comparing it to \"Who want telephone her?\" it would be considered the grammatical phrase. The listener may also be the speaker. When repeating sentences with errors if the error is not comprehended then it is performed. As well if the speaker does notice the error in the sentence they are supposed to repeat they are unaware of the difference between their well-formed sentence and the ungrammatical sentence. An unacceptable utterance can also be performed due to a brain injury. Three types of brain injuries that could cause errors in performance were studied by Fromkin are dysarthria, apraxia and literal paraphasia. Dysarthria is a defect in the neuromuscular connection that involves speech movement. The speech organs involved can be paralyzed or weakened, making it difficult or impossible for the speaker to produce a target utterance. Apraxia is when there is damage to the ability to initiate speech sounds with no paralysis or weakening of the articulators. Literal paraphasia causes disorganization of linguistic properties, resulting in errors of word order of phonemes. Having a brain injury and being unable to perform proper linguistic utterances, some individuals are still able to process complex sentences and formulate syntactically well formed sentences in their mind. Child productions when they are acquiring language are full of errors of linguistic performance. Children must go from imitating adult speech to create new phrases of their own. They will need to use their cognitive operations of the knowledge of their language they are learning to determine the rules and properties of that language. The following are examples of errors in English speaking children's productions.",
            "score": 158.99111938476562
        },
        {
            "docid": "37691878_2",
            "document": "Phonemic restoration effect . Phonemic restoration effect is a perceptual phenomenon where under certain conditions, sounds actually missing from a speech signal can be restored by the brain and may appear to be heard. The effect occurs when missing phonemes in an auditory signal are replaced with a noise that would have the physical properties to mask those phonemes, creating an ambiguity. In such ambiguity, the brain tends towards filling in absent phonemes. The effect can be so strong that some listeners may not even notice that there are phonemes missing. This effect is commonly observed in a conversation with heavy background noise, making it difficult to properly hear every phoneme being spoken. Different factors can change the strength of the effect, including how rich the context or linguistic cues are in speech, as well as the listener's state, such as their hearing status or age.",
            "score": 158.11209106445312
        },
        {
            "docid": "30459919_2",
            "document": "Speech Buddies . Speech Buddies are a series of speech therapy tools that are used to remediate articulation and speech sound disorders using the widely accepted teaching method of tactile feedback. Articulation, or speech sound disorders occur when a person has difficulty producing a sound correctly. Sounds may be left off, substituted, added or changed, making it difficult to be understood. Often occurring when children are learning to speak, these errors are considered a disorder if they persist after a certain developmental age. A developmental age at which a child should be able to correctly produce a certain sound is different depending on the difficulty of the sound. The B-sound is one of the first mastered, while the R-sound is more difficult and may not be mastered until several years later. Most articulation disorders are of no known cause, though many can be attributed to other disorders such as autism, or hearing impairment.",
            "score": 157.63536071777344
        },
        {
            "docid": "12328438_2",
            "document": "Auditory processing disorder . Auditory processing disorder (APD), also known as central auditory processing disorder (CAPD), is an umbrella term for a variety of disorders that affect the way the brain processes auditory information. Individuals with APD usually have normal structure and function of the outer, middle and inner ear (peripheral hearing). However, they cannot process the information they hear in the same way as others do, which leads to difficulties in recognizing and interpreting sounds, especially the sounds composing speech. It is thought that these difficulties arise from dysfunction in the central nervous system.",
            "score": 157.13125610351562
        },
        {
            "docid": "236809_36",
            "document": "Recall (memory) . Recall memory is linked with instincts and mechanisms in order to remember how an event happened to learn from it or avoid the agitator, connections are made with emotions. For instance, if a speaker is very calm and neutral, the effectiveness of encoding memory is very low and listeners get the gist of what the speaker is discussing. On the other hand, if a speaker is shouting and/or using emotionally driven words, listeners tend to remember key phrases and the meaning of the speech. This is full access of the fight or flight mechanism all people have functioning in the brain, but based on what triggers this mechanism will lead to better recall of it. People tend to focus their attention on cues that are loud, very soft, or something unusual. This makes the auditory system pick up the differences in regular speaking and meaningful speech, when something significant is spoken in the discussion people home in on the message at that part of the speech but tend to lose the other part of the discussion. Our brains sense differences in speech and when those differences occur the brain encodes that part of speech into memory and the information can be recalled for future reference.",
            "score": 156.5924530029297
        },
        {
            "docid": "608611_27",
            "document": "Lord Dunmore's War . On 5 May 1774, the Shawanee delivered the following message: \"Brothers: \"[directed at Captain Connolly, Mr. McKee, and Mr. Croghan]\" We have received your Speeches by White Eyes, and as to what Mr. Croghan and Mr. McKee says, we look upon it all to be lies, and perhaps what you say may be lies also, but as it is the first time you have spoke to us we listen to you, and expect that what we may hear from you will be more confined to truth than what we usually hear from the white people. It is you who are frequently passing up and down the Ohio, and making settlements upon it, and as you have informed as that your wise people have met together to consult upon this matter, we desire you to be strong and consider it well. Brethren: We see you speak to us at the head of your warriors, who you have collected together at sundry places upon this river, where we understand they are building forts, and as you have requested us to listen to you, we will do it, but in the same manner that you appear to speak to us. Our people at the Lower Towns have no Chiefs among them, but are all warriors, and are also preparing themselves to be in readiness, that they may be better able to hear what you have to say...You tell us not to take any notice of what your people have done to us; we desire you likewise not to take any notice of what our young men may now be doing, and as no doubt you can command your warriors when you desire them to listen to you, we have reason to expect that ours will take the same advice when we require it, that is, when we have heard from the Governour \"[sic]\" of Virginia.\"\u2014American Archives, Fourth Series, Vol. 1. p.\u00a0479.",
            "score": 155.97975158691406
        },
        {
            "docid": "14241792_3",
            "document": "TRACE (psycholinguistics) . TRACE was created during the formative period of connectionism, and was included as a chapter in \"Parallel Distributed Processing: Explorations in the Microstructures of Cognition\". The researchers found that certain problems regarding speech perception could be conceptualized in terms of a connectionist interactive activation model. The problems were that (1) speech is extended in time, (2) the sounds of speech (phonemes) overlap with each other, (3) the articulation of a speech sound is affected by the sounds that come before and after it, and (4) there is natural variability in speech (e.g. foreign accent) as well as noise in the environment (e.g. busy restaurant). Each of these causes the speech signal to be complex and often ambiguous, making it difficult for the human mind/brain to decide what words it is really hearing. In very simple terms, an interactive activation model solves this problem by placing different kinds of processing units (phonemes, words) in isolated layers, allowing activated units to pass information between layers, and having units within layers compete with one another, until the \u201cwinner\u201d is considered \u201crecognized\u201d by the model.",
            "score": 155.21282958984375
        },
        {
            "docid": "6541938_8",
            "document": "Lexical-gustatory synesthesia . SC is a synesthete who automatically experiences smells, tastes, and feelings of textures in her mouth and throat when she reads, speaks, or hears language, music, and certain environmental sounds. In SC\u2019s case study, researchers utilized fMRI to determine the areas of the brain that were activated during her synesthetic experiences. They compared areas of activation in SC\u2019s brain to those found in literature for other synesthetes, speech processing, language, and sound processing. In SC\u2019s scans, two important regions of the brain were largely activated during her taste sensations: the left anterior insula and the left superior parietal lobe. The scans led researchers to speculate that the anterior insula may play a role in SC\u2019s taste experiences while the superior parietal lobe binds together all of the sensory information for processing. Based off the findings of this study and others like it, it could be possible to determine the type of inducer that leads to synesthetic sensations based on the patterns of brain activity.",
            "score": 154.94140625
        },
        {
            "docid": "6894544_2",
            "document": "Noise-induced hearing loss . Noise-induced hearing loss (NIHL) is hearing impairment resulting from exposure to loud sound. People may have a loss of perception of a narrow range of frequencies, impaired cognitive perception of sound including sensitivity to sound or ringing in the ears. When exposure to hazards such as noise occur at work and is associated with hearing loss, it is referred to as occupational hearing loss.  Hearing may deteriorate gradually from chronic and repeated noise exposure, such as to loud music or background noise, or suddenly, from exposure to impulse noise (a short high intensity noise), such as a gunshot or airhorn. In both types, loud sound overstimulates delicate hearing cells, leading to the permanent injury or death of the cells. Once lost this way, hearing cannot be restored in humans.  There are a variety of prevention strategies available to avoid or reduce hearing loss. Lowering the volume of sound at its source, limiting the time of exposure and physical protection can reduce the impact of excessive noise. If not prevented, hearing loss can be managed through assistive devices and cognitive therapies. The largest burden of NIHL has been through occupational exposures; however, noise-induced hearing loss can also be due to unsafe recreational, residential, social and military service-related noise exposures. It is estimated that 15% of young people are exposed to sufficient leisure noises (i.e. concerts, sporting events, daily activities, personal listening devices, etc.) to cause NIHL. There is not a limited list of noise sources that can cause hearing loss; rather, it is important to understand that exposure to excessively high decibel (dB) levels from any sound source over time, can cause hearing loss. The first symptom of NIHL may be difficulty hearing a conversation against a noisy background. The effect of hearing loss on speech perception has two components. The first component is the loss of audibility, which may be perceived as an overall decrease in volume. Modern hearing aids compensate this loss with amplification. The second component is known as \u201cdistortion\" or \u201cclarity loss\u201d due to selective frequency loss.\u201d Consonants, due to their higher frequency, are typically affected first. For example, the sounds \u201cs\u201d and \u201ct\u201d are often difficult to hear for those with hearing loss, affecting clarity of speech. NIHL can affect either one or both ears. Monaural hearing loss causes problems with directional hearing, affecting the ability to localize sound.",
            "score": 154.76123046875
        },
        {
            "docid": "54133326_3",
            "document": "WaveNet . Generating speech from text is an increasingly common task thanks to the popularity of software such as Apple's Siri, Microsoft\u2019s Cortana, Amazon Alexa and the Google Assistant.  Most such systems use a variation of a technique that involves concatenated sound fragments together to form recognisable sounds and words. The most common of these is called concatenative TTS. It consists of large library of speech fragments, recorded from a single speaker that are then concatenated to produce complete words and sounds. The result sounds unnatural, with an odd cadence and tone. The reliance on a recorded library also makes it difficult to modify or change the voice. Another technique, known as parametric TTS, uses mathematical models to recreate sounds that are then assembled into words and sentences. The information required to generate the sounds is stored in the parameters of the model. The characteristics of the output speech are controlled via the inputs to the model, while the speech is typically created using a voice synthesiser known as a vocoder. This can also result in unnatural sounding audio. WaveNet is a type of feedforward neural network known as a deep convolutional neural network (CNN). These consist of layers of interconnected nodes somewhat analogous to the brain\u2019s neurons. The CNN takes a raw signal as an input and synthesises an output one sample at a time.  In the 2016 paper, the network was fed real waveforms of speech in English and Mandarin. As these pass through the network, it learns a set of rules to describe how the audio waveform evolves over time. The trained network can then be used to create new speech-like waveforms at 16,000 samples per second. These waveforms include realistic breaths and lip smacks - but do not conform to any language.  WaveNet is able to accurately model different voices, with the accent and tone of the input correlating with the output. For example, if it is trained with German, it produces German speech. This ability to clone voices has raised ethical concerns about WaveNets ability to mimic the voices of living persons.  The capability also means that if the WaveNet is fed other inputs - such as music - its output will be musical. At the time of its release, DeepMind showed that WaveNet could produce waveforms that sound like classical music.",
            "score": 154.73171997070312
        },
        {
            "docid": "101970_2",
            "document": "Tinnitus . Tinnitus is the hearing of sound when no external sound is present. While often described as a ringing, it may also sound like a clicking, hiss or roaring. Rarely, unclear voices or music are heard. The sound may be soft or loud, low pitched or high pitched and appear to be coming from one ear or both. Most of the time, it comes on gradually. In some people, the sound causes depression or anxiety and can interfere with concentration. Tinnitus is not a disease but a symptom that can result from a number of underlying causes. One of the most common causes is noise-induced hearing loss. Other causes include ear infections, disease of the heart or blood vessels, M\u00e9ni\u00e8re's disease, brain tumors, emotional stress, exposure to certain medications, a previous head injury, and earwax. It is more common in those with depression. The diagnosis of tinnitus is usually based on the person's description. A number of questionnaires exist that may help to assess how much tinnitus is interfering with a person's life. The diagnosis is commonly supported by an audiogram and a neurological examination. If certain problems are found, medical imaging, such as with MRI, may be performed. Other tests are suitable when tinnitus occurs with the same rhythm as the heartbeat. Rarely, the sound may be heard by someone else using a stethoscope, in which case it is known as objective tinnitus. Spontaneous otoacoustic emissions, which are sounds produced normally by the inner ear, may also occasionally result in tinnitus. Prevention involves avoiding loud noise. If there is an underlying cause, treating it may lead to improvements. Otherwise, typically, management involves talk therapy. Sound generators or hearing aids may help some. As of 2013, there were no effective medications. It is common, affecting about 10\u201315% of people. Most, however, tolerate it well, and it is a significant problem in only 1\u20132% of people. The word tinnitus is from the Latin \"tinn\u012bre\" which means \"to ring\". Tinnitus can be perceived in one or both ears or in the head. It is the description of a noise inside a person\u2019s head in the absence of auditory stimulation. The noise can be described in many different ways. It is usually described as a ringing noise but, in some patients, it takes the form of a high-pitched whining, electric buzzing, hissing, humming, tinging or whistling sound or as ticking, clicking, roaring, \"crickets\" or \"tree frogs\" or \"locusts (cicadas)\", tunes, songs, beeping, sizzling, sounds that slightly resemble human voices or even a pure steady tone like that heard during a hearing test. It has also been described as a \"whooshing\" sound because of acute muscle spasms, as of wind or waves. Tinnitus can be intermittent or continuous: in the latter case, it can be the cause of great distress. In some individuals, the intensity can be changed by shoulder, head, tongue, jaw or eye movements. Most people with tinnitus have some degree of hearing loss.",
            "score": 154.5713348388672
        }
    ]
}