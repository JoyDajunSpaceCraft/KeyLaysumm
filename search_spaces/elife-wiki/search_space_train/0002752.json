{
    "q": [
        {
            "docid": "32116125_4",
            "document": "Amblyaudia . Amblyaudia is a deficit in binaural integration of environmental information entering the auditory system. It is a disorder related to brain organization and function rather than what is typically considered a \u201chearing loss\u201d (damage to the cochlea). It may be genetic or developmentally acquired or both. When animals are temporarily deprived of hearing from an early age, profound changes occur in the brain. Specifically, cell sizes in brainstem nuclei are reduced, the configuration of brainstem dendrites are altered and neurons respond in different ways to sounds presented to both the deprived and non-deprived ears (in cases of asymmetric deprivation). This last point is particularly important for listening tasks that require inputs from two ears to perform well. There are multiple auditory functions that rely on the computation of well calibrated inputs from the two ears. Chief among these is the ability to localize sound sources and separate what we want to hear from a background of noise. In the brainstem, the auditory system compares the timing and levels of sounds between the two ears to encode the location of sound sources (sounds that originate from our right as opposed to left side are louder and arrive earlier in our right ear). This ability to separate sound sources not only helps us locate the trajectories of moving objects, but also to separate different sound sources in noisy environments.",
            "score": 120.9615980386734
        },
        {
            "docid": "30176398_23",
            "document": "Golding Bird . The required direction of current depended on the direction in which electric current was thought to flow in nerves in the human or animal body. For motor functions, for instance, the flow was taken to be from the centre towards the muscles at the extremities, so artificial electrical stimulation needed to be in the same direction. For sensory nerves, the opposite applied: flow was from the extremity to the centre, and the positive electrode would be applied to the extremity. This principle was demonstrated by Bird in an experiment with a living frog. A supply of frogs was usually on hand, as they were used in the frog galvanoscope. The electromagnetic galvanometer was available at the time, but frogs' legs were still used by Bird because of their much greater sensitivity to small currents. In the experiment, the frog's leg was almost completely severed from its body, leaving only the sciatic nerve connected, and electric current was then applied from the body to the leg. Convulsions of the leg were seen when the muscle was stimulated. Reversing the current, however, produced no movement of the muscle, merely croaks of pain from the frog. In his lectures, Bird describes many experiments with a similar aim on human sensory organs. In one experiment by Grapengiesser, for instance, electric current is passed through the subject's head from ear to ear, causing a sound to be hallucinated. The ear connected to the positive terminal hears a louder sound than that connected to the negative.",
            "score": 166.72312724590302
        },
        {
            "docid": "35988494_3",
            "document": "Selective auditory attention . The cocktail party problem was first brought up in 1953 by Colin Cherry. This common problem is how our minds solves the issue of knowing what in the auditory scene is important and combining those in a coherent whole, such as the problem of how we can perceive our friend talking in the midst of a crowded cocktail party. He suggested that the auditory system can filter sounds being heard. Physical characteristics of the auditory information such as speaker's voice or location can improve a person's ability to focus on certain stimuli even if there is other auditory stimuli present. Cherry also did work with shadowing which involves different information being played into both ears and only one ear's information can be processed and remembered (Eysneck, 2012, p.\u00a084). Another psychologist, Albert Bregman, came up with the auditory scene analysis model. The model has three main characteristics: segmentation, integration, and segregation. Segmentation involves the division of auditory messages into segments of importance. The process of combining parts of an auditory message to form a whole is associated with integration. Segregation is the separation of important auditory messages and the unwanted information in the brain. It is important to note that Bregman also makes a link back to the idea of perception. He states that it is essential for one to make a useful representation of the world from sensory inputs around us. Without perception, an individual will not recognize or have the knowledge of what is going on around them. While Begman's seminal work is critical to understanding selective auditory attention, his studies did not focus on the way in which an auditory message is selected, if and when it was correctly segregated from other sounds in a mixture, which is a critical stage of selective auditory attention. Inspired in part by Bregman's work, a number of researchers then set out to link directly work on auditory scene analysis to the processes governing attention, including Maria Chait, Mounya Elhilali, Shihab Shamma, and Barbara Shinn-Cunningham.",
            "score": 97.96036279201508
        },
        {
            "docid": "32116125_2",
            "document": "Amblyaudia . Amblyaudia (amblyos- blunt; audia-hearing) is a term coined by Dr. Deborah Moncrieff from the University of Pittsburgh to characterize a specific pattern of performance from dichotic listening tests. Dichotic listening tests are widely used to assess individuals for binaural integration, a type of auditory processing skill. During the tests, individuals are asked to identify different words presented simultaneously to the two ears. Normal listeners can identify the words fairly well and show a small difference between the two ears with one ear slightly dominant over the other. For the majority of listeners, this small difference is referred to as a \"right-ear advantage\" because their right ear performs slightly better than their left ear. But some normal individuals produce a \"left-ear advantage\" during dichotic tests and others perform at equal levels in the two ears. Amblyaudia is diagnosed when the scores from the two ears are significantly different with the individual's dominant ear score much higher than the score in the non-dominant ear  Researchers interested in understanding the neurophysiological underpinnings of amblyaudia consider it to be a brain based hearing disorder that may be inherited or that may result from auditory deprivation during critical periods of brain development. Individuals with amblyaudia have normal hearing sensitivity (in other words they hear soft sounds) but have difficulty hearing in noisy environments like restaurants or classrooms. Even in quiet environments, individuals with amblyaudia may fail to understand what they are hearing, especially if the information is new or complicated. Amblyaudia can be conceptualized as the auditory analog of the better known central visual disorder amblyopia. The term \u201clazy ear\u201d has been used to describe amblyaudia although it is currently not known whether it stems from deficits in the auditory periphery (middle ear or cochlea) or from other parts of the auditory system in the brain, or both. A characteristic of amblyaudia is suppression of activity in the non-dominant auditory pathway by activity in the dominant pathway which may be genetically determined and which could also be exacerbated by conditions throughout early development.",
            "score": 124.18167746067047
        },
        {
            "docid": "37759941_5",
            "document": "Crossmodal attention . While research on cross-modal attention has found that deficits in attending often occur, this research has led to a better understanding of attentional processing. Some studies have used positron emission tomography (PET) to examine the neurological basis for how we selectively attend to information using different sensory modalities. Event related potentials (ERPs). have also been used to help researchers measure how humans encode and process attended information in the brain. By increasing our understanding of modality-specific and cross-modal attention, we are better able to understand how we think and direct our attention.",
            "score": 53.1187686920166
        },
        {
            "docid": "2357280_6",
            "document": "The Mount (novel) . The Hoots are a race that have evolved from prey. They are herbivores who have developed a set of very keen senses which allowed them to tame and use predators as mounts. The need for mounts is due to the fact that Hoots have very weak leg muscles, which prevents them from moving about efficiently. Most Hoots have never developed their leg muscles and use either mounts or small bicycle devices to move about. Hoots have very large and strong hands which were evolved for strangling predators. They also have large eyes and very large ears. The ears are used as a way of expressing emotions. For example, when a Hoot laughs, their ears flap up and down.",
            "score": 97.08375382423401
        },
        {
            "docid": "191884_54",
            "document": "Headphones . Although modern headphones have been particularly widely sold and used for listening to stereo recordings since the release of the Walkman, there is subjective debate regarding the nature of their reproduction of stereo sound. Stereo recordings represent the position of horizontal depth cues (stereo separation) via volume and phase differences of the sound in question between the two channels. When the sounds from two speakers mix, they create the phase difference the brain uses to locate direction. Through most headphones, because the right and left channels do not combine in this manner, the illusion of the phantom center can be perceived as lost. Hard panned sounds are also heard only in one ear rather than from one side.",
            "score": 108.33899140357971
        },
        {
            "docid": "41045246_11",
            "document": "Sound map . Sound and space are closely linked. Our ears help define our surroundings by picking up on spatial clues in reflected sound waves. This innate ability to situate ourselves in our soundscape was probably more overtly useful in the days before electricity, when we had to rely on our ears to alert us to danger our eyes could not detect. There is, however, a movement in the visually impaired community to cultivate this ability to help them navigate in the world and participate in sports, and artists such as Janet Cardiff use sound and spatiality as integral parts of their work (see The Forty Part Motet).",
            "score": 121.97901630401611
        },
        {
            "docid": "1021754_31",
            "document": "Sound localization . The human outer ear, i.e. the structures of the pinna and the external ear canal, form direction-selective filters. Depending on the sound input direction in the median plane, different filter resonances become active. These resonances implant direction-specific patterns into the frequency responses of the ears, which can be evaluated by the auditory system (directional bands) for vertical sound localization. Together with other direction-selective reflections at the head, shoulders and torso, they form the outer ear transfer functions. These patterns in the ear's frequency responses are highly individual, depending on the shape and size of the outer ear. If sound is presented through headphones, and has been recorded via another head with different-shaped outer ear surfaces, the directional patterns differ from the listener's own, and problems will appear when trying to evaluate directions in the median plane with these foreign ears. As a consequence, front\u2013back permutations or inside-the-head-localization can appear when listening to dummy head recordings, or otherwise referred to as binaural recordings. It has been shown that human subjects can monaurally localize high frequency sound but not low frequency sound. Binaural localization, however, was possible with lower frequencies. This is likely due to the pinna being small enough to only interact with sound waves of high frequency. It seems that people can only accurately localize the elevation of sounds that are complex and include frequencies above 7,000\u00a0Hz, and a pinna must be present.",
            "score": 161.96685791015625
        },
        {
            "docid": "380541_30",
            "document": "Skeletal muscle . The electrical activity associated with muscle contraction are measured via electromyography (EMG). EMG is a common technique used in many disciplines within the Exercise and Rehab Sciences. Skeletal muscle has two physiological responses: relaxation and contraction. The mechanisms for which these responses occur generate electrical activity measured by EMG. Specifically, EMG can measure the action potential of a skeletal muscle, which occurs from the hyperpolarization of the motor axons from nerve impulses sent to the muscle (1). EMG is used in research for determining if the skeletal muscle of interest is being activated, the amount of force generated, and an indicator of muscle fatigue. The two types of EMG are intra-muscular EMG and the most common, surface EMG. The EMG signals are much greater when a skeletal muscle is contracting verses relaxing. However, for smaller and deeper skeletal muscles the EMG signals are reduced and therefore are viewed as a less valued technique for measuring the activation. In research using EMG, a maximal voluntary contraction (MVC) is commonly performed on the skeletal muscle of interest, to have reference data for the rest of the EMG recordings during the main experimental testing for that same skeletal muscle.",
            "score": 74.82207012176514
        },
        {
            "docid": "3246329_10",
            "document": "Hearing range . Cats have excellent hearing and can detect an extremely broad range of frequencies. They can hear higher-pitched sounds than humans or most dogs, detecting frequencies from 55\u00a0Hz up to 79\u00a0kHz. Cats do not use this ability to hear ultrasound for communication but it is probably important in hunting, since many species of rodents make ultrasonic calls. Cat hearing is also extremely sensitive and is among the best of any mammal, being most acute in the range of 500\u00a0Hz to 32\u00a0kHz. This sensitivity is further enhanced by the cat's large movable outer ears (their \"pinnae\"), which both amplify sounds and help a cat sense the direction from which a noise is coming. The hearing ability of a dog is dependent on breed and age, though the range of hearing is usually around 67\u00a0Hz to 45\u00a0kHz. As with humans, some dog breeds' hearing ranges narrow with age, such as the German shepherd and miniature poodle. When dogs hear a sound, they will move their ears towards it in order to maximise reception. In order to achieve this, the ears of a dog are controlled by at least 18 muscles, which allow the ears to tilt and rotate. The ear's shape also allows the sound to be heard more accurately. Many breeds often have upright and curved ears, which direct and amplify sounds.",
            "score": 164.77492666244507
        },
        {
            "docid": "161005_4",
            "document": "Head-related transfer function . Humans have just two ears, but can locate sounds in three dimensions \u2013 in range (distance), in direction above and below, in front and to the rear, as well as to either side. This is possible because the brain, inner ear and the external ears (pinna) work together to make inferences about location. This ability to localize sound sources may have developed in humans and ancestors as an evolutionary necessity, since the eyes can only see a fraction of the world around a viewer, and vision is hampered in darkness, while the ability to localize a sound source works in all directions, to varying accuracy,  regardless of the surrounding light.",
            "score": 134.04542422294617
        },
        {
            "docid": "1021754_32",
            "document": "Sound localization . When the head is stationary, the binaural cues for lateral sound localization (interaural time difference and interaural level difference) do not give information about the location of a sound in the median plane. Identical ITDs and ILDs can be produced by sounds at eye level or at any elevation, as long as the lateral direction is constant. However, if the head is rotated, the ITD and ILD change dynamically, and those changes are different for sounds at different elevations. For example, if an eye-level sound source is straight ahead and the head turns to the left, the sound becomes louder (and arrives sooner) at the right ear than at the left. But if the sound source is directly overhead, there will be no change in the ITD and ILD as the head turns. Intermediate elevations will produce intermediate degrees of change, and if the presentation of binaural cues to the two ears during head movement is reversed, the sound will be heard behind the listener. Hans Wallach artificially altered a sound\u2019s binaural cues during movements of the head. Although the sound was objectively placed at eye level, the dynamic changes to ITD and ILD as the head rotated were those that would be produced if the sound source had been elevated. In this situation, the sound was heard at the synthesized elevation. The fact that the sound sources objectively remained at eye level prevented monaural cues from specifying the elevation, showing that it was the dynamic change in the binaural cues during head movement that allowed the sound to be correctly localized in the vertical dimension. The head movements need not be actively produced; accurate vertical localization occurred in a similar setup when the head rotation was produced passively, by seating the blindfolded subject in a rotating chair. As long as the dynamic changes in binaural cues accompanied a perceived head rotation, the synthesized elevation was perceived.",
            "score": 118.5248076915741
        },
        {
            "docid": "752723_2",
            "document": "Dichotic listening test . The Dichotic listening test is a psychological test commonly used to investigate selective attention within the auditory system and is a subtopic of cognitive psychology and neuroscience. Specifically, it is \"used as a behavioral test for hemispheric lateralization of speech sound perception.\" During a standard dichotic listening test, a participant is presented with two different auditory stimuli simultaneously (usually speech). The different stimuli are directed into different ears over headphones. Research Participants were instructed to repeat aloud the words they heard in one ear while a different message was presented to the other ear. As a result of focusing to repeat the words, participants noticed little of the message to the other ear, often not even realizing that at some point it changed from English to German. At the same time, participants did notice when the voice in the unattended ear changed from a male\u2019s to a female\u2019s, suggesting that the selectivity of consciousness can work to tune in some information.\"",
            "score": 141.25635814666748
        },
        {
            "docid": "31209304_17",
            "document": "Righting reflex . Recently, vestibular reflexes have been investigated using leg rotation experiments. A leg and foot rotation test can be used to investigate changes in neuron activity within the labyrinth, or the inner ear. When the head is rotated while the leg and foot are rotated 90 degrees, the vestibular signals cause the brain to inhibit movement in the direction of the rotation. At the same time, it activates the muscles on the opposite side in an attempt to correct for the displacement.",
            "score": 110.53635096549988
        },
        {
            "docid": "4548229_7",
            "document": "Interaural time difference . Feddersen et al. (1957) also conducted experiments taking measurements on how ITDs alter with changing the azimuth of the loudspeaker around the head at different frequencies. But unlike the Woodworth experiments human subjects were used rather than a model of the head. The experiment results agreed with the conclusion made by Woodworth about ITDs. The experiments also concluded that is there is no difference in ITDs when sounds are provided from directly in front or behind at 0\u00b0 and 180\u00b0 azimuth. The explanation for this is that the sound is equidistant from both ears. Interaural time differences alter as the loudspeaker is moved around the head. The maximum ITD of 660 \u03bcs occurs when a sound source is positioned at 90\u00b0 azimuth to one ear.",
            "score": 125.5981981754303
        },
        {
            "docid": "33193162_14",
            "document": "Vision in fishes . There is a need for some mechanism that stabilises images during rapid head movements. This is achieved by the vestibulo-ocular reflex, which is a reflex eye movement that stabilises images on the retina by producing eye movements in the direction opposite to head movements, thus preserving the image on the centre of the visual field. For example, when the head moves to the right, the eyes move to the left, and vice versa. In many animals, including human beings, the inner ear functions as the biological analogue of an accelerometer in camera image stabilization systems, to stabilize the image by moving the eyes. When a rotation of the head is detected, an inhibitory signal is sent to the extraocular muscles on one side and an excitatory signal to the muscles on the other side. The result is a compensatory movement of the eyes. Typical human eye movements lag head movements by less than 10 ms.",
            "score": 115.22820949554443
        },
        {
            "docid": "1519473_5",
            "document": "Cat anatomy . Thirty-two individual muscles in each ear allow for a manner of directional hearing; a cat can move each ear independently of the other. Because of this mobility, a cat can move its body in one direction and point its ears in another direction. Most cats have straight ears pointing upward. Unlike dogs, flap-eared breeds are extremely rare (\"Scottish Folds\" are one such exceptional mutation). When angry or frightened, a cat will lay back its ears to accompany the growling or hissing sounds it makes. Cats also turn their ears back when they are playing or to listen to a sound coming from behind them. The fold of skin forming a pouch on the lower posterior part of the ear, known as Henry's pocket, is usually prominent in a cat's ear. It is of unknown function, though it may assist in filtering sounds.",
            "score": 167.43492078781128
        },
        {
            "docid": "3154127_4",
            "document": "Virtual acoustic space . When one listens to sounds over headphones (in what is known as the \"closed field\") the sound source appears to arise from center of the head. On the other hand, under normal, so-called free-field, listening conditions sounds are perceived as being externalized. The direction of a sound in space (see sound localization) is determined by the brain when it analyses the interaction of incoming sound with head and external ears. A sound arising to one side reaches the near ear before the far ear (creating an interaural time difference, ITD), and will also be louder at the near ear (creating an interaural level difference, ILD \u2013 also known as interaural intensity difference, IID). These binaural cues allow sounds to be lateralized. Although conventional stereo headphone signals make used of ILDs (not ITDs) the sound is not perceived as being externalized.",
            "score": 133.00703716278076
        },
        {
            "docid": "2094955_30",
            "document": "Salience (language) . Our minds and bodies are bombarded by relevant and irrelevant knowledge and experiences every day. We will tune into salient ones (crane the ears to more fully hear enjoyable music) and tune-out non-salient ones (cover our ears from jackhammer noise). There is difference between seeing something and looking at it. In seeing, the capacity of our retina to take in the light energy is engaged and the brain processes that information into an image. When one looks at an object, not only are visual perceptive capacities engaged, but other mental processes for evaluation and ordering of the object are activated (Skinner, 1974).",
            "score": 76.12535262107849
        },
        {
            "docid": "422247_33",
            "document": "Self-awareness . Autism spectrum disorder (ASD) is a range of neurodevelopmental disabilities that can adversely impact social communication and create behavioral challenges (Understanding Autism, 2003). \"Autism spectrum disorder (ASD) and autism are both general terms for a group of complex disorders of brain development. These disorders are characterized, in varying degrees, by difficulties in social interaction, verbal and nonverbal communication and repetitive behaviors.\" ASDs can also cause imaginative abnormalities and can range from mild to severe, especially in sensory-motor, perceptual and affective dimensions. Children with ASD may struggle with self-awareness and self acceptance. Their different thinking patterns and brain processing functions in the area of social thinking and actions may compromise their ability to understand themselves and social connections to others. About 75% diagnosed autistics are mentally handicapped in some general way and the other 25% diagnosed with Asperger's Syndrome show average to good cognitive functioning. When we compare our own behavior to the morals and values that we were taught, we can focus more attention on ourselves which increases self-awareness. To understand the many effects of autism spectrum disorders on those afflicted have led many scientists to theorize what level of self-awareness occurs and in what degree. Research found that ASD can be associated with intellectual disability and difficulties in motor coordination and attention. It can also result in physical health issues as well, such as sleep and gastrointestinal disturbances. As a result of all those problems, individuals are literally unaware of themselves. It is well known that children suffering from varying degrees of autism struggle in social situations. Scientists at the University of Cambridge have produced evidence that self-awareness is a main problem for people with ASD. Researchers used functional magnetic resonance scans (FMRI) to measure brain activity in volunteers being asked to make judgments about their own thoughts, opinions, preferences, as well as about someone else's. One area of the brain closely examined was the ventromedial pre-frontal cortex (vMPFC) which is known to be active when people think about themselves. A study out of Stanford University has tried to map out brain circuits with understanding self-awareness in Autism Spectrum Disorders. This study suggests that self-awareness is primarily lacking in social situations but when in private they are more self-aware and present. It is in the company of others while engaging in interpersonal interaction that the self-awareness mechanism seems to fail. Higher functioning individuals on the ASD scale have reported that they are more self-aware when alone unless they are in sensory overload or immediately following social exposure. Self-awareness dissipates when an autistic is faced with a demanding social situation. This theory suggests that this happens due to the behavioral inhibitory system which is responsible for self-preservation. This is the system that prevents human from self-harm like jumping out of a speeding bus or putting our hand on a hot stove. Once a dangerous situation is perceived then the behavioral inhibitory system kicks in and restrains our activities. \"For individuals with ASD, this inhibitory mechanism is so powerful, it operates on the least possible trigger and shows an over sensitivity to impending danger and possible threats. Some of these dangers may be perceived as being in the presence of strangers, or a loud noise from a radio. In these situations self-awareness can be compromised due to the desire of self preservation, which trumps social composure and proper interaction. The Hobson hypothesis reports that autism begins in infancy due to the lack of cognitive and linguistic engagement which in turn results in impaired reflective self-awareness. In this study ten children with Asperger's Syndrome were examined using the Self-understanding Interview. This interview was created by Damon and Hart and focuses on seven core areas or schemas that measure the capacity to think in increasingly difficult levels. This interview will estimate the level of self understanding present. \"The study showed that the Asperger group demonstrated impairment in the 'self-as-object' and 'self-as-subject' domains of the Self-understanding Interview, which supported Hobson's concept of an impaired capacity for self-awareness and self-reflection in people with ASD.\". Self-understanding is a self description in an individual's past, present and future. Without self-understanding it is reported that self-awareness is lacking in people with ASD. Joint attention (JA) was developed as a teaching strategy to help increase positive self-awareness in those with autism spectrum disorder. JA strategies were first used to directly teach about reflected mirror images and how they relate to their reflected image. Mirror Self Awareness Development (MSAD) activities were used as a four-step framework to measure increases in self-awareness in those with ASD. Self-awareness and knowledge is not something that can simply be taught through direct instruction. Instead, students acquire this knowledge by interacting with their environment. Mirror understanding and its relation to the development of self leads to measurable increases in self-awareness in those with ASD. It also proves to be a highly engaging and highly preferred tool in understanding the developmental stages of self- awareness. There have been many different theories and studies done on what degree of self-awareness is displayed among people with autism spectrum disorder. Scientists have done research about the various parts of the brain associated with understanding self and self-awareness. Studies have shown evidence of areas of the brain that are impacted by ASD. Other theories suggest that helping an individual learn more about themselves through Joint Activities, such as the Mirror Self Awareness Development may help teach positive self-awareness and growth. In helping to build self-awareness it is also possible to build self-esteem and self acceptance. This in turn can help to allow the individual with ASD to relate better to their environment and have better social interactions with others.",
            "score": 55.803191900253296
        },
        {
            "docid": "34898033_6",
            "document": "John Philip Newell . New science speaks of being able to detect the sound of the beginning in the universe. It vibrates within the matter of everything that has being. New science is echoing the ancient wisdom of spiritual insight. In the twelfth century Hildegard of Bingen taught that the sound of God resonates \u2018in every creature\u2019. It is \u2018the holy sound\u2019, she says, \u2018which echoes through the whole creation.\u2019 If we are to listen for the One from whom we have come, it is not away from creation that we are to turn our ears, it is not away from the true depths of our being that we are to listen. It is rather to the very heart of all life that we are to turn our inner attention. For then we will hear that the deepest sound within us is the deepest sound within one another and within everything that has being. We will hear that the true harmony of our being belongs to the universe and that the true harmony of the universe belongs to us. \u2026 Everything arises from that sacred sound.\"",
            "score": 92.47165083885193
        },
        {
            "docid": "2534964_17",
            "document": "Sensory processing . In the future, research on sensory integration will be used to better understand how different sensory modalities are incorporated within the brain to help us perform even the simplest of tasks. For example, we do not currently have the understanding needed to comprehend how neural circuits transform sensory cues into changes in motor activities. More research done on the sensorimotor system can help understand how these movements are controlled. This understanding can potentially be used to learn more about how to make better prosthetics, and eventually help patients who have lost the use of a limb. Also, by learning more about how different sensory inputs can combine can have profound effects on new engineering approaches using robotics. The robot's sensory devices may take in inputs of different modalities, but if we understand multisensory integration better, we might be able to program these robots to convey these data into a useful output to better serve our purposes.",
            "score": 41.998586654663086
        },
        {
            "docid": "1837721_18",
            "document": "Short-beaked echidna . The echidna's optical system is an uncommon hybrid of both mammalian and reptilian characteristics. The cartilaginous layer beneath the sclera of the eyeball is similar to that of reptiles and avians. The small corneal surface is keratinised and hardened, possibly to protect it from chemicals secreted by prey insects or self-impalement when it rolls itself up, which has been observed. The echidna has the flattest lens of any animal, giving it the longest focal length. This similarity to primates and humans allows it to see distant objects clearly. Unlike placental mammals, including humans, the echidna does not have a ciliary muscle to distort the geometry of the lens and thereby change the focal length and allow objects at different distances to be viewed clearly; the whole eye is believed to distort, so the distance between the lens and retina instead changes to allow focusing. The visual ability of an echidna is not great, and it is not known whether it can perceive colour; however, it can distinguish between black and white, and horizontal and vertical stripes. Eyesight is not a crucial factor in the animal's ability to survive, as blind echidnas are able to live healthily. Its ears are sensitive to low-frequency sound, which may be ideal for detecting sounds emitted by termites and ants underground. The pinnae are obscured and covered by hair, so predators cannot grab them in an attack, and prey or foreign material cannot enter, although ticks are known to reside there. The macula of the ear is very large compared to other animals, and is used as a gravity sensor to orient the echidna. The large size may be important for burrowing downwards.",
            "score": 102.09433877468109
        },
        {
            "docid": "12082283_13",
            "document": "Human vestigiality . The ears of a macaque monkey and most other monkeys have far more developed muscles than those of humans, and therefore have the capability to move their ears to better hear potential threats. Humans and other primates such as the orangutan and chimpanzee however have ear muscles that are minimally developed and non-functional, yet still large enough to be identifiable. A muscle attached to the ear that cannot move the ear, for whatever reason, can no longer be said to have any biological function. In humans there is variability in these muscles, such that some people are able to move their ears in various directions, and it can be possible for others to gain such movement by repeated trials. In such primates, the inability to move the ear is compensated mainly by the ability to turn the head on a horizontal plane, an ability which is not common to most monkeys\u2014a function once provided by one structure is now replaced by another.",
            "score": 179.3995760679245
        },
        {
            "docid": "62729_2",
            "document": "Sense of balance . The sense of balance or equilibrioception is one of the physiological senses related to balance. It helps prevent humans and animals from falling over when standing or moving. Balance is the result of a number of body systems working together: the eyes (visual system), ears (vestibular system) and the body's sense of where it is in space (proprioception) ideally need to be intact. The vestibular system, the region of the inner ear where three semicircular canals converge, works with the visual system to keep objects in focus when the head is moving. This is called the vestibulo-ocular reflex (VOR). The balance system works with the visual and skeletal systems (the muscles and joints and their sensors) to maintain orientation or balance. Visual signals sent to the brain about the body's position in relation to its surroundings are processed by the brain and compared to information from the vestibular, visual and skeletal systems.",
            "score": 89.76566088199615
        },
        {
            "docid": "101970_2",
            "document": "Tinnitus . Tinnitus is the hearing of sound when no external sound is present. While often described as a ringing, it may also sound like a clicking, hiss or roaring. Rarely, unclear voices or music are heard. The sound may be soft or loud, low pitched or high pitched and appear to be coming from one ear or both. Most of the time, it comes on gradually. In some people, the sound causes depression or anxiety and can interfere with concentration. Tinnitus is not a disease but a symptom that can result from a number of underlying causes. One of the most common causes is noise-induced hearing loss. Other causes include ear infections, disease of the heart or blood vessels, M\u00e9ni\u00e8re's disease, brain tumors, emotional stress, exposure to certain medications, a previous head injury, and earwax. It is more common in those with depression. The diagnosis of tinnitus is usually based on the person's description. A number of questionnaires exist that may help to assess how much tinnitus is interfering with a person's life. The diagnosis is commonly supported by an audiogram and a neurological examination. If certain problems are found, medical imaging, such as with MRI, may be performed. Other tests are suitable when tinnitus occurs with the same rhythm as the heartbeat. Rarely, the sound may be heard by someone else using a stethoscope, in which case it is known as objective tinnitus. Spontaneous otoacoustic emissions, which are sounds produced normally by the inner ear, may also occasionally result in tinnitus. Prevention involves avoiding loud noise. If there is an underlying cause, treating it may lead to improvements. Otherwise, typically, management involves talk therapy. Sound generators or hearing aids may help some. As of 2013, there were no effective medications. It is common, affecting about 10\u201315% of people. Most, however, tolerate it well, and it is a significant problem in only 1\u20132% of people. The word tinnitus is from the Latin \"tinn\u012bre\" which means \"to ring\". Tinnitus can be perceived in one or both ears or in the head. It is the description of a noise inside a person\u2019s head in the absence of auditory stimulation. The noise can be described in many different ways. It is usually described as a ringing noise but, in some patients, it takes the form of a high-pitched whining, electric buzzing, hissing, humming, tinging or whistling sound or as ticking, clicking, roaring, \"crickets\" or \"tree frogs\" or \"locusts (cicadas)\", tunes, songs, beeping, sizzling, sounds that slightly resemble human voices or even a pure steady tone like that heard during a hearing test. It has also been described as a \"whooshing\" sound because of acute muscle spasms, as of wind or waves. Tinnitus can be intermittent or continuous: in the latter case, it can be the cause of great distress. In some individuals, the intensity can be changed by shoulder, head, tongue, jaw or eye movements. Most people with tinnitus have some degree of hearing loss.",
            "score": 138.9874893426895
        },
        {
            "docid": "997173_12",
            "document": "Electromyography . Intramuscular EMG can be performed using a variety of different types of recording electrodes. The simplest approach is a monopolar needle electrode. This can be a fine wire inserted into a muscle with a surface electrode as a reference; or two fine wires inserted into muscle referenced to each other. Most commonly fine wire recordings are for research or kinesiology studies. Diagnostic monopolar EMG electrodes are typically insulated and stiff enough to penetrate skin, with only the tip exposed using a surface electrode for reference. Needles for injecting therapeutic botulinum toxin or phenol are typically monopolar electrodes that use a surface reference, in this case, however, the metal shaft of a hypodermic needle, insulated so that only the tip is exposed, is used both to record signals and to inject. Slightly more complex in design is the concentric needle electrode. These needles have a fine wire, embedded in a layer of insulation that fills the barrel of a hypodermic needle, that has an exposed shaft, and the shaft serves as the reference electrode. The exposed tip of the fine wire serves as the active electrode. As a result of this configuration, signals tend to be smaller when recorded from a concentric electrode than when recorded from a monopolar electrode and they are more resistant to electrical artifacts from tissue and measurements tend to be somewhat more reliable. However, because the shaft is exposed throughout its length, superficial muscle activity can contaminate the recording of deeper muscles. Single fiber EMG needle electrodes are designed to have very tiny recording areas, and allow for the discharges of individual muscle fibers to be discriminated.",
            "score": 70.5002932548523
        },
        {
            "docid": "525667_10",
            "document": "Human echolocation . In a 2014 study by Thaler and colleagues, the researchers first made recordings of the clicks and their very faint echoes using tiny microphones placed in the ears of the blind echolocators as they stood outside and tried to identify different objects such as a car, a flag pole, and a tree. The researchers then played the recorded sounds back to the echolocators while their brain activity was being measured using functional magnetic resonance imaging. Remarkably, when the echolocation recordings were played back to the blind experts, not only did they perceive the objects based on the echoes, but they also showed activity in those areas of their brain that normally process visual information in sighted people, primarily primary visual cortex or V1. This result is surprising, as visual areas, as their names suggest, are only active during visual tasks. The brain areas that process auditory information were no more activated by sound recordings of outdoor scenes containing echoes than they were by sound recordings of outdoor scenes with the echoes removed. Importantly, when the same experiment was carried out with sighted people who did not echolocate, these individuals could not perceive the objects and there was no echo-related activity anywhere in the brain. This suggests that the cortex of blind echolocators is plastic and reorganizes such that primary visual cortex, rather than any auditory area, becomes involved in the computation of echolocation tasks.",
            "score": 122.4092288017273
        },
        {
            "docid": "1021754_40",
            "document": "Sound localization . This kind of sound localization technique provides us the real virtual stereo system. It utilizes \"smart\" manikins, such as KEMAR, to glean signals or use DSP methods to simulate the transmission process from sources to ears. After amplifying, recording and transmitting, the two channels of received signals will be reproduced through earphones or speakers. This localization approach uses electroacoustic methods to obtain the spatial information of the original sound field by transferring the listener's auditory apparatus to the original sound field. The most considerable advantages of it would be that its acoustic images are lively and natural. Also, it only needs two independent transmitted signal to reproduce the acoustic image of a 3D system. The representatives of this kind of system are SRS Audio Sandbox, Spatializer Audio Lab and Qsound Qxpander. They use HRTF to simulate the received acoustic signals at the ears from different directions with common binary-channel stereo reproduction. Therefore, they can simulate reflected sound waves and improve subjective sense of space and envelopment. Since they are para-virtualization stereo systems, the major goal of them is to simulate stereo sound information. Traditional stereo systems use sensors that are quite different from human ears. Although those sensors can receive the acoustic information from different directions, they do not have the same frequency response of human auditory system. Therefore, when binary-channel mode is applied, human auditory systems still cannot feel the 3D sound effect field. However, the 3D para-virtualization stereo system overcome such disadvantages. It uses HRTF principles to glean acoustic information from the original sound field then produce a lively 3D sound field through common earphones or speakers.",
            "score": 129.676323056221
        },
        {
            "docid": "433584_4",
            "document": "McGurk effect . Vision is the primary sense for humans, but speech perception is multimodal, which means that it involves information from more than one sensory modality, in particular, audition and vision. The McGurk effect arises during phonetic processing because the integration of audio and visual information happens early in speech perception. The McGurk effect is very robust; that is, knowledge about it seems to have little effect on one's perception of it. This is different from certain optical illusions, which break down once one 'sees through' them. Some people, including those that have been researching the phenomenon for more than twenty years, experience the effect even when they are aware that it is taking place. With the exception of people who can identify most of what is being said from speech-reading alone, most people are quite limited in their ability to identify speech from visual-only signals. A more extensive phenomenon is the ability of visual speech to increase the intelligibility of heard speech in a noisy environment. Visible speech can also alter the perception of perfectly audible speech sounds when the visual speech stimuli are mismatched with the auditory speech. Normally, speech perception is thought to be an auditory process; however, our use of information is immediate, automatic, and, to a large degree, unconscious and therefore, despite what is widely accepted as true, speech is not only something we hear. Speech is perceived by all of the senses working together (seeing, touching, and listening to a face move). The brain is often unaware of the separate sensory contributions of what it perceives. Therefore, when it comes to recognizing speech the brain cannot differentiate whether it is seeing or hearing the incoming information.",
            "score": 64.68224608898163
        },
        {
            "docid": "768413_51",
            "document": "Ear . Some large primates such as gorillas and orang-utans (and also humans) have undeveloped ear muscles that are non-functional vestigial structures, yet are still large enough to be easily identified. An ear muscle that cannot move the ear, for whatever reason, has lost that biological function. This serves as evidence of homology between related species. In humans, there is variability in these muscles, such that some people are able to move their ears in various directions, and it has been said that it may be possible for others to gain such movement by repeated trials. In such primates, the inability to move the ear is compensated for mainly by the ability to easily turn the head on a horizontal plane, an ability which is not common to most monkeys\u2014a function once provided by one structure is now replaced by another.",
            "score": 167.85919904708862
        }
    ],
    "r": [
        {
            "docid": "12082283_13",
            "document": "Human vestigiality . The ears of a macaque monkey and most other monkeys have far more developed muscles than those of humans, and therefore have the capability to move their ears to better hear potential threats. Humans and other primates such as the orangutan and chimpanzee however have ear muscles that are minimally developed and non-functional, yet still large enough to be identifiable. A muscle attached to the ear that cannot move the ear, for whatever reason, can no longer be said to have any biological function. In humans there is variability in these muscles, such that some people are able to move their ears in various directions, and it can be possible for others to gain such movement by repeated trials. In such primates, the inability to move the ear is compensated mainly by the ability to turn the head on a horizontal plane, an ability which is not common to most monkeys\u2014a function once provided by one structure is now replaced by another.",
            "score": 179.39956665039062
        },
        {
            "docid": "768413_51",
            "document": "Ear . Some large primates such as gorillas and orang-utans (and also humans) have undeveloped ear muscles that are non-functional vestigial structures, yet are still large enough to be easily identified. An ear muscle that cannot move the ear, for whatever reason, has lost that biological function. This serves as evidence of homology between related species. In humans, there is variability in these muscles, such that some people are able to move their ears in various directions, and it has been said that it may be possible for others to gain such movement by repeated trials. In such primates, the inability to move the ear is compensated for mainly by the ability to easily turn the head on a horizontal plane, an ability which is not common to most monkeys\u2014a function once provided by one structure is now replaced by another.",
            "score": 167.85919189453125
        },
        {
            "docid": "1519473_5",
            "document": "Cat anatomy . Thirty-two individual muscles in each ear allow for a manner of directional hearing; a cat can move each ear independently of the other. Because of this mobility, a cat can move its body in one direction and point its ears in another direction. Most cats have straight ears pointing upward. Unlike dogs, flap-eared breeds are extremely rare (\"Scottish Folds\" are one such exceptional mutation). When angry or frightened, a cat will lay back its ears to accompany the growling or hissing sounds it makes. Cats also turn their ears back when they are playing or to listen to a sound coming from behind them. The fold of skin forming a pouch on the lower posterior part of the ear, known as Henry's pocket, is usually prominent in a cat's ear. It is of unknown function, though it may assist in filtering sounds.",
            "score": 167.43492126464844
        },
        {
            "docid": "30176398_23",
            "document": "Golding Bird . The required direction of current depended on the direction in which electric current was thought to flow in nerves in the human or animal body. For motor functions, for instance, the flow was taken to be from the centre towards the muscles at the extremities, so artificial electrical stimulation needed to be in the same direction. For sensory nerves, the opposite applied: flow was from the extremity to the centre, and the positive electrode would be applied to the extremity. This principle was demonstrated by Bird in an experiment with a living frog. A supply of frogs was usually on hand, as they were used in the frog galvanoscope. The electromagnetic galvanometer was available at the time, but frogs' legs were still used by Bird because of their much greater sensitivity to small currents. In the experiment, the frog's leg was almost completely severed from its body, leaving only the sciatic nerve connected, and electric current was then applied from the body to the leg. Convulsions of the leg were seen when the muscle was stimulated. Reversing the current, however, produced no movement of the muscle, merely croaks of pain from the frog. In his lectures, Bird describes many experiments with a similar aim on human sensory organs. In one experiment by Grapengiesser, for instance, electric current is passed through the subject's head from ear to ear, causing a sound to be hallucinated. The ear connected to the positive terminal hears a louder sound than that connected to the negative.",
            "score": 166.72312927246094
        },
        {
            "docid": "3246329_10",
            "document": "Hearing range . Cats have excellent hearing and can detect an extremely broad range of frequencies. They can hear higher-pitched sounds than humans or most dogs, detecting frequencies from 55\u00a0Hz up to 79\u00a0kHz. Cats do not use this ability to hear ultrasound for communication but it is probably important in hunting, since many species of rodents make ultrasonic calls. Cat hearing is also extremely sensitive and is among the best of any mammal, being most acute in the range of 500\u00a0Hz to 32\u00a0kHz. This sensitivity is further enhanced by the cat's large movable outer ears (their \"pinnae\"), which both amplify sounds and help a cat sense the direction from which a noise is coming. The hearing ability of a dog is dependent on breed and age, though the range of hearing is usually around 67\u00a0Hz to 45\u00a0kHz. As with humans, some dog breeds' hearing ranges narrow with age, such as the German shepherd and miniature poodle. When dogs hear a sound, they will move their ears towards it in order to maximise reception. In order to achieve this, the ears of a dog are controlled by at least 18 muscles, which allow the ears to tilt and rotate. The ear's shape also allows the sound to be heard more accurately. Many breeds often have upright and curved ears, which direct and amplify sounds.",
            "score": 164.77491760253906
        },
        {
            "docid": "1021754_31",
            "document": "Sound localization . The human outer ear, i.e. the structures of the pinna and the external ear canal, form direction-selective filters. Depending on the sound input direction in the median plane, different filter resonances become active. These resonances implant direction-specific patterns into the frequency responses of the ears, which can be evaluated by the auditory system (directional bands) for vertical sound localization. Together with other direction-selective reflections at the head, shoulders and torso, they form the outer ear transfer functions. These patterns in the ear's frequency responses are highly individual, depending on the shape and size of the outer ear. If sound is presented through headphones, and has been recorded via another head with different-shaped outer ear surfaces, the directional patterns differ from the listener's own, and problems will appear when trying to evaluate directions in the median plane with these foreign ears. As a consequence, front\u2013back permutations or inside-the-head-localization can appear when listening to dummy head recordings, or otherwise referred to as binaural recordings. It has been shown that human subjects can monaurally localize high frequency sound but not low frequency sound. Binaural localization, however, was possible with lower frequencies. This is likely due to the pinna being small enough to only interact with sound waves of high frequency. It seems that people can only accurately localize the elevation of sounds that are complex and include frequencies above 7,000\u00a0Hz, and a pinna must be present.",
            "score": 161.96685791015625
        },
        {
            "docid": "11713533_24",
            "document": "Vocal pedagogy . Describing vocal sound is an inexact science largely because the human voice is a self-contained instrument. Since the vocal instrument is internal, the singer's ability to monitor the sound produced is complicated by the vibrations carried to the ear through the Eustachean (auditory) tube and the bony structures of the head and neck. In other words, most singers hear something different in their ears/head than what a person listening to them hears. As a result, voice teachers often focus less on how it \"sounds\" and more on how it \"feels\". Vibratory sensations resulting from the closely related processes of phonation and resonation, and kinesthetic ones arising from muscle tension, movement, body position, and weight serve as a guide to the singer on correct vocal production.",
            "score": 146.5478057861328
        },
        {
            "docid": "752723_2",
            "document": "Dichotic listening test . The Dichotic listening test is a psychological test commonly used to investigate selective attention within the auditory system and is a subtopic of cognitive psychology and neuroscience. Specifically, it is \"used as a behavioral test for hemispheric lateralization of speech sound perception.\" During a standard dichotic listening test, a participant is presented with two different auditory stimuli simultaneously (usually speech). The different stimuli are directed into different ears over headphones. Research Participants were instructed to repeat aloud the words they heard in one ear while a different message was presented to the other ear. As a result of focusing to repeat the words, participants noticed little of the message to the other ear, often not even realizing that at some point it changed from English to German. At the same time, participants did notice when the voice in the unattended ear changed from a male\u2019s to a female\u2019s, suggesting that the selectivity of consciousness can work to tune in some information.\"",
            "score": 141.25636291503906
        },
        {
            "docid": "37654_17",
            "document": "Owl . Owls exhibit specialized hearing functions and ear shapes that also aid in hunting. They are noted for asymmetrical ear placements on the skull in some genera. Owls can have either internal or external ears, both of which are asymmetrical. Asymmetry has not been reported to extend to the middle or internal ear of the owl. Asymmetrical ear placement on the skull allows the owl to pinpoint the location of its prey. This is especially true for strictly nocturnal species such as the barn owls \"Tyto\" or Tengmalm's owl. With ears set at different places on its skull, an owl is able to determine the direction from which the sound is coming by the minute difference in time that it takes for the sound waves to penetrate the left and right ears. The owl turns its head until the sound reaches both ears at the same time, at which point it is directly facing the source of the sound. This time difference between ears is a matter of about 0.00003 seconds, or 30 millionths of a second. Behind the ear openings are modified, dense feathers, densely packed to form a facial ruff, which creates an anterior-facing, concave wall that cups the sound into the ear structure. This facial ruff is poorly defined in some species, and prominent, nearly encircling the face, in other species. The facial disk also acts to direct sound into the ears, and a downward-facing, sharply triangular beak minimizes sound reflection away from the face. The shape of the facial disk is adjustable at will to focus sounds more effectively.",
            "score": 139.54449462890625
        },
        {
            "docid": "101970_2",
            "document": "Tinnitus . Tinnitus is the hearing of sound when no external sound is present. While often described as a ringing, it may also sound like a clicking, hiss or roaring. Rarely, unclear voices or music are heard. The sound may be soft or loud, low pitched or high pitched and appear to be coming from one ear or both. Most of the time, it comes on gradually. In some people, the sound causes depression or anxiety and can interfere with concentration. Tinnitus is not a disease but a symptom that can result from a number of underlying causes. One of the most common causes is noise-induced hearing loss. Other causes include ear infections, disease of the heart or blood vessels, M\u00e9ni\u00e8re's disease, brain tumors, emotional stress, exposure to certain medications, a previous head injury, and earwax. It is more common in those with depression. The diagnosis of tinnitus is usually based on the person's description. A number of questionnaires exist that may help to assess how much tinnitus is interfering with a person's life. The diagnosis is commonly supported by an audiogram and a neurological examination. If certain problems are found, medical imaging, such as with MRI, may be performed. Other tests are suitable when tinnitus occurs with the same rhythm as the heartbeat. Rarely, the sound may be heard by someone else using a stethoscope, in which case it is known as objective tinnitus. Spontaneous otoacoustic emissions, which are sounds produced normally by the inner ear, may also occasionally result in tinnitus. Prevention involves avoiding loud noise. If there is an underlying cause, treating it may lead to improvements. Otherwise, typically, management involves talk therapy. Sound generators or hearing aids may help some. As of 2013, there were no effective medications. It is common, affecting about 10\u201315% of people. Most, however, tolerate it well, and it is a significant problem in only 1\u20132% of people. The word tinnitus is from the Latin \"tinn\u012bre\" which means \"to ring\". Tinnitus can be perceived in one or both ears or in the head. It is the description of a noise inside a person\u2019s head in the absence of auditory stimulation. The noise can be described in many different ways. It is usually described as a ringing noise but, in some patients, it takes the form of a high-pitched whining, electric buzzing, hissing, humming, tinging or whistling sound or as ticking, clicking, roaring, \"crickets\" or \"tree frogs\" or \"locusts (cicadas)\", tunes, songs, beeping, sizzling, sounds that slightly resemble human voices or even a pure steady tone like that heard during a hearing test. It has also been described as a \"whooshing\" sound because of acute muscle spasms, as of wind or waves. Tinnitus can be intermittent or continuous: in the latter case, it can be the cause of great distress. In some individuals, the intensity can be changed by shoulder, head, tongue, jaw or eye movements. Most people with tinnitus have some degree of hearing loss.",
            "score": 138.98748779296875
        },
        {
            "docid": "8595464_8",
            "document": "Cat behavior . Cats rely strongly on body language to communicate. A cat may rub against an object, lick a person, and purr. Much of a cat's body language is through its tail, ears, head position, and back posture. Cats flick their tails in an oscillating, snake-like motion, or abruptly from side to side, often just before pouncing on an object or animal in what looks like \"play\" hunting behavior. If spoken to, a cat may flutter its tail in response, which may be the only indication of the interaction, though movement of its ears or head toward the source of the sound may be a better indication of the cat's awareness that a sound was made in their direction.",
            "score": 137.8351287841797
        },
        {
            "docid": "14910292_5",
            "document": "Seashell resonance . The human ear picks up sounds made by the human body as well, including the sounds of blood flowing and muscles acting. These sounds are normally discarded by the brain; however, they become more obvious when louder external sounds are filtered out. This occlusion effect occurs with seashells, cups, or hands held over one's ears, and also with circumaural headphones, whose cups form a seal around the ear, raising the acoustic impedance to external sounds.",
            "score": 136.38449096679688
        },
        {
            "docid": "3864383_7",
            "document": "Virtual surround . Perception of direction is greatly affected by the relative time that a sound arrives at each ear and any difference in the amplitude of a sound at each ear. It is possible to create a sound source having an output characteristic which is rapidly varying with direction and frequency of signal. These kinds of sources create sound fields which are rapidly variable around the listeners room. These are often referred to as diffuse sources, this is because their output resembles a diffuse sound field \u2014 a sound field where soundwaves are traveling in all directions with equal probability. In a diffuse field the sound at each of a listeners' ears is so completely different that it is impossible for the brain to work out where the sound has come from. A diffuse source located in front of the listener will be hard to localize and can be used to carry the surround signals.",
            "score": 135.66819763183594
        },
        {
            "docid": "161005_4",
            "document": "Head-related transfer function . Humans have just two ears, but can locate sounds in three dimensions \u2013 in range (distance), in direction above and below, in front and to the rear, as well as to either side. This is possible because the brain, inner ear and the external ears (pinna) work together to make inferences about location. This ability to localize sound sources may have developed in humans and ancestors as an evolutionary necessity, since the eyes can only see a fraction of the world around a viewer, and vision is hampered in darkness, while the ability to localize a sound source works in all directions, to varying accuracy,  regardless of the surrounding light.",
            "score": 134.04542541503906
        },
        {
            "docid": "8526165_7",
            "document": "Cat senses . Humans and cats have a similar range of hearing on the low end of the scale, but cats can hear much higher-pitched sounds, up to 64 kHz, which is 1.6 octaves above the range of a human, and even 1 octave above the range of a dog. When listening for something, a cat's ears will swivel in that direction; a cat's ear flaps (pinnae) can independently point backwards as well as forwards and sideways to pinpoint the source of the sound. Cats can judge within the location of a sound being made away\u2014this can be useful for locating their prey.",
            "score": 133.88092041015625
        },
        {
            "docid": "2186011_12",
            "document": "Tensor tympani muscle . In many people with hyperacusis, an increased activity develops in the tensor tympani muscle in the middle ear as part of the startle response to some sounds. This lowered reflex threshold for tensor tympani contraction is activated by the perception/anticipation of loud sound, and is called tonic tensor tympani syndrome (TTTS). In some people with hyperacusis, the tensor tympani muscle can contract just by thinking about a loud sound. Following exposure to intolerable sounds, this contraction of the tensor tympani muscle tightens the ear drum, which can lead to the symptoms of ear pain/a fluttering sensation/a sensation of fullness in the ear (in the absence of any middle or inner ear pathology).",
            "score": 133.62594604492188
        },
        {
            "docid": "3154127_4",
            "document": "Virtual acoustic space . When one listens to sounds over headphones (in what is known as the \"closed field\") the sound source appears to arise from center of the head. On the other hand, under normal, so-called free-field, listening conditions sounds are perceived as being externalized. The direction of a sound in space (see sound localization) is determined by the brain when it analyses the interaction of incoming sound with head and external ears. A sound arising to one side reaches the near ear before the far ear (creating an interaural time difference, ITD), and will also be louder at the near ear (creating an interaural level difference, ILD \u2013 also known as interaural intensity difference, IID). These binaural cues allow sounds to be lateralized. Although conventional stereo headphone signals make used of ILDs (not ITDs) the sound is not perceived as being externalized.",
            "score": 133.0070343017578
        },
        {
            "docid": "39369611_3",
            "document": "Echolocation jamming . Echolocating animals can jam themselves in a number of ways. Bats, for example, produce some of the loudest sounds in nature, and then they immediately listen for echoes that are hundreds of times fainter than the sounds they emit. To avoid deafening themselves, whenever a bat makes an echolocation emission, a small muscle in the bat's middle ear (the stapedius muscle) clamps down on small bones called ossicles, which normally amplify sounds between the ear drum and the cochlea. This dampens the intensity of the sounds that the bat hears during this time, preserving hearing sensitivity to target echoes.",
            "score": 132.50318908691406
        },
        {
            "docid": "4548229_2",
            "document": "Interaural time difference . The interaural time difference (or ITD) when concerning humans or animals, is the difference in arrival time of a sound between two ears. It is important in the localization of sounds, as it provides a cue to the direction or angle of the sound source from the head. If a signal arrives at the head from one side, the signal has further to travel to reach the far ear than the near ear. This pathlength difference results in a time difference between the sound's arrivals at the ears, which is detected and aids the process of identifying the direction of sound source.",
            "score": 132.05926513671875
        },
        {
            "docid": "45871_60",
            "document": "Loudspeaker . The interaction of a loudspeaker system with its environment is complex and is largely out of the loudspeaker designer's control. Most listening rooms present a more or less reflective environment, depending on size, shape, volume, and furnishings. This means the sound reaching a listener's ears consists not only of sound directly from the speaker system, but also the same sound delayed by traveling to and from (and being modified by) one or more surfaces. These reflected sound waves, when added to the direct sound, cause cancellation and addition at assorted frequencies (e.g., from resonant room modes), thus changing the timbre and character of the sound at the listener's ears. The human brain is very sensitive to small variations, including some of these, and this is part of the reason why a loudspeaker system sounds different at different listening positions or in different rooms.",
            "score": 131.6486358642578
        },
        {
            "docid": "33069847_9",
            "document": "Cropping (animal) . Historically, ear cropping has been advocated as a health benefit for certain breeds with long, hanging ears. There is some evidence that dogs with standing ears may suffer from fewer ear infections than dogs with hanging ears. It has also been hypothesized that standing ears are less prone to damage and subsequent medical complications, especially in working dogs. Some claim that cropped ears enhance Boxers' hearing. Long, hanging ears can not function the same way as erect ears which can swivel toward a sound source. The erect shape directs sound waves into the ear canal and additionally amplifies the sound slightly. Long, hanging pinnae also impose a physical barrier to sound waves entering the ear canal.",
            "score": 131.24232482910156
        },
        {
            "docid": "17523336_22",
            "document": "Olivocochlear system . Although Scharf et al.\u2019s (1993, 1994, 1997) experiments failed to produce any clear differences in the basic psychophysical characteristics of hearing (other than the detection of unexpected sounds), many other studies using both animals and humans have implicated the OCB in listening-in-noise tasks using more complex stimuli. In constant BGN, rhesus monkeys with intact OCBs have been observed to perform better in vowel discrimination tasks than those without (Dewson, 1968). In cats, an intact OCB is associated with better vowel identification (Heinz et al., 1998), sound localisation (May et al., 2004), and intensity discrimination (May and McQuone, 1995). All of these studies were performed in constant BGN. In humans, speech-in-noise discrimination measurements have been performed on individuals who had undergone unilateral vestibular neurectomy (resulting in OCB sectioning). Giraud et al. (1997) observed a small advantage in the healthy ear over the operated ear for phoneme recognition and speech intelligibility in BGN. Scharf et al. (1988) had previously investigated the role of auditory attention during speech perception, and suggested that speech-in-noise discrimination is assisted by attentional focus on frequency regions. In 2000, Zeng et al., reported that vestibular neurectomy did not directly affect pure-tone thresholds or intensity discrimination, confirming earlier findings of Scharf et al. 1994; 1997. For the listening-in-noise tasks, they observed a number of discrepancies between the healthy and operated ear. Consistent with the earlier findings of May and McQuone (1995), intensity discrimination in noise was observed to be slightly worse in the ear without olivocochlear bundle (OCB) input. However, Zeng et al.\u2019s main finding related to the \u201covershoot\u201d effect, which was found to be significantly reduced (~50%) in the operated ears. This effect was first observed by Zwicker (1965), and was characterised as an increased detection threshold of a tone when it is presented at the onset of the noise compared to when it is presented in constant, steady-state noise. Zeng et al. proposed that this finding is consistent with MOCS-evoked antimasking; that is, MOCS-evoked antimasking being absent at the onset of noise however becoming active during steady-state noise. This theory was supported by the time course of MOC activation; being similar to the time course of the overshoot effect (Zwicker, 1965), as well as the overshoot effect being disrupted in subjects with sensorineural hearing loss, for whom the MOCS would be most likely ineffectual (Bacon and Takahashi, 1992).",
            "score": 129.74978637695312
        },
        {
            "docid": "1021754_40",
            "document": "Sound localization . This kind of sound localization technique provides us the real virtual stereo system. It utilizes \"smart\" manikins, such as KEMAR, to glean signals or use DSP methods to simulate the transmission process from sources to ears. After amplifying, recording and transmitting, the two channels of received signals will be reproduced through earphones or speakers. This localization approach uses electroacoustic methods to obtain the spatial information of the original sound field by transferring the listener's auditory apparatus to the original sound field. The most considerable advantages of it would be that its acoustic images are lively and natural. Also, it only needs two independent transmitted signal to reproduce the acoustic image of a 3D system. The representatives of this kind of system are SRS Audio Sandbox, Spatializer Audio Lab and Qsound Qxpander. They use HRTF to simulate the received acoustic signals at the ears from different directions with common binary-channel stereo reproduction. Therefore, they can simulate reflected sound waves and improve subjective sense of space and envelopment. Since they are para-virtualization stereo systems, the major goal of them is to simulate stereo sound information. Traditional stereo systems use sensors that are quite different from human ears. Although those sensors can receive the acoustic information from different directions, they do not have the same frequency response of human auditory system. Therefore, when binary-channel mode is applied, human auditory systems still cannot feel the 3D sound effect field. However, the 3D para-virtualization stereo system overcome such disadvantages. It uses HRTF principles to glean acoustic information from the original sound field then produce a lively 3D sound field through common earphones or speakers.",
            "score": 129.6763153076172
        },
        {
            "docid": "55972_7",
            "document": "Middle ear . The movement of the ossicles may be stiffened by two muscles. The stapedius muscle, the smallest skeletal muscle in the body, connects to the stapes and is controlled by the facial nerve; the tensor tympani muscle connects to the base of the malleus and is under the control of the medial pterygoid nerve which is a branch of the mandibular nerve of the trigeminal nerve. These muscles contract in response to loud sounds, thereby reducing the transmission of sound to the inner ear. This is called the acoustic reflex.",
            "score": 128.52772521972656
        },
        {
            "docid": "161005_5",
            "document": "Head-related transfer function . Humans estimate the location of a source by taking cues derived from one ear (\"monaural cues\"), and by comparing cues received at both ears (\"difference cues\" or \"binaural cues\"). Among the difference cues are time differences of arrival and intensity differences. The monaural cues come from the interaction between the sound source and the human anatomy, in which the original source sound is modified before it enters the ear canal for processing by the auditory system. These modifications encode the source location, and may be captured via an impulse response which relates the source location and the ear location. This impulse response is termed the \"head-related impulse response\" (HRIR). Convolution of an arbitrary source sound with the HRIR converts the sound to that which would have been heard by the listener if it had been played at the source location, with the listener's ear at the receiver location. HRIRs have been used to produce virtual surround sound.",
            "score": 128.4062042236328
        },
        {
            "docid": "26573_17",
            "document": "Rabbit . Within the order lagomorphs, the ears are utilized to detect and avoid predators. In the family leporidae, the ears are typically longer than they are wide. For example, in black tailed jack rabbits, their long ears cover a greater surface area relative to their body size that allow them to detect predators from far away. Contrasted to cotton tailed rabbits, their ears are smaller and shorter, requiring predators to be closer to detect them before fleeing. Evolution has favored rabbits to have shorter ears so the larger surface area does not cause them to lose heat in more temperate regions. The opposite can be seen in rabbits that live in hotter climates, mainly because they possess longer ears that have a larger surface area that help with dispersion of heat as well as the theory that sound does not travel well in more arid air, opposed to cooler air. Therefore, longer ears are meant to aid the organism in detecting prey sooner rather than later in warmer temperatures. The rabbit is characterized by its shorter ears while hares are characterized by their longer ears. Rabbits ears are an important structure to aid thermoregulation and detect predators due to how the outer, middle, and inner ear muscles coordinate with one another. The ear muscles also aid in maintaining balance and movement when fleeing predators.Outer ear",
            "score": 128.11892700195312
        },
        {
            "docid": "768413_9",
            "document": "Ear . Two sets of muscles are associated with the outer ear: the intrinsic and extrinsic muscles. In some mammals, these muscles can adjust the direction of the pinna. In humans, these muscles have little or no effect. The ear muscles are supplied by the facial nerve, which also supplies sensation to the skin of the ear itself, as well as to the external ear cavity. The great auricular nerve, auricular nerve, auriculotemporal nerve, and lesser and greater occipital nerves of the cervical plexus all supply sensation to parts of the outer ear and the surrounding skin.",
            "score": 127.91816711425781
        },
        {
            "docid": "101970_3",
            "document": "Tinnitus . The sound perceived may range from a quiet background noise to one that can be heard even over loud external sounds. The specific type of tinnitus called pulsatile tinnitus is characterized by hearing the sounds of one's own pulse or muscle contractions, which is typically a result of sounds that have been created by the movement of muscles near to one's ear, or the sounds are related to blood flow of the neck or face.",
            "score": 127.6302261352539
        },
        {
            "docid": "39200136_12",
            "document": "Hans Wallach . In a series of papers Wallach explored the ability of humans to locate sounds in the median plane \u2013 that is, to determine whether a sound comes from a source at the same elevation as the ears or from a source that is higher or lower, or even in back of the head. Binaural sound cues, including the phasing or time of the sound\u2019s arrival at each ear and the sound\u2019s relative intensity at the two ears (known respectively as ITD and ILD) enable a listener to determine a sound\u2019s lateral location (whether it is on the left, right, or straight ahead). But two sounds at different elevations can present identical ITD and ILD information to the ears, and so binaural cues to a stationary ear do not suffice to identify a sound\u2019s location in the median plane. Monaural cues that depend on the shape of the head and the structure of the external ear help with vertical localization, but binaural cues also play a part if the head is not stationary.",
            "score": 127.61714935302734
        },
        {
            "docid": "8436042_6",
            "document": "MPEG Surround . MPEG Surround coding uses our capacity to perceive sound in the 3D and captures that perception in a compact set of parameters. Spatial perception is primarily attributed to three parameters, or cues, describing how humans localize sound in the horizontal plane: Interaural level difference (ILD), Interaural time difference (ITD) and Interaural coherence (IC). This three concepts are illustrated in next image. Direct, or first-arrival, waveforms from the source hit the left ear at time, while direct sound received by the right ear is diffracted around the head, with time delay and level attenuation, associated. These two effects result in ITD and ILD are associated with the main source. At last, in a reverberant environment, reflected sound from the source, or sound from diffuse source, or uncorrelated sound can hit both ears, all of them are related with IC.",
            "score": 127.50325012207031
        },
        {
            "docid": "635490_10",
            "document": "Auditory system . Sound waves travel through the ear canal and hit the tympanic membrane, or eardrum. This wave information travels across the air-filled middle ear cavity via a series of delicate bones: the malleus (hammer), incus (anvil) and stapes (stirrup). These ossicles act as a lever, converting the lower-pressure eardrum sound vibrations into higher-pressure sound vibrations at another, smaller membrane called the oval window or vestibular window. The manubrium (handle) of the malleus articulates with the tympanic membrane, while the footplate (base) of the stapes articulates with the oval window. Higher pressure is necessary at the oval window than at the typanic membrane because the inner ear beyond the oval window contains liquid rather than air. The stapedius reflex of the middle ear muscles helps protect the inner ear from damage by reducing the transmission of sound energy when the stapedius muscle is activated in response to sound. The middle ear still contains the sound information in wave form; it is converted to nerve impulses in the cochlea.",
            "score": 127.17364501953125
        },
        {
            "docid": "1021754_49",
            "document": "Sound localization . The tiny parasitic fly Ormia ochracea has become a model organism in sound localization experiments because of its unique ear. The animal is too small for the time difference of sound arriving at the two ears to be calculated in the usual way, yet it can determine the direction of sound sources with exquisite precision. The tympanic membranes of opposite ears are directly connected mechanically, allowing resolution of sub-microsecond time differences and requiring a new neural coding strategy. Ho showed that the coupled-eardrum system in frogs can produce increased interaural vibration disparities when only small arrival time and sound level differences were available to the animal's head. Efforts to build directional microphones based on the coupled-eardrum structure are underway.",
            "score": 127.0486068725586
        }
    ]
}