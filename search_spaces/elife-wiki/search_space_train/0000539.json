{
    "q": [
        {
            "docid": "39182554_23",
            "document": "Catastrophic interference . Following the same basic idea contributed by Robins, Ans and Rousset (1997) have also proposed a two-network artificial neural architecture with \"memory self-refreshing\" that overcomes catastrophic interference when sequential learning tasks are carried out in distributed networks trained by backpropagation. The principle is to interleave, at the time when new external patterns are learned, those to-be-learned new external patterns with internally generated pseudopatterns, or 'pseudo-memories', that reflect the previously learned information. What mainly distinguishes this model from those that use classical pseudorehearsal in feedforward multilayer networks is a \"reverberating\" process that is used for generating pseudopatterns. This process which, after a number of activity re-injections from a single random seed, tends to go up to nonlinear network \"attractors\", is more suitable for optimally capturing the deep structure of previously learned knowledge than a single feedforward pass of random activation. Ans and Rousset (2000) have shown that the learning mechanism they proposed avoiding catastrophic forgetting, provides a more appropriate way to deal with knowledge transfer as measured by learning speed, ability to generalize and vulnerability to network damages. Musca, Rousset and Ans (2009) have also shown that pseudopatterns originating from an artificial reverberating neural network could induce familiarity in humans with never seen items in the way predicted by simulations conducted with a two-network artificial neural architecture. Furthermore, Ans (2004) has implemented a version of the self-refreshing mechanism using only one network trained by the Contrastive Hebbian Learning rule, a training rule considered as more realistic than the largely used backpropagation algorithm, but fortunately equivalent to the latter.",
            "score": 114.45845866203308
        },
        {
            "docid": "1164_53",
            "document": "Artificial intelligence . Neural networks, or neural nets, were inspired by the architecture of neurons in the human brain. A simple \"neuron\" \"N\" accepts input from multiple other neurons, each of which, when activated (or \"fired\"), cast a weighted \"vote\" for or against whether neuron \"N\" should itself activate. Learning requires an algorithm to adjust these weights based on the training data; one simple algorithm (dubbed \"fire together, wire together\") is to increase the weight between two connected neurons when the activation of one triggers the successful activation of another. The net forms \"concepts\" that are distributed among a subnetwork of shared neurons that tend to fire together; a concept meaning \"leg\" might be coupled with a subnetwork meaning \"foot\" that includes the sound for \"foot\". Neurons have a continuous spectrum of activation; in addition, neurons can process inputs in a nonlinear way rather than weighing straightforward votes. Modern neural nets can learn both continuous functions and, surprisingly, digital logical operations. Neural networks' early successes included predicting the stock market and (in 1995) a mostly self-driving car. In the 2010s, advances in neural networks using deep learning thrust AI into widespread public consciousness and contributed to an enormous upshift in corporate AI spending; for example, AI-related M&A in 2017 was over 25 times as large as in 2015.",
            "score": 108.12515687942505
        },
        {
            "docid": "33244792_14",
            "document": "Non-spiking neuron . Very little is known about the application of these networks to memory and learning. There are indications that spiking and nonspiking networks both play a vital role in memory and learning. Research has been conducted with the use of learning algorithms, microelectrode arrays, and hybrots. By studying how neurons transfer information, it becomes more possible to enhance those model neural networks and better define what clear information streams could be presented. Perhaps, by conjoining this study with the many neurotrophic factors present, neural networks could be manipulated for optimal routing, and consequently optimal learning.",
            "score": 124.32904410362244
        },
        {
            "docid": "31217535_44",
            "document": "Memory . One question that is crucial in cognitive neuroscience is how information and mental experiences are coded and represented in the brain. Scientists have gained much knowledge about the neuronal codes from the studies of plasticity, but most of such research has been focused on simple learning in simple neuronal circuits; it is considerably less clear about the neuronal changes involved in more complex examples of memory, particularly declarative memory that requires the storage of facts and events (Byrne 2007). Convergence-divergence zones might be the neural networks where memories are stored and retrieved. Considering that there are several kinds of memory, depending on types of represented knowledge, underlying mechanisms, processes functions and modes of acquisition, it is likely that different brain areas support different memory systems and that they are in mutual relationships in neuronal networks: \"components of memory representation are distributed widely across different parts of the brain as mediated by multiple neocortical circuits\".",
            "score": 152.4218246936798
        },
        {
            "docid": "3717_69",
            "document": "Brain . Theorists have worked to understand these response patterns by constructing mathematical models of neurons and neural networks, which can be simulated using computers. Some useful models are abstract, focusing on the conceptual structure of neural algorithms rather than the details of how they are implemented in the brain; other models attempt to incorporate data about the biophysical properties of real neurons. No model on any level is yet considered to be a fully valid description of brain function, though. The essential difficulty is that sophisticated computation by neural networks requires distributed processing in which hundreds or thousands of neurons work cooperatively\u2014current methods of brain activity recording are only capable of isolating action potentials from a few dozen neurons at a time.",
            "score": 141.54979157447815
        },
        {
            "docid": "233488_23",
            "document": "Machine learning . An artificial neural network (ANN) learning algorithm, usually called \"neural network\" (NN), is a learning algorithm that is vaguely inspired by biological neural networks. Computations are structured in terms of an interconnected group of artificial neurons, processing information using a connectionist approach to computation. Modern neural networks are non-linear statistical data modeling tools. They are usually used to model complex relationships between inputs and outputs, to find patterns in data, or to capture the statistical structure in an unknown joint probability distribution between observed variables.",
            "score": 108.56608295440674
        },
        {
            "docid": "3737445_11",
            "document": "Quantum neural network . Most learning algorithms follow the classical model of training an artificial neural network to learn the input-output function of a given training set and use classical feedback loops to update parameters of the quantum system until they converge to an optimal configuration. Learning as a parameter optimisation problem has also been approached by adiabatic models of quantum computing. Recently there has been proposed a new post-learning strategy to allow the search for improved set of weights based on analogy with quantum effects occurring in nature. The technique, proposed in is based on the analogy of modeling a biological neuron as a semiconductor heterostructure consisting of one energetic barrier sandwiched between two energetically lower areas. The activation function of the neuron is therefore considered as a particle entering the heterostructure and interacting with the barrier. In this way auxiliary reinforcement to the classical learning process of neural networks is achieved with minimal additional computational costs.",
            "score": 98.7648297548294
        },
        {
            "docid": "2593441_13",
            "document": "Stephen Grossberg . As noted in the section on Education and Early Research, Grossberg has studied how brains give rise to minds since he took the introductory psychology course as a freshman at Dartmouth College in 1957. At that time, Grossberg introduced the paradigm of using nonlinear systems of differential equations to show how brain mechanisms can give rise to behavioral functions. This paradigm is helping to solve the classical mind/body problem, and is the basic mathematical formalism that is used in biological neural network research today. In particular, in 1957-1958, Grossberg discovered widely used equations for (1) short-term memory (STM), or neuronal activation (often called the Additive and Shunting models, or the Hopfield model after John Hopfield's 1984 application of the Additive model equation); (2) medium-term memory (MTM), or activity-dependent habituation (often called habituative transmitter gates, or depressing synapses after Larry Abbott's 1997 introduction of this term); and (3) long-term memory (LTM), or neuronal learning (often called gated steepest descent learning). One variant of these learning equations, called Instar Learning, was introduced by Grossberg in 1976 into Adaptive Resonance Theory and Self-Organizing Maps for the learning of adaptive filters in these models. This learning equation was also used by Kohonen in his applications of Self-Organizing Maps starting in 1984. Another variant of these learning equations, called Outstar Learning, was used by Grossberg starting in 1967 for spatial pattern learning. Outstar and Instar learning were combined by Grossberg in 1976 in a three-layer network for the learning of multi-dimensional maps from any m-dimensional input space to any n-dimensional output space. This application was called Counter-propagation by Hecht-Nielsen in 1987.",
            "score": 129.58598136901855
        },
        {
            "docid": "1095131_20",
            "document": "Kinesthetic learning . The cerebral cortex is the brain tissue covering the top and sides of the brain in most vertebrates. It is involved in storing and processing of sensory inputs and motor outputs. In the human brain, the cerebral cortex is actually a sheet of neural tissue about 1/8th inch thick. The sheet is folded so that it can fit inside the skull. The neural circuits in this area of the brain expand with practice of an activity, just like the synaptic plasticity grows with practice. Clarification of some of the mechanisms of learning by neuro science has been advanced, in part, by the advent of non-invasive imaging technologies, such as positron emission tomography (PET) and functional magnetic resonance imaging (FMRI). These technologies have allowed researchers to observe human learning processes directly. Through these types of technologies, we are now able to see and study what happens in the process of learning. In different tests performed the brain being imaged showed a greater blood flow and activation to that area of the brain being stimulated through different activities such as finger tapping in a specific sequence. It has been revealed that the process at the beginning of learning a new skill happens quickly, and later on slows down to almost a plateau. This process can also be referred to as The Law of Learning. The slower learning showed in the FMRI that in the cerebral cortex this was when the long term learning was occurring, suggesting that the structural changes in the cortex reflect the enhancement of skill memories during later stages of training. When a person studies a skill for a longer duration of time, but in a shorter amount of time they will learn quickly, but also only retain the information into their short-term memory. Just like studying for an exam; if a student tries to learn everything the night before, it will not stick in the long run. If a person studies a skill for a shorter duration of time, but more frequently and long-term, their brain will retain this information much longer as it is stored in the long-term memory. Functional and structural studies of the brain have revealed a vast interconnectivity between diverse regions of the cerebral cortex. For example, large numbers of axons interconnect the posterior sensory areas serving vision, audition, and touch with anterior motor regions. Constant communication between sensation and movement makes sense, because to execute smooth movement through the environment, movement must be continuously integrated with knowledge about one's surroundings obtained via sensory perception. The cerebral cortex plays a role in allowing humans to do this.",
            "score": 146.88170790672302
        },
        {
            "docid": "8402086_16",
            "document": "Computational neurogenetic modeling . Artificial Neural Networks designed to simulate of the human brain require an ability to learn a variety of tasks that is not required by those designed to accomplish a specific task. Supervised learning is a mechanism by which an artificial neural network can learn by receiving a number of inputs with a correct output already known. An example of an artificial neural network that uses supervised learning is a multilayer perceptron (MLP). In unsupervised learning, an artificial neural network is trained using only inputs. Unsupervised learning is the learning mechanism by which a type of artificial neural network known as a self-organizing map (SOM) learns. Some types of artificial neural network, such as evolving connectionist systems, can learn in both a supervised and unsupervised manner.",
            "score": 130.5538787841797
        },
        {
            "docid": "39182554_20",
            "document": "Catastrophic interference . French (1997) proposed the idea of a pseudo-recurrent backpropagation network in order to help reduce catastrophic interference (see Figure 2). In this model the network is separated into two functionally distinct but interacting sub-networks. This model is biologically inspired and is based on research from McClelland, McNaughton, and O'Reilly (1995). In this research McClelland et al. (1995), suggested that the hippocampus and neocortex act as separable but complementary memory systems. Specifically, the hippocampus short term memory storage and acts gradually over time to transfer memories into the neocortex for long term memory storage. They suggest that the information that is stored can be \"brought back\" to the hippocampus during active rehearsal, reminiscence, and sleep and renewed activation is what acts to transfer the information to the neocortex over time. In the pseudo-recurrent network, one of the sub-networks acts as an early processing area, akin to the hippocampus, and functions to learn new input patters. The other sub-network acts as a final-storage area, akin to the neocortex. However, unlike in McClelland et al. (1995) model, the final-storage area sends internally generated representation back to the early processing area. This creates a recurrent network. French proposed that this interleaving of old representations with new representations is the only way to reduce radical forgetting. Since the brain would most likely not have access to the original input patterns, the patterns that would be fed back to the neocortex would be internally generated representations called \"pseudopatterns\". These pseudopatterns are approximations of previous inputs and they can be interleaved with the learning of new inputs. The use of these pseudopatterns could be biologically plausible as parallels between the consolidation of learning that occurs during sleep and the use of interleaved pseudopatterns. Specifically, they both serve to integrate new information with old information without disruption of the old information. When given an input (and a teacher value) is fed into the pseudo-recurrent network would act as follows:",
            "score": 109.48728597164154
        },
        {
            "docid": "4231622_6",
            "document": "Inferior temporal gyrus . The light energy that comes from the rays bouncing off of an object is converted into chemical energy by the cells in the retina of the eye. This chemical energy is then converted into action potentials that are transferred through the optic nerve and across the optic chiasm, where it is first processed by the lateral geniculate nucleus of the thalamus. From there the information is sent to the primary visual cortex, region V1. It then travels from the visual areas in the occipital lobe to the parietal and temporal lobes via two distinct anatomical streams. These two cortical visual systems were classified by Ungerleider and Mishkin (1982, see two-streams hypothesis). One stream travels ventrally to the inferior temporal cortex (from V1 to V2 then through V4 to ITC) while the other travels dorsally to the posterior parietal cortex. They are labeled the \u201cwhat\u201d and \u201cwhere\u201d streams, respectively. The Inferior Temporal Cortex receives information from the ventral stream, understandably so, as it is known to be a region essential in recognizing patterns, faces, and objects.  The understanding at the single-cell level of the IT cortex and its role of utilizing memory to identify objects and or process the visual field based on color and form visual information is a relatively recent in neuroscience. Early research indicated that the cellular connections of the temporal lobe to other memory associated areas of the brain \u2013 namely the hippocampus, the amygdala, the prefrontal cortex, among others. These cellular connections have recently been found to explain unique elements of memory, suggesting that unique single-cells can be linked to specific unique types and even specific memories. Research into the single-cell understanding of the IT cortex reveals many compelling characteristics of these cells: single-cells with similar selectivity of memory are clustered together across the cortical layers of the IT cortex; the temporal lobe neurons have recently been shown to display learning behaviors and possibly relate to long-term memory; and, cortical memory within the IT cortex is likely to be enhanced over time thanks to the influence of the afferent-neurons of the medial-temporal region. Further research of the single-cells of the IT cortex suggests that these cells not only have a direct link to the visual system pathway but also are deliberate in the visual stimuli they respond to: in certain cases, the single-cell IT cortex neurons do not initiate responses when spots or slits, namely simple visual stimuli, are present in the visual field; however, when complicated objects are put in place, this initiates a response in the single-cell neurons of the IT cortex. This provides evidence that not only are the single-cell neurons of the IT cortex related in having a unique specific response to visual stimuli but rather that each individual single-cell neuron has a specific response to a specific stimuli. The same study also reveals how the magnitude of the response of these single-cell neurons of the IT cortex do not change due to color and size but are only influenced by the shape. This led to even more interesting observations where specific IT neurons have been linked to the recognition of faces and hands. This is very interesting as to the possibility of relating to neurological disorders of prosopagnosia and explaining the complexity and interest in the human hand. Additional research form this study goes into more depth on the role of \"face neurons\" and \"hand neurons\" involved in the IT cortex.  The significance of the single-cell function in the IT cortex is that it is another pathway in addition to the lateral geniculate pathway that processes most visual system: this raises questions about how does it benefit our visual information processing in addition to normal visual pathways and what other functional units are involved in additional visual information processing.",
            "score": 133.00590896606445
        },
        {
            "docid": "52588865_9",
            "document": "William T. Greenough . This view of brain structure, neural activity, and learning was completely overturned by Greenough's research. Greenough initially worked with mice and rat models, later studying primates and humans. His studies demonstrated that fundamental physical changes occurred in neurons in the brain in response to stimulating environments. At the most basic cellular level, the brains of rats that lived in stimulating environments developed more synapses than those that did not. He went on to demonstrate that new synapses were formed as a result of activities that involved learning, not just increased activity. Moreover, changes occurred in areas of the brain that were associated with the performance of specific learned tasks. Observed changes in learning, memory, and synapse formation persisted after training. Learning and memory formation were therefore fundamentally related to ongoing synapse formation. The result of Greenough's work has been a new model of brain 'plasticity' in which long-term memories are formed at a structural level in the brain as part of lifelong processes of learning.",
            "score": 142.04375052452087
        },
        {
            "docid": "40158142_9",
            "document": "Nonlinear system identification . Artificial neural networks try loosely to imitate the network of neurons in the brain where computation takes place through a large number of simple processing elements. A typical neural network consists of a number of simple processing units interconnected to form a complex network. Layers of such units are arranged so that data is entered at the input layer and passes through either one or several intermediate layers before reaching the output layer. In supervised learning the network is trained by operating on the difference between the actual output and the desired output of the network, the prediction error, to change the connection strengths between the nodes. By iterating the weights are modified until the output error reaches an acceptable level. This process is called machine learning because the network adjusts the weights so that the output pattern is reproduced. Neural networks have been extensively studied and there are many excellent textbooks devoted to this topic in general, and more focussed textbooks which emphasise control and systems applications. There are two main problem types that can be studied using neural networks: static problems, and dynamic problems. Static problems include pattern recognition, classification, and approximation. Dynamic problems involve lagged variables and are more appropriate for system identification and related applications. Depending on the architecture of the network the training problem can be either nonlinear-in-the-parameters which involves optimisation or linear-in-the-parameters which can be solved using classical approaches. The training algorithms can be categorised into supervised, unsupervised, or reinforcement learning. Neural networks have excellent approximation properties but these are usually based on standard function approximation results using for example the Weierstrass Theorem that applies equally well to polynomials, rational functions, and other well-known models.  Neural networks have been applied extensively to system identification problems which involve nonlinear and dynamic relationships. However, classical neural networks are purely gross static approximating machines. There is no dynamics within the network. Hence when fitting dynamic models all the dynamics arise by allocating lagged inputs and outputs to the input layer of the network. The training procedure then produces the best static approximation that relates the lagged variables assigned to the input nodes to the output. There are more complex network architectures, including recurrent networks, that produce dynamics by introducing increasing orders of lagged variables to the input nodes. But in these cases it is very easy to over specify the lags and this can lead to over fitting and poor generalisation properties.  Neural networks have several advantages; they are conceptually simple, easy to train and to use, have excellent approximation properties, the concept of local and parallel processing is important and this provides integrity and fault tolerant behaviour. The biggest criticism of the classical neural network models is that the models produced are completely opaque and usually cannot be written down or analysed. It is therefore very difficult to know what is causing what, to analyse the model, or to compute dynamic characteristics from the model. Some of these points will not be relevant to all applications but they are for dynamic modelling.",
            "score": 122.61370635032654
        },
        {
            "docid": "21312297_20",
            "document": "Memory consolidation . Rapid eye movement (REM) sleep has been thought of to be an important concept in the overnight learning in humans by establishing information in the hippocampal and cortical regions of the brain. REM sleep elicits an increase in neuronal activity following an enriched or novel waking experience, thus increasing neuronal plasticity and therefore playing an essential role in the consolidation of memories. This has come into question in recent years however and studies on sleep deprivation have shown that animals and humans who are denied REM sleep do not show deficits in task learning. It has been proposed that since the brain is in a non-memory encoding state during sleep, consolidation would be unlikely to occur. Recent studies have examined the relationship between REM sleep and procedural learning consolidation.  In particular studies have been done on sensory and motor related tasks. In one study testing finger-tapping, people were split into two groups and tested post-training with or without intervening sleep; results concluded that sleep post-training increases both speed and accuracy in this particular task, while increasing the activation of both cortical and hippocampal regions; whereas the post-training awake group had no such improvements. It has been theorized that this may be related more-so to a process of synaptic consolidation rather than systems consolidation because of the short-term nature of the process involved. Researchers examining the effect of sleep on motor learning have noted that while consolidation occurs over a period of 4\u20136 hours during sleep, this is also true during waking hours, which may negate any role of sleep in learning. In this sense sleep would serve no special purpose to enhance consolidation of memories because it occurs independently of sleep. Other studies have examined the process of replay which has been described as a reactivation of patterns that were stimulated during a learning phase. Replay has been demonstrated in the hippocampus and this has lent support to the notion that it serves a consolidation purpose. However, replay is not specific to sleep and both rats and primates show signs during restful-awake periods. Also, replay may simply be residual activation in areas that were involved previously in the learning phase and may have no actual effect on consolidation. This reactivation of the memory traces has also been seen in non-REM sleep specifically for hippocampus-dependant memories. Researchers have noted strong reactivation of the hippocampus during sleep immediately after a learning task. This reactivation led to enhanced performance on the learned task. Researchers following this line of work have come to assume that dreams are a by-product of the reactivation of the brain areas and this can explain why dreams may be unrelated to the information being consolidated. The dream experience itself is not what enhances memory performance but rather it is the reactivation of the neural circuits that causes this.",
            "score": 118.2224634885788
        },
        {
            "docid": "37957946_6",
            "document": "De novo protein synthesis theory of memory formation . A line of research investigates long term potentiation (LTP), a process that describes how a memory can be consolidated between two neurons, or brain cells, ultimately by creating a circuit within the brain that can encode a memory. To initiate a learning circuit between two neurons, one prominent study described using tetanus stimulations to depolarize one neuron by 30mV, which, in turn, activated its NMDA glutamate receptors (Nowak, Bregestovski, Ascher, Herbert, & Prochiantz, 1984). The activation of these receptors resulted in Ca flooding the cell, initiating a cascade of secondary messengers. The cascade of resulting reactions, brought about by secondary messengers, terminates with the activation of cAMP response binding element protein (CREB), which acts as a transcription factor for various genes and initiates their expression (Hawkins, Kandel, & Bailey, 2006). Some proponents argue that the genes stimulate changes in communication between neurons, which underlie the encoding of memory; others suggest that the genes are byproducts of the LTP signaling pathway and are not directly involved in LTP. However, following the cascade of secondary messengers, no one would dispute that more AMPA receptors appear in the postsynaptic terminal (Hayashi et al., 2000). Higher numbers of AMPA receptors, taken together with the aforementioned events, allow for increased firing potential in the postsynaptic cell, which creates an improved learning circuit between these two neurons (Hayashi et al., 2000). Because of the specific, activity-dependent nature of LTP, it is an ideal model for a neural correlate of memory, as postulated by numerous studies; together, these studies show that the abolishment of LTP prevents the formation of memory at the neuronal level (Hawkins, Kandel, & Bailey, 2006).",
            "score": 131.3159590959549
        },
        {
            "docid": "8402086_18",
            "document": "Computational neurogenetic modeling . Because the amount of data on the interplay of genes and neurons and their effects is not enough to construct a rigorous model,  evolutionary computation is used to optimize artificial neural networks and gene regulatory networks, a common technique being the genetic algorithm. A genetic algorithm is a process that can be used to refine models by mimicking the process of natural selection observed in biological ecosystems. The primary advantages are that, due to not requiring derivative information, it can be applied to black box problems and multimodal optimization. The typical process for using genetic algorithms to refine a gene  regulatory network is: first, create a population; next, to create offspring via a crossover operation and  evaluate their fitness; then, on a group chosen for high fitness, simulate mutation via a mutation operator;  finally, taking the now mutated group, repeat this process until a desired level of fitness is demonstrated. Methods by which artificial neural networks may alter their structure without simulated mutation and fitness selection have been developed. A dynamically evolving neural network is one approach, as the creation of new connections and new neurons can  be modeled as the system adapts to new data. This enables the network to evolve in modeling accuracy without simulated natural selection. One method by which dynamically evolving networks may be optimized, called evolving layer neuron aggregation, combines neurons with sufficiently similar input weights into one neuron. This can take place during the training of the network, referred to as online aggregation, or between periods of training, referred to as offline aggregation. Experiments have suggested that offline aggregation is more efficient.",
            "score": 107.32232308387756
        },
        {
            "docid": "39182554_10",
            "document": "Catastrophic interference . Many researchers have suggested that the main cause of catastrophic interference is overlap in the representations at the hidden layer of distributed neural networks. In a distributed representation any given input will tend to create changes in the weights to many of the nodes. Catastrophic forgetting occurs because when many of the weights where \"knowledge is stored\" are changed, it is impossible for prior knowledge to be kept intact. During sequential learning, the inputs become mixed with the new input being superimposed over top of the old input. Another way to conceptualize this is through visualizing learning as movement through a weight space. This weight space can be likened to a spatial representation of all of the possible combinations of weights that the network can possess. When a network first learns to represent a set of patterns, it has found a point in weight space which allows it to recognize all of the patterns that it has seen. However, when the network learns a new set of patterns sequentially it will move to a place in the weight space that allows it to only recognize the new pattern. To recognize both sets of patterns, the network must find a place in weight space that can represent both the new and the old output. One way to do this is by connecting a hidden unit to only a subset of the input units. This reduces the likelihood that two different inputs will be encoded by the same hidden units and weights, and so will decrease the chance of interference. Indeed, a number of the proposed solutions to catastrophic interference involve reducing the amount of overlap that occurs when storing information in these weights.",
            "score": 93.31951761245728
        },
        {
            "docid": "32472154_49",
            "document": "Deep learning . Alternatively, engineers may look for other types of neural networks with more straightforward and convergent training algorithms. CMAC (cerebellar model articulation controller) is one such kind of neural network. It doesn't require learning rates or randomized initial weights for CMAC. The training process can be guaranteed to converge in one step with a new batch of data, and the computational complexity of the training algorithm is linear with respect to the number of neurons involved.",
            "score": 108.62802147865295
        },
        {
            "docid": "33818014_21",
            "document": "Nervous system network models . As mentioned in Section 2.4, development of artificial neural network (ANN), or neural network as it is now called, started as simulation of biological neuron network and ended up using artificial neurons. Major development work has gone into industrial applications with learning process. Complex problems were addressed by simplifying the assumptions. Algorithms were developed to achieve a neurological related performance, such as learning from experience. Since the background and overview have been covered in the other internal references, the discussion here is limited to the types of models. The models are at the system or network level.",
            "score": 89.63256359100342
        },
        {
            "docid": "31217535_15",
            "document": "Memory . Short-term memory is supported by transient patterns of neuronal communication, dependent on regions of the frontal lobe (especially dorsolateral prefrontal cortex) and the parietal lobe. Long-term memory, on the other hand, is maintained by more stable and permanent changes in neural connections widely spread throughout the brain. The hippocampus is essential (for learning new information) to the consolidation of information from short-term to long-term memory, although it does not seem to store information itself. It was thought that without the hippocampus new memories were unable to be stored into long-term memory and that there would be a very short attention span, as first gleaned from patient Henry Molaison after what was thought to be the full removal of both his hippocampi. More recent examination of his brain, post-mortem, shows that the hippocampus was more intact than first thought, throwing theories drawn from the initial data into question. The hippocampus may be involved in changing neural connections for a period of three months or more after the initial learning.",
            "score": 145.91351103782654
        },
        {
            "docid": "32168948_5",
            "document": "Sepp Hochreiter . Sepp Hochreiter developed the long short-term memory (LSTM) for which the first results were reported in his diploma thesis in 1991. The main LSTM paper appeared in 1997 and is considered as a discovery that is a milestone in the timeline of machine learning. LSTM overcomes the problem of recurrent neural networks (RNNs) and deep networks to forget information over time or, equivalently, through layers (vanishing or exploding gradient). LSTM learns from training sequences to process new sequences in order to produce an output (sequence classification) or generate an output sequence (sequence to sequence mapping). Neural networks with LSTM cells solved numerous tasks in biological sequence analysis, drug design, automatic music composition, machine translation, speech recognition, reinforcement learning, and robotics. LSTM with an optimized architecture was successfully applied to very fast protein homology detection without requiring a sequence alignment. LSTM has been used to learn a learning algorithm, that is, LSTM serves as a Turing machine, i.e. as a computer, on which a learning algorithm is executed. Since the LSTM Turing machine is a neural network, it can develop novel learning algorithms by learning on learning problems. It turns out that the learned new learning techniques are superior to those designed by humans. LSTM networks are used in Google Voice transcription, Google voice search, and Google's Allo as core technology for voice searches and commands in the Google App (on Android and iOS), and for dictation on Android devices. Also Apple applies LSTM since iOS 10 in the \"Quicktype\" function.",
            "score": 104.10179734230042
        },
        {
            "docid": "46696891_5",
            "document": "Patricia Janak . Janak is an investigator in the field of the biological basis of behavior working on associative learning. Her work empirically bridges formal learning theories and systems neuroscience in the mammalian brain. Using models based on theories of learning in combination with laboratory experiments, Janak investigates associative processes and the hierarchical organization of relational and representational neural encoding. Electrophysiological recordings from neurons in defined circuits are done using pharmacological and neurophysiological tools to manipulate circuit properties. She has a demonstrated interest in a translational approach to clinical conditions. The fundamental aspects of learning and memory in the models she studies are basic to a broad range of human behaviors and can serve as a guide to intervention and therapies when capacities fail to develop normally or break down through disease. Her most cited article, titled, \"A causal link between prediction errors, dopamine neurons and learning,\" was published in 2013 in \"Nature Neuroscience\".",
            "score": 115.16863775253296
        },
        {
            "docid": "14923645_3",
            "document": "Confabulation (neural networks) . In cognitive science, the generation of confabulatory patterns is symptomatic of some forms of brain trauma. In this, confabulations relate to pathologically induced neural activation patterns depart from direct experience and learned relationships. In computational modeling of such damage, related brain pathologies such as dyslexia and hallucination result from simulated lesioning and neuron death. Forms of confabulation in which missing or incomplete information is incorrectly filled in by the brain are generally modelled by the well known neural network process called pattern completion.",
            "score": 115.75291848182678
        },
        {
            "docid": "1164_9",
            "document": "Artificial intelligence . The earliest (and easiest to understand) approach to AI was symbolism (such as formal logic): \"If an otherwise healthy adult has a fever, then they may have influenza\". A second, more general, approach is Bayesian inference: \"If the current patient has a fever, adjust the probability they have influenza in such-and-such way\". The third major approach, extremely popular in routine business AI applications, are analogizers such as SVM and nearest-neighbor: \"After examining the records of known past patients whose temperature, symptoms, age, and other factors mostly match the current patient, X% of those patients turned out to have influenza\". A fourth approach is harder to intuitively understand, but is inspired by how the brain's machinery works: the artificial neural network approach uses artificial \"neurons\" that can learn by comparing itself to the desired output and altering the strengths of the connections between its internal neurons to \"reinforce\" connections that seemed to be useful. These four main approaches can overlap with each other and with evolutionary systems; for example, neural nets can learn to make inferences, to generalize, and to make analogies. Some systems implicitly or explicitly use multiple of these approaches, alongside many other AI and non-AI algorithms; the best approach is often different depending on the problem. Learning algorithms work on the basis that strategies, algorithms, and inferences that worked well in the past are likely to continue working well in the future. These inferences can be obvious, such as \"since the sun rose every morning for the last 10,000 days, it will probably rise tomorrow morning as well\". They can be nuanced, such as \"X% of families have geographically separate species with color variants, so there is an Y% chance that undiscovered black swans exist\". Learners also work on the basis of \"Occam's razor\": The simplest theory that explains the data is the likeliest. Therefore, to be successful, a learner must be designed such that it prefers simpler theories to complex theories, except in cases where the complex theory is proven substantially better. Settling on a bad, overly complex theory gerrymandered to fit all the past training data is known as overfitting. Many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data, but penalizing the theory in accordance with how complex the theory is. Besides classic overfitting, learners can also disappoint by \"learning the wrong lesson\". A toy example is that an image classifier trained only on pictures of brown horses and black cats might conclude that all brown patches are likely to be horses. A real-world example is that, unlike humans, current image classifiers don't determine the spatial relationship between components of the picture; instead, they learn abstract patterns of pixels that humans are oblivious to, but that linearly correlate with images of certain types of real objects. Faintly superimposing such a pattern on a legitimate image results in an \"adversarial\" image that the system misclassifies. Compared with humans, existing AI lacks several features of human \"commonsense reasoning\"; most notably, humans have powerful mechanisms for reasoning about \"na\u00efve physics\" such as space, time, and physical interactions. This enables even young children to easily make inferences like \"If I roll this pen off a table, it will fall on the floor\". Humans also have a powerful mechanism of \"folk psychology\" that helps them to interpret natural-language sentences such as \"The city councilmen refused the demonstrators a permit because they advocated violence\". (A generic AI has difficulty inferring whether the councilmen or the demonstrators are the ones alleged to be advocating violence.) This lack of \"common knowledge\" means that AI often makes different mistakes than humans make, in ways that can seem incomprehensible. For example, existing self-driving cars cannot reason about the location nor the intentions of pedestrians in the exact way that humans do, and instead must use non-human modes of reasoning to avoid accidents.",
            "score": 96.28059840202332
        },
        {
            "docid": "1896271_17",
            "document": "Holonomic brain theory . In 1969 scientists D. Wilshaw, O. P. Buneman and H. Longuet-Higgins proposed an alternative, non-holographic model that fulfilled many of the same requirements as Gabor's original holographic model. The Gabor model did not explain how the brain could use Fourier analysis on incoming signals or how it would deal with the low signal-noise ratio in reconstructed memories. Longuet-Higgin's correlograph model built on the idea that any system could perform the same functions as a Fourier holograph if it could correlate pairs of patterns. It uses minute pinholes that do not produce diffraction patterns to create a similar reconstruction as that in Fourier holography. Like a hologram, a discrete correlograph can recognize displaced patterns and store information in a parallel and non-local way so it usually will not be destroyed by localized damage. They then expanded the model beyond the correlograph to an associative net where the points become parallel lines arranged in a grid. Horizontal lines represent axons of input neurons while vertical lines represent output neurons. Each intersection represents a modifiable synapse. Though this cannot recognize displaced patterns, it has a greater potential storage capacity. This was not necessarily meant to show how the brain is organized, but instead to show the possibility of improving on Gabor's original model. P. Van Heerden countered this model by demonstrating mathematically that the signal-noise ratio of a hologram could reach 50% of ideal. He also used a model with a 2D neural hologram network for fast searching imposed upon a 3D network for large storage capacity. A key quality of this model was its flexibility to change the orientation and fix distortions of stored information, which is important for our ability to recognize an object as the same entity from different angles and positions, something the correlograph and association network models lack.",
            "score": 138.7385652065277
        },
        {
            "docid": "10839226_23",
            "document": "Cultured neuronal network . In order to establish learning in a cultured network, researchers have attempted to re-embody the dissociated neuronal networks in either simulated or real environments (see MEART and animat). Through this method the networks are able to interact with their environment and, therefore, have the opportunity to learn in a more realistic setting. Other studies have attempted to imprint signal patterns onto the networks via artificial stimulation. This can be done by inducing network bursts or by inputing specific patterns to the neurons, from which the network is expected to derive some meaning (as in experiments with animats, where an arbitrary signal to the network indicates that the simulated animal has run into a wall or is moving in a direction, etc.). The latter technique attempts to take advantage of the inherent ability of neuronal networks to make sense of patterns. However, experiments have had limited success in demonstrating a definition of learning that is widely agreed upon. Nevertheless, plasticity in neuronal networks is a phenomenon that is well-established in the neuroscience community, and one that is thought to play a very large role in learning.",
            "score": 74.2588324546814
        },
        {
            "docid": "29798108_7",
            "document": "Malleability of intelligence . Neural plasticity refers to any change in the structure of the neural network that forms the central nervous system. Neural plasticity is the neuronal basis for changes in how the mind works, including learning, the formation of memory, and changes in intelligence. One well-studied form of plasticity is Long-Term Potentiation (LTP). It refers to a change in neural connectivity as a result of high activation on both sides of a synaptic cleft. This change in neural connectivity allows information to be more easily processed, as the neural connection associated with that information becomes stronger through LTP. Other forms of plasticity involve the growth of new neurons, the growth of new connections between neurons, and the selective elimination of such connection, called \"dendritic pruning\".",
            "score": 104.1710205078125
        },
        {
            "docid": "32168948_6",
            "document": "Sepp Hochreiter . Neural networks are different types of simplified mathematical models of biological neural networks like those in human brains.  In feedforward neural networks (NNs) the information moves forward in only one direction,  from the input layer that receives information from the environment,  through the hidden layers to the output layer that supplies the information to the environment. Unlike NNs, recurrent neural networks (RNNs)  can use their internal memory to process arbitrary sequences of inputs.  If data mining is based on neural networks, overfitting reduces the network's capability to correctly process future data. To avoid overfitting, Sepp Hochreiter developed algorithms for finding low complexity neural networks like \"Flat Minimum Search\" (FMS), which searches for a \"flat\" minimum \u2014 a large connected region in the parameter space where the network function is constant. Thus, the network parameters can be given with low precision which means a low complex network that avoids overfitting. Low complexity neural networks are well suited for deep learning because they control the complexity in each network layer and, therefore, learn hierarchical representations of the input. Sepp Hochreiter's group introduced \"exponential linear units\" (ELUs) which speed up learning in deep neural networks and lead to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs), and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However, ELUs have improved learning characteristics compared to ReLUs, due to negative values which push mean unit activations closer to zero. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect.",
            "score": 118.61923670768738
        },
        {
            "docid": "2856189_4",
            "document": "Caleb Gattegno . Gattegno noticed that there is an \u201cenergy budget\u201d for learning. Human beings have a highly developed sense of the economics of their own energy and are very sensitive to the cost involved in using it. It is therefore essential to teach in ways that are efficient in terms of the amount of energy spent by learners. To be able to mathematically determine whether one method was more efficient than another, he created a unit of measurement for the effort used to learn. He called that unit an \"ogden\", and one can only say an \"ogden\" has been spent if the learning was done outside of ordinary functionings, and was retained. For example, learning one word in a foreign language costs one \"ogden\", but if the word cannot be recalled, the \"ogden\" has not truly been spent. Gattegno's teaching materials and techniques were designed to be economical with \"ogdens\", so that the most information can be recalled with the least sense of effort.",
            "score": 112.29830169677734
        },
        {
            "docid": "7330954_3",
            "document": "Pattern recognition (psychology) . Pattern recognition occurs when information from the environment is received and entered into short-term memory, causing automatic activation of a specific content of long-term memory. An early example of this is learning the alphabet in order. When a carer repeats \u2018A, B, C\u2019 multiple times to a child, utilizing the pattern recognition, the child says \u2018C\u2019 after he/she hears \u2018A, B\u2019 in order. Recognizing patterns allow us to predict and expect what is coming. The process of pattern recognition involves matching the information received with the information already stored in the brain. Making the connection between memories and information perceived is a step of pattern recognition called identification. Pattern recognition requires repetition of experience. Semantic memory, which is used implicitly and subconsciously is the main type of memory involved with recognition.",
            "score": 97.02759504318237
        },
        {
            "docid": "21523_125",
            "document": "Artificial neural network . Many types of models are used, defined at different levels of abstraction and modeling different aspects of neural systems. They range from models of the short-term behavior of individual neurons, models of how the dynamics of neural circuitry arise from interactions between individual neurons and finally to models of how behavior can arise from abstract neural modules that represent complete subsystems. These include models of the long-term, and short-term plasticity, of neural systems and their relations to learning and memory from the individual neuron to the system level.",
            "score": 112.38220286369324
        }
    ],
    "r": [
        {
            "docid": "31217535_44",
            "document": "Memory . One question that is crucial in cognitive neuroscience is how information and mental experiences are coded and represented in the brain. Scientists have gained much knowledge about the neuronal codes from the studies of plasticity, but most of such research has been focused on simple learning in simple neuronal circuits; it is considerably less clear about the neuronal changes involved in more complex examples of memory, particularly declarative memory that requires the storage of facts and events (Byrne 2007). Convergence-divergence zones might be the neural networks where memories are stored and retrieved. Considering that there are several kinds of memory, depending on types of represented knowledge, underlying mechanisms, processes functions and modes of acquisition, it is likely that different brain areas support different memory systems and that they are in mutual relationships in neuronal networks: \"components of memory representation are distributed widely across different parts of the brain as mediated by multiple neocortical circuits\".",
            "score": 152.42181396484375
        },
        {
            "docid": "39177819_4",
            "document": "Cognitive computer . Intel's self-learning neuromorphic chip, named Loihi, perhaps named after the Hawaiian seamount Loihi, offers substantial power efficiency designed after the human brain. Intel claims Loihi is about 1000 times more energy efficient than the general-purpose computing power needed to train the neural networks that rival Loihi\u2019s performance.  In theory, this would support both machine learning training and inference on the same silicon independently of a cloud connection, and more efficient than using convolutional neural networks (CNNs) or deep learning neural networks. Intel points to a system for monitoring a person\u2019s heartbeat, taking readings after events such as exercise or eating, and uses the cognitive computing chip to normalize the data and work out the \u2018normal\u2019 heartbeat. It can then spot abnormalities, but also deal with any new events or conditions.",
            "score": 148.68479919433594
        },
        {
            "docid": "27811_4",
            "document": "Sleep and learning . Popular sayings can reflect the notion that remolded memories produce new creative associations in the morning, and that performance often improves after a time-interval that includes sleep. Current studies demonstrate that a healthy sleep produces a significant learning-dependent performance boost. The idea is that sleep helps the brain to edit its memory, looking for important patterns and extracting overarching rules which could be described as 'the gist', and integrating this with existing memory. The 'synaptic scaling' hypothesis suggests that sleep plays an important role in regulating learning that has taken place while awake, enabling more efficient and effective storage in the brain, making better use of space and energy.",
            "score": 147.45606994628906
        },
        {
            "docid": "1095131_20",
            "document": "Kinesthetic learning . The cerebral cortex is the brain tissue covering the top and sides of the brain in most vertebrates. It is involved in storing and processing of sensory inputs and motor outputs. In the human brain, the cerebral cortex is actually a sheet of neural tissue about 1/8th inch thick. The sheet is folded so that it can fit inside the skull. The neural circuits in this area of the brain expand with practice of an activity, just like the synaptic plasticity grows with practice. Clarification of some of the mechanisms of learning by neuro science has been advanced, in part, by the advent of non-invasive imaging technologies, such as positron emission tomography (PET) and functional magnetic resonance imaging (FMRI). These technologies have allowed researchers to observe human learning processes directly. Through these types of technologies, we are now able to see and study what happens in the process of learning. In different tests performed the brain being imaged showed a greater blood flow and activation to that area of the brain being stimulated through different activities such as finger tapping in a specific sequence. It has been revealed that the process at the beginning of learning a new skill happens quickly, and later on slows down to almost a plateau. This process can also be referred to as The Law of Learning. The slower learning showed in the FMRI that in the cerebral cortex this was when the long term learning was occurring, suggesting that the structural changes in the cortex reflect the enhancement of skill memories during later stages of training. When a person studies a skill for a longer duration of time, but in a shorter amount of time they will learn quickly, but also only retain the information into their short-term memory. Just like studying for an exam; if a student tries to learn everything the night before, it will not stick in the long run. If a person studies a skill for a shorter duration of time, but more frequently and long-term, their brain will retain this information much longer as it is stored in the long-term memory. Functional and structural studies of the brain have revealed a vast interconnectivity between diverse regions of the cerebral cortex. For example, large numbers of axons interconnect the posterior sensory areas serving vision, audition, and touch with anterior motor regions. Constant communication between sensation and movement makes sense, because to execute smooth movement through the environment, movement must be continuously integrated with knowledge about one's surroundings obtained via sensory perception. The cerebral cortex plays a role in allowing humans to do this.",
            "score": 146.8817138671875
        },
        {
            "docid": "31217535_15",
            "document": "Memory . Short-term memory is supported by transient patterns of neuronal communication, dependent on regions of the frontal lobe (especially dorsolateral prefrontal cortex) and the parietal lobe. Long-term memory, on the other hand, is maintained by more stable and permanent changes in neural connections widely spread throughout the brain. The hippocampus is essential (for learning new information) to the consolidation of information from short-term to long-term memory, although it does not seem to store information itself. It was thought that without the hippocampus new memories were unable to be stored into long-term memory and that there would be a very short attention span, as first gleaned from patient Henry Molaison after what was thought to be the full removal of both his hippocampi. More recent examination of his brain, post-mortem, shows that the hippocampus was more intact than first thought, throwing theories drawn from the initial data into question. The hippocampus may be involved in changing neural connections for a period of three months or more after the initial learning.",
            "score": 145.91351318359375
        },
        {
            "docid": "35242701_4",
            "document": "Internet bottleneck . The network demands of users continues to grow and with it so do the pressures on networks. The way current technologies process information over the network is slow and consumes large amounts of energy. ISPs and engineers argue that these issues with the increased demand on the networks result in some necessary congestion, but the bottlenecks also occur because of the lack of technology to handle such huge data needs using minimal energy. There are attempts being made to increase the speed, amount of data, and reduce power consumption of the networks. For example, optical memory devices could be used in the future to send and receive light signals working much faster and more efficiently than electrical signals. Some researchers see optical memory as needed to reduce the demands on the network routers in data transmission, while others do not. The research will continue to explore possibilities for greater network bandwidth and data transfer. As data consumption needs increase, so will the need for better technology that facilitates the transfer and storage of that data.",
            "score": 145.209228515625
        },
        {
            "docid": "52588865_9",
            "document": "William T. Greenough . This view of brain structure, neural activity, and learning was completely overturned by Greenough's research. Greenough initially worked with mice and rat models, later studying primates and humans. His studies demonstrated that fundamental physical changes occurred in neurons in the brain in response to stimulating environments. At the most basic cellular level, the brains of rats that lived in stimulating environments developed more synapses than those that did not. He went on to demonstrate that new synapses were formed as a result of activities that involved learning, not just increased activity. Moreover, changes occurred in areas of the brain that were associated with the performance of specific learned tasks. Observed changes in learning, memory, and synapse formation persisted after training. Learning and memory formation were therefore fundamentally related to ongoing synapse formation. The result of Greenough's work has been a new model of brain 'plasticity' in which long-term memories are formed at a structural level in the brain as part of lifelong processes of learning.",
            "score": 142.0437469482422
        },
        {
            "docid": "3717_69",
            "document": "Brain . Theorists have worked to understand these response patterns by constructing mathematical models of neurons and neural networks, which can be simulated using computers. Some useful models are abstract, focusing on the conceptual structure of neural algorithms rather than the details of how they are implemented in the brain; other models attempt to incorporate data about the biophysical properties of real neurons. No model on any level is yet considered to be a fully valid description of brain function, though. The essential difficulty is that sophisticated computation by neural networks requires distributed processing in which hundreds or thousands of neurons work cooperatively\u2014current methods of brain activity recording are only capable of isolating action potentials from a few dozen neurons at a time.",
            "score": 141.54978942871094
        },
        {
            "docid": "19378_43",
            "document": "Mind . Neuroscience studies the nervous system, the physical basis of the mind. At the systems level, neuroscientists investigate how biological neural networks form and physiologically interact to produce mental functions and content such as reflexes, multisensory integration, motor coordination, circadian rhythms, emotional responses, learning, and memory. At a larger scale, efforts in computational neuroscience have developed large-scale models that simulate simple, functioning brains. As of 2012, such models include the thalamus, basal ganglia, prefrontal cortex, motor cortex, and occipital cortex, and consequentially simulated brains can learn, respond to visual stimuli, coordinate motor responses, form short-term memories, and learn to respond to patterns. Currently, researchers aim to program the hippocampus and limbic system, hypothetically imbuing the simulated mind with long-term memory and crude emotions.",
            "score": 139.03749084472656
        },
        {
            "docid": "1896271_17",
            "document": "Holonomic brain theory . In 1969 scientists D. Wilshaw, O. P. Buneman and H. Longuet-Higgins proposed an alternative, non-holographic model that fulfilled many of the same requirements as Gabor's original holographic model. The Gabor model did not explain how the brain could use Fourier analysis on incoming signals or how it would deal with the low signal-noise ratio in reconstructed memories. Longuet-Higgin's correlograph model built on the idea that any system could perform the same functions as a Fourier holograph if it could correlate pairs of patterns. It uses minute pinholes that do not produce diffraction patterns to create a similar reconstruction as that in Fourier holography. Like a hologram, a discrete correlograph can recognize displaced patterns and store information in a parallel and non-local way so it usually will not be destroyed by localized damage. They then expanded the model beyond the correlograph to an associative net where the points become parallel lines arranged in a grid. Horizontal lines represent axons of input neurons while vertical lines represent output neurons. Each intersection represents a modifiable synapse. Though this cannot recognize displaced patterns, it has a greater potential storage capacity. This was not necessarily meant to show how the brain is organized, but instead to show the possibility of improving on Gabor's original model. P. Van Heerden countered this model by demonstrating mathematically that the signal-noise ratio of a hologram could reach 50% of ideal. He also used a model with a 2D neural hologram network for fast searching imposed upon a 3D network for large storage capacity. A key quality of this model was its flexibility to change the orientation and fix distortions of stored information, which is important for our ability to recognize an object as the same entity from different angles and positions, something the correlograph and association network models lack.",
            "score": 138.7385711669922
        },
        {
            "docid": "17454631_17",
            "document": "Selfish brain theory . The \"Selfish Brain\" theory can be considered as a new way to understand obesity. Disorders in the control centers of the brain such as the hippocampus, amygdala and hypothalamus are thought to underlie this, as outlined above. Whatever the type of disruption that exists, it entails that the energy procurement for the brain is accomplished less by allocation and more by the intake of nutrients even though the muscles have no additional energy requirement. If one imagines the energy supply of the human organism as a supply-chain that passes from the outside world with its numerous options for nutrient intake via the body to the brain as the end user and control organ, then obesity can be considered as being caused by a build-up in this supply-chain. This is characterized by an excessive accumulation of energy in the adipose tissue or blood. An allocation failure is expressed as a weakening of the sympathetic nervous system (SNS). The result is that energy intended for the brain mainly enters buffer storage areas, i.e. the adipose tissue and the musculature. Only a small proportion reaches the brain. In order to cover its huge energy needs the brain commands the individual to consume more food. The accumulation process escalates, and the buffer storage areas are continuously filled up. This leads to the development of obesity. In many cases, at a time which is dependent on an affected individual's personal disposition, obesity can also be overlain by a diabetes mellitus. In such a situation the adipose tissue and musculature can no longer accept any energy, and the energy then accumulates in the blood so that hyperglycemia results.",
            "score": 138.54833984375
        },
        {
            "docid": "3267910_7",
            "document": "Solar chemical . In order for an isomer to store energy then, it must be metastable as shown above. This results in a trade-off between the stability of the fuel isomer and how much energy must be put in to reverse the reaction when it is time to use the fuel. The isomer stores energy as strain energy in its bonds. The more strained the bonds are the more energy they can store, but the less stable the molecule is. The activation energy, Ea, is used to characterize how easy or hard it is for the reaction to proceed. If the activation energy is too small the fuel will tend to spontaneously move to the more stable state, providing limited usefulness as a storage medium. However, if the activation energy is very large, the energy expended to extract the energy from the fuel will effectively reduce the amount of energy that the fuel can store. Finding a useful molecule for a solar fuel requires finding the proper balance between the yield, the light absorption of the molecule, the stability of the molecule in the metastable state, and how many times the molecule can be cycled without degrading.",
            "score": 138.267333984375
        },
        {
            "docid": "2872287_26",
            "document": "Neural binding . Cognitive binding is associated with the different states of human consciousness. Two of the most studied states of consciousness are the wakefulness and REM sleep. There have been multiple studies showing, electrophysiologically, that these two states are quite similar in nature. This has led some neural binding theorists to study the modes of cognitive awareness in each state. Certain observations have even led these scientists to hypothesize that since there is little cognition going on during REM sleep, the increased thalamocortical responses show the action of processing in the waking preconscious. The thalamus and cortex are important anatomical features in cognitive and sensory awareness. The understanding of how these neurons fire and relate to one other in each of these states (REM and Waking) is paramount to understanding awareness and its relation to neural binding. In the waking state, neuronal activity in animals is subject to changes based on the current environment. Changes in environment act as a form of stress on the brain so that when sensory neurons are then fired synchronously, they acclimate to the new state. This new state can then be moved to the hippocampus where it can be stored for later use. In the words of James Newman and Anthony A. Grace in their article, \"Binding Across Time\" this idea is put forth: \"The hippocampus is the primary recipient of inferotemporal outputs and is known to be the substrate for the consolidation of working memories to long term, episodic memories.\" The logging of \"episodes\" is then used for \"streaming\", which can mediate by the selective gating of certain information reentering sensory awareness. Streaming and building of episodic memories would not be possible if neural binding did not unconsciously connect the two synchronous oscillations. The pairing of these oscillations can then help input the correct sensory material. If these paired oscillations are not new, then cognitively these firings will be easily understood. If there are new firings, the brain will have to acclimate to the new understanding. In REM sleep, the only extreme difference from the waking state is that the brain does not have the actual waking amount of sensory firings, so cognitively, there is not as much awareness here, although the activity of the \"brain\u2019s eye\" is still quite significant and very similar to the waking state. Studies have shown that during sleep there are still 40\u00a0Hz Oscillation firings. These firings are due to the perceived stimuli happening in dreams. \"",
            "score": 138.22958374023438
        },
        {
            "docid": "13967547_5",
            "document": "Dry lab . As a means of surpassing the limitations of these techniques, projects such as Folding@home and Rosetta@home are aimed at resolving this problem using computational analysis, this means of resolving protein structure is referred to as protein structure prediction. Although many labs have a slightly different approach, the main concept is to find, from a myriad of protein conformations, which conformation has the lowest energy or, in the case of Folding@Home, to find relatively low energies of proteins that could cause the protein to misfold and aggregate other proteins to itself\u2014like in the case of sickle cell anemia. The general scheme in these projects is that a small number of computations are parsed to, or sent to be calculated on, a computer, generally a home computer, and then that computer analyzes the likelihood that a specific protein will take a certain shape or conformation based on the amount of energy required for that protein to stay in that shape, this way of processing data is what is generally referred to as distributed computing. This analysis is done on an extraordinarily large number of different conformations, owing to the support of hundreds of thousands of home-based computers, in hopes to find the conformation of lowest possible energy or set of conformations of lowest possible energy relative to any conformations that are just slightly different. Although doing so is quite difficult, one can, by observing the energy distribution of a large number of conformations, despite the almost infinite number of different protein conformations possible for any given protein (see Levinthal Paradox), with a reasonably large number of protein energy samplings, predict relatively closely what conformation, within a range of conformations, has the expected lowest energy using methods in statistical inference. There are other factors such as salt concentration, pH, ambient temperature or chaperonins, which are proteins that assist in the folding process of other proteins, that can greatly affect how a protein folds. However, if the given protein is shown to fold on its own, especially in vitro, these findings can be further supported. Once we can see how a protein folds then we can see how it works as a catalyst, or in intracellular communication, e.g. neuroreceptor-neurotransmitter interaction. How certain compounds may be used to enhance or prevent the function of these proteins and how an elucidated protein overall plays a role in diseases such as Alzheimer's Disease or Huntington's Disease can also be much better understood.",
            "score": 136.85653686523438
        },
        {
            "docid": "39403556_16",
            "document": "Free energy principle . Free energy minimisation provides a useful way to formulate normative (Bayes optimal) models of neuronal inference and learning under uncertainty and therefore subscribes to the Bayesian brain hypothesis. The neuronal processes described by free energy minimisation depend on the nature of hidden states: formula_20 that can comprise time dependent variables, time invariant parameters and the precision (inverse variance or temperature) of random fluctuations. Minimising variables, parameters and precision corresponds to inference, learning and the encoding of uncertainty, respectively:",
            "score": 135.4463653564453
        },
        {
            "docid": "3717_40",
            "document": "Brain . Brain tissue consumes a large amount of energy in proportion to its volume, so large brains place severe metabolic demands on animals. The need to limit body weight in order, for example, to fly, has apparently led to selection for a reduction of brain size in some species, such as bats. Most of the brain's energy consumption goes into sustaining the electric charge (membrane potential) of neurons. Most vertebrate species devote between 2% and 8% of basal metabolism to the brain. In primates, however, the percentage is much higher\u2014in humans it rises to 20\u201325%. The energy consumption of the brain does not vary greatly over time, but active regions of the cerebral cortex consume somewhat more energy than inactive regions; this forms the basis for the functional brain imaging methods of PET, fMRI, and NIRS. The brain typically gets most of its energy from oxygen-dependent metabolism of glucose (i.e., blood sugar), but ketones provide a major alternative source, together with contributions from medium chain fatty acids (caprylic and heptanoic acids), lactate, acetate, and possibly amino acids.",
            "score": 134.9250030517578
        },
        {
            "docid": "52036598_5",
            "document": "Differentiable neural computer . DNC networks were introduced as an extension of the Neural Turing Machine (NTM), with the addition of memory attention mechanisms that control where the memory is stored, and temporal attention that records the order of events. This structure allows DNCs to be more robust and abstract than a NTM, and still perform tasks that have longer-term dependencies than some of its predecessors such as the LSTM network. The memory, which is simply a matrix, can be allocated dynamically and accessed indefinitely. The DNC is differentiable end-to-end (each subcomponent of the model is differentiable, therefore so is the whole model). This makes it possible to optimize them efficiently using gradient descent. It learns how to store and retrieve the information such that it satisfies the task execution.",
            "score": 134.65428161621094
        },
        {
            "docid": "26685721_36",
            "document": "Methods used to study memory . Songbirds are excellent animal models for studying learning and auditory memory. These birds have highly developed vocal organs, which give them the ability to create diverse and elaborate birdsongs. Neurotoxic lesions can be used to study how specific brain structures are key for this type of learning and scientists can also manipulate the environment in which these birds are raised. In addition, with functional MRI the response to different auditory stimuli has been studied in vivo. In experiments with songbirds, using these two methodologies has led to very interesting discoveries about auditory learning and memory. Much like humans, birds have a critical period when they must be exposed to adult birdsong. The cognitive systems of vocal production and auditory recognition parallel those in humans, and experiments have shown that there are critical brain structures for these processes.",
            "score": 134.16050720214844
        },
        {
            "docid": "1729542_31",
            "document": "Neural network . Arguments for Dewdney's position are that to implement large and effective software neural networks, much processing and storage resources need to be committed. While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a most simplified form on Von Neumann technology may compel a neural network designer to fill many millions of database rows for its connections - which can consume vast amounts of computer memory and hard disk space. Furthermore, the designer of neural network systems will often need to simulate the transmission of signals through many of these connections and their associated neurons - which must often be matched with incredible amounts of CPU processing power and time. While neural networks often yield \"effective\" programs, they too often do so at the cost of \"efficiency\" (they tend to consume considerable amounts of time and money).",
            "score": 133.8635711669922
        },
        {
            "docid": "21107290_10",
            "document": "Bayesian approaches to brain function . During the 1990s some researchers such as Geoffrey Hinton and Karl Friston began examining the concept of free energy as a calculably tractable measure of the discrepancy between actual features of the world and representations of those features captured by neural network models. A synthesis has been attempted recently by Karl Friston, in which the Bayesian brain emerges from a general principle of free energy minimisation. In this framework, both action and perception are seen as a consequence of suppressing free-energy, leading to perceptual and active inference and a more embodied (enactive) view of the Bayesian brain. Using variational Bayesian methods, it can be shown how internal models of the world are updated by sensory information to minimize free energy or the discrepancy between sensory input and predictions of that input. This can be cast (in neurobiologically plausible terms) as predictive coding or, more generally, Bayesian filtering.",
            "score": 133.75601196289062
        },
        {
            "docid": "21312313_45",
            "document": "Procedural memory . It is evident that long-term Cocaine abuse alters brain structures. Research has shown that the brain structures that are immediately affected by long-term cocaine abuse include: cerebral hypoperfusion in the frontal, periventricular and temporal-parietal. These structures play a role in various memory systems. Furthermore, the drug cocaine elicits its desirable effects by blocking the DRD1 dopamine receptors in the striatum, resulting in increased dopamine levels in the brain. These receptors are important for the consolidation of procedural memory. These increased dopamine levels in the brain resultant of cocaine use is similar to the increased dopamine levels in the brain found in schizophrenics. Studies have compared the common memory deficits caused by both cases to further understand the neural networks of procedural memory. To learn more about the effects of dopamine and its role in schizophrenia see: dopamine hypothesis of schizophrenia. Studies using rats have shown that when rats are administered trace amounts of cocaine, their procedural memory systems are negatively impacted. Specifically, the rats are unable to effectively consolidate motor-skill learning. With cocaine abuse being associated with poor procedural learning, research has shown that abstinence from cocaine is associated with sustained improvement of motor-skill learning (Wilfred et al.).",
            "score": 133.5614471435547
        },
        {
            "docid": "173354_57",
            "document": "Automation . Automation of homes and home appliances is also thought to impact the environment, but the benefits of these features are also questioned. A study of energy consumption of automated homes in Finland showed that smart homes could reduce energy consumption by monitoring levels of consumption in different areas of the home and adjusting consumption to reduce energy leaks (such as automatically reducing consumption during the nighttime when activity is low). This study, along with others, indicated that the smart home\u2019s ability to monitor and adjust consumption levels would reduce unnecessary energy usage. However, new research suggests that smart homes might not be as efficient as non-automated homes. A more recent study has indicated that, while monitoring and adjusting consumption levels does decrease unnecessary energy use, this process requires monitoring systems that also consume a significant amount of energy. This study suggested that the energy required to run these systems is so much so that it negates any benefits of the systems themselves, resulting in little to no ecological benefit.",
            "score": 133.07171630859375
        },
        {
            "docid": "4231622_6",
            "document": "Inferior temporal gyrus . The light energy that comes from the rays bouncing off of an object is converted into chemical energy by the cells in the retina of the eye. This chemical energy is then converted into action potentials that are transferred through the optic nerve and across the optic chiasm, where it is first processed by the lateral geniculate nucleus of the thalamus. From there the information is sent to the primary visual cortex, region V1. It then travels from the visual areas in the occipital lobe to the parietal and temporal lobes via two distinct anatomical streams. These two cortical visual systems were classified by Ungerleider and Mishkin (1982, see two-streams hypothesis). One stream travels ventrally to the inferior temporal cortex (from V1 to V2 then through V4 to ITC) while the other travels dorsally to the posterior parietal cortex. They are labeled the \u201cwhat\u201d and \u201cwhere\u201d streams, respectively. The Inferior Temporal Cortex receives information from the ventral stream, understandably so, as it is known to be a region essential in recognizing patterns, faces, and objects.  The understanding at the single-cell level of the IT cortex and its role of utilizing memory to identify objects and or process the visual field based on color and form visual information is a relatively recent in neuroscience. Early research indicated that the cellular connections of the temporal lobe to other memory associated areas of the brain \u2013 namely the hippocampus, the amygdala, the prefrontal cortex, among others. These cellular connections have recently been found to explain unique elements of memory, suggesting that unique single-cells can be linked to specific unique types and even specific memories. Research into the single-cell understanding of the IT cortex reveals many compelling characteristics of these cells: single-cells with similar selectivity of memory are clustered together across the cortical layers of the IT cortex; the temporal lobe neurons have recently been shown to display learning behaviors and possibly relate to long-term memory; and, cortical memory within the IT cortex is likely to be enhanced over time thanks to the influence of the afferent-neurons of the medial-temporal region. Further research of the single-cells of the IT cortex suggests that these cells not only have a direct link to the visual system pathway but also are deliberate in the visual stimuli they respond to: in certain cases, the single-cell IT cortex neurons do not initiate responses when spots or slits, namely simple visual stimuli, are present in the visual field; however, when complicated objects are put in place, this initiates a response in the single-cell neurons of the IT cortex. This provides evidence that not only are the single-cell neurons of the IT cortex related in having a unique specific response to visual stimuli but rather that each individual single-cell neuron has a specific response to a specific stimuli. The same study also reveals how the magnitude of the response of these single-cell neurons of the IT cortex do not change due to color and size but are only influenced by the shape. This led to even more interesting observations where specific IT neurons have been linked to the recognition of faces and hands. This is very interesting as to the possibility of relating to neurological disorders of prosopagnosia and explaining the complexity and interest in the human hand. Additional research form this study goes into more depth on the role of \"face neurons\" and \"hand neurons\" involved in the IT cortex.  The significance of the single-cell function in the IT cortex is that it is another pathway in addition to the lateral geniculate pathway that processes most visual system: this raises questions about how does it benefit our visual information processing in addition to normal visual pathways and what other functional units are involved in additional visual information processing.",
            "score": 133.0059051513672
        },
        {
            "docid": "27024757_4",
            "document": "Nutritional neuroscience . Relatively speaking, the brain consumes an immense amount of energy in comparison to the rest of the body. The human brain is approximately 2% of the human body mass and uses 20\u201325% of the total energy expenditure. Therefore, mechanisms involved in the transfer of energy from foods to neurons are likely to be fundamental to the control of brain function. Insufficient intake of selected vitamins, or certain metabolic disorders, affect cognitive processes by disrupting the nutrient-dependent processes within the body that are associated with the management of energy in neurons, which can subsequently affect neurotransmission, synaptic plasticity, and cell survival.",
            "score": 132.91683959960938
        },
        {
            "docid": "37957946_2",
            "document": "De novo protein synthesis theory of memory formation . The de novo protein synthesis theory of memory formation is a hypothesis about the formation of the physical correlates of memory in the brain. It is widely accepted that the physiological correlates for memories are stored at the synapse between various neurons. The relative strength of various synapses in a network of neurons form the memory trace, or \u2018engram,\u2019 though the processes that support this finding are less thoroughly understood. The de novo protein synthesis theory states that the production of proteins is required to initiate and potentially maintain these plastic changes within the brain. It has much support within the neuroscience community, but some critics claim that memories can be made independent of protein synthesis.",
            "score": 132.8490447998047
        },
        {
            "docid": "10974486_13",
            "document": "Storage (memory) . Anderson shows that combination of Hebbian learning rule and McCullough\u2013Pitts dynamical rule allow network to generate a weight matrix that can store associations between different memory patterns \u2013 such matrix is the form of memory storage for the neural network model. Major differences between the matrix of multiple traces hypothesis and the neural network model is that while new memory indicates extension of the existing matrix for the multiple traces hypothesis, weight matrix of the neural network model does not extend; rather, the weight is said to be updated with introduction of new association between neurons.",
            "score": 132.6331024169922
        },
        {
            "docid": "9848870_23",
            "document": "Energy efficiency in transport . Finally, vehicle energy efficiency calculations would be misleading without factoring the energy cost of producing the vehicle itself. This initial energy cost can of course be depreciated over the life of the vehicle to calculate an average energy efficiency over its effective life span. In other words, vehicles that take a lot of energy to produce and are used for relatively short periods will require a great deal more energy over their effective lifespan than those that do not, and are therefore much less energy efficient than they may otherwise seem. Hybrid and electric cars use less energy in their operation than comparable petroleum-fueled cars but more energy is used to manufacture them, so the overall difference would be less than immediately apparent. Compare, for example, walking, which requires no special equipment at all, and an automobile, produced in and shipped from another country, and made from parts manufactured around the world from raw materials and minerals mined and processed elsewhere again, and used for a limited number of years. According to the French energy and environment agency ADEME, an average motor car has an embodied energy content of 20,800 kWh and an average electric vehicle amounts to 34,700 kWh. The electric car requires nearly twice as much energy to produce, primarily due to the large amount of mining and purification necessary for the rare earth metals and other materials used in lithium-ion batteries and in the electric drive motors. This represents a significant portion of the energy used over the life of the car (in some cases nearly as much as energy that is used through the fuel that is consumed, effectively doubling the car's per-distance energy consumption), and cannot be ignored when comparing automobiles to other transport modes. It is important to note, also, that as these are average numbers for French automobiles and they are likely to be significantly larger in more auto-centric countries like the United States and Canada, where much larger and heavier cars are more common.",
            "score": 131.52072143554688
        },
        {
            "docid": "43646756_3",
            "document": "Cosmic Evolution (book) . Chaisson argues that cosmic history can be examined from the perspective of energy flows. He analyzes the flows of energy through various objects, and argues that these flows are relevant to understanding the relative complexity of these objects. He suggests that a key measure for scientific analysis should be energy per second per gram, termed \"energy rate density,\" and that analysis using this yardstick can be used to explain not only human evolution but cosmic evolution. He sees energy as \"work per unit time\" which he equates with power, and shows how energy rate density in some structures has increased over time. For example, in Chaisson's view, the human brain uses a much greater amount of energy, relative to its size, than a galaxy. He suggests that energy lets us make \"order out of disorder\"; for example, an air conditioner, which draws current from an electric outlet, can turn a less-complex zone of lukewarm air into two more-complex zones of hot air and cold air, and in so doing, it reverses the disorder in a room. According to his view, organisms do much the same thing with energy but in a more complex way, by taking in food instead of electrons, to keep themselves from disintegrating and becoming less complex; he analyzes energy flows in not just organisms and society, but in inanimate structures such as stars, galaxies, planets.",
            "score": 131.46763610839844
        },
        {
            "docid": "32107929_2",
            "document": "Mark A. Gluck . Mark A. Gluck is a professor of neuroscience at Rutgers\u2013Newark in New Jersey, director of the Rutgers Memory Disorders Project, and publisher of the public health newsletter, \"Memory Loss and the Brain\". He works at the interface between neuroscience, psychology, and computer science, studying the neural bases of learning and memory. His research spans numerous methodologies, including neurocomputational modeling, clinical studies of brain-damaged patients, functional and structural brain imaging, behavioral genetics, and comparative studies of rodent and human learning. He is the co-author of \"Gateway to Memory: An Introduction to Neural Network Models of the Hippocampus\" and an undergraduate textbook \"Learning and Memory: From Brain to Behavior\" (Worth Publishers, 2008).",
            "score": 131.41217041015625
        },
        {
            "docid": "37957946_6",
            "document": "De novo protein synthesis theory of memory formation . A line of research investigates long term potentiation (LTP), a process that describes how a memory can be consolidated between two neurons, or brain cells, ultimately by creating a circuit within the brain that can encode a memory. To initiate a learning circuit between two neurons, one prominent study described using tetanus stimulations to depolarize one neuron by 30mV, which, in turn, activated its NMDA glutamate receptors (Nowak, Bregestovski, Ascher, Herbert, & Prochiantz, 1984). The activation of these receptors resulted in Ca flooding the cell, initiating a cascade of secondary messengers. The cascade of resulting reactions, brought about by secondary messengers, terminates with the activation of cAMP response binding element protein (CREB), which acts as a transcription factor for various genes and initiates their expression (Hawkins, Kandel, & Bailey, 2006). Some proponents argue that the genes stimulate changes in communication between neurons, which underlie the encoding of memory; others suggest that the genes are byproducts of the LTP signaling pathway and are not directly involved in LTP. However, following the cascade of secondary messengers, no one would dispute that more AMPA receptors appear in the postsynaptic terminal (Hayashi et al., 2000). Higher numbers of AMPA receptors, taken together with the aforementioned events, allow for increased firing potential in the postsynaptic cell, which creates an improved learning circuit between these two neurons (Hayashi et al., 2000). Because of the specific, activity-dependent nature of LTP, it is an ideal model for a neural correlate of memory, as postulated by numerous studies; together, these studies show that the abolishment of LTP prevents the formation of memory at the neuronal level (Hawkins, Kandel, & Bailey, 2006).",
            "score": 131.3159637451172
        },
        {
            "docid": "8402086_16",
            "document": "Computational neurogenetic modeling . Artificial Neural Networks designed to simulate of the human brain require an ability to learn a variety of tasks that is not required by those designed to accomplish a specific task. Supervised learning is a mechanism by which an artificial neural network can learn by receiving a number of inputs with a correct output already known. An example of an artificial neural network that uses supervised learning is a multilayer perceptron (MLP). In unsupervised learning, an artificial neural network is trained using only inputs. Unsupervised learning is the learning mechanism by which a type of artificial neural network known as a self-organizing map (SOM) learns. Some types of artificial neural network, such as evolving connectionist systems, can learn in both a supervised and unsupervised manner.",
            "score": 130.5538787841797
        },
        {
            "docid": "31217535_58",
            "document": "Memory . Making memories occurs through a three-step process, which can be enhanced by sleep. The three steps are as follows: Sleep affects memory consolidation. During sleep, the neural connections in the brain are strengthened. This enhances the brain's abilities to stabilize and retain memories. There have been several studies which show that sleep improves the retention of memory, as memories are enhanced through active consolidation. System consolidation takes place during slow-wave sleep (SWS). This process implicates that memories are reactivated during sleep, but that the process doesn't enhance every memory. It also implicates that qualitative changes are made to the memories when they are transferred to long-term store during sleep. During sleep, the hippocampus replays the events of the day for the neocortex. The neocortex then reviews and processes memories, which moves them into long-term memory. When one does not get enough sleep it makes it more difficult to learn as these neural connections are not as strong, resulting in a lower retention rate of memories. Sleep deprivation makes it harder to focus, resulting in inefficient learning. Furthermore, some studies have shown that sleep deprivation can lead to false memories as the memories are not properly transferred to long-term memory. One of the primary functions of sleep is thought to be the improvement of the consolidation of information, as several studies have demonstrated that memory depends on getting sufficient sleep between training and test. Additionally, data obtained from neuroimaging studies have shown activation patterns in the sleeping brain that mirror those recorded during the learning of tasks from the previous day, suggesting that new memories may be solidified through such rehearsal.",
            "score": 130.24293518066406
        }
    ]
}