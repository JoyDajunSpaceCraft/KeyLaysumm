{
    "q": [
        {
            "docid": "1222458_3",
            "document": "Tonotopy . Tonotopy in the auditory system begins at the cochlea, the small snail-like structure in the inner ear that sends information about sound to the brain. Different regions of the basilar membrane in the organ of Corti, the sound-sensitive portion of the cochlea, vibrate at different sinusoidal frequencies due to variations in thickness and width along the length of the membrane. Nerves that transmit information from different regions of the basilar membrane therefore encode frequency tonotopically. This tonotopy then projects through the vestibulocochlear nerve and associated midbrain structures to the primary auditory cortex via the auditory radiation pathway. Throughout this radiation, organization is linear with relation to placement on the organ of Corti, in accordance to the best frequency response (that is, the frequency at which that neuron is most sensitive) of each neuron. However, binaural fusion in the superior olivary complex onward adds significant amounts of information encoded in the signal strength of each ganglion. Thus, the number of tonotopic maps varies between species and the degree of binaural synthesis and separation of sound intensities; in humans, six tonotopic maps have been identified in the primary auditory cortex. their anatomical locations along the auditory cortex.",
            "score": 228.95833003520966
        },
        {
            "docid": "56439577_8",
            "document": "Temporal envelope and fine structure . Responses to the temporal-envelope cues of speech or other complex sounds persist up the auditory pathway, eventually to the various fields of the auditory cortex in many animals. In the Primary Auditory Cortex, responses can encode AM rates by phase-locking up to about 20\u201330\u00a0Hz, while faster rates induce sustained and often tuned responses. A topographical representation of AM rate has been demonstrated in the primary auditory cortex of awake macaques. This representation is approximately perpendicular to the axis of the tonotopic gradient, consistent with an orthogonal organization of spectral and temporal features in the auditory cortex. Combining these temporal responses with the spectral selectivity of A1 neurons gives rise to the spectro-temporal receptive fields that often capture well cortical responses to complex modulated sounds. In secondary auditory cortical fields, responses become temporally more sluggish and spectrally broader, but are still able to phase-lock to the salient features of speech and musical sounds. Tuning to AM rates below about 64\u00a0Hz is also found in the human auditory cortex as revealed by brain-imaging techniques (fMRI) and cortical recordings in epileptic patients (electrocorticography). This is consistent with neuropsychological studies of brain-damaged patients and with the notion that the central auditory system performs some form of spectral decomposition of the ENVp of incoming sounds. Interestingly, the ranges over which cortical responses encode well the temporal-envelope cues of speech have been shown to be predictive of the human ability to understand speech. In the human superior temporal gyrus (STG), an anterior-posterior spatial organization of spectro-temporal modulation tuning has been found in response to speech sounds, the posterior STG being tuned for temporally fast varying speech sounds with low spectral modulations and the anterior STG being tuned for temporally slow varying speech sounds with high spectral modulations.",
            "score": 204.55284869670868
        },
        {
            "docid": "6766895_8",
            "document": "Topographic map (neuroanatomy) . The auditory system is the sensory system for hearing in which the brain interprets information from the frequency of sound waves, yielding the perception of tones. Sound waves enter the ear through the auditory canal. These waves arrive at the eardrum where the properties of the waves are transduced into vibrations. The vibrations travel through the bones of the inner ear to the cochlea. In the cochlea, the vibrations are transduced into electrical information through the firing of hair cells in the organ of Corti. The organ of Corti projects in an orderly fashion to structures in the brainstem (namely, the cochlear nuclei and the inferior colliculus), and from there to the medial geniculate nucleus of the thalamus and the primary auditory cortex. Adjacent sites on the organ of Corti, which are themselves selective for the sound frequency, are represented by adjacent neurons in the aforementioned CNS structures. This projection pattern has been termed tonotopy.",
            "score": 226.396244764328
        },
        {
            "docid": "9736652_7",
            "document": "Auditory masking . If two sounds of two different frequencies are played at the same time, two separate sounds can often be heard rather than a combination tone. The ability to hear frequencies separately is known as \"frequency resolution\" or \"frequency selectivity\". When signals are perceived as a combination tone, they are said to reside in the same \"critical bandwidth\". This effect is thought to occur due to filtering within the cochlea, the hearing organ in the inner ear. A complex sound is split into different frequency components and these components cause a peak in the pattern of vibration at a specific place on the cilia inside the basilar membrane within the cochlea. These components are then coded independently on the auditory nerve which transmits sound information to the brain. This individual coding only occurs if the frequency components are different enough in frequency, otherwise they are in the same critical band and are coded at the same place and are perceived as one sound instead of two.",
            "score": 161.2173434495926
        },
        {
            "docid": "635490_7",
            "document": "Auditory system . Simplified, nerve fibers\u2019 signals are transported by bushy cells to the binaural areas in the olivary complex, while signal peaks and valleys are noted by stellate cells, and signal timing is extracted by octopus cells. The lateral lemniscus has three nuclei: dorsal nuclei respond best to bilateral input and have complexity tuned responses; intermediate nuclei have broad tuning responses; and ventral nuclei have broad and moderately complex tuning curves. Ventral nuclei of lateral lemniscus help the inferior colliculus (IC) decode amplitude modulated sounds by giving both phasic and tonic responses (short and long notes, respectively). IC receives inputs not shown, including visual (pretectal area: moves eyes to sound. superior colliculus: orientation and behavior toward objects, as well as eye movements (saccade)) areas, Pons (superior cerebellar peduncle: thalamus to cerebellum connection/hear sound and learn behavioral response), spinal cord (periaqueductal grey: hear sound and instinctually move), and thalamus. The above are what implicate IC in the \u2018startle response\u2019 and ocular reflexes. Beyond multi-sensory integration IC responds to specific amplitude modulation frequencies, allowing for the detection of pitch. IC also determines time differences in binaural hearing. The medial geniculate nucleus divides into ventral (relay and relay-inhibitory cells: frequency, intensity, and binaural info topographically relayed), dorsal (broad and complex tuned nuclei: connection to somatosensory info), and medial (broad, complex, and narrow tuned nuclei: relay intensity and sound duration). The auditory cortex (AC) brings sound into awareness/perception. AC identifies sounds (sound-name recognition) and also identifies the sound\u2019s origin location. AC is a topographical frequency map with bundles reacting to different harmonies, timing and pitch. Right-hand-side AC is more sensitive to tonality, left-hand-side AC is more sensitive to minute sequential differences in sound. Rostromedial and ventrolateral prefrontal cortices are involved in activation during tonal space and storing short-term memories, respectively. The Heschl\u2019s gyrus/transverse temporal gyrus includes Wernicke\u2019s area and functionality, it is heavily involved in emotion-sound, emotion-facial-expression, and sound-memory processes. The entorhinal cortex is the part of the \u2018hippocampus system\u2019 that aids and stores visual and auditory memories. The supramarginal gyrus (SMG) aids in language comprehension and is responsible for compassionate responses. SMG links sounds to words with the angular gyrus and aids in word choice. SMG integrates tactile, visual, and auditory info.",
            "score": 154.64601373672485
        },
        {
            "docid": "35075711_26",
            "document": "Spontaneous recovery . The pathway of recall associated with the retrieval of sound memories is the auditory system. Within the auditory system is the auditory cortex, which can be broken down into the primary auditory cortex and the belt areas. The primary auditory cortex is the main region of the brain that processes sound and is located on the superior temporal gyrus in the temporal lobe where it receives point-to-point input from the medial geniculate nucleus. From this, the primary auditory complex had a topographic map of the cochlea. The belt areas of the auditory complex receive more diffuse input from peripheral areas of the medial geniculate nucleus and therefore are less precise in tonotopic organization compared to the primary visual cortex. A 2001 study by Trama examined how different kinds of brain damage interfere with normal perception of music. One of his studied patients lost most of his auditory cortex to strokes, allowing him to still hear but making it difficult to understand music since he could not recognize harmonic patterns. Detecting a similarity between speech perception and sound perception, spontaneous recovery of lost auditory information is possible in those patients who have experienced a stroke or other major head trauma. Amusia is a disorder manifesting itself as a defect in processing pitch but also affects one's memory and recognition for music.",
            "score": 209.77670407295227
        },
        {
            "docid": "25049383_4",
            "document": "Cognitive neuroscience of music . Successive parts of the tonotopically organized basilar membrane in the cochlea resonate to corresponding frequency bandwidths of incoming sound. The hair cells in the cochlea release neurotransmitter as a result, causing action potentials down the auditory nerve. The auditory nerve then leads to several layers of synapses at numerous nuclei in the auditory brainstem. These nuclei are also tonotopically organized, and the process of achieving this tonotopy after the cochlea is not well understood. This tonotopy is in general maintained up to primary auditory cortex in mammals, however it is often found that cells in primary and non-primary auditory cortex have spatio-temporal receptive fields, rather than being strictly responsive or phase-locking their action potentials to narrow frequency regions.",
            "score": 205.0642445087433
        },
        {
            "docid": "6766895_24",
            "document": "Topographic map (neuroanatomy) . A variety of techniques have been used to establish the topographic maps in the brain. The existence of topographical maps was shown early by electrical stimulation of the cortex, tracing patterns of epileptic seizures, stimulation sequences, and impairments due to lesions. Details in the maps came later through microelectrode stimulation and recording techniques became commonly used in demonstrating somatotopic maps and later in the auditory and visual systems, both cortically and in subcortical structures such as the colliculi and geniculate nuclei of the thalamus. Single-cell recording, Transcranial magnetic stimulation(TMS), electrical stimulation of the cortex, and Functional magnetic resonance imaging(fMRI) are some of the techniques used to study maps in the brain. Many of the existing topographic maps have been further studied or refined using fMRI. For example, Hubel and Wiesel originally studied the retinotopic maps in the primary visual cortex using single-cell recording. Recently, however, imaging of the retinotopic map in the cortex and in sub-cortical areas, such as the lateral geniculate nucleus, have been improved using the fMRI technique.",
            "score": 123.90756106376648
        },
        {
            "docid": "994097_9",
            "document": "Auditory cortex . Neurons in the auditory cortex are organized according to the frequency of sound to which they respond best. Neurons at one end of the auditory cortex respond best to low frequencies; neurons at the other respond best to high frequencies. There are multiple auditory areas (much like the multiple areas in the visual cortex), which can be distinguished anatomically and on the basis that they contain a complete \"frequency map.\" The purpose of this frequency map (known as a tonotopic map) is unknown, and is likely to reflect the fact that the cochlea is arranged according to sound frequency. The auditory cortex is involved in tasks such as identifying and segregating \"auditory objects\" and identifying the location of a sound in space. For example, it has been shown that A1 encodes complex and abstract aspects of auditory stimuli without encoding their \"raw\" aspects like frequency content, presence of a distinct sound or its echoes.",
            "score": 220.17891716957092
        },
        {
            "docid": "6766895_9",
            "document": "Topographic map (neuroanatomy) . The tonotopic layout of sound information begins in the cochlea where the basilar membrane vibrates at different positions along its length depending upon the frequency of the sound. Higher frequency sounds are at the base of the cochlea, if it were unrolled, and low frequency sounds are at the apex. This arrangement is also found in the auditory cortex in the temporal lobe. In areas that are tonotopically organized, the frequency varies systematically from low to high along the surface of the cortex, but is relatively constant across cortical depth. The general image of topographic organization in animals is multiple tonotopic maps distributed over the surface of the cortex.",
            "score": 203.94063925743103
        },
        {
            "docid": "25260196_25",
            "document": "Neuronal encoding of sound . Primary auditory neurons carry action potentials from the cochlea into the transmission pathway shown in the adjacent image. Multiple relay stations act as integration and processing centers. The signals reach the first level of cortical processing at the primary auditory cortex (A1), in the superior temporal gyrus of the temporal lobe. Most areas up to and including A1 are tonotopically mapped (that is, frequencies are kept in an ordered arrangement). However, A1 participates in coding more complex and abstract aspects of auditory stimuli without coding well the frequency content, including the presence of a distinct sound or its echoes.  Like lower regions, this region of the brain has combination-sensitive neurons that have nonlinear responses to stimuli.",
            "score": 222.78869819641113
        },
        {
            "docid": "579799_8",
            "document": "Georg von B\u00e9k\u00e9sy . He concluded that his observations showed how different sound wave frequencies are locally dispersed before exciting different nerve fibers that lead from the cochlea to the brain. He theorized that the placement of each sensory cell (hair cell) along the coil of the cochlea corresponds to a specific frequency of sound (the so-called tonotopy). B\u00e9k\u00e9sy later developed a mechanical model of the cochlea, which confirmed the concept of frequency dispersion by the basilar membrane in the mammalian cochlea. But this model could not provide any information as to a possible function of this frequency dispersion in the process of hearing.",
            "score": 120.14463067054749
        },
        {
            "docid": "11747471_17",
            "document": "Group C nerve fiber . Central sensitization of the dorsal horn neurons that is evoked from C fiber activity is responsible for temporal summation of \"second pain\" (TSSP). This event is called \u2018windup\u2019 and relies on a frequency greater or equal to 0.33Hz of the stimulus. Windup is associated with chronic pain and central sensitization. This minimum frequency was determined experimentally by comparing healthy patient fMRI\u2019s when subjected to varying frequencies of heat pulses. The fMRI maps show common areas activated by the TSSP responses which include contralateral thalamus (THAL), S1, bilateral S2, anterior and posterior insula (INS), mid-anterior cingulate cortex (ACC), and supplemental motor areas (SMA). TSSP events are also associated with other regions of the brain that process functions such as somatosensory processing, pain perception and modulation, cognition, pre-motor activity in the cortex.",
            "score": 121.88991582393646
        },
        {
            "docid": "36560848_4",
            "document": "Temporal dynamics of music and language . The primary auditory cortex is located on the temporal lobe of the cerebral cortex. This region is important in music processing and plays an important role in determining the pitch and volume of a sound. Brain damage to this region often results in a loss of the ability to hear any sounds at all. The frontal cortex has been found to be involved in processing melodies and harmonies of music. For example, when a patient is asked to tap out a beat or try to reproduce a tone, this region is very active on fMRI and PET scans. The cerebellum is the \"mini\" brain at the rear of the skull. Similar to the frontal cortex, brain imaging studies suggest that the cerebellum is involved in processing melodies and determining tempos. The medial prefrontal cortex along with the primary auditory cortex has also been implicated in tonality, or determining pitch and volume.",
            "score": 185.131409406662
        },
        {
            "docid": "6894544_29",
            "document": "Noise-induced hearing loss . NIHL occurs when too much sound intensity is transmitted into and through the auditory system. An acoustic signal from a sound source, such as a radio, enters into the external auditory canal (ear canal), and is funneled through to the tympanic membrane (eardrum), causing it to vibrate. The vibration of the tympanic membrane drives the middle ear ossicles, the malleus, incus, and stapes into motion. The middle ear ossicles transfer mechanical energy to the cochlea by way of the stapes footplate hammering against the oval window of the cochlea. This hammering causes the fluid within the cochlea (perilymph and endolymph) to be displaced. Displacement of the fluid causes movement of the hair cells (sensory cells in the cochlea) and an electrical signal to be sent from the auditory nerve (CN VIII) to the central auditory system within the brain. This is where sound is perceived. Different groups of hair cells are responsive to different frequencies. Hair cells at or near the base of the cochlea are most sensitive to higher frequency sounds while those at the apex are most sensitive to lower frequency sounds. There are two known biological mechanisms of NIHL from excessive sound intensity: damage to the hair cells and damage to the myelination or synaptic regions of auditory nerves.",
            "score": 200.37629389762878
        },
        {
            "docid": "8953842_3",
            "document": "Computational auditory scene analysis . Since CASA serves to model functionality parts of the auditory system, it is necessary to view parts of the biological auditory system in terms of known physical models. Consisting of three areas, the outer, middle and inner ear, the auditory periphery acts as a complex transducer that converts sound vibrations into action potentials in the auditory nerve. The outer ear consists of the external ear, ear canal and the ear drum. The outer ear, like an acoustic funnel, helps locating the sound source. The ear canal acts as a resonant tube (like an organ pipe) to amplify frequencies between 2\u20135.5\u00a0kHz with a maximum amplification of about 11\u00a0dB occurring around 4\u00a0kHz. As the organ of hearing, the cochlea consists of two membranes, Reissner\u2019s and the basilar membrane. The basilar membrane moves to audio stimuli through the specific stimulus frequency matches the resonant frequency of a particular region of the basilar membrane. The movement the basilar membrane displaces the inner hair cells in one direction, which encodes a half-wave rectified signal of action potentials in the spiral ganglion cells. The axons of these cells make up the auditory nerve, encoding the rectified stimulus. The auditory nerve responses select certain frequencies, similar to the basilar membrane. For lower frequencies, the fibers exhibit \"phase locking\". Neurons in higher auditory pathway centers are tuned to specific stimuli features, such as periodicity, sound intensity, amplitude and frequency modulation.  There are also neuroanatomical associations of ASA through the posterior cortical areas, including the posterior superior temporal lobes and the posterior cingulate. Studies have found that impairments in ASA and segregation and grouping operations are affected in patients with Alzheimer's disease.",
            "score": 186.51163733005524
        },
        {
            "docid": "525667_10",
            "document": "Human echolocation . In a 2014 study by Thaler and colleagues, the researchers first made recordings of the clicks and their very faint echoes using tiny microphones placed in the ears of the blind echolocators as they stood outside and tried to identify different objects such as a car, a flag pole, and a tree. The researchers then played the recorded sounds back to the echolocators while their brain activity was being measured using functional magnetic resonance imaging. Remarkably, when the echolocation recordings were played back to the blind experts, not only did they perceive the objects based on the echoes, but they also showed activity in those areas of their brain that normally process visual information in sighted people, primarily primary visual cortex or V1. This result is surprising, as visual areas, as their names suggest, are only active during visual tasks. The brain areas that process auditory information were no more activated by sound recordings of outdoor scenes containing echoes than they were by sound recordings of outdoor scenes with the echoes removed. Importantly, when the same experiment was carried out with sighted people who did not echolocate, these individuals could not perceive the objects and there was no echo-related activity anywhere in the brain. This suggests that the cortex of blind echolocators is plastic and reorganizes such that primary visual cortex, rather than any auditory area, becomes involved in the computation of echolocation tasks.",
            "score": 132.12103366851807
        },
        {
            "docid": "176315_10",
            "document": "Evoked potential . This technique allows several (e.g., four) SSEPs to be recorded simultaneously from any given location on the scalp. Different sites of stimulation or different stimuli can be tagged with slightly different frequencies that are virtually identical to the brain, but easily separated by Fourier series analyzers. For example, when two unpatterned lights are modulated at slightly different frequencies (F1 and F2) and superimposed, multiple nonlinear cross-modulation components of frequency (mF1 \u00b1 nF2) are created in the SSEP, where m and n are integers. These components allow nonlinear processing in the brain to be investigated. By frequency-tagging two superimposed gratings, spatial frequency and orientation tuning properties of the brain mechanisms that process spatial form can be isolated and studied. Stimuli of different sensory modalities can also be tagged. For example, a visual stimulus was flickered at Fv Hz and a simultaneously presented auditory tone was amplitude modulated at Fa Hz. The existence of a (2Fv + 2Fa) component in the evoked magnetic brain response demonstrated an audio-visual convergence area in the human brain, and the distribution of this response over the head allowed this brain area to be localized. More recently, frequency tagging has been extended from studies of sensory processing to studies of selective attention and of consciousness.",
            "score": 90.88647794723511
        },
        {
            "docid": "25260196_26",
            "document": "Neuronal encoding of sound . Recent studies conducted in bats and other mammals have revealed that the ability to process and interpret modulation in frequencies primarily occurs in the superior and middle temporal gyri of the temporal lobe. Lateralization of brain function exists in the cortex, with the processing of speech in the left cerebral hemisphere and environmental sounds in the right hemisphere of the auditory cortex. Music, with its influence on emotions, is also processed in the right hemisphere of the auditory cortex. While the reason for such localization is not quite understood, lateralization in this instance does not imply exclusivity as both hemispheres do participate in the processing, but one hemisphere tends to play a more significant role than the other.",
            "score": 189.35863828659058
        },
        {
            "docid": "1947410_29",
            "document": "Critical period . In a related study, Barkat, Polley and Hensch (2011) looked at how exposure to different sound frequencies influences the development of the tonotopic map in the primary auditory cortex and the ventral medical geniculate body. In this experiment, mice were reared either in normal environments or in the presence of 7\u00a0kHz tones during early postnatal days. They found that mice that were exposed to an abnormal auditory environment during a critical period P11- P15 had an atypical tonotopic map in the primary auditory cortex. These studies support the notion that exposure to certain sounds within the critical period can influence the development of tonotopic maps and the response properties of neurons. Critical periods are important for the development of the brain for the function from a pattern of connectivity. In general, the early auditory environment influences the structural development and response specificity of the primary auditory cortex.",
            "score": 188.69984316825867
        },
        {
            "docid": "994097_15",
            "document": "Auditory cortex . When each instrument of a symphony orchestra or the jazz band plays the same note, the quality of each sound is different \u2014 but the musician perceives each note as having the same pitch. The neurons of the auditory cortex of the brain are able to respond to pitch. Studies in the marmoset monkey have shown that pitch-selective neurons are located in a cortical region near the anterolateral border of the primary auditory cortex. This location of a pitch-selective area has also been identified in recent functional imaging studies in humans.",
            "score": 164.4952428340912
        },
        {
            "docid": "1764639_17",
            "document": "Levels-of-processing effect . Several brain imaging studies using positron emission tomography and functional magnetic resonance imaging techniques have shown that higher levels of processing correlate with more brain activity and activity in different parts of the brain than lower levels. For example, in a lexical analysis task, subjects showed activity in the left inferior prefrontal cortex only when identifying whether the word represented a living or nonliving object, and not when identifying whether or not the word contained an \"a\". Similarly, an auditory analysis task showed increased activation in the left inferior prefrontal cortex when subjects performed increasingly semantic word manipulations. Synaptic aspects of word recognition have been correlated with the left frontal operculum and the cortex lining the junction of the inferior frontal and inferior precentral sulcus. The self-reference effect also has neural correlates with a region of the medial prefrontal cortex, which was activated in an experiment where subjects analyzed the relevance of data to themselves. Specificity of processing is explained on a neurological basis by studies that show brain activity in the same location when a visual memory is encoded and retrieved, and lexical memory in a different location. Visual memory areas were mostly located within the bilateral extrastriate visual cortex.",
            "score": 114.9779144525528
        },
        {
            "docid": "1222458_2",
            "document": "Tonotopy . In physiology, tonotopy (from Greek tono=frequency and topos = place) is the spatial arrangement of where sounds of different frequency are processed in the brain. Tones close to each other in terms of frequency are represented in topologically neighbouring regions in the brain. Tonotopic maps are a particular case of topographic organization, similar to retinotopy in the visual system.",
            "score": 97.67397737503052
        },
        {
            "docid": "994097_20",
            "document": "Auditory cortex . The auditory cortex has distinct responses to sounds in the gamma band. When subjects are exposed to three or four cycles of a 40 hertz click, an abnormal spike appears in the EEG data, which is not present for other stimuli. The spike in neuronal activity correlating to this frequency is not restrained to the tonotopic organization of the auditory cortex. It has been theorized that gamma frequencies are resonant frequencies of certain areas of the brain, and appear to affect the visual cortex as well. Gamma band activation (25 to 100\u00a0Hz) has been shown to be present during the perception of sensory events and the process of recognition. In a 2000 study by Kneif and colleagues, subjects were presented with eight musical notes to well-known tunes, such as \"Yankee Doodle\" and \"Fr\u00e8re Jacques\". Randomly, the sixth and seventh notes were omitted and an electroencephalogram, as well as a magnetoencephalogram were each employed to measure the neural results. Specifically, the presence of gamma waves, induced by the auditory task at hand, were measured from the temples of the subjects. The OSP response, or omitted stimulus response, was located in a slightly different position; 7\u00a0mm more anterior, 13\u00a0mm more medial and 13\u00a0mm more superior in respect to the complete sets. The OSP recordings were also characteristically lower in gamma waves, as compared to the complete musical set. The evoked responses during the sixth and seventh omitted notes are assumed to be imagined, and were characteristically different, especially in the right hemisphere. The right auditory cortex has long been shown to be more sensitive to tonality, while the left auditory cortex has been shown to be more sensitive to minute sequential differences in sound, such as in speech.",
            "score": 157.71511828899384
        },
        {
            "docid": "226722_25",
            "document": "Functional magnetic resonance imaging . Researchers have checked the BOLD signal against both signals from implanted electrodes (mostly in monkeys) and signals of field potentials (that is the electric or magnetic field from the brain's activity, measured outside the skull) from EEG and MEG. The local field potential, which includes both post-neuron-synaptic activity and internal neuron processing, better predicts the BOLD signal. So the BOLD contrast reflects mainly the inputs to a neuron and the neuron's integrative processing within its body, and less the output firing of neurons. In humans, electrodes can be implanted only in patients who need surgery as treatment, but evidence suggests a similar relationship at least for the auditory cortex and the primary visual cortex. Activation locations detected by BOLD fMRI in cortical areas (brain surface regions) are known to tally with CBF-based functional maps from PET scans. Some regions just a few millimeters in size, such as the lateral geniculate nucleus (LGN) of the thalamus, which relays visual inputs from the retina to the visual cortex, have been shown to generate the BOLD signal correctly when presented with visual input. Nearby regions such as the pulvinar nucleus were not stimulated for this task, indicating millimeter resolution for the spatial extent of the BOLD response, at least in thalamic nuclei. In the rat brain, single-whisker touch has been shown to elicit BOLD signals from the somatosensory cortex.",
            "score": 155.88291263580322
        },
        {
            "docid": "36560848_8",
            "document": "Temporal dynamics of music and language . Positron emission tomography involves injecting a short-lived radioactive tracer isotope into the blood. When the radioisotope decays, it emits positrons which are detected by the machine sensor. The isotope is chemically incorporated into a biologically active molecule, such as glucose, which powers metabolic activity. Whenever brain activity occurs in a given area these molecules are recruited to the area. Once the concentration of the biologically active molecule, and its radioactive \"dye\", rises enough, the scanner can detect it. About one second elapses from when brain activity begins to when the activity is detected by the PET device. This is because it takes a certain amount of time for the dye to reach the needed concentrations can be detected. Functional magnetic resonance imaging or fMRI is a form of the traditional MRI imaging device that allows for brain activity to be observed in real time. An fMRI device works by detecting changes in neural blood flow that is associated with brain activity. fMRI devices use a strong, static magnetic field to align nuclei of atoms within the brain. An additional magnetic field, often called the gradient field, is then applied to elevate the nuclei to a higher energy state. When the gradient field is removed, the nuclei revert to their original state and emit energy. The emitted energy is detected by the fMRI machine and is used to form an image. When neurons become active blood flow to those regions increases. This oxygen-rich blood displaces oxygen depleted blood in these areas. Hemoglobin molecules in the oxygen-carrying red blood cells have different magnetic properties depending on whether it is oxygenated. By focusing the detection on the magnetic disturbances created by hemoglobin, the activity of neurons can be mapped in near real time. Few other techniques allow for researchers to study temporal dynamics in real time.",
            "score": 101.8700133562088
        },
        {
            "docid": "32116125_4",
            "document": "Amblyaudia . Amblyaudia is a deficit in binaural integration of environmental information entering the auditory system. It is a disorder related to brain organization and function rather than what is typically considered a \u201chearing loss\u201d (damage to the cochlea). It may be genetic or developmentally acquired or both. When animals are temporarily deprived of hearing from an early age, profound changes occur in the brain. Specifically, cell sizes in brainstem nuclei are reduced, the configuration of brainstem dendrites are altered and neurons respond in different ways to sounds presented to both the deprived and non-deprived ears (in cases of asymmetric deprivation). This last point is particularly important for listening tasks that require inputs from two ears to perform well. There are multiple auditory functions that rely on the computation of well calibrated inputs from the two ears. Chief among these is the ability to localize sound sources and separate what we want to hear from a background of noise. In the brainstem, the auditory system compares the timing and levels of sounds between the two ears to encode the location of sound sources (sounds that originate from our right as opposed to left side are louder and arrive earlier in our right ear). This ability to separate sound sources not only helps us locate the trajectories of moving objects, but also to separate different sound sources in noisy environments.",
            "score": 154.07294988632202
        },
        {
            "docid": "941909_5",
            "document": "Receptive field . The auditory system processes the temporal and spectral (i.e. frequency) characteristics of sound waves, so the receptive fields of neurons in the auditory system are modeled as spectro-temporal patterns that cause the firing rate of the neuron to modulate with the auditory stimulus. Auditory receptive fields are often modeled as spectro-temporal receptive fields (STRFs), which are the specific pattern in the auditory domain that causes modulation of the firing rate of a neuron. Linear STRFs are created by first calculating a spectrogram of the acoustic stimulus, which determines the how the spectral density of the acoustic stimulus changes over time, often using the Short-time Fourier transform (STFT). Firing rate is modeled over time for the neuron, possibly using a peristimulus time histogram if combining over multiple repetitions of the acoustic stimulus. Then, linear regression is used to predict the firing rate of that neuron as a weighted sum of the spectrogram. The weights learned by the linear model are the STRF, and represent the specific acoustic pattern that causes modulation in the firing rate of the neuron. STRFs can also be understood as the transfer function that maps an acoustic stimulus input to a firing rate response output.",
            "score": 171.0696349143982
        },
        {
            "docid": "25146378_12",
            "document": "Functional specialization (brain) . One of the most well known examples of functional specialization is the fusiform face area (FFA). Justine Sergent was one of the first researchers that brought forth evidence towards the functional neuroanatomy of face processing. Using positron emission tomography (PET), Sergent found that there were different patterns of activation in response to the two different required tasks, face processing verses object processing. These results can be linked with her studies of brain-damaged patients with lesions in the occipital and temporal lobes. Patients revealed that there was an impairment of face processing but no difficulty recognizing everyday objects, a disorder also known as prosopagnosia. Later research by Nancy Kanwisher using functional magnetic resonance imaging (fMRI), found specifically that the region of the inferior temporal cortex, known as the fusiform gyrus, was significantly more active when subjects viewed, recognized and categorized faces in comparison to other regions of the brain. Lesion studies also supported this finding where patients were able to recognize objects but unable to recognize faces. This provided evidence towards domain specificity in the visual system, as Kanwisher acknowledges the Fusiform Face Area as a module in the brain, specifically the extrastriate cortex, that is specialized for face perception.",
            "score": 98.55433428287506
        },
        {
            "docid": "5051081_4",
            "document": "Eric Knudsen . In 1978, Knudsen and Konishi presented the discovery of an auditory map of space in the midbrain of the barn owl. This discovery was groundbreaking because it unearthed the first non-somatotopic space map in the brain. The map was found in the owl\u2019s midbrain, in the lateral and anterior mesencephalicus lateralis dorsalis (MLD), a structure now referred to as the inferior colliculus. Unlike most sound-localization maps, this map was found to be two-dimensional, with units arranged spatially to represent both the vertical and horizontal location of sound. Knudsen and Konishi discovered that units in this structure respond preferentially to sounds originating in a particular region in space. In the 1978 paper, elevation and azimuth (location in the horizontal plane) were shown to be the two coordinates of the map. Using a speaker set on a rotatable hemispherical track, Knudsen and Konishi presented owls with auditory stimulus from various locations in space and recorded the resulting neuronal activity. They found that neurons in this part of the MLD were organized according to the location of their receptive field, with azimuth varying along the horizontal plane of the space map and elevation varying vertically.  Knudsen followed this discovery with research into specific sound localization mechanisms. Two main auditory cues used by the barn owl to localize sound are interaural time difference (ITD) and interaural intensity difference (IID). The owl\u2019s ears are asymmetric, with the right ear\u2019s opening being directed higher than that of the left. This asymmetry allows the barn owl to determine the elevation of a sound by comparing sound levels between its two ears. Interaural time differences provide the owl with information regarding a sound\u2019s azimuth; sound will reach the ear closer to the sound source before reaching the farther ear, and this time difference can be detected and interpreted as an azimuthal direction. At low frequencies, the wavelength of a sound is wider than the owl's facial ruff, and the ruff does not affect detection of azimuth. At high frequencies, the ruff plays a role in reflecting sound for heightened sensitivity to vertical elevation. Therefore, with wide-band noise, containing both high and low frequencies, the owl could use interaural spectrum difference to obtain information about both azimuth and elevation. In 1979, Knudsen and Konishi showed that the barn owl uses interaural spectrum information in sound localization. They presented owls with both wide-bandwidth noise and pure tones. The birds were able to successfully locate pure tones (since they could still gather information from IID and ITD), but their error rate was much lower when localizing wide-bandwidth noise. This indicates that the birds utilize interaural spectrum differences to improve their accuracy.",
            "score": 140.98103189468384
        },
        {
            "docid": "25140_41",
            "document": "Perception . Hearing (or \"audition\") is the ability to perceive sound by detecting vibrations. Frequencies capable of being heard by humans are called audio or \"sonic\". The range is typically considered to be between 20\u00a0Hz and 20,000\u00a0Hz. Frequencies higher than audio are referred to as ultrasonic, while frequencies below audio are referred to as infrasonic. The auditory system includes the outer ears which collect and filter sound waves, the middle ear for transforming the sound pressure (impedance matching), and the inner ear which produces neural signals in response to the sound. By the ascending auditory pathway these are led to the primary auditory cortex within the temporal lobe of the human brain, which is where the auditory information arrives in the cerebral cortex and is further processed there.",
            "score": 212.33844423294067
        },
        {
            "docid": "25146378_20",
            "document": "Functional specialization (brain) . Other researchers who provide evidence to support the theory of distributive processing include Anthony McIntosh and William Uttal, who question and debate localization and modality specialization within the brain. McIntosh's research suggests that human cognition involves interactions between the brain regions responsible for processes sensory information, such as vision, audition, and other mediating areas like the prefrontal cortex. McIntosh explains that modularity is mainly observed in sensory and motor systems, however, beyond these very receptors, modularity becomes \"fuzzier\" and you see the cross connections between systems increase. He also illustrates that there is an overlapping of functional characteristics between the sensory and motor systems, where these regions are close to one another. These different neural interactions influence each other, where activity changes in one area influence other connected areas. With this, McIntosh suggest that if you only focus on activity in one area, you may miss the changes in other integrative areas. Neural interactions can be measured using analysis of covariance in neuroimaging. McIntosh used this analysis to convey a clear example of the interaction theory of distributive processing. In this study, subjects learned that an auditory stimulus signalled a visual event. McIntosh found activation (an increase blood flow), in an area of the occipital cortex, a region of the brain involved in visual processing, when the auditory stimulus was presented alone. Correlations between the occipital cortex and different areas of the brain such as the prefrontal cortex, premotor cortex and superior temporal cortex showed a pattern of co-variation and functional connectivity.",
            "score": 144.55139243602753
        }
    ],
    "r": [
        {
            "docid": "1222458_3",
            "document": "Tonotopy . Tonotopy in the auditory system begins at the cochlea, the small snail-like structure in the inner ear that sends information about sound to the brain. Different regions of the basilar membrane in the organ of Corti, the sound-sensitive portion of the cochlea, vibrate at different sinusoidal frequencies due to variations in thickness and width along the length of the membrane. Nerves that transmit information from different regions of the basilar membrane therefore encode frequency tonotopically. This tonotopy then projects through the vestibulocochlear nerve and associated midbrain structures to the primary auditory cortex via the auditory radiation pathway. Throughout this radiation, organization is linear with relation to placement on the organ of Corti, in accordance to the best frequency response (that is, the frequency at which that neuron is most sensitive) of each neuron. However, binaural fusion in the superior olivary complex onward adds significant amounts of information encoded in the signal strength of each ganglion. Thus, the number of tonotopic maps varies between species and the degree of binaural synthesis and separation of sound intensities; in humans, six tonotopic maps have been identified in the primary auditory cortex. their anatomical locations along the auditory cortex.",
            "score": 228.9583282470703
        },
        {
            "docid": "6766895_8",
            "document": "Topographic map (neuroanatomy) . The auditory system is the sensory system for hearing in which the brain interprets information from the frequency of sound waves, yielding the perception of tones. Sound waves enter the ear through the auditory canal. These waves arrive at the eardrum where the properties of the waves are transduced into vibrations. The vibrations travel through the bones of the inner ear to the cochlea. In the cochlea, the vibrations are transduced into electrical information through the firing of hair cells in the organ of Corti. The organ of Corti projects in an orderly fashion to structures in the brainstem (namely, the cochlear nuclei and the inferior colliculus), and from there to the medial geniculate nucleus of the thalamus and the primary auditory cortex. Adjacent sites on the organ of Corti, which are themselves selective for the sound frequency, are represented by adjacent neurons in the aforementioned CNS structures. This projection pattern has been termed tonotopy.",
            "score": 226.396240234375
        },
        {
            "docid": "25260196_25",
            "document": "Neuronal encoding of sound . Primary auditory neurons carry action potentials from the cochlea into the transmission pathway shown in the adjacent image. Multiple relay stations act as integration and processing centers. The signals reach the first level of cortical processing at the primary auditory cortex (A1), in the superior temporal gyrus of the temporal lobe. Most areas up to and including A1 are tonotopically mapped (that is, frequencies are kept in an ordered arrangement). However, A1 participates in coding more complex and abstract aspects of auditory stimuli without coding well the frequency content, including the presence of a distinct sound or its echoes.  Like lower regions, this region of the brain has combination-sensitive neurons that have nonlinear responses to stimuli.",
            "score": 222.7886962890625
        },
        {
            "docid": "5442380_15",
            "document": "Sensory cue . The auditory system of humans and animals allows individuals to assimilate information from the surroundings, represented as sound waves. Sound waves first pass through the pinnae and the auditory canal, the parts of the ear that comprise the outer ear. Sound then reaches the tympanic membrane in the middle ear (also known as the eardrum). The tympanic membrane sets the malleus, incus, and stapes into vibration. The stapes transmits these vibrations to the inner ear by pushing on the membrane covering the oval window, which separates the middle and inner ear. The inner ear contains the cochlea, the liquid-filled structure containing the hair cells. These cells serve to transform the incoming vibration to electrical signals, which can then be transmitted to the brain. The auditory nerve carries the signal generated by the hair cells away from the inner ear and towards the auditory receiving area in the cortex. The signal then travels through fibers to several subcortical structures and then to the primary auditory receiving area in the temporal lobe",
            "score": 222.4393310546875
        },
        {
            "docid": "994097_9",
            "document": "Auditory cortex . Neurons in the auditory cortex are organized according to the frequency of sound to which they respond best. Neurons at one end of the auditory cortex respond best to low frequencies; neurons at the other respond best to high frequencies. There are multiple auditory areas (much like the multiple areas in the visual cortex), which can be distinguished anatomically and on the basis that they contain a complete \"frequency map.\" The purpose of this frequency map (known as a tonotopic map) is unknown, and is likely to reflect the fact that the cochlea is arranged according to sound frequency. The auditory cortex is involved in tasks such as identifying and segregating \"auditory objects\" and identifying the location of a sound in space. For example, it has been shown that A1 encodes complex and abstract aspects of auditory stimuli without encoding their \"raw\" aspects like frequency content, presence of a distinct sound or its echoes.",
            "score": 220.1789093017578
        },
        {
            "docid": "49604_30",
            "document": "Hearing loss . Sound travels to the brain: as follows: sound waves reach the outer ear and are conducted down the ear canal to the eardrum. The sound waves cause the eardrum to vibrate. These vibrations are passed through the 3 tiny ear bones in the middle ear, which transfer the vibrations to the fluid in the inner ear. The fluid moves hair cells, and the movement of the hair cells converts the vibrations into nerve impulses, which are then taken to the brain by the auditory nerve. The auditory nerve takes the impulses to the brainstem, which sends the impulses to the midbrain. Finally, the signal goes to the auditory cortex of the temporal lobe to be interpreted as sound.",
            "score": 220.13697814941406
        },
        {
            "docid": "25140_41",
            "document": "Perception . Hearing (or \"audition\") is the ability to perceive sound by detecting vibrations. Frequencies capable of being heard by humans are called audio or \"sonic\". The range is typically considered to be between 20\u00a0Hz and 20,000\u00a0Hz. Frequencies higher than audio are referred to as ultrasonic, while frequencies below audio are referred to as infrasonic. The auditory system includes the outer ears which collect and filter sound waves, the middle ear for transforming the sound pressure (impedance matching), and the inner ear which produces neural signals in response to the sound. By the ascending auditory pathway these are led to the primary auditory cortex within the temporal lobe of the human brain, which is where the auditory information arrives in the cerebral cortex and is further processed there.",
            "score": 212.3384552001953
        },
        {
            "docid": "35075711_26",
            "document": "Spontaneous recovery . The pathway of recall associated with the retrieval of sound memories is the auditory system. Within the auditory system is the auditory cortex, which can be broken down into the primary auditory cortex and the belt areas. The primary auditory cortex is the main region of the brain that processes sound and is located on the superior temporal gyrus in the temporal lobe where it receives point-to-point input from the medial geniculate nucleus. From this, the primary auditory complex had a topographic map of the cochlea. The belt areas of the auditory complex receive more diffuse input from peripheral areas of the medial geniculate nucleus and therefore are less precise in tonotopic organization compared to the primary visual cortex. A 2001 study by Trama examined how different kinds of brain damage interfere with normal perception of music. One of his studied patients lost most of his auditory cortex to strokes, allowing him to still hear but making it difficult to understand music since he could not recognize harmonic patterns. Detecting a similarity between speech perception and sound perception, spontaneous recovery of lost auditory information is possible in those patients who have experienced a stroke or other major head trauma. Amusia is a disorder manifesting itself as a defect in processing pitch but also affects one's memory and recognition for music.",
            "score": 209.77670288085938
        },
        {
            "docid": "569399_14",
            "document": "Stimulus (physiology) . Changes in pressure caused by sound reaching the external ear resonate in the tympanic membrane, which articulates with the auditory ossicles, or the bones of the middle ear. These tiny bones multiply these pressure fluctuations as they pass the disturbance into the cochlea, a spiral-shaped bony structure within the inner ear. Hair cells in the cochlear duct, specifically the organ of Corti, are deflected as waves of fluid and membrane motion travel through the chambers of the cochlea. Bipolar sensory neurons located in the center of the cochlea monitor the information from these receptor cells and pass it on to the brainstem via the cochlear branch of cranial nerve VIII. Sound information is processed in the temporal lobe of the CNS, specifically in the primary auditory cortex.",
            "score": 208.6037139892578
        },
        {
            "docid": "25049383_4",
            "document": "Cognitive neuroscience of music . Successive parts of the tonotopically organized basilar membrane in the cochlea resonate to corresponding frequency bandwidths of incoming sound. The hair cells in the cochlea release neurotransmitter as a result, causing action potentials down the auditory nerve. The auditory nerve then leads to several layers of synapses at numerous nuclei in the auditory brainstem. These nuclei are also tonotopically organized, and the process of achieving this tonotopy after the cochlea is not well understood. This tonotopy is in general maintained up to primary auditory cortex in mammals, however it is often found that cells in primary and non-primary auditory cortex have spatio-temporal receptive fields, rather than being strictly responsive or phase-locking their action potentials to narrow frequency regions.",
            "score": 205.06423950195312
        },
        {
            "docid": "56439577_8",
            "document": "Temporal envelope and fine structure . Responses to the temporal-envelope cues of speech or other complex sounds persist up the auditory pathway, eventually to the various fields of the auditory cortex in many animals. In the Primary Auditory Cortex, responses can encode AM rates by phase-locking up to about 20\u201330\u00a0Hz, while faster rates induce sustained and often tuned responses. A topographical representation of AM rate has been demonstrated in the primary auditory cortex of awake macaques. This representation is approximately perpendicular to the axis of the tonotopic gradient, consistent with an orthogonal organization of spectral and temporal features in the auditory cortex. Combining these temporal responses with the spectral selectivity of A1 neurons gives rise to the spectro-temporal receptive fields that often capture well cortical responses to complex modulated sounds. In secondary auditory cortical fields, responses become temporally more sluggish and spectrally broader, but are still able to phase-lock to the salient features of speech and musical sounds. Tuning to AM rates below about 64\u00a0Hz is also found in the human auditory cortex as revealed by brain-imaging techniques (fMRI) and cortical recordings in epileptic patients (electrocorticography). This is consistent with neuropsychological studies of brain-damaged patients and with the notion that the central auditory system performs some form of spectral decomposition of the ENVp of incoming sounds. Interestingly, the ranges over which cortical responses encode well the temporal-envelope cues of speech have been shown to be predictive of the human ability to understand speech. In the human superior temporal gyrus (STG), an anterior-posterior spatial organization of spectro-temporal modulation tuning has been found in response to speech sounds, the posterior STG being tuned for temporally fast varying speech sounds with low spectral modulations and the anterior STG being tuned for temporally slow varying speech sounds with high spectral modulations.",
            "score": 204.5528564453125
        },
        {
            "docid": "4455796_10",
            "document": "Lateral inhibition . Similarities between sensory processes of the skin and the auditory system suggest lateral inhibition could play a role in auditory processing. The basilar membrane in the cochlea has receptive fields similar to the receptive fields of the skin and eyes. Also, neighboring cells in the auditory cortex have similar specific frequencies that cause them to fire, creating a map of sound frequencies similar to that of the somatosensory cortex. Lateral inhibition in tonotopic channels can be found in the inferior colliculus and at higher levels of auditory processing in the brain. However, the role that lateral inhibition plays in auditory sensation is unclear. Some scientists found that lateral inhibition could play a role in sharpening spatial input patterns and temporal changes in sensation, others propose it plays an important role in processing low or high tones.",
            "score": 204.4164581298828
        },
        {
            "docid": "1007062_5",
            "document": "Superior temporal gyrus . The superior temporal gyrus contains the primary auditory cortex, which is responsible for processing sounds. Specific sound frequencies map precisely onto the primary auditory cortex. This auditory (or tonotopic) map is similar to the homunculus map of the primary motor cortex. Some areas of the superior temporal gyrus are specialized for processing combinations of frequencies, and other areas are specialized for processing changes in amplitude or frequency. The superior temporal gyrus also includes the Wernicke's area, which (in most people) is located in the left hemisphere. It is the major area involved in the comprehension of language. The superior temporal gyrus (STG) is involved in auditory processing, including language, but also has been implicated as a critical structure in social cognition.",
            "score": 204.2767791748047
        },
        {
            "docid": "6766895_9",
            "document": "Topographic map (neuroanatomy) . The tonotopic layout of sound information begins in the cochlea where the basilar membrane vibrates at different positions along its length depending upon the frequency of the sound. Higher frequency sounds are at the base of the cochlea, if it were unrolled, and low frequency sounds are at the apex. This arrangement is also found in the auditory cortex in the temporal lobe. In areas that are tonotopically organized, the frequency varies systematically from low to high along the surface of the cortex, but is relatively constant across cortical depth. The general image of topographic organization in animals is multiple tonotopic maps distributed over the surface of the cortex.",
            "score": 203.94064331054688
        },
        {
            "docid": "31251362_5",
            "document": "Beat deafness . When sound waves reach the ears, the energy they contain is converted into electrical signals, which are sent via the auditory nerves to the brain. Sound processing begins when these electrical signals reach the primary auditory receiving area in the core part of the temporal lobe. Signals then travel to the area surrounding the core, known as the belt area, and are then transmitted to the parabelt area, which is located next to the belt. Simple sounds such as pure tones are able to activate the core area of the brain, but both the belt and parabelt areas are activated by only complex sounds, such as those found in speech and music. The auditory cortex in the left hemisphere of the brain is responsible for processing beat and rhythm in music. The right auditory cortex is primarily used in distinguishing between different harmonics, which are simple pure tones that combine to create complex tones.",
            "score": 200.71292114257812
        },
        {
            "docid": "6894544_29",
            "document": "Noise-induced hearing loss . NIHL occurs when too much sound intensity is transmitted into and through the auditory system. An acoustic signal from a sound source, such as a radio, enters into the external auditory canal (ear canal), and is funneled through to the tympanic membrane (eardrum), causing it to vibrate. The vibration of the tympanic membrane drives the middle ear ossicles, the malleus, incus, and stapes into motion. The middle ear ossicles transfer mechanical energy to the cochlea by way of the stapes footplate hammering against the oval window of the cochlea. This hammering causes the fluid within the cochlea (perilymph and endolymph) to be displaced. Displacement of the fluid causes movement of the hair cells (sensory cells in the cochlea) and an electrical signal to be sent from the auditory nerve (CN VIII) to the central auditory system within the brain. This is where sound is perceived. Different groups of hair cells are responsive to different frequencies. Hair cells at or near the base of the cochlea are most sensitive to higher frequency sounds while those at the apex are most sensitive to lower frequency sounds. There are two known biological mechanisms of NIHL from excessive sound intensity: damage to the hair cells and damage to the myelination or synaptic regions of auditory nerves.",
            "score": 200.3762969970703
        },
        {
            "docid": "25260196_26",
            "document": "Neuronal encoding of sound . Recent studies conducted in bats and other mammals have revealed that the ability to process and interpret modulation in frequencies primarily occurs in the superior and middle temporal gyri of the temporal lobe. Lateralization of brain function exists in the cortex, with the processing of speech in the left cerebral hemisphere and environmental sounds in the right hemisphere of the auditory cortex. Music, with its influence on emotions, is also processed in the right hemisphere of the auditory cortex. While the reason for such localization is not quite understood, lateralization in this instance does not imply exclusivity as both hemispheres do participate in the processing, but one hemisphere tends to play a more significant role than the other.",
            "score": 189.35862731933594
        },
        {
            "docid": "1947410_29",
            "document": "Critical period . In a related study, Barkat, Polley and Hensch (2011) looked at how exposure to different sound frequencies influences the development of the tonotopic map in the primary auditory cortex and the ventral medical geniculate body. In this experiment, mice were reared either in normal environments or in the presence of 7\u00a0kHz tones during early postnatal days. They found that mice that were exposed to an abnormal auditory environment during a critical period P11- P15 had an atypical tonotopic map in the primary auditory cortex. These studies support the notion that exposure to certain sounds within the critical period can influence the development of tonotopic maps and the response properties of neurons. Critical periods are important for the development of the brain for the function from a pattern of connectivity. In general, the early auditory environment influences the structural development and response specificity of the primary auditory cortex.",
            "score": 188.69984436035156
        },
        {
            "docid": "8953842_3",
            "document": "Computational auditory scene analysis . Since CASA serves to model functionality parts of the auditory system, it is necessary to view parts of the biological auditory system in terms of known physical models. Consisting of three areas, the outer, middle and inner ear, the auditory periphery acts as a complex transducer that converts sound vibrations into action potentials in the auditory nerve. The outer ear consists of the external ear, ear canal and the ear drum. The outer ear, like an acoustic funnel, helps locating the sound source. The ear canal acts as a resonant tube (like an organ pipe) to amplify frequencies between 2\u20135.5\u00a0kHz with a maximum amplification of about 11\u00a0dB occurring around 4\u00a0kHz. As the organ of hearing, the cochlea consists of two membranes, Reissner\u2019s and the basilar membrane. The basilar membrane moves to audio stimuli through the specific stimulus frequency matches the resonant frequency of a particular region of the basilar membrane. The movement the basilar membrane displaces the inner hair cells in one direction, which encodes a half-wave rectified signal of action potentials in the spiral ganglion cells. The axons of these cells make up the auditory nerve, encoding the rectified stimulus. The auditory nerve responses select certain frequencies, similar to the basilar membrane. For lower frequencies, the fibers exhibit \"phase locking\". Neurons in higher auditory pathway centers are tuned to specific stimuli features, such as periodicity, sound intensity, amplitude and frequency modulation.  There are also neuroanatomical associations of ASA through the posterior cortical areas, including the posterior superior temporal lobes and the posterior cingulate. Studies have found that impairments in ASA and segregation and grouping operations are affected in patients with Alzheimer's disease.",
            "score": 186.5116424560547
        },
        {
            "docid": "36560848_4",
            "document": "Temporal dynamics of music and language . The primary auditory cortex is located on the temporal lobe of the cerebral cortex. This region is important in music processing and plays an important role in determining the pitch and volume of a sound. Brain damage to this region often results in a loss of the ability to hear any sounds at all. The frontal cortex has been found to be involved in processing melodies and harmonies of music. For example, when a patient is asked to tap out a beat or try to reproduce a tone, this region is very active on fMRI and PET scans. The cerebellum is the \"mini\" brain at the rear of the skull. Similar to the frontal cortex, brain imaging studies suggest that the cerebellum is involved in processing melodies and determining tempos. The medial prefrontal cortex along with the primary auditory cortex has also been implicated in tonality, or determining pitch and volume.",
            "score": 185.13140869140625
        },
        {
            "docid": "1374343_12",
            "document": "Audiometry . Sound waves enter the outer ear and travel through the external auditory canal until they reach the tympanic membrane, causing the membrane and the attached chain of auditory ossicles to vibrate. The motion of the stapes against the oval window sets up waves in the fluids of the cochlea, causing the basilar membrane to vibrate. This stimulates the sensory cells of the organ of Corti, atop the basilar membrane, to send nerve impulses to the central auditory processing areas of the brain, the auditory cortex, where sound is perceived and interpreted.",
            "score": 184.13954162597656
        },
        {
            "docid": "8953842_15",
            "document": "Computational auditory scene analysis . While many models consider the audio signal as a complex combination of different frequencies, modeling the auditory system can also require consideration for the neural components. By taking a holistic process, where a stream (of feature-based sounds) correspond to neuronal activity distributed in many brain areas, the perception of the sound could be mapped and modeled. Two different solutions have been proposed to the binding of the audio perception and the area in the brain. Hierarchical coding models many cells to encode all possible combinations of features and objects in the auditory scene. Temporal or oscillatory correlation addressing the binding problem by focusing on the synchrony and desynchrony between neural oscillations to encode the state of binding among the auditory features. These two solutions are very similar to the debacle between place coding and temporal coding. While drawing from modeling neural components, another phenomenon of ASA comes into play with CASA systems: the extent of modeling neural mechanisms. The studies of CASA systems have involved modeling some known mechanisms, such as the bandpass nature of cochlear filtering and random auditory nerve firing patterns, however, these models may not lead to finding new mechanisms, but rather give an understanding of purpose to the known mechanisms.",
            "score": 183.56800842285156
        },
        {
            "docid": "569444_5",
            "document": "Transduction (physiology) . In the auditory system, sound vibrations (mechanical energy) are transduced into electrical energy by hair cells in the inner ear. Sound vibrations from an object cause vibrations in air molecules, which in turn, vibrate the ear drum. The movement of the eardrum causes the bones of the middle ear (the ossicles) to vibrate. These vibrations then pass into the cochlea, the organ of hearing. Within the cochlea, the hair cells on the sensory epithelium of the organ of Corti bend and cause movement of the basilar membrane. The membrane undulates in different sized waves according to the frequency of the sound. Hair cells are then able to convert this movement (mechanical energy) into electrical signals (graded receptor potentials) which travel along auditory nerves to hearing centres in the brain.",
            "score": 183.45994567871094
        },
        {
            "docid": "890232_4",
            "document": "Hair cell . The inner hair cells transform the sound vibrations in the fluids of the cochlea into electrical signals that are then relayed via the auditory nerve to the auditory brainstem and to the auditory cortex.",
            "score": 182.8430633544922
        },
        {
            "docid": "14532984_7",
            "document": "Coincidence detection in neurobiology . Coincidence detection has been shown to be a major factor in sound localization along the azimuth plane in several organisms. In 1948, Lloyd A. Jeffress proposed that some organisms may have a collection of neurons that receive auditory input from each ear. The neural pathways to these neurons are called delay lines. Jeffress claimed that the neurons that the delay lines link act as coincidence detectors by firing maximally when receiving simultaneous inputs from both ears. When a sound is heard, sound waves may reach the ears at different times. This is referred to as the interaural time difference (ITD). Due to differing lengths and a finite conduction speed within the axons of the delay lines, different coincidence detector neurons will fire when sound comes from different positions along the azimuth. Jeffress' model proposes that two signals even from an asynchronous arrival of sound in the cochlea of each ear will converge synchronously on a coincidence detector in the auditory cortex based on the magnitude of the ITD (Fig. 2). Therefore, the ITD should correspond to an anatomical map that can be found within the brain. Masakazu Konishi's study on barn owls shows that this is true. Sensory information from the hair cells of the ears travels to the ipsilateral nucleus magnocellularis. From here, the signals project ipsilaterally and contralaterally to two nucleus laminari. Each nucleus laminaris contains coincidence detectors that receive auditory input from the left and the right ear. Since the ipsilateral axons enter the nucleus laminaris dorsally while the contralateral axons enter ventrally, sounds from various positions along the azimuth correspond directly to stimulation of different depths of the nucleus laminaris. From this information, a neural map of auditory space was formed. The function of the nucleus laminaris parallels that of the medial superior olive in mammals.",
            "score": 179.1378173828125
        },
        {
            "docid": "4301708_10",
            "document": "Cochlear nucleus . The cochlear nuclear complex is the first integrative, or processing, stage in the auditory system. Information is brought to the nuclei from the ipsilateral cochlea via the cochlear nerve. Several tasks are performed in the cochlear nuclei. By distributing acoustic input to multiple types of principal cells, the auditory pathway is subdivided into parallel ascending pathways, which can simultaneously extract different types of information. The cells of the ventral cochlear nucleus extract information that is carried by the auditory nerve in the timing of firing and in the pattern of activation of the population of auditory nerve fibers. The cells of the dorsal cochlear nucleus perform a non-linear spectral analysis and place that spectral analysis into the context of the location of the head, ears and shoulders and that separate expected, self-generated spectral cues from more interesting, unexpected spectral cues using input from the auditory cortex, pontine nuclei, trigeminal ganglion and nucleus, dorsal column nuclei and the second dorsal root ganglion. It is likely that these neurons help mammals to use spectral cues for orienting toward those sounds. The information is used by higher brainstem regions to achieve further computational objectives (such as sound source location or improvement in signal to noise ratio). The inputs from these other areas of the brain probably play a role in sound localization.",
            "score": 174.59439086914062
        },
        {
            "docid": "47338295_6",
            "document": "Sound localization in owls . The axons of the auditory nerve originate from the hair cells of the cochlea in the inner ear. Different sound frequencies are encoded by different fibers of the auditory nerve, arranged along the length of the auditory nerve, but codes for the timing and level of the sound are not segregated within the auditory nerve. Instead, the ITD is encoded by phase locking, i.e. firing at or near a particular phase angle of the sinusoidal stimulus sound wave, and the IID is encoded by spike rate. Both parameters are carried by each fiber of the auditory nerve.",
            "score": 174.36236572265625
        },
        {
            "docid": "941909_5",
            "document": "Receptive field . The auditory system processes the temporal and spectral (i.e. frequency) characteristics of sound waves, so the receptive fields of neurons in the auditory system are modeled as spectro-temporal patterns that cause the firing rate of the neuron to modulate with the auditory stimulus. Auditory receptive fields are often modeled as spectro-temporal receptive fields (STRFs), which are the specific pattern in the auditory domain that causes modulation of the firing rate of a neuron. Linear STRFs are created by first calculating a spectrogram of the acoustic stimulus, which determines the how the spectral density of the acoustic stimulus changes over time, often using the Short-time Fourier transform (STFT). Firing rate is modeled over time for the neuron, possibly using a peristimulus time histogram if combining over multiple repetitions of the acoustic stimulus. Then, linear regression is used to predict the firing rate of that neuron as a weighted sum of the spectrogram. The weights learned by the linear model are the STRF, and represent the specific acoustic pattern that causes modulation in the firing rate of the neuron. STRFs can also be understood as the transfer function that maps an acoustic stimulus input to a firing rate response output.",
            "score": 171.06964111328125
        },
        {
            "docid": "1602257_29",
            "document": "Presbycusis . In cases of severe or profound hearing loss, a surgical cochlear implant is possible. This is an electronic device that replaces the cochlea of the inner ear. Electrodes are typically inserted through the round window of the cochlea, into the fluid-filled scala tympani. They stimulate the peripheral axons of the primary auditory neurons, which then send information to the brain via the auditory nerve. The cochlea is tonotopically mapped in a spiral fashion, with lower frequencies localizing at the apex of the cochlea, and high frequencies at the base of the cochlea, near the oval and round windows. With age, comes a loss in distinction of frequencies, especially higher ones. The electrodes of the implant are designed to stimulate the array of nerve fibers that previously responded to different frequencies accurately. It is important to note that due to spatial constraints, the cochlear implant may not be inserted all the way into the cochlear apex. It provides a different kind of sound spectrum than natural hearing, but may enable the recipient to recognize speech and environmental sounds.",
            "score": 167.4019012451172
        },
        {
            "docid": "25049383_6",
            "document": "Cognitive neuroscience of music . Studies suggest that individuals are capable of automatically detecting a difference or anomaly in a melody such as an out of tune pitch which does not fit with their previous music experience. This automatic processing occurs in the secondary auditory cortex. Brattico, Tervaniemi, Naatanen, and Peretz (2006) performed one such study to determine if the detection of tones that do not fit an individual's expectations can occur automatically. They recorded event-related potentials (ERPs) in nonmusicians as they were presented unfamiliar melodies with either an out of tune pitch or an out of key pitch while participants were either distracted from the sounds or attending to the melody. Both conditions revealed an early frontal negativity independent of where attention was directed. This negativity originated in the auditory cortex, more precisely in the supratemporal lobe (which corresponds with the secondary auditory cortex) with greater activity from the right hemisphere. The negativity response was larger for pitch that was out of tune than that which was out of key. Ratings of musical incongruity were higher for out of tune pitch melodies than for out of key pitch. In the focused attention condition, out of key and out of tune pitches produced late parietal positivity. The findings of Brattico et al. (2006) suggest that there is automatic and rapid processing of melodic properties in the secondary auditory cortex. The findings that pitch incongruities were detected automatically, even in processing unfamiliar melodies, suggests that there is an automatic comparison of incoming information with long term knowledge of musical scale properties, such as culturally influenced rules of musical properties (common chord progressions, scale patterns, etc.) and individual expectations of how the melody should proceed. The auditory area processes the sound of the music. The auditory area is located in the temporal lobe. The temporal lobe deals with the recognition and perception of auditory stimuli, memory, and speech (Kinser, 2012).",
            "score": 166.60928344726562
        },
        {
            "docid": "19415143_2",
            "document": "Auditory agnosia . Auditory agnosia is a form of agnosia that manifests itself primarily in the inability to recognize or differentiate between sounds. It is not a defect of the ear or \"hearing\", but a neurological inability of the brain to process sound meaning. It is a disruption of the \"what\" pathway in the brain. Persons with auditory agnosia can physically hear the sounds and describe them using unrelated terms, but are unable to recognize them. They might describe the sound of some environmental sounds, such as a motor starting, as resembling a lion roaring, but would not be able to associate the sound with \"car\" or \"engine\", nor would they say that it \"was\" a lion creating the noise. Auditory agnosia is caused by damage to the secondary and tertiary auditory cortex of the temporal lobe of the brain.",
            "score": 164.63034057617188
        },
        {
            "docid": "569650_19",
            "document": "Stimulus modality . The human ear is able to detect differences in pitch through the movement of auditory hair cells found on the basilar membrane. High frequency sounds will stimulate the auditory hair cells at the base of the basilar membrane while medium frequency sounds cause vibrations of auditory hair cells located at the middle of the basilar membrane. For frequencies that are lower than 200\u00a0Hz, the tip of the basilar membrane vibrates in sync with the sound waves. In turn, neurons are fired at the same rate as the vibrations. The brain is able to measure the vibrations and is then aware of any low frequency pitches.",
            "score": 164.551025390625
        }
    ]
}