{
    "q": [
        {
            "docid": "1059791_2",
            "document": "Computational photography . Computational photography or computational imaging refers to digital image capture and processing techniques that use digital computation instead of optical processes. Computational photography can improve the capabilities of a camera, or introduce features that were not possible at all with film based photography, or reduce the cost or size of camera elements. Examples of computational photography include in-camera computation of digital panoramas, high-dynamic-range images, and light field cameras. Light field cameras use novel optical elements to capture three dimensional scene information which can then be used to produce 3D images, enhanced depth-of-field, and selective de-focusing (or \"post focus\"). Enhanced depth-of-field reduces the need for mechanical focusing systems. All of these features use computational imaging techniques.",
            "score": 102.04163074493408
        },
        {
            "docid": "2723838_8",
            "document": "Image-based modeling and rendering . \u2022 Light-field rendering It is a method for generating new views from arbitrary camera positions without depth information or feature matching, simply by combining and resampling the available images. [9]. \u2022 Plenoptic stitching It gives the viewer the ability to explore unobstructed environments of arbitrary sizes and shapes, using appropriate sampling for most viewpoints in the environment by moving omnidirectional video camera over the grid [10]. \u2022 Cylindrical panoramas It provides horizontal orientation independence when exploring an environment from a single point. Cylindrical panoramas can be created using specialized panoramic cameras [11, 12, 13]. \u2022 Concentric mosaics It is a generalization of cylindrical panoramas that allows the viewer to explore a circular region and experience horizontal parallax and lighting effects. In this case, instead of using a single cylindrical image, slit cameras are rotated along planar concentric circles. A series of concentric manifold mosaics are created by composing the slit images acquired by each camera along their circular paths. Unlike light field and lumigraph where cameras are placed on a two-dimensional grid, the concentric mosaics representation reduces the amount of data by capturing a sequence of images along a circle path [14, 15]. \u2022 Lumigraph It is similar to light field rendering, but it applies approximated geometry to compensate for non-uniform sampling, in order to improve rendering performance [16]. \u2022 Transfer methods They are characterized by the use of a relatively small number of images with the application of geometric constraints (either recovered at some stage or known a priori) to reproject image pixels appropriately at a given virtual camera viewpoint [Laveau and Faugeras [17, 18]. \u2022 Relief texture To improve the rendering speed of 3D warping, the warping process is factored into a relatively simple pre-warping step and a traditional texture mapping step [19]. \u2022 Image-based objects They provide a compact image-based representation for 3D objects that can be rendered in occlusion-compatible order. An image-based object is constructed by acquiring multiple views of the object, registering and resampling them from every center of projection onto the faces of a parallelepiped. The use of a parallelepiped allows such a representation to be decomposed into parameterized planar regions for which a warper can be efficiently implemented [20]. \u2022 Image-based visual hulls It is based on efficient computation and shading visual hulls from silhouette image data. The algorithm takes advantage of epipolar geometry and incremental computation to achieve a constant rendering cost per rendered pixel [21]. \u2022 3D Warping With available depth information for every point in one or more images, 3D warping techniques can be used to render from any nearby point of view by projecting the pixels of the original image to their proper 3D locations and re-projecting them onto the new picture [22] \u2022 Layered depth images To deal with the disocclusion artifacts in 3D warping, Layered Depth Image is proposed to store not only what is visible in the input image, but also what is behind the visible surface. Each pixel in the input image contains a list of depth and color values where the ray from the pixel intersects with the scene [23].  \u2022 View-dependent texture maps View-dependent texture mapping is used to render novel views, by warping and compositing several input images. A three-step view-dependent texture mapping method is considered to further reduce the computational cost and provide smoother blending. This method employs visibility preprocessing, polygon-view maps, and projective texture mapping [24, 25]. \u2022 Surface light field It is a function that assigns a color to each ray originating on a surface. Surface light fields are well suited to constructing virtual images of shiny objects under complex lighting conditions [26].  \u2022 Light field mapping This method is a representation and interactive visualization of surface light fields, by partitioning the radiance data over elementary surface primitives and by approximating each partitioned data by a small set of lower-dimensional discrete functions. The rendering algorithm decodes directly from this compact representation at interactive frame rates [27]. For an exhaustive overview of the currently available methods and algorithms in this topic, see the following surveys [1, 28]  [1] Oliveira, Manuel M. \"Image-based modeling and rendering techniques: A survey.\" RITA 9.2 (2002): 37-66.",
            "score": 101.64127373695374
        },
        {
            "docid": "20903754_30",
            "document": "Robotics . Computer vision systems rely on image sensors which detect electromagnetic radiation which is typically in the form of either visible light or infra-red light. The sensors are designed using solid-state physics. The process by which light propagates and reflects off surfaces is explained using optics. Sophisticated image sensors even require quantum mechanics to provide a complete understanding of the image formation process. Robots can also be equipped with multiple vision sensors to be better able to compute the sense of depth in the environment. Like human eyes, robots' \"eyes\" must also be able to focus on a particular area of interest, and also adjust to variations in light intensities.",
            "score": 109.8512511253357
        },
        {
            "docid": "157898_16",
            "document": "Eye . The ocelli of insects bear a simple lens, but their focal point always lies behind the retina; consequently they can never form a sharp image. Ocelli (pit-type eyes of arthropods) blur the image across the whole retina, and are consequently excellent at responding to rapid changes in light intensity across the whole visual field; this fast response is further accelerated by the large nerve bundles which rush the information to the brain. Focusing the image would also cause the sun's image to be focused on a few receptors, with the possibility of damage under the intense light; shielding the receptors would block out some light and thus reduce their sensitivity. This fast response has led to suggestions that the ocelli of insects are used mainly in flight, because they can be used to detect sudden changes in which way is up (because light, especially UV light which is absorbed by vegetation, usually comes from above).",
            "score": 106.06613993644714
        },
        {
            "docid": "49769929_8",
            "document": "Lighthouse paradox . The fundamental misunderstanding of this paradox is the assumption that the projected image caused by the light ray is a physical object, and therefore must follow physical law. In reality no physical laws are being broken as there is no physical object travelling faster than light. This paradox uses kinematic processes to explain the motion of this apparent object. However, the projected image on the moon, or the image created by the lighthouse, is not a real object. The apparent lateral motion across the surface of the moon is a result of the light source moving through some angular rotation, as depicted in Figure 2, not superluminal motion across its surface. The angular motion of the source creates a translation of the image projected on the moon, proportional to distance the between the screen (which in this case is moon) and the source. Thus, if one was to go close enough to the moon and rotate the laser through the same angle the image would travel at subliminal speeds even though nothing that should effect its speed has changed. If the image was a physical object, it should be able to travel across the surface of the moon at the same speed regardless of the distance of the observer. Understanding this, the paradox begins to unravel.",
            "score": 70.76664280891418
        },
        {
            "docid": "166689_20",
            "document": "Interferometry . The heart of the Fabry\u2013P\u00e9rot interferometer is a pair of partially silvered glass optical flats spaced several millimeters to centimeters apart with the silvered surfaces facing each other. (Alternatively, a Fabry\u2013P\u00e9rot \"etalon\" uses a transparent plate with two parallel reflecting surfaces.) As with the Fizeau interferometer, the flats are slightly beveled. In a typical system, illumination is provided by a diffuse source set at the focal plane of a collimating lens. A focusing lens produces what would be an inverted image of the source if the paired flats were not present; \"i.e.\" in the absence of the paired flats, all light emitted from point A passing through the optical system would be focused at point A'. In Fig.\u00a06, only one ray emitted from point A on the source is traced. As the ray passes through the paired flats, it is multiply reflected to produce multiple transmitted rays which are collected by the focusing lens and brought to point A' on the screen. The complete interference pattern takes the appearance of a set of concentric rings. The sharpness of the rings depends on the reflectivity of the flats. If the reflectivity is high, resulting in a high Q factor (\"i.e.\" high finesse), monochromatic light produces a set of narrow bright rings against a dark background. In Fig.\u00a06, the low-finesse image corresponds to a reflectivity of 0.04 (\"i.e.\" unsilvered surfaces) \"versus\" a reflectivity of 0.95 for the high-finesse image.",
            "score": 70.58719253540039
        },
        {
            "docid": "1217870_3",
            "document": "Spectrohelioscope . The basic spectrohelioscope is a complex machine that uses a spectroscope to scan the surface of the sun. The image from the objective lens is focused on a narrow slit revealing only a thin portion of the suns surface. The light is then passed through a prism or diffraction grating to spread the light into a spectrum. The spectrum is then focused on another slit that allows only a narrow part of the spectrum (the desired wavelength of light for viewing) to pass. The light is finally focused on an eyepiece so the surface of the Sun can be seen. The view, however, would be only a narrow strip of the Sun's surface. The slits are moved in unison to scan across the whole surface of the sun giving a full image. Independently nodding mirrors can be used instead of moving slits to produce the same scan: the first mirror selects a slice of the sun, the second selects the desired wavelength.",
            "score": 60.5389347076416
        },
        {
            "docid": "31329046_6",
            "document": "Pre-attentive processing . Information for pre-attentive processing is detected through the five senses. In the visual system, the receptive fields at the back of the eye (retina) transfer the image via axons to the thalamus, specifically the lateral geniculate nuclei. The image then travels to the primary visual cortex and continues on to be processed by the visual association cortex. At each stage, the image is processed with increasing complexity. Pre-attentive processing starts with the retinal image; this image is magnified as it moves from retina to the cortex of the brain. Shades of light and dark are processed in the lateral geniculate nuclei of the thalamus. Simple and complex cells in the brain process boundary and surface information by deciphering the image's contrast, orientation, and edges. When the image hits the fovea, it is highly magnified, facilitating object recognition. The images in the periphery are less clear but help to create a complete image used for scene perception.",
            "score": 135.40505695343018
        },
        {
            "docid": "18320_23",
            "document": "Lens (optics) . In some cases \"S\" is negative, indicating that the image is formed on the opposite side of the lens from where those rays are being considered. Since the diverging light rays emanating from the lens never come into focus, and those rays are not physically present at the point where they \"appear\" to form an image, this is called a virtual image. Unlike real images, a virtual image cannot be projected on a screen, but appears to an observer looking through the lens as if it were a real object at the location of that virtual image. Likewise, it appears to a subsequent lens as if it were an object at that location, so that second lens could again focus that light into a real image, \"S\" then being measured from the virtual image location behind the first lens to the second lens. This is exactly what the eye does when looking through a magnifying glass. The magnifying glass creates a (magnified) virtual image behind the magnifying glass, but those rays are then re-imaged by the lens of the eye to create a \"real image\" on the retina. Using a positive lens of focal length \"f\", a virtual image results when , the lens thus being used as a magnifying glass (rather than if as for a camera). Using a negative lens () with a \"real object\" () can only produce a virtual image (), according to the above formula. It is also possible for the object distance \"S\" to be negative, in which case the lens sees a so-called \"virtual object\". This happens when the lens is inserted into a converging beam (being focused by a previous lens) \"before\" the location of its real image. In that case even a negative lens can project a real image, as is done by a Barlow lens.",
            "score": 84.65305542945862
        },
        {
            "docid": "1316947_4",
            "document": "Ambiguous image . When we see an image, the first thing we do is attempt to organize all the parts of the scene into different groups. To do this, one of the most basic methods used is finding the edges. Edges can include obvious perceptions such as the edge of a house, and can include other perceptions that the brain needs to process deeper, such as the edges of a person's facial features. When finding edges, the brain's visual system detects a point on the image with a sharp contrast of lighting. Being able to detect the location of the edge of an object aids in recognizing the object. In ambiguous images, detecting edges still seems natural to the person perceiving the image. However, the brain undergoes deeper processing to resolve the ambiguity. For example, consider an image that involves an opposite change in magnitude of luminance between the object and the background (e.g. From the top, the background shifts from black to white, and the object shifts from white to black). The opposing gradients will eventually come to a point where there is an equal degree of luminance of the object and the background. At this point, there is no edge to be perceived. To counter this, the visual system connects the image as a whole rather than a set of edges, allowing one to see an object rather than edges and non-edges. Although there is no complete image to be seen, the brain is able to accomplish this because of its understanding of the physical world and real incidents of ambiguous lighting. In ambiguous images, an illusion is often produced from illusory contours. An illusory contour is a perceived contour without the presence of a physical gradient. In examples where a white shape appears to occlude black objects on a white background, the white shape appears to be brighter than the background, and the edges of this shape produce the illusory contours. These illusory contours are processed by the brain in a similar way as real contours. The visual system accomplishes this by making inferences beyond the information that is presented in much the same way as the luminance gradient.",
            "score": 182.62331414222717
        },
        {
            "docid": "6596_13",
            "document": "Computer vision . Solid-state physics is another field that is closely related to computer vision. Most computer vision systems rely on image sensors, which detect electromagnetic radiation, which is typically in the form of either visible or infra-red light. The sensors are designed using quantum physics. The process by which light interacts with surfaces is explained using physics. Physics explains the behavior of optics which are a core part of most imaging systems. Sophisticated image sensors even require quantum mechanics to provide a complete understanding of the image formation process. Also, various measurement problems in physics can be addressed using computer vision, for example motion in fluids.",
            "score": 89.86426162719727
        },
        {
            "docid": "276115_13",
            "document": "Jumping spider . The anterior median eyes (AME) have very good vision. This pair of eyes is built like a telescopic tube with a corneal lens in the front and a second lens in the back that focus images onto a four-layered retina, a narrow boomerang-shaped strip oriented vertically. Physiological experiments have shown they may have up to four different kinds of receptor cells, with different absorption spectra, giving them the possibility of tetrachromatic color vision, with sensitivity extending into the ultraviolet range. As the eyes are too close together to allow depth perception, and the animals do not make use of motion parallax, they have evolved a method called image defocus instead. Of the four photoreceptor layers in the retina, the first two closest to the surface contain ultraviolet-sensitive pigments while the two deepest contain green-sensitive pigments. The incoming green light is only focused on the deepest layer, while the other one receives defocused or fuzzy images. By measuring the amount of defocus from the fuzzy layer, it is possible to calculate the distance to the objects in front of them. In addition to receptor cells, also red filters have been detected, located in front of the cells that normally register green light. It seems that all salticids, regardless of whether they have two, three, or four kinds of color receptors, are highly sensitive to UV light. Some species (for example, \"Cosmophasis umbratica\") are highly dimorphic in the UV spectrum, suggesting a role in sexual signaling (Lim & Li, 2005). Color discrimination has been demonstrated in behavioral experiments.",
            "score": 95.53041660785675
        },
        {
            "docid": "10416897_6",
            "document": "Image fusion . Multi-focus image fusion is used to collect useful and necessary information from input images with different focus depths in order to create an output image that ideally has all information from input images . In visual sensor network (VSN), sensors are cameras which record images and video sequences. In many applications of VSN, a camera can\u2019t give a perfect illustration including all details of the scene. This is because of the limited depth of focus exists in the optical lens of cameras . Therefore, just the object located in the focal length of camera is focused and cleared and the other parts of image are blurred. VSN has an ability to capture images with different depth of focuses in the scene using several cameras. Due to the large amount of data generated by camera compared to other sensors such as pressure and temperature sensors and some limitation such as limited band width, energy consumption and processing time, it is essential to process the local input images to decrease the amount of transmission data. The aforementioned reasons emphasize the necessary of multi-focus images fusion. Multi-focus image fusion is a process which combines the input multi-focus images into a single image including all important information of the input images and it\u2019s more accurate explanation of the scene than every single input image.",
            "score": 97.37732243537903
        },
        {
            "docid": "2057223_2",
            "document": "Path tracing . Path tracing is a computer graphics Monte Carlo method of rendering images of three-dimensional scenes such that the global illumination is faithful to reality. Fundamentally, the algorithm is integrating over all the illuminance arriving to a single point on the surface of an object. This illuminance is then reduced by a surface reflectance function (BRDF) to determine how much of it will go towards the viewpoint camera. This integration procedure is repeated for every pixel in the output image. When combined with physically accurate models of surfaces, accurate models of real light sources (light bulbs), and optically-correct cameras, path tracing can produce still images that are indistinguishable from photographs.",
            "score": 75.68609547615051
        },
        {
            "docid": "475202_11",
            "document": "Rg chromaticity . Computer vision algorithms tend to suffer from varying imaging conditions. To make more robust computer vision algorithms it is important to use a color invariant color space. Color invariant color spaces are desensitized to disturbances in the image. One common problem in computer vision is varying light source (color and intensity) between multiple images and within a single image. To properly perform image segmentation and object detection requires an increased need for images that are stable to variations in imaging conditions. Normalizing RGB color space to rgb color system performs a linear transform. Normalized rgb space eliminates the effect of varying intensities from the light source. Uniform surfaces of color with varying geometric features are affected by the angle and intensity of the light source. Where a uniform red surface with a uniform green object placed on top, should easily be segmented. Due to the shape of the 3D object shades are formed preventing uniform fields of color. Normalizing intensity out removes the shadow. A lambertian reflector under a white illumination is defined by the equation below: formula_10",
            "score": 93.3495090007782
        },
        {
            "docid": "5645086_24",
            "document": "Evolution of the eye . In a lensless eye, the light emanating from a distant point hits the back of the eye with about the same size as the eye's aperture. With the addition of a lens this incoming light is concentrated on a smaller surface area, without reducing the overall intensity of the stimulus. The focal length of an early lobopod with lens-containing simple eyes focused the image \"behind\" the retina, so while no part of the image could be brought into focus, the intensity of light allowed the organism to see in deeper (and therefore darker) waters. A subsequent increase of the lens's refractive index probably resulted in an in-focus image being formed.",
            "score": 69.91493654251099
        },
        {
            "docid": "234653_7",
            "document": "Schlieren photography . The basic idea is that the illumination pattern is imaged onto a geometrically congruent cutoff pattern (essentially a multiplicity of knife edges) with focusing optics, while density gradients lying between the illumination pattern and the cutoff pattern are imaged, typically by a camera system. As in classical schlieren, the distortions produce regions of brightening or darkening corresponding to the position and direction of the distortion, because they redirect rays either away from or onto the opaque part of the cutoff pattern. While in classical schlieren, distortions over the whole beam path are visualized equally, in focusing schlieren, only distortions in the object field of the camera are clearly imaged. Distortions away from the object field become blurred, so this technique allows some degree of depth selection. It also has the advantage that a wide variety of illuminated backgrounds can be used, since collimation is not required. This allows construction of projection-based focusing schlieren systems, which are much easier to build and align than classical schlieren systems. The requirement of collimated light in classical schlieren is often a substantial practical barrier for constructing large systems due to the need for the collimating optic to be the same size as the field of view. Focusing schlieren systems can use compact optics with a large background illumination pattern, which is particularly easy to produce with a projection system. For systems with large demagnification, the illumination pattern needs to be around twice larger than the field of view to allow defocusing of the background pattern.",
            "score": 96.7744402885437
        },
        {
            "docid": "14917968_26",
            "document": "Sediment Profile Imagery . Experience building and testing the first prototype identified a number of key issues. The scanner technology chosen provided great depth of field (useful for identifying surface features), but required a large volume for the mirror assembly (which had to be strengthened to withstand vibrations). Furthermore, the armour, support flanges, and water pipes limited further sediment penetration and caused sediment disturbance. It was desirable to move the entire water gallery into the centre of the scanner module so that penetrator heads could be rapidly changed in the field. It was likely that different shapes would be more effective in different sediment textures and fabrics.  These decisions led to an alternate scanner technology that had been developed and marketed mostly in the early 2000s. It is known by various names such as contact imaging, direct imaging, or LED indirect exposure (US Patent 5499112). In this technology, a string of LEDs strobe the primary colours onto an imaging plane. Illumination is crucial so the imaging plane must be close. Reflected light from the imaging plane is directed into an array of light guides which lead to CCD elements. The physical arrangement between the light guides and the imaging plane is what limits the depth of field using this technology. Tests using consumer scanners indicated that the imaging plane could be 1\u20133\u00a0mm away from the scan head for full resolution images, but dropped off quickly beyond that. Scene features 5\u00a0mm or more away from the scan head were almost unidentifiable. Since the primary value of SP imagery is two-dimensional, this limitation was a small trade off for the great savings in space. The solid-state technology is robust to vibration and no mirrors are necessary. Unfortunately, UV illumination was difficult to provide without a custom-designed scan head and was therefore not included in the second prototype. One major advantage of SPI is that it reliably provides sediment information regardless of water clarity. However, many SPI applications such as habitat mapping and side-scan sonar ground-truthing, would benefit from imagery of the seabed\u2019s surface when visibility permits. Since the tether provided a source of power and computer connectivity with the surface vessel, adding a digital camera to image the seabed surface immediately adjacent to the sediment profile was another conceptually simple addition. A laser array surrounding the camera provided a means to correct the geometry of the seabed surface image (since it is captured at a variable angle) and its scale. Such imagery provides a larger reference frame in which to interpret the adjacent sediment profile and permits a more informed estimation of the habitat connectivity of multiple profiles. A longitudinal section of the second prototype with the seabed surface camera is presented in Figure 11. The typical deployment configuration is shown in Figure 12.",
            "score": 79.75910067558289
        },
        {
            "docid": "6319245_10",
            "document": "Camera resectioning . When a camera is used, light from the environment is focused on an image plane and captured. This process reduces the dimensions of the data taken in by the camera from three to two (light from a 3D scene is stored on a 2D image). Each pixel on the image plane therefore corresponds to a shaft of light from the original scene. Camera resectioning determines which incoming light is associated with each pixel on the resulting image. In an ideal pinhole camera, a simple projection matrix is enough to do this. With more complex camera systems, errors resulting from misaligned lenses and deformations in their structures can result in more complex distortions in the final image. The camera projection matrix is derived from the intrinsic and extrinsic parameters of the camera, and is often represented by the series of transformations; e.g., a matrix of camera intrinsic parameters, a 3\u00a0\u00d7\u00a03 rotation matrix, and a translation vector. The camera projection matrix can be used to associate points in a camera's image space with locations in 3D world space.",
            "score": 72.15417838096619
        },
        {
            "docid": "45350085_6",
            "document": "Visual computing . At least the following disciplines are sub-fields of visual computing. More detailed descriptions of each of these fields can be found on the linked special pages. Computer graphics is a general term for all techniques that produce images as result with the help of a computer. To transform the description of objects to nice images is called rendering which is always a compromise between image quality and run-time. Techniques that can extract content information from images are called image analysis techniques. Computer vision is the ability of computers (or of robots) to recognize their environment and to interpret it correctly. Visualization is used to produce images that shall communicate messages. Data may be abstract or concrete, often with no a priori geometrical components. Visual analytics describes the discipline of interactive visual analysis of data, also described as \u201cthe science of analytical reasoning supported by the interactive visual interface\u201d. To represent objects for rendering it needs special methods and data structures, which subsumed with the term geometric modeling. In addition to describing and interactive geometric techniques, sensor data are more and more used to reconstruct geometrical models. Algorithms for the efficient control of 3D printers also belong to the field of visual computing. In contrast to image analysis image processing manipulates images to produce better images. \u201cBetter\u201d can have very different meanings subject to the respective application. Also, it has to be discriminated from image editing which describes interactive manipulation of images based on human validation. Techniques that produce the feeling of immersion into a fictive world are called virtual reality (VR). Requirements for VR include head-mounted displays, real-time tracking, and high-quality real-time rendering. Augmented reality enables the user to see the real environment in addition to the virtual objects, which augment this reality. Accuracy requirements on rendering speed and tracking precision are significantly higher here. The planning, design and uses of interfaces between people and computers is not only part of every system involving images. Due to the high bandwidth of the human visual channel (eye), images are also a preferred part of ergonomic user interfaces in any system, so that human-computer interaction is also an integral part of visual computing.",
            "score": 132.37498950958252
        },
        {
            "docid": "3520765_5",
            "document": "Depth-of-field adapter . Static (non-moving) adapters suffer greater image degradation from low-light situations because texture on the focusing screen becomes more noticeable. The camcorder used in conjunction with the adapter must focus on the focusing screen inside the adapter which is used as a projection surface. As a result, the camcorder also picks up the pits, dimples and/or specks in the material that give it its translucent properties. The solution to this problem is to shake, rotate or otherwise move the focusing screen so that the texture of the screen is blurred. In a non-static solution such as this, the texture is only a problem at very high shutter speeds, where blurring is reduced.",
            "score": 48.168391942977905
        },
        {
            "docid": "489992_6",
            "document": "Diving mask . Light rays bend when they travel from one medium to another; the amount of bending is determined by the refractive indices of the two media. If one medium has a particular curved shape, it functions as a lens. The cornea, humours, and crystalline lens of the eye together form a lens that focuses images on the retina. Our eyes are adapted for viewing in air. Water, however, has approximately the same refractive index as the cornea (both about 1.33), so immersion effectively eliminates the cornea's focusing properties. When our eyes are in water, instead of focusing images on the retina, they now focus them far behind the retina, resulting in an extremely blurred image from hypermetropia.",
            "score": 73.13119292259216
        },
        {
            "docid": "5212945_2",
            "document": "Visual neuroscience . Visual Neuroscience is a branch of neuroscience that focuses on the visual system of the human body, mainly located in the brain's visual cortex. The main goal of visual neuroscience is to understand how neural activity results in visual perception, as well as behaviors dependent on vision. In the past, visual neuroscience has focused primarily on how the brain (and in particular the Visual Cortex) responds to light rays projected from static images and onto the retina. While this provides a reasonable explanation for the visual perception of a static image, it does not provide an accurate explanation for how we perceive the world as it really is, an ever-changing, and ever-moving 3-D environment. The topics summarized below are representative of this area, but far from exhaustive.",
            "score": 136.07539677619934
        },
        {
            "docid": "50073184_6",
            "document": "Generative adversarial network . GANs have been used to produce samples of photorealistic images for the purposes of visualizing new interior/industrial design, shoes, bags and clothing items or items for computer games' scenes. These networks were reported to be used by Facebook. Recently, GANs have modeled patterns of motion in video. They have also been used to reconstruct 3D models of objects from images and to improve astronomical images. In 2017 a fully convolutional feedforward GAN was used for image enhancement using automated texture synthesis in combination with perceptual loss. The system focused on realistic textures rather than pixel-accuracy. The result was a higher image quality at high magnification.",
            "score": 79.01406216621399
        },
        {
            "docid": "53653658_9",
            "document": "Imaging of cultural heritage . Raking illumination highlights texture on the surface of an object. This is achieved by using a single light source at a low angle relative to the object. Images taken under these conditions can reveal deviations in the surface of an object \u2013 gouges, scratches, paint loss, bulges, and more. With archaeological objects this may reveal how tools were made or how food was processed (e.g. cut marks on bones). In paintings it can show how the artist used the paint.",
            "score": 73.23474955558777
        },
        {
            "docid": "57066835_2",
            "document": "Multi-focus image fusion . Multi-focus image fusion is a multiple image compression technique using input images with different focus depths to make an output image that preserves information. . In visual sensor network (VSN), sensors are cameras which record images and video sequences. In many applications of VSN, a camera can\u2019t give a perfect illustration including all details of the scene. This is because of the limited depth of focus exists in the optical lens of cameras . Therefore, just the object located in the focal length of camera is focused and cleared and the other parts of image are blurred. VSN has an ability to capture images with different depth of focuses in the scene using several cameras. Due to the large amount of data generated by camera compared to other sensors such as pressure and temperature sensors and some limitation such as limited band width, energy consumption and processing time, it is essential to process the local input images to decrease the amount of transmission data. The aforementioned reasons emphasize the necessary of multi-focus images fusion. Multi-focus image fusion is a process which combines the input multi-focus images into a single image including all important information of the input images and it\u2019s more accurate explanation of the scene than every single input image .",
            "score": 97.38126397132874
        },
        {
            "docid": "6752609_6",
            "document": "Underwater vision . Light rays bend when they travel from one medium to another; the amount of bending is determined by the refractive indices of the two media. If one medium has a particular curved shape, it functions as a lens. The cornea, humours, and crystalline lens of the eye together form a lens that focuses images on the retina. The human eye is adapted for viewing in air. Water, however, has approximately the same refractive index as the cornea (both about 1.33), effectively eliminating the cornea's focusing properties. When immersed in water, instead of focusing images on the retina, they are focused behind the retina, resulting in an extremely blurred image from hypermetropia.",
            "score": 76.92431259155273
        },
        {
            "docid": "31614402_4",
            "document": "Polynomial texture mapping . A series of images is captured in a darkened room with the camera in a fixed position and the object lit from different angles. These images are then processed and combined to enable a virtual light source to be controlled by the user inspecting the object. The virtual light source may be manipulated to simulate light from different angles and of different intensity or wavelengths to illuminate the surface of artefacts and reveal details. Open source tools for processing the captured images and publishing the resulting relightable images on the web are freely available.",
            "score": 75.74623799324036
        },
        {
            "docid": "23604_3",
            "document": "Photography . Typically, a lens is used to focus the light reflected or emitted from objects into a real image on the light-sensitive surface inside a camera during a timed exposure. With an electronic image sensor, this produces an electrical charge at each pixel, which is electronically processed and stored in a digital image file for subsequent display or processing. The result with photographic emulsion is an invisible latent image, which is later chemically \"developed\" into a visible image, either negative or positive depending on the purpose of the photographic material and the method of processing. A negative image on film is traditionally used to photographically create a positive image on a paper base, known as a print, either by using an enlarger or by contact printing.",
            "score": 68.84654927253723
        },
        {
            "docid": "27309832_18",
            "document": "P200 . The P2 has also been found to be sensitive to other forms of visual cognitive processing. Researchers recorded visual evoked potentials in response to non-stereoscopic two-dimensional and three-dimensional images in order to study neurophysiological correlates of depth perception. These non-stereoscopic images depict depth using line drawings that can be perceived as three-dimensional by one eye as opposed to by binocular depth perception that is the result of different angles of view integrated between the two eyes. In this study, P2 amplitude was significantly larger in the condition with three-dimensional convex and concave images, than in condition with two-dimensional images. These changes were found for electrodes placed over bilateral parieto-occipital regions. This study showed that the P2 generated around the visual cortex region is sensitive to the difference between two and three-dimensional images, without using actual depth or information integrated across both eyes.",
            "score": 100.13588857650757
        },
        {
            "docid": "1842685_2",
            "document": "Gaussian blur . In image processing, a Gaussian blur (also known as Gaussian smoothing) is the result of blurring an image by a Gaussian function (named after mathematician and scientist Carl Friedrich Gauss). It is a widely used effect in graphics software, typically to reduce image noise and reduce detail. The visual effect of this blurring technique is a smooth blur resembling that of viewing the image through a translucent screen, distinctly different from the bokeh effect produced by an out-of-focus lens or the shadow of an object under usual illumination. Gaussian smoothing is also used as a pre-processing stage in computer vision algorithms in order to enhance image structures at different scales\u2014see scale space representation and scale space implementation.",
            "score": 97.3531084060669
        },
        {
            "docid": "26184833_5",
            "document": "Interference reflection microscopy . To form an image of the attached cell, light of a specific wavelength is passed through a polarizer. This linear polarized light is reflected by a beam splitter towards the objective, which focuses the light on the specimen. The glass surface is reflective to a certain degree and will reflect the polarized light. Light that is not reflected by the glass will travel into the cell and be reflected by the cell membrane. Three situations can occur. First, when the membrane is close to the glass, the reflected light from the glass is shifted half of a wavelength, so that light reflected from the membrane will have a phase shift compared to the reflected light from the glass phases and therefore cancel each other out (interference). This interference results in a dark pixel in the final image (the left case in the figure). Second, when the membrane is not attached to the glass, the reflection from the membrane has a smaller phase shift compared to the reflected light from the glass, and therefore they will not cancel each other out, resulting in a bright pixel in the image (the right case in the figure). Third, when there is no specimen, only the reflected light from the glass is detected and will appear as bright pixels in the final image.",
            "score": 73.12749552726746
        }
    ],
    "r": [
        {
            "docid": "1316947_4",
            "document": "Ambiguous image . When we see an image, the first thing we do is attempt to organize all the parts of the scene into different groups. To do this, one of the most basic methods used is finding the edges. Edges can include obvious perceptions such as the edge of a house, and can include other perceptions that the brain needs to process deeper, such as the edges of a person's facial features. When finding edges, the brain's visual system detects a point on the image with a sharp contrast of lighting. Being able to detect the location of the edge of an object aids in recognizing the object. In ambiguous images, detecting edges still seems natural to the person perceiving the image. However, the brain undergoes deeper processing to resolve the ambiguity. For example, consider an image that involves an opposite change in magnitude of luminance between the object and the background (e.g. From the top, the background shifts from black to white, and the object shifts from white to black). The opposing gradients will eventually come to a point where there is an equal degree of luminance of the object and the background. At this point, there is no edge to be perceived. To counter this, the visual system connects the image as a whole rather than a set of edges, allowing one to see an object rather than edges and non-edges. Although there is no complete image to be seen, the brain is able to accomplish this because of its understanding of the physical world and real incidents of ambiguous lighting. In ambiguous images, an illusion is often produced from illusory contours. An illusory contour is a perceived contour without the presence of a physical gradient. In examples where a white shape appears to occlude black objects on a white background, the white shape appears to be brighter than the background, and the edges of this shape produce the illusory contours. These illusory contours are processed by the brain in a similar way as real contours. The visual system accomplishes this by making inferences beyond the information that is presented in much the same way as the luminance gradient.",
            "score": 182.62332153320312
        },
        {
            "docid": "32197396_2",
            "document": "Form perception . Form perception is the recognition of visual elements of objects, specifically those to do with shapes, patterns and previously identified important characteristics. An object is perceived by the retina as a two-dimensional image, but the image can vary for the same object in terms of the context with which it is viewed, the apparent size of the object, the angle from which it is viewed, how illuminated it is, as well as where it resides in the field of vision.  Despite the fact that each instance of observing an object leads to a unique retinal response pattern, the visual processing in the brain is capable of recognizing these experiences as analogous, allowing invariant object recognition. Visual processing occurs in a hierarchy with the lowest levels recognizing lines and contours, and slightly higher levels performing tasks such as completing boundaries and recognizing contour combinations. The highest levels integrate the perceived information to recognize an entire object. Essentially object recognition is the ability to assign labels to objects in order to categorize and identify them, thus distinguishing one object from another. During visual processing information is not created, but rather reformatted in a way that draws out the most detailed information of the stimulus.",
            "score": 154.2308349609375
        },
        {
            "docid": "51462681_3",
            "document": "Objective vision . This is the story of what's happening when you see a picture, even too fast, the brain's visual cortex recognizes what it sees immediately. The visual cortex has a critical job in processing and it's the most complex part of brain. The human brain is much more aware of how it solves complex problems such as playing chess or solving algebra equations, which is why computer programmers have had so much success building machines that emulate this type of activity. but when entities visionary system starts to convert the signals to image(actually the separated shapes and colors) to find a relation between brain's information and those images. The system actually is concentrating on the separable sections, this separation gives the brain a visionary system the excellence processing result, because with this method the system do not waste much time on processing non significant sections and signals. this operation in the Objective Vision project called objective processing and because the O.V. mission is around human visionary simulation, so the developer refers with Objective Vision.",
            "score": 144.7322540283203
        },
        {
            "docid": "24990312_3",
            "document": "Optics and vision . The visual system in humans allows individuals to assimilate information from the environment. The act of seeing starts when the lens of the eye focuses an image of its surroundings onto a light-sensitive membrane in the back of the eye, called the retina. The retina converts patterns of light into neuronal signals. The lens of the eye focuses light on the photoreceptive cells of the retina, which detect the photons of light and respond by producing neural impulses. These signals are processed in a hierarchical fashion by different parts of the brain, from the retina to the lateral geniculate nucleus, to the primary and secondary visual cortex of the brain. Signals from the retina can also travel directly from the retina to the Superior colliculus.",
            "score": 141.372802734375
        },
        {
            "docid": "23416874_9",
            "document": "Sense . Sight or vision (adjectival form: visual/optical) is the capability of the eye(s) to focus and detect images of visible light on photoreceptors in the retina of each eye that generates electrical nerve impulses for varying colors, hues, and brightness. There are two types of photoreceptors: rods and cones. Rods are very sensitive to light, but do not distinguish colors. Cones distinguish colors, but are less sensitive to dim light. There is some disagreement as to whether this constitutes one, two or three senses. Neuroanatomists generally regard it as two senses, given that different receptors are responsible for the perception of color and brightness. Some argue that stereopsis, the perception of depth using both eyes, also constitutes a sense, but it is generally regarded as a cognitive (that is, post-sensory) function of the visual cortex of the brain where patterns and objects in images are recognized and interpreted based on previously learned information. This is called visual memory.",
            "score": 136.72860717773438
        },
        {
            "docid": "5212945_2",
            "document": "Visual neuroscience . Visual Neuroscience is a branch of neuroscience that focuses on the visual system of the human body, mainly located in the brain's visual cortex. The main goal of visual neuroscience is to understand how neural activity results in visual perception, as well as behaviors dependent on vision. In the past, visual neuroscience has focused primarily on how the brain (and in particular the Visual Cortex) responds to light rays projected from static images and onto the retina. While this provides a reasonable explanation for the visual perception of a static image, it does not provide an accurate explanation for how we perceive the world as it really is, an ever-changing, and ever-moving 3-D environment. The topics summarized below are representative of this area, but far from exhaustive.",
            "score": 136.0753936767578
        },
        {
            "docid": "31329046_6",
            "document": "Pre-attentive processing . Information for pre-attentive processing is detected through the five senses. In the visual system, the receptive fields at the back of the eye (retina) transfer the image via axons to the thalamus, specifically the lateral geniculate nuclei. The image then travels to the primary visual cortex and continues on to be processed by the visual association cortex. At each stage, the image is processed with increasing complexity. Pre-attentive processing starts with the retinal image; this image is magnified as it moves from retina to the cortex of the brain. Shades of light and dark are processed in the lateral geniculate nuclei of the thalamus. Simple and complex cells in the brain process boundary and surface information by deciphering the image's contrast, orientation, and edges. When the image hits the fovea, it is highly magnified, facilitating object recognition. The images in the periphery are less clear but help to create a complete image used for scene perception.",
            "score": 135.40505981445312
        },
        {
            "docid": "1070221_16",
            "document": "Human eye . The visual system in the human brain is too slow to process information if images are slipping across the retina at more than a few degrees per second. Thus, to be able to see while moving, the brain must compensate for the motion of the head by turning the eyes. Frontal-eyed animals have a small area of the retina with very high visual acuity, the fovea centralis. It covers about 2 degrees of visual angle in people. To get a clear view of the world, the brain must turn the eyes so that the image of the object of regard falls on the fovea. Any failure to make eye movements correctly can lead to serious visual degradation.",
            "score": 135.2856903076172
        },
        {
            "docid": "3883287_8",
            "document": "Tranquillity . Within tranquillity studies, much of the emphasis has been placed on understanding the role of vision in the perception of natural environments, which is probably not surprising, considering that upon first viewing a scene its configurational coherence can be established with incredible speed. Indeed, scene information can be captured in a single glance and the gist of a scene determined in as little as 100ms. The speed of processing of complex natural images was tested by Thorpe \"et al.\" using colour photographs of a wide range of animals (mammals, birds, reptiles and fish), in their natural environments, mixed with distracters that included pictures of forests, mountains, lakes, buildings and fruit. During this experiment, subjects were shown an image for 20ms and asked to determine whether it contained an animal or not. The electrophysiological brain responses obtained in this study showed that a decision could be made within 150ms of the image being seen, indicating the speed at which cognitive visual processing occurs. However, audition, and in particular the individual components that collectively comprise the soundscape, a term coined by Schafer to describe the ever-present array of sounds that constitute the sonic environment, also significantly inform the various schemata used to characterise differing landscape types. This interpretation is supported by the auditory reaction times, which are 50 to 60ms faster than that of the visual modality. It is also known that sound can alter visual perception and that under certain conditions areas of the brain involved in processing auditory information can be activated in response to visual stimuli.  Research conducted by Pheasant \"et al.\" has shown that when individuals make tranquillity assessments based on a uni-modal auditory or visual sensory input, they characterise the environment by drawing upon a number of key landscape and soundscape characteristics. For example, when making assessments in response to visual-only stimuli the percentage of water, flora and geological features present within a scene, positively influence how tranquil a location is perceived to be. Likewise when responding to uni-modal auditory stimuli, the perceived loudness of biological sounds positively influences the perception of tranquillity, whilst the perceived loudness of mechanical sounds have a negative effect. However, when presented with bi-modal auditory-visual stimuli the individual soundscape and landscape components alone no longer influenced the perception of tranquillity. Rather configurational coherence was provided by the percentage of natural and contextual features present within the scene and the equivalent continuous sound pressure level (LAeq).",
            "score": 135.049560546875
        },
        {
            "docid": "2175836_4",
            "document": "Hering illusion . A different framework suggests that the Hering illusion (and several other geometric illusions) are caused by temporal delays with which the visual system must cope. In this framework, the visual system extrapolates current information to \u201cperceive the present\u201d: instead of providing a conscious image of how the world was ~100 ms in the past (when signals first struck the retina), the visual system estimates how the world is likely to look in the next moment. In the case of the Hering illusion, the radial lines trick the visual system into thinking it is moving forward. Since we are not actually moving and the figure is static, we misperceive the straight lines as curved\u2014as they would appear in the next moment.",
            "score": 134.28842163085938
        },
        {
            "docid": "7289806_2",
            "document": "Screenless . Screenless video describes systems for transmitting visual information from a video source without the use of a screen. Screenless computing systems can be divided into three groups: Visual Image, Retinal Direct, and Synaptic Interface. Visual Image screenless display includes any image that the eye can perceive. The most common example of Visual Image screenless display is a hologram. In these cases, light is reflected off some intermediate object (hologram, LCD panel, or cockpit window) before it reaches the retina. In the case of LCD panels the light is refracted from the back of the panel, but is nonetheless a reflected source. Google has proposed a similar system to replace the screens of tablet computers and smartphones.",
            "score": 134.03607177734375
        },
        {
            "docid": "1316947_18",
            "document": "Ambiguous image . To go further than just perceiving the object is to recognize the object. Recognizing an object plays a crucial role in resolving ambiguous images, and relies heavily on memory and prior knowledge. To recognize an object, the visual system detects familiar components of it, and compares the perceptual representation of it with a representation of the object stored in memory. This can be done using various templates of an object, such as \"dog\" to represent dogs in general. The template method is not always successful because members of a group may significantly differ visually from each other, and may look much different if viewed from different angles. To counter the problem of viewpoint, the visual system detects familiar components of an object in 3-dimensional space. If the components of an object perceived are in the same position and orientation of an object in memory, recognition is possible. Research has shown that people that are more creative in their imagery are better able to resolve ambiguous images. This may be due to their ability to quickly identify patterns in the image. When making a mental representation of an ambiguous image, in the same way as normal images, each part is defined and then put onto the mental representation. The more complex the scene is, the longer it takes to process and add to the representation. Figures drawn in a way that avoids depth cues may become ambiguous. Classic examples of this phenomenon are the Necker cube, and the rhombille tiling (viewed as an isometric drawing of cubes).",
            "score": 133.9150848388672
        },
        {
            "docid": "599917_31",
            "document": "Mental image . As cognitive neuroscience approaches to mental imagery continued, research expanded beyond questions of serial versus parallel or topographic processing to questions of the relationship between mental images and perceptual representations. Both brain imaging (fMRI and ERP) and studies of neuropsychological patients have been used to test the hypothesis that a mental image is the reactivation, from memory, of brain representations normally activated during the perception of an external stimulus. In other words, if perceiving an apple activates contour and location and shape and color representations in the brain\u2019s visual system, then imagining an apple activates some or all of these same representations using information stored in memory. Early evidence for this idea came from neuropsychology. Patients with brain damage that impairs perception in specific ways, for example by damaging shape or color representations, seem to generally to have impaired mental imagery in similar ways. Studies of brain function in normal human brains support this same conclusion, showing activity in the brain\u2019s visual areas while subjects imagined visual objects and scenes.",
            "score": 132.58424377441406
        },
        {
            "docid": "45350085_6",
            "document": "Visual computing . At least the following disciplines are sub-fields of visual computing. More detailed descriptions of each of these fields can be found on the linked special pages. Computer graphics is a general term for all techniques that produce images as result with the help of a computer. To transform the description of objects to nice images is called rendering which is always a compromise between image quality and run-time. Techniques that can extract content information from images are called image analysis techniques. Computer vision is the ability of computers (or of robots) to recognize their environment and to interpret it correctly. Visualization is used to produce images that shall communicate messages. Data may be abstract or concrete, often with no a priori geometrical components. Visual analytics describes the discipline of interactive visual analysis of data, also described as \u201cthe science of analytical reasoning supported by the interactive visual interface\u201d. To represent objects for rendering it needs special methods and data structures, which subsumed with the term geometric modeling. In addition to describing and interactive geometric techniques, sensor data are more and more used to reconstruct geometrical models. Algorithms for the efficient control of 3D printers also belong to the field of visual computing. In contrast to image analysis image processing manipulates images to produce better images. \u201cBetter\u201d can have very different meanings subject to the respective application. Also, it has to be discriminated from image editing which describes interactive manipulation of images based on human validation. Techniques that produce the feeling of immersion into a fictive world are called virtual reality (VR). Requirements for VR include head-mounted displays, real-time tracking, and high-quality real-time rendering. Augmented reality enables the user to see the real environment in addition to the virtual objects, which augment this reality. Accuracy requirements on rendering speed and tracking precision are significantly higher here. The planning, design and uses of interfaces between people and computers is not only part of every system involving images. Due to the high bandwidth of the human visual channel (eye), images are also a preferred part of ergonomic user interfaces in any system, so that human-computer interaction is also an integral part of visual computing.",
            "score": 132.37498474121094
        },
        {
            "docid": "1894873_16",
            "document": "Eye movement . The visual system in the brain is too slow to process that information if the images are slipping across the retina at more than a few degrees per second. Thus, to be able to see while we are moving, the brain must compensate for the motion of the head by turning the eyes. Another specialisation of visual system in many vertebrate animals is the development of a small area of the retina with a very high visual acuity. This area is called the fovea, and covers about 2 degrees of visual angle in people. To get a clear view of the world, the brain must turn the eyes so that the image of the object of regard falls on the fovea. Eye movement is thus very important for visual perception, and any failure can lead to serious visual disabilities. To see a quick demonstration of this fact, try the following experiment: hold your hand up, about one foot (30\u00a0cm) in front of your nose. Keep your head still, and shake your hand from side to side, slowly at first, and then faster and faster. At first you will be able to see your fingers quite clearly. But as the frequency of shaking passes about 1 Hz, the fingers will become a blur. Now, keep your hand still, and shake your head (up and down or left and right). No matter how fast you shake your head, the image of your fingers remains clear. This demonstrates that the brain can move the eyes opposite to head motion much better than it can follow, or pursue, a hand movement. When your pursuit system fails to keep up with the moving hand, images slip on the retina and you see a blurred hand.",
            "score": 131.6280517578125
        },
        {
            "docid": "27693293_14",
            "document": "Approximate number system . The intraparietal region relies on several other brain systems to accurately perceive numbers. When using the ANS we must view the sets of objects in order to evaluate their magnitude. The primary visual cortex is responsible for disregarding irrelevant information, such as the size or shape of the objects. Certain visual cues can sometimes affect how the ANS functions.",
            "score": 131.4352569580078
        },
        {
            "docid": "29150377_11",
            "document": "Empirical theory of perception . Color vision is dependent on activation of three cone cell types in the human retina, each of which is primarily responsive to a different spectrum of light frequencies. While these retinal mechanisms enable subsequent color processing, their properties alone cannot account for the full range of color perception phenomena. In part this is due to the fact that illuminance (the amount of light shining on an object), reflectance (the amount of light an object is predisposed to reflect), and transmittance (the extent to which the light medium distorts the light as it travels) are conflated in the retinal image. This is problematic because, if color vision is to be useful, it must somehow guide behavior in line with these properties. Even so, the visual system only has access to retinal input, which does not distinguish the relative contributions of each of these factors to the final light spectra that stimulate the retina.",
            "score": 130.26150512695312
        },
        {
            "docid": "53472_6",
            "document": "Illusion . An optical illusion is characterised by visually perceived images that are deceptive or misleading. Therefore, the information gathered by the eye is processed by the brain to give, on the face of it, a percept that does not tally with a physical measurement of the stimulus source. A conventional assumption is that there are physiological illusions that occur naturally and cognitive illusions that can be demonstrated by specific visual tricks that say something more basic about how human perceptual systems work. The human brain constructs a world inside our head based on what it samples from the surrounding environment. However, sometimes it tries to organise this information it thinks best while other times it fills in the gaps. This way in which our brain works is the basis of an illusion.",
            "score": 130.0901336669922
        },
        {
            "docid": "22483_92",
            "document": "Optics . Optical illusions (also called visual illusions) are characterized by visually perceived images that differ from objective reality. The information gathered by the eye is processed in the brain to give a percept that differs from the object being imaged. Optical illusions can be the result of a variety of phenomena including physical effects that create images that are different from the objects that make them, the physiological effects on the eyes and brain of excessive stimulation (e.g. brightness, tilt, colour, movement), and cognitive illusions where the eye and brain make unconscious inferences.",
            "score": 129.53675842285156
        },
        {
            "docid": "1903855_7",
            "document": "Sensory substitution . In a regular visual system, the data collected by the retina is converted into an electrical stimulus in the optic nerve and relayed to the brain, which re-creates the image and perceives it. Because it is the brain that is responsible for the final perception, sensory substitution is possible. During sensory substitution an intact sensory modality relays information to the visual perception areas of the brain so that the person can perceive to see. With sensory substitution, information gained from one sensory modality can reach brain structures physiologically related to other sensory modalities. Touch-to-visual sensory substitution transfers information from touch receptors to the visual cortex for interpretation and perception. For example, through fMRI, we can determine which parts of the brain are activated during sensory perception. In blind persons, we can see that while they are only receiving tactile information, their visual cortex is also activated as they perceive to \"see\" objects. We can also have touch to touch sensory substitution where information from touch receptors of one region can be used to perceive touch in another region. For example, in one experiment by Bach-y-Rita, he was able to restore the touch perception in a patient who lost peripheral sensation from leprosy.",
            "score": 129.06040954589844
        },
        {
            "docid": "5664_64",
            "document": "Consciousness . In neuroscience, a great deal of effort has gone into investigating how the perceived world of conscious awareness is constructed inside the brain. The process is generally thought to involve two primary mechanisms: (1) hierarchical processing of sensory inputs, and (2) memory. Signals arising from sensory organs are transmitted to the brain and then processed in a series of stages, which extract multiple types of information from the raw input. In the visual system, for example, sensory signals from the eyes are transmitted to the thalamus and then to the primary visual cortex; inside the cerebral cortex they are sent to areas that extract features such as three-dimensional structure, shape, color, and motion. Memory comes into play in at least two ways. First, it allows sensory information to be evaluated in the context of previous experience. Second, and even more importantly, working memory allows information to be integrated over time so that it can generate a stable representation of the world\u2014Gerald Edelman expressed this point vividly by titling one of his books about consciousness \"The Remembered Present\". In computational neuroscience, Bayesian approaches to brain function have been used to understand both the evaluation of sensory information in light of previous experience, and the integration of information over time. Bayesian models of the brain are probabilistic inference models, in which the brain takes advantage of prior knowledge to interpret uncertain sensory inputs in order to formulate a conscious percept; Bayesian models have successfully predicted many perceptual phenomena in vision and the nonvisual senses.",
            "score": 129.00953674316406
        },
        {
            "docid": "1619306_63",
            "document": "Multisensory integration . Additionally, to rationalize sensory dominance, Gori et al. (2008) advocates that the brain utilises the most direct source of information during sensory immaturity. In this case, orientation is primarily a visual characteristic. It can be derived directly from the object image that forms on the retina, irrespective of other visual factors. In fact, data shows that a functional property of neurons within primate visual cortices' are their discernment to orientation. In contrast, haptic orientation judgements are recovered through collaborated patterned stimulations, evidently an indirect source susceptible to interference. Likewise, when size is concerned haptic information coming from positions of the fingers is more immediate. Visual-size perceptions, alternatively, have to be computed using parameters such as slant and distance. Considering this, sensory dominance is a useful instinct to assist with calibration. During sensory immaturity, the more simple and robust information source could be used to tweak the accuracy of the alternate source. Follow-up work by Gori et al. (2012) showed that, at all ages, vision-size perceptions are near perfect when viewing objects within the haptic workspace (i.e. at arm's reach). However, systematic errors in perception appeared when the object was positioned beyond this zone. Children younger than 14 years tend to underestimate object size, whereas adults overestimated. However, if the object was returned to the haptic workspace, those visual biases disappeared. These results support the hypothesis that haptic information may educate visual perceptions. If sources are used for cross-calibration they cannot, therefore, be combined (integrated). Maintaining access to individual estimates is a trade-off for extra plasticity over accuracy, which could be beneficial in retrospect to the developing body. Alternatively, Ernst (2008) advocates that efficient integration initially relies upon establishing correspondence \u2013 which sensory signals belong together. Indeed, studies have shown that visuo-haptic integration fails in adults when there is a perceived spatial separation, suggesting sensory information is coming from different targets. Furthermore, if the separation can be explained, for example viewing an object through a mirror, integration is re-established and can even be optimal. Ernst (2008) suggests that adults can obtain this knowledge from previous experiences to quickly determine which sensory sources depict the same target, but young children could be deficient in this area. Once there is a sufficient bank of experiences, confidence to correctly integrate sensory signals can then be introduced in their behaviour.",
            "score": 128.97366333007812
        },
        {
            "docid": "2195324_10",
            "document": "Associative visual agnosia . In the object recognition unit model by Marr (1980), the process begins with sensory perception (vision) of the object, which results in an initial representation via feature extraction of basic forms and shapes. This is followed by an integration stage, where elements of the visual field combine to form a visual percept image, the 'primary sketch'. This is a 2\u00bd dimensional (2\u00bdD) stage with a 'viewer-centered' object representation, where the features and qualities of the object are presented from the viewer's perspective. The next stage is formation of a 3 dimensional (3D) 'object-centered' object representation, where the object's features and qualities are independent of any particular perspective. Impairment at this stage would be consistent with apperceptive agnosia. This fully formed percept then triggers activation of stored structural object knowledge for familiar things. This stage is referred to as \"object recognition units\" and distinctions between apperceptive and associative forms can be made based on presentation of a defect before or after this stage, respectively. This is the level at which one is proposed to perceive familiarity toward an object, which activates the semantic memory system, containing meaning information for objects, as well as descriptive information about individual items and object classes. The semantic system can then trigger name retrieval for the objects. A patient who is not impaired up until the level of naming, retaining access to meaning information, are distinguished from agnostics and labeled as anomic.",
            "score": 128.3782196044922
        },
        {
            "docid": "1316947_16",
            "document": "Ambiguous image . Many ambiguous images are produced through some occlusion, wherein an object's texture suddenly stops. An occlusion is the visual perception of one object being behind or in front of another object, providing information about the order of the layers of texture. The illusion of occlusion is apparent in the effect of illusory contours, where occlusion is perceived despite being non-existent. Here, an ambiguous image is perceived to be an instance of occlusion. When an object is occluded, the visual system only has information about the parts of the object that can be seen, so the rest of the processing must be done deeper and must involve memory.",
            "score": 128.15090942382812
        },
        {
            "docid": "9598046_4",
            "document": "Parallax scanning . In his 1995 book, Foundations of Vision, Brian Wandell states, \"Perception is an interpretation of the retinal image, not a description. Information in the retinal image may be interpreted in many different ways. Because we begin with ambiguous information, we cannot make deductions from the retinal image, only inferences. ...we have learned that the visual system succeeds in interpreting images because of statistical regularities present in the visual environment and hence in the retinal image. These regularities permit the visual system to use fragmentary information present in the retinal image to draw accurate inferences about the physical cause of the image. For example, when we make inferences from the retinal image, the knowledge that we live in a three-dimensional world is essential to the correct interpretation of the image. Often, we are made aware of the existence of these powerful interpretations and their assumptions when they are in error, that is, when we discover a visual illusion.\"",
            "score": 127.63024139404297
        },
        {
            "docid": "525667_10",
            "document": "Human echolocation . In a 2014 study by Thaler and colleagues, the researchers first made recordings of the clicks and their very faint echoes using tiny microphones placed in the ears of the blind echolocators as they stood outside and tried to identify different objects such as a car, a flag pole, and a tree. The researchers then played the recorded sounds back to the echolocators while their brain activity was being measured using functional magnetic resonance imaging. Remarkably, when the echolocation recordings were played back to the blind experts, not only did they perceive the objects based on the echoes, but they also showed activity in those areas of their brain that normally process visual information in sighted people, primarily primary visual cortex or V1. This result is surprising, as visual areas, as their names suggest, are only active during visual tasks. The brain areas that process auditory information were no more activated by sound recordings of outdoor scenes containing echoes than they were by sound recordings of outdoor scenes with the echoes removed. Importantly, when the same experiment was carried out with sighted people who did not echolocate, these individuals could not perceive the objects and there was no echo-related activity anywhere in the brain. This suggests that the cortex of blind echolocators is plastic and reorganizes such that primary visual cortex, rather than any auditory area, becomes involved in the computation of echolocation tasks.",
            "score": 126.16171264648438
        },
        {
            "docid": "41173_3",
            "document": "Frame rate . The temporal sensitivity and resolution of human vision varies depending on the type and characteristics of visual stimulus, and it differs between individuals. The human visual system can process 10 to 12 images per second and perceive them individually, while higher rates are perceived as motion. Modulated light (such as a computer display) is perceived as stable by the majority of participants in studies when the rate is higher than 50\u00a0Hz through 90\u00a0Hz. This perception of modulated light as steady is known as the flicker fusion threshold. However, when the modulated light is non-uniform and contains an image, the flicker fusion threshold can be much higher, in the hundreds of hertz. With regard to image recognition, people have been found to recognize a specific image in an unbroken series of different images, each of which lasts as little as 13 milliseconds. Persistence of vision sometimes accounts for very short single-millisecond visual stimulus having a perceived duration of between 100\u00a0ms and 400\u00a0ms. Multiple stimuli that are very short are sometimes perceived as a single stimulus, such as a 10\u00a0ms green flash of light immediately followed by a 10\u00a0ms red flash of light perceived as a single yellow flash of light.",
            "score": 125.9283218383789
        },
        {
            "docid": "941909_23",
            "document": "Receptive field . Idealized models of visual receptive fields similar to those found in the retina, lateral geniculate nucleus (LGN) and the primary visual cortex of higher mammals can be derived in an axiomatic way from structural requirements on the first stages of visual processing that reflect symmetry properties of the surrounding world. Specifically, functional models for linear receptive fields can be derived in a principled manner to constitute a combination of Gaussian derivatives over the spatial domain and either non-causal Gaussian derivatives or truly time-causal temporal scale-space kernels over the temporal domain. Such receptive fields can be shown to enable computation of invariant visual representations under natural image transformations. By these results, the different shapes of receptive field profiles found in biological vision, which are tuned to different sizes and orientations in the image domain as well as to different image velocities in space-time, can be seen as well adapted to structure of the physical world and be explained from the requirement that the visual system should be invariant to the natural types of image transformations that occur in its environment.",
            "score": 125.87274169921875
        },
        {
            "docid": "5611461_2",
            "document": "Contrast (vision) . Contrast is the difference in luminance or colour that makes an object (or its representation in an image or display) distinguishable. In visual perception of the real world, contrast is determined by the difference in the color and brightness of the object and other objects within the same field of view. The human visual system is more sensitive to contrast than absolute luminance, we can perceive the world similarly regardless of the huge changes in illumination over the day or from place to place. The maximum \"contrast\" of an image is the contrast ratio or dynamic range.",
            "score": 125.49999237060547
        },
        {
            "docid": "5209682_9",
            "document": "Infant visual development . To perceive depth, infants as well as adults rely on several signals such as distances and kinetics. For instance, the fact that objects closer to the observer fill more space in our visual field than farther objects provides some cues into depth perception for infants. Evidence has shown that newborns' eyes do not work in the same fashion as older children or adults \u2013 mainly due to poor coordination of the eyes. Newborn\u2019s eyes move in the same direction only about half of the time. The strength of eye muscle control is positively correlated to achieve depth perception. Human eyes are formed in such a way that each eye reflects a stimulus at a slightly different angle thereby producing two images that are processed in the brain. These images provide the essential visual information regarding 3D features of the external world. Therefore, an infant\u2019s ability to control his eye movement and converge on one object is critical for developing depth perception.",
            "score": 125.40592956542969
        },
        {
            "docid": "5212945_3",
            "document": "Visual neuroscience . A recent study using Event-Related Potentials (ERPs) linked an increased neural activity in the occipito-temporal region of the brain to the visual categorization of facial expressions. Results focus on a negative peak in the ERP that occurs 170 milliseconds after the stimulus onset. This action potential, called the N170, was measured using electrodes in the occipito-temporal region, an area already known to be changed by face stimuli. Studying by using the EEG, and ERP methods allow for an extremely high temporal resolution of 4 milliseconds, which makes these kinds of experiments extremely well suited for accurately estimating and comparing the time it takes the brain to perform a certain function. Scientists used classification image techniques, to determine what parts of complex visual stimuli (such as a face) will be relied on when patients are asked to assign them to a category, or emotion. They computed the important features when the stimulus face exhibited one of five different emotions. Stimulus faces exhibiting fear had the distinguishing feature of widening eyes, and stimuli exhibiting happiness exhibited a change in the mouth to make a smile. Regardless of the expression of the stimuli's face, the region near the eyes affected the EEG before the regions near the mouth. This revealed a sequential, and predetermined order to the perception and processing of faces, with the eye being the first, and the mouth, and nose being processed after. This process of downward integration only occurred when the inferior facial features were crucial to the categorization of the stimuli. This is best explained by comparing what happens when participants were shown a face exhibiting fear, versus happiness. The N170 peaked slightly earlier for the fear stimuli at about 175 milliseconds, meaning that it took a participants less time to recognize the facial expression. This is expected because only the eyes need to be processed to recognize the emotion. However, when processing a happy expression, where the mouth is crucial to categorization, downward integration must take place, and thus the N170 peak occurred later at around 185 milliseconds. Eventually visual neuroscience aims to completely explain how the visual system processes all changes in faces as well as objects. This will give a complete view to how the world is constantly visually perceived, and may provide insight into a link between perception and consciousness.",
            "score": 125.15512084960938
        },
        {
            "docid": "24965027_26",
            "document": "Cognitive neuroscience of visual object recognition . Agnosia is a rare occurrence and can be the result of a stroke, dementia, head injury, brain infection, or hereditary. Apperceptive agnosia is a deficit in object perception creating an inability to understand the significance of objects. Similarly, associative visual agnosia is the inability to understand the significance of objects; however, this time the deficit is in semantic memory. Both of these agnosias can affect the pathway to object recognition, like Marr's Theory of Vision. More specifically unlike apperceptive agnosia, associative agnosic patients are more successful at drawing, copying, and matching tasks; however, these patients demonstrate that they can perceive but not recognize. Integrative agnosia(a subtype of associative agnosia) is the inability to integrate separate parts to form a whole image. With these types of agnosias there is damage to the ventral (what) stream of the visual processing pathway. Object orientation agnosia is the inability to extract the orientation of an object despite adequate object recognition. With this type of agnosia there is damage to the dorsal (where) stream of the visual processing pathway. This can affect object recognition in terms of familiarity and even more so in unfamiliar objects and viewpoints. A difficulty in recognizing faces can be explained by prosopagnosia. Someone with prosopagnosia cannot identify the face but is still able to perceive age, gender, and emotional expression. The brain region that specifies in facial recognition is the fusiform face area. Prosopagnosia can also be divided into apperceptive and associative subtypes. Recognition of individual chairs, cars, animals can also be impaired; therefore, these object share similar perceptual features with the face that are recognized in the fusiform face area.",
            "score": 125.05287170410156
        }
    ]
}