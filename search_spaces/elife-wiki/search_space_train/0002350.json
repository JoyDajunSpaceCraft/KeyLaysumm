{
    "q": [
        {
            "docid": "1931185_3",
            "document": "Automatic image annotation . This method can be regarded as a type of multi-class image classification with a very large number of classes - as large as the vocabulary size. Typically, image analysis in the form of extracted feature vectors and the training annotation words are used by machine learning techniques to attempt to automatically apply annotations to new images. The first methods learned the correlations between image features and training annotations, then techniques were developed using machine translation to try to translate the textual vocabulary with the 'visual vocabulary', or clustered regions known as \"blobs\". Work following these efforts have included classification approaches, relevance models and so on.",
            "score": 266.21281230449677
        },
        {
            "docid": "23167397_21",
            "document": "GENCODE . The main approach to manual gene annotation is to annotate transcripts aligned to the genome and take the genomic sequences as the reference rather than the cDNAs. The finished genomic sequence is analyzed using a modified Ensembl pipeline, and BLAST results of cDNAs/ESTs and proteins, along with various ab initio predictions, can be analyzed manually in the annotation browser tool Otterlace. Thus, more alternative spliced variants can be predicted compared with cDNA annotation. Moreover, genomic annotation produces a more comprehensive analysis of pseudogenes. There are several analysis groups in the GENCODE consortium that run pipelines that aid the manual annotators in producing models in unannotated regions, and to identify potential missed or incorrect manual annotation, including completely missing loci, missing alternative isoforms, incorrect splice sites and incorrect biotypes. These are fed back to the manual annotators using the AnnoTrack tracking system. Some of these pipelines use data from other ENCODE subgroups including RNASeq data, histone modification and CAGE and Ditag data. RNAseq data is an important new source of evidence, but generating complete gene models from it is a difficult problem. As part of GENCODE, a competition was run to assess the quality of predictions produced by various RNAseq prediction pipelines (Refer to RGASP below). To confirm uncertain models, GENCODE also has an experimental validation pipeline using RNA sequencing and RACE",
            "score": 205.21439588069916
        },
        {
            "docid": "47489445_17",
            "document": "BisQue (Bioimage Analysis and Management Platform) . BisQue has been used to manage and analyze 23.3 hours (884GB) of high definition video from dives in Bering Sea submarine canyons to evaluate the density of fishes, structure-forming corals and sponges and to document and describe fishing damage. Non-overlapping frames were extracted from each video transect at a constant frequency of 1 frame per 30s. An image processing algorithm developed in Matlab was used to detect laser dots projected onto the seafloor as a scale reference. BisQue's module system allows to wrap this Matlab code into an analysis module that can be parallelized across a compute cluster. In addition, each frame was manually annotated with objects of interest (e.g., fishes, sponges, substrates) and these annotations and other image metadata (e.g., pixel resolution, GPS location) was stored in BisQue's flexible metadata store. The annotations were then used to compute the average density of species and co-habitation behavior in different regions of the canyons, resulting in new insights into this ecosystem.",
            "score": 200.07413983345032
        },
        {
            "docid": "21017316_14",
            "document": "Fault detection and isolation . In fault detection and diagnosis, mathematical classification models which in fact belong to supervised learning methods, are trained on the training set of a labeled dataset to accurately identify the redundancies, faults and anomalous samples. During the past decades, there are different classification and preprocessing models that have been developed and proposed in this research area. \"K\"-nearest-neighbors algorithm (\"k\"NN) is one of the oldest techniques which has been used to solve fault detection and diagnosis problems. Despite the simple logic that this instance-based algorithm has, there are some problems with large dimensionality and processing time when it is used on large datasets. Since \"k\"NN is not able to automatically extract the features to overcome the curse of dimensionality, so often some data preprocessing techniques like Principal component analysis(PCA), Linear discriminant analysis(LDA) or Canonical correlation analysis(CCA) accompany it to reach a better performance. In many industrial cases, the effectiveness of \"k\"NN has been compared with other methods, specially with more complex classification models such as Support Vector Machines (SVMs), which is widely used in this field. Thanks to their appropriate nonlinear mapping using kernel methods, SVMs have an impressive performance in generalization, even with small training data. However, general SVMs do not have automatic feature extraction themselves and just like \"k\"NN, are often coupled with a data pre-processing technique. Another drawback of SVMs is that their performance is highly sensitive to the initial parameters, particularly to the kernel methods, so in each signal dataset, a parameter tuning process is required to be conducted first. Therefore, the low speed of the training phase is a limitation of SVMs when it comes to its usage in fault detection and diagnosis cases.",
            "score": 203.0609406232834
        },
        {
            "docid": "22155527_23",
            "document": "Bioimage informatics . Cell image segmentation as an important procedure is often used to study gene expression and colocalization relationship etc. of individual cells. In such cases of single-cell analysis it is often needed to uniquely determine the identities of cells while segmenting the cells. Such a recognition task is often non-trivial computationally. For model organisms such as C. elegans that have well-defined cell lineages, it is possible to explicitly recognize the cell identities via image analysis, by combining both image segmentation and pattern recognition methods. Simultaneous segmentation and recognition of cells has also been proposed as a more accurate solution for this problem when an \"atlas\" or other prior information of cells is available. Since gene expression at single cell resolution can be obtained using these types of imaging based approaches, it is possible to combine these methods with other single cell gene expression quantification methods such as RNAseq.",
            "score": 155.52132070064545
        },
        {
            "docid": "4214_4",
            "document": "Bioinformatics . Bioinformatics has become an important part of many areas of biology. In experimental molecular biology, bioinformatics techniques such as image and signal processing allow extraction of useful results from large amounts of raw data. In the field of genetics and genomics, it aids in sequencing and annotating genomes and their observed mutations. It plays a role in the text mining of biological literature and the development of biological and gene ontologies to organize and query biological data. It also plays a role in the analysis of gene and protein expression and regulation. Bioinformatics tools aid in the comparison of genetic and genomic data and more generally in the understanding of evolutionary aspects of molecular biology. At a more integrative level, it helps analyze and catalogue the biological pathways and networks that are an important part of systems biology. In structural biology, it aids in the simulation and modeling of DNA, RNA, proteins as well as biomolecular interactions.",
            "score": 175.12474954128265
        },
        {
            "docid": "1751978_8",
            "document": "Robert Haralick . In the low-and mid-level areas, Haralick has worked in image texture analysis using spatial gray tone co-occurrence texture features. These features have been used with success on biological cell images, x-ray images, satellite images, aerial images and many other kinds of images taken at small and large scales. In the feature detection area, Haralick has developed the facet model for image processing. The facet model states that many low-level image processing operations can be interpreted relative to what the processing does to the estimated underlying gray tone intensity surface of which the given image is a sampled noisy version. The facet papers develop techniques for edge detection, line detection, noise removal, peak and pit detection, as well as a variety of other topographic gray tone surface features.",
            "score": 139.0160013437271
        },
        {
            "docid": "47489445_5",
            "document": "BisQue (Bioimage Analysis and Management Platform) . BisQue provides an online resource for management and analysis of 5D biological images. In addition to image collection management, the system facilitates common biological workflows typical of biological images: imaging, experimental annotation, repeated analysis and presentation of images and results.",
            "score": 165.32028150558472
        },
        {
            "docid": "32472154_35",
            "document": "Deep learning . Artificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve their ability) to do tasks by considering examples, generally without task-specific programming. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as \"cat\" or \"no cat\" and using the analytic results to identify cats in other images. They have found most use in applications difficult to express with a traditional computer algorithm using rule-based programming.",
            "score": 192.32342314720154
        },
        {
            "docid": "47489445_3",
            "document": "BisQue (Bioimage Analysis and Management Platform) . Project BisQue originally started in 2004 as part of the US National Science Foundation (NSF) supported Center for Bio-Image Informatics at UCSB, to facilitate integration of database and image analysis methods, specifically in the context of microscopy images. Given the diversity of imaging equipment and image formats, there was an urgent need to access multiple formats in a uniform way. More importantly, there was also a need for maintaining the analysis provenance for reproducing image analysis results. Very early on, it was realized that BisQue has to go schema-less to support the needs of diverse biological experiments\u2014each experiment and analysis results are unique and slightly different. Further, from the beginning, BisQue focused on using the web browser as the standard interface. These posed unique database and visualization challenges while dealing with large scale multimodal data, and in the process BisQue has developed a unique and novel framework for visualizing very large images (100k x 100k pixels, for example), and currently supports over 250 different image file formats. Within the browser, users can now visualize 2D, 3D, 4D and 5D images, and export them to many other standardized formats. Over the years the BisQue team has closely worked with the iPlant Cyberinfrastructure (now the CyVerse), supporting the image database management needs of the plant biology community.",
            "score": 148.44805455207825
        },
        {
            "docid": "47489445_14",
            "document": "BisQue (Bioimage Analysis and Management Platform) . Biological image sharing has often been difficult due to proprietary formats. In BisQue, sharing images, metadata and analysis results can be performed through the web. The system contains an export facility that allows conversions of image formats, application of a variety of image-processing operations and export of textual or graphical annotations as XML, CSV or to Google Docs.",
            "score": 157.10689330101013
        },
        {
            "docid": "22155527_18",
            "document": "Bioimage informatics . High throughput screens using automated imaging technology (sometimes called high-content screening) have become a standard method for both drug discovery and basic biological research. Using multi-well plates, robotics, and automated microscopy, the same assay can be applied to a large library of possible reagents (typically either small molecules or RNAi) very rapidly, obtaining thousands of images in a short amount of time. Due to the high volume of data generated, automatic image analysis is a necessity.",
            "score": 141.93377351760864
        },
        {
            "docid": "45350085_6",
            "document": "Visual computing . At least the following disciplines are sub-fields of visual computing. More detailed descriptions of each of these fields can be found on the linked special pages. Computer graphics is a general term for all techniques that produce images as result with the help of a computer. To transform the description of objects to nice images is called rendering which is always a compromise between image quality and run-time. Techniques that can extract content information from images are called image analysis techniques. Computer vision is the ability of computers (or of robots) to recognize their environment and to interpret it correctly. Visualization is used to produce images that shall communicate messages. Data may be abstract or concrete, often with no a priori geometrical components. Visual analytics describes the discipline of interactive visual analysis of data, also described as \u201cthe science of analytical reasoning supported by the interactive visual interface\u201d. To represent objects for rendering it needs special methods and data structures, which subsumed with the term geometric modeling. In addition to describing and interactive geometric techniques, sensor data are more and more used to reconstruct geometrical models. Algorithms for the efficient control of 3D printers also belong to the field of visual computing. In contrast to image analysis image processing manipulates images to produce better images. \u201cBetter\u201d can have very different meanings subject to the respective application. Also, it has to be discriminated from image editing which describes interactive manipulation of images based on human validation. Techniques that produce the feeling of immersion into a fictive world are called virtual reality (VR). Requirements for VR include head-mounted displays, real-time tracking, and high-quality real-time rendering. Augmented reality enables the user to see the real environment in addition to the virtual objects, which augment this reality. Accuracy requirements on rendering speed and tracking precision are significantly higher here. The planning, design and uses of interfaces between people and computers is not only part of every system involving images. Due to the high bandwidth of the human visual channel (eye), images are also a preferred part of ergonomic user interfaces in any system, so that human-computer interaction is also an integral part of visual computing.",
            "score": 181.01534926891327
        },
        {
            "docid": "14622989_2",
            "document": "LabelMe . LabelMe is a project created by the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) which provides a dataset of digital images with annotations. The dataset is dynamic, free to use, and open to public contribution. The most applicable use of LabelMe is in computer vision research. As of October 31, 2010, LabelMe has 187,240 images, 62,197 annotated images, and 658,992 labeled objects.",
            "score": 145.89007759094238
        },
        {
            "docid": "48876541_11",
            "document": "Integral channel feature . The ICF method (ChnFtrs) has been widely exploited by researchers in Computer Vision after the work was initially published by Dollar \"et al\".. In fact, it is now used as a baseline detector due to its proven efficiency and reasonable performance. Several authors have obtained even better performance by either extending feature pool in various ways or by carefully choosing the classifier and training it with a larger dataset. Work by Zhang \"et al\" also exploited integral channel features in developing Informed Haar detector for pedestrian detection. They used the same combination of channels as Doll\u00e1r et al. but were able achieve approximately 20% higher performance than the baseline ChnFtrs method. The added performance was due to the fact that they provided better prior knowledge to their detector. It is also important to note that they used informed Haar-like features, which are second order features according to the terminology described in, whereas Doll\u00e1r \"et al\". demonstrated their results using first order channel features only, as their analysis showed that second order features barely added 0.6% increase to their detection rate. Further, Benenson \"et al\". were able to increase the detection speed of baseline ChnFtrs method by avoiding the need to resize input image.",
            "score": 172.21530508995056
        },
        {
            "docid": "28055575_2",
            "document": "Single particle analysis . Single particle analysis is a group of related computerized image processing techniques used to analyze images from transmission electron microscopy (TEM). These methods were developed to improve and extend the information obtainable from TEM images of particulate samples, typically proteins or other large biological entities such as viruses. Individual images of stained or unstained particles are very noisy, and so hard to interpret. Combining several digitized images of similar particles together gives an image with stronger and more easily interpretable features. An extension of this technique uses single particle methods to build up a three-dimensional reconstruction of the particle. Using cryogenic transmission electron microscopy it has become possible to generate reconstructions with sub-nanometer resolution and near-atomic resolution first in the case of highly symmetric viruses, and now in smaller, asymmetric proteins as well.",
            "score": 163.8476186990738
        },
        {
            "docid": "23843184_2",
            "document": "Intravital microscopy . Intravital microscopy is a form of microscopy that allows observing biological processes in live animals (\"in vivo\") at a high resolution that makes distinguishing between individual cells of a tissue possible.  Before an animal can be used for intravital microscopy imaging it has to undergo a surgery involving implantation of an imaging window. For example, if researchers want to visualize liver cells of a live mouse they will implant an imaging window into mouse\u2019s abdomen. Mice are the most common choice of animals for intravital microscopy but in special cases other rodents such as rats might be more suitable. Animals are always anesthetized throughout surgeries and imaging sessions. Intravital microscopy is used in several areas of research including neurology, immunology, stem cell and others. This technique is particularly useful to assess a progression of a disease or an effect of a drug.  Intravital microscopy involves imaging cells of a live animal through an imaging window that is implanted into the animal tissue during a special surgery. The main advantage of intravital microscopy is that it allows imaging living cells while they are in the true environment of a complex multicellular organism. Thus, intravital microscopy allows researchers to study the behavior of cells in their natural environment or in vivo rather than in a cell culture. Another advantage of intravital microscopy is that the experiment can be set up in a way to allow observing changes in a living tissue of an organism over a period of time. This is useful for many areas of research including cancer and stem cell research.  <br> High quality of modern microscopes and imaging software also permits subcellular imaging in live animals that in turn allows studying cell biology at molecular level \"in vivo\". Advancements in fluorescent protein technology and genetic tools that enable controlled expression of a given gene at a specific time in a tissue of interest also played important role in intravital microscopy development.",
            "score": 131.19852077960968
        },
        {
            "docid": "575963_32",
            "document": "Scientific visualization . \"Image annotations\": The featured plot shows Leaf Area Index (LAI), a measure of global vegetative matter, from a NetCDF dataset. The primary plot is the large plot at the bottom, which shows the LAI for the whole world. The plots on top are actually annotations that contain images generated earlier. Image annotations can be used to include material that enhances a visualization such as auxiliary plots, images of experimental data, project logos, etc.",
            "score": 174.6152741909027
        },
        {
            "docid": "1931185_4",
            "document": "Automatic image annotation . The advantages of automatic image annotation versus content-based image retrieval (CBIR) are that queries can be more naturally specified by the user. CBIR generally (at present) requires users to search by image concepts such as color and texture, or finding example queries. Certain image features in example images may override the concept that the user is really focusing on. The traditional methods of image retrieval such as those used by libraries have relied on manually annotated images, which is expensive and time-consuming, especially given the large and constantly growing image databases in existence.",
            "score": 154.52926194667816
        },
        {
            "docid": "4366564_2",
            "document": "CellProfiler . CellProfiler is free, open-source, public domain software designed to enable biologists without training in computer vision or programming to quantitatively measure phenotypes from thousands of images automatically. Advanced algorithms for image analysis are available as individual modules that can be placed in sequential order together to form a pipeline; the pipeline is then used to identify and measure  biological objects and features in images, particularly those obtained through fluorescence microscopy.",
            "score": 143.6057198047638
        },
        {
            "docid": "22155527_9",
            "document": "Bioimage informatics . The advent of automated microscopes that can acquire many images automatically is one of the reasons why analysis cannot be done by eye (otherwise, annotation would rapidly become the research bottleneck). Using automated microscopes means that some images might be out-of-focus (automated focus finding systems may sometimes be incorrect), contain a small number of cells, or be filled with debris. Therefore, the images generated will be harder to analyse than images acquired by an operator as they would have chosen other locations to image and focus correctly. On the other hand, the operator might introduce an unconscious bias in his selection by choosing only the cells whose phenotype is most like the one expected before the experiment.",
            "score": 151.45945239067078
        },
        {
            "docid": "14642741_4",
            "document": "Digital image correlation and tracking . The concept of using cross-correlation to measure shifts in datasets has been known for a long time, and it has been applied to digital images since at least the early 1970s. The present-day applications are almost innumerable and include image analysis, image compression, velocimetry, and strain estimation. Much early work in DIC in the field of mechanics was led by researchers at the University of South Carolina in the early 1980s and has been optimized and improved in recent years. Commonly, DIC relies on finding the maximum of the correlation array between pixel intensity array subsets on two or more corresponding images, which gives the integer translational shift between them. It is also possible to estimate shifts to a finer resolution than the resolution of the original images, which is often called \"subpixel\" registration because the measured shift is smaller than an integer pixel unit. For subpixel interpolation of the shift, there are other methods that do not simply maximize the correlation coefficient. An iterative approach can also be used to maximize the interpolated correlation coefficient by using nonlinear optimization techniques. The nonlinear optimization approach tends to be conceptually simpler, but as with most nonlinear optimization techniques , it is quite slow, and the problem can sometimes be reduced to a much faster and more stable linear optimization in phase space.",
            "score": 128.82743847370148
        },
        {
            "docid": "152038_36",
            "document": "Karyotype . Multicolor FISH and the older spectral karyotyping are molecular cytogenetic techniques used to simultaneously visualize all the pairs of chromosomes in an organism in different colors. Fluorescently labeled probes for each chromosome are made by labeling chromosome-specific DNA with different fluorophores. Because there are a limited number of spectrally distinct fluorophores, a combinatorial labeling method is used to generate many different colors. Fluorophore combinations are captured and analyzed by a fluorescence microscope using up to 7 narrow-banded fluorescence filters or, in the case of spectral karyotyping, by using an interferometer attached to a fluorescence microscope. In the case of an mFISH image, every combination of fluorochromes from the resulting original images is replaced by a pseudo color in a dedicated image analysis software. Thus, chromosomes or chromosome sections can be visualized and identified, allowing for the analysis of chromosomal rearrangements. In the case of spectral karyotyping, image processing software assigns a pseudo color to each spectrally different combination, allowing the visualization of the individually colored chromosomes.",
            "score": 142.18075346946716
        },
        {
            "docid": "3505075_4",
            "document": "Colocalization . Although the use of coefficients can significantly improve the reliability of colocalization detection, it depends on the number of factors, including the conditions of how samples with fluorescence were prepared and how images with colocalization were acquired and processed. Studies should be conducted with great caution, and after careful background reading. Currently the field is dogged by confusion and a standardized approach is yet to be firmly established. Attempts to rectify this include re-examination and revision of some of the coefficients, application of a factor to correct for noise, \"Replicate based noise corrected correlations for accurate measurements of colocalization\". and the proposal of further protocols, which were thoroughly reviewed by Bolte and Cordelieres (2006). In addition, due to the tendency of fluorescence images to contain a certain amount of out-of-focus signal, and poisson shot and other noise, they usually require pre-processing prior to quantification. Careful image restoration by deconvolution removes noise and increases contrast in images, improving the quality of colocalization analysis results. Up to now, most frequently used methods to quantify colocalization calculate the statistical correlation of pixel intensities in two distinct microscopy channels. More recent studies have shown that this can lead to high correlation coefficients even for targets that are known to reside in different cellular compartments. A more robust quantification of colocalization can be achieved by combining digital object recognition, the calculation of the area overlap and combination with a pixel-intensity correlation value. This led to the concept of an object-corrected Pearson's correlation coefficient.",
            "score": 119.49146103858948
        },
        {
            "docid": "37749401_29",
            "document": "Multispectral optoacoustic tomography . Fluorescent proteins that are already widespread, powerful tools for biomedical research, such as green fluorescent protein, can also be visualized using MSOT. Newly developed fluorescent proteins that absorb in the near-IR range (e.g. red fluorescent protein) allow imaging deep inside tissues. MSOT based on in situ expression of fluorescent proteins can take advantage of tissue- and development-specific promoters, allowing imaging of specific parts of an organism at specific stages of development. For example, eGFP and mCherry fluorescent proteins have been imaged in model organisms such as \"Drosophila melanogaster\" pupae and adult zebrafish, and mCherry has been imaged in tumor cells in the mouse brain. This transgenic approach is not limited to fluorescent proteins: infecting tissue with a vaccinia virus carrying the tyrosinase gene allows in situ production of melanin, which generates strong optoacoustic signal for MSOT.",
            "score": 159.0517656803131
        },
        {
            "docid": "35691407_3",
            "document": "Amira (software) . Amira is an extendable software system for scientific visualization, data analysis, and presentation of 3D and 4D data. It is being used by several thousand researchers and engineers in academia and industry around the world. Its flexible user interface and modular architecture make it a universal tool for processing and analysis of data from various modalities; e.g. micro-CT, PET, Ultrasound. Its ever-expanding functionality has made it a versatile data analysis and visualization solution, applicable to and being used in many fields, such as microscopy in biology and materials science, molecular biology, quantum physics, astrophysics, computational fluid dynamics (CFD), finite element modeling (FEM), non-destructive testing (NDT), and many more.  One of the key features, besides data visualization, is Amira\u2019s set of tools for image segmentation and geometry reconstruction. This allows the user to mark (or segment) structures and regions of interest in 3D image volumes using automatic, semi-automatic, and manual tools. The segmentation can then be used for a variety of subsequent tasks, such as volumetric analysis, density analysis, shape analysis, or the generation of 3D computer models for visualization, numerical simulations, or rapid prototyping or 3D printing, to name a few.  Other key Amira features are multi-planar and volume visualization, image registration, filament tracing, cell separation and analysis, tetrahedral mesh generation, fiber-tracking from diffusion tensor imaging (DTI) data, skeletonization, spatial graph analysis, and stereoscopic rendering of 3D data over multiple displays including CAVEs (Cave automatic virtual environments). As a commercial product Amira requires the purchase of a license or an academic subscription. A time-limited, but full-featured evaluation version is available for download free of charge.",
            "score": 161.96701109409332
        },
        {
            "docid": "9788270_12",
            "document": "Automated species identification . Properly designed, flexible, and robust, automated identification systems, organized around distributed computing architectures and referenced to authoritatively identified collections of training set data (e.g., images, and gene sequences) can, in principle, provide all systematists with access to the electronic data archives and the necessary analytic tools to handle routine identifications of common taxa. Properly designed systems can also recognize when their algorithms cannot make a reliable identification and refer that image to a specialist (whose address can be accessed from another database). Such systems can also include elements of artificial intelligence and so improve their performance the more they are used. Most tantalizingly, once morphological (or molecular) models of a species have been developed and demonstrated to be accurate, these models can be queried to determine which aspects of the observed patterns of variation and variation limits are being used to achieve the identification, thus opening the way for the discovery of new and (potentially) more reliable taxonomic characters.",
            "score": 156.16761207580566
        },
        {
            "docid": "23843184_6",
            "document": "Intravital microscopy . Fluorescence labeling of different cell lineages with differently coloured proteins allows visualizing cellular dynamics in a context of their microenvironment. If the image resolution is high enough (50 \u2013 100 \u03bcm) it can be possible to use several images to generate 3D models of cellular interactions, including protrusions that cells make while extending toward each other. 3D models from time-lapse image sequences allow assessing speed and directionality of cellular movements. Vascular structures can also be reconstructed in 3D space and changes of their permeability can be monitored throughout a period of time as fluorescent signal intensity of dyes changes when vascular permeability does. High resolution intravital microscopy can be used to visualize spontaneous and transient events. <br> It might be useful to pair up multiphoton and confocal microscopy as this allows getting more information from every imaging session. This includes visualization of more different cell types and structures to obtain more informative images and using a single animal to obtain images of all the different cell types and structures that are of interest for a given experiment. This latter is an example of The Three Rs principle implementation.",
            "score": 150.95271217823029
        },
        {
            "docid": "46975535_3",
            "document": "Multimodal learning . A lot of models/algorithms have been implemented to retrieve and classify a certain type of data, e.g. image or text (where humans who interacts with machines can extract images in a form of pictures and text that could be any message etc). However, data usually comes with different modalities (it is the degree to which a system's components may be separated or combined) which carry different information. For example, it is very common to caption an image to convey the information not presented by this image. Similarly, sometimes it is more straightforward to use an image to describe the information which may not be obvious from texts. As a results, if some different words appear in similar images, these words are very likely used to describe the same thing. Conversely, if some words are used in different images, these images may represent the same object. Thus, it is important to invite a novel model which is able to jointly represent the information such that the model can capture the correlation structure between different modalities. Moreover, it should also be able to recover missing modalities given observed ones, e.g. predicting possible image object according to text description. The Multimodal Deep Boltzmann Machine model satisfies the above purposes.",
            "score": 236.3293651342392
        },
        {
            "docid": "36466699_2",
            "document": "CellCognition . CellCognition is a free open-source computational framework for quantitative analysis of high-throughput fluorescence microscopy (time-lapse) images in the field of bioimage informatics and systems microscopy. The CellCognition framework uses image processing, computer vision and machine learning techniques for single-cell tracking and classification of cell morphologies. This enables measurements of temporal progression of cell phases, modeling of cellular dynamics and generation of phenotype map.",
            "score": 163.61464953422546
        },
        {
            "docid": "15945619_3",
            "document": "Computer-assisted surgery . The most important component for CAS is the development of an accurate model of the patient. This can be conducted through a number of medical imaging technologies including CT, MRI, x-rays, ultrasound plus many more. For the generation of this model, the anatomical region to be operated has to be scanned and uploaded into the computer system. It is possible to employ a number of scanning methods, with the datasets combined through data fusion techniques. The final objective is the creation of a 3D dataset that reproduces the exact geometrical situation of the normal and pathological tissues and structures of that region. Of the available scanning methods, the CT is preferred, because MRI data sets are known to have volumetric deformations that may lead to inaccuracies. An example data set can include the collection of data compiled with 180 CT slices, that are 1\u00a0mm apart, each having 512 by 512 pixels. The contrasts of the 3D dataset (with its tens of millions of pixels) provide the detail of soft vs hard tissue structures, and thus allow a computer to differentiate, and visually separate for a human, the different tissues and structures. The image data taken from a patient will often include intentional landmark features, in order to be able to later realign the virtual dataset against the actual patient during surgery. See patient registration.",
            "score": 156.2507619857788
        },
        {
            "docid": "21017316_16",
            "document": "Fault detection and isolation . With the research advances in ANNs and the advent of deep learning algorithms using deep and complex layers, novel classification models have been developed to cope with fault detection and diagnosis. Most of the shallow learning models extract a few feature values from signals, causing a dimensionality reduction from the original signal. By using Convolutional neural networks, the continuous wavelet transform scalogram can be directly classified to normal and faulty classes. Such a technique avoids omitting any important fault message and results in a better performance of fault detection and diagnosis. In addition, by transforming signals to image constructions, 2D Convolutional neural networks can be implemented to identify faulty signals from vibration image features.",
            "score": 217.18721532821655
        }
    ],
    "r": [
        {
            "docid": "1931185_3",
            "document": "Automatic image annotation . This method can be regarded as a type of multi-class image classification with a very large number of classes - as large as the vocabulary size. Typically, image analysis in the form of extracted feature vectors and the training annotation words are used by machine learning techniques to attempt to automatically apply annotations to new images. The first methods learned the correlations between image features and training annotations, then techniques were developed using machine translation to try to translate the textual vocabulary with the 'visual vocabulary', or clustered regions known as \"blobs\". Work following these efforts have included classification approaches, relevance models and so on.",
            "score": 266.2127990722656
        },
        {
            "docid": "3259720_7",
            "document": "Multifactor dimensionality reduction . As illustrated above, the basic constructive induction algorithm in MDR is very simple. However, its implementation for mining patterns from real data can be computationally complex. As with any machine learning algorithm there is always concern about overfitting. That is, machine learning algorithms are good at finding patterns in completely random data. It is often difficult to determine whether a reported pattern is an important signal or just chance. One approach is to estimate the generalizability of a model to independent datasets using methods such as cross-validation. Models that describe random data typically don't generalize. Another approach is to generate many random permutations of the data to see what the data mining algorithm finds when given the chance to overfit. Permutation testing makes it possible to generate an empirical p-value for the result. Replication in independent data may also provide evidence for an MDR model but can be sensitive to difference in the data sets. These approaches have all been shown to be useful for choosing and evaluating MDR models. An important step in an machine learning exercise is interpretation. Several approaches have been used with MDR including entropy analysis and pathway analysis. Tips and approaches for using MDR to model gene-gene interactions have been reviewed.",
            "score": 239.17269897460938
        },
        {
            "docid": "33413026_21",
            "document": "Artificial muscle . Control problems regarding highly nonlinear systems have generally been addressed through a trial-and-error approach through which \"fuzzy models\" (Chan et al., 2003) of the system's behavioral capacities could be teased out (from the experimental results of the specific system being tested) by a knowledgeable human expert. However, some research has employed \"real data\" (Nelles O., 2000) to train up the accuracy of a given fuzzy model while simultaneously avoiding the mathematical complexities of previous models. Ahn et al.'s experiment is simply one example of recent experiments that use modified genetic algorithms (MGAs) to train up fuzzy models using experimental input-output data from a PAM robot arm.",
            "score": 238.4452362060547
        },
        {
            "docid": "49082762_2",
            "document": "List of datasets for machine learning research . These datasets are used for machine-learning research and have been cited in peer-reviewed academic journals. Datasets are an integral part of the field of machine learning. Major advances in this field can result from advances in learning algorithms (such as deep learning), computer hardware, and, less-intuitively, the availability of high-quality training datasets. High-quality labeled training datasets for supervised and semi-supervised machine learning algorithms are usually difficult and expensive to produce because of the large amount of time needed to label the data. Although they do not need to be labeled, high-quality datasets for unsupervised learning can also be difficult and costly to produce.",
            "score": 238.16636657714844
        },
        {
            "docid": "46975535_3",
            "document": "Multimodal learning . A lot of models/algorithms have been implemented to retrieve and classify a certain type of data, e.g. image or text (where humans who interacts with machines can extract images in a form of pictures and text that could be any message etc). However, data usually comes with different modalities (it is the degree to which a system's components may be separated or combined) which carry different information. For example, it is very common to caption an image to convey the information not presented by this image. Similarly, sometimes it is more straightforward to use an image to describe the information which may not be obvious from texts. As a results, if some different words appear in similar images, these words are very likely used to describe the same thing. Conversely, if some words are used in different images, these images may represent the same object. Thus, it is important to invite a novel model which is able to jointly represent the information such that the model can capture the correlation structure between different modalities. Moreover, it should also be able to recover missing modalities given observed ones, e.g. predicting possible image object according to text description. The Multimodal Deep Boltzmann Machine model satisfies the above purposes.",
            "score": 236.32937622070312
        },
        {
            "docid": "53970843_3",
            "document": "Machine learning in bioinformatics . Prior to the emergence of machine learning algorithms, bioinformatics algorithms had to be explicitly programmed by hand which, for problems such as protein structure prediction, proves extremely difficult. Machine learning techniques such as deep learning enable the algorithm to make use of automatic feature learning which means that based on the dataset alone, the algorithm can learn how to combine multiple features of the input data into a more abstract set of features from which to conduct further learning. This multi-layered approach to learning patterns in the input data allows such systems to make quite complex predictions when trained on large datasets. In recent years, the size and number of available biological datasets have skyrocketed, enabling bioinformatics researchers to make use of these machine learning systems. Machine learning has been applied to six main subfields of bioinformatics: genomics, proteomics, microarrays, systems biology, evolution, and text mining.",
            "score": 236.0722198486328
        },
        {
            "docid": "43502368_6",
            "document": "Vanishing gradient problem . Similar ideas have been used in feed-forward neural network for unsupervised pre-training to structure a neural network, making it first learn generally useful feature detectors. Then the network is trained further by supervised back-propagation to classify labeled data. The deep belief network model by Hinton et al. (2006) involves learning the distribution of a high level representation using successive layers of binary or real-valued latent variables. It uses a restricted Boltzmann machine to model each new layer of higher level features. Each new layer guarantees an increase on the lower-bound of the log likelihood of the data, thus improving the model, if trained properly. Once sufficiently many layers have been learned the deep architecture may be used as a generative model by reproducing the data when sampling down the model (an \"ancestral pass\") from the top level feature activations. Hinton reports that his models are effective feature extractors over high-dimensional, structured data. This work plays a key role in reintroducing the interests in deep neural network research and consequently leads to the developments of Deep learning, although deep belief network is no longer the main deep learning technique.",
            "score": 235.53330993652344
        },
        {
            "docid": "15261618_6",
            "document": "Constellation model . In the late '90s, Burl et al. revisited the Fischler and Elschlager model for the purpose of face recognition. In their work, Burl et al. used manual selection of constellation parts in training images to construct a statistical model for a set of detectors and the relative locations at which they should be applied. In 2000, Weber et al. made the significant step of training the model using a more unsupervised learning process, which precluded the necessity for tedious hand-labeling of parts. Their algorithm was particularly remarkable because it performed well even on cluttered and occluded image data. Fergus et al. then improved upon this model by making the learning step fully unsupervised, having both shape and appearance learned simultaneously, and accounting explicitly for the relative scale of parts.",
            "score": 227.03953552246094
        },
        {
            "docid": "2852956_9",
            "document": "Market sentiment . Under the third direction, researchers propose to use text mining and sentiment analysis algorithms to extract information about investors\u2019 mood from social networks, media platforms, blogs, newspaper articles, and other relevant sources of textual data (sometimes referred as news analytics). A thread of publications (Barber & Odean (2008), Dougal \"et al.\" (2012), and Ahern & Sosyura (2015)) report a significant influence of financial articles and sensational news on behavior of stock prices. It is also not surprising, that such popular sources of news as Wall Street Journal, New York Times or Financial Times have a profound influence on the market. The strength of the impact can vary between different columnists even inside a particular journal (Dougal \"et al.\" (2012)). Tetlock (2007) suggests a successful measure of investors\u2019 mood by counting the number of \"negative\" words in a popular Wall Street Journal column \"Abreast of the market\". Zhang \"et al.\" (2011) and Bollen \"et al.\" (2011) report Twitter to be an extremely important source of sentiment data, which helps to predict stock prices and volatility. The usual way to analyze the influence of the data from micro-blogging platforms on behavior of stock prices is to construct special mood tracking indexes. The easiest way would be to count the number of \"positive\" and \"negative\" words in each relevant tweet and construct a combined indicator based on this data. Nasseri \"et al.\" (2014) reports the predictive power of StockTwits (Twitter-like platform specialized on exchanging trading-related opinions) data with respect to behavior of stock prices. An alternative, but more demanding, way is to engage human experts to annotate a large number of tweets with the expected stock moves, and then construct a machine learning model for prediction. The application of the event study methodology to Twitter mood shows significant correlation to cumulative abnormal returns (Sprenger \"et al.\" (2014), Ranco \"et al. (2015)\", Gabrov\u0161ek \"et al.\" (2017) ). Karabulut (2013) reports Facebook to be a good source of information about investors\u2019 mood. Overall, most popular social networks, finance-related media platforms, magazines, and journals can be a valuable source of sentiment data, summarized in Peterson (2016). However, important to notice that it is relatively more difficult to collect such type of data (in most cases a researcher needs a special software). In addition, analysis of such data can also require deep machine learning and data mining knowledge (Hotho \"et al.\" (2005)).",
            "score": 225.356201171875
        },
        {
            "docid": "15951862_16",
            "document": "Sparse approximation . Sparse approximation ideas and algorithms has been extensively used in signal processing, image processing, machine learning, medical imaging, array processing, data mining, and more. In most of these applications, the unknown signal of interest is modeled as a sparse combination of few atoms from a given dictionary, and this is used as the regularization of the problem. These problems are typically accompanied by a dictionary learning mechanism that aims to fit formula_2 to best match the model to the given data. The use of sparsity-inspired models has led to state-of-the-art results in a wide set of applications. Recent work suggests that there is a tight connection between sparse representation modeling and deep-learning.",
            "score": 223.9411163330078
        },
        {
            "docid": "938663_8",
            "document": "Multi-task learning . Related to multi-task learning is the concept of knowledge transfer. Whereas traditional multi-task learning implies that a shared representation is developed concurrently across tasks, transfer of knowledge implies a sequentially shared representation. Large scale machine learning projects such as the deep convolutional neural network GoogLeNet, an image-based object classifier, can develop robust representations which may be useful to further algorithms learning related tasks. For example, the pre-trained model can be used as a feature extractor to perform pre-processing for another learning algorithm. Or the pre-trained model can be used to initialize a model with similar architecture which is then fine-tuned to learn a different classification task.",
            "score": 223.5729522705078
        },
        {
            "docid": "30667214_4",
            "document": "Evolving intelligent system . EISs form the theoretical and methodological basis for the Autonomous Learning Machines (ALMA) and autonomous multi-model systems (ALMMo) as well as of the Autonomous Learning Systems. Evolving Fuzzy Rule-based classifiers, in particular, is a very powerful new concept that offers much more than simply incremental or online classifiers \u2013 it can cope with new classes being added or existing classes being merged. This is much more than just adapting to new data samples being added or classification surfaces being evolved. Fuzzy rule-based classifiers are the methodological basis of a new approach to deep learning that was until now considered as a form of multi-layered neural networks. Deep Learning offers high precision levels surpassing the level of human ability and grabbed the imagination of the researchers, industry and the wider public. However, it has a number of intrinsic constraints and limitations. These include: Most, if not all, of the above limitations can be avoided with the use of the Deep (Fuzzy) Rule-based Classifiers, which were recently introduced based on ALMMo, while achieving similar or even better performance. The resulting prototype-based IF\u2026THEN\u2026models are fully interpretable and dynamically evolving (they can adapt quickly and automatically to new data patterns or even new classes). They are non-parametric and, therefore, their training is non-iterative and fast (it can take few milliseconds per data sample/image on a normal laptop which contrasts with the multiple hours the current deep learning methods require for training even when they use GPUs and HPC). Moreover, they can be trained incrementally/online/in real-time.",
            "score": 222.12896728515625
        },
        {
            "docid": "32472154_28",
            "document": "Deep learning . The 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech, and the possibility that given more capable hardware and large-scale data sets that deep neural nets (DNN) might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets. However, it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large, context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems. The nature of the recognition errors produced by the two types of systems was characteristically different, offering technical insights into how to integrate deep learning into the existing highly efficient, run-time speech decoding system deployed by all major speech recognition systems. Analysis around 2009-2010, contrasted the GMM (and other generative speech models) vs. DNN models, stimulated early industrial investment in deep learning for speech recognition, eventually leading to pervasive and dominant use in that industry. That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models.",
            "score": 221.35455322265625
        },
        {
            "docid": "24027777_3",
            "document": "Figure Eight Inc. . Figure Eight uses human intelligence to do simple tasks such as transcribing text or annotating images to train machine learning algorithms. Figure Eight's software automates tasks for machine learning algorithms, which can be used to improve catalog search results, approve photos or support customers and the technology can be used in the development of self-driving cars, intelligent personal assistants and other technology that uses machine learning.",
            "score": 221.2936553955078
        },
        {
            "docid": "48841414_5",
            "document": "Multiple instance learning . Keeler et al., in his work in early 1990s was the first one to explore the area of MIL. The actual term multi-instance learning was introduced in the middle of the 1990s, by Dietterich et al. while they were investigating the problem of drug activity prediction. They tried to create a learning systems that could predict whether new molecule was qualified to make some drug, or not, through analyzing a collection of known molecules. Molecules can have many alternative low-energy states, but only one, or some of them, are qualified to make a drug. The problem arose because scientists could only determine if molecule is qualified, or not, but they couldn\u2019t say exactly which of its low-energy shapes are responsible for that. One of the proposed ways to solve this problem was to use supervised learning, and regard all the low-energy shapes of the qualified molecule as positive training instances, while all of the low-energy shapes of unqualified molecules as negative instances. Dietterich et al. showed that such method would have a high false positive noise, from all low-energy shapes that are mislabeled as positive, and thus wasn\u2019t really useful. Their approach was to regard each molecule as a labeled bag, and all the alternative low-energy shapes of that molecule as instances in the bag, without individual labels. Thus formulating multiple-instance learning.  Solution to the multiple instance learning problem that Dietterich et al. proposed is three axis-parallel rectangle (APR) algorithm. It attempts to search for appropriate axis-parallel rectangles constructed by the conjunction of the features. They tested the algorithm on Musk dataset, which is a concrete test data of drug activity prediction and the most popularly used benchmark in multiple-instance learning. APR algorithm achieved the best result, but it should be noted that APR was designed with Musk data in mind.",
            "score": 218.70843505859375
        },
        {
            "docid": "1514392_4",
            "document": "Training, test, and validation sets . The model is initially fit on a training dataset, that is a set of examples used to fit the parameters (e.g. weights of connections between neurons in artificial neural networks) of the model. The model (e.g. a neural net or a naive Bayes classifier) is trained on the training dataset using a supervised learning method (e.g. gradient descent or stochastic gradient descent). In practice, the training dataset often consist of pairs of an input vector and the corresponding \"answer\" vector or scalar, which is commonly denoted as the \"target\". The current model is run with the training dataset and produces a result, which is then compared with the \"target\", for each input vector in the training dataset. Based on the result of the comparison and the specific learning algorithm being used, the parameters of the model are adjusted. The model fitting can include both variable selection and parameter estimation.",
            "score": 217.68898010253906
        },
        {
            "docid": "34633465_9",
            "document": "Geometric feature learning . 2.According to the recognition algorithm, evaluate the result. If the result is true, new object classes are recognised. The key point of recognition algorithm is to find the most distinctive features among all features of all classes. So using below equation to maximise the feature formula_6 Measure the value of a feature in images, formula_6 and formula_10, and localise a feature:  Where formula_12 is defined as  formula_13  After recognise the features, the results should be evaluated to determine whether the classes can be recognised, There are five evaluation categories of recognition results: correct, wrong, ambiguous, confused and ignorant. When the evaluation is correct, add a new training image and train it. If the recognition failed, the feature nodes should be maximise their distinctive power which is defined by the Kolmogorov-Smirno distance (KSD).  3.Feature learning algorithm After a feature is recognised, it should be applied to Bayesian network to recognise the image, using the feature learning algorithm to test.",
            "score": 217.60284423828125
        },
        {
            "docid": "21017316_16",
            "document": "Fault detection and isolation . With the research advances in ANNs and the advent of deep learning algorithms using deep and complex layers, novel classification models have been developed to cope with fault detection and diagnosis. Most of the shallow learning models extract a few feature values from signals, causing a dimensionality reduction from the original signal. By using Convolutional neural networks, the continuous wavelet transform scalogram can be directly classified to normal and faulty classes. Such a technique avoids omitting any important fault message and results in a better performance of fault detection and diagnosis. In addition, by transforming signals to image constructions, 2D Convolutional neural networks can be implemented to identify faulty signals from vibration image features.",
            "score": 217.1872100830078
        },
        {
            "docid": "32472154_18",
            "document": "Deep learning . By 1991 such systems were used for recognizing isolated 2-D hand-written digits, while recognizing 3-D objects was done by matching 2-D images with a handcrafted 3-D object model. Weng \"et al.\" suggested that a human brain does not use a monolithic 3-D object model and in 1992 they published Cresceptron, a method for performing 3-D object recognition in cluttered scenes. Cresceptron is a cascade of layers similar to Neocognitron. But while Neocognitron required a human programmer to hand-merge features, Cresceptron learned an open number of features in each layer without supervision, where each feature is represented by a convolution kernel. Cresceptron segmented each learned object from a cluttered scene through back-analysis through the network. Max pooling, now often adopted by deep neural networks (e.g. ImageNet tests), was first used in Cresceptron to reduce the position resolution by a factor of (2x2) to 1 through the cascade for better generalization.",
            "score": 215.8390350341797
        },
        {
            "docid": "7466947_8",
            "document": "Multi-label classification . Based on learning paradigms, the existing multi-label classification techniques can be classified into batch learning and online machine learning. Batch learning algorithms require all the data samples to be available beforehand. It trains the model using the entire training data and then predicts the test sample using the found relationship. The online learning algorithms, on the other hand, incrementally build their models in sequential iterations. In iteration t, an online algorithm receives a sample, x and predicts its label(s) \u0177 using the current model; the algorithm then receives y, the true label(s) of x and updates its model based on the sample-label pair: (x, y). Recently, a new learning paradigm called the progressive learning technique has been developed. The progressive learning technique is capable of not only learning from new samples but also capable of learning multiple new labels of data being introduced to the model and yet retain the knowledge learnt thus far.",
            "score": 213.36886596679688
        },
        {
            "docid": "27122321_15",
            "document": "Synaptic tagging . While the information gained on the synaptic tagging hypothesis mainly resulted from experiments that apply stimulation to synapse, a similar model can be applied when considering the process of learning in a broader behavioral sense. Fabricio Ballarini and colleagues created this behavioral tagging model by testing spatial object recognition, contextual conditioning, and conditioned taste aversion in rats with weak training, which is a training that normal only creates a short term memory. However, they paired this weak training with a separate, arbitrary behavioral event that induces protein synthesis and found that so long as the two behavioral events were coupled within a certain time frame, the weak training was sufficient to produce long term memory of that learning task. The researchers believed that the weak learning established a \"learning tag\" to be used later when proteins arrived as a result of the other task, resulting in the formation of long-term memory for even the weak training. This behavioral learning model mirrors the synaptic tagging model, in which a weak stimulation establishes E-LTP that may be serve as the tag used in converting the weak potentiation to the stronger, more persistent L-LTP, once the high-frequency stimulation presents itself.",
            "score": 212.5956573486328
        },
        {
            "docid": "21652_10",
            "document": "Natural language processing . Recent research has increasingly focused on unsupervised and semi-supervised learning algorithms. Such algorithms are able to learn from data that has not been hand-annotated with the desired answers, or using a combination of annotated and non-annotated data. Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data. However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the inferior results.",
            "score": 212.37803649902344
        },
        {
            "docid": "27837170_12",
            "document": "History of natural language processing . Recent research has increasingly focused on unsupervised and semi-supervised learning algorithms. Such algorithms are able to learn from data that has not been hand-annotated with the desired answers, or using a combination of annotated and non-annotated data. Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data. However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the inferior results.",
            "score": 212.37803649902344
        },
        {
            "docid": "48777199_2",
            "document": "Manifold regularization . In machine learning, Manifold regularization is a technique for using the shape of a dataset to constrain the functions that should be learned on that dataset. In many machine learning problems, the data to be learned do not cover the entire input space. For example, a facial recognition system may not need to classify any possible image, but only the subset of images that contain faces. The technique of manifold learning assumes that the relevant subset of data comes from a manifold, a mathematical structure with useful properties. The technique also assumes that the function to be learned is \"smooth\": data with different labels are not likely to be close together, and so the labeling function should not change quickly in areas where there are likely to be many data points. Because of this assumption, a manifold regularization algorithm can use unlabeled data to inform where the learned function is allowed to change quickly and where it is not, using an extension of the technique of Tikhonov regularization. Manifold regularization algorithms can extend supervised learning algorithms in semi-supervised learning and transductive learning settings, where unlabeled data are available. The technique has been used for applications including medical imaging, geographical imaging, and object recognition.",
            "score": 212.19509887695312
        },
        {
            "docid": "32472154_32",
            "document": "Deep learning . Significant additional impacts in image or object recognition were felt from 2011 to 2012. Although CNNs trained by backpropagation had been around for decades, and GPU implementations of NNs for years, including CNNs, fast implementations of CNNs with max-pooling on GPUs in the style of Ciresan and colleagues were needed to progress on computer vision. In 2011, this approach achieved for the first time superhuman performance in a visual pattern recognition contest. Also in 2011, it won the ICDAR Chinese handwriting contest, and in May 2012, it won the ISBI image segmentation contest. Until 2011, CNNs did not play a major role at computer vision conferences, but in June 2012, a paper by Ciresan et al. at the leading conference CVPR showed how max-pooling CNNs on GPU can dramatically improve many vision benchmark records. In October 2012, a similar system by Krizhevsky et al. won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. In November 2012, Ciresan et al.'s system also won the ICPR contest on analysis of large medical images for cancer detection, and in the following year also the MICCAI Grand Challenge on the same topic. In 2013 and 2014, the error rate on the ImageNet task using deep learning was further reduced, following a similar trend in large-scale speech recognition. The Wolfram Image Identification project publicized these improvements.",
            "score": 210.48886108398438
        },
        {
            "docid": "35587999_12",
            "document": "Toponym Resolution . Toponym resolution methods can be generally divided into supervised and unsupervised models. Supervised methods typically cast the problem as a learning task wherein the model first extracts contextual and non-contextual features and then, a classifier is trained on a labelled dataset. Adaptive model is one of the prominent models proposed in resolving toponyms. For each interpretation of a toponym, the model derives context-sensitive features based on geographical proximity and sibling relationships with other interpretations. In addition to context related features, the model benefits from context-free features including population, and audience location. On the other hand, unsupervised models do not warrant annotated data. They are superior to supervised models when the annotated corpus is not sufficiently large, and supervised models may not generalize well.",
            "score": 209.35153198242188
        },
        {
            "docid": "32472154_69",
            "document": "Deep learning . Deep learning has been successfully applied to inverse problems such as denoising, super-resolution, and inpainting. These applications include learning methods such \"Shrinkage Fields for Effective Image Restoration\" which trains on an image dataset, and Deep Image Prior, which trains on the image that needs restoration.",
            "score": 207.73851013183594
        },
        {
            "docid": "17878526_13",
            "document": "Digital pathology . Image analysis tools are used to derive objective quantification measures from digital slides. Image segmentation and classification algorithms are used to identify medically significant regions and objects on digital slides. Recent developments in machine learning using deep learning methods are very promising and allow to make information hidden in integrated pathological data (images, patient history and *omics data) in arbitrarily high-dimensional spaces accessible and quantifiable, thereby generating a novel source of information which is not yet available to the expert and not exploited in current Digital Pathology settings.",
            "score": 206.97413635253906
        },
        {
            "docid": "34633465_10",
            "document": "Geometric feature learning . The probably approximately correct (PAC) model was applied by D. Roth (2002) to solve computer vision problem by developing a distribution-free learning theory based on this model. This theory heavily relied on the development of feature-efficient learning approach. The goal of this algorithm is to learn an object represented by some geometric features in an image. The input is a feature vector and the output is 1 which means successfully detect the object or 0 otherwise. The main point of this learning approach is collecting representative elements which can represent the object through a function and testing by recognising an object from image to find the representation with high probability.  The learning algorithm aims to predict whether the learned target concept formula_15 is belongs to a class, where X is the instance space consists with parameters and then test whether the prediction is correct.",
            "score": 206.42994689941406
        },
        {
            "docid": "42358441_4",
            "document": "Deep lambertian networks . Combining Deep Belief Nets with the Lambertian reflectance assumption, the model can learn good priors over the albedo from 2D images. Illumination variations can be explained by changing only the lighting latent variable. By transferring learned knowledge from similar objects, albedo and surface normals estimation from a single image is also possible. Experiments demonstrate that this model is able to generalize as well as improve over standard baselines in one-shot face recognition.",
            "score": 205.30775451660156
        },
        {
            "docid": "23167397_21",
            "document": "GENCODE . The main approach to manual gene annotation is to annotate transcripts aligned to the genome and take the genomic sequences as the reference rather than the cDNAs. The finished genomic sequence is analyzed using a modified Ensembl pipeline, and BLAST results of cDNAs/ESTs and proteins, along with various ab initio predictions, can be analyzed manually in the annotation browser tool Otterlace. Thus, more alternative spliced variants can be predicted compared with cDNA annotation. Moreover, genomic annotation produces a more comprehensive analysis of pseudogenes. There are several analysis groups in the GENCODE consortium that run pipelines that aid the manual annotators in producing models in unannotated regions, and to identify potential missed or incorrect manual annotation, including completely missing loci, missing alternative isoforms, incorrect splice sites and incorrect biotypes. These are fed back to the manual annotators using the AnnoTrack tracking system. Some of these pipelines use data from other ENCODE subgroups including RNASeq data, histone modification and CAGE and Ditag data. RNAseq data is an important new source of evidence, but generating complete gene models from it is a difficult problem. As part of GENCODE, a competition was run to assess the quality of predictions produced by various RNAseq prediction pipelines (Refer to RGASP below). To confirm uncertain models, GENCODE also has an experimental validation pipeline using RNA sequencing and RACE",
            "score": 205.2144012451172
        },
        {
            "docid": "17115475_15",
            "document": "Anastasios Venetsanopoulos . For thousands of years, humans have used visually-perceived body characteristics such as face and gait to recognize one another. This remarkable ability of human visual system led Professor Venetsanopoulos to build automated systems to recognize individuals from digitally captured facial images and gait sequences. Face and gait recognition belong to the field of biometrics, a very active area of research in computer science, mainly motivated by government and security-related considerations. Face and gait are two typical physiological and behavioral biometrics. Venetsanopoulos contributed to both areas and his research has been extensively cited. There are two general approaches to the subject: the appearance-based approach and the model-based approach. Appearance-based face recognition processes a 2-D facial image as 2-D holistic patterns. The whole face region is the raw input to a recognition system and each face image is commonly represented by a high-dimensional vector consisting of the pixel intensity values in the image. Thus, face recognition is transformed to a multivariate, statistical pattern recognition problem. In a similar fashion to appearance-based face recognition, an appearance-based gait recognition approach considers gait as a holistic pattern and uses a full-body representation of a human subject as silhouettes or contours. Gait video sequences are naturally three-dimensional objects, formally named tensor objects, and they are very difficult to deal with using traditional vector-based learning algorithms. In order to deal with these tensor objects effectively, Venetsanopoulos and his research team developed a framework of multilinear subspace learning, so that computation and memory demands are reduced, natural structure and correlation in the original data are preserved, and more compact and useful features can be obtained. The Model-based gait recognition approach considers a human subject as an articulated object, represented by various body poses. Professor Venetsanopoulos proposed a full-body, layered deformable model (LDM) inspired by the manually labeled body-part-level silhouettes. The LDM has a layered structure to model self-occlusion between body parts and it is deformable, so simple limb deformation is taken into consideration. In addition, it also models shoulder swing. The LDM parameters can be recovered from automatically extracted silhouettes and then used for recognition.",
            "score": 204.87197875976562
        }
    ]
}