{
    "q": [
        {
            "docid": "599917_31",
            "document": "Mental image . As cognitive neuroscience approaches to mental imagery continued, research expanded beyond questions of serial versus parallel or topographic processing to questions of the relationship between mental images and perceptual representations. Both brain imaging (fMRI and ERP) and studies of neuropsychological patients have been used to test the hypothesis that a mental image is the reactivation, from memory, of brain representations normally activated during the perception of an external stimulus. In other words, if perceiving an apple activates contour and location and shape and color representations in the brain\u2019s visual system, then imagining an apple activates some or all of these same representations using information stored in memory. Early evidence for this idea came from neuropsychology. Patients with brain damage that impairs perception in specific ways, for example by damaging shape or color representations, seem to generally to have impaired mental imagery in similar ways. Studies of brain function in normal human brains support this same conclusion, showing activity in the brain\u2019s visual areas while subjects imagined visual objects and scenes.",
            "score": 245.49396276474
        },
        {
            "docid": "5212945_3",
            "document": "Visual neuroscience . A recent study using Event-Related Potentials (ERPs) linked an increased neural activity in the occipito-temporal region of the brain to the visual categorization of facial expressions. Results focus on a negative peak in the ERP that occurs 170 milliseconds after the stimulus onset. This action potential, called the N170, was measured using electrodes in the occipito-temporal region, an area already known to be changed by face stimuli. Studying by using the EEG, and ERP methods allow for an extremely high temporal resolution of 4 milliseconds, which makes these kinds of experiments extremely well suited for accurately estimating and comparing the time it takes the brain to perform a certain function. Scientists used classification image techniques, to determine what parts of complex visual stimuli (such as a face) will be relied on when patients are asked to assign them to a category, or emotion. They computed the important features when the stimulus face exhibited one of five different emotions. Stimulus faces exhibiting fear had the distinguishing feature of widening eyes, and stimuli exhibiting happiness exhibited a change in the mouth to make a smile. Regardless of the expression of the stimuli's face, the region near the eyes affected the EEG before the regions near the mouth. This revealed a sequential, and predetermined order to the perception and processing of faces, with the eye being the first, and the mouth, and nose being processed after. This process of downward integration only occurred when the inferior facial features were crucial to the categorization of the stimuli. This is best explained by comparing what happens when participants were shown a face exhibiting fear, versus happiness. The N170 peaked slightly earlier for the fear stimuli at about 175 milliseconds, meaning that it took a participants less time to recognize the facial expression. This is expected because only the eyes need to be processed to recognize the emotion. However, when processing a happy expression, where the mouth is crucial to categorization, downward integration must take place, and thus the N170 peak occurred later at around 185 milliseconds. Eventually visual neuroscience aims to completely explain how the visual system processes all changes in faces as well as objects. This will give a complete view to how the world is constantly visually perceived, and may provide insight into a link between perception and consciousness.",
            "score": 200.45926296710968
        },
        {
            "docid": "3380919_3",
            "document": "David Heeger . In the fields of perceptual psychology, systems neuroscience, cognitive neuroscience, and computational neuroscience, Heeger has developed computational theories of neuronal processing in the visual system, and he has performed psychophysics (perceptual psychology) and neuroimaging (functional magnetic resonance imaging, fMRI) experiments on human vision. His contributions to computational neuroscience include theories for how the brain can sense optic flow and egomotion, and a theory of neural processing called the normalization model. His empirical research has contributed to our understanding of the topographic organization of visual cortex (retinotopy), visual awareness, visual pattern detection/discrimination, visual motion perception, stereopsis (depth perception), attention, working memory, the control of eye and hand movements, neural processing of complex audio-visual and emotional experiences (movies, music, narrative), abnormal visual processing in dyslexia, and neurophysiological characteristics of autism.",
            "score": 136.5764877796173
        },
        {
            "docid": "49045837_6",
            "document": "Spatial ability . Spatial perception is also very relevant in sports. For example, a study found that cricket players who were faster at picking up information from briefly presented visual displays were significantly better batsmen in an actual game. A 2015 study published in the \"Journal of Vision\" found that soccer players had higher perceptual ability for body kinematics such as processing multitasking crowd scenes which involve pedestrians crossing a street or complex dynamic visual scenes. Another study published in the \"Journal of Human Kinetics\" on fencing athletes found that achievement level was highly correlated with spatial perceptual skills such as visual discrimination, visual-spatial relationships, visual sequential memory, narrow attentional focus and visual information processing. A review published in the journal of \"Neuropsychologia\" found that spatial perception involves attributing meaning to an object or space, so that their sensory processing is actually part of semantic processing of the incoming visual information. The review also found that spatial perception involves the human visual system in the brain and the parietal lobule which is responsible for visuomotor processing and visually goal-directed action. Studies have also found that individuals who played first person shooting games had better spatial perceptual skills like faster and more accurate performance in a peripheral and identification task while simultaneously performing a central search. Researchers suggested that, in addition to enhancing the ability to divide attention, playing action games significantly enhances perceptual skills like top-down guidance of attention to possible target locations.",
            "score": 133.89096093177795
        },
        {
            "docid": "9186444_5",
            "document": "Visual modularity . Another clinical case that would a priori suggest a module for modularity in visual processing is visual agnosia. The well studied patient DF is unable to recognize or discriminate objects owing to damage in areas of the lateral occipital cortex although she can see scenes without problem \u2013 she can literally see the forest but not the trees. Neuroimaging of intact individuals reveals strong occipito-temporal activation during object presentation and greater activation still for object recognition. Of course, such activation could be due to other processes, such as visual attention. However, other evidence that shows a tight coupling of perceptual and physiological changes suggests activation in this area does underpin object recognition. Within these regions are more specialized areas for face or fine grained analysis, place perception and human body perception. Perhaps some of the strongest evidence for the modular nature of these processing systems is the double dissociation between object- and face (prosop-) agnosia. However, as with color and motion, early areas (see for a comprehensive review) are implicated too, lending support to the idea of a multistage stream terminating in the inferotemporal cortex rather than an isolated module.",
            "score": 166.28624534606934
        },
        {
            "docid": "24978422_5",
            "document": "Visual adaptation . Perceptual aftereffects for face recognition occur for several different stimuli, including gender, ethnicity, identity, emotion, and attractiveness of a face. The fact that this distinction occurs, implies that face recognition is a process that happens on a higher level and later on in the visual encoding, rather than early on within visual adaptation. The fact that the aftereffects in face recognition in particular are so strong, suggests that it is for the purpose of regulation of how processes work. This provides a sense of constancy in an individual's perception, while adapting to differences and possible versions of a stimulus allows for constancy and stability, and makes it easier to adapt to variations in a stimulus, while recognizing commonalities. These face perception cues are encoded in an individual's brain for extended periods of time, ensuring consistency over the individual's lifespan. A young person would perceive stimuli the same way as an older individual.",
            "score": 132.00655555725098
        },
        {
            "docid": "386062_12",
            "document": "Wishful thinking . Some speculate that wishful seeing results from cognitive penetrability in that higher cognitive functions are able to directly influence perceptual experience instead of only influencing perception at higher levels of processing. Those that argue against cognitive penetrability feel that sensory systems operate in a modular fashion with cognitive states exerting their influence only after the stimuli has been perceived. The phenomenon of wishful seeing implicates cognitive penetrability in the perceptual experience. Wishful seeing has been observed to occur in early stages of categorization. Research using ambiguous figures and binocular rivalry exhibit this tendency. Perception is influenced by both top-down and bottom-up processing. In visual processing, bottom-up processing is a rigid route compared to flexible top-down processing. Within bottom-up processing, the stimuli are recognized by fixation points, proximity and focal areas to build objects, while top-down processing is more context sensitive. This effect can be observed via priming as well as with emotional states. The traditional hierarchical models of information processing describe early visual processing as a one-way street: early visual processing goes into conceptual systems, but conceptual systems do not affect visual processes. Currently, research rejects this model and suggests conceptual information can penetrate early visual processing rather than just biasing the perceptual systems. This occurrence is called conceptual or cognitive penetrability. Research on conceptual penetrability utilize stimuli of conceptual-category pairs and measure the reaction time to determine if the category effect influenced visual processing, The category effect is the difference in reaction times within the pairs such as \"Bb\" to \"Bp\". To test conceptual penetrability, there were simultaneous and sequential judgments of pairs. The reaction times decreased as the stimulus onset asynchrony increased, supporting categories affect visual representations and conceptual penetrability. Research with richer stimuli such as figures of cats and dogs allow for greater perceptual variability and analysis of stimulus typicality (cats and dogs were arranged in various positions, some more or less typical for recognition). Differentiating the pictures took longer when they were within the same category (dog-dog) compared between categories (dog-cat) supporting category knowledge influences categorization. Therefore, visual processing measured by physical differential judgments is affected by non-visual processing supporting conceptual penetrability.",
            "score": 119.56508600711823
        },
        {
            "docid": "599917_9",
            "document": "Mental image . The biological foundation of the mind's eye is not fully understood. Studies using fMRI have shown that the lateral geniculate nucleus and the V1 area of the visual cortex are activated during mental imagery tasks. Ratey writes: The visual pathway is not a one-way street. Higher areas of the brain can also send visual input back to neurons in lower areas of the visual cortex. [...] As humans, we have the ability to see with the mind's eye \u2013 to have a perceptual experience in the absence of visual input. For example, PET scans have shown that when subjects, seated in a room, imagine they are at their front door starting to walk either to the left or right, activation begins in the visual association cortex, the parietal cortex, and the prefrontal cortex - all higher cognitive processing centers of the brain.",
            "score": 242.4113246202469
        },
        {
            "docid": "2363287_6",
            "document": "Visual learning . Various areas of the brain work together in a multitude of ways in order to produce the images that we see with our eyes and that are encoded by our brains. The basis of this work takes place in the visual cortex of the brain. The visual cortex is located in the occipital lobe of the brain and harbors many other structures that aid in visual recognition, categorization, and learning. One of the first things the brain must do when acquiring new visual information is recognize the incoming material. Brain areas involved in recognition are the inferior temporal cortex, the superior parietal cortex, and the cerebellum. During tasks of recognition, there is increased activation in the left inferior temporal cortex and decreased activation in the right superior parietal cortex. Recognition is aided by neural plasticity, or the brain's ability to reshape itself based on new information. Next the brain must categorize the material. The three main areas that are used when categorizing new visual information are the orbitofrontal cortex and two dorsolateral prefrontal regions which begin the process of sorting new information into groups and further assimilating that information into things that you might already know. After recognizing and categorizing new material entered into the visual field, the brain is ready to begin the encoding process \u2013 the process which leads to learning. Multiple brain areas are involved in this process such as the frontal lobe, the right extrastriate cortex, the neocortex, and again, the neostriatum. One area in particular, the limbic-diencephalic region, is essential for transforming perceptions into memories. With the coming together of tasks of recognition, categorization and learning; schemas help make the process of encoding new information and relating it to things you already know much easier. One can remember visual images much better when they can apply it to an already known schema. Schemas actually provide enhancement of visual memory and learning.",
            "score": 232.25996816158295
        },
        {
            "docid": "34042719_5",
            "document": "Visual processing abnormalities in schizophrenia . Motion perception is an important visual function and occurs from the earliest stages of cortical visual processing, with individual neurons being tuned to a preferred direction of motion. The cortical area MT (medial temporal cortex, also known as V5) plays a significant role in motion processing, and deactivation of this region using Transcranial magnetic stimulation can affect perception of motion. Subjects with schizophrenia have shown abnormalities in perceptual judgments of motion, speed and direction, with deficits in these judgments generally being reported. It has been suggested that these findings are related to the aforementioned magnocellular deficit purported to exist in this disorder. Inhibition of motion perception by the addition of a surround stimulus has also been examined in schizophrenia, with one group finding evidence both of impaired motion perception and weaker perceptual suppression effects in schizophrenia. This agrees with the findings mentioned previously related to weaker suppression of perceived contrast in this disorder. However, another recent report has disputed this finding, instead showing evidence consistent with stronger surround influence on motion perception in schizophrenia.",
            "score": 90.94175493717194
        },
        {
            "docid": "23483_5",
            "document": "Philosophy of perception . An object at some distance from an observer will reflect light in all directions, some of which will fall upon the corneae of the eyes, where it will be focussed upon each retina, forming an image. The disparity between the electrical output of these two slightly different images is resolved either at the level of the lateral geniculate nucleus or in a part of the visual cortex called 'V1'. The resolved data is further processed in the visual cortex where some areas have specialised functions, for instance area V5 is involved in the modelling of motion and V4 in adding colour. The resulting single image that subjects report as their experience is called a 'percept'. Studies involving rapidly changing scenes show the percept derives from numerous processes that involve time delays. Recent fMRI studies show that dreams, imaginings and perceptions of things such as faces are accompanied by activity in many of the same areas of brain as are involved with physical sight. Imagery that originates from the senses and internally generated imagery may have a shared ontology at higher levels of cortical processing.",
            "score": 203.54211723804474
        },
        {
            "docid": "4236583_34",
            "document": "Visual search . Studies have consistently shown that autistic individuals performed better and with lower reaction times in feature and conjunctive visual search tasks than matched controls without autism. Several explanations for these observations have been suggested. One possibility is that people with autism have enhanced perceptual capacity. This means that autistic individuals are able to process larger amounts of perceptual information, allowing for superior parallel processing and hence faster target location. Second, autistic individuals show superior performance in discrimination tasks between similar stimuli and therefore may have an enhanced ability to differentiate between items in the visual search display. A third suggestion is that autistic individuals may have stronger top-down target excitation processing and stronger distractor inhibition processing than controls. Keehn et al. (2008) used an event-related functional magnetic resonance imaging design to study the neurofunctional correlates of visual search in autistic children and matched controls of typically developing children. Autistic children showed superior search efficiency and increased neural activation patterns in the frontal, parietal, and occipital lobes when compared to the typically developing children. Thus, autistic individuals' superior performance on visual search tasks may be due to enhanced discrimination of items on the display, which is associated with occipital activity, and increased top-down shifts of visual attention, which is associated with the frontal and parietal areas.",
            "score": 118.20810854434967
        },
        {
            "docid": "35982062_3",
            "document": "Biased Competition Theory . Research into the subject of attentional mechanisms in regard to visual perception was undertaken as an attempt to better understand the functional principles and potential constraints surrounding visual perception Visual search tasks are commonly used by experimenters to aid the exploration of visual perception. The classical view of visual attention suggests that there are two basic principles: the pre-attentive stage and the attentive stage. In the pre-attentive stage, an individual has an unlimited capacity for perception which is capable of processing information from the entire visual field concurrently. During the attentive stage, the processing of visual information corresponding to local spatial areas takes place. This classical view of visual attention suggests that there is no competition within the visual field. Within this theory an individual is assumed to be capable of processing all information provided concurrently. Until recently it was still thought that individuals had a pre-attentive stage. This is no longer the case, research has now suggested that the pre-attentive stage is now limited in its capacity.  The attentive stage of being able to process important information has now transformed into what is known as selectivity. The classical view of attention has built the ground work for the recent emergence of two new principles to benefit the understanding of visual attention. The first of these relates to the limited capacity of information processing. This suggests that at any given time, only a small amount of information can actually be retained and used to control behaviour. The principle of selectivity incorporates the notion that a person has the ability to filter out unwanted information. Koch and Ullman proposed that attentive selection could be implemented by competitive \"winner-takes-all\" networks. Robert Desimone and John Duncan expanded on this idea. They proposed that at some point between the visual input of objects and the response to objects in the visual field there is some competition occurring; competition for representation, analysis, and behavior. This suggests that attention to stimuli makes more demands on processing capacity than unattended stimuli. This idea of competition led researchers to develop a new theory of attention, which they termed the \u201cbiased competition theory\". The theory attempts to provide an explanation of the processes leading visual attention and their effects on the brain\u2019s neural systems.",
            "score": 144.8307671546936
        },
        {
            "docid": "176997_6",
            "document": "Blindsight . Blindsight patients show awareness of single visual features, such as edges and motion, but cannot gain a holistic visual percept. This suggests that perceptual awareness is modular and that\u2014in sighted individuals\u2014there is a \"binding process that unifies all information into a whole percept\", which is interrupted in patients with such conditions as blindsight and visual agnosia. Therefore, object identification and object recognition are thought to be separate processes and occur in different areas of the brain, working independently from one another. The modular theory of object perception and integration would account for the \"hidden perception\" experienced in blindsight patients. Research has shown that visual stimuli with the single visual features of sharp borders, sharp onset/offset times, motion, and low spatial frequency contribute to, but are not strictly necessary for, an object's salience in blindsight.",
            "score": 154.44038140773773
        },
        {
            "docid": "50821295_13",
            "document": "Fusiform body area . In the 2013 article \"Reduced Connectivity Between the Left Fusiform Body Area and the Extrastriate Body Area in Anorexia Nervosa is Associated with Body Image Distortion\" Suchan and colleagues determined hat the perceptual distortion associated with AN can in part be explained by the reduced connectivity between the EBA and the FBA; both brain regions involved in visual body processing. Results from this study strongly suggest that EBA activation modulates FBA activation, and the authors indicate that visual information converge at the FBA for processing and due to a lack of simultaneous processing due to reduced connectivity a perceptual distortion occurs. The authors note that further research into the significance of this connectivity is needed.",
            "score": 132.12918400764465
        },
        {
            "docid": "24766693_7",
            "document": "Glob (visual system) . Three types of retinal cone create signals that get transformed in the visual pathway to create the perception of color. However the neurons processing them in the retina, lateral geniculate nucleus, and V1 and V2 early parts of the visual cortex encode using the opponent process only a limited range of colors that does not reflect the dimensions of perceptual color space. It is only the next area where globs are found that along the visual processing hierarchy, show hue sensitivity, with the population of neurons representing most (if not all) of perceptual color space and which the color responses of neurons correspond to perception.",
            "score": 89.28414511680603
        },
        {
            "docid": "9186444_7",
            "document": "Visual modularity . Much of the confusion concerning modularity exists in neuroscience because there is evidence for specific areas (e.g. V4 or V5/hMT+) and the concomitant behavioral deficits following brain insult (thus taken as evidence for modularity). In addition, evidence shows other areas are involved and that these areas subserve processing of multiple properties (e.g. V1) (thus taken as evidence against modularity). That these streams have the same implementation in early visual areas, like V1, is not inconsistent with a modular viewpoint: to adopt the canonical analogy in cognition, it is possible for different software to run on the same hardware. A consideration of psychophysics and neuropsychological data would suggest support for this. For example, psychophysics has shown that percepts for different properties are realized asynchronously. In addition, although achromats experience other cognitive defects they do not have motion deficits when their lesion is restricted to V4, or total loss of form perception. Relatedly, Zihl and colleagues' akinetopsia patient shows no deficit to color or object perception (although deriving depth and structure from motion is problematic, see above) and object agnostics do not have damaged motion or color perception, making the three disorders triply dissociable. Taken together this evidence suggests that even though distinct properties may employ the same early visual areas they are functionally independent. Furthermore, that the intensity of subjective perceptual experience (e.g. color) correlates with activity in these specific areas (e.g. V4), the recent evidence that synaesthetes show V4 activation during the perceptual experience of color, as well as the fact that damage to these areas results in concomitant behavioral deficits (the processing may be occurring but perceivers do not have access to the information) are all evidence for visual modularity.",
            "score": 164.70220625400543
        },
        {
            "docid": "6989876_2",
            "document": "Vision for perception and vision for action . Vision for perception and vision for action in neuroscience literature refers to two types of visual processing in the brain: visual processing to obtain information about the features of objects such as color, size, shape (vision for perception) versus processing needed to guide movements such as catching a baseball (vision for action). An idea is currently debated that these types of processing are done by anatomically different brain networks. Ventral visual stream subserves vision for perception, whereas dorsal visual stream subserves vision for action. This idea finds support in clinical research and animal experiments.",
            "score": 146.8756000995636
        },
        {
            "docid": "1038052_32",
            "document": "Neuroesthetics . Different artistic styles may also be processed differently by the brain. In a study between filtered forms of abstract and representation art, the bilateral occipital gyri, left cingulate sulcus, and bilateral fusiform gyrus showed increased activation with increased preference when viewing art. However, activation in the bilateral occipital gyri may be caused by the large processing requirements placed on the visual system when viewing high levels of visual detail in artwork such as representational paintings. Several areas of the brain have been shown to respond particularly to forms representational art perhaps due to the brain's ability to make object associations and other functions relating to attention and memory. This form of stimuli leads to increased activation in the left frontal lobe and bilaterally in the parietal and limbic lobes. Also, the left superior parietal lobule, Brodmann's area 7, has been shown to play a role in active image construction during the viewing of art specifically containing indeterminate forms such as soft edge paintings. Bottom up processes such as edge detection and the exploration of visual stimuli are engaged during this type of aesthetic perception. These roles are consistent with previously known parietal lobe responsibilities in spatial cognition and visual imagery.",
            "score": 196.74679374694824
        },
        {
            "docid": "32197396_4",
            "document": "Form perception . In addition to photoreceptors, the eye requires a properly functioning lens, retina, and an undamaged optic nerve to recognize form. Light travels through the lens, hits the retina, activates the appropriate photoreceptors, depending on available light, which convert the light into an electrical signal that travels along the optic nerve to the lateral geniculate nucleus of the thalamus and then to the primary visual cortex. In the cortex, the adult brain processes information such as lines, orientation, and color. These inputs are integrated in the occipito-temporal cortex where a representation of the object as a whole is created. Visual information continues to be processed in the posterior parietal cortex, also known as the dorsal stream, where the representation of an object\u2019s shape is formed using motion-based cues. It is believed that simultaneously information is processed in the anterior temporal cortex, also known as the ventral stream, where object recognition, identification and naming occur. In the process of recognizing an object, both the dorsal and ventral streams are active, but the ventral stream is more important in discriminating between and recognizing objects. The dorsal stream contributes to object recognition only when two objects have similar shapes and the images are degraded. Observed latency in activation of different parts of the brain supports the idea of hierarchal processing of visual stimuli, with object representations progressing from simple to complex.",
            "score": 213.7469173669815
        },
        {
            "docid": "34045015_18",
            "document": "History of pain theory . The use of fMRI to study brain activity confirms the link between visual perception and pain perception. It has been found that the brain regions that convey the perception of pain are the same regions that encode the size of visual inputs. One specific area, the magnitude-related insula of the insular cortex, functions to perceive the size of a visual stimulation and integrate the concept of that size across various sensory systems, including the perception of pain. This area also overlaps with the nociceptive-specific insula, part of the insula that selectively processes nociception, leading to the conclusion that there is an interaction and interface between the two areas. This interaction tells the individual how much relative pain they are experiencing, leading to the subjective perception of pain based on the current visual stimulus.",
            "score": 169.31701493263245
        },
        {
            "docid": "6989876_6",
            "document": "Vision for perception and vision for action . However, while there exists to be two different hypotheses regarding the processing of vision in the human brain, it is still possible to accept both. Recent experiments prove that difficulties arise when deciphering between vision for action and vision for perception. A clear distinction between the two is difficult to make. Studies prove visual illusions that involve perception more so have considerable results on action. This can clearly rule out the first hypothesis noted above, indicating the thought that visually directed actions always avoid the matter of perception. However, a weaker form of the first hypothesis can still be considered. This states that the content of conscious perception will sometimes influence action, but that its impact on action is less asserted. Both the assumed ventral and dorsal streams can provide guidance of action, however information processed ventrally appears less pronounced and appears more substantial in the processing of perceptual tasks. It has been noted that one can still accept the two-stream hypothesis, but in doing so one must also realize that such a hypothesis still acknowledges the sharing of visual information across pathways and functions, heavily shaped by behavioral tasks.",
            "score": 101.14953744411469
        },
        {
            "docid": "734667_6",
            "document": "Iconic memory . Underlying visible persistence is neural persistence of the visual sensory pathway. A prolonged visual representation begins with activation of photoreceptors in the retina. Although activation in both rods and cones has been found to persist beyond the physical offset of a stimulus, the rod system persists longer than cones. Other cells involved in a sustained visible image include M and P retinal ganglion cells. M cells (transient cells), are active only during stimulus onset and stimulus offset. P cells (sustained cells), show continuous activity during stimulus onset, duration, and offset. Cortical persistence of the visual image has been found in the primary visual cortex (V1) in the occipital lobe which is responsible for processing visual information.",
            "score": 94.43056833744049
        },
        {
            "docid": "32197396_7",
            "document": "Form perception . Dysfunctions in form perception occur in several areas that involve visual processing, which is how visual information is interpreted. These dysfunctions have nothing to do with actual vision but rather affect how the brain understands what the eye sees. Problems can occur in the areas of visual closure, visual-spatial relationships, visual memory, and visual tracking. After identifying the specific visual problem that exists, intervention can include eye exercises, work with computer programs, neurotherapy, physical activities, and academic adjustments.",
            "score": 189.36068654060364
        },
        {
            "docid": "2315029_13",
            "document": "Neural adaptation . Adaptation is considered to be the cause of perceptual phenomena like afterimages and the motion aftereffect. In the absence of fixational eye movements, visual perception may fade out or disappear due to neural adaptation. (See Adaptation (eye)). When an observer's visual stream adapts to a single direction of real motion, imagined motion can be perceived at various speeds. If the imagined motion is in the same direction as that experienced during adaptation, imagined speed is slowed; when imagined motion is in the opposite direction, its speed is increased; when adaptation and imagined motions are orthogonal, imagined speed is unaffected. Studies using magnetoencephalography (MEG) have demonstrated that subjects exposed to a repeated visual stimulus at brief intervals become attenuated to the stimulus in comparison to the initial stimulus. The results revealed that visual responses to the repeated compared with novel stimulus showed a significant reduction in both activation strength and peak latency but not in the duration of neural processing.",
            "score": 148.38378190994263
        },
        {
            "docid": "11068276_11",
            "document": "Vividness of Visual Imagery Questionnaire . Lee, Kravitz and Baker (2012) used fMRI and multi-voxel pattern analysis to investigate the specificity, distribution, and similarity of information for individual seen and imagined objects. Participants either viewed or imagined individual named object images on which they had been trained prior to the scan. Lee et al. found that the identity of both seen and imagined objects could be decoded from the pattern of activity throughout the ventral visual processing stream. Further, there was enough correspondence between imagery and perception to allow discrimination of individual imagined objects based on the response during perception.",
            "score": 165.43176102638245
        },
        {
            "docid": "525667_10",
            "document": "Human echolocation . In a 2014 study by Thaler and colleagues, the researchers first made recordings of the clicks and their very faint echoes using tiny microphones placed in the ears of the blind echolocators as they stood outside and tried to identify different objects such as a car, a flag pole, and a tree. The researchers then played the recorded sounds back to the echolocators while their brain activity was being measured using functional magnetic resonance imaging. Remarkably, when the echolocation recordings were played back to the blind experts, not only did they perceive the objects based on the echoes, but they also showed activity in those areas of their brain that normally process visual information in sighted people, primarily primary visual cortex or V1. This result is surprising, as visual areas, as their names suggest, are only active during visual tasks. The brain areas that process auditory information were no more activated by sound recordings of outdoor scenes containing echoes than they were by sound recordings of outdoor scenes with the echoes removed. Importantly, when the same experiment was carried out with sighted people who did not echolocate, these individuals could not perceive the objects and there was no echo-related activity anywhere in the brain. This suggests that the cortex of blind echolocators is plastic and reorganizes such that primary visual cortex, rather than any auditory area, becomes involved in the computation of echolocation tasks.",
            "score": 236.78436732292175
        },
        {
            "docid": "36086848_17",
            "document": "Fear processing in the brain . The perception of fear is elicited by many different stimuli and involves the process described above in biochemical terms. Neural correlates of the interaction between language and visual information has been studied by Roel Willems \"et al\". The study consisted of observing how visual and linguistic information interact in the perception of emotion. A common phenomenon from film theory was borrowed which states that the presentation of a neutral visual scene intensifies the percept of fear or suspense induced by a different channel of information, such as language. This principle has been applied in a way in which the percept of fear was present and amplified in the presence of a neutral visual stimuli. The main idea is that the visual stimuli intensify the fearful content of the stimuli (i.e. language) by subtly implying and concretizing what is described in the context (i.e. sentence). Activation levels in the right anterior temporal pole were selectively increased and is believed to serve as a binding function of emotional information across domains such as visual and linguistic information.",
            "score": 125.66224503517151
        },
        {
            "docid": "3883287_8",
            "document": "Tranquillity . Within tranquillity studies, much of the emphasis has been placed on understanding the role of vision in the perception of natural environments, which is probably not surprising, considering that upon first viewing a scene its configurational coherence can be established with incredible speed. Indeed, scene information can be captured in a single glance and the gist of a scene determined in as little as 100ms. The speed of processing of complex natural images was tested by Thorpe \"et al.\" using colour photographs of a wide range of animals (mammals, birds, reptiles and fish), in their natural environments, mixed with distracters that included pictures of forests, mountains, lakes, buildings and fruit. During this experiment, subjects were shown an image for 20ms and asked to determine whether it contained an animal or not. The electrophysiological brain responses obtained in this study showed that a decision could be made within 150ms of the image being seen, indicating the speed at which cognitive visual processing occurs. However, audition, and in particular the individual components that collectively comprise the soundscape, a term coined by Schafer to describe the ever-present array of sounds that constitute the sonic environment, also significantly inform the various schemata used to characterise differing landscape types. This interpretation is supported by the auditory reaction times, which are 50 to 60ms faster than that of the visual modality. It is also known that sound can alter visual perception and that under certain conditions areas of the brain involved in processing auditory information can be activated in response to visual stimuli.  Research conducted by Pheasant \"et al.\" has shown that when individuals make tranquillity assessments based on a uni-modal auditory or visual sensory input, they characterise the environment by drawing upon a number of key landscape and soundscape characteristics. For example, when making assessments in response to visual-only stimuli the percentage of water, flora and geological features present within a scene, positively influence how tranquil a location is perceived to be. Likewise when responding to uni-modal auditory stimuli, the perceived loudness of biological sounds positively influences the perception of tranquillity, whilst the perceived loudness of mechanical sounds have a negative effect. However, when presented with bi-modal auditory-visual stimuli the individual soundscape and landscape components alone no longer influenced the perception of tranquillity. Rather configurational coherence was provided by the percentage of natural and contextual features present within the scene and the equivalent continuous sound pressure level (LAeq).",
            "score": 163.82612335681915
        },
        {
            "docid": "33932515_25",
            "document": "Social cue . Higher level visual regions, such as the fusiform gyrus, extrastriate cortex and superior temporal sulcus (STS) are the areas of the brain that studies have found that link to perceptual processing of social/biological stimuli. Data collected from behavioral studies have found that the right hemisphere is highly connected with the processing of left visual field advantage for face and gaze stimuli. Researchers believe the right STS is also involved in using gaze to understand the intentions of others. While looking at social and nonsocial cues, it has been found that a high level of activity has been found in the bilateral extrastriate cortices in regards to gaze cues versus peripheral cues. There was a study done on two people with split-brain, in order to study each hemisphere to see what their involvement is in gaze cuing. Results suggest that gaze cues show a strong effect with the facial recognition hemisphere of the brain, compared to nonsocial cues. The results of Greene and Zaidel's study suggest that in relation to visual fields, information is processed independently and that the right hemisphere shows greater orienting.",
            "score": 155.93289959430695
        },
        {
            "docid": "15670228_4",
            "document": "Integrative agnosia . Cases with integrative agnosia appear to have medial ventral lesions in the extrastriate cortex. Those who have integrative agnosia are better able to identify inanimate than animate items, which indicates processes that lead to accurate perceptual organization of visual information can be impaired. This is attributed to the importance of perceptual updating of stored visual knowledge, which is particularly important for classes of stimuli that have many perceptual neighbors and/or stimuli for which perceptual features are central to their stored representations. Patients also show a tendency to process visual stimuli initially at a global rather than local level. Although the grouping of local elements into perceptual wholes can be impaired, patients can remain sensitive to holistic visual representations.",
            "score": 65.51878356933594
        },
        {
            "docid": "53472_6",
            "document": "Illusion . An optical illusion is characterised by visually perceived images that are deceptive or misleading. Therefore, the information gathered by the eye is processed by the brain to give, on the face of it, a percept that does not tally with a physical measurement of the stimulus source. A conventional assumption is that there are physiological illusions that occur naturally and cognitive illusions that can be demonstrated by specific visual tricks that say something more basic about how human perceptual systems work. The human brain constructs a world inside our head based on what it samples from the surrounding environment. However, sometimes it tries to organise this information it thinks best while other times it fills in the gaps. This way in which our brain works is the basis of an illusion.",
            "score": 154.17095708847046
        }
    ],
    "r": [
        {
            "docid": "599917_31",
            "document": "Mental image . As cognitive neuroscience approaches to mental imagery continued, research expanded beyond questions of serial versus parallel or topographic processing to questions of the relationship between mental images and perceptual representations. Both brain imaging (fMRI and ERP) and studies of neuropsychological patients have been used to test the hypothesis that a mental image is the reactivation, from memory, of brain representations normally activated during the perception of an external stimulus. In other words, if perceiving an apple activates contour and location and shape and color representations in the brain\u2019s visual system, then imagining an apple activates some or all of these same representations using information stored in memory. Early evidence for this idea came from neuropsychology. Patients with brain damage that impairs perception in specific ways, for example by damaging shape or color representations, seem to generally to have impaired mental imagery in similar ways. Studies of brain function in normal human brains support this same conclusion, showing activity in the brain\u2019s visual areas while subjects imagined visual objects and scenes.",
            "score": 245.4939727783203
        },
        {
            "docid": "599917_9",
            "document": "Mental image . The biological foundation of the mind's eye is not fully understood. Studies using fMRI have shown that the lateral geniculate nucleus and the V1 area of the visual cortex are activated during mental imagery tasks. Ratey writes: The visual pathway is not a one-way street. Higher areas of the brain can also send visual input back to neurons in lower areas of the visual cortex. [...] As humans, we have the ability to see with the mind's eye \u2013 to have a perceptual experience in the absence of visual input. For example, PET scans have shown that when subjects, seated in a room, imagine they are at their front door starting to walk either to the left or right, activation begins in the visual association cortex, the parietal cortex, and the prefrontal cortex - all higher cognitive processing centers of the brain.",
            "score": 242.4113311767578
        },
        {
            "docid": "613052_12",
            "document": "Direct and indirect realism . Direct realists can potentially deny the existence of any such thing as a mental image but this is difficult to maintain, since we seem able to visually imagine all sorts of things with ease. Even if perception does not involve images other mental processes like imagination certainly seem to. One view, similar to Reid's, is that we do have images of various sorts in our minds when we perceive, dream, hallucinate and imagine but when we actually perceive things, our sensations cannot be considered objects of perception or attention. The only objects of perception are external objects. Even if perception is accompanied by images, or sensations, it is wrong to say we perceive sensations. Direct realism defines perception as perception of external objects where an \"external object\" is allowed to be a photon in the eye but not an impulse in a nerve leading from the eye. Recent work in neuroscience suggests a shared ontology for perception, imagination and dreaming, with similar areas of brain being used for all of these.",
            "score": 238.234619140625
        },
        {
            "docid": "525667_10",
            "document": "Human echolocation . In a 2014 study by Thaler and colleagues, the researchers first made recordings of the clicks and their very faint echoes using tiny microphones placed in the ears of the blind echolocators as they stood outside and tried to identify different objects such as a car, a flag pole, and a tree. The researchers then played the recorded sounds back to the echolocators while their brain activity was being measured using functional magnetic resonance imaging. Remarkably, when the echolocation recordings were played back to the blind experts, not only did they perceive the objects based on the echoes, but they also showed activity in those areas of their brain that normally process visual information in sighted people, primarily primary visual cortex or V1. This result is surprising, as visual areas, as their names suggest, are only active during visual tasks. The brain areas that process auditory information were no more activated by sound recordings of outdoor scenes containing echoes than they were by sound recordings of outdoor scenes with the echoes removed. Importantly, when the same experiment was carried out with sighted people who did not echolocate, these individuals could not perceive the objects and there was no echo-related activity anywhere in the brain. This suggests that the cortex of blind echolocators is plastic and reorganizes such that primary visual cortex, rather than any auditory area, becomes involved in the computation of echolocation tasks.",
            "score": 236.78436279296875
        },
        {
            "docid": "2363287_6",
            "document": "Visual learning . Various areas of the brain work together in a multitude of ways in order to produce the images that we see with our eyes and that are encoded by our brains. The basis of this work takes place in the visual cortex of the brain. The visual cortex is located in the occipital lobe of the brain and harbors many other structures that aid in visual recognition, categorization, and learning. One of the first things the brain must do when acquiring new visual information is recognize the incoming material. Brain areas involved in recognition are the inferior temporal cortex, the superior parietal cortex, and the cerebellum. During tasks of recognition, there is increased activation in the left inferior temporal cortex and decreased activation in the right superior parietal cortex. Recognition is aided by neural plasticity, or the brain's ability to reshape itself based on new information. Next the brain must categorize the material. The three main areas that are used when categorizing new visual information are the orbitofrontal cortex and two dorsolateral prefrontal regions which begin the process of sorting new information into groups and further assimilating that information into things that you might already know. After recognizing and categorizing new material entered into the visual field, the brain is ready to begin the encoding process \u2013 the process which leads to learning. Multiple brain areas are involved in this process such as the frontal lobe, the right extrastriate cortex, the neocortex, and again, the neostriatum. One area in particular, the limbic-diencephalic region, is essential for transforming perceptions into memories. With the coming together of tasks of recognition, categorization and learning; schemas help make the process of encoding new information and relating it to things you already know much easier. One can remember visual images much better when they can apply it to an already known schema. Schemas actually provide enhancement of visual memory and learning.",
            "score": 232.25997924804688
        },
        {
            "docid": "15559385_11",
            "document": "Tactile discrimination . When a person has become blind, in order to \u201csee\u201d the world, their other senses become heightened. An important sense for the blind is their sense of touch, which becomes more frequently used to help them perceive the world. People that are blind have displayed that their visual cortices become more responsive to auditory and tactile stimulation. Braille allows the blind to be able to use their sense of touch to feel the roughness, and distance of various patterns to be used as a form of language. Within the brain, the activation of the occipital cortex is functionally relevant for tactile braille reading, as well as the somatosensory cortex. These various parts of the brain function in their own way, in which they each contribute to the effectiveness of how braille is read by the blind. People that are blind also rely heavily on Tactile Gnosis, Spatial discrimination, Graphesthesia, and Two-point discrimination. Essentially, the occipital cortex allows one to effectively make judgements on the distance of braille patterns, which is related to spatial discrimination. Meanwhile, the somatosensory cortex allows one to effectively make judgements on the roughness of braille patterns, which is related to two-point discrimination. The various visual areas in the brain are very essential for a blind person to read braille, just as much as it is for a person that has sight. Essentially, whether one is blind or not, the perception of objects that involves tactile discrimination is not impaired if one cannot see. When comparing people that are blind to people that have sight, the amount of activity within the their somatosensory and visual areas of the brain do differ. The activity in the somatosensory and visual areas are not as high in tactile gnosis for people that are not blind, and are more-so active for more visual related stimuli that does not involve touch. Nonetheless, there is a difference in these various areas within the brain when comparing the blind to the sighted, which is that shape discrimination causes a difference in brain activity, as well as tactile gnosis. The visual cortices of blind individuals are active during various vision related tasks including tactile discrimination, and the function of the cortices resemble the activity of adults with sight.",
            "score": 222.3941192626953
        },
        {
            "docid": "1903855_7",
            "document": "Sensory substitution . In a regular visual system, the data collected by the retina is converted into an electrical stimulus in the optic nerve and relayed to the brain, which re-creates the image and perceives it. Because it is the brain that is responsible for the final perception, sensory substitution is possible. During sensory substitution an intact sensory modality relays information to the visual perception areas of the brain so that the person can perceive to see. With sensory substitution, information gained from one sensory modality can reach brain structures physiologically related to other sensory modalities. Touch-to-visual sensory substitution transfers information from touch receptors to the visual cortex for interpretation and perception. For example, through fMRI, we can determine which parts of the brain are activated during sensory perception. In blind persons, we can see that while they are only receiving tactile information, their visual cortex is also activated as they perceive to \"see\" objects. We can also have touch to touch sensory substitution where information from touch receptors of one region can be used to perceive touch in another region. For example, in one experiment by Bach-y-Rita, he was able to restore the touch perception in a patient who lost peripheral sensation from leprosy.",
            "score": 222.19686889648438
        },
        {
            "docid": "33702464_5",
            "document": "Extrastriate body area . The experiment had subjects view images of different objects, including faces (as a control group), body parts, animals, parts of the face and intimate objects. While viewing the images, the subjects were scanned with an fMRI to see what area of the brain was activated. Through the trials a compilation of the fMRI\u2019s was made. From this compilation image a specific region was determined to have increased activity when shown visual stimuli of body parts and even more activity when viewing whole bodies. There have been no studies involving brain damage to the EBA. Thus far, only scans of brain activity, as well as transcranial magnetic stimulation, have been used to study the EBA. To find the specific functions of the EBA, Comimo Urgesi, Giovanni Berlucchi and Salvatore M. Aglioti used repetitive transcranial magnetic stimulation (rTMS) to disrupt part of the brain, making the brain less responsive in the target area. The study used event-related rTMS to disrupt the EBA, resulting in inactivation of cortical areas. This inactivation caused a slower response time in discriminating body parts. The study used facial features and motorcycle parts as non human parts for control groups. The facial features and motorcycle body parts did not display any change in response time. The neural activity data shows the EBA handles some of the visual processing of human body and parts but is not related to the processing of the face or other objects.",
            "score": 221.8262481689453
        },
        {
            "docid": "51462681_3",
            "document": "Objective vision . This is the story of what's happening when you see a picture, even too fast, the brain's visual cortex recognizes what it sees immediately. The visual cortex has a critical job in processing and it's the most complex part of brain. The human brain is much more aware of how it solves complex problems such as playing chess or solving algebra equations, which is why computer programmers have had so much success building machines that emulate this type of activity. but when entities visionary system starts to convert the signals to image(actually the separated shapes and colors) to find a relation between brain's information and those images. The system actually is concentrating on the separable sections, this separation gives the brain a visionary system the excellence processing result, because with this method the system do not waste much time on processing non significant sections and signals. this operation in the Objective Vision project called objective processing and because the O.V. mission is around human visionary simulation, so the developer refers with Objective Vision.",
            "score": 217.86029052734375
        },
        {
            "docid": "32197396_4",
            "document": "Form perception . In addition to photoreceptors, the eye requires a properly functioning lens, retina, and an undamaged optic nerve to recognize form. Light travels through the lens, hits the retina, activates the appropriate photoreceptors, depending on available light, which convert the light into an electrical signal that travels along the optic nerve to the lateral geniculate nucleus of the thalamus and then to the primary visual cortex. In the cortex, the adult brain processes information such as lines, orientation, and color. These inputs are integrated in the occipito-temporal cortex where a representation of the object as a whole is created. Visual information continues to be processed in the posterior parietal cortex, also known as the dorsal stream, where the representation of an object\u2019s shape is formed using motion-based cues. It is believed that simultaneously information is processed in the anterior temporal cortex, also known as the ventral stream, where object recognition, identification and naming occur. In the process of recognizing an object, both the dorsal and ventral streams are active, but the ventral stream is more important in discriminating between and recognizing objects. The dorsal stream contributes to object recognition only when two objects have similar shapes and the images are degraded. Observed latency in activation of different parts of the brain supports the idea of hierarchal processing of visual stimuli, with object representations progressing from simple to complex.",
            "score": 213.74691772460938
        },
        {
            "docid": "1316947_4",
            "document": "Ambiguous image . When we see an image, the first thing we do is attempt to organize all the parts of the scene into different groups. To do this, one of the most basic methods used is finding the edges. Edges can include obvious perceptions such as the edge of a house, and can include other perceptions that the brain needs to process deeper, such as the edges of a person's facial features. When finding edges, the brain's visual system detects a point on the image with a sharp contrast of lighting. Being able to detect the location of the edge of an object aids in recognizing the object. In ambiguous images, detecting edges still seems natural to the person perceiving the image. However, the brain undergoes deeper processing to resolve the ambiguity. For example, consider an image that involves an opposite change in magnitude of luminance between the object and the background (e.g. From the top, the background shifts from black to white, and the object shifts from white to black). The opposing gradients will eventually come to a point where there is an equal degree of luminance of the object and the background. At this point, there is no edge to be perceived. To counter this, the visual system connects the image as a whole rather than a set of edges, allowing one to see an object rather than edges and non-edges. Although there is no complete image to be seen, the brain is able to accomplish this because of its understanding of the physical world and real incidents of ambiguous lighting. In ambiguous images, an illusion is often produced from illusory contours. An illusory contour is a perceived contour without the presence of a physical gradient. In examples where a white shape appears to occlude black objects on a white background, the white shape appears to be brighter than the background, and the edges of this shape produce the illusory contours. These illusory contours are processed by the brain in a similar way as real contours. The visual system accomplishes this by making inferences beyond the information that is presented in much the same way as the luminance gradient.",
            "score": 207.0416259765625
        },
        {
            "docid": "2094955_30",
            "document": "Salience (language) . Our minds and bodies are bombarded by relevant and irrelevant knowledge and experiences every day. We will tune into salient ones (crane the ears to more fully hear enjoyable music) and tune-out non-salient ones (cover our ears from jackhammer noise). There is difference between seeing something and looking at it. In seeing, the capacity of our retina to take in the light energy is engaged and the brain processes that information into an image. When one looks at an object, not only are visual perceptive capacities engaged, but other mental processes for evaluation and ordering of the object are activated (Skinner, 1974).",
            "score": 206.3536834716797
        },
        {
            "docid": "7800961_12",
            "document": "Fusiform face area . The FFA is underdeveloped in children and does not fully develop until adolescence. This calls into question the evolutionary purpose of the FFA, as children show the ability to differentiate faces. Two-year-old babies have been shown to prefer the face of their mother. Although the FFA is underdeveloped in two-year-old babies, they have the ability to recognize their mother. Babies as early as three months old have shown the ability to distinguish between faces. During this time, babies exhibit the ability to differentiate between genders, showing a clear preference for female faces. It is theorized that, in terms of evolution, babies focus on women for food, although the preference could simply reflect a bias for the caregivers they experience. Infants do not appear to use this area for the perception of faces. Recent fMRI work has found no face selective area in the brain of infants 4 to 6 months old. However, given that the adult human brain has been studied far more extensively than the infant brain, and that infants are still undergoing major neurodevelopmental processes, it may simply be that the FFA is not located in anatomically familiar area. It may also be that activation for many different percepts and cognitive tasks in infants is diffuse in terms of neural circuitry, as infants are still undergoing periods of neurogenesis and neural pruning; this may make it more difficult to distinguish the signal, or what we would imagine as visual and complex familiar objects (like faces), from the noise, including static firing rates of neurons, and activity that is dedicated to a different task entirely than the activity of face processing. Infant vision involves only light and dark recognition, recognizing only major features of the face, activating the amygdala. These findings question the evolutionary purpose of the FFA.",
            "score": 205.30055236816406
        },
        {
            "docid": "20395179_7",
            "document": "Vittorio Gallese . Observing the world is more complex than the mere activation of the visual brain. Vision is multimodal: it encompasses the activation of motor, somatosensory and emotion-related brain networks. Any intentional relation entertained with the external world has an intrinsic pragmatic nature, hence it always bears a motor content. The same motor circuits that control our motor behavior also map the space around us, the objects at hand in that very same space, thus defining and shaping in motor terms their representational content. The space around us is defined by the motor potentialities of our body. Motor neurons also respond to visual, tactile and auditory stimuli. Indeed, premotor neurons controlling the movements of the upper arm also respond to tactile stimuli applied to it, to visual stimuli moved within the arm's peripersonal space, or to auditory stimuli also coming from the same peri-personal space. The same applies to artifacts, like three-dimensional objects. The manipulable objects we look at are classified by the motor brain as potential targets of the interactions we might entertain with them. Premotor and parietal 'canonical neurons' control the grasping and manipulation of objects and also respond to their mere observation. The functional architecture of embodied simulation seems to constitute a basic characteristic of our brain, making possible our rich and diversified experiences of space, objects and other individuals, being at the basis of our capacity to empathize with them.\"",
            "score": 203.9285430908203
        },
        {
            "docid": "1061157_11",
            "document": "Dual-coding theory . Two different methods have been used to identify the regions involved in visual perception and visual imagery. First, functional magnetic resonance imaging (fMRI) is used to measure cerebral blood flow, which allows researchers to identify the amount of glucose and oxygen being consumed by a specific part of the brain, with an increase in blood flow providing a measure of brain activity. Second, an event related potential (ERP) can be used to show the amount of electrical brain activity that is occurring due to a particular stimulus. Researchers have used both methods to determine which areas of the brain are active with different stimuli, and results have supported the dual-coding theory. Other research has been done with positron emission tomography (PET) scans and fMRI to show that participants had improved memory for spoken words and sentences when paired with an image, imagined or real, and showed increased brain activation to process abstract words not easily paired with an image.",
            "score": 203.71749877929688
        },
        {
            "docid": "23483_5",
            "document": "Philosophy of perception . An object at some distance from an observer will reflect light in all directions, some of which will fall upon the corneae of the eyes, where it will be focussed upon each retina, forming an image. The disparity between the electrical output of these two slightly different images is resolved either at the level of the lateral geniculate nucleus or in a part of the visual cortex called 'V1'. The resolved data is further processed in the visual cortex where some areas have specialised functions, for instance area V5 is involved in the modelling of motion and V4 in adding colour. The resulting single image that subjects report as their experience is called a 'percept'. Studies involving rapidly changing scenes show the percept derives from numerous processes that involve time delays. Recent fMRI studies show that dreams, imaginings and perceptions of things such as faces are accompanied by activity in many of the same areas of brain as are involved with physical sight. Imagery that originates from the senses and internally generated imagery may have a shared ontology at higher levels of cortical processing.",
            "score": 203.5421142578125
        },
        {
            "docid": "31148473_12",
            "document": "Transsaccadic memory . This is an area within the visual cortex that has been found to play an important role in the target selection of saccades. In other words, this area is important for determining which objects our eyes shift to when they move. Studies have shown that there is a large amount of activation within the visual area V4 before the saccade even takes place. This occurs in the form of shrinking receptive fields. The receptive fields of these brain cells tend to shift towards the object that the eye is about to move towards, generally more so if the object is close to the original fixation point. This dynamic change in receptive fields is thought to enhance the perception and recognition of objects in a visual scene. Because the receptive fields become smaller around the targeted objects, attention within the visual scene is very focused on these objects. Increased attention to target objects within a visual scene help direct eye movements from one object to another. Understanding of the visual scene becomes more efficient because these attention shifts guide the eyes towards relevant objects as opposed to objects that may not be as important.",
            "score": 201.54086303710938
        },
        {
            "docid": "1894873_16",
            "document": "Eye movement . The visual system in the brain is too slow to process that information if the images are slipping across the retina at more than a few degrees per second. Thus, to be able to see while we are moving, the brain must compensate for the motion of the head by turning the eyes. Another specialisation of visual system in many vertebrate animals is the development of a small area of the retina with a very high visual acuity. This area is called the fovea, and covers about 2 degrees of visual angle in people. To get a clear view of the world, the brain must turn the eyes so that the image of the object of regard falls on the fovea. Eye movement is thus very important for visual perception, and any failure can lead to serious visual disabilities. To see a quick demonstration of this fact, try the following experiment: hold your hand up, about one foot (30\u00a0cm) in front of your nose. Keep your head still, and shake your hand from side to side, slowly at first, and then faster and faster. At first you will be able to see your fingers quite clearly. But as the frequency of shaking passes about 1 Hz, the fingers will become a blur. Now, keep your hand still, and shake your head (up and down or left and right). No matter how fast you shake your head, the image of your fingers remains clear. This demonstrates that the brain can move the eyes opposite to head motion much better than it can follow, or pursue, a hand movement. When your pursuit system fails to keep up with the moving hand, images slip on the retina and you see a blurred hand.",
            "score": 201.4376983642578
        },
        {
            "docid": "4087208_12",
            "document": "David Marks (psychologist) . Rodway, Gillies and Schepman (2006) found that high vividness participants were significantly more accurate at detecting salient changes to pictures compared to low vividness participants, replicating an earlier study by Gur and Hilgard (1975). Recently Cui et al. (2007) found that reported image vividness correlates with increased activity in the visual cortex. This study shows that the subjective experience of forming a mental image is reflected by increased visual cortical activity. Logie, Pernet, Buonocore and Della Sala (2011) used behavioural and fMRI data for mental rotation from individuals reporting vivid and poor imagery on the VVIQ. Groups differed in brain activation patterns suggesting that the groups performed the same tasks in different ways. These findings help to explain the lack of association previously reported between VVIQ scores and mental rotation performance. Lee, Kravitz and Baker (2012) used fMRI and multi-voxel pattern analysis to investigate the specificity, distribution, and similarity of information for individual seen and imagined objects. Participants either viewed or imagined individual named object images on which they had been trained prior to the scan. Correlation between fMRI and VVIQ scores showed that, in both object-selective and early visual cortex, Lee et al.'s (2012) measure of discrimination across imagery and perception correlated with the vividness of imagery.",
            "score": 200.80775451660156
        },
        {
            "docid": "33246145_4",
            "document": "Neural decoding . When looking at a picture, people's brains are constantly making decisions about what object they are looking at, where they need to move their eyes next, and what they find to be the most salient aspects of the input stimulus. As these images hit the back of the retina, these stimuli are converted from varying wavelengths to a series of neural spikes called action potentials. These pattern of action potentials are different for different objects and different colors; we therefore say that the neurons are encoding objects and colors by varying their spike rates or temporal pattern. Now, if someone were to probe the brain by placing electrodes in the primary visual cortex, they may find what appears to be random electrical activity. These neurons are actually firing in response to the lower level features of visual input, possibly the edges of a picture frame. This highlights the crux of the neural decoding hypothesis: that it is possible to reconstruct a stimulus from the response of the ensemble of neurons that represent it. In other words, it is possible to look at spike train data and say that the person or animal being recorded is looking at a red ball.",
            "score": 200.7946319580078
        },
        {
            "docid": "5212945_3",
            "document": "Visual neuroscience . A recent study using Event-Related Potentials (ERPs) linked an increased neural activity in the occipito-temporal region of the brain to the visual categorization of facial expressions. Results focus on a negative peak in the ERP that occurs 170 milliseconds after the stimulus onset. This action potential, called the N170, was measured using electrodes in the occipito-temporal region, an area already known to be changed by face stimuli. Studying by using the EEG, and ERP methods allow for an extremely high temporal resolution of 4 milliseconds, which makes these kinds of experiments extremely well suited for accurately estimating and comparing the time it takes the brain to perform a certain function. Scientists used classification image techniques, to determine what parts of complex visual stimuli (such as a face) will be relied on when patients are asked to assign them to a category, or emotion. They computed the important features when the stimulus face exhibited one of five different emotions. Stimulus faces exhibiting fear had the distinguishing feature of widening eyes, and stimuli exhibiting happiness exhibited a change in the mouth to make a smile. Regardless of the expression of the stimuli's face, the region near the eyes affected the EEG before the regions near the mouth. This revealed a sequential, and predetermined order to the perception and processing of faces, with the eye being the first, and the mouth, and nose being processed after. This process of downward integration only occurred when the inferior facial features were crucial to the categorization of the stimuli. This is best explained by comparing what happens when participants were shown a face exhibiting fear, versus happiness. The N170 peaked slightly earlier for the fear stimuli at about 175 milliseconds, meaning that it took a participants less time to recognize the facial expression. This is expected because only the eyes need to be processed to recognize the emotion. However, when processing a happy expression, where the mouth is crucial to categorization, downward integration must take place, and thus the N170 peak occurred later at around 185 milliseconds. Eventually visual neuroscience aims to completely explain how the visual system processes all changes in faces as well as objects. This will give a complete view to how the world is constantly visually perceived, and may provide insight into a link between perception and consciousness.",
            "score": 200.4592742919922
        },
        {
            "docid": "176997_5",
            "document": "Blindsight . Patients with blindsight have damage to the visual system that allows perception (the visual cortex of the brain and some of the nerve fibers that bring information to it from the eyes) rather than the system that controls eye movements. This phenomenon shows how, after the more complex visual system is damaged, people can use the latter visual system of their brains to guide hand movements towards an object even though they cannot see what they are reaching for. Hence, visual information can control behavior without producing a conscious sensation. This ability of those with blindsight to \"see\" objects that they are unconscious of suggests that consciousness is not a general property of all parts of the brain; yet it suggests that only certain parts of the brain play a special role in consciousness.",
            "score": 200.3014678955078
        },
        {
            "docid": "12510615_4",
            "document": "Disjunctive cognition . Neurobiological research has identified separate areas of the brain responsible for recognizing faces. In humans, identifying unfamiliar faces activates one region of the brain (the Fusiform face area) while recognizing familiar faces also activates another area of the brain (in the lateral midtemporal cortex). A similar division of function is found in macaque monkeys. Such findings indicate that the process of recognizing faces may be achieved by special parts of the brain that are different from the brain areas involved in analyzing the general visual features of things. Since the brain has separate systems for deciding what a person looks like and who the person is, this division of labor may be responsible not only for disjunctive cognitions, but also the phenomenon of transference. In psychoanalytic treatment, patients frequently experience transference, in which the psychoanalyst is perceived to be very much like someone from the patient's past. As in disjunctive cognitions of dreams, the patient may feel \"You look like Dr. X, but you feel like my mother.\" The separate areas of the brain involved in telling us what the person looks like and who the person is may give a neurobiological basis for transference, the phenomenon in which we know who a person is, yet we react emotionally to that person as if they are someone else.",
            "score": 199.1696014404297
        },
        {
            "docid": "1038052_32",
            "document": "Neuroesthetics . Different artistic styles may also be processed differently by the brain. In a study between filtered forms of abstract and representation art, the bilateral occipital gyri, left cingulate sulcus, and bilateral fusiform gyrus showed increased activation with increased preference when viewing art. However, activation in the bilateral occipital gyri may be caused by the large processing requirements placed on the visual system when viewing high levels of visual detail in artwork such as representational paintings. Several areas of the brain have been shown to respond particularly to forms representational art perhaps due to the brain's ability to make object associations and other functions relating to attention and memory. This form of stimuli leads to increased activation in the left frontal lobe and bilaterally in the parietal and limbic lobes. Also, the left superior parietal lobule, Brodmann's area 7, has been shown to play a role in active image construction during the viewing of art specifically containing indeterminate forms such as soft edge paintings. Bottom up processes such as edge detection and the exploration of visual stimuli are engaged during this type of aesthetic perception. These roles are consistent with previously known parietal lobe responsibilities in spatial cognition and visual imagery.",
            "score": 196.74679565429688
        },
        {
            "docid": "51547415_16",
            "document": "Interindividual differences in perception . A follow-up fMRI study by Gutchess, Welsh, Boduroglu, and Park (2006) confirmed the previous findings by using a rather complex stimulus, which consists of \"only the object\" pictures, \"object with background\" pictures and \"only the background\" pictures without the object. This particular study was done on East-Asian Americans and non-Asian Americans. Though the performance of both subject groups was equally good, the activity of the involved brain areas was significantly different. Non-Asian Americans had a higher activation in the object processing areas in the ventral visual cortex during the object recognition task whereas the East-Asian Americans exhibited higher activity in the left occipital and fusiform areas which are associated with perceptual analysis.",
            "score": 196.11936950683594
        },
        {
            "docid": "485309_16",
            "document": "Face perception . There are several parts of the brain that play a role in face perception. Rossion, Hanseeuw, and Dricot used BOLD fMRI mapping to identify activation in the brain when subjects viewed both cars and faces. The majority of BOLD fMRI studies use blood oxygen level dependent (BOLD) contrast to determine which areas of the brain are activated by various cognitive functions. They found that the occipital face area, located in the occipital lobe, the fusiform face area, the superior temporal sulcus, the amygdala, and the anterior/inferior cortex of the temporal lobe, all played roles in contrasting the faces from the cars, with the initial face perception beginning in the area and occipital face areas. This entire region links to form a network that acts to distinguish faces. The processing of faces in the brain is known as a \"sum of parts\" perception. However, the individual parts of the face must be processed first in order to put all of the pieces together. In early processing, the occipital face area contributes to face perception by recognizing the eyes, nose, and mouth as individual pieces. Furthermore, Arcurio, Gold, and James used BOLD fMRI mapping to determine the patterns of activation in the brain when parts of the face were presented in combination and when they were presented singly. The occipital face area is activated by the visual perception of single features of the face, for example, the nose and mouth, and preferred combination of two-eyes over other combinations. This research supports that the occipital face area recognizes the parts of the face at the early stages of recognition. On the contrary, the fusiform face area shows no preference for single features, because the fusiform face area is responsible for \"holistic/configural\" information, meaning that it puts all of the processed pieces of the face together in later processing. This theory is supported by the work of Gold et al. who found that regardless of the orientation of a face, subjects were impacted by the configuration of the individual facial features. Subjects were also impacted by the coding of the relationships between those features. This shows that processing is done by a summation of the parts in the later stages of recognition.",
            "score": 195.73614501953125
        },
        {
            "docid": "25225295_10",
            "document": "Consumer neuroscience . Much of consumer research is devoted to studying the effect of brand associations on consumer preferences and how they manifest into brand memories. Brand memories can be defined as \u201ceverything that exists in the minds of customers with respect to a brand (e.g. thoughts, feelings, experiences, images, perceptions, beliefs and attitudes)\u201d. Several studies have indicated there is not a designated area of the brain devoted to brand recognition. Studies have shown that different areas of the brain are activated when exposed to a brand as opposed to a person, and decisions regarding the evaluation of brands in different product categories activate the area of the brain responsible for semantic object processing rather than areas involved with the judgment of people. These two findings suggest that brands are not processed by the brain in the same manner as human personalities, indicating that personality theory cannot be used to explain brand preferences.",
            "score": 195.4635467529297
        },
        {
            "docid": "25146378_12",
            "document": "Functional specialization (brain) . One of the most well known examples of functional specialization is the fusiform face area (FFA). Justine Sergent was one of the first researchers that brought forth evidence towards the functional neuroanatomy of face processing. Using positron emission tomography (PET), Sergent found that there were different patterns of activation in response to the two different required tasks, face processing verses object processing. These results can be linked with her studies of brain-damaged patients with lesions in the occipital and temporal lobes. Patients revealed that there was an impairment of face processing but no difficulty recognizing everyday objects, a disorder also known as prosopagnosia. Later research by Nancy Kanwisher using functional magnetic resonance imaging (fMRI), found specifically that the region of the inferior temporal cortex, known as the fusiform gyrus, was significantly more active when subjects viewed, recognized and categorized faces in comparison to other regions of the brain. Lesion studies also supported this finding where patients were able to recognize objects but unable to recognize faces. This provided evidence towards domain specificity in the visual system, as Kanwisher acknowledges the Fusiform Face Area as a module in the brain, specifically the extrastriate cortex, that is specialized for face perception.",
            "score": 194.9737091064453
        },
        {
            "docid": "1764639_17",
            "document": "Levels-of-processing effect . Several brain imaging studies using positron emission tomography and functional magnetic resonance imaging techniques have shown that higher levels of processing correlate with more brain activity and activity in different parts of the brain than lower levels. For example, in a lexical analysis task, subjects showed activity in the left inferior prefrontal cortex only when identifying whether the word represented a living or nonliving object, and not when identifying whether or not the word contained an \"a\". Similarly, an auditory analysis task showed increased activation in the left inferior prefrontal cortex when subjects performed increasingly semantic word manipulations. Synaptic aspects of word recognition have been correlated with the left frontal operculum and the cortex lining the junction of the inferior frontal and inferior precentral sulcus. The self-reference effect also has neural correlates with a region of the medial prefrontal cortex, which was activated in an experiment where subjects analyzed the relevance of data to themselves. Specificity of processing is explained on a neurological basis by studies that show brain activity in the same location when a visual memory is encoded and retrieved, and lexical memory in a different location. Visual memory areas were mostly located within the bilateral extrastriate visual cortex.",
            "score": 194.26902770996094
        },
        {
            "docid": "37527148_7",
            "document": "Psychology of film . Cognitive neuroscience research demonstrates that some movies can exert considerable control over brain activity and eye movements. Studying the neuroscience of film is based on the hypothesis that some films, or film segments, lead viewers through a similar sequence of perceptual, emotional and cognitive states. Using fMRI brain imaging, researchers asked participants to watch 30 minutes of The Good, the Bad and the Ugly (1966) as they lay on their backs in the MRI scanner. Despite the seemingly uncontrolled task and complex nature of the stimulus, brain activity was similar across viewers\u2019 brains, particularly in spatiotemporal areas. When compared to a random sequence of scenes, the specific order of events seemed to be strongly associated with this similarity in brain activity. It was also determined that the level of control a movie has on someone\u2019s mental state is highly dependent upon the cinematic devices (pans, cuts and close-ups) it contains. Tightly edited films exert more control on brain activity and eye-movement than open-ended films. However, similar eye-movement and similarity in visual processing does not guarantee similar brain responses. In addition, the average correlation in taste between individual viewers is rather low and not well predicted by film critics.",
            "score": 193.2991485595703
        },
        {
            "docid": "5664_46",
            "document": "Consciousness . A number of studies have shown that activity in primary sensory areas of the brain is not sufficient to produce consciousness: it is possible for subjects to report a lack of awareness even when areas such as the primary visual cortex show clear electrical responses to a stimulus. Higher brain areas are seen as more promising, especially the prefrontal cortex, which is involved in a range of higher cognitive functions collectively known as executive functions. There is substantial evidence that a \"top-down\" flow of neural activity (i.e., activity propagating from the frontal cortex to sensory areas) is more predictive of conscious awareness than a \"bottom-up\" flow of activity. The prefrontal cortex is not the only candidate area, however: studies by Nikos Logothetis and his colleagues have shown, for example, that visually responsive neurons in parts of the temporal lobe reflect the visual perception in the situation when conflicting visual images are presented to different eyes (i.e., bistable percepts during binocular rivalry).",
            "score": 193.06202697753906
        },
        {
            "docid": "7725524_2",
            "document": "Colour centre . The colour centre is a region in the brain primarily responsible for visual perception and cortical processing of colour signals received by the eye, which ultimately results in colour vision. The colour centre in humans is thought to be located in the ventral occipital lobe as part of the visual system, in addition to other areas responsible for recognizing and processing specific visual stimuli, such as faces, words, and objects. Many functional magnetic resonance imaging (fMRI) studies in both humans and macaque monkeys have shown colour stimuli to activate multiple areas in the brain, including the fusiform gyrus and the lingual gyrus. These areas, as well as others identified as having a role in colour vision processing, are collectively labelled visual area 4 (V4). The exact mechanisms, location, and function of V4 are still being investigated.",
            "score": 192.93267822265625
        }
    ]
}