{
    "q": [
        {
            "docid": "53757504_6",
            "document": "Viktor K. Jirsa . The Virtual Brain is a free open source neuroinformatics tool designed to aid in the exploration of network mechanisms of brain function and associated pathologies. TVB provides the possibility to feed computational neuronal network models with information about structural and functional imaging data including population (sEEG/EEG/MEG) activity, spatially highly resolved whole brain metabolic/vascular signals (fMRI) and global measures of neuronal connections (DTI) \u2013 for intact as well as pathologically altered connectivity. TVB is model agnostic and offers a wide range of neural population models to be used as network nodes. The software infrastructure of the Virtual Brain is composed of a functional core running the large-scale brain simulations independently or in batch mode, a web based interface to access the simulator, as well as a command line interface to develop more extensive applications. All simulations may be performed on workstations and labtops, as well as on high-performance clusters (HPCs). Manipulations of network parameters within the Virtual Brain allow researchers and clinicians to test the effects of experimental paradigms, interventions (such as stimulation and surgery) and therapeutic strategies (such as pharmaceutical interventions targeting local areas). The computational environment allows the user to visualise the simulated data in 2D and 3D and perform data analyses in the same way as commonly performed with empirical data.",
            "score": 171.2708193063736
        },
        {
            "docid": "19480072_2",
            "document": "Chronux . Chronux is an open-source software package developed for the loading, visualization and analysis of a variety of modalities / formats of neurobiological time series data. Usage of this tool enables neuroscientists to perform a variety of analysis on multichannel electrophysiological data such as LFP (local field potentials), EEG, MEG, Neuronal spike times and also on spatiotemporal data such as FMRI and dynamic optical imaging data. The software consists of a set of MATLAB routines interfaced with C libraries that can be used to perform the tasks that constitute a typical study of neurobiological data. These include local regression and smoothing, spike sorting and spectral analysis - including multitaper spectral analysis, a powerful nonparametric method to estimate power spectrum. The package also includes some GUIs for time series visualization and analysis. Chronux is GNU GPL v2 licensed (and MATLAB is proprietary).",
            "score": 122.88481259346008
        },
        {
            "docid": "38245286_7",
            "document": "Phenotype microarray . Proprietary and commercially available software is available that provides a solution for storage, retrieval, and analysis of high throughput phenotype data. A powerful free and open source software is the \"opm\" package based on R. \"opm\" contains tools for analyzing PM data including management, visualization and statistical analysis of PM data, covering curve-parameter estimation, dedicated and customizable plots, metadata management, statistical comparison with genome and pathway annotations, automatic generation of taxonomic reports, data discretization for phylogenetic software and export in the YAML markup language. In conjunction with other R packages it was used to apply boosting to re-analyse autism PM data and detect more determining factors. The \"opm\" package has been developed and is maintained at the Deutsche Sammlung von Mikroorganismen und Zellkulturen. Another free and open source software developed to analyze Phenotype Microarray data is \"DuctApe\", a Unix command-line tool that also correlates genomic data. Other software tools are PheMaDB, which provides a solution for storage, retrieval, and analysis of high throughput phenotype data, and the PMViewer software which focuses on graphical display but does not enable further statistical analysis. The latter is not publicly available.",
            "score": 134.5059015750885
        },
        {
            "docid": "2155752_54",
            "document": "Citizen science . The Internet has been a boon to citizen science, particularly through gamification. One of the first Internet-based citizen science experiments was NASA's Clickworkers, which enabled the general public to assist in the classification of images, greatly reducing the time to analyze large data sets. Another was the Citizen Science Toolbox, launched in 2003, of the Australian Coastal Collaborative Research Centre. Mozak is a game in which players create 3D reconstructions from images of actual human and mouse neurons, helping to advance understanding of the brain. One of the largest citizen science games is Eyewire, a brain-mapping puzzle game developed at the Massachusetts Institute of Technology that now has over 200,000 players. Another example is Quantum Moves, a game developed by the Center for Driven Community Research at Aarhus University, which uses online community efforts to solve quantum physics problems. The solutions found by players can then be used in the lab to feed computational algorithms used in building a scalable quantum computer.",
            "score": 186.07544374465942
        },
        {
            "docid": "35691407_3",
            "document": "Amira (software) . Amira is an extendable software system for scientific visualization, data analysis, and presentation of 3D and 4D data. It is being used by several thousand researchers and engineers in academia and industry around the world. Its flexible user interface and modular architecture make it a universal tool for processing and analysis of data from various modalities; e.g. micro-CT, PET, Ultrasound. Its ever-expanding functionality has made it a versatile data analysis and visualization solution, applicable to and being used in many fields, such as microscopy in biology and materials science, molecular biology, quantum physics, astrophysics, computational fluid dynamics (CFD), finite element modeling (FEM), non-destructive testing (NDT), and many more.  One of the key features, besides data visualization, is Amira\u2019s set of tools for image segmentation and geometry reconstruction. This allows the user to mark (or segment) structures and regions of interest in 3D image volumes using automatic, semi-automatic, and manual tools. The segmentation can then be used for a variety of subsequent tasks, such as volumetric analysis, density analysis, shape analysis, or the generation of 3D computer models for visualization, numerical simulations, or rapid prototyping or 3D printing, to name a few.  Other key Amira features are multi-planar and volume visualization, image registration, filament tracing, cell separation and analysis, tetrahedral mesh generation, fiber-tracking from diffusion tensor imaging (DTI) data, skeletonization, spatial graph analysis, and stereoscopic rendering of 3D data over multiple displays including CAVEs (Cave automatic virtual environments). As a commercial product Amira requires the purchase of a license or an academic subscription. A time-limited, but full-featured evaluation version is available for download free of charge.",
            "score": 142.29957735538483
        },
        {
            "docid": "1029211_28",
            "document": "Metabolomics . The data generated in metabolomics usually consist of measurements performed on subjects under various conditions. These measurements may be digitized spectra, or a list of metabolite levels. In its simplest form this generates a matrix with rows corresponding to subjects and columns corresponding with metabolite levels. Several statistical programs are currently available for analysis of both NMR and mass spectrometry data. A great number of free software packages are already available for the analysis of metabolomics data shows in the table. Some statistical tools listed in the table were designed for NMR data analyses were also useful for MS data.For mass spectrometry data, software is available that identifies molecules that vary in subject groups on the basis of mass and sometimes retention time depending on the experimental design. The first comprehensive software to analyze global mass spectrometry-based metabolomics datasets was developed by the Siuzdak laboratory at The Scripps Research Institute in 2006. This software, called XCMS, is freely available, has over 20,000 downloads since its inception in 2006, and is one of the most widely cited mass spectrometry-based metabolomics software programs in scientific literature. XCMS has now been surpassed in usage by a cloud-based version of XCMS called XCMS Online. It is available through an open source software project Bioconductor (http://www.bioconductor.org/) or METLIN metabolomics database (http://metlin.scripps.edu/download/). The software is capable of non-linear retention time alignment, peak picking, and relative quantitation and works with universal netCDF file format.",
            "score": 124.0968873500824
        },
        {
            "docid": "4479709_2",
            "document": "FreeSurfer . FreeSurfer is a brain imaging software package developed by the Athinoula A. Martinos Center for Biomedical Imaging at Massachusetts General Hospital for analyzing magnetic resonance imaging (MRI) scan data. It is an important tool in functional brain mapping and facilitates the visualization of the functional regions of the highly folded cerebral cortex. It contains tools to conduct both volume based and surface based analysis, which primarily use the white matter surface. FreeSurfer includes tools for the reconstruction of topologically correct and geometrically accurate models of both the gray/white and pial surfaces, for measuring cortical thickness, surface area and folding, and for computing inter-subject registration based on the pattern of cortical folds. In addition, an automated labeling of 35 non-cortical regions is included in the package.",
            "score": 135.89158082008362
        },
        {
            "docid": "5309_2",
            "document": "Software . Computer software, or simply software, is a generic term that refers to a collection of data or computer instructions that tell the computer how to work, in contrast to the physical hardware from which the system is built, that actually performs the work. In computer science and software engineering, computer software is all information processed by computer systems, programs and data. Computer software includes computer programs, libraries and related non-executable data, such as online documentation or digital media. Computer hardware and software require each other and neither can be realistically used on its own.",
            "score": 80.85057234764099
        },
        {
            "docid": "13561127_4",
            "document": "Online auction tools . This software enable tools such as: Auction Sniping Tools that allow consumers to place last minute bids Auction sniping is the process, in a timed online auction (such as on eBay), of placing a winning bid at the last possible moment (often seconds before the end of the auction), giving the other bidders no time to outbid the sniper. This can be done manually, or by software. The software can be designed for the purpose running on a bidder's computer, or for an online Internet sniping service, where the software agent is run from a website. Use of a service is stated to decrease the failure rate of the snipe, because the website is expected to have more reliable servers and Internet connection with lower and less variable delay in sending data (latency) enabling the bid to be placed closer to the deadline, even arriving within the last second. A bid sniper is a person who, or software agent which performs auction sniping.",
            "score": 80.9361982345581
        },
        {
            "docid": "127511_14",
            "document": "DNA sequencer . Roche produces a number of software tools which are optimised for the analysis of 454 sequencing data. GS Run Processor converts raw images generated by a sequencing run into intensity values. The process consists of two main steps: image processing and signal processing. The software also applies normalization, signal correction, base-calling and quality scores for individual reads. The software outputs data in Standard Flowgram Format (or SFF) files to be used in data analysis applications (GS De Novo Assembler, GS Reference Mapper or GS Amplicon Variant Analyzer). GS De Novo Assembler is a tool for \"de novo\" assembly of whole-genomes up to 3GB in size from shotgun reads alone or combined with paired end data generated by 454 sequencers. It also supports de novo assembly of transcripts (including analysis), and also isoform variant detection. GS Reference Mapper maps short reads to a reference genome, generating a consensus sequence. The software is able to generate output files for assessment, indicating insertions, deletions and SNPs. Can handle large and complex genomes of any size. Finally, the GS Amplicon Variant Analyzer aligns reads from amplicon samples against a reference, identifying variants (linked or not) and their frequencies. It can also be used to detect unknown and low-frequency variants. It includes graphical tools for analysis of alignments.",
            "score": 133.05912137031555
        },
        {
            "docid": "28464_17",
            "document": "Super-Kamiokande . An online monitor computer located in the control room reads data from the DAQ host computer via an FDDI link. It provides shift operators with a flexible tool for selecting event display features, makes online and recent-history histograms to monitor detector performance, and performs a variety of additional tasks needed to efficiently monitor status and diagnose detector and DAQ problems. Events in the data stream can be skimmed off and elementary analysis tools can be applied to check data quality during calibrations or after changes in hardware or online software.",
            "score": 84.59516620635986
        },
        {
            "docid": "57103423_12",
            "document": "German Network for Bioinformatics Intrastructure . de.NBI develops and supplies about 100 bioinformatics tools for the German and global life sciences community, e.g. Galaxy (computational biology)/useGalaxy.eu (Workflow engine for all Freiburg RNA Tools), EDGAR (Comparative Genome Analyses Plattform), KNIME (Workflow engine), OpenMS (Open-source software C++ library for LC/MS data management and analyses), SeqAN (Open source C++ library of efficient algorithms and data structures), PIA (toolbox for MS based protein inference and identification analysis), Fiji (software) (Image processing package), MetFrag (in silico fragmenter combines compound database searching and fragmentation prediction for small molecule identification from tandem mass spectrometry data), COPASI (Open source software application for creating and solving mathematical models of biological processes), SIAMCAT (Framework for the statistical inference of associations between microbial communities and host phenotypes), e!DAL - PGP (Open source software framework for publishing and sharing research data), MGX (Metagenome analysis) and many more.",
            "score": 103.17449903488159
        },
        {
            "docid": "12493244_14",
            "document": "Intel Fortran Compiler . The Intel Fortran compiler is available as part of the Intel Parallel Studio XE 2016 suite, which focuses on development of parallelism models in application software. It also includes Intel C++, Intel Math Kernel Library, Intel Integrated Performance Primitives, Intel Data Analytics Acceleration Library and performance analysis tools such as Intel VTune Amplifier and Intel Inspector. There are three forms of Parallel Studio XE: Composer, Professional, and Cluster. The Composer Edition includes the C++ and/or Fortran compilers, the performance libraries, and parallel models support. The Professional Edition adds the analysis tools that assist in debugging and tuning parallel applications. The Cluster Edition adds support for development of software for computer clusters. It includes all of the above plus a standards-based MPI Library, MPI communications profiling and analysis tool, MPI error checking and tuning tools, and cluster checker.",
            "score": 73.56744837760925
        },
        {
            "docid": "1228060_43",
            "document": "Internet privacy . Face recognition technology can be used to gain access to a person's private data, according to a new study. Researchers at Carnegie Mellon University combined image scanning, cloud computing and public profiles from social network sites to identify individuals in the offline world. Data captured even included a user's social security number. Experts have warned of the privacy risks faced by the increased merging of our online and offline identities. The researchers have also developed an 'augmented reality' mobile app that can display personal data over a person's image captured on a smartphone screen. Since these technologies are widely available, our future identities may become exposed to anyone with a smartphone and an Internet connection. Researchers believe this could force us to reconsider our future attitudes to privacy.",
            "score": 103.6722252368927
        },
        {
            "docid": "46580631_4",
            "document": "Text Database and Dictionary of Classic Mayan . TWKM will employ digital technologies in order to compile and make available the data and metadata, as well as to publish the project\u2019s research results. The project thereby methodologically positions itself in the field of the digital humanities. The project will be conducted in cooperation with the project partners (below), the research association for the eHumanities TextGrid, as well as the University and Regional Library of Bonn (ULB). The working environment that is currently under construction, in which the data and metadata will be compiled and annotated, will be realized in theTextGrid Laboratory, a software of the virtual research environment. A further component of this software, the TextGrid Repository, will make the data that are authorized for publication freely available online and ensure their long-term storage.  The tools for data compilation and annotation attained from the modularly constructed and extended TextGrid lab thereby provide all the necessary materials for facilitating the research team\u2019s the typical epigraphic workflow. The workflow usually begins by documenting the texts and the objects on which they are preserved, and by compiling descriptive data. It then continues with the various levels of epigraphic and linguistic analysis, and concludes in the best case scenario with a translation of the analyzed inscription and a corresponding publication. In cooperation with the ULB, selected data will additionally be made available. The project\u2019s Virtual Inscription Archive will present online, in the Digital Collections of the ULB, hieroglyphic inscriptions selected from the published data in the repository, including an image of and brief information about the texts and the objects on which they are written, epigraphic analysis, and translation.",
            "score": 144.6882643699646
        },
        {
            "docid": "16620919_3",
            "document": "Network Centric Product Support . Simply put, this architecture extends the existing World Wide Web infrastructure of networked web servers down into the product at its subsystem's controller level using a Systems Engineering \"system of systems\" nested approach. Its core is an embedded dual function webserver/computer workstation connected to the product controller's test ports (as used in retrofit applications, or integrated directly into the controller for new products), hence providing access to operational cycles, sensor and other information in a clustered, internet addressable node that allows for local or remote access, and the ability to host remotely reconfigurable software that can collect and process data from its mated subsystem controller onboard and pull in other computing resources throughout the network. It can then establish a localized wireless World Wide Web in and around the product that can be securely connected to by a mechanic with any web browser-equipped handheld independent of the greater World Wide Web, as well as seamlessly integrate into global networks when external wireless communications is available - thus creating data Digital Twins at the factory, connecting deployed product usage in the Product lifecycle with constantly updated digital threads. This allows for an integrated approach which enables both offline and online updates to occur. Legacy systems usually require a human to physically connect a laptop to the system controller or a telematic solution to manually collect data and carry it back to a location where it can be later transferred to the factory or to restricted webserver-based download sites for offboard analysis.",
            "score": 96.52419149875641
        },
        {
            "docid": "5323_24",
            "document": "Computer science . Formal methods are a particular kind of mathematically based technique for the specification, development and verification of software and hardware systems. The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design. They form an important theoretical underpinning for software engineering, especially where safety or security is involved. Formal methods are a useful adjunct to software testing since they help avoid errors and can also give a framework for testing. For industrial use, tool support is required. However, the high cost of using formal methods means that they are usually only used in the development of high-integrity and life-critical systems, where safety or security is of utmost importance. Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification.",
            "score": 80.00857543945312
        },
        {
            "docid": "15978180_16",
            "document": "Connectome . Mapping the connectome at the \"microscale\" (micrometer resolution) means building a complete map of the neural systems, neuron-by-neuron. The challenge of doing this becomes obvious: the number of neurons comprising the brain easily ranges into the billions in more highly evolved organisms. The human cerebral cortex alone contains on the order of 10 neurons linked by 10 synaptic connections. By comparison, the number of base-pairs in a human genome is 3\u00d710. A few of the main challenges of building a human connectome at the microscale today include: (1) data collection would take years given current technology; (2) machine vision tools to annotate the data remain in their infancy, and are inadequate; and (3) neither theory nor algorithms are readily available for the analysis of the resulting \"brain-graphs\". To address the data collection issues, several groups are building high-throughput serial electron microscopes (Kasthuri \"et al.\", 2009; Bock \"et al\". 2011). To address the machine-vision and image-processing issues, the Open Connectome Project is \"alg-sourcing\" (algorithm outsourcing) this hurdle. Finally, statistical graph theory is an emerging discipline which is developing sophisticated pattern recognition and inference tools to parse these brain-graphs (Goldenberg \"et al.\", 2009).",
            "score": 186.05030059814453
        },
        {
            "docid": "9466116_2",
            "document": "Generalized audit software . Generalized audit software (GAS) refers to software designed to read, process and write data with the help of functions performing specific audit routines and with self-made macros. It is a tool in applying Computer Assisted Auditing Techniques. Functions of generalized audit software include importing computerized data; thereafter other functions can be applied: the data can be e.g. browsed, sorted, summarized, stratified, analyzed, taken samples from, and made calculations, conversions and other operations with. Two of the most popular software packages are Audit Command Language (ACL) and Interactive Data Extraction and Analysis (IDEA, edited by CaseWare International).",
            "score": 107.04541110992432
        },
        {
            "docid": "37367048_2",
            "document": "Heidelberg Institute for Theoretical Studies . The Heidelberg Institute for Theoretical Studies (HITS gGmbH) was established in 2010 by SAP co-founder Klaus Tschira through his foundation, the \u201cKlaus Tschira Stiftung\u201d, as a private, non-profit research institute. HITS conducts basic research involving the processing structuring and analysis of large amounts of data in the natural sciences, mathematics and computer science. The research topics range from molecular biology to astrophysics. Shareholders of HITS are the \u201cHITS-Stiftung\u201d, Heidelberg University and the Karlsruhe Institute of Technology (KIT). HITS cooperates with universities and research institutes, as well as with industrial partners. The prime external funding sources are the Federal Ministry of Education and Research, the German Research Foundation and the European Union. At the moment, HITS comprises following research groups: The junior group Astroinformatics was founded 2013 at HITS to develop new approaches to analyze and process the increasing amount of data in astronomy. The approaches of this group are based on machine/statistical learning and assist the researchers in performing the required analyses. The CST group works on mathematical foundations and statistical methodology for forecasting. The aim is to develop methods for probabilistic forecasts, to generate predictive probability distributions for future events and quantities. For example, probabilistic forecasts are used in weather prediction and economics. The group\u2019s second research focus is on spatial statistics, which is concerned with the analysis and interpretation of spatially distributed data. The research group DMQ makes use of state-of-the-art technology from the fields of High Performance Computing and Uncertainty Quantification in order to quantify uncertainties in large data sets towards reliable insights in Data Mining. The research group \u201cGroups and Geometry\u201d investigates various mathematical problems in the fields of geometry and topology, which involve the interplay between geometric spaces, such as Riemannian manifolds or metric spaces, and groups, arising for example from symmetries, acting on them. This group develops simulation techniques and continuum mechanics models for identifying the force-bearing structural elements in complex biological materials and for modifying them so that they have certain desired properties. The overall goal is to investigate how proteins respond to mechanical forces and why. With computer-aided methods and software tools, the group detects and simulates the behavior of molecules. Furthermore, they develop interactive web-based visualization tools and applications for complex molecular simulations. The research focus lies on the semantics and pragmatics of discourse. The group develops software facilitating the multimodal dialogue between users and machines. The aim is to use the computer for understanding and generating language and texts and to make use of computers more naturally in the long term. The research group \"Physics of Stellar Objects\" is conducting research on stars and stellar explosions. One of the main goals of the group is to simulate thermonuclear explosions of white dwarfs, which lead to Type Ia supernovae. The Scientific Computing (SCO) group develops methods, new software, and new computer architectures for computing phylogenies (evolutionary trees). Furthermore, it provides expertise in parallel computing and computer architecture to other research groups. It also maintains and operates the scientific computing cluster and the IT infrastructure at HITS. The SDBV group focuses on scientific databases and on the visualization of scientific data. The objective is to consolidate knowledge scattered all over the world and to make it easily accessible to scientists. With numeric simulations, the TAP research group investigates the Universe. The research focus is on the formation and evolution of galaxies, supermassive black holes, stars, and planets.",
            "score": 136.68493378162384
        },
        {
            "docid": "39563465_29",
            "document": "List of open-source software for mathematics . DataMelt was initially written by S. Chekanov in 2005 for High-energy physics.  The original name was JHepWork, but later it was renamed to SCaViS and then to DataMelt. The main idea was to build a multi-purpose computational environment that is fully multiplatform and combine the best open-source Java libraries. DataMelt uses high-level programming languages, such as Jython (Python implemented in Java), Groovy, JRuby, but Java coding can also be used to call numerical and graphical libraries. The main source of the documentation is the book \"Scientific Data analysis using Jython Scripting and Java\", but the online manual is also available.",
            "score": 60.806678771972656
        },
        {
            "docid": "31552031_2",
            "document": "HPCC . HPCC (High-Performance Computing Cluster), also known as DAS (Data Analytics Supercomputer), is an open source, data-intensive computing system platform developed by LexisNexis Risk Solutions. The HPCC platform incorporates a software architecture implemented on commodity computing clusters to provide high-performance, data-parallel processing for applications utilizing big data. The HPCC platform includes system configurations to support both parallel batch data processing (Thor) and high-performance online query applications using indexed data files (Roxie). The HPCC platform also includes a data-centric declarative programming language for parallel data processing called ECL.",
            "score": 76.52716183662415
        },
        {
            "docid": "38330850_13",
            "document": "Neuronal tracing . A number of neuron tracing tools especially software packages are available. One comprehensive Open Source software package that contains implementation of a number of neuron tracing methods developed in different research groups as well as many neuron utilities functions such as quantitative measurement, parsing, comparison, is Vaa3D and its Vaa3D-Neuron modules (http://vaa3d.org). Some other free tools such as NeuronStudio also provide tracing function based on specific methods. Neuroscientists also use commercial tools such as Neurolucida, Neurolucida 360, Aivia, Amira, etc. to trace and analyse neurons. Recent studies show that Neurolucida is cited over 7 times more than all other available neuron tracing programs combined, and is also the most widely used and versatile system to produce neuronal reconstruction. The BigNeuron project (http://bigneuron.org) is a recent substantial international collaboration effort to integrate the majority of known neuron tracing tools onto a common platform to facilitate Open Source, easy accessing of various tools at one single place. Powerful new tools such as UltraTracer, that can trace arbitrarily large image volume, have been produced through this effort.",
            "score": 144.31902074813843
        },
        {
            "docid": "37115523_11",
            "document": "Institut de recherche et d'innovation . IRI studies, designs and develops accordingly annotation tools and critical equipment of a new kind, based on a combination of documentary and metadata architectures with navigation interfaces hypermedia, modules algorithmic signal detection modules and data representation ( mapping ). The result of this research is regularly embedded software time lines, annotation platform online and offline for the annotation of temporal objects (movies, audio recordings). Research in this area is gradually extended to the field of annotation oral and written language and image annotation.",
            "score": 133.02551984786987
        },
        {
            "docid": "14829511_9",
            "document": "Automounter . In some computing environments, user workstations and computing nodes do not host installations of the full range of software that users might want to access. Systems may be \"imaged\" with a minimal or typical cross-section of the most commonly used software. Also, in some environments, users might require specialized or occasional access to older versions of software (for instance, developers may need to perform bug fixes and regression testing, or some users may need access to archived data using outdated tools).",
            "score": 96.54540371894836
        },
        {
            "docid": "987320_19",
            "document": "Neurotechnology . Magnetic resonance imaging is a vital tool in neurological research in showing activation in the brain as well as providing a comprehensive image of the brain being studied. While MRIs are used clinically for showing brain size, it still has relevance in the study of brains because it can be used to determine extent of injuries or deformation. These can have a significant effect on personality, sense perception, memory, higher order thinking, movement, and spatial understanding. However, current research tends to focus more so on fMRI or real-time functional MRI (rtfMRI). These two methods allow the scientist or the participant, respectively, to view activation in the brain. This is incredibly vital in understanding how a person thinks and how their brain reacts to a person's environment, as well as understanding how the brain works under various stressors or dysfunctions. Real-time functional MRI is a revolutionary tool available to neurologists and neuroscientists because patients can see how their brain reacts to stressors and can perceive visual feedback. CT scans are very similar to MRI in their academic use because they can be used to image the brain upon injury, but they are more limited in perceptual feedback. CTs are generally used in clinical studies far more than in academic studies, and are found far more often in a hospital than a research facility. PET scans are also finding more relevance in academia because they can be used to observe metabolic uptake of neurons, giving researchers a wider perspective about neural activity in the brain for a given condition. Combinations of these methods can provide researchers with knowledge of both physiological and metabolic behaviors of loci in the brain and can be used to explain activation and deactivation of parts of the brain under specific conditions.",
            "score": 154.4455235004425
        },
        {
            "docid": "24043558_4",
            "document": "ScanIP . ScanIP generates high-quality 3D models from image data suitable for a wide range of design and simulation applications related to the life sciences. Image data from sources like MRI and CT can be visualised, analysed, segmented and quantified, before being exported as CAD, CAE and 3D printing models. Different tissues, bones and other parts of the body can be identified using a wide range of segmentation and processing tools in the software. Options are also available for integrating CAD and image data, enabling medical device research to be conducted into how CAD-designed implants interact with the human body. High-quality CAE models can similarly be used in biomechanics research to simulate movement and the effect of different forces on anatomies. An example of this is the US Naval Research Laboratory/Simpleware head model, generated from high-resolution MRI scans and segmented to create data that can be easily meshed to suit specific finite element (FE) applications, such as head impact and concussion.",
            "score": 119.89367425441742
        },
        {
            "docid": "13758862_9",
            "document": "Daniel A. Reed (computer scientist) . Reed\u2019s research focuses on the design of very high-speed computers, providing new computing capabilities for scholars in science, medicine, engineering and the humanities, tools and techniques for capturing and analyzing the performance of parallel systems, and collaborative virtual environments for real-time performance analysis. He led the Pablo Research Group, which investigates the interaction of architecture, system software, and applications on large-scale parallel and distributed computer systems. The group created SvPablo, a graphical environment for instrumenting application source code and browsing dynamic performance data. Key research foci of the group included exploration of performance analysis techniques and compiler-aided scalability analysis, scalable parallel file systems, and real-time adaptive systems for resource policy control.",
            "score": 139.95969939231873
        },
        {
            "docid": "30525054_10",
            "document": "Anders Dale . Dale has been Professor of Radiology, Neurosciences, Psychiatry, and Cognitive Science at UCSD  since 2004, and is the founding Co-Director of UCSD's Multi-Modal Imaging Laboratory (MMIL), which the university's website describes as \u201can interdisciplinary initiative of the Departments of Neurosciences and Radiology.\u201d Dale is \u201cthe designated point person\u201d in both departments \u201cfor integrating the various modes and methods of collecting imaging data, including functional MRI (fMRI), magnetoencephalography (MEG), electroencephalography (EEG), and optical imaging.\u201d Dale's efforts, the website states, \u201care directed in three areas: continuing development and refinement of accurate and automated algorithms for evaluation subjects using multimodality approaches to data collection; statistical analysis of data; and conducting studies in animal models using optical imaging, high field fMRI, and electrophysiological recordings to enhance the interpretation of neuroimaging studies.\u201d His work has \u201cresulted in the development of software tools that enable the automated segmentation of the entire head and brain, including the neocortex and subcortical structures, from MRI data.\u201d Most recently, Dale and his laboratory colleagues have been using the methods they have developed to assess regional morphometric alterations resulting from aging and from such afflictions as schizophrenia, Alzheimer's disease, and Huntington's disease.",
            "score": 110.90783858299255
        },
        {
            "docid": "16725900_2",
            "document": "MEGAN . MEGAN (\"MEtaGenome ANalyzer\") is a computer program written in the program language Java, that allows optimized analysis of large metagenomic datasets. MEGAN can be used to explore taxonomic diversification of the dataset which could be collected from any type of metagenomic project or sequencing platform. In pre-processing step, the set of DNA reads is compared with sequence databases which can be computationally exhaustive and computationally complex for a standard user. The main application of the program is to parse and analyze the results of an alignment of a set of reads against one or more reference databases using BLASTN, BLASTX, or BLASTP. MEGAN makes such a task easy and data analyses can be made on a workstation after completing sequence comparison on a computer cluster. In addition to that, functional analysis using SEED, functional analysis using KEGG and functional analysis using COG/EGGNOG is possible. Principal coordinate analysis (PCoA) is also available in the latest version for taxonomy and functional profiles. Comparative visualization options also provides extra functionality to display and present data. MEGAN has two editions, Community and Ultimate Edition. Community Edition is a free software that contains all features to retrieve the analysis of microbiome samples. Ultimate edition was built on top of the Community Edition. This edition has more features like additional analyzers and charts. You can use command-line tools to customize classification schemes and mapping files by the program. It is recommended to use the Community Edition if you are using it for a straightforward project, otherwise, use Ultimate Edition for a more sophisticated project. Especially, if you would like more charts and analysis tools.",
            "score": 122.63219654560089
        },
        {
            "docid": "170581_11",
            "document": "Game engine . Low-level libraries such as DirectX, Simple DirectMedia Layer (SDL), and OpenGL are also commonly used in games as they provide hardware-independent access to other computer hardware such as input devices (mouse, keyboard, and joystick), network cards, and sound cards. Before hardware-accelerated 3D graphics, software renderers had been used. Software rendering is still used in some modeling tools or for still-rendered images when visual accuracy is valued over real-time performance (frames-per-second) or when the computer hardware does not meet needs such as shader support.",
            "score": 77.05529689788818
        },
        {
            "docid": "23739365_28",
            "document": "Mobile device forensics . Different software tools can extract the data from the memory image. One could use specialized and automated forensic software products or generic file viewers such as any hex editor to search for characteristics of file headers. The advantage of the hex editor is the deeper insight into the memory management, but working with a hex editor means a lot of handwork and file system as well as file header knowledge. In contrast, specialized forensic software simplifies the search and extracts the data but may not find everything. AccessData, Sleuthkit, and EnCase, to mention only some, are forensic software products to analyze memory images. Since there is no tool that extracts all possible information, it is advisable to use two or more tools for examination. There is currently (February 2010) no software solution to get all evidences from flash memories.",
            "score": 114.6981315612793
        }
    ],
    "r": [
        {
            "docid": "2155752_54",
            "document": "Citizen science . The Internet has been a boon to citizen science, particularly through gamification. One of the first Internet-based citizen science experiments was NASA's Clickworkers, which enabled the general public to assist in the classification of images, greatly reducing the time to analyze large data sets. Another was the Citizen Science Toolbox, launched in 2003, of the Australian Coastal Collaborative Research Centre. Mozak is a game in which players create 3D reconstructions from images of actual human and mouse neurons, helping to advance understanding of the brain. One of the largest citizen science games is Eyewire, a brain-mapping puzzle game developed at the Massachusetts Institute of Technology that now has over 200,000 players. Another example is Quantum Moves, a game developed by the Center for Driven Community Research at Aarhus University, which uses online community efforts to solve quantum physics problems. The solutions found by players can then be used in the lab to feed computational algorithms used in building a scalable quantum computer.",
            "score": 186.075439453125
        },
        {
            "docid": "15978180_16",
            "document": "Connectome . Mapping the connectome at the \"microscale\" (micrometer resolution) means building a complete map of the neural systems, neuron-by-neuron. The challenge of doing this becomes obvious: the number of neurons comprising the brain easily ranges into the billions in more highly evolved organisms. The human cerebral cortex alone contains on the order of 10 neurons linked by 10 synaptic connections. By comparison, the number of base-pairs in a human genome is 3\u00d710. A few of the main challenges of building a human connectome at the microscale today include: (1) data collection would take years given current technology; (2) machine vision tools to annotate the data remain in their infancy, and are inadequate; and (3) neither theory nor algorithms are readily available for the analysis of the resulting \"brain-graphs\". To address the data collection issues, several groups are building high-throughput serial electron microscopes (Kasthuri \"et al.\", 2009; Bock \"et al\". 2011). To address the machine-vision and image-processing issues, the Open Connectome Project is \"alg-sourcing\" (algorithm outsourcing) this hurdle. Finally, statistical graph theory is an emerging discipline which is developing sophisticated pattern recognition and inference tools to parse these brain-graphs (Goldenberg \"et al.\", 2009).",
            "score": 186.05030822753906
        },
        {
            "docid": "28022785_3",
            "document": "Serial block-face scanning electron microscopy . One of the first applications of SBFSEM was to analyze the connectivity of axons in the brain. The resolution is sufficient to trace even the thinnest axons and to identify synapses.By now, serial block face imaging contributed to many fields, like developmental biology, plant biology, cancer research, studying neuro-degenerative diseases etc. SBFSEM can generate extremely large data sets, and development of algorithms for automatic segmentation of the very large data sets generated is still a challenge. However much work is being done on this area currently. The EyeWire project harnesses human computation in a game to trace neurons through images of a volume of retina obtained using SBEM.",
            "score": 184.11581420898438
        },
        {
            "docid": "149353_4",
            "document": "Computational biology . Computational Biology, which includes many aspects of bioinformatics, is the science of using biological data to develop algorithms or models to understand biological systems and relationships.  Until recently, biologists did not have access to very large amounts of data. This data has now become commonplace, particularly in molecular biology and genomics. Researchers were able to develop analytical methods for interpreting biological information, but were unable to share them quickly among colleagues.  Bioinformatics began to develop in the early 1970s. It was considered the science of analyzing informatics processes of various biological systems. At this time, research in artificial intelligence was using network models of the human brain in order to generate new algorithms. This use of biological data to develop other fields pushed biological researchers to revisit the idea of using computers to evaluate and compare large data sets. By 1982, information was being shared amongst researchers through the use of punch cards. The amount of data being shared began to grow exponentially by the end of the 1980s. This required the development of new computational methods in order to quickly analyze and interpret relevant information. Since the late 1990s, computational biology has become an important part of developing emerging technologies for the field of biology. The terms computational biology and evolutionary computation have a similar name, but are not to be confused. Unlike computational biology, evolutionary computation is not concerned with modeling and analyzing biological data. It instead creates algorithms based on the ideas of evolution across species. Sometimes referred to as genetic algorithms, the research of this field can be applied to computational biology. While evolutionary computation is not inherently a part of computational biology, Computational evolutionary biology is a subfield of it.",
            "score": 182.40982055664062
        },
        {
            "docid": "26827372_2",
            "document": "Brain morphometry . Brain morphometry is a subfield of both morphometry and the brain sciences, concerned with the measurement of brain structures and changes thereof during development, aging, learning, disease and evolution. Since autopsy-like dissection is generally impossible on living brains, brain morphometry starts with noninvasive neuroimaging data, typically obtained from magnetic resonance imaging (or MRI for short). These data are born digital, which allows researchers to analyze the brain images further by using advanced mathematical and statistical methods such as shape quantification or multivariate analysis. This allows researchers to quantify anatomical features of the brain in terms of shape, mass, volume (e.g. of the hippocampus, or of the primary versus secondary visual cortex), and to derive more specific information, such as the encephalization quotient, grey matter density and white matter connectivity, gyrification, cortical thickness, or the amount of cerebrospinal fluid. These variables can then be mapped within the brain volume or on the brain surface, providing a convenient way to assess their pattern and extent over time, across individuals or even between different biological species. The field is rapidly evolving along with neuroimaging techniques \u2014 which deliver the underlying data \u2014 but also develops in part independently from them, as part of the emerging field of neuroinformatics, which is concerned with developing and adapting algorithms to analyze those data.",
            "score": 174.60400390625
        },
        {
            "docid": "53757504_6",
            "document": "Viktor K. Jirsa . The Virtual Brain is a free open source neuroinformatics tool designed to aid in the exploration of network mechanisms of brain function and associated pathologies. TVB provides the possibility to feed computational neuronal network models with information about structural and functional imaging data including population (sEEG/EEG/MEG) activity, spatially highly resolved whole brain metabolic/vascular signals (fMRI) and global measures of neuronal connections (DTI) \u2013 for intact as well as pathologically altered connectivity. TVB is model agnostic and offers a wide range of neural population models to be used as network nodes. The software infrastructure of the Virtual Brain is composed of a functional core running the large-scale brain simulations independently or in batch mode, a web based interface to access the simulator, as well as a command line interface to develop more extensive applications. All simulations may be performed on workstations and labtops, as well as on high-performance clusters (HPCs). Manipulations of network parameters within the Virtual Brain allow researchers and clinicians to test the effects of experimental paradigms, interventions (such as stimulation and surgery) and therapeutic strategies (such as pharmaceutical interventions targeting local areas). The computational environment allows the user to visualise the simulated data in 2D and 3D and perform data analyses in the same way as commonly performed with empirical data.",
            "score": 171.27081298828125
        },
        {
            "docid": "3062721_16",
            "document": "Neuroinformatics . Biology is concerned with molecular data (from genes to cell specific expression); medicine and anatomy with the structure of synapses and systems level anatomy; engineering \u2013 electrophysiology (from single channels to scalp surface EEG), brain imaging; computer science \u2013 databases, software tools, mathematical sciences \u2013 models, chemistry \u2013 neurotransmitters, etc. Neuroscience uses all aforementioned experimental and theoretical studies to learn about the brain through its various levels. Medical and biological specialists help to identify the unique cell types, and their elements and anatomical connections. Functions of complex organic molecules and structures, including a myriad of biochemical, molecular, and genetic mechanisms which regulate and control brain function, are determined by specialists in chemistry and cell biology. Brain imaging determines structural and functional information during mental and behavioral activity. Specialists in biophysics and physiology study physical processes within neural cells neuronal networks. The data from these fields of research is analyzed and arranged in databases and neural models in order to integrate various elements into a sophisticated system; this is the point where neuroinformatics meets other disciplines.",
            "score": 169.5887908935547
        },
        {
            "docid": "53757504_8",
            "document": "Viktor K. Jirsa . Full brain modelling of the last century was limited to either a few regions of interest modelling or to (mostly unrealistic) approximations of brain connectivity. However, full brain models have always been the interface between human brain imaging and theorising on brain function and dysfunction. In 2002 Jirsa and colleagues demonstrated that the approximations of brain connectivity will never be able to capture most behaviour of brain imaging data (in particular the spatiotemporal symmetries in the data) and thus proposed to use DTI data as a proxy of network connectivity in brain models. Characteristic challenges for this type of large-scale models would be 1) the detailed connection topology and 2) time delays via signal transmission, which do not play a role for modelling on all other levels of organisation. In 2006 Jirsa introduced connectome-based connectivity (from the Cocamac data base with the help of Rolf K\u00f6tter) and presented a large-scale brain network model of resting state brain dynamics in Sendai, Japan, at the Brain Connectivity workshop. Connectome-based brain modelling became an active field of research in the years that followed (Honey et al. 2007; Ghosh et al. 2008; Deco et al. 2009), with many applications devoted to resting state dynamics in healthy subjects, ageing and diseases such as schizophrenia, lesions and epilepsy. Various reviews summarise the findings (Deco et al. 2011, 2013; Kringelbach et al. 2015) and highlight the impact of this new approach. Jirsa and his group have contributed to the conception and development of connectome-based modelling and developed a range of technical tools since the early 2000s (Jirsa & Kelso 2000; Jirsa et al. 2002; Jirsa 2009). Of particular importance are Jirsa\u2019s contributions to a better understanding and treatment of signal transmission time delays in brain networks. The presence of many time delays, which are systematically distributed across the network, is a key characteristic of connectome-based brain models and not encountered in any other system of spatiotemporal pattern formation.",
            "score": 164.39208984375
        },
        {
            "docid": "47169724_3",
            "document": "Rice Center for Neuroengineering . The center's research centers on the fundamental understanding of coding and computation in the human brain, as well as developing technology to treat and diagnose neural disease. Individual faculty research includes integrating neural circuits at the cellular level, analyzing neuronal data in real-time, and manipulating healthy or diseased neural circuit activity and connectivity using nano electronics, optics, and emerging photonics technologies.",
            "score": 162.04537963867188
        },
        {
            "docid": "38567205_7",
            "document": "BRAIN Initiative . In a 2012 scientific commentary outlining experimental plans for a more limited project, Alivisatos \"et al.\" outlined a variety of specific experimental techniques that might be used to achieve what they termed a \"functional connectome\", as well as new technologies that will have to be developed in the course of the project. They indicated that initial studies might be done in \"Caenorhabditis elegans\", followed by \"Drosophila\", because of their comparatively simple neural circuits. Mid-term studies could be done in zebrafish, mice, and the Etruscan shrew, with studies ultimately to be done in primates and humans. They proposed the development of nanoparticles that could be used as voltage sensors that would detect individual action potentials, as well as nanoprobes that could serve as electrophysiological multielectrode arrays. In particular, they called for the use of wireless, noninvasive methods of neuronal activity detection, either utilizing microelectronic very-large-scale integration, or based on synthetic biology rather than microelectronics. In one such proposed method, enzymatically produced DNA would serve as a \"ticker tape record\" of neuronal activity, based on calcium ion-induced errors in coding by DNA polymerase. Data would be analyzed and modeled by large scale computation. A related technique proposed the use of high-throughput DNA sequencing for rapidly mapping neural connectivity.",
            "score": 158.02052307128906
        },
        {
            "docid": "987320_19",
            "document": "Neurotechnology . Magnetic resonance imaging is a vital tool in neurological research in showing activation in the brain as well as providing a comprehensive image of the brain being studied. While MRIs are used clinically for showing brain size, it still has relevance in the study of brains because it can be used to determine extent of injuries or deformation. These can have a significant effect on personality, sense perception, memory, higher order thinking, movement, and spatial understanding. However, current research tends to focus more so on fMRI or real-time functional MRI (rtfMRI). These two methods allow the scientist or the participant, respectively, to view activation in the brain. This is incredibly vital in understanding how a person thinks and how their brain reacts to a person's environment, as well as understanding how the brain works under various stressors or dysfunctions. Real-time functional MRI is a revolutionary tool available to neurologists and neuroscientists because patients can see how their brain reacts to stressors and can perceive visual feedback. CT scans are very similar to MRI in their academic use because they can be used to image the brain upon injury, but they are more limited in perceptual feedback. CTs are generally used in clinical studies far more than in academic studies, and are found far more often in a hospital than a research facility. PET scans are also finding more relevance in academia because they can be used to observe metabolic uptake of neurons, giving researchers a wider perspective about neural activity in the brain for a given condition. Combinations of these methods can provide researchers with knowledge of both physiological and metabolic behaviors of loci in the brain and can be used to explain activation and deactivation of parts of the brain under specific conditions.",
            "score": 154.44552612304688
        },
        {
            "docid": "21923693_5",
            "document": "Stanley A. Klein . Klein has worked to developed new methods for obtaining and analyzing evoked EEG/MEG (electro- and magneto-encephalography) and related fMRI data that will provide needed spatio-temporal resolution. In order for M/EEG to become a widely used tool for analyzing brain function it is necessary to go from the sensor information (magnetic fields for MEG and electric potentials for EEG) to the identification (locations, orientations and time functions) of the multiple brain sources. He has helped to develop a novel set of stimuli that allows the collection of a much larger set of data than ever previously collected without increasing the data collection time. New algorithms overcome the \"rotation problem\" and to minimize the \"mis-specification\" problem so that the location, orientation and time functions of multiple cortical sources are identified.",
            "score": 154.35145568847656
        },
        {
            "docid": "3717_69",
            "document": "Brain . Theorists have worked to understand these response patterns by constructing mathematical models of neurons and neural networks, which can be simulated using computers. Some useful models are abstract, focusing on the conceptual structure of neural algorithms rather than the details of how they are implemented in the brain; other models attempt to incorporate data about the biophysical properties of real neurons. No model on any level is yet considered to be a fully valid description of brain function, though. The essential difficulty is that sophisticated computation by neural networks requires distributed processing in which hundreds or thousands of neurons work cooperatively\u2014current methods of brain activity recording are only capable of isolating action potentials from a few dozen neurons at a time.",
            "score": 154.1380157470703
        },
        {
            "docid": "21402632_49",
            "document": "Electroencephalography . EEG has several strong points as a tool for exploring brain activity. EEGs can detect changes over milliseconds, which is excellent considering an action potential takes approximately 0.5\u2013130 milliseconds to propagate across a single neuron, depending on the type of neuron. Other methods of looking at brain activity, such as PET and fMRI have time resolution between seconds and minutes. EEG measures the brain's electrical activity directly, while other methods record changes in blood flow (e.g., SPECT, fMRI) or metabolic activity (e.g., PET, NIRS), which are indirect markers of brain electrical activity. EEG can be used simultaneously with fMRI so that high-temporal-resolution data can be recorded at the same time as high-spatial-resolution data, however, since the data derived from each occurs over a different time course, the data sets do not necessarily represent exactly the same brain activity. There are technical difficulties associated with combining these two modalities, including the need to remove the \"MRI gradient artifact\" present during MRI acquisition and the ballistocardiographic artifact (resulting from the pulsatile motion of blood and tissue) from the EEG. Furthermore, currents can be induced in moving EEG electrode wires due to the magnetic field of the MRI.",
            "score": 153.9300079345703
        },
        {
            "docid": "4833512_9",
            "document": "Mu wave . Brain-computer interfaces (BCIs) are a developing technology that clinicians hope will one day bring more independence and agency to the severely physically disabled. Those the technology has the potential to help include people with near-total or total paralysis, such as those with tetraplegia (quadriplegia) or advanced amyotrophic lateral sclerosis (ALS); BCIs are intended to help them to communicate or even move objects such as motorized wheelchairs, neuroprostheses, or robotic grasping tools. Few of these technologies are currently in regular use by people with disabilities, but a diverse array are in development at an experimental level. One type of BCI uses event-related desynchronization (ERD) of the mu wave in order to control the computer. This method of monitoring brain activity takes advantage of the fact that when a group of neurons is at rest they tend to fire in synchrony with each other. When a participant is cued to imagine movement (an \"event\"), the resulting desynchronization (the group of neurons that was firing in synchronous waves now firing in complex and individualized patterns) can be reliably detected and analyzed by a computer. Users of such an interface are trained in visualizing movements, typically of the foot, hand, and/or tongue, which are each in different locations on the cortical homunculus and thus distinguishable by an electroencephalograph (EEG) or electrocorticograph (ECoG) recording of electrical activity over the motor cortex. In this method, computers monitor for a typical pattern of mu wave ERD contralateral to the visualized movement combined with event-related synchronization (ERS) in the surrounding tissue. This paired pattern intensifies with training, and the training increasingly takes the form of games, some of which utilize virtual reality. Some researchers have found that the feedback from virtual reality games is particularly effective in giving the user tools to improve control of his or her mu wave patterns. The ERD method can be combined with one or more other methods of monitoring the brain's electrical activity to create hybrid BCIs, which often offer more flexibility than a BCI that uses any single monitoring method.",
            "score": 151.44651794433594
        },
        {
            "docid": "56451610_9",
            "document": "Rafael Yuste . In 2011 at a meeting with funding agencies, Yuste proposed the goal of developing technologies to \"record every spike from every neuron\" and then co-authored together with George M. Church, Paul Alivisatos, Ralph Greenspan, and Michael Roukes a white paper to elaborate this idea as a large-scale scientific project (then called the \"Brain Activity Map Project\") modelled on the Human Genome Project. Two years later then president Barack Obama announced the US BRAIN Initiative that now funds neuroscience research in hundreds of laboratories and is slated to last until 2025. Yuste has warned against spreading the funds of the initiative too thin and argued that a focused effort is required to develop the technologies needed for large-scale, real-time brain imaging with single-neuron resolution that would be made available at observatory-like centers to the scientific community.",
            "score": 149.367919921875
        },
        {
            "docid": "23477227_2",
            "document": "Overhead Imagery Research Data Set . The Overhead Imagery Research Data Set (OIRDS) is a collection of an open-source, annotated, overhead images that computer vision researchers can use to aid in the development of algorithms. Most computer vision and machine learning algorithms function by training on a large set of example data. Further, for many academic and industry researchers, the availability of truth-labeled test data helps drive algorithm research.",
            "score": 149.1488494873047
        },
        {
            "docid": "12266_65",
            "document": "Genetics . Next-generation sequencing (or high-throughput sequencing) came about due to the ever-increasing demand for low-cost sequencing. These sequencing technologies allow the production of potentially millions of sequences concurrently. The large amount of sequence data available has created the field of genomics, research that uses computational tools to search for and analyze patterns in the full genomes of organisms. Genomics can also be considered a subfield of bioinformatics, which uses computational approaches to analyze large sets of biological data. A common problem to these fields of research is how to manage and share data that deals with human subject and personally identifiable information. See also genomics data sharing.",
            "score": 149.1179962158203
        },
        {
            "docid": "5198024_14",
            "document": "Efficient coding hypothesis . In one study by Doi et al. in 2012, the researchers created a predicted response model of the retinal ganglion cells that would be based on the statistics of the natural images used, while considering noise and biological constraints. They then compared the actual information transmission as observed in real retinal ganglion cells to this optimal model to determine the efficiency. They found that the information transmission in the retinal ganglion cells had an overall efficiency of about 80% and concluded that \"the functional connectivity between cones and retinal ganglion cells exhibits unique spatial structure...consistent with coding efficiency. A study by van Hateren and Ruderman in 1998 used ICA to analyze video-sequences and compared how a computer analyzed the independent components of the image to data for visual processing obtained from a cat in DeAngelis et al. 1993. The researchers described the independent components obtained from a video sequence as the \"basic building blocks of a signal\", with the independent component filter (ICF) measuring \"how strongly each building block is present\". They hypothesized that if simple cells are organized to pick out the \"underlying structure\" of images over time then cells should act like the independent component filters. They found that the ICFs determined by the computer were similar to the \"receptive fields\" that were observed in actual neurons.",
            "score": 149.01171875
        },
        {
            "docid": "30883500_3",
            "document": "Magnetomyography . At the early 18th century, the electric signals from living tissues have been investigated. These researchers have promoted many innovations in healthcare especially in medical diagnostic. Some example is based on electrical signals produced by human tissues, including Electrocardiogram (ECG), Electroencephalography (EEG) and Electromyogram (EMG). Besides, with the development of technologies, the biomagnetic measurement from the human body, consisting of Magnetocardiogram (MCG), Magnetoencephalography (MEG) and Magnetomyogram (MMG), provided clear evidence that the existence of the magnetic fields from ionic action currents in electrically active tissues can be utilized to record activities. For the first attempt, Cohen et al. used a point-contact superconducting quantum interference device (SQUID) magnetometer in a shielded room to measure the MCG. They reported that the sensitivity of the recorded MCG was orders of magnitude higher than the previously recorded MCG. The same researcher continued this MEG measurement by using a more sensitive SQUID magnetometer without noise averaging. He compared the EEG and alpha rhythm MEG recorded by both normal and abnormal subjects. It is shown that the MEG has produced some new and different information provided by the EEG. Because the heart can produce a relatively large magnetic field compared to the brain and other organs, the early biomagnetic field research originated from the mathematical modelling of MCG. Early experimental studies also focused on the MCG. In addition, these experimental studies suffer from unavoidable low spatial resolution and low sensitivity due to the lack of sophisticated detection methods. With advances in technology, research has expanded into brain function, and preliminary studies of evoked MEGs began in the 1980s. These studies provided some details about which neuronal populations were contributing to the magnetic signals generated from the brain. However, the signals from single neurons were too weak to be detected. A group of over 10,000 dendrites is required as a group to generate a detectable MEG signal. At the time, the abundance of physical, technical, and mathematical limitations prevented quantitative comparisons of theories and experiments involving human electrocardiograms and other biomagnetic records. Due to the lack of an accurate micro source model, it is more difficult to determine which specific physiological factors influence the strength of MEG and other biomagnetic signals and which factors dominate the achievable spatial resolution. In the past three decades, a great deal of research has been conducted to measure and analyze the magnetic field generated by the flow of ex vivo currents in isolated axons and muscle fibers. These measurements have been supported by some complex theoretical studies and the development of ultra-sensitive room temperature amplifiers and neuromagnetic current probes. Nowadays, cell-level magnetic recording technology has become a quantitative measurement technique for operating currents.",
            "score": 148.865234375
        },
        {
            "docid": "1268939_30",
            "document": "General-purpose computing on graphics processing units . Originally, data was simply passed one-way from a central processing unit (CPU) to a graphics processing unit (GPU), then to a display device. However, as time progressed, it became valuable for GPUs to store at first simple, then complex structures of data to be passed back to the CPU that analyzed an image, or a set of scientific-data represented as a 2D or 3D format that a video card can understand. Because the GPU has access to every draw operation, it can analyze data in these forms quickly, whereas a CPU must poll every pixel or data element much more slowly, as the speed of access between a CPU and its larger pool of random-access memory (or in an even worse case, a hard drive) is slower than GPUs and video cards, which typically contain smaller amounts of more expensive memory that is much faster to access. Transferring the portion of the data set to be actively analyzed to that GPU memory in the form of textures or other easily readable GPU forms results in speed increase. The distinguishing feature of a GPGPU design is the ability to transfer information bidirectionally back from the GPU to the CPU; generally the data throughput in both directions is ideally high, resulting in a multiplier effect on the speed of a specific high-use algorithm. GPGPU pipelines may improve efficiency on especially large data sets and/or data containing 2D or 3D imagery. It is used in complex graphics pipelines as well as scientific computing; more so in fields with large data sets like genome mapping, or where two- or three-dimensional analysis is useful especially at present biomolecule analysis, protein study, and other complex organic chemistry. Such pipelines can also vastly improve efficiency in image processing and computer vision, among other fields; as well as parallel processing generally. Some very heavily optimized pipelines have yielded speed increases of several hundred times the original CPU-based pipeline on one high-use task.",
            "score": 147.27639770507812
        },
        {
            "docid": "3717_58",
            "document": "Brain . The oldest method of studying the brain is anatomical, and until the middle of the 20th century, much of the progress in neuroscience came from the development of better cell stains and better microscopes. Neuroanatomists study the large-scale structure of the brain as well as the microscopic structure of neurons and their components, especially synapses. Among other tools, they employ a plethora of stains that reveal neural structure, chemistry, and connectivity. In recent years, the development of immunostaining techniques has allowed investigation of neurons that express specific sets of genes. Also, \"functional neuroanatomy\" uses medical imaging techniques to correlate variations in human brain structure with differences in cognition or behavior.",
            "score": 146.50233459472656
        },
        {
            "docid": "31824158_5",
            "document": "Sqlstream . SQLstream provides a relational stream processing platform called SQLstream Blaze for analyzing large volumes of service, sensor and machine and log file data in real-time. It performs real-time collection, aggregation, integration, enrichment and real-time analytics on the streaming data. Data streams are analyzed using the industry standard SQL language, using the ANSI standard, functionally rich SQL window function to analyze and aggregate real-time streaming data over fixed or sliding time windows, which can be further partitioned by user defined keys. Unlike a traditional RDBMS SQL query, which returns a result and exits, streaming SQL queries do not exit, generating results continuously as soon as new data become available. Patterns and exception events in data streams are detected, analyzed and reported 'on the fly' as the data arrive, that is, before the data are stored. Like a database or data warehouse, SQLstream allows you to create multiple views over the data so that different applications and users can each get their own customized view of the streaming data. The partitioning allows many different analytics to be incrementally computed using a single SQL statement or window., effectively processing potentially millions of streams with a single statement. For example, partitioning by a customer id would maintain a separate computation for each distinct customer. This is extremely concise, but also allows for efficient parallel execution. SQLstream Blaze also allows changes to be made to the queries and views without bringing down and recompiling existing applications. This is very important for many Internet of Things and other smart services that must operate 24x7 on a continuous real-time basis, where application changes must be made without needing to bringing down the service or rebuild the application. Part of SQLstream Blaze, StreamLab takes advantage of this capability in order to guide users who wish to explore data streams and understand their structure while the data are still flowing by generating new SQL queries on the fly based on user direction and analysis of data values driven by rules. In this way, it provides an effective platform for performing real-time operational intelligence, which you can view as real-time business intelligence over streaming operational data. SQLstream utilizes dataflow technology to execute many queries over high-velocity high-volume Big Data with a massively parallel standards-compliant SQL engine where the queries are executed concurrently and incrementally. Unlike databases, SQL in SQLstream becomes a language for performing continuous parallel processing, in contrast to a language for data retrieval as commonly found in relational databases. SQLstream is able to execute its queries in an optimized C++ multi-threaded dataflow engine which operates lock-free. This enables people to create lock-free parallel processing applications easily, which otherwise require specialist skillets and are often difficult to get working and are often error prone.",
            "score": 146.3579864501953
        },
        {
            "docid": "39592916_16",
            "document": "Digital sociology . Using large data sets, like those obtained from Twitter, can be challenging. First of all, researchers have to figure out how to store this data effectively in a database. Several tools commonly used in Big Data analytics are at their disposal. Since large data sets can be unwieldy and contain numerous types of data (i.e. photos, videos, GIF images), researchers have the option of storing their data in non-relational databases, such as MongoDB and Hadoop. Processing and querying this data is an additional challenge. However, there are several options available to researchers. One common option is to use a querying language, such as Hive, in conjunction with Hadoop to analyze large data sets.",
            "score": 145.94732666015625
        },
        {
            "docid": "37305324_14",
            "document": "National Database for Autism Research . NDAR currently supports the receipt of unprocessed brain images in DICOM format, as well as processed images in variety of formats, including DICOM, MINC 1.0 and 2.0, Analyze, NIfTI-1, AFNI and SPM. Images could be visualized using NDAR\u2019s built-in image registration and visualization tool [MIPAV]. Collaborations are planned with prominent ASD researchers in order to define data structures and develop standardization tools for functional neuroimaging, EEG, TMS, MEG, and eye-tracking.",
            "score": 145.13375854492188
        },
        {
            "docid": "37305324_15",
            "document": "National Database for Autism Research . Investigators working on autism-related projects, regardless of their funding source, are strongly encourages to submit any type of autism-related data generated in their laboratories. After extensive consultations with the research community, NDAR has established a two-tiered submission strategy for investigators receiving NIH funding. Descriptive (raw) data are expected to be submitted biannually in January and July, and includes non-proprietary behavioral and diagnostic data. Examples include standard clinical assessments, family medical history, demographics, unprocessed images, and genomic data. Making this information available early in the research process allows other investigators to understand the general characteristics of the participants enrolled. Experimental (analyzed) data are expected to be submitted within 12 months after accomplishment of each primary aim or objective (or set of interdependent aims or objectives) of the supported research, or at the time of publication of the results of the primary aim(s), whichever occurs first. Examples include outcome measures, analyzed genomics data, results from image analysis, and volumetric data.",
            "score": 144.9669189453125
        },
        {
            "docid": "26065582_10",
            "document": "Preclinical imaging . Cancer research: The study of brain cancers has been significantly hampered by the lack of an easy imaging modality to study animals \"in vivo\". To do so, a craniotomy is often needed, in addition to hours of anesthesia, mechanical ventilation, etc. which significantly alters experimental parameters. For this reason, many researchers have been content to sacrifice animals at different time points and study brain tissue with traditional histological methods. Compared to an \"in vivo\" longitudinal study, many more animals are needed to obtain significant results, and the sensitivity of the entire experiment is cast in doubt. As stated earlier, the problem is not reluctance by researchers to use \"in vivo\" imaging modalities, but rather a lack of suitable ones. For example, although optical imaging provides fast functional data and oxy- and deoxyhemoglobin analysis, it requires a craniotomy and only provides a few hundred micrometres of penetration depth. Furthermore, it is focused on one area of the brain, while research has made it apparently clear that brain function is interrelated as a whole. On the other hand, micro-fMRI is extremely expensive, and offers dismal resolution and image acquisition times when scanning the entire brain. It also provides little vasculature information. Micro-PAT has been demonstrated to be a significant enhancement over existing \"in vivo\" neuro-imaging devices. It is fast, non-invasive, and provides a plethora of data output. Micro-PAT can image the brain with high spatial resolution, detect molecular targeted contrast agents, simultaneously quantify functional parameters such as SO2 and HbT, and provide complementary information from functional and molecular imaging which would be extremely useful in tumor quantification and cell-centered therapeutic analysis.",
            "score": 144.83229064941406
        },
        {
            "docid": "907554_21",
            "document": "History of neuroimaging . To begin with, much of the recent progress has had to do not with the actual brain imaging methods themselves but with our ability to utilize computers in analyzing the data. For example, substantial discoveries in the growth of human brains from age three months to the age of fifteen have been made due to the creation of high-resolution brain maps and computer technology to analyze these maps over various periods of time and growth (Thompson, UCLA). This type of breakthrough represents the nature of most breakthroughs in neuroscience today. With fMRI technology mapping brains beyond what we are already understanding, most innovators time is being spent trying to make sense of the data we already have rather than probing into other realms of brain imaging and mapping.",
            "score": 144.76405334472656
        },
        {
            "docid": "46580631_4",
            "document": "Text Database and Dictionary of Classic Mayan . TWKM will employ digital technologies in order to compile and make available the data and metadata, as well as to publish the project\u2019s research results. The project thereby methodologically positions itself in the field of the digital humanities. The project will be conducted in cooperation with the project partners (below), the research association for the eHumanities TextGrid, as well as the University and Regional Library of Bonn (ULB). The working environment that is currently under construction, in which the data and metadata will be compiled and annotated, will be realized in theTextGrid Laboratory, a software of the virtual research environment. A further component of this software, the TextGrid Repository, will make the data that are authorized for publication freely available online and ensure their long-term storage.  The tools for data compilation and annotation attained from the modularly constructed and extended TextGrid lab thereby provide all the necessary materials for facilitating the research team\u2019s the typical epigraphic workflow. The workflow usually begins by documenting the texts and the objects on which they are preserved, and by compiling descriptive data. It then continues with the various levels of epigraphic and linguistic analysis, and concludes in the best case scenario with a translation of the analyzed inscription and a corresponding publication. In cooperation with the ULB, selected data will additionally be made available. The project\u2019s Virtual Inscription Archive will present online, in the Digital Collections of the ULB, hieroglyphic inscriptions selected from the published data in the repository, including an image of and brief information about the texts and the objects on which they are written, epigraphic analysis, and translation.",
            "score": 144.68826293945312
        },
        {
            "docid": "42744692_11",
            "document": "Marian Diamond . Einstein's Brain: In early 1984, Diamond received four blocks of the preserved brain of Albert Einstein from Thomas Stoltz Harvey. Harvey, pathologist of Princeton Hospital at the time of Einstein's death, had removed Einstein's brain during autopsy in 1955 and maintained personal possession of the brain. The fact that the Einstein brain tissue was already embedded in celloidin when the Diamond lab received it meant that their choice of methods of examination would be somewhat limited. However, they were able to successfully analyze both the superior prefrontal (area 9) and inferior parietal (area 39) association cortices of the left and right hemispheres of Einstein's brain and compare results with the identical regions in the control base of 11 human, male, preserved brains. From previous analysis of the eleven control brains, the Diamond lab \"learned the frontal cortex did have more glial cells/neuron than the parietal cortex.\" After many years of research, Diamond and her team had data proving that, in the rat brain, glial cells increased with enriched conditions, but did not increase with age. Diamond and her associates discovered that the big difference in all four areas was in nonneuronal cells. Einstein had more glial cells per neuron than the average male brains of the control group. Importantly, the biggest difference was found in area 39 of the left hemisphere of Einstein's brain where the increase in the number of glial cells per neuron was statistically significantly greater than in the control brains. Astrocyte and oligodendrocyte glial cells were pooled for these results.",
            "score": 144.66163635253906
        },
        {
            "docid": "38330850_13",
            "document": "Neuronal tracing . A number of neuron tracing tools especially software packages are available. One comprehensive Open Source software package that contains implementation of a number of neuron tracing methods developed in different research groups as well as many neuron utilities functions such as quantitative measurement, parsing, comparison, is Vaa3D and its Vaa3D-Neuron modules (http://vaa3d.org). Some other free tools such as NeuronStudio also provide tracing function based on specific methods. Neuroscientists also use commercial tools such as Neurolucida, Neurolucida 360, Aivia, Amira, etc. to trace and analyse neurons. Recent studies show that Neurolucida is cited over 7 times more than all other available neuron tracing programs combined, and is also the most widely used and versatile system to produce neuronal reconstruction. The BigNeuron project (http://bigneuron.org) is a recent substantial international collaboration effort to integrate the majority of known neuron tracing tools onto a common platform to facilitate Open Source, easy accessing of various tools at one single place. Powerful new tools such as UltraTracer, that can trace arbitrarily large image volume, have been produced through this effort.",
            "score": 144.3190155029297
        },
        {
            "docid": "330879_20",
            "document": "Anthropometry . Direct measurements involve examinations of brains from corpses, or more recently, imaging techniques such as MRI, which can be used on living persons. Such measurements are used in research on neuroscience and intelligence. Brain volume data and other craniometric data are used in mainstream science to compare modern-day animal species and to analyze the evolution of the human species in archeology. With the discovery that many blood proteins vary consistently among populations, followed by the discovery of the DNA code, the invention of the polymerase chain reaction that amplifies trace amounts of DNA, and the decoding of the human genome, phylogeographers largely switched away from craniofacial anthropometry whenever DNA is available.",
            "score": 143.19618225097656
        }
    ]
}