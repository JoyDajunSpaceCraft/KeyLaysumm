{
    "q": [
        {
            "docid": "1534578_6",
            "document": "Motion perception . The phi phenomenon has been referred to as \"first-order\" motion perception. Werner E. Reichardt and Bernard Hassenstein have modelled it in terms of relatively simple \"motion sensors\" in the visual system, that have evolved to detect a change in luminance at one point on the retina and correlate it with a change in luminance at a neighbouring point on the retina after a short delay. Sensors that are proposed to work this way have been referred to as either \"Hassenstein-Reichardt detectors\" after the scientists Bernhard Hassenstein and Werner Reichardt, who first modelled them, motion-energy sensors, or Elaborated Reichardt Detectors. These sensors are described as detecting motion by spatio-temporal correlation and are considered by some to be plausible models for how the visual system may detect motion. (Although, again, the notion of a \"pure motion\" detector suffers from the problem that there is no \"pure motion\" stimulus, i.e. a stimulus lacking perceived figure/ground properties). There is still considerable debate regarding the accuracy of the model and exact nature of this proposed process. It is not clear how the model distinguishes between movements of the eyes and movements of objects in the visual field, both of which produce changes in luminance on points on the retina.",
            "score": 202.6180727481842
        },
        {
            "docid": "29762298_7",
            "document": "Werner E. Reichardt . \"See Motion perception#First-order motion perception\"<br> In the 1950s, Reichardt, along with Hassenstein proposed a model of how a neuron receiving input from photoreceptors, which only respond to changes in luminance, could be used to compute motion. Each photoreceptor, responded to a change in luminance at a given location in visual space. Comparison of the phase shift of activity in adjacent cells indicated the direction of movement from one neuron's receptive field to the other. This model of micro-circuitry became known as a Reichardt detector. Whilst there is experimental evidence consistent with the hypothetical behaviour of a Reichardt detector, the corresponding circuitry has not yet been found.",
            "score": 168.80868005752563
        },
        {
            "docid": "1534578_24",
            "document": "Motion perception . It is now known that motion detection in vision is based on the Hassenstein-Reichardt detector model. This is a model used to detect correlation between the two adjacent points. It consists of two symmetrical subunits. Both subunits have a receptor that can be stimulated by an input (light in the case of visual system). In each subunit, when an input is received, a signal is sent to the other subunit. At the same time, the signal is delayed in time within the subunit, and after the temporal filter, is then multiplied by the signal received from the other subunit. Thus, within each subunit, the two brightness values, one received directly from its receptor with a time delay and the other received from the adjacent receptor, are multiplied. The multiplied values from the two subunits are then subtracted to produce an output. The direction of selectivity or preferred direction is determined by whether the difference is positive or negative. The direction which produces a positive outcome is the preferred direction.",
            "score": 156.257093667984
        },
        {
            "docid": "629599_6",
            "document": "Kismet (robot) . This system processes raw visual and auditory information from cameras and microphones. Kismet's vision system can perform eye detection, motion detection and, albeit controversial, skin-color detection. Whenever Kismet moves its head, it momentarily disables its motion detection system to avoid detecting self-motion. It also uses its stereo cameras to estimate the distance of an object in its visual field, for example to detect threats\u2014large, close objects with a lot of movement.",
            "score": 141.27628755569458
        },
        {
            "docid": "63663_15",
            "document": "Fly . For visual course control, flies' optic flow field is analyzed by a set of motion-sensitive neurons. A subset of these neurons is thought to be involved in using the optic flow to estimate the parameters of self-motion, such as yaw, roll, and sideward translation. Other neurons are thought to be involved in analyzing the content of the visual scene itself, such as separating figures from the ground using motion parallax. The H1 neuron is responsible for detecting horizontal motion across the entire visual field of the fly, allowing the fly to generate and guide stabilizing motor corrections midflight with respect to yaw. The ocelli are concerned in the detection of changes in light intensity, enabling the fly to react swiftly to the approach of an object.",
            "score": 157.30125093460083
        },
        {
            "docid": "30797574_69",
            "document": "History of computer animation . Computer-based motion capture started as a photogrammetric analysis tool in biomechanics research in the 70s and 80s. A performer wears markers near each joint to identify the motion by the positions or angles between the markers. Many different types of markers can be used\u2014lights, reflective markers, LEDs, infra-red, inertial, mechanical, or wireless RF\u2014and may be worn as a form of suit, or attached direct to a performer's body. Some systems include details of face and fingers to capture subtle expressions, and such is often referred to as \"performance capture\". The computer records the data from the markers, and uses it to animate digital character models in 2D or 3D computer animation, and in some cases this can include camera movement as well. In the 90s, these techniques became widely used for visual effects. Video games also began to use motion capture to animate in-game characters in 1995, the earliest examples of this being the Atari Jaguar CD-based game \"\", and the arcade fighting game \"Soul Edge\", which was the first video game to use passive optical motion-capture technology.",
            "score": 147.98647105693817
        },
        {
            "docid": "887388_4",
            "document": "Dragonslayer (1981 film) . The special effects were created at Industrial Light and Magic, where Phil Tippett had co-developed an animation technique called go motion for \"The Empire Strikes Back\" (1980). Go motion is a variation on stop motion animation, and its use in \"Dragonslayer\" led to the film's nomination for the Academy Award for Visual Effects; it lost to \"Raiders of the Lost Ark\", the only other Visual Effects nominee that year, whose special effects were also provided by ILM. Including the hydraulic model, 16 dragon puppets were used for the role of Vermithrax, each one made for different movements; flying, crawling, fire breathing etc. \"Dragonslayer\" also marks the first time ILM's services were used for a film other than a Lucasfilm Ltd. production.",
            "score": 135.93239569664001
        },
        {
            "docid": "49189913_4",
            "document": "MOVIE Index . The MOVIE index is a neuroscience-based model for predicting the perceptual quality of a (possibly compressed or otherwise distorted) motion picture or video against a pristine reference video. Thus, the MOVIE index is a full-reference metric. The MOVIE model is quite different from many other models since it uses neuroscience-based models of how the human brain processes visual signals at various stages along the visual pathway, including the lateral geniculate nucleus, primary visual cortex, and in the motion-sensitive extrastriate cortex visual area MT.",
            "score": 171.58097004890442
        },
        {
            "docid": "2905826_3",
            "document": "Path integration . Charles Darwin first postulated an inertially-based navigation system in animals in 1873. Studies beginning in the middle of the 20th century confirmed that animals could return directly to a starting point, such as a nest, in the absence of vision and having taken a circuitous outwards journey. This shows that they can use cues to track distance and direction in order to estimate their position, and hence how to get home. This process was named \"path integration\" to capture the concept of continuous integration of movement cues over the journey. Manipulation of inertial cues confirmed that at least one of these movement (or \"idiothetic\") cues is information from the vestibular organs, which detect movement in the three dimensions. Other cues probably include proprioception (information from muscles and joints about limb position), motor efference (information from the motor system telling the rest of the brain what movements were commanded and executed), and optic flow (information from the visual system signaling how fast the visual world is moving past the eyes). Together, these sources of information can tell the animal which direction it is moving, at what speed, and for how long. In addition, sensitivity to the earth's magnetic field for underground animals (e.g., mole rat) can give path integration.",
            "score": 114.33678889274597
        },
        {
            "docid": "22396342_7",
            "document": "H1 neuron . Flies are agile flyers and strongly depend on vision during flight. For visual course control, flies optic flow field is analyzed by a set of \u223c60 motion-sensitive neurons, each present in the third visual neuropil of the left and right eyes. A subset of these neurons is thought to be involved in using the optic flow to estimate the parameters of self-motion, such as yaw, roll, and sideward translation. Other neurons are thought to be involved in analyzing the content of the visual scene itself, for example, to separate figure from ground using motion parallax. The H1 neuron is responsible for detecting horizontal motion across the entire visual field of the fly, allowing the fly to generate and guide stabilizing motor corrections mid-flight with respect to yaw.",
            "score": 155.19430327415466
        },
        {
            "docid": "44674478_19",
            "document": "Rigid motion segmentation . Motion segmentation is a field under research as there are many issues which provide scope of improvement. One of the major problems is of feature detection and finding correspondences.There are strong feature detection algorithms but they still give false positives which can lead to unexpected correspondences. Finding these pixel or feature correspondences is a difficult task. These mismatched feature points from the objects and the background often introduce outliers. The presence of image noise and outliers further affect the accuracy of structure from motion (SFM) estimation. Another issue is that of motion models or motion representations. It requires the motion to be modeled or estimated in the given model used in the algorithm. Most algorithms perform 2-D motion segmentation by assuming the motions in the scene can be modeled by 2-D affine motion models. Theoretically, this is valid because 2-D translational motion model can be represented by general affine motion model. However, such approximations in modeling can have negative consequences. The translational model has two parameters and the affine model has 6 parameters so we estimate four extra parameters. Moreover, there may not be enough data to estimate the affine motion model so the parameter estimation might be erroneous. Some of the other problems faced are:",
            "score": 175.4286985397339
        },
        {
            "docid": "44674478_11",
            "document": "Rigid motion segmentation . Image segmentation techniques are interested in segmenting out different parts of the image as per the region of interest. As videos are sequences of images, motion segmentation aims at decomposing a video in moving objects and background by segmenting the objects that undergo different motion patterns. The analysis of these spatial and temporal changes occurring in the image sequence by separating visual features from the scenes into different groups lets us extract visual information. Each group corresponds to the motion of an object in the dynamic sequence. In the simplest case motion segmentation can mean extracting moving objects from a stationary camera but the camera can also move which introduces the relative motion of the static background. Depending upon the type of visual features that are extracted, motion segmentation algorithms can be broadly divided into two categories. The first is known as direct motion segmentation that uses pixel intensities from the image. Such algorithms assume constant illumination. The second category of algorithms computes a set of features corresponding to actual physical points on the objects. These sparse features are then used to characterize either the 2-D motion of the scene or the 3-D motion of the objects in the scene. There are a number of requirements to design a good motion segmentation algorithm. The algorithm must extract distinct features (corners or salient points) that represent the object by a limited number of points and it must have the ability to deal with occlusions. The images will also be affected by noise and will have missing data, thus they must be robust. Some algorithms detect only one object but the video sequence may have different motions. Thus the algorithm must be multiple object detectors. Moreover, the type of camera model, if used, also characterizes the algorithm. Depending upon the object characterization of an algorithm it can detect rigid, non-rigid motion or both. Moreover, algorithms used to estimate single rigid-body motions can provide accurate results with robustness to noise and outliers but when extended to multiple rigid-body motions they fail. In case of view-based segmentation techniques described below, this happens because the single fundamental matrix assumption is violated as each motion will now be represented by means of a new fundamental matrix corresponding to that motion.",
            "score": 175.66854560375214
        },
        {
            "docid": "29762298_6",
            "document": "Werner E. Reichardt . Reichardt's findings have contributed to understanding of information processing in nervous systems. From joint work (with Bernhard Hassenstein and Hans Wenking) on the visual system of insects and its effect on the flight orientation, the correlation model developed the idea that the visual system of man could be similarly investigated, and led to a general theory of motion perception",
            "score": 151.6642506122589
        },
        {
            "docid": "3883287_8",
            "document": "Tranquillity . Within tranquillity studies, much of the emphasis has been placed on understanding the role of vision in the perception of natural environments, which is probably not surprising, considering that upon first viewing a scene its configurational coherence can be established with incredible speed. Indeed, scene information can be captured in a single glance and the gist of a scene determined in as little as 100ms. The speed of processing of complex natural images was tested by Thorpe \"et al.\" using colour photographs of a wide range of animals (mammals, birds, reptiles and fish), in their natural environments, mixed with distracters that included pictures of forests, mountains, lakes, buildings and fruit. During this experiment, subjects were shown an image for 20ms and asked to determine whether it contained an animal or not. The electrophysiological brain responses obtained in this study showed that a decision could be made within 150ms of the image being seen, indicating the speed at which cognitive visual processing occurs. However, audition, and in particular the individual components that collectively comprise the soundscape, a term coined by Schafer to describe the ever-present array of sounds that constitute the sonic environment, also significantly inform the various schemata used to characterise differing landscape types. This interpretation is supported by the auditory reaction times, which are 50 to 60ms faster than that of the visual modality. It is also known that sound can alter visual perception and that under certain conditions areas of the brain involved in processing auditory information can be activated in response to visual stimuli.  Research conducted by Pheasant \"et al.\" has shown that when individuals make tranquillity assessments based on a uni-modal auditory or visual sensory input, they characterise the environment by drawing upon a number of key landscape and soundscape characteristics. For example, when making assessments in response to visual-only stimuli the percentage of water, flora and geological features present within a scene, positively influence how tranquil a location is perceived to be. Likewise when responding to uni-modal auditory stimuli, the perceived loudness of biological sounds positively influences the perception of tranquillity, whilst the perceived loudness of mechanical sounds have a negative effect. However, when presented with bi-modal auditory-visual stimuli the individual soundscape and landscape components alone no longer influenced the perception of tranquillity. Rather configurational coherence was provided by the percentage of natural and contextual features present within the scene and the equivalent continuous sound pressure level (LAeq).",
            "score": 179.0663627386093
        },
        {
            "docid": "6446_40",
            "document": "Camouflage . An animal that is commonly thought to be dazzle-patterned is the zebra. The bold stripes of the zebra have been claimed to be disruptive camouflage, background-blending and countershading. After many years in which the purpose of the coloration was disputed, an experimental study by Tim Caro suggested in 2012 that the pattern reduces the attractiveness of stationary models to biting flies such as horseflies and tsetse flies. However, a simulation study by Martin How and Johannes Zanker in 2014 suggests that when moving, the stripes may confuse observers, such as mammalian predators and biting insects, by two visual illusions: the wagon-wheel effect, where the perceived motion is inverted, and the barberpole illusion, where the perceived motion is in a wrong direction.",
            "score": 133.41380906105042
        },
        {
            "docid": "4236583_2",
            "document": "Visual search . Visual search is a type of perceptual task requiring attention that typically involves an active scan of the visual environment for a particular object or feature (the target) among other objects or features (the distractors). Visual search can take place with or without eye movements. The ability to consciously locate an object or target amongst a complex array of stimuli has been extensively studied over the past 40 years. Practical examples of using visual search can be seen in everyday life, such as when one is picking out a product on a supermarket shelf, when animals are searching for food amongst piles of leaves, when trying to find your friend in a large crowd of people, or simply when playing visual search games such as \"Where's Wally?\" Many visual search paradigms have used eye movement as a means to measure the degree of attention given to stimuli. However, vast research to date suggests that eye movements move independently of attention, and therefore are not a reliable method to examine the role of attention. Much previous literature on visual search uses reaction time in order to measure the time it takes to detect the target amongst its distractors. An example of this could be a green square (the target) amongst a set of red circles (the distractors).",
            "score": 117.01087236404419
        },
        {
            "docid": "42938624_20",
            "document": "Biological motion perception . This model highlights the abilities of form-related cues to detect biological motion and orientation in a neurologically feasible model. The results of the Stage 1 model showed that all behavioral data could be replicated by using form information alone - global motion information was not necessary to detect figures and their orientation. This model shows the possibility of the use of form cues, but can be criticized for a lack of ecological validity. Humans do not detect biological figures in static environments and motion is an inherent aspect in upright figure recognition.",
            "score": 145.9669530391693
        },
        {
            "docid": "18315951_14",
            "document": "Visual odometry . Features are detected in the first frame, and then matched in the second frame. This information is then used to make the optical flow field for the detected features in those two images. The optical flow field illustrates how features diverge from a single point, the \"focus of expansion\". The focus of expansion can be detected from the optical flow field, indicating the direction of the motion of the camera, and thus providing an estimate of the camera motion.",
            "score": 129.38292813301086
        },
        {
            "docid": "44674478_13",
            "document": "Rigid motion segmentation . It is a very useful technique for detecting changes in images due to its simplicity and ability to deal with occlusion and multiple motions. These techniques assume constant light source intensity. The algorithm first considers two frames at a time and then computes the pixel by pixel intensity difference. On this computation it thresholds the intensity difference and maps the changes onto a contour. Using this contour it extracts the spatial and temporal information required to define the motion in the scene. Though it is a simple technique to implement it is not robust to noise. Another difficulty with these techniques is the camera movement. When the camera moves there is a change in the entire image which has to be accounted for. Many new algorithm have been introduced to overcome these difficulties. Motion segmentation can be seen as a classification problem where each pixel has to be classified as background or foreground. Such classifications are modeled under statistic theory and can be used in segmentation algorithms. These approaches can be further divided depending on the statistical framework used. Most commonly used frameworks are maximum a posteriori probability (MAP), Particle Filter (PF) and Expectation Maximization (EM). MAP uses Bayes' Rule for implementation where a particular pixel has to be classified under predefined classes. PF is based on the concept of evolution of a variable with varying weights over time. The final estimation is the weighted sum of all the variables. Both of these methods are iterative. The EM algorithm is also an iterative estimation method. It computes the maximum likelihood (ML) estimate of the model parameters in presence of missing or hidden data and decided the most likely fit of the observed data.",
            "score": 142.51106083393097
        },
        {
            "docid": "33438935_6",
            "document": "Hydrodynamic reception . Since movement of an object through water inevitably creates movement of the water itself, and this resulting water motion persists and travels, the detection of hydrodynamic stimuli is useful for sensing conspecifics, predators, and prey. Many studies are based upon the question of how an aquatic organism can capture prey despite darkness or apparent lack of visual or other sensory systems and find that the sensing of hydrodynamic stimuli left by prey is probably responsible. As for detection of conspecifics, harbor seal pups will enter the water with their mother, but eventually ascend to obtain oxygen, and then dive again to rejoin the mother. Observations suggest that the tracking of water movements produced by the mother and other pups allows this rejoining to occur. Through these trips and the following of conspecifics, pups might learn routes to avoid predators and good places to find food, showing the possible significance of hydrodynamic detection to these seals.",
            "score": 133.81895756721497
        },
        {
            "docid": "34903260_29",
            "document": "Animal navigation . Path integration in mammals makes use of the vestibular organs, which detect accelerations in the three dimensions, together with motor efference, where the motor system tells the rest of the brain which movements were commanded, and optic flow, where the visual system signals how fast the visual world moves past the eyes. Information from other senses such as echolocation and magnetoreception may also be integrated in certain animals. The hippocampus is the part of the brain that integrates linear and angular motion to encode a mammal's relative position in space.",
            "score": 126.62476134300232
        },
        {
            "docid": "32528_41",
            "document": "Visual cortex . However, since neurons in V1 are also tuned to the direction and speed of motion, these early results left open the question of precisely what MT could do that V1 could not. Much work has been carried out on this region, as it appears to integrate local visual motion signals into the global motion of complex objects. For example, \"lesion\" to the V5 leads to deficits in perceiving motion and processing of complex stimuli. It contains many neurons selective for the motion of complex visual features (line ends, corners). \"Microstimulation\" of a neuron located in the V5 affects the perception of motion. For example, if one finds a neuron with preference for upward motion in a monkey's V5 and stimulates it with an electrode, then the monkey becomes more likely to report 'upward' motion when presented with stimuli containing 'left' and 'right' as well as 'upward' components.",
            "score": 177.853382229805
        },
        {
            "docid": "5620745_5",
            "document": "Motion camouflage . Many animals are highly sensitive to motion; for example, frogs readily detect small moving dark spots but ignore stationary ones. Therefore, motion signals can be used to defeat camouflage. Moving objects with disruptive camouflage patterns remain harder to identify than uncamouflaged objects, especially if other similar objects are nearby, even though they are detected, so motion does not completely 'break' camouflage. All the same, the conspicuousness of motion raises the question of whether and how motion itself could be camouflaged. Several mechanisms are possible.",
            "score": 108.8993239402771
        },
        {
            "docid": "22396342_2",
            "document": "H1 neuron . The H1 neuron is located in the visual cortex of true flies of the order Diptera and mediates motor responses to visual stimuli. H1 is sensitive to horizontal motion in the visual field and enables the fly to rapidly and accurately respond to optic flow with motor corrections to stabilize flight. It is particularly responsive to horizontal forward motion associated with movement of the fly\u2019s own body during flight. Damage to H1 impairs the fly\u2019s ability to counteract disturbances during flight, suggesting that it is a necessary component of the optomotor response. H1 is an ideal system for studying the neural basis of information processing due to its highly selective and predictable responses to stimuli. Since the initial anatomical and physiological characterizations of H1 in 1976, study of the neuron has greatly benefited the understanding of neural coding in a wide range of organisms, especially the relationship between the neural code and behavior.",
            "score": 159.30162811279297
        },
        {
            "docid": "2315029_13",
            "document": "Neural adaptation . Adaptation is considered to be the cause of perceptual phenomena like afterimages and the motion aftereffect. In the absence of fixational eye movements, visual perception may fade out or disappear due to neural adaptation. (See Adaptation (eye)). When an observer's visual stream adapts to a single direction of real motion, imagined motion can be perceived at various speeds. If the imagined motion is in the same direction as that experienced during adaptation, imagined speed is slowed; when imagined motion is in the opposite direction, its speed is increased; when adaptation and imagined motions are orthogonal, imagined speed is unaffected. Studies using magnetoencephalography (MEG) have demonstrated that subjects exposed to a repeated visual stimulus at brief intervals become attenuated to the stimulus in comparison to the initial stimulus. The results revealed that visual responses to the repeated compared with novel stimulus showed a significant reduction in both activation strength and peak latency but not in the duration of neural processing.",
            "score": 146.1644036769867
        },
        {
            "docid": "16928506_13",
            "document": "Visual servoing . Over the years many hybrid techniques have been developed. These involve computing partial/complete pose from Epipolar Geometry using multiple views or multiple cameras. The values are obtained by direct estimation or through a learning or a statistical scheme. While others have used a switching approach that changes between image-based and position-based based on a Lyapnov function. The early hybrid techniques that used a combination of image-based and pose-based (2D and 3D information) approaches for servoing required either a full or partial model of the object in order to extract the pose information and used a variety of techniques to extract the motion information from the image. used an affine motion model from the image motion in addition to a rough polyhedral CAD model to extract the object pose with respect to the camera to be able to servo onto the object (on the lines of PBVS).",
            "score": 142.51902437210083
        },
        {
            "docid": "425938_13",
            "document": "Animal cognition . Animals process information from eyes, ears, and other sensory organs to perceive the environment. Perceptual processes have been studied in many species, with results that are often similar to those in humans. Equally interesting are those perceptual processes that differ from, or go beyond those found in humans, such as echolocation in bats and dolphins, motion detection by skin receptors in fish, and extraordinary visual acuity, motion sensitivity and ability to see ultraviolet light in some birds.",
            "score": 124.6971755027771
        },
        {
            "docid": "1942022_64",
            "document": "Motion simulator . Motion or simulator sickness: Simulators work by \u201ctricking\u201d the mind into believing that the inputs it is receiving from visual, vestibular and proprioceptive inputs are a specific type of desired motion. When any of the cues received by the brain do not correlate with the others, motion sickness can occur. In principle, simulator sickness is simply a form of motion sickness that can result from discrepancies between the cues from the three physical source inputs. For example, riding on a ship with no windows sends a cue that the body is accelerating and rotating in various directions from the vestibular system, but the visual system sees no motion since the room is moving in the same manner as the occupant. In this situation, many would feel motion sickness.",
            "score": 133.53722071647644
        },
        {
            "docid": "43382971_5",
            "document": "Claude Desplan . His laboratory has demonstrated the molecular mechanisms that pattern the fly color-sensing photoreceptor neurons and showed how stochastic decisions, a transcription factor network, and a tumor suppressor pathway contribute to the diversification of photoreceptors. It has also sought to understand how color information that arises within the retina is processed in the optic lobe of the Drosophila brain by investigating the development and function of this structure. His lab has shown that neuronal diversity in optic lobes is generated by the lineage of neural stem cells and by spatial input from patterning genes, and has provided a functional understanding of the neuronal and computational mechanisms that underlie color vision and motion detection (the \u2018elementary motion detector\u2019). The Desplan laboratory also uses \u2018evo-devo\u2019 approaches to understand the evolution of patterning mechanisms in the early embryo and in the visual system using the wasp Nasonia and the ant Harpegnathos as model systems. He contributed broadly to the understanding of how insect embryos pattern their antero-posterior axis through extensive rewiring of a network of genes that are otherwise evolutionarily conserved in Drosophila. Desplan serves on multiple scientific advisory boards and in funding agencies. He is an elected member of the American Association for the Advancement of Science, an elected foreign member of EMBO, and an elected fellow of the New York Academy of Sciences.",
            "score": 127.57126498222351
        },
        {
            "docid": "19649338_5",
            "document": "Biological motion . Humans perception of biological motion matures with age, usually capping at around five years of age. In an experiment by Pavlova et al., 2001, three-year olds, four-year olds, five-year olds, and adults were asked to identify animals (walking human, running dog, flying bird, and walking dog) displayed with moving point-light displays. The adults and five-year olds were able to accurately identify the moving animal, while four-year olds and three-year olds struggled (although 4-year olds were significantly better than 3-year olds). This implies that our perception of motion gets better as we age and reach the cap at five years. While animals tend to recognize their own species over others (for example, cats recognize their own point-light displays over scrambled ones), the three-year olds had the most success identifying a walking dog and least with a walking human. A possible explanation of this might be because of the children's small physical stature and their resulting visual perspectives: dogs are on the same horizontal plane as small kids, while human walkers are a lot taller and harder to perceive. In the next part of the experiment, different participants were asked to identify the same point-light display animals but with static images instead of moving dots. Five-year olds and adults gave results of chance performance, while the younger participants were omitted due to the harder nature of the task. Therefore, this experiment suggests that at five-years old, we are adept at identifying animals with point-light displays and that motion is critical to how we perceive it.",
            "score": 133.16188216209412
        },
        {
            "docid": "16928506_14",
            "document": "Visual servoing . 2-1/2-D visual servoing developed by Malis et al. is a well known technique that breaks down the information required for servoing into an organized fashion which decouples rotations and translations. The papers assume that the desired pose is known a priori. The rotational information is obtained from partial pose estimation, a homography, (essentially 3D information) giving an axis of rotation and the angle (by computing the eigenvalues and eigenvectors of the homography). The translational information is obtained from the image directly by tracking a set of feature points. The only conditions being that the feature points being tracked never leave the field of view and that a depth estimate be predetermined by some off-line technique. 2-1/2-D servoing has been shown to be more stable than the techniques that preceded it. Another interesting observation with this formulation is that the authors claim that the visual Jacobian will have no singularities during the motions. The hybrid technique developed by Corke and Hutchinson, popularly called portioned approach partitions the visual (or image) Jacobian into motions (both rotations and translations) relating X and Y axes and motions related to the Z axis. outlines the technique, to break out columns of the visual Jacobian that correspond to the Z axis translation and rotation (namely, the third and sixth columns). The partitioned approach is shown to handle the Chaumette Conundrum discussed in. This technique requires a good depth estimate in order to function properly. namely main and secondary. The main task is keep the features of inter- est within the field of view. While the secondary task is to mark a fixation point and use it as a reference to bring the camera to the desired pose. The technique does need a depth estimate from an off-line procedure. The paper discusses two examples for which depth estimates are obtained from robot odometry and by assuming that all features are on a plane. The secondary task is achieved by using the notion of parallax. The features that are tracked are chosen by an initialization performed on the first frame, which are typi- cally points. modeling and model-based tracking. Primary assumption made is that the 3D model of the object is available. The authors highlights the notion that ideal features should be chosen such that the DOF of motion can be decoupled by linear relation. The authors also introduce an estimate of the target velocity into the interaction matrix to improve tracking performance. The results are compared to well known servoing techniques even when occlusions occur.",
            "score": 162.7312604188919
        },
        {
            "docid": "5442380_10",
            "document": "Sensory cue . The visual system can detect motion both using a simple mechanism based on information from multiple clusters of neurons as well as by aggregate through by integrating multiple cues including contrast, form, and texture. One major source of visual information when determining self-motion is optic flow. Optic flow not only indicates whether an agent is moving but in which direction and at what relative speed.",
            "score": 127.1030445098877
        }
    ],
    "r": [
        {
            "docid": "1534578_6",
            "document": "Motion perception . The phi phenomenon has been referred to as \"first-order\" motion perception. Werner E. Reichardt and Bernard Hassenstein have modelled it in terms of relatively simple \"motion sensors\" in the visual system, that have evolved to detect a change in luminance at one point on the retina and correlate it with a change in luminance at a neighbouring point on the retina after a short delay. Sensors that are proposed to work this way have been referred to as either \"Hassenstein-Reichardt detectors\" after the scientists Bernhard Hassenstein and Werner Reichardt, who first modelled them, motion-energy sensors, or Elaborated Reichardt Detectors. These sensors are described as detecting motion by spatio-temporal correlation and are considered by some to be plausible models for how the visual system may detect motion. (Although, again, the notion of a \"pure motion\" detector suffers from the problem that there is no \"pure motion\" stimulus, i.e. a stimulus lacking perceived figure/ground properties). There is still considerable debate regarding the accuracy of the model and exact nature of this proposed process. It is not clear how the model distinguishes between movements of the eyes and movements of objects in the visual field, both of which produce changes in luminance on points on the retina.",
            "score": 202.61807250976562
        },
        {
            "docid": "43971138_2",
            "document": "Stereoscopic motion . Stereoscopic motion, as introduced by B\u00e9la Julesz in his book \"Foundations of Cyclopean Perception\" of 1971, is a translational motion of figure boundaries defined by changes in binocular disparity over time in a real-life 3D scene, a 3D film or other stereoscopic scene. This translational motion gives rise to a mental representation of three dimensional motion created in the brain on the basis of the binocular motion stimuli. Whereas the motion stimuli as presented to the eyes have a different direction for each eye, the stereoscopic motion is perceived as yet another direction on the basis of the views of both eyes taken together. Stereoscopic motion, as it is perceived by the brain, is also referred to as \"cyclopean motion\", and the processing of visual input that takes place in the visual system relating to stereoscopic motion is called \"stereoscopic motion processing\".",
            "score": 186.23876953125
        },
        {
            "docid": "1534578_12",
            "document": "Motion perception . As in other aspects of vision, the observer's visual input is generally insufficient to determine the true nature of stimulus sources, in this case their velocity in the real world. In monocular vision for example, the visual input will be a 2D projection of a 3D scene. The motion cues present in the 2D projection will by default be insufficient to reconstruct the motion present in the 3D scene. Put differently, many 3D scenes will be compatible with a single 2D projection. The problem of motion estimation generalizes to binocular vision when we consider occlusion or motion perception at relatively large distances, where binocular disparity is a poor cue to depth. This fundamental difficulty is referred to as the inverse problem.",
            "score": 184.1903076171875
        },
        {
            "docid": "3883287_8",
            "document": "Tranquillity . Within tranquillity studies, much of the emphasis has been placed on understanding the role of vision in the perception of natural environments, which is probably not surprising, considering that upon first viewing a scene its configurational coherence can be established with incredible speed. Indeed, scene information can be captured in a single glance and the gist of a scene determined in as little as 100ms. The speed of processing of complex natural images was tested by Thorpe \"et al.\" using colour photographs of a wide range of animals (mammals, birds, reptiles and fish), in their natural environments, mixed with distracters that included pictures of forests, mountains, lakes, buildings and fruit. During this experiment, subjects were shown an image for 20ms and asked to determine whether it contained an animal or not. The electrophysiological brain responses obtained in this study showed that a decision could be made within 150ms of the image being seen, indicating the speed at which cognitive visual processing occurs. However, audition, and in particular the individual components that collectively comprise the soundscape, a term coined by Schafer to describe the ever-present array of sounds that constitute the sonic environment, also significantly inform the various schemata used to characterise differing landscape types. This interpretation is supported by the auditory reaction times, which are 50 to 60ms faster than that of the visual modality. It is also known that sound can alter visual perception and that under certain conditions areas of the brain involved in processing auditory information can be activated in response to visual stimuli.  Research conducted by Pheasant \"et al.\" has shown that when individuals make tranquillity assessments based on a uni-modal auditory or visual sensory input, they characterise the environment by drawing upon a number of key landscape and soundscape characteristics. For example, when making assessments in response to visual-only stimuli the percentage of water, flora and geological features present within a scene, positively influence how tranquil a location is perceived to be. Likewise when responding to uni-modal auditory stimuli, the perceived loudness of biological sounds positively influences the perception of tranquillity, whilst the perceived loudness of mechanical sounds have a negative effect. However, when presented with bi-modal auditory-visual stimuli the individual soundscape and landscape components alone no longer influenced the perception of tranquillity. Rather configurational coherence was provided by the percentage of natural and contextual features present within the scene and the equivalent continuous sound pressure level (LAeq).",
            "score": 179.0663604736328
        },
        {
            "docid": "32528_41",
            "document": "Visual cortex . However, since neurons in V1 are also tuned to the direction and speed of motion, these early results left open the question of precisely what MT could do that V1 could not. Much work has been carried out on this region, as it appears to integrate local visual motion signals into the global motion of complex objects. For example, \"lesion\" to the V5 leads to deficits in perceiving motion and processing of complex stimuli. It contains many neurons selective for the motion of complex visual features (line ends, corners). \"Microstimulation\" of a neuron located in the V5 affects the perception of motion. For example, if one finds a neuron with preference for upward motion in a monkey's V5 and stimulates it with an electrode, then the monkey becomes more likely to report 'upward' motion when presented with stimuli containing 'left' and 'right' as well as 'upward' components.",
            "score": 177.85337829589844
        },
        {
            "docid": "44674478_11",
            "document": "Rigid motion segmentation . Image segmentation techniques are interested in segmenting out different parts of the image as per the region of interest. As videos are sequences of images, motion segmentation aims at decomposing a video in moving objects and background by segmenting the objects that undergo different motion patterns. The analysis of these spatial and temporal changes occurring in the image sequence by separating visual features from the scenes into different groups lets us extract visual information. Each group corresponds to the motion of an object in the dynamic sequence. In the simplest case motion segmentation can mean extracting moving objects from a stationary camera but the camera can also move which introduces the relative motion of the static background. Depending upon the type of visual features that are extracted, motion segmentation algorithms can be broadly divided into two categories. The first is known as direct motion segmentation that uses pixel intensities from the image. Such algorithms assume constant illumination. The second category of algorithms computes a set of features corresponding to actual physical points on the objects. These sparse features are then used to characterize either the 2-D motion of the scene or the 3-D motion of the objects in the scene. There are a number of requirements to design a good motion segmentation algorithm. The algorithm must extract distinct features (corners or salient points) that represent the object by a limited number of points and it must have the ability to deal with occlusions. The images will also be affected by noise and will have missing data, thus they must be robust. Some algorithms detect only one object but the video sequence may have different motions. Thus the algorithm must be multiple object detectors. Moreover, the type of camera model, if used, also characterizes the algorithm. Depending upon the object characterization of an algorithm it can detect rigid, non-rigid motion or both. Moreover, algorithms used to estimate single rigid-body motions can provide accurate results with robustness to noise and outliers but when extended to multiple rigid-body motions they fail. In case of view-based segmentation techniques described below, this happens because the single fundamental matrix assumption is violated as each motion will now be represented by means of a new fundamental matrix corresponding to that motion.",
            "score": 175.66854858398438
        },
        {
            "docid": "44674478_19",
            "document": "Rigid motion segmentation . Motion segmentation is a field under research as there are many issues which provide scope of improvement. One of the major problems is of feature detection and finding correspondences.There are strong feature detection algorithms but they still give false positives which can lead to unexpected correspondences. Finding these pixel or feature correspondences is a difficult task. These mismatched feature points from the objects and the background often introduce outliers. The presence of image noise and outliers further affect the accuracy of structure from motion (SFM) estimation. Another issue is that of motion models or motion representations. It requires the motion to be modeled or estimated in the given model used in the algorithm. Most algorithms perform 2-D motion segmentation by assuming the motions in the scene can be modeled by 2-D affine motion models. Theoretically, this is valid because 2-D translational motion model can be represented by general affine motion model. However, such approximations in modeling can have negative consequences. The translational model has two parameters and the affine model has 6 parameters so we estimate four extra parameters. Moreover, there may not be enough data to estimate the affine motion model so the parameter estimation might be erroneous. Some of the other problems faced are:",
            "score": 175.42869567871094
        },
        {
            "docid": "49189913_4",
            "document": "MOVIE Index . The MOVIE index is a neuroscience-based model for predicting the perceptual quality of a (possibly compressed or otherwise distorted) motion picture or video against a pristine reference video. Thus, the MOVIE index is a full-reference metric. The MOVIE model is quite different from many other models since it uses neuroscience-based models of how the human brain processes visual signals at various stages along the visual pathway, including the lateral geniculate nucleus, primary visual cortex, and in the motion-sensitive extrastriate cortex visual area MT.",
            "score": 171.58096313476562
        },
        {
            "docid": "505717_72",
            "document": "Image segmentation . Pulse-coupled neural networks (PCNNs) are neural models proposed by modeling a cat\u2019s visual cortex and developed for high-performance biomimetic image processing. In 1989, Eckhorn introduced a neural model to emulate the mechanism of a cat\u2019s visual cortex. The Eckhorn model provided a simple and effective tool for studying the visual cortex of small mammals, and was soon recognized as having significant application potential in image processing. In 1994, the Eckhorn model was adapted to be an image processing algorithm by Johnson, who termed this algorithm Pulse-Coupled Neural Network. Over the past decade, PCNNs have been utilized for a variety of image processing applications, including: image segmentation, feature generation, face extraction, motion detection, region growing, noise reduction, and so on. A PCNN is a two-dimensional neural network. Each neuron in the network corresponds to one pixel in an input image, receiving its corresponding pixel\u2019s color information (e.g. intensity) as an external stimulus. Each neuron also connects with its neighboring neurons, receiving local stimuli from them. The external and local stimuli are combined in an internal activation system, which accumulates the stimuli until it exceeds a dynamic threshold, resulting in a pulse output. Through iterative computation, PCNN neurons produce temporal series of pulse outputs. The temporal series of pulse outputs contain information of input images and can be utilized for various image processing applications, such as image segmentation and feature generation. Compared with conventional image processing means, PCNNs have several significant merits, including robustness against noise, independence of geometric variations in input patterns, capability of bridging minor intensity variations in input patterns, etc.",
            "score": 170.7381134033203
        },
        {
            "docid": "42938624_21",
            "document": "Biological motion perception . Old models of biological motion perception are concerned with tracking joint and limb motion relative to one another over time. However, recent experiments in biological motion perception have suggested that motion information is unimportant for action recognition. This model shows how biological motion may be perceived from sequences of posture recognition, rather than from the direct perception of motion information. An experiment was conducted to test the validity of this model, in which subjects are presented moving point-light and stick-figure walking stimuli. Each frame of the walking stimulus is matched to a posture template, the progression of which is recorded on a 2D posture\u2013time plot that implies motion recognition.",
            "score": 169.628173828125
        },
        {
            "docid": "29762298_7",
            "document": "Werner E. Reichardt . \"See Motion perception#First-order motion perception\"<br> In the 1950s, Reichardt, along with Hassenstein proposed a model of how a neuron receiving input from photoreceptors, which only respond to changes in luminance, could be used to compute motion. Each photoreceptor, responded to a change in luminance at a given location in visual space. Comparison of the phase shift of activity in adjacent cells indicated the direction of movement from one neuron's receptive field to the other. This model of micro-circuitry became known as a Reichardt detector. Whilst there is experimental evidence consistent with the hypothetical behaviour of a Reichardt detector, the corresponding circuitry has not yet been found.",
            "score": 168.8086700439453
        },
        {
            "docid": "16928506_14",
            "document": "Visual servoing . 2-1/2-D visual servoing developed by Malis et al. is a well known technique that breaks down the information required for servoing into an organized fashion which decouples rotations and translations. The papers assume that the desired pose is known a priori. The rotational information is obtained from partial pose estimation, a homography, (essentially 3D information) giving an axis of rotation and the angle (by computing the eigenvalues and eigenvectors of the homography). The translational information is obtained from the image directly by tracking a set of feature points. The only conditions being that the feature points being tracked never leave the field of view and that a depth estimate be predetermined by some off-line technique. 2-1/2-D servoing has been shown to be more stable than the techniques that preceded it. Another interesting observation with this formulation is that the authors claim that the visual Jacobian will have no singularities during the motions. The hybrid technique developed by Corke and Hutchinson, popularly called portioned approach partitions the visual (or image) Jacobian into motions (both rotations and translations) relating X and Y axes and motions related to the Z axis. outlines the technique, to break out columns of the visual Jacobian that correspond to the Z axis translation and rotation (namely, the third and sixth columns). The partitioned approach is shown to handle the Chaumette Conundrum discussed in. This technique requires a good depth estimate in order to function properly. namely main and secondary. The main task is keep the features of inter- est within the field of view. While the secondary task is to mark a fixation point and use it as a reference to bring the camera to the desired pose. The technique does need a depth estimate from an off-line procedure. The paper discusses two examples for which depth estimates are obtained from robot odometry and by assuming that all features are on a plane. The secondary task is achieved by using the notion of parallax. The features that are tracked are chosen by an initialization performed on the first frame, which are typi- cally points. modeling and model-based tracking. Primary assumption made is that the 3D model of the object is available. The authors highlights the notion that ideal features should be chosen such that the DOF of motion can be decoupled by linear relation. The authors also introduce an estimate of the target velocity into the interaction matrix to improve tracking performance. The results are compared to well known servoing techniques even when occlusions occur.",
            "score": 162.73126220703125
        },
        {
            "docid": "41175367_4",
            "document": "Natural scene perception . Focused attention played a partial role in early models of natural scene perception. Such models involved two stages of visual processing. According to these models, the first stage is attention free and registers low level features such as brightness gradients, motion and orientation in a parallel manner. Meanwhile, the second stage requires focused attention. It registers high-level object descriptions, has limited capacity and operates serially. These models have been empirically informed by studies demonstrating change blindness, inattentional blindness and attentional blink. Such studies show that when one's visual focused attention is engaged by a task, significant changes in one's environment that are not directly pertinent to the task can escape awareness. It was generally thought that natural scene perception was similarly susceptible to change blindness, inattentional blindness and attentional blink, and that these psychological phenomena occurred because engaging in a task diverts attentional resources that would otherwise be used for natural scene perception.",
            "score": 159.97076416015625
        },
        {
            "docid": "22396342_2",
            "document": "H1 neuron . The H1 neuron is located in the visual cortex of true flies of the order Diptera and mediates motor responses to visual stimuli. H1 is sensitive to horizontal motion in the visual field and enables the fly to rapidly and accurately respond to optic flow with motor corrections to stabilize flight. It is particularly responsive to horizontal forward motion associated with movement of the fly\u2019s own body during flight. Damage to H1 impairs the fly\u2019s ability to counteract disturbances during flight, suggesting that it is a necessary component of the optomotor response. H1 is an ideal system for studying the neural basis of information processing due to its highly selective and predictable responses to stimuli. Since the initial anatomical and physiological characterizations of H1 in 1976, study of the neuron has greatly benefited the understanding of neural coding in a wide range of organisms, especially the relationship between the neural code and behavior.",
            "score": 159.30162048339844
        },
        {
            "docid": "1534578_8",
            "document": "Motion perception . The motion direction of a contour is ambiguous, because the motion component parallel to the line cannot be inferred based on the visual input. This means that a variety of contours of different orientations moving at different speeds can cause identical responses in a motion sensitive neuron in the visual system.",
            "score": 157.75994873046875
        },
        {
            "docid": "63663_15",
            "document": "Fly . For visual course control, flies' optic flow field is analyzed by a set of motion-sensitive neurons. A subset of these neurons is thought to be involved in using the optic flow to estimate the parameters of self-motion, such as yaw, roll, and sideward translation. Other neurons are thought to be involved in analyzing the content of the visual scene itself, such as separating figures from the ground using motion parallax. The H1 neuron is responsible for detecting horizontal motion across the entire visual field of the fly, allowing the fly to generate and guide stabilizing motor corrections midflight with respect to yaw. The ocelli are concerned in the detection of changes in light intensity, enabling the fly to react swiftly to the approach of an object.",
            "score": 157.30125427246094
        },
        {
            "docid": "28354637_6",
            "document": "Richard A. Andersen . Early work centered on the discovery and elucidation of cortical gain fields, a general rule of multiplicative computation used by many areas of the cortex. Andersen and Zipser of UCSD developed one of the first neural network models of cortical function, which generated a mathematical basis for testing hypotheses based on laboratory findings. His research established that the posterior parietal cortex (PPC) is involved in forming movement intentions\u2014the early and abstract plans for movement. Previously this part of the brain was thought only to function for spatial awareness and attention. His laboratory discovered the lateral intraparietal area (LIP) in the PPC and established its role in eye movements. He also discovered the parietal reach region, an area involved in forming early reach plans. His lab has also made a number of discoveries related to visual motion perception. He established that the middle temporal area processes the perception of form from motion. He found that the perception of the direction of heading, important for navigation, is computed in the brain using both visual stimuli and eye movement signals. His lab has also determined how eye position and limb position signals are combined for eye-hand coordination.",
            "score": 157.16244506835938
        },
        {
            "docid": "1534578_26",
            "document": "Motion perception . Although the details of the Hassenstein-Reichardt model have not been confirmed at an anatomical and physiological level, the site of subtraction in the model is now being localized to the tangential cells. When depolarizing current is injected into the tangential cell while presenting a visual stimulus, the response to the preferred direction of motion decreased, and the response to the null direction increased. The opposite was observed with hyperpolarizing current. The T4 and T5 cells, which have been selected as a strong candidate for providing input to the tangential cells, have four subtypes that each project into one of the four strata of the lobula plate that differ in the preferred orientation.",
            "score": 157.11085510253906
        },
        {
            "docid": "1534578_24",
            "document": "Motion perception . It is now known that motion detection in vision is based on the Hassenstein-Reichardt detector model. This is a model used to detect correlation between the two adjacent points. It consists of two symmetrical subunits. Both subunits have a receptor that can be stimulated by an input (light in the case of visual system). In each subunit, when an input is received, a signal is sent to the other subunit. At the same time, the signal is delayed in time within the subunit, and after the temporal filter, is then multiplied by the signal received from the other subunit. Thus, within each subunit, the two brightness values, one received directly from its receptor with a time delay and the other received from the adjacent receptor, are multiplied. The multiplied values from the two subunits are then subtracted to produce an output. The direction of selectivity or preferred direction is determined by whether the difference is positive or negative. The direction which produces a positive outcome is the preferred direction.",
            "score": 156.257080078125
        },
        {
            "docid": "22396342_7",
            "document": "H1 neuron . Flies are agile flyers and strongly depend on vision during flight. For visual course control, flies optic flow field is analyzed by a set of \u223c60 motion-sensitive neurons, each present in the third visual neuropil of the left and right eyes. A subset of these neurons is thought to be involved in using the optic flow to estimate the parameters of self-motion, such as yaw, roll, and sideward translation. Other neurons are thought to be involved in analyzing the content of the visual scene itself, for example, to separate figure from ground using motion parallax. The H1 neuron is responsible for detecting horizontal motion across the entire visual field of the fly, allowing the fly to generate and guide stabilizing motor corrections mid-flight with respect to yaw.",
            "score": 155.19430541992188
        },
        {
            "docid": "5664_64",
            "document": "Consciousness . In neuroscience, a great deal of effort has gone into investigating how the perceived world of conscious awareness is constructed inside the brain. The process is generally thought to involve two primary mechanisms: (1) hierarchical processing of sensory inputs, and (2) memory. Signals arising from sensory organs are transmitted to the brain and then processed in a series of stages, which extract multiple types of information from the raw input. In the visual system, for example, sensory signals from the eyes are transmitted to the thalamus and then to the primary visual cortex; inside the cerebral cortex they are sent to areas that extract features such as three-dimensional structure, shape, color, and motion. Memory comes into play in at least two ways. First, it allows sensory information to be evaluated in the context of previous experience. Second, and even more importantly, working memory allows information to be integrated over time so that it can generate a stable representation of the world\u2014Gerald Edelman expressed this point vividly by titling one of his books about consciousness \"The Remembered Present\". In computational neuroscience, Bayesian approaches to brain function have been used to understand both the evaluation of sensory information in light of previous experience, and the integration of information over time. Bayesian models of the brain are probabilistic inference models, in which the brain takes advantage of prior knowledge to interpret uncertain sensory inputs in order to formulate a conscious percept; Bayesian models have successfully predicted many perceptual phenomena in vision and the nonvisual senses.",
            "score": 153.298095703125
        },
        {
            "docid": "29762298_6",
            "document": "Werner E. Reichardt . Reichardt's findings have contributed to understanding of information processing in nervous systems. From joint work (with Bernhard Hassenstein and Hans Wenking) on the visual system of insects and its effect on the flight orientation, the correlation model developed the idea that the visual system of man could be similarly investigated, and led to a general theory of motion perception",
            "score": 151.66424560546875
        },
        {
            "docid": "3380919_3",
            "document": "David Heeger . In the fields of perceptual psychology, systems neuroscience, cognitive neuroscience, and computational neuroscience, Heeger has developed computational theories of neuronal processing in the visual system, and he has performed psychophysics (perceptual psychology) and neuroimaging (functional magnetic resonance imaging, fMRI) experiments on human vision. His contributions to computational neuroscience include theories for how the brain can sense optic flow and egomotion, and a theory of neural processing called the normalization model. His empirical research has contributed to our understanding of the topographic organization of visual cortex (retinotopy), visual awareness, visual pattern detection/discrimination, visual motion perception, stereopsis (depth perception), attention, working memory, the control of eye and hand movements, neural processing of complex audio-visual and emotional experiences (movies, music, narrative), abnormal visual processing in dyslexia, and neurophysiological characteristics of autism.",
            "score": 151.0370635986328
        },
        {
            "docid": "42238932_5",
            "document": "Tinkerbell effect . In the \"Journal of Consciousness Studies\", Frank H. Durgin applies this expression to the study of human motion detection and perception in his paper \"The Tinkerbell Effect: Motion Perception and Illusion\". He questions the common belief that visual consciousness is a direct translation of the information the visual sensory organs collect. He argues that \"perceptual awareness pretends to have access to more information than is actually available to visual cognition\". He relates his argument about the indirectness in motion perception to how, in the play version of Peter Pan, Tinkerbell's revival depends on the live audience expressing their belief in fairies through clapping. The Tinkerbell effect points out a significant flaw in the brain's system of receiving and interpreting visually available information: it is not directly representative of reality. With the overwhelming amount of sensory information, the brain summarizes it by filling in what it cannot make sense of. In other words, it is an act of imagination.",
            "score": 150.8420867919922
        },
        {
            "docid": "42938624_6",
            "document": "Biological motion perception . Additionally, research on human participants is being conducted as well. While single-cell recording is not conducted on humans, this research uses neuroimaging methods such as fMRI, PET, EEG/ERP to collect information on what brain areas become active when executing biological motion perception tasks, such as viewing point light walker stimuli. Areas uncovered from this type of research are the dorsal visual pathway, extrastriate body area, fusiform gyrus, superior temporal sulcus, and premotor cortex. The dorsal visual pathway (sometimes referred to as the \u201cwhere\u201d pathway), as contrasted with the ventral visual pathway (\u201cwhat\u201d pathway), has been shown to play a significant role in the perception of motion cues. While the ventral pathway is more responsible for form cues.",
            "score": 149.4899139404297
        },
        {
            "docid": "176997_6",
            "document": "Blindsight . Blindsight patients show awareness of single visual features, such as edges and motion, but cannot gain a holistic visual percept. This suggests that perceptual awareness is modular and that\u2014in sighted individuals\u2014there is a \"binding process that unifies all information into a whole percept\", which is interrupted in patients with such conditions as blindsight and visual agnosia. Therefore, object identification and object recognition are thought to be separate processes and occur in different areas of the brain, working independently from one another. The modular theory of object perception and integration would account for the \"hidden perception\" experienced in blindsight patients. Research has shown that visual stimuli with the single visual features of sharp borders, sharp onset/offset times, motion, and low spatial frequency contribute to, but are not strictly necessary for, an object's salience in blindsight.",
            "score": 148.92066955566406
        },
        {
            "docid": "43622755_11",
            "document": "Illusory palinopsia . Light and motion perception are dynamic operations involving processing and feedback from structures throughout the central nervous system. A patient frequently has multiple types of diffuse, persistent illusory symptoms which represent dysfunctions in both light and motion perception. Light and motion are processed via different pathways, which suggests that there are diffuse or global excitability alterations in the visual pathway. Faulty neural adaptation and feedback between the anterior and posterior visual pathways could cause persistent excitability changes. Movement-related palinopsia could be due to inappropriate or incomplete activation of the motion suppression mechanisms (visual masking/backward masking and corollary discharges) related to visual stability during eye or body movements, which are present in saccadic suppression, blinking, smooth pursuit, etc.",
            "score": 148.751953125
        },
        {
            "docid": "41175367_12",
            "document": "Natural scene perception . Ultra-rapid visual categorization is a model proposing an automatic feedforward mechanism that forms high-level object representations in parallel without focused attention. In this model, the mechanism cannot be sped up by training. Evidence for a feedforward mechanism can be found in studies that have shown that many neurons are already highly selective at the beginning of a visual response, thus suggesting that feedback mechanisms are not required for response selectivity to increase. Furthermore, recent fMRI and ERP studies have shown that masked visual stimuli that participants do not consciously perceive can significantly modulate activity in the motor system, thus suggesting somewhat sophisticated visual processing. VanRullen (2006) ran simulations showing that the feedforward propagation of one wave of spikes through high-level neurons, generated in response to a stimulus, could be enough for crude recognition and categorization that occurs in 150 ms or less.",
            "score": 148.56446838378906
        },
        {
            "docid": "1534578_10",
            "document": "Motion perception . Some have speculated that, having extracted the hypothesized motion signals (first- or second-order) from the retinal image, the visual system must integrate those individual \"local\" motion signals at various parts of the visual field into a 2-dimensional or \"global\" representation of moving objects and surfaces. (It is not clear how this 2D representation is then converted into the perceived 3D percept) Further processing is required to detect coherent motion or \"global motion\" present in a scene.",
            "score": 148.42434692382812
        },
        {
            "docid": "36086848_17",
            "document": "Fear processing in the brain . The perception of fear is elicited by many different stimuli and involves the process described above in biochemical terms. Neural correlates of the interaction between language and visual information has been studied by Roel Willems \"et al\". The study consisted of observing how visual and linguistic information interact in the perception of emotion. A common phenomenon from film theory was borrowed which states that the presentation of a neutral visual scene intensifies the percept of fear or suspense induced by a different channel of information, such as language. This principle has been applied in a way in which the percept of fear was present and amplified in the presence of a neutral visual stimuli. The main idea is that the visual stimuli intensify the fearful content of the stimuli (i.e. language) by subtly implying and concretizing what is described in the context (i.e. sentence). Activation levels in the right anterior temporal pole were selectively increased and is believed to serve as a binding function of emotional information across domains such as visual and linguistic information.",
            "score": 148.26841735839844
        },
        {
            "docid": "30797574_69",
            "document": "History of computer animation . Computer-based motion capture started as a photogrammetric analysis tool in biomechanics research in the 70s and 80s. A performer wears markers near each joint to identify the motion by the positions or angles between the markers. Many different types of markers can be used\u2014lights, reflective markers, LEDs, infra-red, inertial, mechanical, or wireless RF\u2014and may be worn as a form of suit, or attached direct to a performer's body. Some systems include details of face and fingers to capture subtle expressions, and such is often referred to as \"performance capture\". The computer records the data from the markers, and uses it to animate digital character models in 2D or 3D computer animation, and in some cases this can include camera movement as well. In the 90s, these techniques became widely used for visual effects. Video games also began to use motion capture to animate in-game characters in 1995, the earliest examples of this being the Atari Jaguar CD-based game \"\", and the arcade fighting game \"Soul Edge\", which was the first video game to use passive optical motion-capture technology.",
            "score": 147.98646545410156
        },
        {
            "docid": "2951168_17",
            "document": "Moving parts . In recent decades, the use of animation has become more practical and widespread in technical and engineering diagrams for the illustration of the motions of moving parts. Animation represents moving parts more clearly and enables them and their motions to be more readily visualized. Furthermore, computer aided design tools allow the motions of moving parts to be simulated, allowing machine designers to determine, for example, whether the moving parts in a given design would obstruct one another's motion or collide by simple visual inspection of the (animated) computer model rather than by the designer performing a numerical analysis directly.",
            "score": 147.8836669921875
        }
    ]
}