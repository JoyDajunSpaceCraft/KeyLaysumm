{
    "q": [
        {
            "docid": "525667_10",
            "document": "Human echolocation . In a 2014 study by Thaler and colleagues, the researchers first made recordings of the clicks and their very faint echoes using tiny microphones placed in the ears of the blind echolocators as they stood outside and tried to identify different objects such as a car, a flag pole, and a tree. The researchers then played the recorded sounds back to the echolocators while their brain activity was being measured using functional magnetic resonance imaging. Remarkably, when the echolocation recordings were played back to the blind experts, not only did they perceive the objects based on the echoes, but they also showed activity in those areas of their brain that normally process visual information in sighted people, primarily primary visual cortex or V1. This result is surprising, as visual areas, as their names suggest, are only active during visual tasks. The brain areas that process auditory information were no more activated by sound recordings of outdoor scenes containing echoes than they were by sound recordings of outdoor scenes with the echoes removed. Importantly, when the same experiment was carried out with sighted people who did not echolocate, these individuals could not perceive the objects and there was no echo-related activity anywhere in the brain. This suggests that the cortex of blind echolocators is plastic and reorganizes such that primary visual cortex, rather than any auditory area, becomes involved in the computation of echolocation tasks.",
            "score": 151.42576813697815
        },
        {
            "docid": "2872287_23",
            "document": "Neural binding . Much of the experimental evidence for neural binding has traditionally revolved around sensory awareness. Sensory awareness is accomplished by integrating things together by cognitively perceiving them and then segmenting them so that, in total, there is an image created. Since there can be an infinite number of possibilities in the perception of an object, this has been a unique area of study. The way the brain then collectively pieces certain things together via networking is important not only in the global way of perceiving but also in segmentation. Much of sensory awareness has to do with the taking of a single piece of an object's makeup and then binding its total characteristics so that the brain perceives the object in its final form. Much of the research for the understanding of segmentation and how the brain perceives an object has been done by studying cats. A major finding of this research has to do with the understanding of gamma waves oscillating at 40\u00a0Hz. The information was extracted from a study using the cat visual cortex. It was shown that the cortical neurons responded differently to spatially different objects. These firings of neurons ranged from 40\u201360\u00a0Hz in measure and when observed showed that they fired synchronously when observing different parts of the object. Such coherent responses point to the fact that the brain is doing a kind of coding where it is piecing certain neurons together in the works of making the form of an object. Since the brain is putting these segmented pieces together unsupervised, a significant consonance is found with many philosophers (like Sigmund Freud) who theorize an underlying subconscious that helps to form every aspect of our conscious thought processes.",
            "score": 204.1456423997879
        },
        {
            "docid": "1316947_4",
            "document": "Ambiguous image . When we see an image, the first thing we do is attempt to organize all the parts of the scene into different groups. To do this, one of the most basic methods used is finding the edges. Edges can include obvious perceptions such as the edge of a house, and can include other perceptions that the brain needs to process deeper, such as the edges of a person's facial features. When finding edges, the brain's visual system detects a point on the image with a sharp contrast of lighting. Being able to detect the location of the edge of an object aids in recognizing the object. In ambiguous images, detecting edges still seems natural to the person perceiving the image. However, the brain undergoes deeper processing to resolve the ambiguity. For example, consider an image that involves an opposite change in magnitude of luminance between the object and the background (e.g. From the top, the background shifts from black to white, and the object shifts from white to black). The opposing gradients will eventually come to a point where there is an equal degree of luminance of the object and the background. At this point, there is no edge to be perceived. To counter this, the visual system connects the image as a whole rather than a set of edges, allowing one to see an object rather than edges and non-edges. Although there is no complete image to be seen, the brain is able to accomplish this because of its understanding of the physical world and real incidents of ambiguous lighting. In ambiguous images, an illusion is often produced from illusory contours. An illusory contour is a perceived contour without the presence of a physical gradient. In examples where a white shape appears to occlude black objects on a white background, the white shape appears to be brighter than the background, and the edges of this shape produce the illusory contours. These illusory contours are processed by the brain in a similar way as real contours. The visual system accomplishes this by making inferences beyond the information that is presented in much the same way as the luminance gradient.",
            "score": 168.15983414649963
        },
        {
            "docid": "32197396_5",
            "document": "Form perception . By five months of age infants are capable of using line junction information to perceive three-D images, including depth and shape, like adults are able. However, there are differences between younger infants and adults in the ability to use motion and color cues to discriminate between two objects. Visual information then continues to be processed in the posterior parietal cortex, also known as the dorsal stream, where the representation of an objects shape is formed using motion-based cues. The identification of differences between the infant and adult brain make it clear that there is either functional reorganization of the infant\u2019s cortex or simply age related differences in which the breed impulses have been observed in infants. Although the infant brain is not identical to the adult brain, it is similar with areas of specialization and a hierarchy of processing,[7] however, adult abilities to perceive form from stationary viewing are not fully understood.",
            "score": 164.74457013607025
        },
        {
            "docid": "1903855_7",
            "document": "Sensory substitution . In a regular visual system, the data collected by the retina is converted into an electrical stimulus in the optic nerve and relayed to the brain, which re-creates the image and perceives it. Because it is the brain that is responsible for the final perception, sensory substitution is possible. During sensory substitution an intact sensory modality relays information to the visual perception areas of the brain so that the person can perceive to see. With sensory substitution, information gained from one sensory modality can reach brain structures physiologically related to other sensory modalities. Touch-to-visual sensory substitution transfers information from touch receptors to the visual cortex for interpretation and perception. For example, through fMRI, we can determine which parts of the brain are activated during sensory perception. In blind persons, we can see that while they are only receiving tactile information, their visual cortex is also activated as they perceive to \"see\" objects. We can also have touch to touch sensory substitution where information from touch receptors of one region can be used to perceive touch in another region. For example, in one experiment by Bach-y-Rita, he was able to restore the touch perception in a patient who lost peripheral sensation from leprosy.",
            "score": 188.37094902992249
        },
        {
            "docid": "179092_10",
            "document": "Neurolinguistics . Much work in neurolinguistics has, like Broca's and Wernicke's early studies, investigated the locations of specific language \"modules\" within the brain. Research questions include what course language information follows through the brain as it is processed, whether or not particular areas specialize in processing particular sorts of information, how different brain regions interact with one another in language processing, and how the locations of brain activation differs when a subject is producing or perceiving a language other than his or her first language.",
            "score": 165.45605945587158
        },
        {
            "docid": "53497_13",
            "document": "Optical illusion . In the Ponzo illusion the converging parallel lines tell the brain that the image higher in the visual field is farther away therefore the brain perceives the image to be larger, although the two images hitting the retina are the same size. The optical illusion seen in a diorama/false perspective also exploits assumptions based on monocular cues of depth perception. The M.C. Escher painting \"Waterfall\" exploits rules of depth and proximity and our understanding of the physical world to create an illusion. Like depth perception, motion perception is responsible for a number of sensory illusions. Film animation is based on the illusion that the brain perceives a series of slightly varied images produced in rapid succession as a moving picture. Likewise, when we are moving, as we would be while riding in a vehicle, stable surrounding objects may appear to move. We may also perceive a large object, like an airplane, to move more slowly than smaller objects, like a car, although the larger object is actually moving faster. The phi phenomenon is yet another example of how the brain perceives motion, which is most often created by blinking lights in close succession.",
            "score": 173.164182305336
        },
        {
            "docid": "2843988_26",
            "document": "Motor control . Many models of the perceptual system assume indirect perception, or the notion that the world that gets perceived is not identical to the actual environment. Environmental information must go through several stages before being perceived, and the transitions between these stages introduce ambiguity. What actually gets perceived is the mind's best guess about what is occurring in the environment based on previous experience. Support for this idea comes from the Ames room illusion, where a distorted room causes the viewer to see objects known to be a constant size as growing or shrinking as they move around the room. The room itself is seen as being square, or at least consisting of right angles, as all previous rooms the perceiver has encountered have had those properties. Another example of this ambiguity comes from the doctrine of specific nerve energies. The doctrine presents the finding that there are distinct nerve types for different types of sensory input, and these nerves respond in a characteristic way regardless of the method of stimulation. That is to say, the color red causes optical nerves to fire in a specific pattern that is processed by the brain as experiencing the color red. However, if that same nerve is electrically stimulated in an identical pattern, the brain could perceive the color red when no corresponding stimuli is present.",
            "score": 183.27062046527863
        },
        {
            "docid": "27326458_12",
            "document": "Somatosensory evoked potential . Besides the clinical setting, SEPs have shown to be useful in distinct experimental paradigms. Schubert et al. (2006) used SEPs to investigate the differential processing of consciously perceived versus unperceived somatosensory stimuli. The authors used an 'extinction' paradigm to examine the connection between activation of S1 and somatosensory awareness, and observed that early SEPs (P60, N80), generated in the contralateral S1, were independent of stimulus perception. In contrast, amplitude enhancements were observed for the P100 and N140 for consciously perceived stimuli. The authors concluded that early activation of S1 is not sufficient to warrant conscious stimulus perception. Conscious stimulus processing differs significantly from unconscious processing starting around 100 ms after stimulus presentation when the signal is processed in parietal and frontal cortices, brain regions crucial for stimulus access into conscious perception. In another study, Iwadate et al. (2005) looked at the relationship between physical exercise and somatosensory processing using SEPs. The study compared SEPs in athletes (soccer players) and non-athletes, using two oddball tasks following separate somatosensory stimulation at the median nerve and at the tibial nerve. In the athlete group the N140 amplitudes were larger during upper- and lowerlimb tasks when compared to non-athletes. The authors concluded that plastic changes in somatosensory processing might be induced by performing physical exercises that require attention and skilled movements.",
            "score": 280.8176282644272
        },
        {
            "docid": "6733469_3",
            "document": "Max Planck Institute for Biological Cybernetics . The institute is studying signal and information processing in the brain. We know that our brain is constantly processing a vast amount of sensory and intrinsic information by which our behavior is coordinated accordingly. How the brain actually achieves these tasks is less well understood, for example, how it perceives, recognizes, and learns new objects. The scientists at the Max Planck Institute for Biological Cybernetics aim to determine which signals and processes are responsible for creating a coherent percept of our environment and for eliciting the appropriate behavior. Scientists of three departments and seven research groups are working towards answering fundamental questions about processing in the brain, using different approaches and methods.",
            "score": 179.80409264564514
        },
        {
            "docid": "37691351_7",
            "document": "Neuroscience and race . Top-down and bottom-up processing are terms used to describe the differences in memory processing when observing same-race and other-race faces. Bottom-up processing puts together pieces of a whole and develops one grand picture. Top-down processing uses more initial cognitive work by breaking down the whole picture into pieces, and then analyzing those pieces. Bottom-up processing is used in processing same-race faces, and requires much less brain activation than top-down processing, which is used while processing other-race faces. When bottom-up brain processing is used while viewing same-race faces, a holistic face in perceived, encoded and remembered. When top-down brain processing is used while viewing other-race faces, only features of the face are perceived and encoded. Once the face is perceived by the VT cortex, the hippocampus is used to encode the memory in the parietal lobe. Overall, same-race faces undergo better memory encoding processes than other-race faces because they are remembered more often, however, other-race faces that are remembered undergo a more effortful memory encoding process. More brain activation is needed to effectively encode an other-race face. Memory encoding isn't the only found cause of the cross-race effect; memory retrieval is also involved. In retrieving a memory, the parietal lobe is reactivated. When retrieving an other-race face, there is more reactivation of the parietal lobe, meaning more effort is needed to retrieve an other-race face memory. The frontal lobe is also activated while observing other-race faces if the parietal lobe is unable to retrieve the memory, acting as a search engine in the brain looking for the location of the memory.",
            "score": 172.01508903503418
        },
        {
            "docid": "32197396_2",
            "document": "Form perception . Form perception is the recognition of visual elements of objects, specifically those to do with shapes, patterns and previously identified important characteristics. An object is perceived by the retina as a two-dimensional image, but the image can vary for the same object in terms of the context with which it is viewed, the apparent size of the object, the angle from which it is viewed, how illuminated it is, as well as where it resides in the field of vision.  Despite the fact that each instance of observing an object leads to a unique retinal response pattern, the visual processing in the brain is capable of recognizing these experiences as analogous, allowing invariant object recognition. Visual processing occurs in a hierarchy with the lowest levels recognizing lines and contours, and slightly higher levels performing tasks such as completing boundaries and recognizing contour combinations. The highest levels integrate the perceived information to recognize an entire object. Essentially object recognition is the ability to assign labels to objects in order to categorize and identify them, thus distinguishing one object from another. During visual processing information is not created, but rather reformatted in a way that draws out the most detailed information of the stimulus.",
            "score": 149.61222064495087
        },
        {
            "docid": "51547415_15",
            "document": "Interindividual differences in perception . In line with these findings, Hedden and colleagues (2009) used the same visual stimuli to investigate the neural activity with the help of fMRI. Participants of the study were asked to judge the length of a vertical line, either including the contextual information or ignoring it. The results revealed that separate brain regions were employed while performing the task, either incorporating the contextual information or avoiding it, based on one's own culture. The areas associated with attentional control in the frontal and parietal region of the brain were highly activated when the subjects performed the task which was incongruent to their cultural pattern. That is, the activity in the fronto-parietal region enhanced when East Asians had to ignore the contextual information, while similar enhancement happened for Americans when they had to incorporate the contextual information. These findings illustrate that the function of the neural mechanisms are also modulated to some extent by one's own culture.",
            "score": 179.1359144449234
        },
        {
            "docid": "724626_6",
            "document": "Allan Snyder . Snyder is interested in understanding savants, how the savant brain perceives and interprets the world, the neurological and subjective correlates of the savant brain, and how to activate or at least promote savant level brain functions in non-autistic, healthy individuals. Even something as simple as seeing, he explains, requires phenomenally complex information processing. When a person looks at an object, for example, the brain immediately estimates an object's distance by calculating the subtle differences between the two images on each retina (computers programmed to do this require extreme memory and speed). During the process of face recognition, the brain analyzes countless details, such as the texture of skin and the shape of the eyes, jawbone, and lips. The vast majority of people are simply unaware of these calculations due to the brains' information filtering processes. In savants, says Snyder, the top layer of mental processing \u2014conceptual thinking, making logical deductions\u2014 is somehow deactivated. His working hypothesis is that once this layer is inactivate, one can access a startling capacity for recalling the most minute detail or for performing lightning-quick calculations. Snyder's theory has a conclusion of its own: He believes it may be possible someday to create technologies that will allow any non-autistic person to access these abilities.",
            "score": 140.06723725795746
        },
        {
            "docid": "23483_10",
            "document": "Philosophy of perception . The succession of data transfers involved in perception suggests that sense data are somehow available to a perceiving subject that is the substrate of the percept. Indirect realism, the view held by John Locke and Nicolas Malebranche, proposes that we can only be aware of mental representations of objects. however this may imply an infinite regress (a perceiver within a perceiver within a perceiver...), though a finite regress is perfectly possible. It also assumes that perception is entirely due to data transfer and information processing, an argument that can be avoided by proposing that the percept does not depend wholly upon the transfer and rearrangement of data. This still involves basic ontological issues of the sort raised by Leibniz Locke, Hume, Whitehead and others, which remain outstanding particularly in relation to the binding problem, the question of how different perceptions (e.g. color and contour in vision) are \"bound\" to the same object when they are processed by separate areas of the brain.",
            "score": 175.0016609430313
        },
        {
            "docid": "1156527_8",
            "document": "Detection theory . Signal detection theory (SDT) is used when psychologists want to measure the way we make decisions under conditions of uncertainty, such as how we would perceive distances in foggy conditions. SDT assumes that the decision maker is not a passive receiver of information, but an active decision-maker who makes difficult perceptual judgments under conditions of uncertainty. In foggy circumstances, we are forced to decide how far away from us an object is, based solely upon visual stimulus which is impaired by the fog. Since the brightness of the object, such as a traffic light, is used by the brain to discriminate the distance of an object, and the fog reduces the brightness of objects, we perceive the object to be much farther away than it actually is (see also decision theory).",
            "score": 111.59532034397125
        },
        {
            "docid": "37527148_19",
            "document": "Psychology of film . Event segmentation is viewed as \u201cthe brain\u2019s cutting-room floor.\u201d  It is considered to be an automatic and ongoing process that depends on meaningful changes in a perceived situation. To test this, researchers measured brain activity while participants viewed an extended narrative film. They used MRI scanning to show transient evoked brain responses (changes in brain activity) at those points they identified as event boundaries (changes in situation). Situational changes were coded frame by frame into spatial, temporal, object, character, causal and goal changes. Participants were then instructed to perform an event segmentation task by watching a movie and pressing a button to identify units of activity that were natural and meaningful to them. Paying attention to situational changes gives rise to a neural cascade that is consciously perceived at the end of one event and the beginning of another.",
            "score": 175.96967959403992
        },
        {
            "docid": "599837_25",
            "document": "Habituation . Researchers also use habituation and dishabituation procedures in the laboratory to study the perceptual and cognitive capabilities of human infants. The presentation of a visual stimulus to an infant elicits looking behavior that habituates with repeated presentations of the stimulus. When changes to the habituated stimulus are made (or a new stimulus is introduced) the looking behavior returns (dishabituates). A recent fMRI study revealed that the presentation of a dishabituating stimulus has an observable, physical effect upon the brain. In one study the mental spatial representations of infants were assessed using the phenomenon of dishabituation. Infants were presented repeatedly with an object in the same position on a table. Once the infants habituated to the object (i.e., spent less time looking at it) either the object was spatially moved while the infant remained at the same place near the table or the object was left in the same place but the infant was moved to the opposite side of the table. In both cases the spatial relationship between the object and the infant had changed, but only in the former case did the object itself move. Would the infants know the difference? Or would they treat both cases as if the object itself moved? The results revealed a return of looking behavior (dishabituation) when the object's position was changed, but not when the infant's position was changed. Dishabituation indicates that infants perceived a significant change in the stimulus. Therefore, the infants understood when the object itself moved and when it did not. Only when the object itself moved were they interested in it again (dishabituation), When the object remained in the same position as before it was perceived as the same old boring thing (habituation). In general, habituation/dishabituation procedures help researchers determine the way infants perceive their environments.",
            "score": 118.54407024383545
        },
        {
            "docid": "5212945_3",
            "document": "Visual neuroscience . A recent study using Event-Related Potentials (ERPs) linked an increased neural activity in the occipito-temporal region of the brain to the visual categorization of facial expressions. Results focus on a negative peak in the ERP that occurs 170 milliseconds after the stimulus onset. This action potential, called the N170, was measured using electrodes in the occipito-temporal region, an area already known to be changed by face stimuli. Studying by using the EEG, and ERP methods allow for an extremely high temporal resolution of 4 milliseconds, which makes these kinds of experiments extremely well suited for accurately estimating and comparing the time it takes the brain to perform a certain function. Scientists used classification image techniques, to determine what parts of complex visual stimuli (such as a face) will be relied on when patients are asked to assign them to a category, or emotion. They computed the important features when the stimulus face exhibited one of five different emotions. Stimulus faces exhibiting fear had the distinguishing feature of widening eyes, and stimuli exhibiting happiness exhibited a change in the mouth to make a smile. Regardless of the expression of the stimuli's face, the region near the eyes affected the EEG before the regions near the mouth. This revealed a sequential, and predetermined order to the perception and processing of faces, with the eye being the first, and the mouth, and nose being processed after. This process of downward integration only occurred when the inferior facial features were crucial to the categorization of the stimuli. This is best explained by comparing what happens when participants were shown a face exhibiting fear, versus happiness. The N170 peaked slightly earlier for the fear stimuli at about 175 milliseconds, meaning that it took a participants less time to recognize the facial expression. This is expected because only the eyes need to be processed to recognize the emotion. However, when processing a happy expression, where the mouth is crucial to categorization, downward integration must take place, and thus the N170 peak occurred later at around 185 milliseconds. Eventually visual neuroscience aims to completely explain how the visual system processes all changes in faces as well as objects. This will give a complete view to how the world is constantly visually perceived, and may provide insight into a link between perception and consciousness.",
            "score": 194.54012942314148
        },
        {
            "docid": "1275987_10",
            "document": "Moon illusion . In 1813, Schopenhauer wrote about this, that the moon illusion is \"purely intellectual or cerebral and not optical or sensuous.\" The brain takes the sense data that is given to it from the eye and it apprehends a large moon because \"our intuitively perceiving understanding regards everything that is seen in a horizontal direction as being more distant and therefore as being larger than objects that are seen in a vertical direction.\" The brain is accustomed to seeing terrestrially\u2013sized objects in a horizontal direction and also as they are affected by atmospheric perspective, according to Schopenhauer. If we perceive the Moon to be in the general vicinity of the other things we see in the sky, we would expect it to also recede as it approaches the horizon, which should result in a smaller retinal image. But since its retinal image is approximately the same size whether it is near the horizon or not, our brains, attempting to compensate for perspective, assume that a low Moon must be physically larger.",
            "score": 134.48055565357208
        },
        {
            "docid": "485309_11",
            "document": "Face perception . Recognizing and perceiving faces are vital abilities needed to coexist in society. Faces can tell things such as identity, mood, age, sex, race, and the direction that someone is looking. Studies based on neuropsychology, behavior, electrophysiology, and neuro-imaging have supported the notion of a specialized mechanism for perceiving faces. Prosopagnosia patients demonstrate neuropsychological support for a specialized face perception mechanism as these people, due to brain damage, have deficits in facial perception, but their cognitive perception of objects remains intact. The face inversion effect provides behavioral support of a specialized mechanism as people tend to have greater deficits in task performance when prompted to react to an inverted face than to an inverted object. Electrophysiological support comes from the finding that the N170 and M170 responses tend to be face-specific. Neuro-imaging studies such as PET and fMRI studies have shown support for a specialized facial processing mechanism as they have identified regions of the fusiform gyrus that have higher activation during face perception tasks than other visual perception tasks. Theories about the processes involved in adult face perception have largely come from two sources: research on normal adult face perception and the study of impairments in face perception that are caused by brain injury or neurological illness. Novel optical illusions such as the Flashed Face Distortion Effect, in which scientific phenomenology outpaces neurological theory, also provide areas for research.",
            "score": 160.50996339321136
        },
        {
            "docid": "33676217_17",
            "document": "Sensory-motor coupling . Patient R.W. was a man who suffered damage in his parietal and occipital lobes, areas of the brain related to processing visual information, due to a stroke. As a result of his stroke, he experienced vertigo when he tried to track a moving object with his eyes. The vertigo was caused by his brain interpreting the world as moving. In normal people, the world is not perceived as in moving when tracking an object despite the fact that the image of the world is moved across the retina as the eye moves. The reason for this is that the brain predicts the movement of the world across the retina as a consequence of moving the eyes. R.W., however, was unable to make this prediction.",
            "score": 96.06830978393555
        },
        {
            "docid": "2534964_14",
            "document": "Sensory processing . Perhaps one of the most studied sensory integrations is the relationship between vision and audition. These two senses perceive the same objects in the world in different ways, and by combining the two, they help us understand this information better. Vision dominates our perception of the world around us. This is because visual spatial information is one of the most reliable sensory modalities. Visual stimuli are recorded directly onto the retina, and there are few, if any, external distortions that provide incorrect information to the brain about the true location of an object. Other spatial information is not as reliable as visual spatial information. For example, consider auditory spatial input. The location of an object can sometimes be determined solely on its sound, but the sensory input can easily be modified or altered, thus giving a less reliable spatial representation of the object. Auditory information therefore is not spatially represented unlike visual stimuli. But once one has the spatial mapping from the visual information, multisensory integration helps bring the information from both the visual and auditory stimuli together to make a more robust mapping.",
            "score": 184.46742594242096
        },
        {
            "docid": "22483_92",
            "document": "Optics . Optical illusions (also called visual illusions) are characterized by visually perceived images that differ from objective reality. The information gathered by the eye is processed in the brain to give a percept that differs from the object being imaged. Optical illusions can be the result of a variety of phenomena including physical effects that create images that are different from the objects that make them, the physiological effects on the eyes and brain of excessive stimulation (e.g. brightness, tilt, colour, movement), and cognitive illusions where the eye and brain make unconscious inferences.",
            "score": 182.08627843856812
        },
        {
            "docid": "21647661_3",
            "document": "Self model . The PSM is an entity that \u201cactually exists, not only as a distinct theoretical entity but something that will be empirically discovered in the future- for instance, as a specific stage of the global neural dynamics in the human brain\u201d. Involved in the PSM are three phenomenal properties that must occur in order to explain the concept of the self. The first is mineness, \u201ca higher order property of particular forms of phenomenal content,\u201d or the idea of ownership. The second is perspectivalness, which is \u201ca global, structural property of phenomenal space as a whole\u201d. More simply, it is what is commonly referred to as the ecological self, the immovable center of perception. The third phenomenal property is selfhood, which is \u201cthe phenomenal target property\u201d or the idea of the self over time. It is the property of phenomenal selfhood that plays the most important role in creating the fictional self and the first person perspective. Metzinger defines the first person perspective as the \u201cexistence of single coherent and temporally stable model of reality which is representationally centered around or on a single coherent and temporally stable phenomenal subject\u201d. The first-person perspective can be non-conceptual and is autonomously active due to the constant reception of perceptual information by the brain. The brain, specifically the brainstem and hypothalamus, processes this information into representational content, namely linguistic reflections. The PSM then uses this representational content to attribute phenomenal states to our perceived objects and ourselves. We are thus what Metzinger calls na\u00efve realists, who believe we are perceiving reality directly when in actuality we are only perceiving representations of reality. The data structures and transport mechanisms of the data are \u201ctransparent\u201d so that we can introspect on our representations of perceptions, but cannot introspect on the data or mechanisms themselves. These systemic representational experiences are then connected by subjective experience to generate the phenomenal property of selfhood. Subjective experience is the result of the Phenomenal Model of Intentionality Relationship (PMIR). The PMIR is a \u201cconscious mental model, and its content is an ongoing, episodic subject-object relation\u201d. The model is a result of the combination of our unique set of sensory receptors that acquire input, our unique set of experiences that shape connections within the brain, and our unique positions in space that give our perception perspectivalness.",
            "score": 169.9830607175827
        },
        {
            "docid": "433584_4",
            "document": "McGurk effect . Vision is the primary sense for humans, but speech perception is multimodal, which means that it involves information from more than one sensory modality, in particular, audition and vision. The McGurk effect arises during phonetic processing because the integration of audio and visual information happens early in speech perception. The McGurk effect is very robust; that is, knowledge about it seems to have little effect on one's perception of it. This is different from certain optical illusions, which break down once one 'sees through' them. Some people, including those that have been researching the phenomenon for more than twenty years, experience the effect even when they are aware that it is taking place. With the exception of people who can identify most of what is being said from speech-reading alone, most people are quite limited in their ability to identify speech from visual-only signals. A more extensive phenomenon is the ability of visual speech to increase the intelligibility of heard speech in a noisy environment. Visible speech can also alter the perception of perfectly audible speech sounds when the visual speech stimuli are mismatched with the auditory speech. Normally, speech perception is thought to be an auditory process; however, our use of information is immediate, automatic, and, to a large degree, unconscious and therefore, despite what is widely accepted as true, speech is not only something we hear. Speech is perceived by all of the senses working together (seeing, touching, and listening to a face move). The brain is often unaware of the separate sensory contributions of what it perceives. Therefore, when it comes to recognizing speech the brain cannot differentiate whether it is seeing or hearing the incoming information.",
            "score": 206.87736880779266
        },
        {
            "docid": "2534964_16",
            "document": "Sensory processing . Hand eye coordination is one example of sensory integration. In this case, we require a tight integration of what we visually perceive about an object, and what we tactilely perceive about that same object. If these two senses were not combined within the brain, then one would have less ability to manipulate an object. Hand-eye coordination is the tactile sensation in the context of the visual system. The visual system is very static, in that it doesn't move around much, but the hands and other parts used in tactile sensory collection can freely move around. This movement of the hands must be included in the mapping of both the tactile and visual sensations, otherwise one would not be able to comprehend where they were moving their hands, and what they were touching and looking at. An example of this happening is looking at an infant. The infant picks up objects and puts them in his mouth, or touches them to his feet or face. All of these actions are culminating to the formation of spatial maps in the brain and the realization that \"Hey, that thing that's moving this object is actually a part of me.\" Seeing the same thing that they are feeling is a major step in the mapping that is required for infants to begin to realize that they can move their arms and interact with an object. This is the earliest and most explicit way of experiencing sensory integration.",
            "score": 102.09218287467957
        },
        {
            "docid": "9518_24",
            "document": "Edmund Husserl . From the \"Ideen\" onward, Husserl concentrated on the ideal, essential structures of consciousness. The metaphysical problem of establishing the reality of what we perceive, as distinct from the perceiving subject, was of little interest to Husserl in spite of his being a transcendental idealist. Husserl proposed that the world of objects\u2014and of ways in which we direct ourselves toward and perceive those objects\u2014is normally conceived of in what he called the \"natural standpoint\", which is characterized by a belief that objects exist distinct from the perceiving subject and exhibit properties that we see as emanating from them. Husserl proposed a radical new phenomenological way of looking at objects by examining how we, in our many ways of being intentionally directed toward them, actually \"constitute\" them (to be distinguished from materially creating objects or objects merely being figments of the imagination); in the Phenomenological standpoint, the object ceases to be something simply \"external\" and ceases to be seen as providing indicators about what it is, and becomes a grouping of perceptual and functional aspects that imply one another under the idea of a particular object or \"type\". The notion of objects as real is not expelled by phenomenology, but \"bracketed\" as a way in which we regard objectsinstead of a feature that inheres in an object's essence founded in the relation between the object and the perceiver. In order to better understand the world of appearances and objects, phenomenology attempts to identify the invariant features of how objects are perceived and pushes attributions of reality into their role as an attribution about the things we perceive (or an assumption underlying how we perceive objects). The major dividing line in Husserl's thought is the turn to transcendental idealism.",
            "score": 123.57825779914856
        },
        {
            "docid": "2158298_69",
            "document": "Visual impairment . Touch is also an important aspect of how blind or visually impaired people perceive the world. Touch gives immense amount of information in the person's immediate surrounding. Feeling anything with detail gives off information on shape, size, texture, temperature, and many other qualities. Touch also helps with communication; braille is a form of communication in which people use their fingers to feel elevated bumps on a surface and can understand what is meant to be interpreted. There are some issues and limitations with touch as not all objects are accessible to feel, which makes it difficult to perceive the actual object. Another limiting factor is that the learning process of identifying objects with touch is much slower than identifying objects with sight. This is due to the fact the object needs to be approached and carefully felt until a rough idea can be constructed in the brain.",
            "score": 103.7317852973938
        },
        {
            "docid": "80077_130",
            "document": "Ethnomusicology . From the cognitive perspective, the brain perceives auditory stimuli as music according to gestalt principles, or \u201cprinciples of grouping.\u201d Gestalt principles include proximity, similarity, closure, and continuation. Each of the gestalt principles illustrates a different element of auditory stimuli that cause them to be perceived as a group, or as one unit of music. Proximity dictates that auditory stimuli that are near to each other are seen as a group. Similarity dictates that when multiple auditory stimuli are present, the similar stimuli are perceived as a group. Closure is the tendency to perceive an incomplete auditory pattern as a whole\u2014the brain \u201cfills in\u201d the gap. And continuation dictates that auditory stimuli are more likely to be perceived as a group when they follow a continuous, detectable pattern.",
            "score": 165.84418845176697
        },
        {
            "docid": "2363287_6",
            "document": "Visual learning . Various areas of the brain work together in a multitude of ways in order to produce the images that we see with our eyes and that are encoded by our brains. The basis of this work takes place in the visual cortex of the brain. The visual cortex is located in the occipital lobe of the brain and harbors many other structures that aid in visual recognition, categorization, and learning. One of the first things the brain must do when acquiring new visual information is recognize the incoming material. Brain areas involved in recognition are the inferior temporal cortex, the superior parietal cortex, and the cerebellum. During tasks of recognition, there is increased activation in the left inferior temporal cortex and decreased activation in the right superior parietal cortex. Recognition is aided by neural plasticity, or the brain's ability to reshape itself based on new information. Next the brain must categorize the material. The three main areas that are used when categorizing new visual information are the orbitofrontal cortex and two dorsolateral prefrontal regions which begin the process of sorting new information into groups and further assimilating that information into things that you might already know. After recognizing and categorizing new material entered into the visual field, the brain is ready to begin the encoding process \u2013 the process which leads to learning. Multiple brain areas are involved in this process such as the frontal lobe, the right extrastriate cortex, the neocortex, and again, the neostriatum. One area in particular, the limbic-diencephalic region, is essential for transforming perceptions into memories. With the coming together of tasks of recognition, categorization and learning; schemas help make the process of encoding new information and relating it to things you already know much easier. One can remember visual images much better when they can apply it to an already known schema. Schemas actually provide enhancement of visual memory and learning.",
            "score": 157.46903145313263
        },
        {
            "docid": "485309_64",
            "document": "Face perception . Studies are not conclusive as to which brain areas people with ASD use instead. One study found that, when looking at faces, people with ASD exhibit activity in brain regions normally active when typically developing individuals perceive objects. Another study found that during facial perception, people with ASD use different neural systems, with each one of them using their own unique neural circuitry.",
            "score": 181.1043426990509
        }
    ],
    "r": [
        {
            "docid": "27326458_12",
            "document": "Somatosensory evoked potential . Besides the clinical setting, SEPs have shown to be useful in distinct experimental paradigms. Schubert et al. (2006) used SEPs to investigate the differential processing of consciously perceived versus unperceived somatosensory stimuli. The authors used an 'extinction' paradigm to examine the connection between activation of S1 and somatosensory awareness, and observed that early SEPs (P60, N80), generated in the contralateral S1, were independent of stimulus perception. In contrast, amplitude enhancements were observed for the P100 and N140 for consciously perceived stimuli. The authors concluded that early activation of S1 is not sufficient to warrant conscious stimulus perception. Conscious stimulus processing differs significantly from unconscious processing starting around 100 ms after stimulus presentation when the signal is processed in parietal and frontal cortices, brain regions crucial for stimulus access into conscious perception. In another study, Iwadate et al. (2005) looked at the relationship between physical exercise and somatosensory processing using SEPs. The study compared SEPs in athletes (soccer players) and non-athletes, using two oddball tasks following separate somatosensory stimulation at the median nerve and at the tibial nerve. In the athlete group the N140 amplitudes were larger during upper- and lowerlimb tasks when compared to non-athletes. The authors concluded that plastic changes in somatosensory processing might be induced by performing physical exercises that require attention and skilled movements.",
            "score": 280.817626953125
        },
        {
            "docid": "42638938_2",
            "document": "Phillip M. Merikle . Philip M. Merikle is a Distinguished Professor Emeritus in the Department of Psychology at the University of Waterloo, Canada. He is known for his published work on attentional processes, memory and anaesthesia see anaesthesia awareness, perception without awareness (see unconscious perception), and synaesthesia  Merikle's early contributions rebutted against Daniel Holender's 1986 criticism of prior experiments which claimed to demonstrate unconscious priming following Anthony Marcel's work on unconscious processes. Merikle's work sought to shift the debate from indirect-without-direct effects determined by Holender to be the only way unconscious perception could be proved, to what he defined as objective (forced chance level)and subjective thresholds (a threshold of claimed awareness) as a means to distinguish stimuli presentation. He believed that the indirect-without-direct effect was too stringent of a requirement for proving unconscious perception and analyses. Merikle claimed that the subjective threshold is a better boundary between the conscious and unconscious rather than direct and indirect measures on the basis that to distinguish the two, all that is required is a qualitatively different effect between when information is consciously perceived than when it is unconsciously perceived.",
            "score": 225.0400390625
        },
        {
            "docid": "739262_12",
            "document": "Neural correlate . Using such design, Nikos Logothetis and colleagues discovered perception-reflecting neurons in the temporal lobe. They created an experimental situation in which conflicting images were presented to different eyes (\"i.e.\", binocular rivalry). Under such conditions, human subjects report bistable percepts: they perceive alternatively one or the other image. Logothetis and colleagues trained the monkeys to report with their arm movements which image they perceived. Interestingly, temporal lobe neurons in Logothetis experiments often reflected what the monkeys' perceived. Neurons with such properties were less frequently observed in the primary visual cortex that corresponds to relatively early stages of visual processing. Another set of experiments using binocular rivalry in humans showed that certain layers of the cortex can be excluded as candidates of the neural correlate of consciousness. Logothetis and colleagues switched the images between eyes during the percept of one of the images. Surprisingly the percept stayed stable. This means that the conscious percept stayed stable and at the same time the primary input to layer 4, which is the input layer, in the visual cortex changed. Therefore layer 4 can not be a part of the neural correlate of consciousness. Mikhail Lebedev and their colleagues observed a similar phenomenon in monkey prefrontal cortex. In their experiments monkeys reported the perceived direction of visual stimulus movement (which could be an illusion) by making eye movements. Some prefrontal cortex neurons represented actual and some represented perceived displacements of the stimulus. Observation of perception related neurons in prefrontal cortex is consistent with the theory of Christof Koch and Francis Crick who postulated that neural correlate of consciousness resides in prefrontal cortex. Proponents of distributed neuronal processing may likely dispute the view that consciousness has a precise localization in the brain.",
            "score": 220.301513671875
        },
        {
            "docid": "6840372_6",
            "document": "Jeffrey Alan Gray . Gray thought that intentionality was based on unconscious processing. The processing in the visual cortex that underlies conscious perception is not itself conscious. Instead, the perception is argued to spring into consciousness fully formed, including the intentionality of what the conscious perception is about. In arguing for this, Gray uses the example of pictures that can be either of two things, such as a duck or a rabbit. They are never hybrid, but are always completely duck or completely rabbit. The perception of a duck or a rabbit is argued to be constructed unconsciously up to the last moment. Gray's conclusion from this part of his discussion is that intentionality arises from the physical and chemical structure of the brain, but also that if intentionality can be constructed out of unconscious processing, it is unlikely to produce a solution to the 'hard problem' of how consciousness arises.",
            "score": 210.77330017089844
        },
        {
            "docid": "39722511_6",
            "document": "Anthony Marcel . Marcel began the research that would eventually lead him to unconscious perception in the early 1970s, with his work initially rooted in reading. The experiments detailed in Marcel\u2019s 1983 publications were prompted by unexpected findings in masked reading tasks with school children. At that time, Marcel was working with neurological patients who showed deep dyslexia, and was interested in their errors involving associatively or semantically related words (e.g. sleep and dream). Marcel was manipulating interstimulus intervals in the lexical decision task, wherein words are shown as priming stimuli to facilitate judgment of whether the target stimulus is a word (e.g. the target word \"butter\" primes for \"bread\" but not \"nurse\") to test what impact this might have on the facilitation effect. He noticed the children making errors that indicated the masked words were acting as priming stimuli. This was suggestive of unconscious perception and therefore contrary to the theories and masking literature of the time. Marcel also researched the processing of polysemous words (e.g. difference in process of \"palm\" when preceded by masked word \"wrist\" vs. \"tree\"). This research showed qualitative differences between conscious and unconscious perception of words with multiple meanings, namely that both meanings seemed to be simultaneously activated when presented unconsciously, but only one meaning was activated at a time when presented consciously.",
            "score": 209.59706115722656
        },
        {
            "docid": "433584_4",
            "document": "McGurk effect . Vision is the primary sense for humans, but speech perception is multimodal, which means that it involves information from more than one sensory modality, in particular, audition and vision. The McGurk effect arises during phonetic processing because the integration of audio and visual information happens early in speech perception. The McGurk effect is very robust; that is, knowledge about it seems to have little effect on one's perception of it. This is different from certain optical illusions, which break down once one 'sees through' them. Some people, including those that have been researching the phenomenon for more than twenty years, experience the effect even when they are aware that it is taking place. With the exception of people who can identify most of what is being said from speech-reading alone, most people are quite limited in their ability to identify speech from visual-only signals. A more extensive phenomenon is the ability of visual speech to increase the intelligibility of heard speech in a noisy environment. Visible speech can also alter the perception of perfectly audible speech sounds when the visual speech stimuli are mismatched with the auditory speech. Normally, speech perception is thought to be an auditory process; however, our use of information is immediate, automatic, and, to a large degree, unconscious and therefore, despite what is widely accepted as true, speech is not only something we hear. Speech is perceived by all of the senses working together (seeing, touching, and listening to a face move). The brain is often unaware of the separate sensory contributions of what it perceives. Therefore, when it comes to recognizing speech the brain cannot differentiate whether it is seeing or hearing the incoming information.",
            "score": 206.8773651123047
        },
        {
            "docid": "33827415_12",
            "document": "Unconscious cognition . It has been well established that the unconscious plays a vital role in perception and data analysis. The numerous examples of optical illusions, hallucinations and other tricks that the unconscious brain plays on the conscious brain provide ample evidence of the active role of the unconscious mind during data gathering and analysis. Several experiments have been performed to show that the unconscious brain is able to gather data at a much faster rate than the conscious brain and also that the unconscious brain filters out a great amount of information and can use this information to influence cognitive decision making processes.",
            "score": 205.08717346191406
        },
        {
            "docid": "29354346_7",
            "document": "Change deafness . One study used fMRI data to distinguish neural correlates of physical changes in auditory input (independent of conscious change detection), from those of conscious perception of change (independent of an actual physical change). The study made use of a change deafness paradigm in which participants were exposed to complex auditory scenes consisting of six individual auditory streams differing in pitch, rhythm, and sound source location, and received a cue indicating which stream to attend to. Each participant listened to two consecutively presented auditory scenes after which they were prompted to indicate whether both scenes were identical or not. Functional MRI results revealed that physical change in stimulus was correlated with increased BOLD responses in the right auditory cortex, near the lateral portion of Heschl's gyrus, the first cortical structure to process incoming auditory information, but not in hierarchically higher brain regions. Conscious change detection was correlated with increased coupled responses in the ACC and the right insula, consistent with additional evidence that the anterior insula functions to mediate dynamic interactions between other brain networks involved in attention to external stimuli, forming a salience network with the ACC that identifies salient stimulus events and initiates additional processing. In absence of change detection, this salience network was not activated; however increased activity in other cortical areas suggests that undetected changes are still perceived on some level, but fail to trigger conscious change detection, thus producing the change deafness phenomenon.",
            "score": 205.0313720703125
        },
        {
            "docid": "2872287_23",
            "document": "Neural binding . Much of the experimental evidence for neural binding has traditionally revolved around sensory awareness. Sensory awareness is accomplished by integrating things together by cognitively perceiving them and then segmenting them so that, in total, there is an image created. Since there can be an infinite number of possibilities in the perception of an object, this has been a unique area of study. The way the brain then collectively pieces certain things together via networking is important not only in the global way of perceiving but also in segmentation. Much of sensory awareness has to do with the taking of a single piece of an object's makeup and then binding its total characteristics so that the brain perceives the object in its final form. Much of the research for the understanding of segmentation and how the brain perceives an object has been done by studying cats. A major finding of this research has to do with the understanding of gamma waves oscillating at 40\u00a0Hz. The information was extracted from a study using the cat visual cortex. It was shown that the cortical neurons responded differently to spatially different objects. These firings of neurons ranged from 40\u201360\u00a0Hz in measure and when observed showed that they fired synchronously when observing different parts of the object. Such coherent responses point to the fact that the brain is doing a kind of coding where it is piecing certain neurons together in the works of making the form of an object. Since the brain is putting these segmented pieces together unsupervised, a significant consonance is found with many philosophers (like Sigmund Freud) who theorize an underlying subconscious that helps to form every aspect of our conscious thought processes.",
            "score": 204.14564514160156
        },
        {
            "docid": "18345264_13",
            "document": "Neural correlates of consciousness . The possibility of precisely manipulating visual percepts in time and space has made vision a preferred modality in the quest for the NCC. Psychologists have perfected a number of techniques \u2013 masking, binocular rivalry, continuous flash suppression, motion induced blindness, change blindness, inattentional blindness \u2013 in which the seemingly simple and unambiguous relationship between a physical stimulus in the world and its associated percept in the privacy of the subject's mind is disrupted. In particular a stimulus can be perceptually suppressed for seconds or even minutes at a time: the image is projected into one of the observer's eyes but is invisible, not seen. In this manner the neural mechanisms that respond to the subjective percept rather than the physical stimulus can be isolated, permitting visual consciousness to be tracked in the brain. In a \"perceptual illusion\", the physical stimulus remains fixed while the percept fluctuates. The best known example is the \"Necker cube\" whose 12 lines can be perceived in one of two different ways in depth. A perceptual illusion that can be precisely controlled is \"binocular rivalry\". Here, a small image, e.g., a horizontal grating, is presented to the left eye, and another image, e.g., a vertical grating, is shown to the corresponding location in the right eye. In spite of the constant visual stimulus, observers consciously see the horizontal grating alternate every few seconds with the vertical one. The brain does not allow for the simultaneous perception of both images.",
            "score": 202.6315460205078
        },
        {
            "docid": "9125234_4",
            "document": "Axel Cleeremans . His work, broadly situated within the area of consciousness research, has focused on the nature of the processes underlying incidental (or implicit, or unconscious) learning. Specifically, he focuses on the distinction and/or similarities between how learning with and without consciousness takes place in the brain. His main assumption is that consciousness is a graded phenomenon, and that differences between conscious and unconscious information processing result from graded differences in the quality of the underlying neural representations (e.g., strength, stability, distinctiveness), differences which themselves accrue as a result of learning. Thus, while learning may occur without consciousness, consciousness without learning is not possible.",
            "score": 201.4491424560547
        },
        {
            "docid": "5664_46",
            "document": "Consciousness . A number of studies have shown that activity in primary sensory areas of the brain is not sufficient to produce consciousness: it is possible for subjects to report a lack of awareness even when areas such as the primary visual cortex show clear electrical responses to a stimulus. Higher brain areas are seen as more promising, especially the prefrontal cortex, which is involved in a range of higher cognitive functions collectively known as executive functions. There is substantial evidence that a \"top-down\" flow of neural activity (i.e., activity propagating from the frontal cortex to sensory areas) is more predictive of conscious awareness than a \"bottom-up\" flow of activity. The prefrontal cortex is not the only candidate area, however: studies by Nikos Logothetis and his colleagues have shown, for example, that visually responsive neurons in parts of the temporal lobe reflect the visual perception in the situation when conflicting visual images are presented to different eyes (i.e., bistable percepts during binocular rivalry).",
            "score": 201.37774658203125
        },
        {
            "docid": "6840372_12",
            "document": "Jeffrey Alan Gray . Gray likened conscious perception to a sketch made of a particular scene that is retained for use as a record or reminder of that scene. In this way, the sketch is causal in the sense that it performs the function of recalling or assisting memories, but it is not directly active in the brain. In Gray's consciousness model, the conscious perception plays much the same role as the sketch in his analogy. Consciousness is causal, in the sense that downstream unconscious systems respond to it, mainly in the area of error correction. However, this conscious aspect of the brain has no agency or freewill, with which to initiate or inhibit actions, any more than the sketch on a piece of paper can initiate actions independently.",
            "score": 199.5167694091797
        },
        {
            "docid": "3883287_8",
            "document": "Tranquillity . Within tranquillity studies, much of the emphasis has been placed on understanding the role of vision in the perception of natural environments, which is probably not surprising, considering that upon first viewing a scene its configurational coherence can be established with incredible speed. Indeed, scene information can be captured in a single glance and the gist of a scene determined in as little as 100ms. The speed of processing of complex natural images was tested by Thorpe \"et al.\" using colour photographs of a wide range of animals (mammals, birds, reptiles and fish), in their natural environments, mixed with distracters that included pictures of forests, mountains, lakes, buildings and fruit. During this experiment, subjects were shown an image for 20ms and asked to determine whether it contained an animal or not. The electrophysiological brain responses obtained in this study showed that a decision could be made within 150ms of the image being seen, indicating the speed at which cognitive visual processing occurs. However, audition, and in particular the individual components that collectively comprise the soundscape, a term coined by Schafer to describe the ever-present array of sounds that constitute the sonic environment, also significantly inform the various schemata used to characterise differing landscape types. This interpretation is supported by the auditory reaction times, which are 50 to 60ms faster than that of the visual modality. It is also known that sound can alter visual perception and that under certain conditions areas of the brain involved in processing auditory information can be activated in response to visual stimuli.  Research conducted by Pheasant \"et al.\" has shown that when individuals make tranquillity assessments based on a uni-modal auditory or visual sensory input, they characterise the environment by drawing upon a number of key landscape and soundscape characteristics. For example, when making assessments in response to visual-only stimuli the percentage of water, flora and geological features present within a scene, positively influence how tranquil a location is perceived to be. Likewise when responding to uni-modal auditory stimuli, the perceived loudness of biological sounds positively influences the perception of tranquillity, whilst the perceived loudness of mechanical sounds have a negative effect. However, when presented with bi-modal auditory-visual stimuli the individual soundscape and landscape components alone no longer influenced the perception of tranquillity. Rather configurational coherence was provided by the percentage of natural and contextual features present within the scene and the equivalent continuous sound pressure level (LAeq).",
            "score": 199.3357391357422
        },
        {
            "docid": "20429570_14",
            "document": "Motor imagery . Motor imagery is close to the notion of simulation used in cognitive and social neuroscience to account for different processes. An individual who is engaging in simulation may replay his own past experience in order to extract from it pleasurable, motivational or strictly informational properties. Such a view was clearly described by the Swedish physiologist Hesslow. For this author, the simulation hypothesis states that thinking consists of simulated interaction with the environment, and rests on the following three core assumptions: (1) Simulation of actions: we can activate motor structures of the brain in a way that resembles activity during a normal action but does not cause any overt movement; (2) Simulation of perception: imagining perceiving something is essentially the same as actually perceiving it, only the perceptual activity is generated by the brain itself rather than by external stimuli; (3) Anticipation: there exist associative mechanisms that enable both behavioral and perceptual activity to elicit other perceptual activity in the sensory areas of the brain. Most importantly, a simulated action can elicit perceptual activity that resembles the activity that would have occurred if the action had actually been performed.",
            "score": 198.23081970214844
        },
        {
            "docid": "1496804_21",
            "document": "Hidden personality . Unconscious processing goes on in the mind of humans, not because we have to filter out threatening stimuli and impulses, but because many cognitive operations go on without conscious participation. The brain operates in this way in order not to flood the conscious part of the mind with impressions. The unconscious is a type of process, a way of constructing perception, memories and other kinds of cognition, not a portion of the mind. This view agrees with Roger's concept of the unconscious, who theorised that the unconscious is only a part of the phenomenological field and does not control our personality.",
            "score": 198.12107849121094
        },
        {
            "docid": "2872287_26",
            "document": "Neural binding . Cognitive binding is associated with the different states of human consciousness. Two of the most studied states of consciousness are the wakefulness and REM sleep. There have been multiple studies showing, electrophysiologically, that these two states are quite similar in nature. This has led some neural binding theorists to study the modes of cognitive awareness in each state. Certain observations have even led these scientists to hypothesize that since there is little cognition going on during REM sleep, the increased thalamocortical responses show the action of processing in the waking preconscious. The thalamus and cortex are important anatomical features in cognitive and sensory awareness. The understanding of how these neurons fire and relate to one other in each of these states (REM and Waking) is paramount to understanding awareness and its relation to neural binding. In the waking state, neuronal activity in animals is subject to changes based on the current environment. Changes in environment act as a form of stress on the brain so that when sensory neurons are then fired synchronously, they acclimate to the new state. This new state can then be moved to the hippocampus where it can be stored for later use. In the words of James Newman and Anthony A. Grace in their article, \"Binding Across Time\" this idea is put forth: \"The hippocampus is the primary recipient of inferotemporal outputs and is known to be the substrate for the consolidation of working memories to long term, episodic memories.\" The logging of \"episodes\" is then used for \"streaming\", which can mediate by the selective gating of certain information reentering sensory awareness. Streaming and building of episodic memories would not be possible if neural binding did not unconsciously connect the two synchronous oscillations. The pairing of these oscillations can then help input the correct sensory material. If these paired oscillations are not new, then cognitively these firings will be easily understood. If there are new firings, the brain will have to acclimate to the new understanding. In REM sleep, the only extreme difference from the waking state is that the brain does not have the actual waking amount of sensory firings, so cognitively, there is not as much awareness here, although the activity of the \"brain\u2019s eye\" is still quite significant and very similar to the waking state. Studies have shown that during sleep there are still 40\u00a0Hz Oscillation firings. These firings are due to the perceived stimuli happening in dreams. \"",
            "score": 197.87673950195312
        },
        {
            "docid": "5664_54",
            "document": "Consciousness . Regarding the primary function of conscious processing, a recurring idea in recent theories is that phenomenal states somehow integrate neural activities and information-processing that would otherwise be independent. This has been called the \"integration consensus\". Another example has been proposed by Gerald Edelman called dynamic core hypothesis which puts emphasis on reentrant connections that reciprocally link areas of the brain in a massively parallel manner. Edelman also stresses the importance of the evolutionary emergence of higher-order consciousness in humans from the historically older trait of primary consciousness which humans share with non-human animals (see \"Neural correlates\" section above). These theories of integrative function present solutions to two classic problems associated with consciousness: differentiation and unity. They show how our conscious experience can discriminate between a virtually unlimited number of different possible scenes and details (differentiation) because it integrates those details from our sensory systems, while the integrative nature of consciousness in this view easily explains how our experience can seem unified as one whole despite all of these individual parts. However, it remains unspecified which kinds of information are integrated in a conscious manner and which kinds can be integrated without consciousness. Nor is it explained what specific causal role conscious integration plays, nor why the same functionality cannot be achieved without consciousness. Obviously not all kinds of information are capable of being disseminated consciously (e.g., neural activity related to vegetative functions, reflexes, unconscious motor programs, low-level perceptual analyses, etc.) and many kinds of information can be disseminated and combined with other kinds without consciousness, as in intersensory interactions such as the ventriloquism effect. Hence it remains unclear why any of it is conscious. For a review of the differences between conscious and unconscious integrations, see the article of E. Morsella.",
            "score": 197.7406005859375
        },
        {
            "docid": "43389476_7",
            "document": "Consciousness and the Brain . Dehaene reviews unconscious brain processing of various forms: subliminal perception, \u00c9douard Clapar\u00e8de's pinprick experiment, blindsight, hemispatial neglect, subliminal priming, unconscious binding (including across sensory modalities, as in the McGurk effect), etc. Dehaene discusses a debate over whether \"meaning\" can be processed unconsciously and concludes based on his own research that it can be. An N400 meaning-based wave occurs for unexpected words even when masked or not attended to. Unconscious processing is not just bottom-up but can be enhanced when top-down attention is directed toward a target, even if the target never becomes conscious. Brains can even do some mathematical operations unconsciously, and sitting on a problem to let the unconscious mind work out an answer has proved helpful in several experiments.",
            "score": 196.94845581054688
        },
        {
            "docid": "53953041_3",
            "document": "Predictive coding . Theoretical ancestors to predictive coding date back as early as 1860 with Helmholz\u2019s concept of unconscious inference (Clark, 2013). Unconscious inference refers to the idea that the human brain fills in visual information to make sense of a scene. For example, if something is relatively smaller than another object in the visual field, the brain uses that information as a likely cue of depth, such that the perceiver ultimately (and involuntarily) experiences depth. The understanding of perception as the interaction between sensory stimuli (bottom-up) and conceptual knowledge (top-down) continued to be established by Jerome Bruner (psychologist) who, starting in the 1940s, studied the ways in which needs, motivations and expectations influence perception, research that came to be known as 'New Look' psychology. In 1981, McClelland and Rumelhart in their seminal paper examined the interaction between processing features (lines and contours) which form letters, which in turn form words. While the features suggest the presence of a word, they found that when letters were situated in the context of a word, people were able to identify them faster than when they were situated in a non-word without semantic context. McClelland and Rumelhart\u2019s parallel processing model describes perception as the meeting of top-down (conceptual) and bottom-up (sensory) elements.",
            "score": 196.18821716308594
        },
        {
            "docid": "35347567_7",
            "document": "Antti Revonsuo . According to Revonsuo, the dreaming brain is particularly suitable model system for the study of consciousness because it generates a conscious experience while being isolated from both sensory input and motor output. Regarding the rival paradigm of visual awareness, Revonsuo argues that it does not allow one to distinguish between consciousness and perception. Revonsuo holds that there is a \"'double dissociation' between consciousness and perceptual input\". Accordingly, dreams are conscious experiences, which occur without any perceptual stimuli, and, conversely, perceptual input does not automatically engender conscious experience. In support of the independence of consciousness from perception, Revonsuo cites Stephen LaBerge's case study on a lucid dreamer performing previously agreed upon eye movements to signal to the experimenters that he had become conscious of the fact that he was dreaming. A second study that supports Revonsuo's view of dreams was conducted by Allan Rechtschaffen and Foulkes (1965). In this study, subjects were made to sleep with their eyelids open, thus allowing the visual cortex to receive visual stimuli. Though their eyes were open, and the perceptual input was accessible, the subjects could not see the stimuli and did not report dreaming of it. It is the brain that is having the internal experience, independent of perceptual input. This internalist view of consciousness leads Revonsuo to compare both dreaming and waking consciousness with a virtual reality simulation decoupled from or only indirectly informed by a brain's external environment.",
            "score": 196.1018829345703
        },
        {
            "docid": "50460577_7",
            "document": "Evolutionary theory of the self . This variety of results of the studies have provided evidence for Gillihan and Farah to conclude that there is not a unitary and common neural system concerning the self, which leads into the next two problems of the traditional conception of self. These problems are that the traditional conceptions of the self argue that the self is a unified mechanism that exists in a centralized area in the brain, and that the self is something we can consciously sense. The problems with these accounts are that there is scientific research contradicting the claims of traditional conceptions. Llinas suggests that the self is a form of perception and thus that the self is an invention of the brain just as secondary sensory qualities are. He argues that there is no one brain area that could account for the self, and he concludes that the self doesn\u2019t exist. Although Llinas is confusing the conscious and unconscious self, because when he suggests the self as a form of perception he should be referring to the conscious self being a form of perception. Munevar suggests that Llinas is incorrect in assuming the self does not exist, because just as an elephant and the perception of an elephant are different things, so are the self and our perception of the self.",
            "score": 195.9934539794922
        },
        {
            "docid": "36086848_17",
            "document": "Fear processing in the brain . The perception of fear is elicited by many different stimuli and involves the process described above in biochemical terms. Neural correlates of the interaction between language and visual information has been studied by Roel Willems \"et al\". The study consisted of observing how visual and linguistic information interact in the perception of emotion. A common phenomenon from film theory was borrowed which states that the presentation of a neutral visual scene intensifies the percept of fear or suspense induced by a different channel of information, such as language. This principle has been applied in a way in which the percept of fear was present and amplified in the presence of a neutral visual stimuli. The main idea is that the visual stimuli intensify the fearful content of the stimuli (i.e. language) by subtly implying and concretizing what is described in the context (i.e. sentence). Activation levels in the right anterior temporal pole were selectively increased and is believed to serve as a binding function of emotional information across domains such as visual and linguistic information.",
            "score": 194.8917236328125
        },
        {
            "docid": "43389476_15",
            "document": "Consciousness and the Brain . Dehaene and colleagues have developed computer simulations of neural dynamics that successfully replicate the way in which distributed processing at the brain's periphery gives way to a stable, serial \"thought\" at higher levels due to feedback amplification of one signal and inhibition of others. The simulation showed the four signatures of consciousness described in Ch. 4 (p. 184). Consciousness seemed to behave like a \"phase transition\" between one unconscious stable state of low-level activity and another conscious state consisting of snowballing self-amplification and reverberation (p. 184). Subliminal stimuli fail to become conscious because by the time the higher layers try to amplify the signal, the original input stimulation has vanished (p. 193).",
            "score": 194.5608673095703
        },
        {
            "docid": "5212945_3",
            "document": "Visual neuroscience . A recent study using Event-Related Potentials (ERPs) linked an increased neural activity in the occipito-temporal region of the brain to the visual categorization of facial expressions. Results focus on a negative peak in the ERP that occurs 170 milliseconds after the stimulus onset. This action potential, called the N170, was measured using electrodes in the occipito-temporal region, an area already known to be changed by face stimuli. Studying by using the EEG, and ERP methods allow for an extremely high temporal resolution of 4 milliseconds, which makes these kinds of experiments extremely well suited for accurately estimating and comparing the time it takes the brain to perform a certain function. Scientists used classification image techniques, to determine what parts of complex visual stimuli (such as a face) will be relied on when patients are asked to assign them to a category, or emotion. They computed the important features when the stimulus face exhibited one of five different emotions. Stimulus faces exhibiting fear had the distinguishing feature of widening eyes, and stimuli exhibiting happiness exhibited a change in the mouth to make a smile. Regardless of the expression of the stimuli's face, the region near the eyes affected the EEG before the regions near the mouth. This revealed a sequential, and predetermined order to the perception and processing of faces, with the eye being the first, and the mouth, and nose being processed after. This process of downward integration only occurred when the inferior facial features were crucial to the categorization of the stimuli. This is best explained by comparing what happens when participants were shown a face exhibiting fear, versus happiness. The N170 peaked slightly earlier for the fear stimuli at about 175 milliseconds, meaning that it took a participants less time to recognize the facial expression. This is expected because only the eyes need to be processed to recognize the emotion. However, when processing a happy expression, where the mouth is crucial to categorization, downward integration must take place, and thus the N170 peak occurred later at around 185 milliseconds. Eventually visual neuroscience aims to completely explain how the visual system processes all changes in faces as well as objects. This will give a complete view to how the world is constantly visually perceived, and may provide insight into a link between perception and consciousness.",
            "score": 194.54013061523438
        },
        {
            "docid": "5366050_50",
            "document": "Speech perception . Neurophysiological methods rely on utilizing information stemming from more direct and not necessarily conscious (pre-attentative) processes. Subjects are presented with speech stimuli in different types of tasks and the responses of the brain are measured. The brain itself can be more sensitive than it appears to be through behavioral responses. For example, the subject may not show sensitivity to the difference between two speech sounds in a discrimination test, but brain responses may reveal sensitivity to these differences. Methods used to measure neural responses to speech include event-related potentials, magnetoencephalography, and near infrared spectroscopy. One important response used with event-related potentials is the mismatch negativity, which occurs when speech stimuli are acoustically different from a stimulus that the subject heard previously.",
            "score": 192.55398559570312
        },
        {
            "docid": "5664_64",
            "document": "Consciousness . In neuroscience, a great deal of effort has gone into investigating how the perceived world of conscious awareness is constructed inside the brain. The process is generally thought to involve two primary mechanisms: (1) hierarchical processing of sensory inputs, and (2) memory. Signals arising from sensory organs are transmitted to the brain and then processed in a series of stages, which extract multiple types of information from the raw input. In the visual system, for example, sensory signals from the eyes are transmitted to the thalamus and then to the primary visual cortex; inside the cerebral cortex they are sent to areas that extract features such as three-dimensional structure, shape, color, and motion. Memory comes into play in at least two ways. First, it allows sensory information to be evaluated in the context of previous experience. Second, and even more importantly, working memory allows information to be integrated over time so that it can generate a stable representation of the world\u2014Gerald Edelman expressed this point vividly by titling one of his books about consciousness \"The Remembered Present\". In computational neuroscience, Bayesian approaches to brain function have been used to understand both the evaluation of sensory information in light of previous experience, and the integration of information over time. Bayesian models of the brain are probabilistic inference models, in which the brain takes advantage of prior knowledge to interpret uncertain sensory inputs in order to formulate a conscious percept; Bayesian models have successfully predicted many perceptual phenomena in vision and the nonvisual senses.",
            "score": 192.3145751953125
        },
        {
            "docid": "24978422_5",
            "document": "Visual adaptation . Perceptual aftereffects for face recognition occur for several different stimuli, including gender, ethnicity, identity, emotion, and attractiveness of a face. The fact that this distinction occurs, implies that face recognition is a process that happens on a higher level and later on in the visual encoding, rather than early on within visual adaptation. The fact that the aftereffects in face recognition in particular are so strong, suggests that it is for the purpose of regulation of how processes work. This provides a sense of constancy in an individual's perception, while adapting to differences and possible versions of a stimulus allows for constancy and stability, and makes it easier to adapt to variations in a stimulus, while recognizing commonalities. These face perception cues are encoded in an individual's brain for extended periods of time, ensuring consistency over the individual's lifespan. A young person would perceive stimuli the same way as an older individual.",
            "score": 191.591064453125
        },
        {
            "docid": "5119916_49",
            "document": "Character education . Other neurological research is documenting how much the unconscious mind is involved in decision making. According to cognitive neuroscientists, we are conscious of only about 5 percent of our cognitive activity, so most of our decisions, actions, emotions, and behavior depends on the 95 percent of brain activity that goes beyond our conscious awareness. These studies show that actions come from preconscious brain activity patterns and not from people consciously thinking about what they are going to do.\u0094",
            "score": 191.3342742919922
        },
        {
            "docid": "1677048_16",
            "document": "Inattentional blindness . For example, in an functional magnetic resonance imaging (fMRI) study by Rees and colleagues, brain activity was recorded while participants completed a perceptual task. Here they examined the neural processing of meaningful (words) and meaningless (consonant string) stimuli both when attended to, and when these same items were unattended. While no difference in activation patterns were found between the groups when the stimuli were unattended, differences in neural processing were observed for meaningful versus meaningless stimuli to which participants overtly attended. This pattern of results suggests that ignored stimuli are not processed to the level of meaning, i.e. less extensively than attended stimuli. Participants do not seem to be detecting meaning in stimuli to which they are not consciously attending.",
            "score": 190.12872314453125
        },
        {
            "docid": "32349940_14",
            "document": "Berit Brogaard . Brogaard is also the first researcher to show that consciousness comes in degrees and that there can be borderline cases of consciousness. Imagine a case where we slowly destroy the primary visual cortex of a subject, one neuron at a time in an arbitrary fashion. Plausibly such an individual would proceed slowly from perceiving her surroundings normally to perceiving them unconsciously. In this process, the brightness of the perceived content would gradually decrease until a point at which it would be unclear whether the perception counted as weakly conscious.",
            "score": 189.85134887695312
        },
        {
            "docid": "5664_50",
            "document": "Consciousness . Assuming that not only humans but even some non-mammalian species are conscious, a number of evolutionary approaches to the problem of neural correlates of consciousness open up. For example, assuming that birds are conscious \u2014 a common assumption among neuroscientists and ethologists due to the extensive cognitive repertoire of birds \u2014 there are comparative neuroanatomical ways to validate some of the principal, currently competing, mammalian consciousness\u2013brain theories. The rationale for such a comparative study is that the avian brain deviates structurally from the mammalian brain. So how similar are they? What homologues can be identified? The general conclusion from the study by Butler, et al., is that some of the major theories for the mammalian brain also appear to be valid for the avian brain. The structures assumed to be critical for consciousness in mammalian brains have homologous counterparts in avian brains. Thus the main portions of the theories of Crick and Koch, Edelman and Tononi, and Cotterill seem to be compatible with the assumption that birds are conscious. Edelman also differentiates between what he calls primary consciousness (which is a trait shared by humans and non-human animals) and higher-order consciousness as it appears in humans alone along with human language capacity. Certain aspects of the three theories, however, seem less easy to apply to the hypothesis of avian consciousness. For instance, the suggestion by Crick and Koch that layer 5 neurons of the mammalian brain have a special role, seems difficult to apply to the avian brain, since the avian homologues have a different morphology. Likewise, the theory of Eccles seems incompatible, since a structural homologue/analogue to the dendron has not been found in avian brains. The assumption of an avian consciousness also brings the reptilian brain into focus. The reason is the structural continuity between avian and reptilian brains, meaning that the phylogenetic origin of consciousness may be earlier than suggested by many leading neuroscientists.",
            "score": 188.62396240234375
        }
    ]
}