{
    "q": [
        {
            "docid": "5051081_4",
            "document": "Eric Knudsen . In 1978, Knudsen and Konishi presented the discovery of an auditory map of space in the midbrain of the barn owl. This discovery was groundbreaking because it unearthed the first non-somatotopic space map in the brain. The map was found in the owl\u2019s midbrain, in the lateral and anterior mesencephalicus lateralis dorsalis (MLD), a structure now referred to as the inferior colliculus. Unlike most sound-localization maps, this map was found to be two-dimensional, with units arranged spatially to represent both the vertical and horizontal location of sound. Knudsen and Konishi discovered that units in this structure respond preferentially to sounds originating in a particular region in space. In the 1978 paper, elevation and azimuth (location in the horizontal plane) were shown to be the two coordinates of the map. Using a speaker set on a rotatable hemispherical track, Knudsen and Konishi presented owls with auditory stimulus from various locations in space and recorded the resulting neuronal activity. They found that neurons in this part of the MLD were organized according to the location of their receptive field, with azimuth varying along the horizontal plane of the space map and elevation varying vertically.  Knudsen followed this discovery with research into specific sound localization mechanisms. Two main auditory cues used by the barn owl to localize sound are interaural time difference (ITD) and interaural intensity difference (IID). The owl\u2019s ears are asymmetric, with the right ear\u2019s opening being directed higher than that of the left. This asymmetry allows the barn owl to determine the elevation of a sound by comparing sound levels between its two ears. Interaural time differences provide the owl with information regarding a sound\u2019s azimuth; sound will reach the ear closer to the sound source before reaching the farther ear, and this time difference can be detected and interpreted as an azimuthal direction. At low frequencies, the wavelength of a sound is wider than the owl's facial ruff, and the ruff does not affect detection of azimuth. At high frequencies, the ruff plays a role in reflecting sound for heightened sensitivity to vertical elevation. Therefore, with wide-band noise, containing both high and low frequencies, the owl could use interaural spectrum difference to obtain information about both azimuth and elevation. In 1979, Knudsen and Konishi showed that the barn owl uses interaural spectrum information in sound localization. They presented owls with both wide-bandwidth noise and pure tones. The birds were able to successfully locate pure tones (since they could still gather information from IID and ITD), but their error rate was much lower when localizing wide-bandwidth noise. This indicates that the birds utilize interaural spectrum differences to improve their accuracy.",
            "score": 266.03745579719543
        },
        {
            "docid": "4548229_2",
            "document": "Interaural time difference . The interaural time difference (or ITD) when concerning humans or animals, is the difference in arrival time of a sound between two ears. It is important in the localization of sounds, as it provides a cue to the direction or angle of the sound source from the head. If a signal arrives at the head from one side, the signal has further to travel to reach the far ear than the near ear. This pathlength difference results in a time difference between the sound's arrivals at the ears, which is detected and aids the process of identifying the direction of sound source.",
            "score": 231.51270198822021
        },
        {
            "docid": "41087200_8",
            "document": "Perceptual-based 3D sound localization . Interaural level differences (ILD) represents the difference in sound pressure level reaching the two ears. They provide salient cues for localizing high-frequency sounds in space, and populations of neurons that are sensitive to ILD are found at almost every synaptic level from brain stem to cortex. These cells are predominantly excited by stimulation of one ear and predominantly inhibited by stimulation of the other ear, such that the magnitude of their response is determined in large part by the intensities at the 2 ears. This gives rise to the concept of resonant damping. Interaural level difference (ILD) is best for high frequency sounds because low frequency sounds are not attenuated much by the head. ILD (also known as Interaural Intensity Difference) arises when the sound source is not centred, the listener's head partially shadows the ear opposite to the source, diminishing the intensity of the sound in that ear (particularly at higher frequencies). The pinnae filters the sound in a way that is directionally dependent. This is particularly useful in determining if a sound comes from above, below, in front, or behind.",
            "score": 237.29586327075958
        },
        {
            "docid": "47338295_4",
            "document": "Sound localization in owls . ITD occurs whenever the distance from the source of sound to the two ears is different, resulting in differences in the arrival times of the sound at the two ears. When the sound source is directly in front of the owl, there is no ITD, i.e. the ITD is zero. In sound localization, ITDs are used as cues for location in the azimuth. ITD changes systematically with azimuth. Sounds to the right arrive first at the right ear; sounds to the left arrive first at the left ear.",
            "score": 208.59051871299744
        },
        {
            "docid": "1021754_24",
            "document": "Sound localization . For frequencies above 1600\u00a0Hz the dimensions of the head are greater than the length of the sound waves. An unambiguous determination of the input direction based on interaural phase alone is not possible at these frequencies. However, the interaural level differences become larger, and these level differences are evaluated by the auditory system. Also, group delays between the ears can be evaluated, and is more pronounced at higher frequencies; that is, if there is a sound onset, the delay of this onset between the ears can be used to determine the input direction of the corresponding sound source. This mechanism becomes especially important in reverberant environments. After a sound onset there is a short time frame where the direct sound reaches the ears, but not yet the reflected sound. The auditory system uses this short time frame for evaluating the sound source direction, and keeps this detected direction as long as reflections and reverberation prevent an unambiguous direction estimation. The mechanisms described above cannot be used to differentiate between a sound source ahead of the hearer or behind the hearer; therefore additional cues have to be evaluated.",
            "score": 214.80854547023773
        },
        {
            "docid": "14532984_7",
            "document": "Coincidence detection in neurobiology . Coincidence detection has been shown to be a major factor in sound localization along the azimuth plane in several organisms. In 1948, Lloyd A. Jeffress proposed that some organisms may have a collection of neurons that receive auditory input from each ear. The neural pathways to these neurons are called delay lines. Jeffress claimed that the neurons that the delay lines link act as coincidence detectors by firing maximally when receiving simultaneous inputs from both ears. When a sound is heard, sound waves may reach the ears at different times. This is referred to as the interaural time difference (ITD). Due to differing lengths and a finite conduction speed within the axons of the delay lines, different coincidence detector neurons will fire when sound comes from different positions along the azimuth. Jeffress' model proposes that two signals even from an asynchronous arrival of sound in the cochlea of each ear will converge synchronously on a coincidence detector in the auditory cortex based on the magnitude of the ITD (Fig. 2). Therefore, the ITD should correspond to an anatomical map that can be found within the brain. Masakazu Konishi's study on barn owls shows that this is true. Sensory information from the hair cells of the ears travels to the ipsilateral nucleus magnocellularis. From here, the signals project ipsilaterally and contralaterally to two nucleus laminari. Each nucleus laminaris contains coincidence detectors that receive auditory input from the left and the right ear. Since the ipsilateral axons enter the nucleus laminaris dorsally while the contralateral axons enter ventrally, sounds from various positions along the azimuth correspond directly to stimulation of different depths of the nucleus laminaris. From this information, a neural map of auditory space was formed. The function of the nucleus laminaris parallels that of the medial superior olive in mammals.",
            "score": 230.9879915714264
        },
        {
            "docid": "3154127_4",
            "document": "Virtual acoustic space . When one listens to sounds over headphones (in what is known as the \"closed field\") the sound source appears to arise from center of the head. On the other hand, under normal, so-called free-field, listening conditions sounds are perceived as being externalized. The direction of a sound in space (see sound localization) is determined by the brain when it analyses the interaction of incoming sound with head and external ears. A sound arising to one side reaches the near ear before the far ear (creating an interaural time difference, ITD), and will also be louder at the near ear (creating an interaural level difference, ILD \u2013 also known as interaural intensity difference, IID). These binaural cues allow sounds to be lateralized. Although conventional stereo headphone signals make used of ILDs (not ITDs) the sound is not perceived as being externalized.",
            "score": 222.69942378997803
        },
        {
            "docid": "32116125_4",
            "document": "Amblyaudia . Amblyaudia is a deficit in binaural integration of environmental information entering the auditory system. It is a disorder related to brain organization and function rather than what is typically considered a \u201chearing loss\u201d (damage to the cochlea). It may be genetic or developmentally acquired or both. When animals are temporarily deprived of hearing from an early age, profound changes occur in the brain. Specifically, cell sizes in brainstem nuclei are reduced, the configuration of brainstem dendrites are altered and neurons respond in different ways to sounds presented to both the deprived and non-deprived ears (in cases of asymmetric deprivation). This last point is particularly important for listening tasks that require inputs from two ears to perform well. There are multiple auditory functions that rely on the computation of well calibrated inputs from the two ears. Chief among these is the ability to localize sound sources and separate what we want to hear from a background of noise. In the brainstem, the auditory system compares the timing and levels of sounds between the two ears to encode the location of sound sources (sounds that originate from our right as opposed to left side are louder and arrive earlier in our right ear). This ability to separate sound sources not only helps us locate the trajectories of moving objects, but also to separate different sound sources in noisy environments.",
            "score": 215.05756652355194
        },
        {
            "docid": "39200136_12",
            "document": "Hans Wallach . In a series of papers Wallach explored the ability of humans to locate sounds in the median plane \u2013 that is, to determine whether a sound comes from a source at the same elevation as the ears or from a source that is higher or lower, or even in back of the head. Binaural sound cues, including the phasing or time of the sound\u2019s arrival at each ear and the sound\u2019s relative intensity at the two ears (known respectively as ITD and ILD) enable a listener to determine a sound\u2019s lateral location (whether it is on the left, right, or straight ahead). But two sounds at different elevations can present identical ITD and ILD information to the ears, and so binaural cues to a stationary ear do not suffice to identify a sound\u2019s location in the median plane. Monaural cues that depend on the shape of the head and the structure of the external ear help with vertical localization, but binaural cues also play a part if the head is not stationary.",
            "score": 242.31894826889038
        },
        {
            "docid": "8436042_6",
            "document": "MPEG Surround . MPEG Surround coding uses our capacity to perceive sound in the 3D and captures that perception in a compact set of parameters. Spatial perception is primarily attributed to three parameters, or cues, describing how humans localize sound in the horizontal plane: Interaural level difference (ILD), Interaural time difference (ITD) and Interaural coherence (IC). This three concepts are illustrated in next image. Direct, or first-arrival, waveforms from the source hit the left ear at time, while direct sound received by the right ear is diffracted around the head, with time delay and level attenuation, associated. These two effects result in ITD and ILD are associated with the main source. At last, in a reverberant environment, reflected sound from the source, or sound from diffuse source, or uncorrelated sound can hit both ears, all of them are related with IC.",
            "score": 209.6608304977417
        },
        {
            "docid": "32105732_4",
            "document": "Spatial hearing loss . Sound streams arriving from the left or right (the horizontal plane) are localised primarily by the small time differences of the same sound arriving at the two ears. A sound straight in front of the head is heard at the same time by both ears. A sound to the side of the head is heard approximately 0.0005 seconds later by the ear furthest away. A sound halfway to one side is heard approximately 0.0003 seconds later. This is the interaural time difference (ITD) cue and is measured by signal processing in the two central auditory pathways that begin after the cochlea and pass through the brainstem and mid-brain. Some of those with spatial hearing loss are unable to process ITD (low frequency) cues.",
            "score": 211.44086480140686
        },
        {
            "docid": "4302005_7",
            "document": "Superior olivary complex . The medial superior olive is thought to help locate the azimuth of a sound, that is, the angle to the left or right where the sound source is located. Sound elevation cues are not processed in the olivary complex. The fusiform cells of the dorsal cochlear nucleus (DCN), which are thought to contribute to localization in elevation, bypass the SOC and project directly to the inferior colliculus. Only horizontal data is present, but it does come from two different ear sources, which aids in the localizing of sound on the azimuth axis. The way in which the superior olive does this is by measuring the differences in time between two ear signals recording the same stimulus. Traveling around the head takes about 700 \u03bcs, and the medial superior olive is able to distinguish time differences much smaller than this. In fact, it is observed that people can detect interaural differences down to 10 \u03bcs. The nucleus is tonotopically organized, but the azimuthal receptive field projection is \"most likely a complex, nonlinear map\".",
            "score": 188.05831503868103
        },
        {
            "docid": "39265695_7",
            "document": "Stimulus filtering . Female flies of the genus \"Ormia ochracea\" possess organs in their bodies that can detect frequencies of cricket sounds from meters away. This process is important for the survival of their species because females will lay their first instar larvae into the body of the cricket, where they will feed and molt for approximately seven days. After this period, the larvae grow into flies and the cricket usually perishes. Researchers were puzzled about how precise hearing ability could arise from a small ear structure. Normal animals detect and locate sounds using the interaural time difference (ITD) and the interaural level difference (ILD). The ITD is the difference in the time it takes sound to reach the ear. ILD is the difference in sound intensity measure between both ears. At maximum, the ITD would only reach about 1.5 microseconds and the ILD would be less than one decibel. These small values make it hard to sense the differences. To solve these issues, researchers studied the mechanical aspects of flies\u2019 ears. They found that they have a presternum structure linking both tympanal membranes that is critical in detecting sound and localization. The structure acts as a lever by transferring and amplifying vibrational energy between the membranes. After sound hits the membranes at different amplitudes, the presternum sets up symmetrical vibration modes through bending and rocking. This effect helps the nervous system distinguish which side the sound is coming from. Because the presternum acts as an intertympanal bridge, the ITD is increased from 1.5 us to 55 us and the ILD is increased from less than one decibel to over 10 decibels.",
            "score": 192.11480486392975
        },
        {
            "docid": "47338295_5",
            "document": "Sound localization in owls . In mammals there is a level difference in sounds at the two ears caused by the sound-shadowing effect of the head. But in many species of owls, level differences arise primarily for sounds that are shifted above or below the elevation of the horizontal plane. This is due to the asymmetry in placement of the ear openings in the owl's head, such that sounds from below the owl reach the left ear first and sounds from above reach the right ear first. IID is a measure of the difference in the level of the sound as it reaches each ear. In many owls, IIDs for high-frequency sounds (higher than 4 or 5\u00a0kHz) are the principal cues for locating sound elevation.",
            "score": 212.38410925865173
        },
        {
            "docid": "5442380_17",
            "document": "Sensory cue . Unless a sound is directly in front of or behind the individual, the sound stimuli will have a slightly different distance to travel to reach each ear. This difference in distance causes a slight delay in the time the signal is perceived by each ear. The magnitude of the interaural time difference is greater the more the signal comes from the side of the head. Thus, this time delay allows humans to accurately predict the location of incoming sound cues. Interaural level difference is caused by the difference in sound pressure level reaching the two ears. This is because the head blocks the sound waves for the further ear, causing less intense sound to reach it. This level difference between the two ears allows humans to accurately predict azimuth of an auditory signal. This effect only occurs at sounds that are high frequency.",
            "score": 239.06031227111816
        },
        {
            "docid": "1021754_43",
            "document": "Sound localization . If the ears are located at the side of the head, similar lateral localization cues as for the human auditory system can be used. This means: evaluation of interaural time differences (interaural phase differences) for lower frequencies and evaluation of interaural level differences for higher frequencies. The evaluation of interaural phase differences is useful, as long as it gives unambiguous results. This is the case, as long as ear distance is smaller than half the length (maximal one wavelength) of the sound waves. For animals with a larger head than humans the evaluation range for interaural phase differences is shifted towards lower frequencies, for animals with a smaller head, this range is shifted towards higher frequencies.",
            "score": 166.55935192108154
        },
        {
            "docid": "161005_5",
            "document": "Head-related transfer function . Humans estimate the location of a source by taking cues derived from one ear (\"monaural cues\"), and by comparing cues received at both ears (\"difference cues\" or \"binaural cues\"). Among the difference cues are time differences of arrival and intensity differences. The monaural cues come from the interaction between the sound source and the human anatomy, in which the original source sound is modified before it enters the ear canal for processing by the auditory system. These modifications encode the source location, and may be captured via an impulse response which relates the source location and the ear location. This impulse response is termed the \"head-related impulse response\" (HRIR). Convolution of an arbitrary source sound with the HRIR converts the sound to that which would have been heard by the listener if it had been played at the source location, with the listener's ear at the receiver location. HRIRs have been used to produce virtual surround sound.",
            "score": 228.11522388458252
        },
        {
            "docid": "1021754_32",
            "document": "Sound localization . When the head is stationary, the binaural cues for lateral sound localization (interaural time difference and interaural level difference) do not give information about the location of a sound in the median plane. Identical ITDs and ILDs can be produced by sounds at eye level or at any elevation, as long as the lateral direction is constant. However, if the head is rotated, the ITD and ILD change dynamically, and those changes are different for sounds at different elevations. For example, if an eye-level sound source is straight ahead and the head turns to the left, the sound becomes louder (and arrives sooner) at the right ear than at the left. But if the sound source is directly overhead, there will be no change in the ITD and ILD as the head turns. Intermediate elevations will produce intermediate degrees of change, and if the presentation of binaural cues to the two ears during head movement is reversed, the sound will be heard behind the listener. Hans Wallach artificially altered a sound\u2019s binaural cues during movements of the head. Although the sound was objectively placed at eye level, the dynamic changes to ITD and ILD as the head rotated were those that would be produced if the sound source had been elevated. In this situation, the sound was heard at the synthesized elevation. The fact that the sound sources objectively remained at eye level prevented monaural cues from specifying the elevation, showing that it was the dynamic change in the binaural cues during head movement that allowed the sound to be correctly localized in the vertical dimension. The head movements need not be actively produced; accurate vertical localization occurred in a similar setup when the head rotation was produced passively, by seating the blindfolded subject in a rotating chair. As long as the dynamic changes in binaural cues accompanied a perceived head rotation, the synthesized elevation was perceived.",
            "score": 233.3003122806549
        },
        {
            "docid": "4548229_5",
            "document": "Interaural time difference . The duplex theory states that ITDs are used to localise low frequency sounds, in particular, while ILDs are used in the localisation of high frequency sound inputs. However, the frequency ranges for which the auditory system can use ITDs and ILDs significantly overlap, and most natural sounds will have both high and low frequency components, so that the auditory system will in most cases have to combine information from both ITDs and ILDs to judge the location of a sound source.  A consequence of this duplex system is that it is also possible to generate so-called \"cue trading\" or \"time\u2013intensity trading\" stimuli on headphones, where ITDs pointing to the left are offset by ILDs pointing to the right, so the sound is perceived as coming from the midline. A limitation of the duplex theory is that the theory does not completely explain directional hearing, as no explanation is given for the ability to distinguish between a sound source directly in front and behind. Also the theory only relates to localising sounds in the horizontal plane around the head. The theory also does not take into account the use of the pinna in localisation.(Gelfand, 2004)",
            "score": 211.5092270374298
        },
        {
            "docid": "34118956_11",
            "document": "Perception of infrasound . Tests of the ability to localize sounds also showed the significance of low frequency sound perception in elephants. Localization was tested by observing the successful orienting towards the left or the right source loudspeakers when they were positioned at different angles from the elephant\u2019s head. The elephant could localize sounds best at a frequency below 1\u00a0kHz, with perfect identification of the left or right speaker at angles of 20 degrees or more, and chance level discriminations below 2 degrees. Sound localization ability was measured to be best at 125\u00a0Hz and 250\u00a0Hz, intermediate at 500\u00a0Hz, 1\u00a0kHz, and 2\u00a0kHz, and very poor at frequencies at 4\u00a0kHz and above. A possible reason for this is that elephants are very good at using interaural phase differences which are effective for localizing low frequency sounds, but not as good at using interaural intensity differences which are better for higher frequency sounds. Because of the elephant head size and the large distance between their ears, interaural difference cues become confused when wavelengths are shorter, explaining why sound localization was very poor at frequencies above 4\u00a0kHz. It was observed that the elephant spread the pinna of its ears only during the sound localization tasks, however the precise effect of this behavior is unknown.",
            "score": 175.13985002040863
        },
        {
            "docid": "3154127_5",
            "document": "Virtual acoustic space . The perception of an externalized sound source is due to the frequency and direction-dependent filtering of the pinna which makes up the external ear structure. Unlike ILDs and ITDs, these spectral localization cues are generated monaurally. The same sound presented from different directions will produce at the eardrum a different pattern of peaks and notches across frequency. The pattern of these monaural spectral cues is different for different listeners. Spectral cues are vital for making elevation judgments and distinguishing if a sound arose from in front or behind the listener. They are also vital for creating the illusion of an externalized sound source. Since only ILDs are present in stereo recordings, the lack of spectral cues means that the sound is not perceived as being externalized. The easiest way of re-creating this illusion is to make a recording using two microphones placed inside a dummy human head. Playing back the recording via headphones will create the illusion of an externalized sound source.",
            "score": 214.7959200143814
        },
        {
            "docid": "3864383_7",
            "document": "Virtual surround . Perception of direction is greatly affected by the relative time that a sound arrives at each ear and any difference in the amplitude of a sound at each ear. It is possible to create a sound source having an output characteristic which is rapidly varying with direction and frequency of signal. These kinds of sources create sound fields which are rapidly variable around the listeners room. These are often referred to as diffuse sources, this is because their output resembles a diffuse sound field \u2014 a sound field where soundwaves are traveling in all directions with equal probability. In a diffuse field the sound at each of a listeners' ears is so completely different that it is impossible for the brain to work out where the sound has come from. A diffuse source located in front of the listener will be hard to localize and can be used to carry the surround signals.",
            "score": 211.63332843780518
        },
        {
            "docid": "41087200_14",
            "document": "Perceptual-based 3D sound localization . Head-related transfer functions contain all the descriptors of localization cues such as ITD and IID as well as monaural cues. Every HRTF uniquely represents the transfer of sound from a specific position in 3D space to the ears of a listener. The decoding process performed by the auditory system can be imitated using an artificial setup consisting of two microphones, two artificial ears and a HRTF database. To determine the position of an audio source in 3D space, the ear input signals are convolved with the inverses of all possible HRTF pairs, where the correct inverse maximizes cross-correlation between the convolved right and left signals. In the case of multiple simultaneous sound sources, the transmission of sound from source to ears can be considered a multiple-input and multiple-output. Here, the HRTFs the source signals were filtered with en route to the microphones can be found using methods such as convolutive blind source separation, which has the advantage of efficient implementation in real-time systems. Overall, these approaches using HRTFs can be well optimized to localize multiple moving sound sources. The average human has the remarkable ability to locate a sound source with better than 5 accuracy in both azimuth and elevation, in challenging environments.",
            "score": 191.4970120191574
        },
        {
            "docid": "4548229_6",
            "document": "Interaural time difference . Experiments conducted by Woodworth (1938) tested the duplex theory by using a solid sphere to model the shape of the head and measuring the ITDs as a function of azimuth for different frequencies. The model used had a distance between the 2 ears of approximately 22\u201323\u00a0cm. Initial measurements found that there was a maximum time delay of approximately 660 \u03bcs when the sound source was placed at directly 90\u00b0 azimuth to one ear. This time delay correlates to the wavelength of a sound input with a frequency of 1500\u00a0Hz. The results concluded that when a sound played had a frequency less than 1500\u00a0Hz the wavelength is greater than this maximum time delay between the ears. Therefore there is a phase difference between the sound waves entering the ears providing acoustic localisation cues. With a sound input with a frequency closer to 1500\u00a0Hz the wavelength of the sound wave is similar to the natural time delay. Therefore due to the size of the head and the distance between the ears there is a reduced phase difference so localisations errors start to be made. When a high frequency sound input is used with a frequency greater than 1500\u00a0Hz, the wavelength is shorter than the distance between the 2 ears, a head shadow is produced and ILD provide cues for the localisation of this sound.",
            "score": 189.68196868896484
        },
        {
            "docid": "25663206_13",
            "document": "Psychoacoustics . Sound localization is the process of determining the location of a sound source. The brain utilizes subtle differences in loudness, tone and timing between the two ears to allow us to localize sound sources. Localization can be described in terms of three-dimensional position: the azimuth or horizontal angle, the zenith or vertical angle, and the distance (for static sounds) or velocity (for moving sounds). Humans, as most four-legged animals, are adept at detecting direction in the horizontal, but less so in the vertical due to the ears being placed symmetrically. Some species of owls have their ears placed asymmetrically, and can detect sound in all three planes, an adaption to hunt small mammals in the dark.",
            "score": 184.6336100101471
        },
        {
            "docid": "161005_18",
            "document": "Head-related transfer function . The head-related transfer function is involved in resolving the Cone of Confusion, a series of points where ITD and ILD are identical for sound sources from many locations around the \"0\" part of the cone. When a sound is received by the ear it can either go straight down the ear into the ear canal or it can be reflected off the pinnae of the ear, into the ear canal a fraction of a second later. The sound will contain many frequencies, so therefore many copies of this signal will go down the ear all at different times depending on their frequency (according to reflection, diffraction, and their interaction with high and low frequencies and the size of the structures of the ear.) These copies overlap each other, and during this, certain signals are enhanced (where the phases of the signals match) while other copies are canceled out (where the phases of the signal do not match). Essentially, the brain is looking for frequency notches in the signal that correspond to particular known directions of sound.",
            "score": 185.88689494132996
        },
        {
            "docid": "352733_26",
            "document": "Glass harmonica . The somewhat disorienting quality of the ethereal sound is due in part to the way that humans perceive and locate ranges of sounds. Above 4 kHz people primarily use the \"loudness\" of the sound to differentiate between left and right ears and thus triangulate, or locate the source. Below 1\u00a0kHz, they use the \"phase differences\" of sound waves arriving at their left and right ears to identify location. The predominant pitch of the armonica is in the range of 1\u20134\u00a0kHz, which coincides with the sound range where the brain is \"not quite sure\", and thus listeners have difficulty locating it in space (where it comes from), and discerning the source of the sound (the materials and techniques used to produce it).",
            "score": 203.87381720542908
        },
        {
            "docid": "47338295_3",
            "document": "Sound localization in owls . Owls must be able to determine the necessary angle of descent, i.e. the elevation, in addition to azimuth (horizontal angle to the sound). This bi-coordinate sound localization is accomplished through two binaural cues: the interaural time difference (ITD) and the interaural level difference (ILD), also known as the interaural intensity difference (IID). The ability in owls is unusual; in ground-bound mammals such as mice, ITD and ILD are not utilized in the same manner. In these mammals, ITDs tend to be utilized for localization of lower frequency sounds, while ILDs tend to be used for higher frequency sounds.",
            "score": 155.0464789867401
        },
        {
            "docid": "191884_54",
            "document": "Headphones . Although modern headphones have been particularly widely sold and used for listening to stereo recordings since the release of the Walkman, there is subjective debate regarding the nature of their reproduction of stereo sound. Stereo recordings represent the position of horizontal depth cues (stereo separation) via volume and phase differences of the sound in question between the two channels. When the sounds from two speakers mix, they create the phase difference the brain uses to locate direction. Through most headphones, because the right and left channels do not combine in this manner, the illusion of the phantom center can be perceived as lost. Hard panned sounds are also heard only in one ear rather than from one side.",
            "score": 204.6540186405182
        },
        {
            "docid": "37654_17",
            "document": "Owl . Owls exhibit specialized hearing functions and ear shapes that also aid in hunting. They are noted for asymmetrical ear placements on the skull in some genera. Owls can have either internal or external ears, both of which are asymmetrical. Asymmetry has not been reported to extend to the middle or internal ear of the owl. Asymmetrical ear placement on the skull allows the owl to pinpoint the location of its prey. This is especially true for strictly nocturnal species such as the barn owls \"Tyto\" or Tengmalm's owl. With ears set at different places on its skull, an owl is able to determine the direction from which the sound is coming by the minute difference in time that it takes for the sound waves to penetrate the left and right ears. The owl turns its head until the sound reaches both ears at the same time, at which point it is directly facing the source of the sound. This time difference between ears is a matter of about 0.00003 seconds, or 30 millionths of a second. Behind the ear openings are modified, dense feathers, densely packed to form a facial ruff, which creates an anterior-facing, concave wall that cups the sound into the ear structure. This facial ruff is poorly defined in some species, and prominent, nearly encircling the face, in other species. The facial disk also acts to direct sound into the ears, and a downward-facing, sharply triangular beak minimizes sound reflection away from the face. The shape of the facial disk is adjustable at will to focus sounds more effectively.",
            "score": 203.2390365600586
        },
        {
            "docid": "34118956_15",
            "document": "Perception of infrasound . In order to use infrasound for navigation, it is necessary to be able to localize the source of the sounds. The known mechanisms for sound localizations make use of the time difference cues at the two ears. However, infrasound has such long wavelengths that these mechanisms would not be effective for an animal the size of a pigeon. An alternative method that has been hypothesized is through the use of the Doppler shift. A Doppler shift occurs when there is relative motion between a sound source and a perceiver and slightly shifts the perceived frequency of the sound. When a flying bird is changing direction, the amplitude of the Doppler shift between it and an infrasonic source would change, enabling the bird to locate the source. This kind of mechanism would require the ability to detect very small changes in frequency. A pigeon typically flies at 20\u00a0km/hr, so a turn could cause up to a 12% modulation of an infrasonic stimulus. According to response measurements, pigeons are able to distinguish frequency changes of 1-7% in the infrasonic range, showing that the use of Doppler shifts for infrasound localization may be within the pigeon\u2019s perceptive capabilities.",
            "score": 180.36716413497925
        },
        {
            "docid": "1021754_17",
            "document": "Sound localization . Depending on where the source is located, our head acts as a barrier to change the timbre, intensity, and spectral qualities of the sound, helping the brain orient where the sound emanated from. These minute differences between the two ears are known as interaural cues.",
            "score": 192.4115812778473
        }
    ],
    "r": [
        {
            "docid": "5051081_4",
            "document": "Eric Knudsen . In 1978, Knudsen and Konishi presented the discovery of an auditory map of space in the midbrain of the barn owl. This discovery was groundbreaking because it unearthed the first non-somatotopic space map in the brain. The map was found in the owl\u2019s midbrain, in the lateral and anterior mesencephalicus lateralis dorsalis (MLD), a structure now referred to as the inferior colliculus. Unlike most sound-localization maps, this map was found to be two-dimensional, with units arranged spatially to represent both the vertical and horizontal location of sound. Knudsen and Konishi discovered that units in this structure respond preferentially to sounds originating in a particular region in space. In the 1978 paper, elevation and azimuth (location in the horizontal plane) were shown to be the two coordinates of the map. Using a speaker set on a rotatable hemispherical track, Knudsen and Konishi presented owls with auditory stimulus from various locations in space and recorded the resulting neuronal activity. They found that neurons in this part of the MLD were organized according to the location of their receptive field, with azimuth varying along the horizontal plane of the space map and elevation varying vertically.  Knudsen followed this discovery with research into specific sound localization mechanisms. Two main auditory cues used by the barn owl to localize sound are interaural time difference (ITD) and interaural intensity difference (IID). The owl\u2019s ears are asymmetric, with the right ear\u2019s opening being directed higher than that of the left. This asymmetry allows the barn owl to determine the elevation of a sound by comparing sound levels between its two ears. Interaural time differences provide the owl with information regarding a sound\u2019s azimuth; sound will reach the ear closer to the sound source before reaching the farther ear, and this time difference can be detected and interpreted as an azimuthal direction. At low frequencies, the wavelength of a sound is wider than the owl's facial ruff, and the ruff does not affect detection of azimuth. At high frequencies, the ruff plays a role in reflecting sound for heightened sensitivity to vertical elevation. Therefore, with wide-band noise, containing both high and low frequencies, the owl could use interaural spectrum difference to obtain information about both azimuth and elevation. In 1979, Knudsen and Konishi showed that the barn owl uses interaural spectrum information in sound localization. They presented owls with both wide-bandwidth noise and pure tones. The birds were able to successfully locate pure tones (since they could still gather information from IID and ITD), but their error rate was much lower when localizing wide-bandwidth noise. This indicates that the birds utilize interaural spectrum differences to improve their accuracy.",
            "score": 266.0374755859375
        },
        {
            "docid": "39200136_12",
            "document": "Hans Wallach . In a series of papers Wallach explored the ability of humans to locate sounds in the median plane \u2013 that is, to determine whether a sound comes from a source at the same elevation as the ears or from a source that is higher or lower, or even in back of the head. Binaural sound cues, including the phasing or time of the sound\u2019s arrival at each ear and the sound\u2019s relative intensity at the two ears (known respectively as ITD and ILD) enable a listener to determine a sound\u2019s lateral location (whether it is on the left, right, or straight ahead). But two sounds at different elevations can present identical ITD and ILD information to the ears, and so binaural cues to a stationary ear do not suffice to identify a sound\u2019s location in the median plane. Monaural cues that depend on the shape of the head and the structure of the external ear help with vertical localization, but binaural cues also play a part if the head is not stationary.",
            "score": 242.31893920898438
        },
        {
            "docid": "5442380_17",
            "document": "Sensory cue . Unless a sound is directly in front of or behind the individual, the sound stimuli will have a slightly different distance to travel to reach each ear. This difference in distance causes a slight delay in the time the signal is perceived by each ear. The magnitude of the interaural time difference is greater the more the signal comes from the side of the head. Thus, this time delay allows humans to accurately predict the location of incoming sound cues. Interaural level difference is caused by the difference in sound pressure level reaching the two ears. This is because the head blocks the sound waves for the further ear, causing less intense sound to reach it. This level difference between the two ears allows humans to accurately predict azimuth of an auditory signal. This effect only occurs at sounds that are high frequency.",
            "score": 239.06031799316406
        },
        {
            "docid": "41087200_8",
            "document": "Perceptual-based 3D sound localization . Interaural level differences (ILD) represents the difference in sound pressure level reaching the two ears. They provide salient cues for localizing high-frequency sounds in space, and populations of neurons that are sensitive to ILD are found at almost every synaptic level from brain stem to cortex. These cells are predominantly excited by stimulation of one ear and predominantly inhibited by stimulation of the other ear, such that the magnitude of their response is determined in large part by the intensities at the 2 ears. This gives rise to the concept of resonant damping. Interaural level difference (ILD) is best for high frequency sounds because low frequency sounds are not attenuated much by the head. ILD (also known as Interaural Intensity Difference) arises when the sound source is not centred, the listener's head partially shadows the ear opposite to the source, diminishing the intensity of the sound in that ear (particularly at higher frequencies). The pinnae filters the sound in a way that is directionally dependent. This is particularly useful in determining if a sound comes from above, below, in front, or behind.",
            "score": 237.29586791992188
        },
        {
            "docid": "1021754_32",
            "document": "Sound localization . When the head is stationary, the binaural cues for lateral sound localization (interaural time difference and interaural level difference) do not give information about the location of a sound in the median plane. Identical ITDs and ILDs can be produced by sounds at eye level or at any elevation, as long as the lateral direction is constant. However, if the head is rotated, the ITD and ILD change dynamically, and those changes are different for sounds at different elevations. For example, if an eye-level sound source is straight ahead and the head turns to the left, the sound becomes louder (and arrives sooner) at the right ear than at the left. But if the sound source is directly overhead, there will be no change in the ITD and ILD as the head turns. Intermediate elevations will produce intermediate degrees of change, and if the presentation of binaural cues to the two ears during head movement is reversed, the sound will be heard behind the listener. Hans Wallach artificially altered a sound\u2019s binaural cues during movements of the head. Although the sound was objectively placed at eye level, the dynamic changes to ITD and ILD as the head rotated were those that would be produced if the sound source had been elevated. In this situation, the sound was heard at the synthesized elevation. The fact that the sound sources objectively remained at eye level prevented monaural cues from specifying the elevation, showing that it was the dynamic change in the binaural cues during head movement that allowed the sound to be correctly localized in the vertical dimension. The head movements need not be actively produced; accurate vertical localization occurred in a similar setup when the head rotation was produced passively, by seating the blindfolded subject in a rotating chair. As long as the dynamic changes in binaural cues accompanied a perceived head rotation, the synthesized elevation was perceived.",
            "score": 233.30030822753906
        },
        {
            "docid": "4548229_2",
            "document": "Interaural time difference . The interaural time difference (or ITD) when concerning humans or animals, is the difference in arrival time of a sound between two ears. It is important in the localization of sounds, as it provides a cue to the direction or angle of the sound source from the head. If a signal arrives at the head from one side, the signal has further to travel to reach the far ear than the near ear. This pathlength difference results in a time difference between the sound's arrivals at the ears, which is detected and aids the process of identifying the direction of sound source.",
            "score": 231.5126953125
        },
        {
            "docid": "14532984_7",
            "document": "Coincidence detection in neurobiology . Coincidence detection has been shown to be a major factor in sound localization along the azimuth plane in several organisms. In 1948, Lloyd A. Jeffress proposed that some organisms may have a collection of neurons that receive auditory input from each ear. The neural pathways to these neurons are called delay lines. Jeffress claimed that the neurons that the delay lines link act as coincidence detectors by firing maximally when receiving simultaneous inputs from both ears. When a sound is heard, sound waves may reach the ears at different times. This is referred to as the interaural time difference (ITD). Due to differing lengths and a finite conduction speed within the axons of the delay lines, different coincidence detector neurons will fire when sound comes from different positions along the azimuth. Jeffress' model proposes that two signals even from an asynchronous arrival of sound in the cochlea of each ear will converge synchronously on a coincidence detector in the auditory cortex based on the magnitude of the ITD (Fig. 2). Therefore, the ITD should correspond to an anatomical map that can be found within the brain. Masakazu Konishi's study on barn owls shows that this is true. Sensory information from the hair cells of the ears travels to the ipsilateral nucleus magnocellularis. From here, the signals project ipsilaterally and contralaterally to two nucleus laminari. Each nucleus laminaris contains coincidence detectors that receive auditory input from the left and the right ear. Since the ipsilateral axons enter the nucleus laminaris dorsally while the contralateral axons enter ventrally, sounds from various positions along the azimuth correspond directly to stimulation of different depths of the nucleus laminaris. From this information, a neural map of auditory space was formed. The function of the nucleus laminaris parallels that of the medial superior olive in mammals.",
            "score": 230.9879913330078
        },
        {
            "docid": "161005_5",
            "document": "Head-related transfer function . Humans estimate the location of a source by taking cues derived from one ear (\"monaural cues\"), and by comparing cues received at both ears (\"difference cues\" or \"binaural cues\"). Among the difference cues are time differences of arrival and intensity differences. The monaural cues come from the interaction between the sound source and the human anatomy, in which the original source sound is modified before it enters the ear canal for processing by the auditory system. These modifications encode the source location, and may be captured via an impulse response which relates the source location and the ear location. This impulse response is termed the \"head-related impulse response\" (HRIR). Convolution of an arbitrary source sound with the HRIR converts the sound to that which would have been heard by the listener if it had been played at the source location, with the listener's ear at the receiver location. HRIRs have been used to produce virtual surround sound.",
            "score": 228.11521911621094
        },
        {
            "docid": "3154127_4",
            "document": "Virtual acoustic space . When one listens to sounds over headphones (in what is known as the \"closed field\") the sound source appears to arise from center of the head. On the other hand, under normal, so-called free-field, listening conditions sounds are perceived as being externalized. The direction of a sound in space (see sound localization) is determined by the brain when it analyses the interaction of incoming sound with head and external ears. A sound arising to one side reaches the near ear before the far ear (creating an interaural time difference, ITD), and will also be louder at the near ear (creating an interaural level difference, ILD \u2013 also known as interaural intensity difference, IID). These binaural cues allow sounds to be lateralized. Although conventional stereo headphone signals make used of ILDs (not ITDs) the sound is not perceived as being externalized.",
            "score": 222.6994171142578
        },
        {
            "docid": "32116125_4",
            "document": "Amblyaudia . Amblyaudia is a deficit in binaural integration of environmental information entering the auditory system. It is a disorder related to brain organization and function rather than what is typically considered a \u201chearing loss\u201d (damage to the cochlea). It may be genetic or developmentally acquired or both. When animals are temporarily deprived of hearing from an early age, profound changes occur in the brain. Specifically, cell sizes in brainstem nuclei are reduced, the configuration of brainstem dendrites are altered and neurons respond in different ways to sounds presented to both the deprived and non-deprived ears (in cases of asymmetric deprivation). This last point is particularly important for listening tasks that require inputs from two ears to perform well. There are multiple auditory functions that rely on the computation of well calibrated inputs from the two ears. Chief among these is the ability to localize sound sources and separate what we want to hear from a background of noise. In the brainstem, the auditory system compares the timing and levels of sounds between the two ears to encode the location of sound sources (sounds that originate from our right as opposed to left side are louder and arrive earlier in our right ear). This ability to separate sound sources not only helps us locate the trajectories of moving objects, but also to separate different sound sources in noisy environments.",
            "score": 215.05755615234375
        },
        {
            "docid": "1021754_24",
            "document": "Sound localization . For frequencies above 1600\u00a0Hz the dimensions of the head are greater than the length of the sound waves. An unambiguous determination of the input direction based on interaural phase alone is not possible at these frequencies. However, the interaural level differences become larger, and these level differences are evaluated by the auditory system. Also, group delays between the ears can be evaluated, and is more pronounced at higher frequencies; that is, if there is a sound onset, the delay of this onset between the ears can be used to determine the input direction of the corresponding sound source. This mechanism becomes especially important in reverberant environments. After a sound onset there is a short time frame where the direct sound reaches the ears, but not yet the reflected sound. The auditory system uses this short time frame for evaluating the sound source direction, and keeps this detected direction as long as reflections and reverberation prevent an unambiguous direction estimation. The mechanisms described above cannot be used to differentiate between a sound source ahead of the hearer or behind the hearer; therefore additional cues have to be evaluated.",
            "score": 214.8085479736328
        },
        {
            "docid": "3154127_5",
            "document": "Virtual acoustic space . The perception of an externalized sound source is due to the frequency and direction-dependent filtering of the pinna which makes up the external ear structure. Unlike ILDs and ITDs, these spectral localization cues are generated monaurally. The same sound presented from different directions will produce at the eardrum a different pattern of peaks and notches across frequency. The pattern of these monaural spectral cues is different for different listeners. Spectral cues are vital for making elevation judgments and distinguishing if a sound arose from in front or behind the listener. They are also vital for creating the illusion of an externalized sound source. Since only ILDs are present in stereo recordings, the lack of spectral cues means that the sound is not perceived as being externalized. The easiest way of re-creating this illusion is to make a recording using two microphones placed inside a dummy human head. Playing back the recording via headphones will create the illusion of an externalized sound source.",
            "score": 214.79591369628906
        },
        {
            "docid": "3246329_10",
            "document": "Hearing range . Cats have excellent hearing and can detect an extremely broad range of frequencies. They can hear higher-pitched sounds than humans or most dogs, detecting frequencies from 55\u00a0Hz up to 79\u00a0kHz. Cats do not use this ability to hear ultrasound for communication but it is probably important in hunting, since many species of rodents make ultrasonic calls. Cat hearing is also extremely sensitive and is among the best of any mammal, being most acute in the range of 500\u00a0Hz to 32\u00a0kHz. This sensitivity is further enhanced by the cat's large movable outer ears (their \"pinnae\"), which both amplify sounds and help a cat sense the direction from which a noise is coming. The hearing ability of a dog is dependent on breed and age, though the range of hearing is usually around 67\u00a0Hz to 45\u00a0kHz. As with humans, some dog breeds' hearing ranges narrow with age, such as the German shepherd and miniature poodle. When dogs hear a sound, they will move their ears towards it in order to maximise reception. In order to achieve this, the ears of a dog are controlled by at least 18 muscles, which allow the ears to tilt and rotate. The ear's shape also allows the sound to be heard more accurately. Many breeds often have upright and curved ears, which direct and amplify sounds.",
            "score": 213.53976440429688
        },
        {
            "docid": "47338295_5",
            "document": "Sound localization in owls . In mammals there is a level difference in sounds at the two ears caused by the sound-shadowing effect of the head. But in many species of owls, level differences arise primarily for sounds that are shifted above or below the elevation of the horizontal plane. This is due to the asymmetry in placement of the ear openings in the owl's head, such that sounds from below the owl reach the left ear first and sounds from above reach the right ear first. IID is a measure of the difference in the level of the sound as it reaches each ear. In many owls, IIDs for high-frequency sounds (higher than 4 or 5\u00a0kHz) are the principal cues for locating sound elevation.",
            "score": 212.3841094970703
        },
        {
            "docid": "3864383_7",
            "document": "Virtual surround . Perception of direction is greatly affected by the relative time that a sound arrives at each ear and any difference in the amplitude of a sound at each ear. It is possible to create a sound source having an output characteristic which is rapidly varying with direction and frequency of signal. These kinds of sources create sound fields which are rapidly variable around the listeners room. These are often referred to as diffuse sources, this is because their output resembles a diffuse sound field \u2014 a sound field where soundwaves are traveling in all directions with equal probability. In a diffuse field the sound at each of a listeners' ears is so completely different that it is impossible for the brain to work out where the sound has come from. A diffuse source located in front of the listener will be hard to localize and can be used to carry the surround signals.",
            "score": 211.63333129882812
        },
        {
            "docid": "5442380_26",
            "document": "Sensory cue . There are strong interactions between visual and auditory stimuli. Since both auditory and visual cues provide an accurate source of information about the location of an object, most times there will be minimal discrepancy between the two. However, it is possible to have a disparity in the information provided by the two sets of cues. Visual capture, also known as the ventriloquism effect, occurs when an individual\u2019s visual system locates the source of an auditory stimulus at a different position than where the auditory system locates it. When this occurs, the visual cues will override the auditory ones. The individual will perceive the sound as coming from the location where the object is seen. Audition can also affect visual perception. Research has demonstrated this effect by showing two objects on a screen, one moving diagonally from top-right to bottom-left and the other from top-left to bottom-right, intersecting in the middle. The paths of these identical objects could have been interpreted as crossing over each other, or as bouncing off each other. Without any auditory cue, a vast majority of subjects saw the objects crossing paths and continuing in their original trajectory. But with the addition of a small \u201cclick\u201d sound, a majority of subjects perceived the objects as bouncing off each other. In this case, auditory cues help interpret visual cues.",
            "score": 211.51437377929688
        },
        {
            "docid": "4548229_5",
            "document": "Interaural time difference . The duplex theory states that ITDs are used to localise low frequency sounds, in particular, while ILDs are used in the localisation of high frequency sound inputs. However, the frequency ranges for which the auditory system can use ITDs and ILDs significantly overlap, and most natural sounds will have both high and low frequency components, so that the auditory system will in most cases have to combine information from both ITDs and ILDs to judge the location of a sound source.  A consequence of this duplex system is that it is also possible to generate so-called \"cue trading\" or \"time\u2013intensity trading\" stimuli on headphones, where ITDs pointing to the left are offset by ILDs pointing to the right, so the sound is perceived as coming from the midline. A limitation of the duplex theory is that the theory does not completely explain directional hearing, as no explanation is given for the ability to distinguish between a sound source directly in front and behind. Also the theory only relates to localising sounds in the horizontal plane around the head. The theory also does not take into account the use of the pinna in localisation.(Gelfand, 2004)",
            "score": 211.50921630859375
        },
        {
            "docid": "32105732_4",
            "document": "Spatial hearing loss . Sound streams arriving from the left or right (the horizontal plane) are localised primarily by the small time differences of the same sound arriving at the two ears. A sound straight in front of the head is heard at the same time by both ears. A sound to the side of the head is heard approximately 0.0005 seconds later by the ear furthest away. A sound halfway to one side is heard approximately 0.0003 seconds later. This is the interaural time difference (ITD) cue and is measured by signal processing in the two central auditory pathways that begin after the cochlea and pass through the brainstem and mid-brain. Some of those with spatial hearing loss are unable to process ITD (low frequency) cues.",
            "score": 211.44085693359375
        },
        {
            "docid": "14339999_6",
            "document": "Virtual pitch . Terhardt rejected the idea of periodicity pitch, because it was not consistent with empirical data on pitch perception, e.g. measurements of the gradual shift of the virtual pitch of a complex tone with a missing fundamental when the partials were gradually shifted. Terhardt instead broke pitch perception into two steps: auditory frequency analysis in the inner ear, and harmonic pitch pattern recognition in the brain. The inner ear effectively performs a running frequency analysis of incoming sounds - otherwise we would not be able to hear out spectral pitches within a complex tone. Physiologically, each spectral pitch depends on both temporal and spectral aspects (i.e. periodicity of the waveform and position of excitation on the basilar membrane), but in Terhardt's approach the spectral pitch itself is a purely experiential parameter, not a physical parameter: it is the outcome of a psychoacoustical experiment in which the conscious listener plays an active role. Psychoacoustic measurements and models can predict which partials are \"perceptually relevant\" in a given complex tone; they are perceptually relevant if you can hear a difference in the whole sound if the frequency or amplitude of a partial is changed). The ear has evolved to separate spectral frequencies, because due to reflection and superposition in everyday environments spectral frequencies are more reliably carriers of environmental information than spectral amplitudies, which in turn are more reliable carriers of environmentally relevant information than phase relationships between partials (when perceived monoaurally). On this basis, Terhardt proposed that spectral pitches - which are what the listener experiences when hearing out partials (as opposed to the physical partials themselves) - are the only information available to the brain for the purpose of extracting virtual pitches. The \"pitch extraction\" process then involves the recognition of incomplete harmonic patterns and happens in neural networks.",
            "score": 209.94580078125
        },
        {
            "docid": "8436042_6",
            "document": "MPEG Surround . MPEG Surround coding uses our capacity to perceive sound in the 3D and captures that perception in a compact set of parameters. Spatial perception is primarily attributed to three parameters, or cues, describing how humans localize sound in the horizontal plane: Interaural level difference (ILD), Interaural time difference (ITD) and Interaural coherence (IC). This three concepts are illustrated in next image. Direct, or first-arrival, waveforms from the source hit the left ear at time, while direct sound received by the right ear is diffracted around the head, with time delay and level attenuation, associated. These two effects result in ITD and ILD are associated with the main source. At last, in a reverberant environment, reflected sound from the source, or sound from diffuse source, or uncorrelated sound can hit both ears, all of them are related with IC.",
            "score": 209.66082763671875
        },
        {
            "docid": "47338295_4",
            "document": "Sound localization in owls . ITD occurs whenever the distance from the source of sound to the two ears is different, resulting in differences in the arrival times of the sound at the two ears. When the sound source is directly in front of the owl, there is no ITD, i.e. the ITD is zero. In sound localization, ITDs are used as cues for location in the azimuth. ITD changes systematically with azimuth. Sounds to the right arrive first at the right ear; sounds to the left arrive first at the left ear.",
            "score": 208.59051513671875
        },
        {
            "docid": "5442380_23",
            "document": "Sensory cue . Pitch refers to the frequency of the sound wave reaching us. Although a single object could produce a variety of pitches overtime, it more likely that it would produce sounds in a similar range. Erratic changes in pitches are more likely to be perceived as originating from different sources.",
            "score": 205.34725952148438
        },
        {
            "docid": "191884_54",
            "document": "Headphones . Although modern headphones have been particularly widely sold and used for listening to stereo recordings since the release of the Walkman, there is subjective debate regarding the nature of their reproduction of stereo sound. Stereo recordings represent the position of horizontal depth cues (stereo separation) via volume and phase differences of the sound in question between the two channels. When the sounds from two speakers mix, they create the phase difference the brain uses to locate direction. Through most headphones, because the right and left channels do not combine in this manner, the illusion of the phantom center can be perceived as lost. Hard panned sounds are also heard only in one ear rather than from one side.",
            "score": 204.6540069580078
        },
        {
            "docid": "31352483_6",
            "document": "Soundscape ecology . The function and importance of sound in the environment may not be fully appreciated unless one adopts an organismal perspective on sound perception, and, in this way, soundscape ecology is also informed by sensory ecology. Sensory ecology focuses on understanding the sensory systems of organisms and the biological function of information obtained from these systems. In many cases, humans must acknowledge that sensory modalities and information used by other organisms may not be obvious from an anthropocentric viewpoint. This perspective has already highlighted many instances where organisms rely heavily on sound cues generated within their natural environments to perform important biological functions. For example, a broad range of crustaceans are known to respond to biophony generated around coral reefs. Species that must settle on reefs to complete their developmental cycle are attracted to reef noise while pelagic and nocturnal crustaceans are repelled by the same acoustic signal, presumably as a mechanism to avoid predation (predator densities are high in reef habitats). Similarly, juvenile fish may use biophony as a navigational cue to locate their natal reefs. Other species\u2019 movement patterns are influenced by geophony, as in the case of the reed frog which is known to disperse away from the sound of fire. In addition, a variety of bird and mammal species use auditory cues, such as movement noise, in order to locate prey. Disturbances created by periods of environmental noise may also be exploited by some animals while foraging. For example, insects that prey on spiders concentrate foraging activities during episodes of environmental noise to avoid detection by their prey. These examples demonstrate that many organisms are highly capable of extracting information from soundscapes.",
            "score": 204.22021484375
        },
        {
            "docid": "2263473_17",
            "document": "Volley theory . A fundamental frequency is the lowest frequency of a harmonic. In some cases, sound can have all the frequencies of a harmonic but be missing the fundamental frequency, this is known as missing fundamental. When listening to a sound with a missing fundamental, the human brain still receives information for all frequencies, including the fundamental frequency which does not exist in the sound. This implies that sound is encoded by neurons firing at all frequencies of a harmonic, therefore, the neurons must be locked in some way to result in the hearing of one sound. Congenital deafness or sensorineural hearing loss is an often used model for the study of the inner ear regarding pitch perception and theories of hearing in general. Frequency analysis of these individuals\u2019 hearing has given insight on common deviations from normal tuning curves, excitation patterns, and frequency discrimination ranges. By applying pure or complex tones, information on pitch perception can be obtained. In 1983, it was shown that subjects with low frequency sensorineural hearing loss demonstrated abnormal psychophysical tuning curves. Changes in the spatial responses in these subjects showed similar pitch judgment abilities when compared to subjects with normal spatial responses. This was especially true regarding low frequency stimuli. These results suggest that the place theory of hearing does not explain pitch perception at low frequencies, but that the temporal (frequency) theory is more likely. This conclusion is due to the finding that when deprived of basilar membrane place information, these patients still demonstrated normal pitch perception. Computer models for pitch perception and loudness perception are often used during hearing studies on acoustically impaired subjects. The combination of this modeling and knowledge of natural hearing allows for better development of hearing aids.",
            "score": 203.9224395751953
        },
        {
            "docid": "352733_26",
            "document": "Glass harmonica . The somewhat disorienting quality of the ethereal sound is due in part to the way that humans perceive and locate ranges of sounds. Above 4 kHz people primarily use the \"loudness\" of the sound to differentiate between left and right ears and thus triangulate, or locate the source. Below 1\u00a0kHz, they use the \"phase differences\" of sound waves arriving at their left and right ears to identify location. The predominant pitch of the armonica is in the range of 1\u20134\u00a0kHz, which coincides with the sound range where the brain is \"not quite sure\", and thus listeners have difficulty locating it in space (where it comes from), and discerning the source of the sound (the materials and techniques used to produce it).",
            "score": 203.87380981445312
        },
        {
            "docid": "37654_17",
            "document": "Owl . Owls exhibit specialized hearing functions and ear shapes that also aid in hunting. They are noted for asymmetrical ear placements on the skull in some genera. Owls can have either internal or external ears, both of which are asymmetrical. Asymmetry has not been reported to extend to the middle or internal ear of the owl. Asymmetrical ear placement on the skull allows the owl to pinpoint the location of its prey. This is especially true for strictly nocturnal species such as the barn owls \"Tyto\" or Tengmalm's owl. With ears set at different places on its skull, an owl is able to determine the direction from which the sound is coming by the minute difference in time that it takes for the sound waves to penetrate the left and right ears. The owl turns its head until the sound reaches both ears at the same time, at which point it is directly facing the source of the sound. This time difference between ears is a matter of about 0.00003 seconds, or 30 millionths of a second. Behind the ear openings are modified, dense feathers, densely packed to form a facial ruff, which creates an anterior-facing, concave wall that cups the sound into the ear structure. This facial ruff is poorly defined in some species, and prominent, nearly encircling the face, in other species. The facial disk also acts to direct sound into the ears, and a downward-facing, sharply triangular beak minimizes sound reflection away from the face. The shape of the facial disk is adjustable at will to focus sounds more effectively.",
            "score": 203.23902893066406
        },
        {
            "docid": "32105732_5",
            "document": "Spatial hearing loss . Sound streams arriving from below the head, above the head, and over behind the head (the vertical plane) are localised again by signal processing in the central auditory pathways. The cues this time however are the notches/peaks that are added to the sound arriving at the ears by the complex shapes of the pinna. Different notches/peaks are added to sounds coming from below compared to sounds coming from above, and compared to sounds coming from behind. The most significant notches are added to sounds in the 4\u00a0kHz to 10\u00a0kHz range. Some of those with spatial hearing loss are unable to process pinna related (high frequency) cues.",
            "score": 202.9301300048828
        },
        {
            "docid": "5442380_16",
            "document": "Sensory cue . Humans use several cues to determine the location of a given stimuli, mainly by using the timing difference between ears. These cues allow individuals to identify both the elevation, the height of the stimuli relative to the individual, and azimuth, the angle of the sound relative to the direction the individual is facing.",
            "score": 201.94573974609375
        },
        {
            "docid": "4301708_10",
            "document": "Cochlear nucleus . The cochlear nuclear complex is the first integrative, or processing, stage in the auditory system. Information is brought to the nuclei from the ipsilateral cochlea via the cochlear nerve. Several tasks are performed in the cochlear nuclei. By distributing acoustic input to multiple types of principal cells, the auditory pathway is subdivided into parallel ascending pathways, which can simultaneously extract different types of information. The cells of the ventral cochlear nucleus extract information that is carried by the auditory nerve in the timing of firing and in the pattern of activation of the population of auditory nerve fibers. The cells of the dorsal cochlear nucleus perform a non-linear spectral analysis and place that spectral analysis into the context of the location of the head, ears and shoulders and that separate expected, self-generated spectral cues from more interesting, unexpected spectral cues using input from the auditory cortex, pontine nuclei, trigeminal ganglion and nucleus, dorsal column nuclei and the second dorsal root ganglion. It is likely that these neurons help mammals to use spectral cues for orienting toward those sounds. The information is used by higher brainstem regions to achieve further computational objectives (such as sound source location or improvement in signal to noise ratio). The inputs from these other areas of the brain probably play a role in sound localization.",
            "score": 200.84512329101562
        },
        {
            "docid": "1021754_31",
            "document": "Sound localization . The human outer ear, i.e. the structures of the pinna and the external ear canal, form direction-selective filters. Depending on the sound input direction in the median plane, different filter resonances become active. These resonances implant direction-specific patterns into the frequency responses of the ears, which can be evaluated by the auditory system (directional bands) for vertical sound localization. Together with other direction-selective reflections at the head, shoulders and torso, they form the outer ear transfer functions. These patterns in the ear's frequency responses are highly individual, depending on the shape and size of the outer ear. If sound is presented through headphones, and has been recorded via another head with different-shaped outer ear surfaces, the directional patterns differ from the listener's own, and problems will appear when trying to evaluate directions in the median plane with these foreign ears. As a consequence, front\u2013back permutations or inside-the-head-localization can appear when listening to dummy head recordings, or otherwise referred to as binaural recordings. It has been shown that human subjects can monaurally localize high frequency sound but not low frequency sound. Binaural localization, however, was possible with lower frequencies. This is likely due to the pinna being small enough to only interact with sound waves of high frequency. It seems that people can only accurately localize the elevation of sounds that are complex and include frequencies above 7,000\u00a0Hz, and a pinna must be present.",
            "score": 200.8196563720703
        },
        {
            "docid": "5442380_25",
            "document": "Sensory cue . When one sound is presented for a long interval before the introduction of a second one originating from a different location, individuals will hear them as two distinct sounds, each originating from the correct location. However, when the delay between the onset of the first and second sound is shortened, listeners are unable to distinguish between the two sounds. Instead, they perceive them as both coming from the location of the lead sound. This effect counteracts the small disparity between the perception of sound caused by the difference in distance between each ear and the source of the auditory stimuli.",
            "score": 200.55648803710938
        }
    ]
}