{
    "q": [
        {
            "docid": "2485942_5",
            "document": "Audiogram . Hearing thresholds of humans and other mammals can be found by using behavioural hearing tests or physiological tests. An audiogram can be obtained using a behavioural hearing test called Audiometry. For humans the test involves different tones being presented at a specific frequency (pitch) and intensity (loudness). When the person hears the sound they raise their hand or press a button so that the tester knows that they have heard it. The lowest intensity sound they can hear is recorded. The test varies for children, their response to the sound can be a head turn or using a toy. The child learns what they can do when they hear the sound, for example they are taught that when they heard the sound they can put a toy man in a boat. This is referred to as conditioned play audiometry. Visual reinforcement audiometry is also used with children. When the child hears the sound, they look in the direction the sound came from and are reinforced with a light and/or animated toy. A similar technique can be used when testing some animals but instead of a toy, food can be used as a reward for responding to the sound.  Physiological tests do not need the patient to respond (Katz 2002). For example when performing the brainstem auditory evoked potentials the patient\u2019s brainstem responses are being measured when a sound is played into their ear, or otoacoustic emissions (OAEs) which are generated by a healthy inner ear either spontaneously or evoked by an outside stimulus.  In the US, the NIOSH recommends that people who are regularly exposed to hazardous noise have their hearing tested once a year, or every three years otherwise.",
            "score": 81.47821724414825
        },
        {
            "docid": "31148473_15",
            "document": "Transsaccadic memory . Irwin's early experiments tested participants ability to identify random dot patterns as the same, or as different across saccades. The control condition for this experiment presented dot patterns in the same spatial location without saccades, meaning participants had a single fixation point. A no-overlap control condition presented dot patterns in different spatial locations, while participants maintained fixation on one point. This tested the ability of visual short-term memory in identifying two individual patterns. The experimental condition showed dot patterns in the same spatial location but were viewed at separate fixations, forcing the use of transsaccadic memory. For the experimental condition, participants underwent a calibration phase, where they were shown five points in separate location to fixate on individually, for less than two seconds. The next phase presented a single fixation point for less than two seconds, which was followed by a random dot pattern presented in a different location, acting as a saccade target. The dot pattern disappeared when the saccade was initiated. Another dot pattern then appeared in the same location. Participants had to identify whether the two patterns were the same or different.  Results of the experiment showed performance of the experimental condition to be slightly less accurate than the no-overlap condition. Irwin attributed this decrease in accuracy to the extra neural processes involved in performing an eye movement, combined with processing the dot pattern. He concluded from this that transsaccadic memory does exist, but that it is very similar if not identical to short term visual memory and less similar to sensory memory.",
            "score": 119.84887063503265
        },
        {
            "docid": "10269587_2",
            "document": "Echoic memory . Echoic memory is the sensory memory register specific to auditory information (sounds). The sensory memory for sounds that people have just perceived is the form of echoic memory. Unlike visual memory, in which our eyes can scan the stimuli over and over, the auditory stimuli cannot be scanned over and over, although from a classical physics definition they both can and can not be so equally. Imaging input will always be at least slightly different due to other stray bits of light, just as a recorded sound will almost never have an identical total sound profile when played at different times. Time itself affects the sound every bit as much as photons such as in things seen. etc. even when the image being looked at is the same. So true with sound. Playing a digital music selection for example, through noise controlling headphones over and over again, is every bit as accurate with relation to sound as reading the same few words over and over again. Overall, echoic memories are stored for slightly longer periods of time than iconic memories (visual memories). Auditory stimuli are received by the ear one at a time before they can be processed and understood. For instance, hearing the radio is very different from reading a magazine. A person can only hear the radio once at a given time, while the magazine can be read over and over again. It can be said that the echoic memory is like a \"holding tank\" concept, because a sound is unprocessed (or held back) until the following sound is heard, and only then can it be made meaningful. This particular sensory store is capable of storing large amounts of auditory information that is only retained for a short period of time (3\u20134 seconds). This echoic sound resonates in the mind and is replayed for this brief amount of time shortly after the presentation of auditory stimuli. Echoic memory encrypts only moderately primitive aspects of the stimuli, for example pitch, which specifies localization to the non-association brain regions.",
            "score": 105.17714881896973
        },
        {
            "docid": "3246329_5",
            "document": "Hearing range . Behavioural hearing tests or physiological tests can be used to find hearing thresholds of humans and other animals. For humans, the test involves tones being presented at specific frequencies (pitch) and intensities (loudness). When the subject hears the sound, he or she indicates it by raising a hand or pressing a button. The lowest intensity they can hear is recorded. The test varies for children; their response to the sound can be indicated by a turn of the head or using a toy. The child learns what to do upon hearing the sound, such as placing a toy man in a boat. A similar technique can be used when testing animals, where food is used as a reward for responding to the sound. The information on different mammals hearing was obtained primarily by behavioural hearing tests.",
            "score": 69.5352635383606
        },
        {
            "docid": "7330954_3",
            "document": "Pattern recognition (psychology) . Pattern recognition occurs when information from the environment is received and entered into short-term memory, causing automatic activation of a specific content of long-term memory. An early example of this is learning the alphabet in order. When a carer repeats \u2018A, B, C\u2019 multiple times to a child, utilizing the pattern recognition, the child says \u2018C\u2019 after he/she hears \u2018A, B\u2019 in order. Recognizing patterns allow us to predict and expect what is coming. The process of pattern recognition involves matching the information received with the information already stored in the brain. Making the connection between memories and information perceived is a step of pattern recognition called identification. Pattern recognition requires repetition of experience. Semantic memory, which is used implicitly and subconsciously is the main type of memory involved with recognition.",
            "score": 128.8206126689911
        },
        {
            "docid": "6637571_6",
            "document": "List of whale vocalizations . All of the baleen whale sound files on this page (with the exception of the humpback vocalizations) are reproduced at 10x speed to bring the sound into the human auditory band. Like other whales, the male fin whale has been observed to make long, loud, low-frequency sounds. Most sounds are frequency-modulated (FM) down-swept infrasonic pulses from 16 to 40\u00a0hertz frequency (the range of sounds that most humans can hear falls between 20\u00a0hertz and 20\u00a0kilohertz). Each sound lasts between one and two seconds, and various combinations of sounds occur in patterned sequences lasting 7 to 15 minutes each. These sequences are then repeated in bouts lasting up to many days. The humpback whale is well known for its long and complex song. Humpbacks repeat patterns of low notes that vary in amplitude and frequency in consistent patterns over a period of hours or even days. Only male humpbacks sing, so it was at first assumed that the songs were solely for courting. While the primary purpose of whalesong may be to attract females, it's almost certain that whalesong serves myriad purposes. As with other dolphins, orcas are very vocal animals. They produce a variety of clicks and whistles that are used for communication and echolocation. The vocalization types vary with activity. While resting they are much quieter, merely emitting an occasional call that is distinct from those heard when engaging in more active behaviour.",
            "score": 130.89290976524353
        },
        {
            "docid": "7330954_34",
            "document": "Pattern recognition (psychology) . Pattern recognition of music can build and strengthen other skills, such as musical synchrony and attentional performance and musical notation and brain engagement. Even a few years of musical training enhances memory and attention levels. Scientists at University of Newcastle conducted a study on patients with severe acquired brain injuries (ABIs) and healthy participants, using popular music to examine music-evoked autobiographical memories (MEAMs). The participants were asked to record their familiarity with the songs, whether they liked them and what memories they evoked. The results showed that the ABI patients had the highest MEAMs, and all the participants had MEAMs of a person, people or life period that were generally positive. The participants completed the task by utilizing pattern recognition skills. Memory evocation caused the songs to sound more familiar and well-liked. This research can be beneficial to rehabilitating patients of autobiographical amnesia who do not have fundamental deficiency in autobiographical recall memory and intact pitch perception.",
            "score": 135.20535385608673
        },
        {
            "docid": "14339999_6",
            "document": "Virtual pitch . Terhardt rejected the idea of periodicity pitch, because it was not consistent with empirical data on pitch perception, e.g. measurements of the gradual shift of the virtual pitch of a complex tone with a missing fundamental when the partials were gradually shifted. Terhardt instead broke pitch perception into two steps: auditory frequency analysis in the inner ear, and harmonic pitch pattern recognition in the brain. The inner ear effectively performs a running frequency analysis of incoming sounds - otherwise we would not be able to hear out spectral pitches within a complex tone. Physiologically, each spectral pitch depends on both temporal and spectral aspects (i.e. periodicity of the waveform and position of excitation on the basilar membrane), but in Terhardt's approach the spectral pitch itself is a purely experiential parameter, not a physical parameter: it is the outcome of a psychoacoustical experiment in which the conscious listener plays an active role. Psychoacoustic measurements and models can predict which partials are \"perceptually relevant\" in a given complex tone; they are perceptually relevant if you can hear a difference in the whole sound if the frequency or amplitude of a partial is changed). The ear has evolved to separate spectral frequencies, because due to reflection and superposition in everyday environments spectral frequencies are more reliably carriers of environmental information than spectral amplitudies, which in turn are more reliable carriers of environmentally relevant information than phase relationships between partials (when perceived monoaurally). On this basis, Terhardt proposed that spectral pitches - which are what the listener experiences when hearing out partials (as opposed to the physical partials themselves) - are the only information available to the brain for the purpose of extracting virtual pitches. The \"pitch extraction\" process then involves the recognition of incomplete harmonic patterns and happens in neural networks.",
            "score": 103.72142088413239
        },
        {
            "docid": "497973_5",
            "document": "Fire alarm notification appliance . Coding refers to the pattern or tones a notification appliance sounds in and is controlled either by the panel or by setting jumpers or DIP switches on the notification appliances. The majority of audible notification appliances installed prior to 1996 produced a steady sound for evacuation. In general, no common standard at that time mandated any particular tone, or pattern for audible fire alarm evacuation signals. While less common than a steady sound, differing signaling methods were used for the same purpose. These are named with respect to their distinctive structure and include, March Time (usually 120 pulses per minute but sometimes at 90 pulses or 20 pulses per minute, depending on the panel), Hi-Lo (two different tones that alternate), Slow-Whoop (slow rising sweep upwards in tone) among others. Today these methods are confined to applications intended to trigger a response other than evacuation alone. In 1996, the ANSI and the NFPA recommended a standard evacuation pattern to eliminate confusion. The pattern is uniform without regard to the sound used. This pattern, which is also used for smoke alarms, is named the Temporal-Three alarm signal, often referred to as \"T-3\" (ISO 8201 and ANSI/ASA S3.41 Temporal Pattern) and produces an interrupted four count (three half second pulses, followed by a one and one half second pause, repeated for a minimum of 180 seconds). CO (carbon monoxide) detectors are specified to use a similar pattern using four pulses of tone (often referred to as T4).",
            "score": 109.08821380138397
        },
        {
            "docid": "25049383_36",
            "document": "Cognitive neuroscience of music . Absolute pitch (AP) is defined as the ability to identify the pitch of a musical tone or to produce a musical tone at a given pitch without the use of an external reference pitch. Neuroscientific research has not discovered a distinct activation pattern common for possessors of AP. Zatorre, Perry, Beckett, Westbury and Evans (1998) examined the neural foundations of AP using functional and structural brain imaging techniques. Positron emission tomography (PET) was utilized to measure cerebral blood flow (CBF) in musicians possessing AP and musicians lacking AP. When presented with musical tones, similar patterns of increased CBF in auditory cortical areas emerged in both groups. AP possessors and non-AP subjects demonstrated similar patterns of left dorsolateral frontal activity when they performed relative pitch judgments. However, in non-AP subjects activation in the right inferior frontal cortex was present whereas AP possessors showed no such activity. This finding suggests that musicians with AP do not need access to working memory devices for such tasks. These findings imply that there is no specific regional activation pattern unique to AP. Rather, the availability of specific processing mechanisms and task demands determine the recruited neural areas.",
            "score": 84.43520402908325
        },
        {
            "docid": "752723_2",
            "document": "Dichotic listening test . The Dichotic listening test is a psychological test commonly used to investigate selective attention within the auditory system and is a subtopic of cognitive psychology and neuroscience. Specifically, it is \"used as a behavioral test for hemispheric lateralization of speech sound perception.\" During a standard dichotic listening test, a participant is presented with two different auditory stimuli simultaneously (usually speech). The different stimuli are directed into different ears over headphones. Research Participants were instructed to repeat aloud the words they heard in one ear while a different message was presented to the other ear. As a result of focusing to repeat the words, participants noticed little of the message to the other ear, often not even realizing that at some point it changed from English to German. At the same time, participants did notice when the voice in the unattended ear changed from a male\u2019s to a female\u2019s, suggesting that the selectivity of consciousness can work to tune in some information.\"",
            "score": 67.90646839141846
        },
        {
            "docid": "21312304_25",
            "document": "Olfactory memory . The development of a sense of smell is also thought to have arisen to function as an arousal system. Once an odor enters into conscious memory, it can signal the presence of a threat, like the smell of gas or smoke. However, odor memory can also be an implicit or unconscious process. This ability to respond automatically to a warning stimulus is much like pre-attentive processes in other sensory systems which involve the use of automatic forms of memory. These response patterns have evolved over time and involve a wide variety of motor and autonomic responses which are integrated into the behaviour pattern of reacting to a warning stimulus. odor-induced anxiety can be caused when an animal senses a predator. A study conducted on rats showed that when a rat was exposed to cat odors, there was increased anxiety-related behaviour in the rat. The cat odor induced an inhibition of the endocannabinoid system in the amygdala which has been suggested to induce anxiety-related responses.",
            "score": 97.52506399154663
        },
        {
            "docid": "31075772_15",
            "document": "Thought identification . On 31 January 2012 Brian Pasley and colleagues of University of California Berkeley published their paper in PLoS Biology wherein subjects' internal neural processing of auditory information was decoded and reconstructed as sound on computer by gathering and analyzing electrical signals directly from subjects' brains. The research team conducted their studies on the superior temporal gyrus, a region of the brain that is involved in higher order neural processing to make semantic sense from auditory information. The research team used a computer model to analyze various parts of the brain that might be involved in neural firing while processing auditory signals. Using the computational model, scientists were able to identify the brain activity involved in processing auditory information when subjects were presented with recording of individual words. Later, the computer model of auditory information processing was used to reconstruct some of the words back into sound based on the neural processing of the subjects. However the reconstructed sounds were not of good quality and could be recognized only when the audio wave patterns of the reconstructed sound were visually matched with the audio wave patterns of the original sound that was presented to the subjects. However this research marks a direction towards more precise identification of neural activity in cognition.",
            "score": 99.41190576553345
        },
        {
            "docid": "33106906_48",
            "document": "Eyewitness memory . Researchers have also investigated to what extent the distinctiveness of a voice, such as heightened emotion, can aid or impair an individual's recollection of it. There is evidence that faces are better remembered if they display emotion compared to when they appear neutral; in one study healthy control participants remembered more accurately happy faces than they did neutral faces. Likewise, a host of studies have found that memories that are more emotional in nature are more complex and are less likely to be forgotten compared to memories that are more neutral. It therefore seems logical for researchers to explore whether auditory material which is emotional in nature is also remembered better. Research has produced conflicting results. Bradley and Lang (2000) found that there was a memory advantage for auditory material when it was more emotional compared to when it was more neutral. The authors also found that participants' physiological activity when they listened to emotionally arousing sounds was very similar to the physiological arousal produced when they were shown emotional images. However, studies investigating emotion in voices have found no significant differences between recall rates for emotional voices and neutral voices, with some research even demonstrating that emotion can impair memory recall for the voice. For instance, it was found that angry voices were recalled to a lesser extent compared to if they were neutral in tone. This finding has been supported by other studies which have also found that rather than enhancing voice identification, emotion may significantly interfere with it. However, ethical guidelines will confine the levels of emotionality that are appropriate to be induced in participants in a laboratory study environment.",
            "score": 95.19452691078186
        },
        {
            "docid": "9736652_25",
            "document": "Auditory masking . When a sinusoidal signal and a sinusoidal masker (tone) are presented simultaneously the envelope of the combined stimulus fluctuates in a regular pattern described as beats. The fluctuations occur at a rate defined by the difference between the frequencies of the two sounds. If the frequency difference is small then the sound is perceived as a periodic change in the loudness of a single tone. If the beats are fast then this can be described as a sensation of roughness. When there is a large frequency separation, the two components are heard as separate tones without roughness or beats. Beats can be a cue to the presence of a signal even when the signal itself is not audible. The influence of beats can be reduced by using a narrowband noise rather than a sinusoidal tone for either signal or masker.",
            "score": 75.54930591583252
        },
        {
            "docid": "9736652_7",
            "document": "Auditory masking . If two sounds of two different frequencies are played at the same time, two separate sounds can often be heard rather than a combination tone. The ability to hear frequencies separately is known as \"frequency resolution\" or \"frequency selectivity\". When signals are perceived as a combination tone, they are said to reside in the same \"critical bandwidth\". This effect is thought to occur due to filtering within the cochlea, the hearing organ in the inner ear. A complex sound is split into different frequency components and these components cause a peak in the pattern of vibration at a specific place on the cilia inside the basilar membrane within the cochlea. These components are then coded independently on the auditory nerve which transmits sound information to the brain. This individual coding only occurs if the frequency components are different enough in frequency, otherwise they are in the same critical band and are coded at the same place and are perceived as one sound instead of two.",
            "score": 89.58359849452972
        },
        {
            "docid": "31815567_10",
            "document": "Glitch art . Misalignment glitches are produced by opening a digital file of one type with a program designed for a different type of file, such as opening a video file as a sound file, or using the wrong codec to decompress a file. Tools commonly used to create glitches of this type include Audacity and WordPad. Artist Jamie Boulton explains the process and the glitches it produces, noting that these glitches depend on how Audacity handles files, even when they are not audio-encoded: The easiest way to manipulate a file in Audacity is to select a section of the file and apply one of the built in sound effects to it. Now I\u2019m no computing whizz kid but the way I see it when you apply a sound effect to a sound file, the program takes that file and alters the file data in the manner which it\u2019s been told will achieve that effect. So, for example, if you were to apply an echo effect then it would repeat parts of the file, diminishing the repetition after each iteration. The wonderful thing is that it will do this regardless of what the file actually is. Audacity doesn\u2019t know or care whether the file is a sound or not, it will alter it in the manner instructed. Hardware failure happens by altering the physical wiring or other internal connections of the machine itself, such as a short-circuit, in a process called \"circuit bending\" causes the machine to create glitches that produce new sounds and visuals. For example, by damaging internal pieces of something like a VHS player, one can achieve different colorful visual images. Video artist Tom DeFanti explained the role of hardware failure in a voice-over for Jamie Fenton's early glitch video \"Digital TV Dinner\" that used the Bally video game console system: This piece represents the absolute cheapest one can go in home computer art. This involves taking a $300 video game system, pounding it with your fist so the cartridge pops out while its trying to write the menu. The music here is done by Dick Ainsworth using the same system, but pounding it with your fingers instead of your fist. Physically beating the case of the game system would cause the game cartridge to pop out, interrupting the computer's operation. The glitches that resulted from this failure were a result of how the machine was set-up: There was ROM memory in the cartridge and ROM memory built into the console. Popping out the cartridge while executing code in the console ROM created garbage references in the stack frames and invalid pointers, which caused the strange patterns to be drawn. [...] The Bally Astrocade was unique among cartridge games in that it was designed to allow users to change game cartridges with power-on. When pressing the reset button, it was possible to remove the cartridge from the system and induce various memory dump pattern sequences. Digital TV Dinner is a collection of these curious states of silicon epilepsy set to music composed and generated upon this same platform. Misregistration is produced by the physical noise of historically analog media such as motion picture film. It includes dirt, scratches, smudges and markings that can distort physical media also impact the playback of digital recordings on media such as CDs and DVDs, as electronic music composer Kim Cascone explained in 2002:",
            "score": 108.27491283416748
        },
        {
            "docid": "158478_66",
            "document": "Pedestrian crossing . In the United States, the standards in the 2009 MUTCD require APS units to have a pushbutton locator tone, audible and vibrotactile walk indications, a tactile arrow aligned with the direction of travel on the crosswalk, and to respond to ambient sound. The pushbutton locator tone is a beep or tick, repeating at once per second, to allow people who are blind to find the device. If APS units are installed in more than one crossing direction (e.g. if there are APS units at a curb for both the north\u2013south and west\u2013east crossing directions), different sounds or speech messages may be used for each direction. Under the MUTCD guideline, the walk indication may be a speech message if two or more units on the same curb are separated by less than . These speech messages usually follow the pattern \"[Street name]. Walk sign is on to cross [Street Name].\" Otherwise, the walk indication may be a \"percussive tone,\" which usually consists of repeated, rapid sounds that can be clearly heard from the opposite curb and can oscillate between high and low volumes. In both cases, when the \"don't walk\" indication is flashing, the device will beep at every second until the \"don't walk\" indication becomes steady and the pedestrian countdown indication reaches \"0\", at which point the device will beep intermittently at lower volume. When activated, the APS units are mandated to be accompanied by a vibrating arrow on the APS during the walk signal.",
            "score": 81.80704426765442
        },
        {
            "docid": "21312265_6",
            "document": "Retrospective memory . Retrospective episodic memory is the memory of moments from the past. It is frequently used in studies of Alzheimer patients and testing their dementia. A study by Livner et al. (2009) compared the effect of the disease on both prospective and retrospective memory. In this case the episodic memory being tested was the ability to remember the testing instructions. To test retrospective memory participants were presented with a list of nouns that had been divided into four categories. The results of retrospective memory were divided into three sections: number of categories, number of items remembered and forgetting ratio, in order to look at the three separate process in creating memory (encoding, retrieval, storage). Using their results and knowledge of episodic memory the researchers were able to find a pattern of functional impairments in the brain.",
            "score": 104.49865412712097
        },
        {
            "docid": "501222_108",
            "document": "Isan language . Despite the differences in pattern, the orthography used to write words is nearly the same in Thai and Lao, even using the same tone marks in most places, so it is knowing the spoken language and how it maps out to the rules of the written language that determine the tone. However, as the Tai languages are tonal languages, with tone being an important phonemic feature, spoken Lao or Isan words out of context, even if they are cognate, may sound closer to Thai words of different meaning. Thai \u0e04\u0e32 \"kha\" , 'to stick' is cognate to Isan \u0e04\u0e32 and Lao \u0e84\u0eb2, which in Vientiane Lao is pronounced , which may sound like Thai \u0e04\u0e49\u0e32 \"kha\" , 'to trade' due to similarity in tone. The same word in some parts of Isan near Roi Et Province would confusingly sound to Thai ears like \u0e02\u0e32 \"kha\" with a rising tone, where the local tone patterns would have many pronounce the word with a rising-high-falling heavier on the rising. Although a native Thai speaker would be able to pick up the meaning of the similar words of Isan through context, and after a period of time, would get used to the different tones (with most Lao and Isan speech varieties having an additional one or two tones to the five of Thai), it can cause many initial misunderstandings.",
            "score": 96.96708166599274
        },
        {
            "docid": "3557219_19",
            "document": "Neuroimaging . Most fMRI scanners allow subjects to be presented with different visual images, sounds and touch stimuli, and to make different actions such as pressing a button or moving a joystick. Consequently, fMRI can be used to reveal brain structures and processes associated with perception, thought and action. The resolution of fMRI is about 2-3 millimeters at present, limited by the spatial spread of the hemodynamic response to neural activity. It has largely superseded PET for the study of brain activation patterns. PET, however, retains the significant advantage of being able to identify specific brain receptors (or transporters) associated with particular neurotransmitters through its ability to image radiolabelled receptor \"ligands\" (receptor ligands are any chemicals that stick to receptors).",
            "score": 75.01409840583801
        },
        {
            "docid": "22266890_13",
            "document": "LC4MP . Methodologically, the LC4MP uses three primary sets of measures to study how people allocate resources and process information. Secondary task reaction time (STRT) is a measure of the amount of resources allocated to attention. STRT involves people engaging in a primary task such as watching TV while also monitoring a second task such as pressing a button whenever a cue such as a tone sounds or a light comes on. The amount of resources that are available (i.e., resources allocated minus resources required) has been demonstrated to influence how long it takes people to respond in the STRT. When there are a lot of resources available, people quickly notice the light or tone and are quick to press a button. But when resources are low or taxed, people take longer to notice the light or tone and are slower to respond. For example, a cut to an unrelated scene tends to tax resources because of the new information that needs to be processed, and STRT is slower. Critically, STRT is a function of both the resources allocated to the task and the resources required by the task.",
            "score": 48.099247217178345
        },
        {
            "docid": "10992052_12",
            "document": "Methods of neuro-linguistic programming . The notion that experience is processed by the sensory systems or representational systems, was incorporated into NLP from psychology and gestalt therapy shortly after its creation. This teaches that people perceive the world through the senses and store the information from the senses in the mind. Memories are closely linked to sensory experience. When people are processing information they see images and hear sounds and voices and process this with internally created feelings. Some representations are within conscious awareness but information is largely processed at the unconscious level. When involved in any task, such as making conversation, describing a problem in therapy, reading a book, kicking a ball or riding a horse, their representational systems, consisting of images, sounds, feelings (and possibly smell and taste) are being activated at the same time. Moreover, the way representational systems are organised and the links between them impact on behavioral performance. Many NLP techniques rely on interrupting maladaptive patterns and replacing them with more positive and creative thought patterns which will in turn impact on behavior.",
            "score": 100.48494696617126
        },
        {
            "docid": "45464854_4",
            "document": "Sensory memory . Echoic memory represents SM for the auditory sense of hearing. Auditory information travels as sound waves which are sensed by hair cells in the ears. Information is sent to and processed in the temporal lobe. The echoic sensory store holds information for 2\u20133 seconds to allow for proper processing. The first studies of echoic memory came shortly after Sperling investigated iconic memory using an adapted partial report paradigm. Today, characteristics of echoic memory have been found mainly using a mismatch negativity (MMN) paradigm which utilizes EEG and MEG recordings. MMN has been used to identify some of the key roles of echoic memory such as change detection and language acquisition. Change detection, or the ability to detect an unusual or possibly dangerous change in the environment independent of attention, is key to the survival of an organism. One study focusing on echoic sensory changes suggested that when a sound is presented to a subject, it is enough to shape an echoic memory trace that can be compared to a physically different sound. Change-related cortical responses were detected in the superior temporal gyrus using EEG . With regards to language, a characteristic of children who begin speaking late in development is reduced duration of echoic memory. In short, \"Echoic memory is a fast-decaying store of auditory information.\" In the case of damage to or lesions developing on the frontal lobe, parietal lobe, or hippocampus, echoic memory will likely be shortened and/or have a slower reaction time.",
            "score": 95.01869583129883
        },
        {
            "docid": "2263473_17",
            "document": "Volley theory . A fundamental frequency is the lowest frequency of a harmonic. In some cases, sound can have all the frequencies of a harmonic but be missing the fundamental frequency, this is known as missing fundamental. When listening to a sound with a missing fundamental, the human brain still receives information for all frequencies, including the fundamental frequency which does not exist in the sound. This implies that sound is encoded by neurons firing at all frequencies of a harmonic, therefore, the neurons must be locked in some way to result in the hearing of one sound. Congenital deafness or sensorineural hearing loss is an often used model for the study of the inner ear regarding pitch perception and theories of hearing in general. Frequency analysis of these individuals\u2019 hearing has given insight on common deviations from normal tuning curves, excitation patterns, and frequency discrimination ranges. By applying pure or complex tones, information on pitch perception can be obtained. In 1983, it was shown that subjects with low frequency sensorineural hearing loss demonstrated abnormal psychophysical tuning curves. Changes in the spatial responses in these subjects showed similar pitch judgment abilities when compared to subjects with normal spatial responses. This was especially true regarding low frequency stimuli. These results suggest that the place theory of hearing does not explain pitch perception at low frequencies, but that the temporal (frequency) theory is more likely. This conclusion is due to the finding that when deprived of basilar membrane place information, these patients still demonstrated normal pitch perception. Computer models for pitch perception and loudness perception are often used during hearing studies on acoustically impaired subjects. The combination of this modeling and knowledge of natural hearing allows for better development of hearing aids.",
            "score": 96.20106256008148
        },
        {
            "docid": "705433_8",
            "document": "Roland RE-201 . The Roland RE-100 Echo Chamber was one of the earliest models of tape delay that Roland produced being released in 1973. Roland had previously been releasing tape delays under the name Ace Tone, these units included the Ace Tone EC-1 (Echo Chamber 1). The Roland RE-100 was released alongside the Roland RE-200 which had the added addition of a spring reverb. The RE-100 was similar in styling to the later Roland tape delay units, house in a similar black wooden casing. The front panel had a mode selector dial with six different delay settings. For repeat sounds settings 1-3 were used and for more of a 'swelling' sound settings 4-6 could be selected. There were controls for intensity which when turned clockwise would increase the number of echoes. The repeat rate control was used for adjusting the repeat times of the echoes. Controls were also available for the echo volume, this would decide how much effected signal would be heard along with the original signal. There were two controls for the tone of the output signal, these were simple bass and treble controls. The difference that these early units had from later models was the standby switch. This switch controlled the tape movement. When the lever was pushed upwards the tape began moving to activate the echo sound. When the echo was not needed the switch was pushed downwards, the idea of the standby switch was to assure longer lives of the tape heads and the tape itself.",
            "score": 57.949747920036316
        },
        {
            "docid": "18190064_13",
            "document": "Eyewitness testimony . In a 1932 study, Frederic Bartlett demonstrated how serial reproduction of a story distorted accuracy in recalling information. He told participants a complicated Native American story and had them repeat it over a series of intervals. With each repetition, the stories were altered. Even when participants recalled accurate information, they filled in gaps with information that would fit their personal experiences. His work showed long term memory to be adaptable. Bartlett viewed schemas as a major cause of this occurrence. People attempt to place past events into existing representations of the world, making the memory more coherent. Instead of remembering precise details about commonplace occurrences, a schema is developed. A schema is a generalization formed mentally based on experience. The common use of these schemas suggests that memory is not an identical reproduction of experience, but a combination of actual events with already existing schemas. Bartlett summarized this issue, explaining  Further research of schemas shows memories that are inconsistent with a schema decay faster than those that match up with a schema. Tuckey and Brewer found pieces of information that were inconsistent with a typical robbery decayed much faster than those that were schema consistent over a 12-week period, unless the information stood out as being extremely unusual. The use of schemas has been shown to increase the accuracy of recall of schema-consistent information but this comes at the cost of decreased recall of schema-inconsistent information.",
            "score": 72.93521404266357
        },
        {
            "docid": "33107185_14",
            "document": "Music and emotion . Which emotion is perceived is dependent on the context of the piece of music. Past research has argued that opposing emotions like happiness and sadness fall on a bipolar scale, where both cannot be felt at the same time. More recent research has suggested that happiness and sadness are experienced separately, which implies that they can be felt concurrently. One study investigated the latter possibility by having participants listen to computer-manipulated musical excerpts that have mixed cues between tempo and mode. Examples of mix-cue music include a piece with major key and slow tempo, and a minor-chord piece with a fast tempo. Participants then rated the extent to which the piece conveyed happiness or sadness. The results indicated that mixed-cue music conveys both happiness and sadness; however, it remained unclear whether participants perceived happiness and sadness simultaneously or vacillated between these two emotions. A follow up study was done to examine these possibilities. While listening to mixed or consistent cue music, participants pressed one button when the music conveyed happiness, and another button when it conveyed sadness. The results revealed that subjects pressed both buttons simultaneously during songs with conflicting cues. These findings indicate that listeners can perceive both happiness and sadness concurrently. This has significant implications for how the structural features influence emotion, because when a mix of structural cues is used, a number of emotions may be conveyed.",
            "score": 61.781408190727234
        },
        {
            "docid": "28070_37",
            "document": "Communication during the September 11 attacks . On the audio track, an outside agency, possibly in New Jersey and using a repeater, comes through the receive audio on the Port Authority Repeater 7 system. An ambulance being dispatched by the outside (non-FDNY) agency is heard. This may be what the FDNY had described as interference caused when the repeater was left enabled at all times. The distant user appears to be repeated through the system, (possibly on the same CTCSS tone as was configured in Repeater 7). This appears to be a distant co-channel user on the same input frequency as Repeater 7. It's possible that by the random button pressing, a user sent a function tone that temporarily put the base station in \"monitor\" and that's what caused the outside agency's traffic to be heard. This is unlikely because subsequent transmit function tones should have toggled the receiver from monitor back to CTCSS-enabled.",
            "score": 41.91759705543518
        },
        {
            "docid": "1379269_3",
            "document": "Musical acoustics . Whenever two different pitches are played at the same time, their sound waves interact with each other \u2013 the highs and lows in the air pressure reinforce each other to produce a different sound wave. Any repeating sound wave that is not a sine wave can be modeled by many different sine waves of the appropriate frequencies and amplitudes (a frequency spectrum). In humans the hearing apparatus (composed of the ears and brain) can usually isolate these tones and hear them distinctly. When two or more tones are played at once, a variation of air pressure at the ear \"contains\" the pitches of each, and the ear and/or brain isolate and decode them into distinct tones.",
            "score": 75.60880780220032
        },
        {
            "docid": "315578_7",
            "document": "Information processing . According to thefreedictionary.com, the definition of information processing is \"the sciences concerned with gathering, manipulating, storing, retrieving, and classifying recorded information\". It suggests that for information to be firmly implanted in memory, it must pass through three stages of mental processing; sensory memory, short-term memory, and long-term memory. An example of this is the working memory model. This includes the central executive, phonologic loop, episodic buffer,visuospatial sketchpad, verbal information, long term memory, and visual information (Sternberg & Sternberg, 2012). The central executive is like the secretary of the brain. It decides what needs attention and how to respond.The central executive then leads to three different subsections. The first is phonological storage, subvocal rehearsal, and the phonological loop. These sections work together to understand words, put the information into memory, and then hold the memory. The result is verbal information storage. The next subsection is the visuospatial sketchpad which works to store visual images. The storage capacity is brief but leads to understanding of visual stimuli. Finally, there is an episodic buffer. This section is capable of taking information and putting it into long-term memory. It is also able to take information from the phonological loop and visuospatial sketchpad, combining them with long-term memory to make \"a unitary episodic representation (Sternberg & Sternberg, 2012).  In order for these to work, the sensory register takes in via the five senses: visual, auditory, tactile, olfactory, and taste. These are all present since birth and are able to handle simultaneous processing (e.g., food \u2013 taste it, smell it, see it). In general, learning benefits occur when there is a developed process of pattern recognition. The sensory register has a large capacity and its behavioral response is very short (1\u20133 seconds).  Within this model, sensory store and short term memory or working memory has limited capacity. Sensory store is able to hold very limited amounts of information for very limited amounts of time. This phenomenon is very similar to having a picture taken with a flash. For a few brief moments after the flash goes off, the flash it seems to still be there. However, it is soon gone and there is no way to know it was there (Sternberg & Sternberg, 2012). Short term memory holds information for slightly longer periods of time, but still has a limited capacity. According to Linden (2007), \"The capacity of STM had initially been estimated at \"seven plus or minus two\" items (Miller 1956), which fits the observation from neuropsychological testing that the average digit span of healthy adults is about seven (Cowan and others 2005). However, it emerged that these numbers of items can only be retained if they are grouped into so-called chunks, using perceptual or conceptual associations between individual stimuli.\" Its duration is of 5\u201320 seconds before it is out of the subject's mind. This occurs often with names of people newly introduced to. Images or information based on meaning are stored here as well, but it decays without rehearsal or repetition of such information.  On the other hand, long-term memory has a potentially unlimited capacity (Sternberg & Sternberg, 2012) and its duration is indefinite. Although sometimes it is difficult to access, it encompasses everything learned until this point in time. One might become forgetful or feel as if the information is on the tip of the tongue.",
            "score": 89.80844366550446
        },
        {
            "docid": "8165347_53",
            "document": "Psychology of art . In another study using eye-movement patterns to investigate how experts view art, participants were shown realistic and abstract works of art under two conditions: one asking them to free scan the works, and the other asking them to memorize them. Participants' eye movements were tracked as they either looked at the images or tried to memorize them, and their recall for the memorized images was recorded. The researchers found no differences in the fixation frequency or time between picture types for experts and nonexperts. However, across sessions, the non-experts had more short fixations while free scanning the works, and fewer long fixations while trying to memorize; experts followed the opposite pattern. There was no significant difference in the recall of the images across groups, except experts recalled abstract images better that non-experts, and more pictorial details. These results show that people with arts expertise view repeated images less than non-experts, and can recall more details about images they have previously seen.",
            "score": 87.46678066253662
        }
    ],
    "r": [
        {
            "docid": "7330954_28",
            "document": "Pattern recognition (psychology) . Music provides deep and emotional experiences for the listener. These experiences become contents in long-term memory, and every time we hear the same tunes, those contents are activated. Recognizing the content by the pattern of the music affects our emotion. The mechanism that forms the pattern recognition of music and the experience has been studied by multiple researchers. The sensation felt when listening to our favorite music is evident by the dilation of the pupils, the increase in pulse and blood pressure, the streaming of blood to the leg muscles, and the activation of the cerebellum, the brain region associated with physical movement.  While retrieving the memory of a tune demonstrates general recognition of musical pattern, pattern recognition also occurs while listening to a tune for the first time. The recurring nature of the metre allows the listener to follow a tune, recognize the metre, expect its upcoming occurrence, and figure the rhythm. The excitement of following a familiar music pattern happens when the pattern breaks and becomes unpredictable. This following and breaking of a pattern creates a problem-solving opportunity for the mind that form the experience. Psychologist Daniel Levitin argues that the repetitions, melodic nature and organization of this music create meaning for the brain. The brain stores information in an arrangement of neurons which retrieve the same information when activated by the environment. By constantly referencing information and additional stimulation from the environment, the brain constructs musical features into a perceptual whole.",
            "score": 138.85971069335938
        },
        {
            "docid": "4778800_27",
            "document": "Gunfire locator . Another method of classifying gunfire uses \"temporal pattern recognition,\" as referred by its developer, that employs artificial neural networks that are trained and then listen for a sound signature in acoustic events. Like other acoustic sensing systems they are fundamentally based on the physics of acoustics, but they analyze the physical acoustic data using a neural network. Information in the network is coded in terms of variation in the sequence of all-or-none (spike) events, or temporal patterns, transmitted between artificial \"neurons\". Identifying the nonlinear input/output properties of neurons involved in forming memories for new patterns and developing mathematical models of those nonlinear properties enable the identification of specific types of sounds. These neural networks can then be trained as \"recognizers\" of a target sound, like a gunshot, even in the presence of high noise.",
            "score": 138.06436157226562
        },
        {
            "docid": "7330954_34",
            "document": "Pattern recognition (psychology) . Pattern recognition of music can build and strengthen other skills, such as musical synchrony and attentional performance and musical notation and brain engagement. Even a few years of musical training enhances memory and attention levels. Scientists at University of Newcastle conducted a study on patients with severe acquired brain injuries (ABIs) and healthy participants, using popular music to examine music-evoked autobiographical memories (MEAMs). The participants were asked to record their familiarity with the songs, whether they liked them and what memories they evoked. The results showed that the ABI patients had the highest MEAMs, and all the participants had MEAMs of a person, people or life period that were generally positive. The participants completed the task by utilizing pattern recognition skills. Memory evocation caused the songs to sound more familiar and well-liked. This research can be beneficial to rehabilitating patients of autobiographical amnesia who do not have fundamental deficiency in autobiographical recall memory and intact pitch perception.",
            "score": 135.20535278320312
        },
        {
            "docid": "7536395_5",
            "document": "Langue and parole . The underlying basis to \"langue\" is the interpretation that it is made up of signs and not sentences. Signs are thought to have a two part aspect in that each sign relates a notion with a sound pattern (or a written symbol). A sign cannot exist as a single part for if there is a sound pattern without a notion the sound becomes only noise. Similarly, a notion cannot be communicated without a sound pattern. It is also interesting to note that the sound pattern for each notion can be extremely diverse and vice versa. For example, the notion of oneself may use the sound patterns of 'I' or 'me' while the sound pattern of 'rose' may have the notion of a flower or the past of 'rise'. The notion or sound pattern remains unchanged even if the other changes. It is by understanding the relationship of the two parts of a sign through \"langue\" that the gist of communication or \"parole\" may be understood. Without the understanding of \"langue\", \"parole\" would be meaningless sounds or symbols grouped together haphazardly. Saussure used the example of chess to explain how \"langue\" and \"parole\" work together. \"Langue\" is the normative rules in a chess game while parole represents the individual's choice of moves. If one were to study the parole of a chess game an understanding could be derived but it would not be a universal understanding of chess. However, by studying the langue of a chess game the derived understanding may be applicable to further chess games. Thus Saussure argued when studying language, especially a foreign language, it is more important to understand the \"langue\" than to gain a large vocabulary of \"parole\" so that sense may be made equal to that of native speaker.",
            "score": 133.00245666503906
        },
        {
            "docid": "6637571_6",
            "document": "List of whale vocalizations . All of the baleen whale sound files on this page (with the exception of the humpback vocalizations) are reproduced at 10x speed to bring the sound into the human auditory band. Like other whales, the male fin whale has been observed to make long, loud, low-frequency sounds. Most sounds are frequency-modulated (FM) down-swept infrasonic pulses from 16 to 40\u00a0hertz frequency (the range of sounds that most humans can hear falls between 20\u00a0hertz and 20\u00a0kilohertz). Each sound lasts between one and two seconds, and various combinations of sounds occur in patterned sequences lasting 7 to 15 minutes each. These sequences are then repeated in bouts lasting up to many days. The humpback whale is well known for its long and complex song. Humpbacks repeat patterns of low notes that vary in amplitude and frequency in consistent patterns over a period of hours or even days. Only male humpbacks sing, so it was at first assumed that the songs were solely for courting. While the primary purpose of whalesong may be to attract females, it's almost certain that whalesong serves myriad purposes. As with other dolphins, orcas are very vocal animals. They produce a variety of clicks and whistles that are used for communication and echolocation. The vocalization types vary with activity. While resting they are much quieter, merely emitting an occasional call that is distinct from those heard when engaging in more active behaviour.",
            "score": 130.8928985595703
        },
        {
            "docid": "7330954_3",
            "document": "Pattern recognition (psychology) . Pattern recognition occurs when information from the environment is received and entered into short-term memory, causing automatic activation of a specific content of long-term memory. An early example of this is learning the alphabet in order. When a carer repeats \u2018A, B, C\u2019 multiple times to a child, utilizing the pattern recognition, the child says \u2018C\u2019 after he/she hears \u2018A, B\u2019 in order. Recognizing patterns allow us to predict and expect what is coming. The process of pattern recognition involves matching the information received with the information already stored in the brain. Making the connection between memories and information perceived is a step of pattern recognition called identification. Pattern recognition requires repetition of experience. Semantic memory, which is used implicitly and subconsciously is the main type of memory involved with recognition.",
            "score": 128.82061767578125
        },
        {
            "docid": "7330954_6",
            "document": "Pattern recognition (psychology) . Template matching theory describes the most basic approach to human pattern recognition. It is a theory that assumes every perceived object is stored as a \"template\" into long-term memory. Incoming information is compared to these templates to find an exact match. In other words, all sensory input is compared to multiple representations of an object to form one single conceptual understanding. The theory defines perception as a fundamentally recognition-based process. It assumes that everything we see, we understand only through past exposure, which then informs our future perception of the external world. For example, A, A, and \"A\" are all recognized as the letter A, but not B. This viewpoint is limited, however, in explaining how new experiences can be understood without being compared to an internal memory template.",
            "score": 125.91970825195312
        },
        {
            "docid": "18994087_28",
            "document": "Sound . Pitch is perceived as how \"low\" or \"high\" a sound is and represents the cyclic, repetitive nature of the vibrations that make up sound. For simple sounds, pitch relates to the frequency of the slowest vibration in the sound (called the fundamental harmonic). In the case of complex sounds, pitch perception can vary. Sometimes individuals identify different pitches for the same sound, based on their personal experience of particular sound patterns. Selection of a particular pitch is determined by pre-conscious examination of vibrations, including their frequencies and the balance between them. Specific attention is given to recognising potential harmonics. Every sound is placed on a pitch continuum from low to high. For example: white noise (random noise spread evenly across all frequencies) sounds higher in pitch than pink noise (random noise spread evenly across octaves) as white noise has more high frequency content. Figure 1 shows an example of pitch recognition. During the listening process, each sound is analysed for a repeating pattern (See Figure 1: orange arrows) and the results forwarded to the auditory cortex as a single pitch of a certain height (octave) and chroma (note name).",
            "score": 123.09599304199219
        },
        {
            "docid": "744997_22",
            "document": "Electronic voice phenomenon . Skeptics such as David Federlein, Chris French, Terence Hines and Michael Shermer say that EVP are usually recorded by raising the \"noise floor\" \u2013 the electrical noise created by all electrical devices \u2013 in order to create white noise. When this noise is filtered, it can be made to produce noises which sound like speech. Federlein says that this is no different from using a wah pedal on a guitar, which is a focused sweep filter which moves around the spectrum and creates open vowel sounds. This, according to Federlein, sounds exactly like some EVP. This, in combination with such things as cross modulation of radio stations or faulty ground loops can cause the impression of paranormal voices. The human brain evolved to recognize patterns, and if a person listens to enough noise the brain will detect words, even when there is no intelligent source for them. Expectation also plays an important part in making people believe they are hearing voices in random noise.",
            "score": 121.75923919677734
        },
        {
            "docid": "38365867_9",
            "document": "How to Create a Mind . Kurzweil opens the book by reminding us of the importance of thought experiments in the development of major theories, including evolution and relativity. It's worth noting that Kurzweil sees Darwin as \"a good contender\" for the leading scientist of the 19th century. He suggests his own thought experiments related to how the brain thinks and remembers things. For example, he asks the reader to recite the alphabet, but then to recite the alphabet backwards. The difficulty in going backwards suggests \"our memories are sequential and in order\". Later he asks the reader to visualize someone he has met only once or twice, the difficulty here suggests \"there are no images, videos, or sound recordings stored in the brain\" only sequences of patterns. Eventually he concludes the brain uses a hierarchy of pattern recognizers.",
            "score": 120.9427719116211
        },
        {
            "docid": "31148473_15",
            "document": "Transsaccadic memory . Irwin's early experiments tested participants ability to identify random dot patterns as the same, or as different across saccades. The control condition for this experiment presented dot patterns in the same spatial location without saccades, meaning participants had a single fixation point. A no-overlap control condition presented dot patterns in different spatial locations, while participants maintained fixation on one point. This tested the ability of visual short-term memory in identifying two individual patterns. The experimental condition showed dot patterns in the same spatial location but were viewed at separate fixations, forcing the use of transsaccadic memory. For the experimental condition, participants underwent a calibration phase, where they were shown five points in separate location to fixate on individually, for less than two seconds. The next phase presented a single fixation point for less than two seconds, which was followed by a random dot pattern presented in a different location, acting as a saccade target. The dot pattern disappeared when the saccade was initiated. Another dot pattern then appeared in the same location. Participants had to identify whether the two patterns were the same or different.  Results of the experiment showed performance of the experimental condition to be slightly less accurate than the no-overlap condition. Irwin attributed this decrease in accuracy to the extra neural processes involved in performing an eye movement, combined with processing the dot pattern. He concluded from this that transsaccadic memory does exist, but that it is very similar if not identical to short term visual memory and less similar to sensory memory.",
            "score": 119.848876953125
        },
        {
            "docid": "7330954_30",
            "document": "Pattern recognition (psychology) . To understand music pattern recognition, we need to understand the underlying cognitive systems that each handle a part of this process. Various activities are at work in this recognition of a piece of music and its patterns. Researchers have begun to unveil the reasons behind the stimulated reactions to music. Montreal-based researchers asked ten volunteers who got \"chills\" listening to music to listen to their favorite songs while their brain activity was being monitored. The results show the significant role of the nucleus accumbens (NAcc) region \u2013 involved with cognitive processes such as motivation, reward, addiction, etc. \u2013 creating the neural arrangements that make up the experience. A sense of reward prediction is created by anticipation before the climax of the tune, which comes to a sense of resolution when the climax is reached. The longer the listener is denied the expected pattern, the greater the emotional arousal when the pattern returns. Musicologist Leonard Meyer used fifty measures of Beethoven\u2019s 5th movement of the String Quartet in C-sharp minor, Op. 131 to examine this notion. The stronger this experience is, the more vivid memory it will create and store. This strength affects the speed and accuracy of retrieval and recognition of the musical pattern. The brain not only recognizes specific tunes, it distinguishes standard acoustic features, speech and music.",
            "score": 119.52271270751953
        },
        {
            "docid": "230361_26",
            "document": "Fin whale . Like other whales, males make long, loud, low-frequency sounds. The vocalizations of blue and fin whales are the lowest-frequency sounds made by any animal. Most sounds are frequency-modulated (FM) down-swept infrasonic pulses from 16 to 40\u00a0hertz frequency (the range of sounds that most humans can hear falls between 20\u00a0hertz and 20\u00a0kilohertz). Each sound lasts one to two seconds, and various sound combinations occur in patterned sequences lasting 7 to 15 minutes each. The whale then repeats the sequences in bouts lasting up to many days. The vocal sequences have source levels of up to 184\u2013186\u00a0decibels relative to 1\u00a0micropascal at a reference distance of one metre and can be detected hundreds of miles from their source.",
            "score": 118.8195571899414
        },
        {
            "docid": "14713486_9",
            "document": "Frog hearing and communication . Dr. Feng\u2019s work applies the neuroethology of frog communication to medicine. A recent project on hearing aids is based on how female frogs find their mates. Females must recognize the male they choose by his call. By localizing where his call is coming from she can find him. An additional challenge is that she is localizing his call while listening to the many other frogs in the chorus, and to the noise of the stream and insects. The breeding pond is a very noisy place, and females must distinguish a male\u2019s calls from the other noise. How they recognize the sound pattern of the male they are pursuing from the surrounding noise is similar to how intelligent hearing aids help people hear certain sounds and cancel out others. The underlying neural mechanisms are fast neural oscillations, and synaptic inhibition to cancel out noise. The timing and frequency of the sound also play a part in frog communication and may be used in Feng\u2019s work. He also studies bat echolocation to create intelligent hearing aids. He is also working on cochlear implants.",
            "score": 117.45508575439453
        },
        {
            "docid": "858937_32",
            "document": "Spatial memory . Also known as the Corsi Span Test, this psychological test is commonly used to determine the visual-spatial memory span and the implicit visual-spatial learning abilities of an individual. Participants sit with nine wooden 3x3-cm blocks fastened before them on a 25- x 30-cm baseboard in a standard random order. The experiment taps onto the blocks a sequence pattern which participants must then replicate. The blocks are numbered on the experimenters' side to allow for efficient pattern demonstration. The sequence length increases each trial until the participant is no longer able to replicate the pattern correctly. The test can be used to measure both short-term and long-term spatial memory, depending on the length of time between test and recall.",
            "score": 117.24237823486328
        },
        {
            "docid": "35075711_26",
            "document": "Spontaneous recovery . The pathway of recall associated with the retrieval of sound memories is the auditory system. Within the auditory system is the auditory cortex, which can be broken down into the primary auditory cortex and the belt areas. The primary auditory cortex is the main region of the brain that processes sound and is located on the superior temporal gyrus in the temporal lobe where it receives point-to-point input from the medial geniculate nucleus. From this, the primary auditory complex had a topographic map of the cochlea. The belt areas of the auditory complex receive more diffuse input from peripheral areas of the medial geniculate nucleus and therefore are less precise in tonotopic organization compared to the primary visual cortex. A 2001 study by Trama examined how different kinds of brain damage interfere with normal perception of music. One of his studied patients lost most of his auditory cortex to strokes, allowing him to still hear but making it difficult to understand music since he could not recognize harmonic patterns. Detecting a similarity between speech perception and sound perception, spontaneous recovery of lost auditory information is possible in those patients who have experienced a stroke or other major head trauma. Amusia is a disorder manifesting itself as a defect in processing pitch but also affects one's memory and recognition for music.",
            "score": 115.88996887207031
        },
        {
            "docid": "6979161_7",
            "document": "Ensoniq VFX . In 'Sequence' mode, the different patterns can be arranged and named (e.g. as 'Intro', 'Verse', 'Bridge', 'Ending', 'Part 1' etc.), and arranged in any order to make a finished song. Each pattern can be repeated up to 99 times each, which can save a lot of sequencer memory by, for example, recording a standard 2-bar drum/bass/piano part, then repeating it 8 times to make a 16-bar Verse. Tempo changes for each pattern could be programmed in to the patterns themselves or in song mode, to speed up or slow down the tempo at certain points. The user could also change the effects on a 'per-pattern' basis, allowing for more dramatic changes in the song, although that would make the output mute for a moment, if a different effect were selected.",
            "score": 115.6983871459961
        },
        {
            "docid": "7330954_26",
            "document": "Pattern recognition (psychology) . The first step in infant language acquisition is to decipher between the most basic sound units of their native language. This includes every consonant, every short and long vowel sound, and any additional letter combinations like \"th\" and \"ph\" in English. These units, called phonemes, are detected through exposure and pattern recognition. Infants use their \"innate feature detector\" capabilities to distinguish between the sounds of words. They split them into phonemes through a mechanism of categorical perception. Then they extract statistical information by recognizing which combinations of sounds are most likely to occur together, like \"qu\" or \"h\" plus a vowel. In this way, their ability to learn words is based directly on the accuracy of their earlier phonetic patterning.",
            "score": 115.5810546875
        },
        {
            "docid": "33547203_49",
            "document": "Sparse distributed memory . What goes on in the world-the system's \"subjective\" experience-is represented internally by a sequence of patterns in the focus. The memory stores this sequence and can recreate it later in the focus if addressed with a pattern similar to one encountered in the past. Thus, the memory learns to \"predict\" what is about to happen. Wide applications of the memory would be in systems that deal with real-world information in real time.",
            "score": 114.60807800292969
        },
        {
            "docid": "38365867_11",
            "document": "How to Create a Mind . Kurzweil's main thesis is that these hierarchical pattern recognizers are used not just for sensing the world, but for nearly all aspects of thought. For example, Kurzweil says memory recall is based on the same patterns that were used when sensing the world in the first place. Kurzweil says that learning is critical to human intelligence. A computer version of the neocortex would initially be like a new born baby, unable to do much. Only through repeated exposure to patterns would it eventually self-organize and become functional.",
            "score": 113.12910461425781
        },
        {
            "docid": "7394057_5",
            "document": "E-mu SP-1200 . The SP-1200 can store up to 100 patterns, 100 songs, and has a 5,000-note maximum memory for drum sequences. It also has a mono mix output and eight individual outputs, MIDI in/out/through, SMPTE sync, and a metronome output. There is one button that allows you to select between banks A, B, C, and D, which gives the user easy access to each of the 32 sounds. The front panel contains several LED lights, buttons, and eight volume and pitch faders for each sound in the selected bank. Below each fader is a large button to initialize the sound, or select the sound for editing, and a switch to turn the trigger's velocity sensitivity off or on. The sequencer works in the familiar pattern-style of placing short consecutive sections of samples into a song. The user can easily add swing quantization and tempo changes. The sequencer can sync the tempo to SMPTE, MIDI, or analogue clock pulses and is also capable of synchronizing the tempo to a tapping finger with the tap-tempo button.\"",
            "score": 113.0103988647461
        },
        {
            "docid": "45839_28",
            "document": "Drum machine . Programming of drum machines varies from product to product. On most products, it can be done in real time: the user creates drum patterns by pressing the trigger pads as though a drum kit were being played; or using step-sequencing: the pattern is built up over time by adding individual sounds at certain points by placing them, as with the TR-808 and TR-909, along a 16-step bar. For example, a generic 4-on-the-floor dance pattern could be made by placing a closed high hat on the 3rd, 7th, 11th, and 15th steps, then a kick drum on the 1st, 5th, 9th, and 13th steps, and a clap or snare on the 5th and 13th. This pattern could be varied in a multitude of ways to obtain fills, break-downs and other elements that the programmer sees fit, which in turn could be sequenced with song-sequence \u2014 essentially the drum machine plays back the programmed patterns from memory in an order the programmer has chosen. The machine will quantize entries that are slightly off-beat in order to make them exactly in time.",
            "score": 112.87982940673828
        },
        {
            "docid": "931802_11",
            "document": "Chunking (psychology) . Chunking is a flexible way of learning. Karl Lashley, in his classic paper on serial order (Lashley, 1951), argued that the sequential responses that appear to be organized in a linear and flat fashion concealed an underlying hierarchical structure. This was demonstrated in motor control by Rosenbaum et al. (1983). Thus sequences can consist of sub-sequences and these can in turn consist of sub-sub-sequences. Hierarchical representations of sequences have an edge over linear representations. They combine efficient local action at low hierarchical levels while maintaining the guidance of an overall structure. While the representation of a linear sequence is simple from storage point of view, there can be potential problems during retrieval. For instance, if there is a break in the sequence chain, subsequent elements will become inaccessible. On the other hand, a hierarchical representation would have multiple levels of representation. A break in the link between lower level nodes does not render any part of the sequence inaccessible, since the control nodes (chunk nodes) at the higher level would still be able to facilitate access to the lower level nodes.  Chunks in motor learning are identified by pauses between successive actions (Terrace, 2001). He also suggested that during the sequence performance stage (after learning), participants download list items as chunks during pauses. Terrace also argued for an operational definition of chunks suggesting a distinction between the notions of input and output chunks from the ideas of short-term and long-term memory. Input chunks reflect the limitation of working memory during the encoding of new information (how new information is stored in long-term memory), and how it is retrieved during subsequent recall. Output chunks reflect the organization of over-learned motor programs that are generated on-line in working memory. Sakai et al. (2003) showed that participants spontaneously organize a sequence into a number of chunks across few sets, and that these chunks were distinct among participants tested on the same sequence. Sakai et al. (2003) showed that performance of a shuffled sequence was poorer when the chunk patterns were disrupted than when the chunk patterns were preserved. Chunking patterns also seem to depend on the effectors used.",
            "score": 111.92253875732422
        },
        {
            "docid": "21106959_2",
            "document": "Revoicer . A revoicer provides communication assistance by carefully listening to the speech patterns uttered by an individual with a speech disability, using lipreading (speechreading) and attention to other cues if necessary for full understanding of the utterances, and then repeats the same words in a manner that is more clear and understandable to the listener. Revoicers generally have excellent skills in auditory phonetic/phonemic pattern recognition, similar to those utilized by a court reporter or stenographer, to identify the sounds of speech (phonemic sounds) of the speaker.",
            "score": 110.1869125366211
        },
        {
            "docid": "11273721_19",
            "document": "Hierarchical temporal memory . Cortical learning algorithms are able to learn continuously from each new input pattern, therefore no separate inference mode is necessary. During inference, HTM tries to match the stream of inputs to fragments of previously learned sequences. This allows each HTM layer to be constantly predicting the likely continuation of the recognized sequences. The index of the predicted sequence is the output of the layer. Since predictions tend to change less frequently than the input patterns, this leads to increasing temporal stability of the output in higher hierarchy levels. Prediction also helps to fill in missing patterns in the sequence and to interpret ambiguous data by biasing the system to infer what it predicted.",
            "score": 110.05147552490234
        },
        {
            "docid": "29468_72",
            "document": "Speech recognition . As mentioned earlier in this article, accuracy of speech recognition may vary depending on the following factors: e.g. the 10 digits \"zero\" to \"nine\" can be recognized essentially perfectly, but vocabulary sizes of 200, 5000 or 100000 may have error rates of 3%, 7% or 45% respectively. e.g. the 26 letters of the English alphabet are difficult to discriminate because they are confusable words (most notoriously, the E-set: \"B, C, D, E, G, P, T, V, Z\"); an 8% error rate is considered good for this vocabulary. A speaker-dependent system is intended for use by a single speaker. A speaker-independent system is intended for use by any speaker (more difficult). With isolated speech, single words are used, therefore it becomes easier to recognize the speech. With discontinuous speech full sentences separated by silence are used, therefore it becomes easier to recognize the speech as well as with isolated speech.  With continuous speech naturally spoken sentences are used, therefore it becomes harder to recognize the speech, different from both isolated and discontinuous speech. e.g. Querying application may dismiss the hypothesis \"The apple is red.\"  e.g. Constraints may be semantic; rejecting \"The apple is angry.\"  e.g. Syntactic; rejecting \"Red is apple the.\"  Constraints are often represented by a grammar.  When a person reads it's usually in a context that has been previously prepared, but when a person uses spontaneous speech, it is difficult to recognize the speech because of the disfluencies (like \"uh\" and \"um\", false starts, incomplete sentences, stuttering, coughing, and laughter) and limited vocabulary.  Environmental noise (e.g. Noise in a car or a factory)  Acoustical distortions (e.g. echoes, room acoustics) Speech recognition is a multi-levelled pattern recognition task. e.g. Phonemes, Words, Phrases, and Sentences; e.g. Known word pronunciations or legal word sequences, which can compensate for errors or uncertainties at lower level; By combining decisions probabilistically at all lower levels, and making more deterministic decisions only at the highest level, speech recognition by a machine is a process broken into several phases. Computationally, it is a problem in which a sound pattern has to be recognized or classified into a category that represents a meaning to a human. Every acoustic signal can be broken in smaller more basic sub-signals. As the more complex sound signal is broken into the smaller sub-sounds, different levels are created, where at the top level we have complex sounds, which are made of simpler sounds on lower level, and going to lower levels even more, we create more basic and shorter and simpler sounds. The lowest level, where the sounds are the most fundamental, a machine would check for simple and more probabilistic rules of what sound should represent. Once these sounds are put together into more complex sound on upper level, a new set of more deterministic rules should predict what new complex sound should represent. The most upper level of a deterministic rule should figure out the meaning of complex expressions. In order to expand our knowledge about speech recognition we need to take into a consideration neural networks. There are four steps of neural network approaches:  For telephone speech the sampling rate is 8000 samples per second;  computed every 10\u00a0ms, with one 10\u00a0ms section called a frame;",
            "score": 109.96978759765625
        },
        {
            "docid": "1502471_10",
            "document": "Prehistoric music . Motherese has a gestural vocabulary that is similar across cultures. The way mothers and babies raise and lower their voices and simultaneously change their expressions and move their hands is similar in Asia and Europe, for example (in spite of linguistic differences such as tone languages versus non-tone languages). The apparent universality of motherese could be explained either genetically or by universals of the human environment. A genetic explanation for the vocabulary of motherese would have to be biological and evolutionary; no such explanation has yet been found. Regarding environment, motherese may stem from universals of the prenatal environment. The human fetus can hear for 20 weeks before birth\u00a0\u2013 considerably longer than other animals, most of which cannot hear before birth at all. The fetus can also perceive movement and orientation for 20 weeks before birth. This is presumably not an accident of evolution, but an adaptation that promotes the survival of the infant after birth by improving bonding between the infant and the mother. If the fetus learns to perceive the emotional state of the mother via the internal sounds of her body (voice, heartbeat, footsteps, digestion etc.), it can presumably adjust its postnatal demands (e.g. crying) depending on her availability and in that way enhance its own survival as a fragile being in a dangerous world. Research on the ability of the fetus to learn and remember sound patterns, and on the active two-way nature of mother-infant communication, is consistent with this theory. If this theory is true, the internal sounds of the human body and the relationship between those patterns and emotional state may be the ultimate source of the relationship between patterns of sound and movement in music and their strong emotional connotations. This theory is consistent with the universal link between music and religion and the changed states of consciousness that music can co-evoke.",
            "score": 109.81873321533203
        },
        {
            "docid": "31111024_14",
            "document": "Memory error . Intrusion errors refer to when information that is related to the theme of a certain memory, but was not actually a part of the original episode, become associated with the event. This makes it difficult to distinguish which elements are in fact part of the original memory. One idea regarding how intrusion errors work is due to a lack of recall inhibition, which allows irrelevant information to be brought to awareness while attempting to remember. Another possible explanation is that intrusion errors result from a lack of new context integration into a viable memory trace, or into an already existing memory trace that is related to the appropriate memory. More explanations involve the temporal aspect of recall, meaning that as the time difference between the study periods of different lists approaches zero, the amount of intrusions between the lists tends to increase, the semantic aspect, meaning that the list of target words may have induced a false recall of non-target words that happen to have a similar or same meaning as the targets, and the similarity aspect, for example subjects who were given list of letters to recall were likely to replace target vowels with non-target vowels. Intrusion errors can be divided into two categories. The first are known as extra-list errors, which occur when incorrect and non-related items are recalled, and were not part of the word study list. These types of intrusion errors often follow the DRM Paradigm effects, in which the incorrectly recalled items are often thematically related to the study list one is attempting to recall from. Another pattern for extra-list intrusions would be an acoustic similarity pattern, this pattern states that targets that have a similar sound to non-targets may be replaced with those non-targets in recall. One major type of extra-list intrusions is called the \"Prior-List Intrusion\" (PLI), a PLI occurs when targets from previously studied lists are recalled instead of the targets in the current list. PLIs often follow under the temporal aspect of intrusions in that since they were recalled recently they have a high chance of being recalled now. The second type of intrusion error is known as intra-list errors, which is similar to extra-list errors, except it refers to irrelevant recall for items that were on the word study list. Although these two categories of intrusion errors are based on word list studies in laboratories, the concepts can be extrapolated to real-life situations. Also, the same three factors that play a critical role in correct recall (recency, temporal association and semantic relatedness) play a role in intrusions as well. .",
            "score": 109.70976257324219
        },
        {
            "docid": "1896271_17",
            "document": "Holonomic brain theory . In 1969 scientists D. Wilshaw, O. P. Buneman and H. Longuet-Higgins proposed an alternative, non-holographic model that fulfilled many of the same requirements as Gabor's original holographic model. The Gabor model did not explain how the brain could use Fourier analysis on incoming signals or how it would deal with the low signal-noise ratio in reconstructed memories. Longuet-Higgin's correlograph model built on the idea that any system could perform the same functions as a Fourier holograph if it could correlate pairs of patterns. It uses minute pinholes that do not produce diffraction patterns to create a similar reconstruction as that in Fourier holography. Like a hologram, a discrete correlograph can recognize displaced patterns and store information in a parallel and non-local way so it usually will not be destroyed by localized damage. They then expanded the model beyond the correlograph to an associative net where the points become parallel lines arranged in a grid. Horizontal lines represent axons of input neurons while vertical lines represent output neurons. Each intersection represents a modifiable synapse. Though this cannot recognize displaced patterns, it has a greater potential storage capacity. This was not necessarily meant to show how the brain is organized, but instead to show the possibility of improving on Gabor's original model. P. Van Heerden countered this model by demonstrating mathematically that the signal-noise ratio of a hologram could reach 50% of ideal. He also used a model with a 2D neural hologram network for fast searching imposed upon a 3D network for large storage capacity. A key quality of this model was its flexibility to change the orientation and fix distortions of stored information, which is important for our ability to recognize an object as the same entity from different angles and positions, something the correlograph and association network models lack.",
            "score": 109.49515533447266
        },
        {
            "docid": "497973_5",
            "document": "Fire alarm notification appliance . Coding refers to the pattern or tones a notification appliance sounds in and is controlled either by the panel or by setting jumpers or DIP switches on the notification appliances. The majority of audible notification appliances installed prior to 1996 produced a steady sound for evacuation. In general, no common standard at that time mandated any particular tone, or pattern for audible fire alarm evacuation signals. While less common than a steady sound, differing signaling methods were used for the same purpose. These are named with respect to their distinctive structure and include, March Time (usually 120 pulses per minute but sometimes at 90 pulses or 20 pulses per minute, depending on the panel), Hi-Lo (two different tones that alternate), Slow-Whoop (slow rising sweep upwards in tone) among others. Today these methods are confined to applications intended to trigger a response other than evacuation alone. In 1996, the ANSI and the NFPA recommended a standard evacuation pattern to eliminate confusion. The pattern is uniform without regard to the sound used. This pattern, which is also used for smoke alarms, is named the Temporal-Three alarm signal, often referred to as \"T-3\" (ISO 8201 and ANSI/ASA S3.41 Temporal Pattern) and produces an interrupted four count (three half second pulses, followed by a one and one half second pause, repeated for a minimum of 180 seconds). CO (carbon monoxide) detectors are specified to use a similar pattern using four pulses of tone (often referred to as T4).",
            "score": 109.08821868896484
        },
        {
            "docid": "34731827_20",
            "document": "Episodic-like memory . Researchers in Australia found what they consider to be Circadian timed episodic-like memory in honey bees. In their study they examined foraging bees in three Y-mazes. Two mazes represented training and testing zones with different time placements. Maze C acted as the location for transfer tests. The bees were placed in Maze A in the afternoon hours of 2:30 to 5:30, whereas Maze B held bees in the morning hours of 9:30 to 12:30. All of the three mazes had two compartments, and in Mazes A and B, one held a sugar solution as a reward for making the positive decision. During training, the compartment containing the positive pattern was changed every thirty minutes. In Maze A, the positive (rewarded) pattern was a blue horizontal pattern, while the negative (non-rewarded) pattern was a blue vertical pattern. Additionally, the positive pattern for Maze B was a yellow vertical (rewarded) pattern, with a yellow horizontal pattern as a negative (non-rewarded) pattern. The three aspects of episodic-like memory in this experiment are the morning or afternoon times (when), either Maze A or B (where), and by using horizontal/vertical patterns (what).",
            "score": 108.74314880371094
        },
        {
            "docid": "3747039_6",
            "document": "Roland MC-909 . The 909's sequencer is based on pattern composition. Each pattern has 16-tracks (parts) and can have up to 999 measures (bars).  The \"pattern\" in the Groovebox concept as developed by Roland (and thence adopted by other manufacturers) is intended to be a 4-to-16 bars-long small musical phrase made up of 8-to-16 tracks, and the chaining of several patterns together (with seamless passage between one another) will create a full song, or the patterns can be looped as wanted and messed with using the onboard realtime controls. However, if this was more properly true in the older MC-303 and 505, in the 909 the massive capacity of the sequencer makes the patterns capable of storing almost 1000 bars, and 16 tracks, rendering them, by all means, capable of storing complete songs with arrangements and drum styles.  Each of the 16 parts (tracks) is set to a specific patch, with its own mixer settings (pan, volume, key, effect, routing, and so on). There are a variety of editing modes: The main modes allow realtime recording, step recording and TR-REC recording. In step recording, notes or chords can be added one at a time. In TR-REC mode, each of the 16 pads represents a point along a musical measure. This speeds up the entry of percussion tracks. Patterns can be strung together into \"songs\", which, in fact, are mislabelled, merely being chains of patterns played in a specific order. In fact, there is no recording or sequencing capability in Song mode besides pattern chaining and some playback settings. The sequencer can load Standard Midi Files (albeit with some workaround in order to avoid some loading bugs that have never been fixed) and play them back. Additionally, the sequencer will also include samples stored into its memory in the pattern tracks.",
            "score": 108.73432159423828
        }
    ]
}