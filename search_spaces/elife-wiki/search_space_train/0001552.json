{
    "q": [
        {
            "docid": "39544099_2",
            "document": "Local World Evolving Network Models . Evolving networks are dynamic networks which change in time. In each period t there are new nodes and edges that join the network while the old ones disappear. Such dynamic behaviour is characteristic for most real-world networks, regardless of their range - global or local. However, networks differ not only in their range but also in their topological structure. It is possible to distinguish:  One of the main feature which allows to differentiate networks is their evolution process. In random networks points are added and removed from the network in a totally random way (model of Erd\u0151s and R\u00e9nyi). Evolution of free scale networks is based on the preferential attachment \u2013 nodes connect to nodes that have already possessed a large number of links. In result hubs (nodes that have the largest number of edges) are created and networks follow power law of distribution (model of Barab\u00e1si and Albert's). In opposite, in small world networks there are no hubs, and nodes are rather egalitarian and locally grouped in smaller clusters. These kind of networks are described by Watts and Strogatz (WS) model. All aforementioned models assume that newly added points have a global information about the whole network. However, in case of large systems, such knowledge is rather rare. This strongly limits nodes\u2019 possibilities of connection choice. As a result, decisions about links are made rather in a local world than in the whole network. Networks which consider this locality are called local-world networks and were first described by the Li and Chen model (2003). The local world model was extended inter alia by Garde\u00f1es and Moreno (2004), Sen and Zhong, Wen et al. or Xuan et al.",
            "score": 154.8365217447281
        },
        {
            "docid": "8402086_9",
            "document": "Computational neurogenetic modeling . For the parameters in the gene regulatory network to affect the neurons in the artificial neural network as intended there must be some connection between them. In an organizational context, each node (neuron) in the artificial neural network has its own gene regulatory network associated with it. The weights (and in some networks, frequencies of synaptic transmission to the node), and the resulting membrane potential of the node (including whether an action potential is produced or not), affect the expression of different genes in the gene regulatory network. Factors affecting connections between neurons, such as synaptic plasticity, can be modeled by inputting the values of synaptic activity-associated genes and proteins to a function that re-evaluates the weight of an input from a particular neuron in the artificial neural network.",
            "score": 119.49720621109009
        },
        {
            "docid": "22072718_3",
            "document": "Biological network . Complex biological systems may be represented and analyzed as computable networks. For example, ecosystems can be modeled as networks of interacting species or a protein can be modeled as a network of amino acids. Breaking a protein down farther, amino acids can be represented as a network of connected atoms, such as carbon, nitrogen, and oxygen. Nodes and edges are the basic components of a network. Nodes represent units in the network, while edges represent the interactions between the units. Nodes can represent a wide-array of biological units, from individual organisms to individual neurons in the brain. Two important properties of a network are degree and betweenness centrality. Degree (or connectivity, a distinct usage from that used in graph theory) is the number of edges that connect a node, while betweenness is a measure of how central a node is in a network. Nodes with high betweenness essentially serve as bridges between different portions of the network (i.e. interactions must pass through this node to reach other portions of the network). In social networks, nodes with high degree or high betweenness may play important roles in the overall composition of a network.",
            "score": 167.10728859901428
        },
        {
            "docid": "40409788_31",
            "document": "Convolutional neural network . When dealing with high-dimensional inputs such as images, it is impractical to connect neurons to all neurons in the previous volume because such a network architecture does not take the spatial structure of the data into account. Convolutional networks exploit spatially local correlation by enforcing a local connectivity pattern between neurons of adjacent layers: each neuron is connected to only a small region of the input volume. The extent of this connectivity is a hyperparameter called the receptive field of the neuron. The connections are local in space (along width and height), but always extend along the entire depth of the input volume. Such an architecture ensures that the learnt filters produce the strongest response to a spatially local input pattern.",
            "score": 86.13239336013794
        },
        {
            "docid": "36815492_4",
            "document": "NEST (software) . The neural system is defined by a possibly large number of neurons and their connections. In a NEST network, different neuron and synapse models can coexist. Any two neurons can have multiple connections with different properties. Thus, the connectivity can in general not be described by a weight or connectivity matrix but rather as an adjacency list.",
            "score": 104.73532056808472
        },
        {
            "docid": "1896271_2",
            "document": "Holonomic brain theory . The holonomic brain theory, developed by neuroscientist Karl Pribram initially in collaboration with physicist David Bohm, is a model of human cognition that describes the brain as a holographic storage network. Pribram suggests these processes involve electric oscillations in the brain's fine-fibered dendritic webs, which are different from the more commonly known action potentials involving axons and synapses. These oscillations are waves and create wave interference patterns in which memory is encoded naturally, and the waves may be analyzed by a Fourier transform. Gabor, Pribram and others noted the similarities between these brain processes and the storage of information in a hologram, which can also be analyzed with a Fourier transform. In a hologram, any part of the hologram with sufficient size contains the whole of the stored information. In this theory, a piece of a long-term memory is similarly distributed over a dendritic arbor so that each part of the dendritic network contains all the information stored over the entire network. This model allows for important aspects of human consciousness, including the fast associative memory that allows for connections between different pieces of stored information and the non-locality of memory storage (a specific memory is not stored in a specific location, i.e. a certain neuron).",
            "score": 147.45202016830444
        },
        {
            "docid": "1706303_41",
            "document": "Recurrent neural network . A multiple timescales recurrent neural network (MTRNN) is a neural-based computational model that can simulate the functional hierarchy of the brain through self-organization that depends on spatial connection between neurons and on distinct types of neuron activities, each with distinct time properties. With such varied neuronal activities, continuous sequences of any set of behaviors are segmented into reusable primitives, which in turn are flexibly integrated into diverse sequential behaviors. The biological approval of such a type of hierarchy was discussed in the memory-prediction theory of brain function by Hawkins in his book \"On Intelligence\".",
            "score": 131.15678215026855
        },
        {
            "docid": "33813121_4",
            "document": "Network controllability . It is also notable, that Liu's et al formulation would predict same values of formula_13 for a chain graph and for a weak densely connected graph. Obviously, both these graphs have very different in and out degree distributions. A recent unpublished work, questions whether degree, which is a purely local measure in networks, would completely describe controllability and whether even slightly distant nodes would have no role in deciding network controllability. Indeed, for many real-word networks, namely, food webs, neuronal and metabolic networks, the mismatch in values of formula_14 and formula_15 calculated by Liu et al. is notable. It is obvious that if controllability is decided mainly by degree, why are formula_14 and formula_15 so different for many real world networks? They argued (arXiv:1203.5161v1), that this might be due to the effect of degree correlations. However, it has been shown that network controllability can be altered only by using betweenness centrality and closeness centrality, without using degree (graph theory) or degree correlations at all.",
            "score": 129.60108137130737
        },
        {
            "docid": "39544099_3",
            "document": "Local World Evolving Network Models . The model starts with the set of small number of nodes formula_1 and the small number of edges formula_2. There are M nodes that were selected randomly from the whole global network, so that they constitute a so-called \u201clocal world\u201d for new coming nodes. Thus, every new node with m edges connects only to m existing nodes from its local world and does not link with nodes which are in the global system (the main difference from the BA model). In such case, the probability of connection may be defined as: Where \u00a0formula_4 and the term \"Local-World\" refers to all nodes, which are in interest of newly added node at time t. Thus, it may be rewritten: while the dynamics are: In every time \"t\", it is true that \u00a0formula_7, so that two corner solutions are possible: \u00a0formula_8 and \u00a0formula_9.",
            "score": 105.56838893890381
        },
        {
            "docid": "2860430_16",
            "document": "Neural oscillation . Apart from intrinsic properties of neurons, biological neural network properties are also an important source of oscillatory activity. Neurons communicate with one another via synapses and affect the timing of spike trains in the post-synaptic neurons. Depending on the properties of the connection, such as the coupling strength, time delay and whether coupling is excitatory or inhibitory, the spike trains of the interacting neurons may become synchronized. Neurons are locally connected, forming small clusters that are called neural ensembles. Certain network structures promote oscillatory activity at specific frequencies. For example, neuronal activity generated by two populations of interconnected \"inhibitory\" and \"excitatory\" cells can show spontaneous oscillations that are described by the Wilson-Cowan model.",
            "score": 102.21937775611877
        },
        {
            "docid": "1170097_31",
            "document": "Hopfield network . The Network capacity of the Hopfield network model is determined by neuron amounts and connections within a given network. Therefore, the number of memories that are able to be stored is dependent on neurons and connections. Furthermore, it was shown that the recall accuracy between vectors and nodes was 0.138 (approximately 138 vectors can be recalled from storage for every 1000 nodes) (Hertz et al., 1991). Therefore, it is evident that many mistakes will occur if one tries to store a large number of vectors. When the Hopfield model does not recall the right pattern, it is possible that an intrusion has taken place, since semantically related items tend to confuse the individual, and recollection of the wrong pattern occurs. Therefore, the Hopfield network model is shown to confuse one stored item with that of another upon retrieval. Perfect recalls and high capacity, >0.14, can be loaded in the network by Storkey learning method.",
            "score": 120.2069765329361
        },
        {
            "docid": "40158142_9",
            "document": "Nonlinear system identification . Artificial neural networks try loosely to imitate the network of neurons in the brain where computation takes place through a large number of simple processing elements. A typical neural network consists of a number of simple processing units interconnected to form a complex network. Layers of such units are arranged so that data is entered at the input layer and passes through either one or several intermediate layers before reaching the output layer. In supervised learning the network is trained by operating on the difference between the actual output and the desired output of the network, the prediction error, to change the connection strengths between the nodes. By iterating the weights are modified until the output error reaches an acceptable level. This process is called machine learning because the network adjusts the weights so that the output pattern is reproduced. Neural networks have been extensively studied and there are many excellent textbooks devoted to this topic in general, and more focussed textbooks which emphasise control and systems applications. There are two main problem types that can be studied using neural networks: static problems, and dynamic problems. Static problems include pattern recognition, classification, and approximation. Dynamic problems involve lagged variables and are more appropriate for system identification and related applications. Depending on the architecture of the network the training problem can be either nonlinear-in-the-parameters which involves optimisation or linear-in-the-parameters which can be solved using classical approaches. The training algorithms can be categorised into supervised, unsupervised, or reinforcement learning. Neural networks have excellent approximation properties but these are usually based on standard function approximation results using for example the Weierstrass Theorem that applies equally well to polynomials, rational functions, and other well-known models.  Neural networks have been applied extensively to system identification problems which involve nonlinear and dynamic relationships. However, classical neural networks are purely gross static approximating machines. There is no dynamics within the network. Hence when fitting dynamic models all the dynamics arise by allocating lagged inputs and outputs to the input layer of the network. The training procedure then produces the best static approximation that relates the lagged variables assigned to the input nodes to the output. There are more complex network architectures, including recurrent networks, that produce dynamics by introducing increasing orders of lagged variables to the input nodes. But in these cases it is very easy to over specify the lags and this can lead to over fitting and poor generalisation properties.  Neural networks have several advantages; they are conceptually simple, easy to train and to use, have excellent approximation properties, the concept of local and parallel processing is important and this provides integrity and fault tolerant behaviour. The biggest criticism of the classical neural network models is that the models produced are completely opaque and usually cannot be written down or analysed. It is therefore very difficult to know what is causing what, to analyse the model, or to compute dynamic characteristics from the model. Some of these points will not be relevant to all applications but they are for dynamic modelling.",
            "score": 172.16197752952576
        },
        {
            "docid": "33818014_18",
            "document": "Nervous system network models . There are three types of brain connectivity models of a network (Sporns, O. (2007)). \u201cAnatomical (or structural) connectivity\u201d describes a network with anatomical links having specified relationship between connected \u201cunits.\u201d If the dependent properties are stochastic, it is defined as \u201cfunctional connectivity.\u201d \u201cEffective connectivity\u201d has causal interactions between distinct units in the system. As stated earlier, brain connectivity can be described at three levels. At microlevel, it connects neurons through electrical or chemical synapses. A column of neurons can be considered as a unit in the mesolevel and regions of the brain comprising a large number of neurons and neuron populations as units in the macrolevel. The links in the latter case are the inter-regional pathways, forming large-scale connectivity. Figure 2 shows the three types of connectivity. The analysis is done using the directed graphs (see Sporns, O. (2007) and Hilgetag, C. C. (2002)). In the structural brain connectivity type, the connectivity is a sparse and directed graph. The functional brain connectivity has bidirectional graphs. The effective brain connectivity is bidirectional with interactive cause and effect relationships. Another representation of the connectivity is by matrix representation (See Sporns, O. (2007)). Hilgetag, C. C. (2002) describes the computational analysis of brain connectivity.",
            "score": 118.46095144748688
        },
        {
            "docid": "3474296_4",
            "document": "Neuronal noise . Single neurons demonstrate different responses to specific neuronal input signals. This is commonly referred to as neural response variability. If a specific input signal is initiated in the dendrites of a neuron, then a hypervariability exists in the number of vesicles released from the axon terminal fiber into the synapse. This characteristic is true for fibers without neural input signals, such as pacemaker neurons, as mentioned previously, and cortical pyramidal neurons that have highly-irregular firing pattern. Noise generally hinders neural performance, but recent studies show, in dynamical non-linear neural networks, this statement does not always hold true. Non-linear neural networks are a network of complex neurons that have many connections with one another such as the neuronal systems found within our brains. Comparatively, linear networks are an experimental view of analyzing a neural system by placing neurons in series with each other.",
            "score": 119.68637871742249
        },
        {
            "docid": "39199253_2",
            "document": "Percolation (cognitive psychology) . Percolation (from the Latin word \"percolatio\", meaning filtration) is a theoretical model used to understand the way activation and diffusion of neural activity occur within neural networks. Percolation is a model used to explain how neural activity is transmitted across the various connections within the brain. Often it is easiest to understand percolation theory by explaining its use in epidemiology. Individuals that are infected with a disease can spread the disease through contact with others in their social network. Those who are more social and come into contact with more people will help to propagate the disease quicker than those who are less social. Therefore factors such as occupation and sociability influence the rate of infection. Now, if one were to think of \"neurons\" as the \"individuals\" and \"synaptic connections\" as the \"social bonds\" between people, then one can determine how easily messages between neurons will spread. When a neuron fires, the message is transmitted along all synaptic connections to other neurons until it can no longer continue. Synaptic connections are considered either open or closed (like a social or unsocial person) and messages will flow along any and all open connections until they can go no further. Just like occupation and sociability play a key role in the spread of disease, so too do the number of neurons, synaptic plasticity and long-term potentiation when talking about neural percolation.",
            "score": 100.94449281692505
        },
        {
            "docid": "12142270_3",
            "document": "GENESIS (software) . GENESIS works by creating simulation environments for constructing models of neurons or neural systems. \"Nerve cells are capable of communicating with each other in such a highly structured manner as to form neuronal networks. To understand neural networks, it is necessary to understand the ways in which one neuron communicates with another through synaptic connections and the process called synaptic transmission\". Neurons have a specialized structure for their function, they \"are different from most other cells in the body in that they are polarized and have distinct morphological regions, each with specific functions\". The two important regions of a neuron are the dendrite and the axon. \"Dendrites are the region where one neuron receives connections from other neurons. The cell body or soma contains the nucleus and the other organelles necessary for cellular function. The axon is a key component of nerve cells over which information is transmitted from one part of the neuron (e.g., the cell body) to the terminal regions of the neuron\". The third important piece of a neuron is the synapse. \"The synapse is the terminal region of the axon this is where one neuron forms a connection with another and conveys information through the process of synaptic transmission\".",
            "score": 115.63598299026489
        },
        {
            "docid": "39544099_6",
            "document": "Local World Evolving Network Models . The model is the extension of LM model in a sense that it divides nodes on these which have the information about the global network and on these which does not.  To control for this diversification, parameter formula_16 is introduced. Let formula_16 be the ratio of the number of nodes obtaining the information about the global network to the total number of nodes. Because formula_16 is a ratio, it must be that formula_19. When formula_20 there is no nodes that ow the global information and NLW model comes down to the local-world network model. In turn, formula_21 means that each node possesses the global information about the network, which makes NLW model identical with BA model.  The NWL model starts in the same way as LW \u2013 there is a set of small number of nodes m_0 and the small number of edges formula_2. There are M nodes that were selected randomly from the whole global network and established a \u201clocal world\u201d for new coming nodes. However, in NLW model every new node with m edges can connect to global or local system. The decision depends on received information. If a new node gets information about the whole network, the probability that it will be connected with node i depends on the degree ki of that node, such that: In turn, if the node was not provided in the global information and knows only its local world, it will link only with nodes from this system with the probability: Thus, the general probability in the new local world model may be written as: where formula_16 is the probability that a new node possesses a knowledge about the global network. Similarly to the LW model, the NLW model distinguish three cases of local-world selection: The upper bound case (Case C) is the same as in the local world model.",
            "score": 121.72300231456757
        },
        {
            "docid": "33813121_7",
            "document": "Network controllability . By calculating the maximum matchings of a wide range of real networks, Liu et al. asserted that the number of driver nodes is determined mainly by the networks degree distribution formula_18. They also calculated the average number of driver nodes for a network ensemble with arbitrary degree distribution using the cavity method. It is interesting that for a chain graph and a weak densely connected graph, both of which have very different in and out degree distributions; the formulation of Liu et al. would predict same values of formula_13. Also, for many real-word networks, namely, food webs, neuronal and metabolic networks, the mismatch in values of formula_14 and formula_15 calculated by Liu et al. is notable. If controllability is decided purely by degree, why are formula_14 and formula_15 so different for many real world networks? It remains open to scrutiny whether \"control robustness\" in networks is influenced more by using betweenness centrality and closeness centrality over degree (graph theory) based metrics.",
            "score": 132.23562681674957
        },
        {
            "docid": "17747058_14",
            "document": "Perceptrons (book) . What the book does prove is that in three-layered feed-forward perceptrons (with a so-called \"hidden\" or \"intermediary\" layer), it is not possible to compute some predicates unless at least one of the neurons in the first layer of neurons (the \"intermediary\" layer) is connected with a non-null weight to each and every input. This was contrary to a hope held by some researchers in relying mostly on networks with a few layers of \"local\" neurons, each one connected only to a small number of inputs. A feed-forward machine with \"local\" neurons is much easier to build and use than a larger, fully connected neural network, so researchers at the time concentrated on these instead of on more complicated models.",
            "score": 70.69190549850464
        },
        {
            "docid": "941909_26",
            "document": "Receptive field . The term receptive field is also used in the context of artificial neural networks, most often in relation to convolutional neural networks (CNNs). When used in this sense, the term adopts a meaning reminiscent of receptive fields in actual biological nervous systems. CNNs have a distinct architecture, designed to mimic the way in which real animal brains are understood to function; instead of having every neuron in each layer connect to all neurons in the next layer (Multilayer perceptron), the neurons are arranged in a 3-dimensional structure in such a way as to take into account the spatial relationships between different neurons with respect to the original data. Since CNNs are used primarily in the field of computer vision, the data that the neurons represent is typically an image; each input neuron represents one pixel from the original image. The first layer of neurons is composed of all the input neurons; neurons in the next layer will receive connections from some of the input neurons (pixels), but not all, as would be the case in a MLP and in other traditional neural networks. Hence, instead of having each neuron receive connections from all neurons in the previous layer, CNNs use a receptive field-like layout in which each neuron receives connections only from a subset of neurons in the previous (lower) layer. The receptive field of a neuron in one of the lower layers encompasses only a small area of the image, while the receptive field of a neuron in subsequent (higher) layers involves a combination of receptive fields from several (but not all) neurons in the layer before (i. e. a neuron in a higher layer \"looks\" at a larger portion of the image than does a neuron in a lower layer). In this way, each successive layer is capable of learning increasingly abstract features of the original image. The use of receptive fields in this fashion is thought to give CNNs an advantage in recognizing visual patterns when compared to other types of neural networks.",
            "score": 110.14972496032715
        },
        {
            "docid": "1729542_2",
            "document": "Neural network . The term neural network was traditionally used to refer to a network or circuit of neurons. The modern usage of the term often refers to artificial neural networks, which are composed of artificial neurons or nodes. Thus the term may refer to either biological neural networks, made up of real biological neurons, or artificial neural networks, for solving artificial intelligence (AI) problems.The connections of the biological neuron are modeled as weights. A positive weight reflects an excitatory connection, while negative values mean inhibitory connections. All inputs are modified by a weight and summed. This activity is referred as a linear combination. Finally, an activation function controls the amplitude of the output. For example, an acceptable range of output is usually between 0 and 1, or it could be -1 and 1.",
            "score": 112.79774570465088
        },
        {
            "docid": "44433870_12",
            "document": "Quantum complex network . The quantum random network model proposed by Perseguers et al. can be thought of as a quantum version of the Erd\u0151s\u2013R\u00e9nyi model. Instead of the typical links used in to represent other complex networks, in the quantum random network model each pair of nodes is connected through a pair of entangled qubits. In this case each node containsformula_15 quibits, one for each other node. In a quantum random network, the degree of entanglement between two pairs of nodes, represented by formula_16, plays a similar role to the parameter formula_16 in the Erd\u0151s\u2013R\u00e9nyi model. While in the Erd\u0151s\u2013R\u00e9nyi model two nodes form a connection with probability formula_16, in the context of quantum random networks formula_16 means the probability of an entangled pair of qubits being successful converted to a maximally entangled state using only local operations and classical communications, called LOCC operations. We can think of maximally entangled qubits as the true links between nodes.",
            "score": 136.01976132392883
        },
        {
            "docid": "21523_3",
            "document": "Artificial neural network . An ANN is based on a collection of connected units or nodes called artificial neurons which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it.",
            "score": 129.02281880378723
        },
        {
            "docid": "2506529_34",
            "document": "Cellular neural network . CNN processors are neuromorphic processors, meaning that they emulate certain aspects of biological neural networks. The original CNN processors were based on mammalian retinas, which consist of a layer of photodetectors connected to several layers of locally coupled neurons. This makes CNN processors part of an interdisciplinary research area whose goal is to design systems that leverage knowledge and ideas from neuroscience and contribute back via real-world validation of theories. CNN processors have implemented a real-time system that replicates mammalian retinas, validating that the original CNN architecture chosen modeled the correct aspects of the biological neural networks used to perform the task in mammalian life. However, CNN processors are not limited to verifying biological neural networks associated with vision processing; they have been used to simulate dynamic activity seen in mammalian neural networks found in the olfactory bulb and locust antennal lobe, responsible for pre-processing sensory information to detect differences in repeating patterns.",
            "score": 129.83456349372864
        },
        {
            "docid": "10974486_13",
            "document": "Storage (memory) . Anderson shows that combination of Hebbian learning rule and McCullough\u2013Pitts dynamical rule allow network to generate a weight matrix that can store associations between different memory patterns \u2013 such matrix is the form of memory storage for the neural network model. Major differences between the matrix of multiple traces hypothesis and the neural network model is that while new memory indicates extension of the existing matrix for the multiple traces hypothesis, weight matrix of the neural network model does not extend; rather, the weight is said to be updated with introduction of new association between neurons.",
            "score": 92.66810965538025
        },
        {
            "docid": "2860430_25",
            "document": "Neural oscillation . The Kuramoto model of coupled phase oscillators is one of the most abstract and fundamental models used to investigate neural oscillations and synchronization. It captures the activity of a local system (e.g., a single neuron or neural ensemble) by its circular phase alone and hence ignores the amplitude of oscillations (amplitude is constant). Interactions amongst these oscillators are introduced by a simple algebraic form (such as a sine function) and collectively generate a dynamical pattern at the global scale. The Kuramoto model is widely used to study oscillatory brain activity and several extensions have been proposed that increase its neurobiological plausibility, for instance by incorporating topological properties of local cortical connectivity. In particular, it describes how the activity of a group of interacting neurons can become synchronized and generate large-scale oscillations. Simulations using the Kuramoto model with realistic long-range cortical connectivity and time-delayed interactions reveal the emergence of slow patterned fluctuations that reproduce resting-state BOLD functional maps, which can be measured using fMRI.",
            "score": 84.38031089305878
        },
        {
            "docid": "33826069_3",
            "document": "Viral neuronal tracing . Most neuroanatomists would agree that understanding how the brain is connected to itself and the body is of paramount importance. As such, it is of equal importance to have a way to visualize and study the connections among neurons. Neuronal tracing methods offer an unprecedented view into the morphology and connectivity of neural networks. Depending on the tracer used, this can be limited to a single neuron or can progress trans-synaptically to adjacent neurons. After the tracer has spread sufficiently, the extent may be measured either by fluorescence (for dyes) or by immunohistochemistry (for biological tracers). An important innovation in this field is the use of neurotropic viruses as tracers. These not only spread throughout the initial site of infection, but can jump across synapses. The use of a virus provides a self-replicating tracer. This can allow for the elucidation of neural microcircuitry to an extent that was previously unobtainable.  This has significant implications for the real world. If we can better understand what parts of the brain are intimately connected, we can predict the effect of localized brain injury. For example, if a patient has a stroke in the amygdala, primarily responsible for emotion, the patient might also have trouble learning to perform certain tasks because the amygdala is highly interconnected with the orbitofrontal cortex, responsible for reward learning. As always, the first step to solving a problem is fully understanding it, so if we are to have any hope of fixing brain injury, we must first understand its extent and complexity.",
            "score": 104.24733829498291
        },
        {
            "docid": "44433870_18",
            "document": "Quantum complex network . The goal of entanglement percolation models is to determine if a quantum network is capable of establishing a connection between two arbitrary nodes through entanglement, and to find best the strategies to create those same connections. In a model proposed by Cirac et al. and applied to complex networks by Cuquet et al., nodes are distributed in a lattice, or in a complex network, and each pair of neighbors share two pairs of entangled qubits that can be converted to a maximally entangle qubit pair with probability formula_16. We can think of maximally entangled qubits as the true links between nodes. According to classic percolation theory, considering a probability formula_16 of two neighbors being connected, there is a critical formula_16 designed by formula_37, so that if formula_38 there is a finite probability of existing a path between two random selected node, and for formula_39 the probability of existing a path between two random selected nodes goes to zero. formula_37 depends only on the topology of the network. A similar phenomena was found in the model proposed by Cirac et al., where the probability of forming a maximally entangled state between two random selected nodes is zero if formula_39 and finite if formula_38. The main difference between classic and entangled percolation is that in quantum networks it is possible to change the links in the network, in a way changing the effective topology of the network, as a consequence formula_37 will depend on the strategy used to convert partial entangle qubits to maximally connected qubits. A naive approach yields that formula_37 for a quantum network is equal to formula_37 for a classic network with the same topology. Nevertheless, it was shown that is possible to take advantage of quantum swapping to lower that value, both in regular lattices and complex networks.",
            "score": 137.77168655395508
        },
        {
            "docid": "1648765_11",
            "document": "Random matrix . In the field of theoretical neuroscience, random matrices are increasingly used to model the network of synaptic connections between neurons in the brain. Dynamical models of neuronal networks with random connectivity matrix were shown to exhibit a phase transition to chaos when the variance of the synaptic weights crosses a critical value, at the limit of infinite system size. Relating the statistical properties of the spectrum of biologically inspired random matrix models to the dynamical behavior of randomly connected neural networks is an intensive research topic.",
            "score": 104.65430212020874
        },
        {
            "docid": "41413_3",
            "document": "Network topology . Network topology is the topological structure of a network and may be depicted physically or logically. It is an application of graph theory wherein communicating devices are modeled as nodes and the connections between the devices are modeled as links or lines between the nodes. \"Physical topology\" is the placement of the various components of a network (e.g., device location and cable installation), while \"logical topology\" illustrates how data flows within a network. Distances between nodes, physical interconnections, transmission rates, or signal types may differ between two different networks, yet their topologies may be identical. A network\u2019s physical topology is a particular concern of the physical layer of the OSI model. Network topology can be used to define or describe the arrangement of various types of telecommunication  networks, including command and control radio networks, industrial fieldbusses, and computer networks.",
            "score": 124.17328667640686
        },
        {
            "docid": "33818014_2",
            "document": "Nervous system network models . Network of human nervous system comprises nodes (for example, neurons) that are connected by links (for example, synapses). The connectivity may be viewed anatomically, functionally, or electrophysiologically. These are presented in several Wikipedia articles that include Connectionism (a.k.a. Parallel Distributed Processing (PDP)), Biological neural network, Artificial neural network (a.k.a. Neural network), Computational neuroscience, as well as in several books by Ascoli, G. A. (2002), Sterratt, D., Graham, B., Gillies, A., & Willshaw, D. (2011), Gerstner, W., & Kistler, W. (2002), and Rumelhart, J. L., McClelland, J. L., and PDP Research Group (1986) among others. The focus of this article is a comprehensive view of modeling a neural network (technically neuronal network based on neuron model). Once an approach based on the perspective and connectivity is chosen, the models are developed at microscopic (ion and neuron), mesoscopic (functional or population), or macroscopic (system) levels. Computational modeling refers to models that are developed using computing tools.",
            "score": 100.089524269104
        },
        {
            "docid": "2860430_17",
            "document": "Neural oscillation . If a group of neurons engages in synchronized oscillatory activity, the neural ensemble can be mathematically represented as a single oscillator. Different neural ensembles are coupled through long-range connections and form a network of weakly coupled oscillators at the next spatial scale. Weakly coupled oscillators can generate a range of dynamics including oscillatory activity. Long-range connections between different brain structures, such as the thalamus and the cortex (see thalamocortical oscillation), involve time-delays due to the finite conduction velocity of axons. Because most connections are reciprocal, they form feed-back loops that support oscillatory activity. Oscillations recorded from multiple cortical areas can become synchronized to form large scale brain networks, whose dynamics and functional connectivity can be studied by means of spectral analysis and Granger causality measures. Coherent activity of large-scale brain activity may form dynamic links between brain areas required for the integration of distributed information.",
            "score": 128.66398549079895
        }
    ],
    "r": [
        {
            "docid": "40158142_9",
            "document": "Nonlinear system identification . Artificial neural networks try loosely to imitate the network of neurons in the brain where computation takes place through a large number of simple processing elements. A typical neural network consists of a number of simple processing units interconnected to form a complex network. Layers of such units are arranged so that data is entered at the input layer and passes through either one or several intermediate layers before reaching the output layer. In supervised learning the network is trained by operating on the difference between the actual output and the desired output of the network, the prediction error, to change the connection strengths between the nodes. By iterating the weights are modified until the output error reaches an acceptable level. This process is called machine learning because the network adjusts the weights so that the output pattern is reproduced. Neural networks have been extensively studied and there are many excellent textbooks devoted to this topic in general, and more focussed textbooks which emphasise control and systems applications. There are two main problem types that can be studied using neural networks: static problems, and dynamic problems. Static problems include pattern recognition, classification, and approximation. Dynamic problems involve lagged variables and are more appropriate for system identification and related applications. Depending on the architecture of the network the training problem can be either nonlinear-in-the-parameters which involves optimisation or linear-in-the-parameters which can be solved using classical approaches. The training algorithms can be categorised into supervised, unsupervised, or reinforcement learning. Neural networks have excellent approximation properties but these are usually based on standard function approximation results using for example the Weierstrass Theorem that applies equally well to polynomials, rational functions, and other well-known models.  Neural networks have been applied extensively to system identification problems which involve nonlinear and dynamic relationships. However, classical neural networks are purely gross static approximating machines. There is no dynamics within the network. Hence when fitting dynamic models all the dynamics arise by allocating lagged inputs and outputs to the input layer of the network. The training procedure then produces the best static approximation that relates the lagged variables assigned to the input nodes to the output. There are more complex network architectures, including recurrent networks, that produce dynamics by introducing increasing orders of lagged variables to the input nodes. But in these cases it is very easy to over specify the lags and this can lead to over fitting and poor generalisation properties.  Neural networks have several advantages; they are conceptually simple, easy to train and to use, have excellent approximation properties, the concept of local and parallel processing is important and this provides integrity and fault tolerant behaviour. The biggest criticism of the classical neural network models is that the models produced are completely opaque and usually cannot be written down or analysed. It is therefore very difficult to know what is causing what, to analyse the model, or to compute dynamic characteristics from the model. Some of these points will not be relevant to all applications but they are for dynamic modelling.",
            "score": 172.16197204589844
        },
        {
            "docid": "22072718_3",
            "document": "Biological network . Complex biological systems may be represented and analyzed as computable networks. For example, ecosystems can be modeled as networks of interacting species or a protein can be modeled as a network of amino acids. Breaking a protein down farther, amino acids can be represented as a network of connected atoms, such as carbon, nitrogen, and oxygen. Nodes and edges are the basic components of a network. Nodes represent units in the network, while edges represent the interactions between the units. Nodes can represent a wide-array of biological units, from individual organisms to individual neurons in the brain. Two important properties of a network are degree and betweenness centrality. Degree (or connectivity, a distinct usage from that used in graph theory) is the number of edges that connect a node, while betweenness is a measure of how central a node is in a network. Nodes with high betweenness essentially serve as bridges between different portions of the network (i.e. interactions must pass through this node to reach other portions of the network). In social networks, nodes with high degree or high betweenness may play important roles in the overall composition of a network.",
            "score": 167.10728454589844
        },
        {
            "docid": "47511015_2",
            "document": "Large scale brain networks . Large scale brain networks are collections of widespread brain regions showing functional connectivity by statistical analysis of the fMRI BOLD signal or other signal fluctuations. An emerging paradigm in neuroscience is that cognitive tasks are performed not by individual brain regions working in isolation, but by networks consisting of several discrete brain regions that are said to be \"functionally connected\" due to tightly coupled activity. Functional connectivity may be measured as long-range synchronization of the EEG, MEG, or other dynamic brain signals. Synchronized brain regions may also be identified using spatial independent component analysis. The set of identified brain areas that are linked together in a large-scale network varies with cognitive function. When the cognitive state is not explicit (i.e., the subject is at \"rest\"), the large scale brain network is a resting state network (RSN). As a physical system with graph-like properties, a large scale brain network has both nodes and edges, and cannot be identified simply by the co-activation of brain areas. In recent decades, the analysis of brain networks was made feasible by advances in imaging techniques as well as new tools from graph theory and dynamical systems. Large scale brain networks are identified by their function, and provide a coherent framework for understanding cognition by offering a neural model of how different cognitive functions emerge when different sets of brain regions join together as self-organized coalitions. Disruptions in activity in various networks have been implicated in neuropsychiatric disorders such as depression, Alzheimer's, autism spectrum disorder, schizophrenia and bipolar disorder.",
            "score": 158.18809509277344
        },
        {
            "docid": "39544099_2",
            "document": "Local World Evolving Network Models . Evolving networks are dynamic networks which change in time. In each period t there are new nodes and edges that join the network while the old ones disappear. Such dynamic behaviour is characteristic for most real-world networks, regardless of their range - global or local. However, networks differ not only in their range but also in their topological structure. It is possible to distinguish:  One of the main feature which allows to differentiate networks is their evolution process. In random networks points are added and removed from the network in a totally random way (model of Erd\u0151s and R\u00e9nyi). Evolution of free scale networks is based on the preferential attachment \u2013 nodes connect to nodes that have already possessed a large number of links. In result hubs (nodes that have the largest number of edges) are created and networks follow power law of distribution (model of Barab\u00e1si and Albert's). In opposite, in small world networks there are no hubs, and nodes are rather egalitarian and locally grouped in smaller clusters. These kind of networks are described by Watts and Strogatz (WS) model. All aforementioned models assume that newly added points have a global information about the whole network. However, in case of large systems, such knowledge is rather rare. This strongly limits nodes\u2019 possibilities of connection choice. As a result, decisions about links are made rather in a local world than in the whole network. Networks which consider this locality are called local-world networks and were first described by the Li and Chen model (2003). The local world model was extended inter alia by Garde\u00f1es and Moreno (2004), Sen and Zhong, Wen et al. or Xuan et al.",
            "score": 154.83651733398438
        },
        {
            "docid": "18396728_3",
            "document": "Modularity (networks) . Many scientifically important problems can be represented and empirically studied using networks. For example, biological and social patterns, the World Wide Web, metabolic networks, food webs, neural networks and pathological networks are real world problems that can be mathematically represented and topologically studied to reveal some unexpected structural features. Most of these networks possess a certain community structure that has substantial importance in building an understanding regarding the dynamics of the network. For instance, a closely connected social community will imply a faster rate of transmission of information or rumor among them than a loosely connected community. Thus, if a network is represented by a number of individual nodes connected by links which signify a certain degree of interaction between the nodes, communities are defined as groups of densely interconnected nodes that are only sparsely connected with the rest of the network. Hence, it may be imperative to identify the communities in networks since the communities may have quite different properties such as node degree, clustering coefficient, betweenness, centrality. etc., from that of the average network. Modularity is one such measure, which when maximized, leads to the appearance of communities in a given network.",
            "score": 154.09698486328125
        },
        {
            "docid": "2208074_11",
            "document": "Neurophilosophy . The brain regions of interest are somewhat constrained by the size of the voxels. Rs-fcMRI uses voxels that are few millimeters cubed so the brain regions will have to be defined on a larger scale. Two of the statistical methods that are commonly applied to network analysis can work on the single voxel spatial scale, but graph theory methods are extremely sensitive to the way nodes are defined. Brains regions can be divided according to their cellular architectural, according to their connectivity, or according to physiological measures. Alternatively, you could take a theory neutral approach and randomly divide the cortex into partitions of the size of your choosing. As mentioned earlier, there are several approaches to network analysis once the your brain regions have been defined. Seed based analysis begins with an a priori defined seed region and finds all of the regions that are functionally connected to that region. Wig et al. caution that the resulting network structure will not give any information concerning the inter-connectivity of the identified regions or the relations of those regions to regions other than the seed region. Another approach is to use independent component analysis to create spatio-temporal component maps and the components are sorted by components that carry information of interest and those that are caused by noise. Wigs et al. once again warns that inference of functional brain region communities is difficult under ICA. ICA also has the issue of imposing orthogonality on the data. Graph theory uses a matrix to characterize covariance between regions which is then transformed into a network map. The problem with graph theory analysis is that network mapping is heavily influenced by a priori brain region and connectivity (nodes and edges), thus the researcher is at risk for cherry picking regions and connections according to their own theories. However, graph theory analysis is extremely valuable since it is the only method that gives pair-wise relationships between nodes. ICA has the added advantage of being a fairly principled method. It seems that using both methods will be important in uncovering the network connectivity of the brain. Mumford et al. hoped to avoid these issues and use a principled approach that could determine pair-wise relationships using a statistical technique adopted from analysis of gene co-expression networks.",
            "score": 152.0667724609375
        },
        {
            "docid": "37689507_19",
            "document": "Neuroimaging intelligence testing . A 2012 study from Washington University, St. Louis described the global connectivity of the prefrontal cortex. Global connectivity is the mechanism by which components of the frontoparietal brain network might coordinate control of other tasks. Cole et al. wrote that: \"A lateral prefrontal cortex (LPFC) region's activity was found to predict performance in a high control demand working memory task and also to exhibit high global connectivity. Critically, global connectivity in this LPFC region, involving connections both within and outside the frontoparietal network, showed a highly selective relationship with individual differences in fluid intelligence.\" The lateral prefrontal cortex is a region of interest because those who have injuries to that part of the brain often have issues with common, every day tasks such as planning their day. The LPFC is thought to be important for \"cognitive control capacity,\" which can be used to predict future outcomes such as success in school and the workplace. It was found by van den Heuvel et al. that higher intelligence individuals employ more efficient whole-brain network organization. This had led to the thought that cognitive control capacity may be supported by these whole-brain network properties. The 2012 study used a theoretic approach to neuroimage data known as global brain connectivity (GBC) or weighted degree centrality. GBC let the researches look closely at specific regions and their range of connectivity. It was then possible to examine each region's role in human cognitive control and intelligence. The study used fMRI to acquire data and examine each region's connectivity.",
            "score": 151.64639282226562
        },
        {
            "docid": "1896271_2",
            "document": "Holonomic brain theory . The holonomic brain theory, developed by neuroscientist Karl Pribram initially in collaboration with physicist David Bohm, is a model of human cognition that describes the brain as a holographic storage network. Pribram suggests these processes involve electric oscillations in the brain's fine-fibered dendritic webs, which are different from the more commonly known action potentials involving axons and synapses. These oscillations are waves and create wave interference patterns in which memory is encoded naturally, and the waves may be analyzed by a Fourier transform. Gabor, Pribram and others noted the similarities between these brain processes and the storage of information in a hologram, which can also be analyzed with a Fourier transform. In a hologram, any part of the hologram with sufficient size contains the whole of the stored information. In this theory, a piece of a long-term memory is similarly distributed over a dendritic arbor so that each part of the dendritic network contains all the information stored over the entire network. This model allows for important aspects of human consciousness, including the fast associative memory that allows for connections between different pieces of stored information and the non-locality of memory storage (a specific memory is not stored in a specific location, i.e. a certain neuron).",
            "score": 147.4520263671875
        },
        {
            "docid": "1648224_17",
            "document": "Granger causality . A long held belief about neural function maintained that different areas of the brain were task specific; that the structural connectivity local to a certain area somehow dictated the function of that piece. Collecting work that has been performed over many years, there has been a move to a different, network-centric approach to describing information flow in the brain. Explanation of function is beginning to include the concept of networks existing at different levels and throughout different locations in the brain. The behavior of these networks can be described by non-deterministic processes that are evolving through time. That is to say that given the same input stimulus, you will not get the same output from the network. The dynamics of these networks are governed by probabilities so we treat them as stochastic (random) processes so that we can capture these kinds of dynamics between different areas of the brain.",
            "score": 145.8390655517578
        },
        {
            "docid": "2208074_10",
            "document": "Neurophilosophy . Recently, researchers have begun using a new functional imaging technique called resting state functional connectivity MRI. Subjects' brains are scanned while the subject sits idly in the scanner. By looking at the natural fluctuations in the bold pattern while the subject is at rest, the researchers can see which brain regions co-vary in activation together. They can use the patterns of covariance to construct maps of functionally linked brain areas. It is worth noting that the name \"functional connectivity\" is somewhat misleading since the data only indicates co-variation. Still, this is a powerful method for studying large networks throughout the brain. There are a couple of important methodological issues that need to be addressed. Firstly, there are many different possible brain mappings that could be used to define the brain regions for the network. The results could vary significantly depending on the brain region chosen. Secondly, what mathematical techniques are best about to characterize these brain regions?",
            "score": 144.6294708251953
        },
        {
            "docid": "7347241_2",
            "document": "Spreading activation . Spreading activation is a method for searching associative networks, biological and artificial neural networks, or semantic networks. The search process is initiated by labeling a set of source nodes (e.g. concepts in a semantic network) with weights or \"activation\" and then iteratively propagating or \"spreading\" that activation out to other nodes linked to the source nodes. Most often these \"weights\" are real values that decay as activation propagates through the network. When the weights are discrete this process is often referred to as marker passing. Activation may originate from alternate paths, identified by distinct markers, and terminate when two alternate paths reach the same node. However brain studies show that several different brain areas play an important role in semantic processing.",
            "score": 142.73483276367188
        },
        {
            "docid": "39115998_11",
            "document": "Interdependent networks . A typical cascading failure in a system of interdependent networks can be described as follows: We take two networks formula_5 and formula_6 with formula_7 nodes and a given topology . Each node formula_8 in formula_5 relies on a critical resource provided by a node formula_10 in formula_6 and vice versa. If formula_8 stops functioning, formula_10 will also stop functioning and vice versa. The failure is triggered by the removal of a fraction formula_1 of nodes from formula_5 along with the links in formula_5 which were attached to each of those nodes. Since every node in formula_6 depends on a node in formula_5, this causes the removal of the same fraction formula_1 of nodes in formula_6. In network theory, we assume that only nodes which are a part of the largest connected component can continue to function. Since the arrangement of links in formula_5 and formula_6 are different, they fragment into different sets of connected components. The smaller components in formula_5 cease to function and when they do, they cause the same number of nodes (but in different locations) in formula_6 to cease to function as well. This process continues iteratively between the two networks until no more nodes are removed. This process leads to a percolation phase transition at a value formula_3 which is substantially larger than the value obtained for a single network.",
            "score": 141.0945281982422
        },
        {
            "docid": "32168948_6",
            "document": "Sepp Hochreiter . Neural networks are different types of simplified mathematical models of biological neural networks like those in human brains.  In feedforward neural networks (NNs) the information moves forward in only one direction,  from the input layer that receives information from the environment,  through the hidden layers to the output layer that supplies the information to the environment. Unlike NNs, recurrent neural networks (RNNs)  can use their internal memory to process arbitrary sequences of inputs.  If data mining is based on neural networks, overfitting reduces the network's capability to correctly process future data. To avoid overfitting, Sepp Hochreiter developed algorithms for finding low complexity neural networks like \"Flat Minimum Search\" (FMS), which searches for a \"flat\" minimum \u2014 a large connected region in the parameter space where the network function is constant. Thus, the network parameters can be given with low precision which means a low complex network that avoids overfitting. Low complexity neural networks are well suited for deep learning because they control the complexity in each network layer and, therefore, learn hierarchical representations of the input. Sepp Hochreiter's group introduced \"exponential linear units\" (ELUs) which speed up learning in deep neural networks and lead to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs), and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However, ELUs have improved learning characteristics compared to ReLUs, due to negative values which push mean unit activations closer to zero. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect.",
            "score": 141.0709228515625
        },
        {
            "docid": "2640086_24",
            "document": "Affective neuroscience . This approach to emotion hypothesizes that emotions like happiness, sadness, fear, anger and disgust (and many others) are constructed mental states that occur when many different systems in the brain work together. In this view, networks of brain regions underlie psychological operations (e.g., language, attention, etc.) that interact to produce many different kinds of emotion, perception, and cognition. One psychological operation critical for emotion is the network of brain regions that underlie valence (feeling pleasant/unpleasant) and arousal (feeling activated and energized). Emotions emerge when neural systems underlying different psychological operations interact (not just those involved in valence and arousal), producing distributed patterns of activation across the brain. Because emotions emerge from more basic components, there is heterogeneity within each emotion category; for example, a person can experience many different kinds of fear, which feel differently, and which correspond to different neural patterns in the brain. Thus, this view presents a different approach to understanding the neural bases of emotion than locationist approaches.",
            "score": 140.2603302001953
        },
        {
            "docid": "39182554_20",
            "document": "Catastrophic interference . French (1997) proposed the idea of a pseudo-recurrent backpropagation network in order to help reduce catastrophic interference (see Figure 2). In this model the network is separated into two functionally distinct but interacting sub-networks. This model is biologically inspired and is based on research from McClelland, McNaughton, and O'Reilly (1995). In this research McClelland et al. (1995), suggested that the hippocampus and neocortex act as separable but complementary memory systems. Specifically, the hippocampus short term memory storage and acts gradually over time to transfer memories into the neocortex for long term memory storage. They suggest that the information that is stored can be \"brought back\" to the hippocampus during active rehearsal, reminiscence, and sleep and renewed activation is what acts to transfer the information to the neocortex over time. In the pseudo-recurrent network, one of the sub-networks acts as an early processing area, akin to the hippocampus, and functions to learn new input patters. The other sub-network acts as a final-storage area, akin to the neocortex. However, unlike in McClelland et al. (1995) model, the final-storage area sends internally generated representation back to the early processing area. This creates a recurrent network. French proposed that this interleaving of old representations with new representations is the only way to reduce radical forgetting. Since the brain would most likely not have access to the original input patterns, the patterns that would be fed back to the neocortex would be internally generated representations called \"pseudopatterns\". These pseudopatterns are approximations of previous inputs and they can be interleaved with the learning of new inputs. The use of these pseudopatterns could be biologically plausible as parallels between the consolidation of learning that occurs during sleep and the use of interleaved pseudopatterns. Specifically, they both serve to integrate new information with old information without disruption of the old information. When given an input (and a teacher value) is fed into the pseudo-recurrent network would act as follows:",
            "score": 139.21258544921875
        },
        {
            "docid": "17364396_3",
            "document": "Connectivism . The central aspect of connectivism is the metaphor of a network with nodes and connections. In this metaphor, a node is anything that can be connected to another node such as an organization, information, data, feelings, and images. Connectivism recognizes three node types: neural, conceptual (internal) and external. Connectivism sees learning as the process of creating connections and expanding or increasing network complexity. Connections may have different directions and strength. In this sense, a connection joining nodes A and B which goes from A to B is not the same as one that goes from B to A. There are some special kinds of connections such as \"self-join\" and pattern. A self-join connection joins a node to itself and a pattern can be defined as \"a set of connections appearing together as a single whole\".",
            "score": 138.27452087402344
        },
        {
            "docid": "44433870_18",
            "document": "Quantum complex network . The goal of entanglement percolation models is to determine if a quantum network is capable of establishing a connection between two arbitrary nodes through entanglement, and to find best the strategies to create those same connections. In a model proposed by Cirac et al. and applied to complex networks by Cuquet et al., nodes are distributed in a lattice, or in a complex network, and each pair of neighbors share two pairs of entangled qubits that can be converted to a maximally entangle qubit pair with probability formula_16. We can think of maximally entangled qubits as the true links between nodes. According to classic percolation theory, considering a probability formula_16 of two neighbors being connected, there is a critical formula_16 designed by formula_37, so that if formula_38 there is a finite probability of existing a path between two random selected node, and for formula_39 the probability of existing a path between two random selected nodes goes to zero. formula_37 depends only on the topology of the network. A similar phenomena was found in the model proposed by Cirac et al., where the probability of forming a maximally entangled state between two random selected nodes is zero if formula_39 and finite if formula_38. The main difference between classic and entangled percolation is that in quantum networks it is possible to change the links in the network, in a way changing the effective topology of the network, as a consequence formula_37 will depend on the strategy used to convert partial entangle qubits to maximally connected qubits. A naive approach yields that formula_37 for a quantum network is equal to formula_37 for a classic network with the same topology. Nevertheless, it was shown that is possible to take advantage of quantum swapping to lower that value, both in regular lattices and complex networks.",
            "score": 137.77169799804688
        },
        {
            "docid": "2965167_14",
            "document": "NINDS brain trauma research . The plasticity of the brain and the rewiring of neural connections make it possible for one part of the brain to take up the functions of a disabled part. Scientists have long known that the immature brain is generally more plastic than the mature brain, and that the brains of children are better able to adapt and recover from injury than the brains of adults. NINDS researchers are investigating the mechanisms underlying this difference and theorize that children have an overabundance of hard-wired neural networks, many of which naturally decrease through a process called neural pruning. When an injury destroys an important neural network in children, another less useful neural network that would have eventually died takes over the responsibilities of the damaged network. Some researchers are looking at the role of plasticity in memory, while others are using imaging technologies, such as functional MRI, to map regions of the brain and record evidence of plasticity.",
            "score": 136.63209533691406
        },
        {
            "docid": "356382_11",
            "document": "Gene regulatory network . There are primarily two ways that networks can evolve, both of which can occur simultaneously. The first is that network topology can be changed by the addition or subtraction of nodes (genes) or parts of the network (modules) may be expressed in different contexts. The\" Drosophila\" Hippo signaling pathway provides a good example. The Hippo signaling pathway controls both mitotic growth and post-mitotic cellular differentiation. Recently it was found that the network the Hippo signaling pathway operates in differs between these two functions which in turn changes the behavior of the Hippo signaling pathway. This suggests that the Hippo signaling pathway operates as a conserved regulatory module that can be used for multiple functions depending on context. Thus, changing network topology can allow a conserved module to serve multiple functions and alter the final output of the network. The second way networks can evolve is by changing the strength of interactions between nodes, such as how strongly a transcription factor may bind to a cis-regulatory element. Such variation in strength of network edges has been shown to underlie between species variation in vulva cell fate patterning of \"Caenorhabditis\" worms.",
            "score": 136.3671112060547
        },
        {
            "docid": "179092_7",
            "document": "Neurolinguistics . Much work in neurolinguistics involves testing and evaluating theories put forth by psycholinguists and theoretical linguists. In general, theoretical linguists propose models to explain the structure of language and how language information is organized, psycholinguists propose models and algorithms to explain how language information is processed in the mind, and neurolinguists analyze brain activity to infer how biological structures (populations and networks of neurons) carry out those psycholinguistic processing algorithms. For example, experiments in sentence processing have used the ELAN, N400, and P600 brain responses to examine how physiological brain responses reflect the different predictions of sentence processing models put forth by psycholinguists, such as Janet Fodor and Lyn Frazier's \"serial\" model, and Theo Vosse and Gerard Kempen's \"unification model\". Neurolinguists can also make new predictions about the structure and organization of language based on insights about the physiology of the brain, by \"generalizing from the knowledge of neurological structures to language structure\".",
            "score": 136.30690002441406
        },
        {
            "docid": "44433870_12",
            "document": "Quantum complex network . The quantum random network model proposed by Perseguers et al. can be thought of as a quantum version of the Erd\u0151s\u2013R\u00e9nyi model. Instead of the typical links used in to represent other complex networks, in the quantum random network model each pair of nodes is connected through a pair of entangled qubits. In this case each node containsformula_15 quibits, one for each other node. In a quantum random network, the degree of entanglement between two pairs of nodes, represented by formula_16, plays a similar role to the parameter formula_16 in the Erd\u0151s\u2013R\u00e9nyi model. While in the Erd\u0151s\u2013R\u00e9nyi model two nodes form a connection with probability formula_16, in the context of quantum random networks formula_16 means the probability of an entangled pair of qubits being successful converted to a maximally entangled state using only local operations and classical communications, called LOCC operations. We can think of maximally entangled qubits as the true links between nodes.",
            "score": 136.01976013183594
        },
        {
            "docid": "35591871_26",
            "document": "Networks in marketing . \u2018Hubs\u2019 or \u2018connectors\u2019 are important aspects to analyze when examining any system or network and marketing is no exception to this. A hub in a network is a consequence of a Power Law, whereby a small number of nodes or actors in a network have a disproportionately large number of links to other nodes in the network. A Power Law in a market system for instance could be explained in that there are many actors who have a very limited number of business contacts in their 'rolodex', but there are a small few, say 10% of those in the network, who have a huge number of network contacts in their 'rolodex' and can easily facilitate communication between two separated actors. The idea that in a network the majority of nodes will have a couple of links which can allow communication through many pathways, whereas hubs have a large number of links and ensure that a network has full contact and eases the complexity of this. One of the first empirical observations of hubs or connectors in social sciences experimentation came about in Stanley Milgram\u2019s Small world experiments, the first of which taking place in 1967. Of the 64 letters which made it through to the stated destination, 60% of those went through the same four people, and in a further experiment of a similar nature when 24 letters got through, 16 of those went through the same last person as a connection. Hubs are distinctive components of network systems and through understanding them a better understanding of network function and behavior can be attained. Barab\u00e1si states that hubs dominate all networks through the large number of links they possess, that the links hubs provide in areas of high clustering and also between areas of high clustering demonstrate their connective qualities. The presence of hubs in a market can be an important aspect of new product adoption and diffusion rates, by utilizing hubs when bringing a product to market producers can generate more \u2018buzz\u2019 and reach a greater target audience with improved efficiency through the use of hubs and their large number of connections across a network. In terms of hubs acting as facilitators of diffusion, they can take many forms. One example of this is a celebrity endorsement in a marketing campaign for a product, the celebrity acts as the hub by using their fame and the perceived links that consumers have with their favorite celebrities to distribute information about the product which is given to them by the producer. The links involved with hubs exhibit small world properties of a number of short paths between actors which are clustered due to similarities. However, while the interconnectedness which hubs can bring into a network is a great strength it can also be their greatest weakness in that, if functioning correctly hubs keep everyone in touch through a system, but remove a hub and the clusters become segregated and communication and information flow become strained.",
            "score": 135.99366760253906
        },
        {
            "docid": "10571004_5",
            "document": "Biological network inference . Genes are the nodes and the edges are directed. A gene serves as the source of a direct regulatory edge to a target gene by producing an RNA or protein molecule that functions as a transcriptional activator or inhibitor of the target gene. If the gene is an activator, then it is the source of a positive regulatory connection; if an inhibitor, then it is the source of a negative regulatory connection. Computational algorithms take as primary input data measurements of mRNA expression levels of the genes under consideration for inclusion in the network, returning an estimate of the network topology. Such algorithms are typically based on linearity, independence or normality assumptions, which must be verified on a case-by-case basis. Clustering or some form of statistical classification is typically employed to perform an initial organization of the high-throughput mRNA expression values derived from microarray experiments, in particular to select sets of genes as candidates for network nodes. The question then arises: how can the clustering or classification results be connected to the underlying biology? Such results can be useful for pattern classification \u2013 for example, to classify subtypes of cancer, or to predict differential responses to a drug (pharmacogenomics). But to understand the relationships between the genes, that is, to more precisely define the influence of each gene on the others, the scientist typically attempts to reconstruct the transcriptional regulatory network. This can be done by data integration in dynamic models supported by background literature, or information in public databases, combined with the clustering results. The modelling can be done by a Boolean network, by Ordinary differential equations or Linear regression models, e.g. Least-angle regression, by Bayesian network or based on Information theory approaches. For instance it can be done by the application of a correlation-based inference algorithm, as will be discussed below, an approach which is having increased success as the size of the available microarray sets keeps increasing",
            "score": 135.82763671875
        },
        {
            "docid": "44342518_37",
            "document": "Multidimensional network . The question of degree correlations in unidimensional networks is fairly straightforward: do networks of similar degree tend to connect to each other? In multidimensional networks, what this question means becomes less clear. When we refer to a node's degree, are we referring to its degree in one dimension, or collapsed over all? When we seek to probe connectivity between nodes, are we comparing the same nodes across dimensions, or different nodes within dimensions, or a combination? What are the consequences of variations in each of these statistics on other network properties? In one study, assortativity was found to decrease robustness in a duplex network.",
            "score": 135.56851196289062
        },
        {
            "docid": "18396728_2",
            "document": "Modularity (networks) . Modularity is one measure of the structure of networks or graphs. It was designed to measure the strength of division of a network into modules (also called groups, clusters or communities). Networks with high modularity have dense connections between the nodes within modules but sparse connections between nodes in different modules. Modularity is often used in optimization methods for detecting community structure in networks. However, it has been shown that modularity suffers a resolution limit and, therefore, it is unable to detect small communities. Biological networks, including animal brains, exhibit a high degree of modularity.",
            "score": 135.36300659179688
        },
        {
            "docid": "8287543_3",
            "document": "Community structure . In the study of networks, such as computer and information networks, social networks and biological networks, a number of different characteristics have been found to occur commonly, including the small-world property, heavy-tailed degree distributions, and clustering, among others. Another common characteristic is community structure. In the context of networks, community structure refers to the occurrence of groups of nodes in a network that are more densely connected internally than with the rest of the network, as shown in the example image to the right. This inhomogeneity of connections suggests that the network has certain natural divisions within it.",
            "score": 135.16293334960938
        },
        {
            "docid": "766409_11",
            "document": "Network theory . With the recent explosion of publicly available high throughput biological data, the analysis of molecular networks has gained significant interest. The type of analysis in this context is closely related to social network analysis, but often focusing on local patterns in the network. For example, network motifs are small subgraphs that are over-represented in the network. Similarly, activity motifs are patterns in the attributes of nodes and edges in the network that are over-represented given the network structure. The analysis of biological networks with respect to diseases has led to the development of the field of network medicine. Recent examples of application of network theory in biology include applications to understanding the cell cycle. The interactions between physiological systems like brain, heart, eyes, etc. can be regarded as a physiological network.",
            "score": 134.70303344726562
        },
        {
            "docid": "15078_10",
            "document": "Internetwork Packet Exchange . The network number allows to address (and communicate with) the IPX nodes which do not belong to the same network or \"cabling system\". The cabling system is a network in which a data link layer protocol can be used for communication. To allow communication between different networks, they must be connected with IPX routers. A set of interconnected networks is called an internetwork. Any Novell Netware server may serve as an IPX router. Novell also supplied stand-alone routers. Multiprotocol routers of other vendors often support IPX routing. Using different frame formats in one cabling system is possible, but it works similarly as if separate cabling systems were used (i.e. different network numbers must be used for different frame formats even in the same cabling system and a router must be to allow communication between nodes using different frame formats in the same cabling system).",
            "score": 134.65142822265625
        },
        {
            "docid": "14726322_3",
            "document": "Complex network zeta function . One usually thinks of dimension for a set which is dense, like the points on a line, for example. Dimension makes sense in a discrete setting, like for graphs, only in the large system limit, as the size tends to infinity. For example, in Statistical Mechanics, one considers discrete points which are located on regular lattices of different dimensions. Such studies have been extended to arbitrary networks, and it is interesting to consider how the definition of dimension can be extended to cover these cases. A very simple and obvious way to extend the definition of dimension to arbitrary large networks is to consider how the volume (number of nodes within a given distance from a specified node) scales as the distance (shortest path connecting two nodes in the graph) is increased. For many systems arising in physics, this is indeed a useful approach. This definition of dimension could be put on a strong mathematical foundation, similar to the definition of Hausdorff dimension for continuous systems. The mathematically robust definition uses the concept of a zeta function for a graph. The complex network zeta function and the graph surface function were introduced to characterize large graphs. They have also been applied to study patterns in Language Analysis. In this section we will briefly review the definition of the functions and discuss further some of their properties which follow from the definition.",
            "score": 134.5084991455078
        },
        {
            "docid": "8337948_6",
            "document": "Bose\u2013Einstein condensation (network theory) . A network is characterized by a set of nodes or vertices and a set of links between these nodes. In mathematics, graph theory describes networks in general. The theory of random graphs deals in particular with stochastic networks (networks in which each link is present with a given probability \"p\"). A large class of networks that describe real complex systems like the Internet, the world wide web, airport networks or the biological networks of molecular interactions, are described by random networks. Network theory is a recent field of research which investigates methods of characterizing and modeling real complex networks. In particular it has been found that many complex networks have universal features like the small world property and a scale-free degree distribution. The scale-free degree distribution of networks can be caused by the \"preferential attachment\" mechanism.",
            "score": 134.5015411376953
        },
        {
            "docid": "22072718_14",
            "document": "Biological network . In biology, pairwise interactions have historically been the focus of intense study. With the recent advances in network science, it has become possible to scale up pairwise interactions to include individuals of many species involved in many sets of interactions to understand the structure and function of larger ecological networks. The use of network analysis can allow for both the discovery and understanding how these complex interactions link together within the system\u2019s network, a property which has previously been overlooked. This powerful tool allows for the study of various types of interactions (from competitive to cooperative) using the same general framework. For example, plant-pollinator interactions are mutually beneficial and often involve many different species of pollinators as well as many different species of plants. These interactions are critical to plant reproduction and thus the accumulation of resources at the base of the food chain for primary consumers, yet these interaction networks are threatened by anthropogenic change. The use of network analysis can illuminate how pollination networks work and may in turn inform conservation efforts. Within pollination networks, nestedness (i.e., specialists interact with a subset of species that generalists interact with), redundancy (i.e., most plants are pollinated by many pollinators), and modularity play a large role in network stability. These network properties may actually work to slow the spread of disturbance effects through the system and potentially buffer the pollination network from anthropogenic changes somewhat. More generally, the structure of species interactions within an ecological network can tell us something about the diversity, richness, and robustness of the network. Researchers can even compare current constructions of species interactions networks with historical reconstructions of ancient networks to determine how networks have changed over time. Recent research into these complex species interactions networks is highly concerned with understanding what factors (e.g., diversity) lead to network stability.",
            "score": 134.29571533203125
        },
        {
            "docid": "315578_4",
            "document": "Information processing . Information processing may be vertical or horizontal, either of which may be centralized or decentralized (distributed). The horizontal distributed processing approach of the mid-1980s became popular under the name connectionism. The connectionist network is made up of different nodes, and it works by a \"priming effect,\" and this happens when a \"prime node activates a connected node\" (Sternberg & Sternberg, 2012). But \"unlike in semantic networks, it is not a single node that has a specific meaning, but rather the knowledge is represented in a combination of differently activated nodes\"(Goldstein, as cited in Sternberg, 2012).",
            "score": 134.15640258789062
        }
    ]
}