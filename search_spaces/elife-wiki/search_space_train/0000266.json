{
    "q": [
        {
            "docid": "17258308_8",
            "document": "Two-alternative forced choice . The drift-diffusion model (DDM) is a well defined model, that probably implements an optimum decision procedure for 2AFC. It is the continuous analog of a random walk model. The DDM assumes that in a 2AFC task, the subject is accumulating evidence for one or other of the alternatives at each time step, and integrating that evidence until a decision threshold is reached. As the sensory input which constitutes the evidence is noisy, the accumulation to the threshold is stochastic rather than deterministic \u2013 this gives rise to the directed random walk-like behavior. The DDM has been shown to describe accuracy and reaction times in human data for 2AFC tasks.",
            "score": 97.06309866905212
        },
        {
            "docid": "26685741_39",
            "document": "Sleep and memory . REM sleep is known for its vivid creations and similarity to the bioelectric outputs of a waking person. This stage of sleep is characterized by muscle atonia, fast but low voltage EEG and, as the name suggests, rapid eye movement. It is difficult to attribute memory gains to a single stage of sleep when it may be the entire sleep cycle that is responsible for memory consolidation. Recent research conducted by Datta et al. used an avoidance task followed by a post-training REM sleep period to examine changes in P waves affecting reprocessing of recently acquired stimuli. It was found that not only were the P waves increased during post-training sleep but also the density of the waves. These findings may imply that P waves during REM sleep may help to activate critical forebrain and cortical structures dealing with memory consolidation. In a Hennevin et al. study, 1989, the mesencephalic reticular formation (MRF) was given light electrical stimulation, during REM sleep, which is known to have an advantageous effect for learning when applied after training. The rats in the experiment were trained to run a maze in search of a food reward. One group of rats was given non-awakening MRF electrical stimulations after each of their maze trials compared to a control group which did not receive any electrical stimulation. It was noticed that the stimulated rats performed significantly better in respect to error reduction. These findings imply that dynamic memory processes occur both during training as well as during post-training sleep. Another study by Hennevin et al. (1998) conditioned rats to fear a noise that is associated with a subsequent foot shock. The interesting part of the experiment is that fear responding to the noise (measured in the amygdala) was observed when the noise was presented during REM sleep. This was compared to a group of pseudo-conditioned rats who did not display the same amygdalar activation during post-training sleep. This would suggest that neural responding to previously salient stimuli is maintained even during REM sleep. There is no shortage of research conducted on the effects that REM sleep has on the working brain but consistency in the findings is what plagues recent research. There is no guarantee as to what functions REM sleep may perform for our bodies and brains but modern research is always expanding and assimilating new ideas to further our understanding of such processes.",
            "score": 134.74619209766388
        },
        {
            "docid": "17523336_22",
            "document": "Olivocochlear system . Although Scharf et al.\u2019s (1993, 1994, 1997) experiments failed to produce any clear differences in the basic psychophysical characteristics of hearing (other than the detection of unexpected sounds), many other studies using both animals and humans have implicated the OCB in listening-in-noise tasks using more complex stimuli. In constant BGN, rhesus monkeys with intact OCBs have been observed to perform better in vowel discrimination tasks than those without (Dewson, 1968). In cats, an intact OCB is associated with better vowel identification (Heinz et al., 1998), sound localisation (May et al., 2004), and intensity discrimination (May and McQuone, 1995). All of these studies were performed in constant BGN. In humans, speech-in-noise discrimination measurements have been performed on individuals who had undergone unilateral vestibular neurectomy (resulting in OCB sectioning). Giraud et al. (1997) observed a small advantage in the healthy ear over the operated ear for phoneme recognition and speech intelligibility in BGN. Scharf et al. (1988) had previously investigated the role of auditory attention during speech perception, and suggested that speech-in-noise discrimination is assisted by attentional focus on frequency regions. In 2000, Zeng et al., reported that vestibular neurectomy did not directly affect pure-tone thresholds or intensity discrimination, confirming earlier findings of Scharf et al. 1994; 1997. For the listening-in-noise tasks, they observed a number of discrepancies between the healthy and operated ear. Consistent with the earlier findings of May and McQuone (1995), intensity discrimination in noise was observed to be slightly worse in the ear without olivocochlear bundle (OCB) input. However, Zeng et al.\u2019s main finding related to the \u201covershoot\u201d effect, which was found to be significantly reduced (~50%) in the operated ears. This effect was first observed by Zwicker (1965), and was characterised as an increased detection threshold of a tone when it is presented at the onset of the noise compared to when it is presented in constant, steady-state noise. Zeng et al. proposed that this finding is consistent with MOCS-evoked antimasking; that is, MOCS-evoked antimasking being absent at the onset of noise however becoming active during steady-state noise. This theory was supported by the time course of MOC activation; being similar to the time course of the overshoot effect (Zwicker, 1965), as well as the overshoot effect being disrupted in subjects with sensorineural hearing loss, for whom the MOCS would be most likely ineffectual (Bacon and Takahashi, 1992).",
            "score": 98.29399371147156
        },
        {
            "docid": "46182_2",
            "document": "White noise . In signal processing, white noise is a random signal having equal intensity at different frequencies, giving it a constant power spectral density. The term is used, with this or similar meanings, in many scientific and technical disciplines, including physics, acoustic engineering, telecommunications, and statistical forecasting. White noise refers to a statistical model for signals and signal sources, rather than to any specific signal. White noise draws its name from white light, although light that appears white generally does not have a flat power spectral density over the visible band. In discrete time, white noise is a discrete signal whose samples are regarded as a sequence of serially uncorrelated random variables with zero mean and finite variance; a single realization of white noise is a random shock. Depending on the context, one may also require that the samples be independent and have identical probability distribution (in other words independent and identically distributed random variables are the simplest representation of white noise). In particular, if each sample has a normal distribution with zero mean, the signal is said to be Gaussian white noise.",
            "score": 87.53418731689453
        },
        {
            "docid": "10567836_12",
            "document": "Ordinal numerical competence . Experiments have shown that rats are able to be trained to press one lever after hearing two bursts of white noise, then press another lever after four bursts of white noise. The interburst interval is varied between trials so the discrimination is based on number of bursts and not time duration of the sequence. Studies show that rats as well as pigeons learned to make different responses to both short and long durations of signals. During testing, rats exhibited a pattern called \"break-run-break\"; when it came to responding after a stint of little to no response, they would suddenly respond in high frequency, then return to little or no response activity.  Data suggests that rats and pigeons are able to process time and number information at the same time. The Mode Control Model shows that these animals can process number and time information by transmission pulses to accumulators controlled by switches that operate different modes.",
            "score": 122.8201208114624
        },
        {
            "docid": "40521609_19",
            "document": "Stochastic geometry models of wireless networks . The general theory and techniques of stochastic geometry and, in particular, point processes have often been motivated by the understanding of a type of noise that arises in electronic systems known as shot noise. For certain mathematical functions of a point process, a standard method for finding the average (or expectation) of the sum of these functions is Campbell's formula or theorem, which has its origins in the pioneering work by Norman R. Campbell on shot noise over a century ago. Much later in the 1960s Gilbert alongside Henry Pollak studied the shot noise process formed from a sum of response functions of a Poisson process and identically distributed random variables. The shot noise process inspired more formal mathematical work in the field of point processes, often involving the use of characteristic functions, and would later be used for models of signal interference from other nodes in the network.",
            "score": 72.35112631320953
        },
        {
            "docid": "42450197_5",
            "document": "Optical rogue waves . The analogy between these extreme optical events and hydrodynamic rogue waves was initially developed by noting a number of parallels, including the role of solitons, heavy-tailed statistics, dispersion, modulation instability, and frequency downshifting effects. Additionally, forms of the nonlinear Schr\u00f6dinger equation are used to model both optical pulse propagation in nonlinear fiber and deep water waves, including hydrodynamic rogue waves. Simulations were then conducted with the nonlinear Schr\u00f6dinger equation in an effort to model the optical findings. For each trial or event, the initial conditions consisted of an input pulse and a minute amount of broadband input noise. The initial conditions (i.e., pulse power and noise level) were chosen so that the spectral broadening was relatively limited in the typical events. Collecting the results from the trials, very similar filtered energy statistics were observed compared with those seen experimentally. The simulations showed that rare events had experienced significantly more spectral broadening than the others because a soliton had been ejected in the former class of events, but not in the vast majority of events. By applying a correlation analysis between the redshifted output energy and the input noise, it was observed that a particular component of the input noise was elevated each time a surplus in the redshifted noise was generated. The critical noise component has specific frequency and timing relative to the pulse envelope\u2014a noise component that efficiently seeds modulation instability and can, therefore, accelerate the onset of soliton fission.",
            "score": 59.41731536388397
        },
        {
            "docid": "11130010_10",
            "document": "Misattribution of arousal . Misattribution of arousal can also influence how much confidence one feels before completing a task. One study conducted by Savitsky, Medvec, Charlton, and Gilovich focused on how confidence can be affected by misattribution of arousal. Typically people feel more confident before they are supposed to do a task, but the closer they get to having to perform that task, the less confident they feel, which could be due to the arousal from the expectation of the performance. The researchers told participants that they would be exposed to a subliminal noise and were then asked if they could predict how well they would do on two tasks. After they made their predictions, the researchers either told them that the noise could make them nervous, the noise would have no effect on them, or they were told that they would not have to do the tasks until the next session a month away after they were exposed to the noise. The tasks were to unscramble anagrams or to recall as many nonsense syllables as they could after seeing them briefly. They could earn money for the tasks (more money was earned for each anagram unscrambled or each syllable correctly recalled). The participants then predicted how well they did on the tasks, and how well they believed everyone else did on the task. A second experiment replicated this first experiment.The researchers had participants attribute their arousal to noises that they heard, which resulted in those participants feeling more confident that they did well on the tasks than those that attributed their arousal to the performance anxiety from the task.",
            "score": 68.71475911140442
        },
        {
            "docid": "46182_32",
            "document": "White noise . In statistics and econometrics one often assumes that an observed series of data values is the sum of a series of values generated by a deterministic linear process, depending on certain independent (explanatory) variables, and on a series of random noise values. Then regression analysis is used to infer the parameters of the model process from the observed data, e.g. by ordinary least squares, and to test the null hypothesis that each of the parameters is zero against the alternative hypothesis that it is non-zero. Hypothesis testing typically assumes that the noise values are mutually uncorrelated with zero mean and have the same Gaussian probability distributionin other words, that the noise is white. If there is non-zero correlation between the noise values underlying different observations then the estimated model parameters are still unbiased, but estimates of their uncertainties (such as confidence intervals) will be biased (not accurate on average). This is also true if the noise is heteroskedasticthat is, if it has different variances for different data points.",
            "score": 78.29569160938263
        },
        {
            "docid": "34100130_5",
            "document": "Distraction-conflict . This model more broadly predicts that any attentional conflict will produce drive. Distraction-conflict has been supported by several studies which have produced results showing that \"distractions, such as noise or flashing lights, have the same drivelike effects on task performance that audiences do\". This is because \"our attention is divided between the task at hand and observing the reactions of the people in the audience\" in much the same way how one is distracted from the task at hand by sounds or flashing lights. The effects of distraction-conflict are also shown to be the strongest when there is a sense of urgency.",
            "score": 66.39415526390076
        },
        {
            "docid": "15774067_27",
            "document": "Synaptic noise . To understand the future of synaptic noise research, it would be essential to discuss the work of Alain Destexhe, a Belgian doctor who has greatly studied the importance of synaptic noise in neuronal connections. He uses the dynamic-clamp technique to understand the presence and characteristics of noise. While voltage-gated clamps record configurations, dynamic-clamp allows for the control of conductance by way of computer. A computational model of synaptic noise is created and is then implemented into the neuron, simulating synaptic noise. This can be used to compare with in-vivo conditions. Destexhe states that future research can be directed towards four possible ways, in reflection of his research with dynamic-clamp. First, it could be beneficial to understand the control of synaptic noise so that the modulation of noise can be used on humans to turn unresponsive networks into a responsive state. Next, it would be necessary to understand how external noise interacts with internal neuronal properties more fully to coincide models with experimental facts. There also exists the need to further investigate experimentally the methods of dendritic integration and the role of synaptic noise when it is present. Finally, he found support that synaptic noise enhances temporal resolution in neurons, yet experimental proof has not been done to further elaborate on past modeling studies. By use of dynamic-clamp, these pieces of information clarify the role of synaptic noise in the brain and how it can be harnessed for specific therapies.",
            "score": 61.884302735328674
        },
        {
            "docid": "7214278_15",
            "document": "Decision field theory . The Decision Field Theory has demonstrated an ability to account for a wide range of findings from behavioral decision making for which the purely algebraic and deterministic models often used in economics and psychology cannot account. Recent studies that record neural activations in non-human primates during perceptual decision making tasks have revealed that neural firing rates closely mimic the accumulation of preference theorized by behaviorally-derived diffusion models of decision making.",
            "score": 120.93909573554993
        },
        {
            "docid": "46182_33",
            "document": "White noise . Alternatively, in the subset of regression analysis known as time series analysis there are often no explanatory variables other than the past values of the variable being modeled (the dependent variable). In this case the noise process is often modeled as a moving average process, in which the current value of the dependent variable depends on current and past values of a sequential white noise process.",
            "score": 64.76600074768066
        },
        {
            "docid": "19208664_25",
            "document": "Neural modeling fields . Finding patterns below noise can be an exceedingly complex problem. If an exact pattern shape is not known and depends on unknown parameters, these parameters should be found by fitting the pattern model to the data. However, when the locations and orientations of patterns are not known, it is not clear which subset of the data points should be selected for fitting. A standard approach for solving this kind of problem is multiple hypothesis testing (Singer et al. 1974). Since all combinations of subsets and models are exhaustively searched, this method faces the problem of combinatorial complexity. In the current example, noisy \u2018smile\u2019 and \u2018frown\u2019 patterns are sought. They are shown in Fig.1a without noise, and in Fig.1b with the noise, as actually measured. The true number of patterns is 3, which is not known. Therefore, at least 4 patterns should be fit to the data, to decide that 3 patterns fit best. The image size in this example is 100x100 = 10,000 points. If one attempts to fit 4 models to all subsets of 10,000 data points, computation of complexity, M ~ 10. An alternative computation by searching through the parameter space, yields lower complexity: each pattern is characterized by a 3-parameter parabolic shape. Fitting 4x3=12 parameters to 100x100 grid by a brute-force testing would take about 10 to 10 operations, still a prohibitive computational complexity. To apply NMF and dynamic logic to this problem one needs to develop parametric adaptive models of expected patterns. The models and conditional partial similarities for this case are described in details in: a uniform model for noise, Gaussian blobs for highly-fuzzy, poorly resolved patterns, and parabolic models for \u2018smiles\u2019 and \u2018frowns\u2019. The number of computer operations in this example was about 10. Thus, a problem that was not solvable due to combinatorial complexity becomes solvable using dynamic logic.",
            "score": 59.177496552467346
        },
        {
            "docid": "6635218_7",
            "document": "Line source . Roadway noise is the most important example of a linear noise source, since it comprises about 80 percent of the environmental noise exposure for humans worldwide. In the 1960s, when computer modeling of this phenomenon was perfected, the first applications of linear source noise modeling became systematic. After passage of the National Environmental Policy Act and Noise Control Act, the demand for detailed analysis soared, and decision makers began to look to acoustical scientists for answers regarding the planning of new roadways and the design of noise mitigation. The intensity of roadway noise is governed by the following variables: traffic operations (speed, truck mix, age of vehicle fleet), roadway surface type, tire types, roadway geometrics, terrain, micrometeorology and the geometry of area structures.",
            "score": 76.07226705551147
        },
        {
            "docid": "25146378_15",
            "document": "Functional specialization (brain) . During the 1960s, Roger Sperry conducted a natural experiment on epileptic patients who had previously had their corpora callosa cut. The corpus callosum is the area of the brain dedicated to linking both the right and left hemisphere together. Sperry et al.'s experiment was based on flashing images in the right and left visual fields of his participants. Because the participant's corpus callosum was cut, the information processed by each visual field could not be transmitted to the other hemisphere. In one experiment, Sperry flashed images in the right visual field (RVF), which would subsequently be transmitted to the left hemisphere (LH) of the brain. When asked to repeat what they had previously seen, participants were fully capable of remembering the image flashed. However, when the participants were then asked to draw what they had seen, they were unable to. When Sperry et al. flashed images in the left visual field (LVF), the information processed would be sent to the right hemisphere (RH) of the brain. When asked to repeat what they had previously seen, participants were unable to recall the image flashed, but were very successful in drawing the image. Therefore, Sperry concluded that the left hemisphere of the brain was dedicated to language as the participants could clearly speak the image flashed. On the other hand, Sperry concluded that the right hemisphere of the brain was involved in more creative activities such as drawing.",
            "score": 96.25514531135559
        },
        {
            "docid": "14308303_8",
            "document": "Monte Carlo method for photon transport . In our simplified model we use the following variance reduction technique to reduce computational time. Instead of propagating photons individually, we create a photon packet with a specific weight (generally initialized as unity). As the photon interacts in the turbid medium, it will deposit weight due to absorption and the remaining weight will be scattered to other parts of the medium. Any number of variables can be logged along the way, depending on the interest of a particular application. Each photon packet will repeatedly undergo the following numbered steps until it is either terminated, reflected, or transmitted. The process is diagrammed in the schematic to the right. Any number of photon packets can be launched and modeled, until the resulting simulated measurements have the desired signal-to-noise ratio. Note that as Monte Carlo modeling is a statistical process involving random numbers, we will be using the variable \u03be throughout as a pseudo-random number for many calculations.",
            "score": 73.7605710029602
        },
        {
            "docid": "2230325_9",
            "document": "Michael Gazzaniga . Patient W.J. was a World War II paratrooper who got hit in the head with a rifle butt, after which he started having seizures. Before his operation to try to fix the seizures, Gazzaniga tested his brain functions. This included presenting stimuli to the left and right visual fields and identifying objects in his hands that were out of view. He was able to perform these tasks perfectly and afterwards he had the surgery that split his corpus callosum and anterior commissure. After his surgery, he was brought in again for testing with Gazzaniga in which stimuli such as letters and light bursts were flashed to the left and right visual fields. The stimuli flashed to the right visual field were processed by the brain\u2019s left hemisphere, which contains the language center, so he was able to press a button to indicate he saw the stimulus and could verbally report what he had seen. However, when the stimuli were flashed to the left visual field, and thus the right hemisphere, he would press the button, but could not verbally report having seen anything. When they modified the experiment to have him point to the stimulus that was presented to his left visual field and not have to verbally identify it, he was able to perform this task accurately.",
            "score": 73.64715671539307
        },
        {
            "docid": "37822732_10",
            "document": "History of network traffic models . Early traffic models were derived from telecommunications models and focused on simplicity of analysis. They generally operated under the assumption that aggregating traffic from a large number of sources tended to smooth out bursts; that burstiness decreased as the number of traffic sources increased. One of the most widely used and oldest traffic models is the Poisson Model. The memoryless Poisson distribution is the predominant model used for analyzing traffic in traditional telephony networks. The Poisson process is characterized as a renewal process. In a Poisson process the inter-arrival times are exponentially distributed with a rate parameter \u03bb: P{An \u2264 t} = 1 \u2013 exp(-\u03bbt). The Poisson distribution is appropriate if the arrivals are from a large number of independent sources, referred to as Poisson sources. The distribution has a mean and variance equal to the parameter \u03bb. The Poisson distribution can be visualized as a limiting form of the binomial distribution, and is also used widely in queuing models. There are a number of interesting mathematical properties exhibited by Poisson processes. Primarily, superposition of independent Poisson processes results in a new Poisson process whose rate is the sum of the rates of the independent Poisson processes. Further, the independent increment property renders a Poisson process memoryless. Poisson processes are common in traffic applications scenarios that consist of a large number of independent traffic streams. The reason behind the usage stems from Palm's Theorem which states that under suitable conditions, such large number of independent multiplexed streams approach a Poisson process as the number of processes grows, but the individual rates decrease in order to keep the aggregate rate constant. Nevertheless, it is to be noted that traffic aggregation need not always result in a Poisson process. The two primary assumptions that the Poisson model makes are: 1. The number of sources is infinite 2. The traffic arrival pattern is random. In the compound Poisson model, the base Poisson model is extended to deliver batches of packets at once. The inter-batch arrival times are exponentially distributed, while the batch size is geometric Mathematically, this model has two parameters, \u03bb, the arrival rate, and \u03c1 in (0,1), the batch parameter. Thus, the mean number of packets in a batch is 1/ \u03c1, while the mean inter-batch arrival time is 1/ \u03bb. Mean packet arrivals over time period t are t\u03bb/ \u03c1. The compound Poisson model shares some of the analytical benefits of the pure Poisson model: the model is still memoryless, aggregation of streams is still (compound) Poisson, and the steady-state equation is still reasonably simple to calculate, although varying batch parameters for differing flows would complicate the derivation. Markov models attempt to model the activities of a traffic source on a network, by a finite number of states. The accuracy of the model increases linearly with the number of states used in the model. However, the complexity of the model also increases proportionally with increasing number of states. An important aspect of the Markov model - the Markov Property, states that the next (future) state depends only on the current state. In other words, the probability of the next state, denoted by some random variable Xn+1, depends only on the current state, indicated by Xn, and not on any other state Xi, where i<n. The set of random variables referring to different states {Xn} is referred to as a Discrete Markov Chain. Another attempt at providing a bursty traffic model is found in Jain and Routhier\u2019s Packet Trains model. This model was principally designed to recognize that address locality applies to routing decisions; that is, packets that arrive near each other in time are frequently going to the same destination. In generating a traffic model that allows for easier analysis of locality, the authors created the notion of packet trains, a sequence of packets from the same source, traveling to the same destination (with replies in the opposite direction). Packet trains are optionally sub-divided into tandem trailers. Traffic between a source and a destination usually consists of a series of messages back and forth. Thus, a series of packets go one direction, followed by one or more reply packets, followed by a new series in the initial direction. Traffic quantity is then a superposition of packet trains, which generates substantial bursty behavior. This refines the general conception of the compound Poisson model, which recognized that packets arrived in groups, by analyzing why they arrive in groups, and better characterizing the attributes of the group. Finally, the authors demonstrate that packet arrival times are not Poisson distributed, which led to a model that departs from variations on the Poisson theme. The packet train model is characterized by the following parameters and their associated probability distributions: The train model is designed for analyzing and categorizing real traffic, not for generating synthetic loads for simulation. Thus, little claim has been made about the feasibility of packet trains for generating synthetic traffic. Given accurate parameters and distributions, generation should be straightforward, but derivation of these parameters is not addressed.",
            "score": 91.05813801288605
        },
        {
            "docid": "4491258_16",
            "document": "Roadway noise . At the micro level of managing particular roads, because of the complexity of the variables discussed above, it is necessary to create a computer model that can analyze sound levels in the vicinity of roadways. The first meaningful models arose in the late 1960s and early 1970s addressing the noise line source (e.g. roadway). Two of the leading research teams were BBN in Boston and ESL of Sunnyvale, California. Both of these groups developed complex mathematical models to allow the study of alternate roadway designs, traffic operations and noise mitigation strategies in an arbitrary setting. Later model alterations have come into widespread use among state departments of transportation and city planners, but the accuracy of early models has had little change in 40 years.",
            "score": 60.18348002433777
        },
        {
            "docid": "614230_6",
            "document": "Event-related potential . The random (background) brain activity together with other bio-signals (e.g., EOG, EMG, EKG) and electromagnetic interference (e.g., line noise, fluorescent lamps) constitute the noise contribution to the recorded ERP. This noise obscures the signal of interest, which is the sequence of underlying ERPs under study. From an engineering point of view it is possible to define the signal-to-noise ratio (SNR) of the recorded ERPs. The reason that averaging increases the SNR of the recorded ERPs (making them discernible and allowing for their interpretation) has a simple mathematical explanation provided that some simplifying assumptions are made. These assumptions are: Having defined formula_2, the trial number, and formula_3, the time elapsed after the formula_2 event, each recorded trial can be written as formula_5 where formula_6 is the signal and formula_7 is the noise (Note that, under the assumptions above, the signal does not depend on the specific trial while the noise does).",
            "score": 75.63721227645874
        },
        {
            "docid": "226722_49",
            "document": "Functional magnetic resonance imaging . The basic model assumes the observed HDR is the predicted HDR scaled by the weights for each event and then added, with noise mixed in. This generates a set of linear equations with more equations than unknowns. A linear equation has an exact solution, under most conditions, when equations and unknowns match. Hence one could choose any subset of the equations, with the number equal to the number of variables, and solve them. But, when these solutions are plugged into the left-out equations, there will be a mismatch between the right and left sides, the error. The GLM model attempts to find the scaling weights that minimize the sum of the squares of the error. This method is provably optimal if the error were distributed as a bell curve, and if the scaling-and-summing model were accurate. For a more mathematical description of the GLM model, see generalized linear models.",
            "score": 85.84238719940186
        },
        {
            "docid": "33710191_14",
            "document": "POLQA . The key concept inside the perceptual model is Idealisation. The idea behind this is, that POLQA is supposed to simulate Absolute Category Rating (ACR) tests. In an ACR test however, subjects have no comparison to the actual reference signal when they score a speech signal. Instead, it is assumed that subjects have an understanding of what an ideal signal sounds like and they use this as their own reference. Consequently, if they are asked to score a reference signal which is not absolutely perfect (e.g. it has the wrong volume or contains too much timbre, noise or reverberation), it will be scored worse than perfect. In its idealization step POLQA therefore corrects small imperfections of the reference signals in order to derive the same ideal reference for the comparison to the degraded signal as human subjects would use in their minds. Similar to the idealization of the reference signal, some distortions present in the degraded signal which are hardly perceptible in an ACR test will be partially compensated (e.g. small pitch shifts, linear frequency distortions).  The perceptual model starts with scaling the reference signal to an ideal average active speech level of approximately -26dBov. No such scaling is performed on the degraded signal. It is assumed that any deviation of the level of the degraded signal from the ideal -26dBov is to be scored as a degradation of the signal.  Next, the spectra of both signals are computed using an FFT with 50% overlapping frames with a duration of between 32ms and 43ms duration (depending on the sample rate). Subsequently small pitch shifts of the degraded signal will be eliminated (Frequency Dewarping). Now, the spectra will be transformed to a psychoacoustically motivated pitch scale, by combining individual spectral lines (FFT bins) to so-called critical bands. The pitch scale used is similar to the Bark scale with an average resolution of 0.3 Bark per band. The result is the Pitch Power Density. At this stage the first three distortion indicators for frequency response distortions, additive noise and room reverberations are calculated. After this, the excitation of each band is derived. This includes the modeling of masking effects in the frequency as well as in the temporal domain. The result is for each frame of each signal a head-internal representation which indicates roughly how loud each frequency component would be perceived. Now, a further idealization step of the reference signal takes place by filtering out excessive timbre and low level stationary noise. At the same time, linear frequency distortions and stationary noise are partially removed from the degraded signal. A subtraction of the idealized excitations finally leads to the Distortion Density, which is measure for the audibility of distortions.",
            "score": 56.08626329898834
        },
        {
            "docid": "6762618_15",
            "document": "Virgo interferometer . The signal induced by a potential gravitational wave is thus \"embedded\" in the light intensity variations detected at the interferometer output. Yet, several external causes\u2014globally denoted as noises\u2014changes the interference pattern perpetually and significantly. Should nothing be done to remove or mitigate them, the expected physical signals would be buried in noise and would then remain undetectable. The design of detectors like Virgo and LIGO thus requires a detailed inventory of all noise sources which could impact the measurement, allowing a strong and continuing effort to reduce them as much as possible. During the data taking periods, dedicated software monitors in real time the noise levels in the interferometer, and deep studies are carried out to identify the loudest noises and mitigate them. Each period during which a detector is found to be \"too noisy\" is excluded from the data analysis: these dead times need to be reduced as much as possible.",
            "score": 38.3691371679306
        },
        {
            "docid": "7214278_17",
            "document": "Decision field theory . Mathematically, the spike activation pattern, as well as the choice and response time distributions, can be well described by what are known as diffusion models - especially in Two-alternative forced choice tasks(see Smith & Ratcliff for a summary). Diffusion models, such as the decision field theory, can be viewed as stochastic recurrent neural network models, except that the dynamics are approximated by linear systems. The linear approximation is important for maintaining a mathematically tractable analysis of systems perturbed by noisy inputs. In addition to these neuroscience applications, diffusion models (or their discrete time, random walk, analogues) have been used by cognitive scientists to model performance in a variety of tasks ranging from sensory detection, and perceptual discrimination, to memory recognition, and categorization. Thus, diffusion models provide the potential to form a theoretical bridge between neural models of sensory-motor tasks and behavioral models of complex-cognitive tasks.",
            "score": 88.53719484806061
        },
        {
            "docid": "18794805_3",
            "document": "Audio &amp; Design (Recording) Ltd . The F760X 'Compex' limiter was made up of three main parts; a compressor, limiter and a 'noise reducing' expander/gate. All of these parts could be used separately or together for well controlled sound limiting. This combination came about from client feedback. From the feedback ADR designed and built the first units, these were labelled as F760X-RS (RS meaning rack-mount stereo). Later modifications were made and another version was released with a better, ergonomically designed, front panel. This later model was labelled as the 'Compex'. The 'Compex' limiter provided peak limiting with a multi-ratio, variable threshold compressor. Ratio settings ranged from 1:1 to 20:1 and there were controls for threshold and release time. The inclusion of an expander/gate provided attenuation of the gain during pauses in the audio signal to reduce noise caused by compression; the expander/gate could also be used for special 'punchy' effects, favoured on drum and percussive sounds. Noise gates were often criticised at the time for 'hunting' around the threshold point. The 'Compex' used clever hysteresis around the gate, this was important to stop unpredictable opening and closing of the gate; the open threshold was higher than the close threshold. The gain reduction controlled by the 'Compex' was monitored from two meters located to the right of the front panel. There were two light indicators; a red light lit when the signal level reached its peak, and when in expand mode a green light was lit. There were a set of lights for each channel, depending if the unit was the stereo or mono version. There were some different models of the 'Compex' available; one of these included a smaller console module named the F760-N. This version was built without the expander/gate, it later had an expander/gate added and was then known as the F760X-N.  There is also a rare version called F760X-RS/T. One of the visible differences are the VU-meters, which got the brand \"Audio & Design Recording LTD.\" and if you look on the rear, there you can see the type. The \"T\" stands for transformer, which are said to add harmonics to the sound.The \"T\" version was mainly used in broadcasting stations, mainly because of its SNR and headroom. ADR also built an F760 version for the Helios mixing consoles.",
            "score": 57.15983867645264
        },
        {
            "docid": "21312318_18",
            "document": "Recognition memory . Signal detection theory has been applied to recognition memory as a method of estimating the effect of the application of these internal criteria, referred to as bias.  Critical to the dual process model is the assumption that recognition memory reflects a signal detection process in which old and new items each have a distinct distribution along a dimension, such as familiarity. The application of Signal Detection Theory (SDT) to memory depends on conceiving of a memory trace as a signal that the subject must detect in order to perform in a retention task. Given this conception of memory performance, it is reasonable to assume that percentage correct scores may be biased indicators of retention\u2014just as thresholds may be biased indicators of sensory performance\u2014and, in addition, that SDT techniques should be used where possible to separate the truly retention-based aspects of memory performance from the decision aspects.  In particular, we assume that the subject compares the trace strength of the test item with a criterion, responding \"yes\" if the strength exceeds the criterion and \"no\"otherwise. There are two types of test items, \"old\" (a test item that appeared in the list for that trial) and new\" (one that did not appear in the list). Strength theory assumes that there may be noise in the value of the trace strength, the location of the criterion, or both. We assume that this noise is normally distributed.  The reporting criterion can shift along the continuum in the direction of more false hits, or more misses. The momentary memory strength of a test item is compared with the decision criteria and if the strength of the item falls within the judgment category, Jt, defined by the placement of the criteria, S makes judgment. The strength of an item is assumed to decline monotonically (with some error variance) as a continuous function of time or number of intervening items. False hits are 'new' words incorrectly recognized as old, and a greater proportion of these represents a liberal bias. Misses are 'old' words mistakenly not recognized as old, and a greater proportion of these represents a conservative bias. The relative distributions of false hits and misses can be used to interpret recognition task performance and correct for guessing. Only target items can generate an above-threshold recognition response because only they appeared on the list. The lures, along with any targets that are forgotten, fall below threshold, which means that they generate no memory signal whatsoever. False alarms in this model reflect memory-free guesses that are made to some of the lures.",
            "score": 86.22354412078857
        },
        {
            "docid": "9777284_4",
            "document": "Stroboflash . All four Stroboflash models consisted of a power pack that connected to a separate flash head via a power cord and special plug and all used the same Stroboflash flash heads, which were light weight and had a satin reflector that produced very soft, non directional light. Professional photographers used Stroboflash II and IV electronic flashes extensively in the 50's through 80's because of the soft, well diffused light they produce. The Stroboflash electronic flash units are still considered flashes that produce excellent quality high power lighting, but the battery expense and the fact that the batteries only produce between 1,000 to 2,000 flashes (depending on the power level selected) and the fact that these flash batteries were not rechargeable led to professionals replacing these flashes with other models that were powered by batteries that could be recharged.",
            "score": 55.037850737571716
        },
        {
            "docid": "2138419_3",
            "document": "Rapid serial visual presentation . There is a delay of several hundred milliseconds. A person might be asked to identify numbers in a string of letters which are shown one by one. The first number which is an important target, would be caught by the person, however, the second number flashed seconds later might not be observed. RSVP asks the question, What would reading be like if there were no eye movements? A text is delivered at a spot on the screen, like a series of flash cards. The user can set how long each card is to be displayed. The readers are liberated from having to decide how much time to spend on each word because that is set in advance, and saccades, regressive eye movements, line sweeps, and page turning have been eliminated. A reader can fully concentrate on comprehending the text as it flashes through, however, with longer texts the reading experience is found to be monotonous and exhausting. There are a number of theories to explain how and why this works and studies have explored its limitations and parameters to learn more about visual perception. The brain deals with a quick stream of incoming information at all times. With the attentional blink, the brain has to distribute its attentional resources to comprehend, interpret, and store the information properly. The human brain is capable of processing complex tasks, but it has restrictions. The attentional blink is an illustration that has a significant insinuation for individuals who work in environments where they are usually swamped with information. An example of this is an airport baggage screener who might see a knife in one bag, but misses a second knife in another bag that is right behind the first bag. The failure to recognize the second target is because of the attentional processes that are linked with the identification of the first target.",
            "score": 61.360615730285645
        },
        {
            "docid": "6871860_3",
            "document": "MUSIC (algorithm) . In many practical signal processing problems, the objective is to estimate from measurements a set of constant parameters upon which the received signals depend. There have been several approaches to such problems including the so-called maximum likelihood (ML) method of Capon (1969) and Burg's maximum entropy (ME) method. Although often successful and widely used, these methods have certain fundamental limitations (especially bias and sensitivity in parameter estimates), largely because they use an incorrect model (e.g., AR rather than special ARMA) of the measurements. Pisarenko (1973) was one of the first to exploit the structure of the data model, doing so in the context of estimation of parameters of complex sinusoids in additive noise using a covariance approach. Schmidt (1977), while working at of Northrop Grumman) and independently (1979) were the first to correctly exploit the measurement model in the case of sensor arrays of arbitrary form. Schmidt, in particular, accomplished this by first deriving a complete geometric solution in the absence of noise, then cleverly extending the geometric concepts to obtain a reasonable approximate solution in the presence of noise. The resulting algorithm was called MUSIC (MUltiple SIgnal Classification) and has been widely studied. In a detailed evaluation based on thousands of simulations, the Massachusetts Institute of Technology's Lincoln Laboratory concluded that, among currently accepted high-resolution algorithms, MUSIC was the most promising and a leading candidate for further study and actual hardware implementation. However, although the performance advantages of MUSIC are substantial, they are achieved at a cost in computation (searching over parameter space) and storage (of array calibration data).",
            "score": 61.3893256187439
        },
        {
            "docid": "1326926_29",
            "document": "Array processing . While the background and receiver noise in the assumed data model can be thought of as emanating from a large number of independent noise sources, the same is usually not the case for the emitter signals. It therefore appears natural to model the noise as a stationary Gaussian white random process whereas the signal waveforms are deterministic (arbitrary) and unknown. According to the Deterministic ML the signals are considered as unknown, deterministic quantities that need to be estimated in conjunction with the direction of arrival. This is a natural model for digital communication applications where the signals are far from being normal random variables, and where estimation of the signal is of equal interest.",
            "score": 71.30701279640198
        },
        {
            "docid": "317018_6",
            "document": "Quantization (signal processing) . An analog-to-digital converter (ADC) can be modeled as two processes: sampling and quantization. Sampling converts a time-varying voltage signal into a discrete-time signal, a sequence of real numbers. Quantization replaces each real number with an approximation from a finite set of discrete values. Most commonly, these discrete values are represented as fixed-point words. Though any number of quantization levels is possible, common word-lengths are 8-bit (256 levels), 16-bit (65,536 levels) and 24-bit (16.8\u00a0million levels). Quantizing a sequence of numbers produces a sequence of quantization errors which is sometimes modeled as an additive random signal called quantization noise because of its stochastic behavior. The more levels a quantizer uses, the lower is its quantization noise power.",
            "score": 66.97428059577942
        }
    ],
    "r": [
        {
            "docid": "30057275_3",
            "document": "Tryon's Rat Experiment . Prior to Robert Tryon\u2019s study of selective breeding, concluded in 1942, many psychologists believed that environmental, rather than genetic, differences produced individual behavioral variations. Tryon sought to demonstrate that genetic traits often did, in fact, contribute to behavior. To do so, Tryon created an experiment that tested the proficiency of successive generations of rats in completing a maze. He initiated the experiment by exposing a genetically diverse group of rats to the maze, labeling those who made the fewest errors \u201cbright\u201d, and those with the most errors \u201cdull\u201d. Tryon then mated the \u201cbright\u201d males with \u201cbright\u201d females, and \u201cdull\u201d males with \u201cdull\u201d females. After their children matured, Tryon repeated the maze test with them, and again separated the \u201cbright\u201d and the \u201cdull\u201d, again breeding \u201cbright\u201d with \u201cbright\u201d and \u201cdull\u201d with \u201cdull\u201d. Tryon continued this process for seven generations, creating two distinct breeds of \u201cbright\u201d and \u201cdull\u201d rats. In order to demonstrate that behavior had little effect on the genetically selectively bred rats, and lessen the chance of error when making his conclusions, Tryon cross-fostered the rats\u2014that is, he had a \u201cdull\u201d mother raise \u201cbright\u201d children, and vice versa. The independent variables in his experiment were the parental pairings, the choice of environment and parents for upbringing, and number of rats put through the maze. The dependent variable was the number of errors made by the rats in 19 trials of the maze.",
            "score": 138.41412353515625
        },
        {
            "docid": "36086848_5",
            "document": "Fear processing in the brain . It has been observed that fear can contribute to behavioral changes. One way this phenomenon has been studied is on the basis of the repeated stress model done by Camp RM et al.(among others). In this particular study, it was examined that the contribution fear conditioning may play a huge role in altering an animal's (Fischer rat's) behavior in a repeated stress paradigm. Behavioral changes that are commonly referred to as depressive-like behaviors resulted from this model of testing. After setting a control and a valid experimental design, Fischer rats were exposed daily to different stressors in a complex environment. After four days of stressor exposure, both exploratory behavior and social interaction were tested on day 5 in either the same environment or a new environment. The rats showed much decreased exploration and social interaction when tested in different contexts compared to control rats. To further make a correlation to the biochemistry (as mentioned below), chronic infusion of propranolol (beta-adrenergic receptor antagonist) prevented the behavioral changes following repeated stressor exposure thus halting long term potentiation. Some physiological changes also occurred including the decrease in body weight gain and adrenal hypertrophy observed in animals exposed to stress. Overall, the conditioned fear responses can contribute to behavioral changes in a repeated stress paradigm. This can be extended to correlate to other animals as well but with varying degrees of responses.",
            "score": 135.09616088867188
        },
        {
            "docid": "26685741_39",
            "document": "Sleep and memory . REM sleep is known for its vivid creations and similarity to the bioelectric outputs of a waking person. This stage of sleep is characterized by muscle atonia, fast but low voltage EEG and, as the name suggests, rapid eye movement. It is difficult to attribute memory gains to a single stage of sleep when it may be the entire sleep cycle that is responsible for memory consolidation. Recent research conducted by Datta et al. used an avoidance task followed by a post-training REM sleep period to examine changes in P waves affecting reprocessing of recently acquired stimuli. It was found that not only were the P waves increased during post-training sleep but also the density of the waves. These findings may imply that P waves during REM sleep may help to activate critical forebrain and cortical structures dealing with memory consolidation. In a Hennevin et al. study, 1989, the mesencephalic reticular formation (MRF) was given light electrical stimulation, during REM sleep, which is known to have an advantageous effect for learning when applied after training. The rats in the experiment were trained to run a maze in search of a food reward. One group of rats was given non-awakening MRF electrical stimulations after each of their maze trials compared to a control group which did not receive any electrical stimulation. It was noticed that the stimulated rats performed significantly better in respect to error reduction. These findings imply that dynamic memory processes occur both during training as well as during post-training sleep. Another study by Hennevin et al. (1998) conditioned rats to fear a noise that is associated with a subsequent foot shock. The interesting part of the experiment is that fear responding to the noise (measured in the amygdala) was observed when the noise was presented during REM sleep. This was compared to a group of pseudo-conditioned rats who did not display the same amygdalar activation during post-training sleep. This would suggest that neural responding to previously salient stimuli is maintained even during REM sleep. There is no shortage of research conducted on the effects that REM sleep has on the working brain but consistency in the findings is what plagues recent research. There is no guarantee as to what functions REM sleep may perform for our bodies and brains but modern research is always expanding and assimilating new ideas to further our understanding of such processes.",
            "score": 134.74618530273438
        },
        {
            "docid": "425938_29",
            "document": "Animal cognition . The use of rules has sometimes been considered an ability restricted to humans, but a number of experiments have shown evidence of simple rule learning in primates and also in other animals. Much of the evidence has come from studies of sequence learning in which the \"rule\" consists of the order in which a series of events occurs. Rule use is shown if the animal learns to discriminate different orders of events and transfers this discrimination to new events arranged in the same order. For example, Murphy \"et al.\" (2008) trained rats to discriminate between visual sequences. For one group ABA and BAB were rewarded, where A=\"bright light\" and B=\"dim light\". Other stimulus triplets were not rewarded. The rats learned the visual sequence, although both bright and dim lights were equally associated with reward. More importantly, in a second experiment with auditory stimuli, rats responded correctly to sequences of novel stimuli that were arranged in the same order as those previously learned. Similar sequence learning has been demonstrated in birds and other animals as well.",
            "score": 131.29150390625
        },
        {
            "docid": "19337310_46",
            "document": "Rodent . Laboratory (brown) rats may have the capacity for metacognition\u2014to consider their own learning and then make decisions based on what they know, or do not know, as indicated by choices they make apparently trading off difficulty of tasks and expected rewards, making them the first animals other than primates known to have this capacity, but these findings are disputed, since the rats may have been following simple operant conditioning principles, or a behavioral economic model. Brown rats use social learning in a wide range of situations, but perhaps especially so in acquiring food preferences.",
            "score": 123.8615951538086
        },
        {
            "docid": "26565579_50",
            "document": "Neuroscience of free will . Multivariate pattern analysis using EEG has suggested that an evidence based perceptual decision model may be applicable to free will decisions. It was found that decisions could be predicted by neural activity immediately after stimulus perception. Furthermore, when the participant was unable to determine the nature of the stimulus the recent decision history predicted the neural activity (decision). The starting point of evidence accumulation was in effect shifted towards a previous choice (suggesting a priming bias). Another study has found that subliminally priming a participant for a particular decision outcome (showing a cue for 13ms) could be used to influence free decision outcomes. Likewise, it has been found that decision history alone can be used to predict future decisions. The prediction capacities of the Soon et al. (2008) experiment were successfully replicated using a linear SVM model based on participant decision history alone (without any brain activity data). Despite this, a recent study has sought to confirm the applicability of a perceptual decision model to free will decisions. When shown a masked and therefore invisible stimulus, participants were asked to either guess between a category or make a free decision for a particular category. Multivariate pattern analysis using fMRI could be trained on \"free decision\" data to successfully predict \"guess decisions\", and trained on \"guess data\" in order to predict \"free decisions\" (in the precuneus and cuneus region).",
            "score": 123.13951110839844
        },
        {
            "docid": "10567836_12",
            "document": "Ordinal numerical competence . Experiments have shown that rats are able to be trained to press one lever after hearing two bursts of white noise, then press another lever after four bursts of white noise. The interburst interval is varied between trials so the discrimination is based on number of bursts and not time duration of the sequence. Studies show that rats as well as pigeons learned to make different responses to both short and long durations of signals. During testing, rats exhibited a pattern called \"break-run-break\"; when it came to responding after a stint of little to no response, they would suddenly respond in high frequency, then return to little or no response activity.  Data suggests that rats and pigeons are able to process time and number information at the same time. The Mode Control Model shows that these animals can process number and time information by transmission pulses to accumulators controlled by switches that operate different modes.",
            "score": 122.82012176513672
        },
        {
            "docid": "448244_11",
            "document": "Caudate nucleus . Meanwhile, behavioral studies provide another layer to the argument: recent studies suggest that the caudate is fundamental to goal direction action, that is, \"the selection of behavior based on the changing values of goals and a knowledge of which actions lead to what outcomes.\" One such study presented rats with levers that triggered the release of a cinnamon flavored solution. After the rats learned to press the lever, the researchers changed the value of the outcome (the rats were taught to dislike the flavor either by being given too much of the flavor, or by making the rats ill after drinking the solution) and the effects were observed. Normal rats pressed the lever less frequently, while rats with lesions in the caudate did not suppress the behavior as effectively. In this way, the study demonstrates the link between the caudate and goal-directed behavior; rats with damaged caudate nuclei had difficulty assessing the changing value of the outcome. In a 2003 human behavioral study, a similar process was repeated, but the decision this time was whether or not to trust another person when money was at stake. While here the choice was far more complex\u2013\u2013the subjects were not simply asked to press a lever, but had to weigh a host of different factors\u2013\u2013at the crux of the study was still behavioral selection based on changing values of outcomes.",
            "score": 121.79718017578125
        },
        {
            "docid": "7214278_15",
            "document": "Decision field theory . The Decision Field Theory has demonstrated an ability to account for a wide range of findings from behavioral decision making for which the purely algebraic and deterministic models often used in economics and psychology cannot account. Recent studies that record neural activations in non-human primates during perceptual decision making tasks have revealed that neural firing rates closely mimic the accumulation of preference theorized by behaviorally-derived diffusion models of decision making.",
            "score": 120.93909454345703
        },
        {
            "docid": "18771661_4",
            "document": "Knockout rat . Mice, rats, and humans share all but approximately 1% of each other's genes making rodents good model organisms for studying human gene function. Both mice and rats are relatively small, easily handled, have a short generation time, and are genetically inbred. While mice have proven to be a useful rodent model and techniques have been developed for routine disruption of their genes, in many circumstances rats are considered a superior laboratory animal for studying and modeling human disease.  Rats are physiologically more similar to humans than are mice. For example, rats have a heart rate more similar to that of humans, while mice have a heart rate five to ten times as fast. It is widely believed that the rat is a better model than the mouse for human cardiovascular disease, diabetes, arthritis, and many autoimmune, neurological, behavioral, and addiction disorders. In addition, rat models are superior to mouse models for testing the pharmacodynamics and toxicity of potential therapeutic compounds, partially because the number and type of many of their detoxifying enzymes are very similar to those in humans. Their larger size makes rats more conducive to study by instrumentation, and also facilitates manipulation such as blood sampling, nerve conduction, and performing surgeries.",
            "score": 117.04586791992188
        },
        {
            "docid": "53034689_8",
            "document": "Cincinnati Water Maze . Since water mazes have been used mostly with rats and mice, the extrapolation of research data from these experiments to other organisms and humans is limited. The Cincinnati Water Maze (CWM) poses some weaknesses to experimenters as it includes more variables that must be accounted for when drawing conclusions of the rat\u2019s cognitive behavior. For example, the element of water brings in a different stimulus than that which is present in simple grounded mazes. As such, the baseline behavioral characteristic of the rat must be noted prior to conducting a trial within this maze, as the rat\u2019s behavior outside the maze will differ from the behavior displayed when it is forced to swim within the maze. Additionally, previous research has shown that the CWM has a steeper learning curve compared to its Morris Water Maze counterpart; making the initial data collected less useful.",
            "score": 116.90131378173828
        },
        {
            "docid": "36476254_3",
            "document": "T-maze . The T-maze is one of a group of various mazes of differing sizes and many shapes. It is one of the most simple, consisting of just two turns - right or left. The maze is only able to be altered by blocking one of the two paths. The basis behind the T-maze is to place the rat at the base of the maze. By placing a reward at one arm or both arms of the maze, the rat must make the choice of which path to take. The decision made by the rat can be a cause of a natural preference within the rat. A study of alternation can be performed by repeating the experiment multiple times with no reward in either arm of the maze. Another experiment that can be performed is the alternation of rewards each time the experiment is performed, proving the rat will choose the arm that was not visited each time the experiment starts.",
            "score": 116.51458740234375
        },
        {
            "docid": "17258308_3",
            "document": "Two-alternative forced choice . There are various manipulations in the design of the task, engineered to test specific behavioral dynamics of choice. In one well known experiment of attention that examines the attentional shift, the Posner Cueing Task uses a 2AFC design to present two stimuli representing two given locations. In this design there is an arrow that cues which stimulus (location) to attend to. The person then has to make a response between the two stimuli (locations) when prompted. In animals, the 2AFC task has been used to test reinforcement probability learning, for example such as choices in pigeons after reinforcement of trials. A 2AFC task has also been designed to test decision making and the interaction of reward and probability learning in monkeys. Monkeys were trained to look at a center stimulus and were then presented with two salient stimuli side by side. A response can then be made in the form of a saccade to the left or to the right stimulus. A juice reward is then administered after each response. The amount of juice reward is then varied to modulate choice.",
            "score": 115.69313049316406
        },
        {
            "docid": "2515883_5",
            "document": "Remote control animal . Several studies have examined the remote control of rats using micro-electodes implanted into their brains and rely on stimulating the reward centre of the rat. Three electrodes are implanted; two in the ventral posterolateral nucleus of the thalamus which conveys facial sensory information from the left and right whiskers, and a third in the medial forebrain bundle which is involved in the reward process of the rat. This third electrode is used to give a rewarding electrical stimulus to the brain when the rat makes the correct move to the left or right. During training, the operator stimulates the left or right electrode of the rat making it \"feel\" a touch to the corresponding set of whiskers, as though it had come in contact with an obstacle. If the rat then makes the correct response, the operator rewards the rat by stimulating the third electrode.",
            "score": 115.39321899414062
        },
        {
            "docid": "29109801_4",
            "document": "Methastyridone . \"Merck\u2019s behavioral psychopharmacology screening program finally identified one highly promising new antidepressant. Code named MK-202, the chemical increased lever-pressing work output under a range of conditions, in seemingly more adaptive ways than amphetamine. For instance, in the \u201cstrained fixed-ratio\u201d test, designed to measure \u201can animal\u2019s ability to handle an overly large workload with inadequate motivation,\u201d MK-202 performed better than dextroamphetamine. Here, hungry rats were given a drop of condensed milk only after pressing a lever two hundred times in response to a light signal. However, in the middle of their heavy and under-rewarded task a second light would turn on intermittently, and if they immediately responded by pressing a second lever they would get a milk drop instantly. Thus, this experiment measured both willingness to do \u201ca particularly long and tedious job\u201d as well as \u201calertness\u201d to a second stimulus, according to Merck researchers. The rats on amphetamine performed well on the repetitive task but tended to miss the second stimulus; not so the rats on MK-202. Given the similarity between the rat\u2019s situation and the repetitive work that most people must endure to make a living, a drug that increased lever pressing without producing unresponsiveness would seem a likely antidepressant\u2014provided we accept that inefficiency in unrewarding jobs indicates psychiatric depression.\" \"This implicit identification of impaired work efficiency with depressive illness, inscribed in the use of amphetamine-boosted lever pressing as the benchmark that subsequent antidepressants had to meet, applied to the highest level executive type of work also. (The business world is called a \u201crat race\u201d with reason!) This is evident from another test, designed to measure a rat\u2019s capacity to perform complex tasks. Here, to get a reward, rats had to press a lever rapidly when a white light was on, slowly when a red light was on, and not at all when both lights were on. The rats on amphetamine pressed their levers fast no matter what lights were on, but the rats on MK-202 only pressed fast when high speed was rewarded. In these and a half a dozen other experiments with trained rats subject to diabolically ingenious \u201creinforcement schedules\u201d (that is, particular programs of reward and punishment), MK-202 outperformed amphetamine for boosting work output, maximizing reward, and minimizing punishment, particularly when tasks were both difficult and unrewarding. A more promising antidepressant drug candidate could hardly be imagined, and in January 1960 the behavioral psychopharmacology unit passed it on for human testing as an antidepressant, with its highest recommendation.\"",
            "score": 115.25755310058594
        },
        {
            "docid": "425938_56",
            "document": "Animal cognition . In a study that used this approach, rats that were playfully tickled responded differently than rats that were simply handled. The rats that had been tickled were more optimistic than the handled rats. The authors suggested that they had demonstrated \"...for the first time a link between the directly measured positive affective state and decision making under uncertainty in an animal model\".",
            "score": 114.24337768554688
        },
        {
            "docid": "442101_6",
            "document": "Edward C. Tolman . A key paper by Tolman, Ritchie and Kalish in 1946 demonstrated that rats learned the layout of a maze, which they explored freely without reinforcement. After some trials, a food item was placed to a certain point of the maze, and the rats learned to navigate to that point very quickly. However, Hull and his followers were able to produce alternative explanations of Tolman's findings, and the debate between S-S and S-R learning theories became increasingly complicated. Skinner's iconoclastic paper of 1950, entitled \"Are theories of learning necessary?\", persuaded many psychologists interested in animal learning that it was more productive to focus on the behavior itself rather than using it to make hypotheses about mental states. The influence of Tolman's ideas faded temporarily in the later 1950s and 1960s. However, his achievements had been considerable. His 1938 and 1955 papers, produced to answer Hull's charge that he left the rat \"buried in thought\" in the maze, unable to respond, anticipated and prepared the ground for much later work in cognitive psychology, as psychologists began to discover and apply decision theory \u2013 a stream of work that was recognized by the award of a Nobel prize to Daniel Kahneman in 2002. In his 1948 paper \"Cognitive Maps in Rats and Men\", Tolman introduced the concept of a cognitive map, which has found extensive application in almost every field of psychology, frequently among scientists who are unaware that they are using the early ideas that were formulated to explain the behavior of rats in mazes. Tolman assessed both response learning and place learning. Response learning is when the rat knows that the response of going a certain way in the maze will always lead to food; place learning is when the rats learn to associate the food in a specific spot each time. In his trials he observed that all of the rats in the place-learning maze learned to run the correct path within eight trials and that none of the response-learning rats learned that quickly, and some did not even learn it at all after seventy-two trials.",
            "score": 111.84612274169922
        },
        {
            "docid": "265752_42",
            "document": "Decision-making . A common laboratory paradigm for studying neural decision-making is the two-alternative forced choice task (2AFC), in which a subject has to choose between two alternatives within a certain time. A study of a two-alternative forced choice task involving rhesus monkeys found that neurons in the parietal cortex not only represent the formation of a decision but also signal the degree of certainty (or \"confidence\") associated with the decision. Another recent study found that lesions to the ACC in the macaque resulted in impaired decision-making in the long run of reinforcement guided tasks suggesting that the ACC may be involved in evaluating past reinforcement information and guiding future action. A 2012 study found that rats and humans can optimally accumulate incoming sensory evidence, to make statistically optimal decisions.",
            "score": 111.1750259399414
        },
        {
            "docid": "739262_10",
            "document": "Neural correlate . Neurophysiological studies in animals provided some insights on the neural correlates of conscious behavior. Vernon Mountcastle, in the early 1960s, set up to study this set of problems, which he termed \"the Mind/Brain problem\", by studying the neural basis of perception in the somatic sensory system. His labs at Johns Hopkins were among the first, along with Edward V.Evarts at NIH, to record neural activity from behaving monkeys. Struck with the elegance of SS Stevens approach of magnitude estimation, Mountcastle's group discovered three different modalities of somatic sensation shared one cognitive attribute: in all cases the firing rate of peripheral neurons was linearly related to the strength of the percept elicited. More recently, Ken H. Britten, William T. Newsome, and C. Daniel Salzman have shown that in area MT of monkeys, neurons respond with variability that suggests they are the basis of decision making about direction of motion. They first showed that neuronal rates are predictive of decisions using signal detection theory, and then that stimulation of these neurons could predictably bias the decision. Such studies were followed by Ranulfo Romo in the somatic sensory system, to confirm, using a different percept and brain area, that a small number of neurons in one brain area underlie perceptual decisions.",
            "score": 110.59088897705078
        },
        {
            "docid": "21488622_21",
            "document": "Self-administration . A recent study published in Nature showed an upregulation of microRNA-212 in the dorsal striatum of rats previously exposed to cocaine for extended periods. Animals infected with a viral vector overexpressing miR-212 in the dorsal striatum produced the same initial levels of cocaine intake; however, drug consumption progressively decreased as net cocaine exposure increased. The authors of the study noted that viral-infected animals exhibited decreased operant responding during the post-infusion time-out period and proposed that this demonstrated a reduction in compulsive drug-seeking behavior.(Hollander \"et al.\") miR-212 acts through Raf1 to enhance the CREB response; CREB-TORC is known to negatively regulate the reinforcing effects of cocaine. (Hollander \"et al.\") This study provides one example (miR-212, owing to its amplification of CREB) of a self-administration study that may provide potential therapeutic targets for the treatment of cocaine addiction. One of the most important advances to emerge from self-administration studies comes from a behavioral model for addiction in animals. This model relies on observation of three separate phenomena to classify a rat as \u201caddicted:\u201d 1) Persistence in drug-seeking: Depends on the attempts of rats to obtain drug during time-out or no-periods in the self-administration apparatus. 2) Resistance to punishment: Measured by how much rats maintain rates of self-administration when cocaine infusion is paired with an electric shock. 3) Motivation for the drug: Measured by the breakpoint in progressive ratio reinforcement. (Deroche-Gamonet et al.)",
            "score": 110.31602478027344
        },
        {
            "docid": "41578765_18",
            "document": "Paul Glimcher . Glimcher\u2019s research aims to describe the neural events that underlie behavioral decision-making using tools from neuroscience, psychology, and economics. His research merges psychological and economic models with computational neuroscience, including pioneering uses of fMRI (function magnetic resonance imaging) for behavioral science, to understand how value is encoded in the brain and how the brain uses those neural representations of value to guide decision-making; for example, how the brain carries out delay discounting or action-selection in the face of both risk and ambiguity. His laboratory in NYU\u2019s Center for Neural Science uses a wide range of methods including cohort studies in experimental economics, brain imaging, and single-neuron studies in non-human animals.",
            "score": 109.39037322998047
        },
        {
            "docid": "53034689_7",
            "document": "Cincinnati Water Maze . The Cincinnati Water Maze is most often used to measure the escape latency, which is the time required for the subject to escape the maze. Researchers may also measure the number of errors the subject makes, which are counted when the subject moves into dead ends. The animal will typically be put in the same maze for multiple trials per day with the goal being to assess the rat\u2019s procedural learning. The subject must learn the maze because it is unable to simply follow a random path, as with a standard maze. By studying the escape latency of the animal, researchers have a standardized test for the rate of learning in a subject. CWMs are especially useful because they are a direct test of egocentric learning/navigation. Without external visual cues, the mouse is forced to remember their movement in previous trials to escape. This is useful for determining the effect of drugs on short term memory creation, or egocentric learning as previously mentioned. The test is also useful in mapping areas of the brain where spatial learning occurs by recording areas of brain activity during the test. In one variation, adding light, or other visual cues, researchers may measure allocentric learning/memory. With this procedure, the test becomes similar to the Morris Water Maze, where spatial learning is tested. Furthermore, in this variation, the rat is able to use both allocentric and egocentric cues to escape. This is particularly useful for studying spatial memory, as the mouse is able to use both types of navigational cues.",
            "score": 108.83207702636719
        },
        {
            "docid": "1225841_31",
            "document": "Dog training . Strictly following the model set out in the \"Koehler Method of Dog Training\", some 50 years later, the Koehler method continues to be taught in both class and private training formats. The method is based in the philosophy that a dog acts on its right to choose its actions. Koehler explained that a dog's learned behavior is an act of choice based on its own learning experience. When those choices are influenced by the expectation of reward, the behavior will most likely be repeated, and when those choices are influenced by the anticipation of punishment, they will most likely cease. Once the dog has learned that its choices result in comfort or discomfort it can be taught to make the correct decisions. Action\u2192Memory\u2192Desire encapsulates the learning pattern used by the method; the dog acts, remembers the consequences, and forms the desire to repeat or avoid those consequences. Adherents believe that once the behavior has been correctly taught, it should be performed, thus making any correction, fair, reasonable, and expected. While the model has been used consistently since 1962, some of the punishment procedures described in the book are now not considered necessary, humane, or appropriate by many trainers.",
            "score": 108.62379455566406
        },
        {
            "docid": "3766002_20",
            "document": "Orbitofrontal cortex . Animal models, and cell specific manipulations in relation to drug seeking behavior implicate dsyfunction of the OFC in addiction. Substance abuse disorders are associated with a variety of deficits related to flexible goal directed behavior and decision making. These deficits overlap with symptoms related to OFC lesions, and are also associated with reduced orbitofrontal grey matter, resting state hypometabolism, and blunted OFC activity during tasks involving decision making or goal directed behavior. In contrast to resting state and decision related activity, cues associated with drugs evoke robust OFC activity that correlates with craving. Rodent studies also demonstrate that lOFC to BLA projections are necessary for cue induced reinstatement of self administration. These findings are all congruent with the role that the OFC plays in encoding the outcomes associated with certain stimuli. The progression towards compulsive substance abuse may reflect a shift between model based decision making, where an internal model of future outcomes guides decisions, to model free learning, where decisions are based on reinforcement history. Model based learning involves the OFC and is flexible and goal directed, while model free learning is more rigid; as shift to more model free behavior due to dysfunction in the OFC, like that produced by drugs of abuse, could underlie drug seeking habits.",
            "score": 108.1580581665039
        },
        {
            "docid": "402652_21",
            "document": "Compulsive hoarding . Some evidence based on brain lesion case studies also suggests that the anterior ventromedial prefrontal and cingulate cortices may be involved in abnormal hoarding behaviors, but sufferers of such injuries display less purposeful behavior than other individuals who hoard compulsively, thus making the involvement of these brain structures unclear. Other neuropsychological factors that have been found to be associated with individuals exhibiting hoarding behaviors include slower and more variable reaction times, increased impulsivity, and decreased spatial attention. A study comparing neural activity in hoarders, people with OCD, and a control group when deciding to throw possessions away found that when hoarders were trying to decide to throw away their own possessions, they had lower activity in the anterior cingulate cortex and insula regions of the brain. The study suggested this lower activity was related to \"problems in identifying the emotional significance of a stimulus, generating appropriate emotional response, or regulating affective state during decision making.\" Hoarders had normal levels of activity in those regions when making decisions about possessions that did not belong to them.",
            "score": 107.62226104736328
        },
        {
            "docid": "30057275_4",
            "document": "Tryon's Rat Experiment . While Tryon's results showed that the \u201cbright\" rats made significantly fewer errors in the maze than the \u201cdull\" rats did, the question exists of what other sensory, motor, motivational, and learning processes also influenced the results of the experiment. A common misconception of this experiment and other similar experiments is that the observed change in the performance in the maze directly correlates with general learning ability. This is not the case. Rather, it has become a widely accepted belief among behavior geneticists that the superiority of the bright rats was confined to Tryon\u2019s specific test; thus, it is not possible to claim that there is a difference in learning capacity between the two groups of rats. Genetic variation, such as better peripheral vision, can make some rats \u201cbright\u201d and others \u201cdull\u201d, but does not determine their intelligence. Nonetheless, Tryon\u2019s famous rat-maze experiment demonstrated that the difference between rat performances was genetic since their environments were controlled and identical.",
            "score": 106.88101196289062
        },
        {
            "docid": "393535_12",
            "document": "Coolidge effect . Though there is no single reason for why males will choose a novel partner, there have been experiments that show that the major determining factor for detecting a novel partner is through olfactory preference. An experiment using Long-Evans rats, showed that odour played a major role in distinguishing the difference between a novel partner and familiar partner. In their experiment, Carr et al. paired each male rat with a female rat and allowed them to mate. Male rats were then tested for preference through the use of an apparatus which had two cylinders that were attached to their home cage, and contained the familiar female and the novel female in each cylinder. Caps at the end of these cylinders prevented access to the females, but had a hole in them to allow their odours to pass through to the male's cage. Before the testing phase, the females were removed, and then the caps were removed to allow the male to explore both cylinders. From this experiment, they found that males preferred the scent of the novel female. While these males did not have access to these females to demonstrate mating preferences, this odour preference is believed to reflect promiscuous behaviour, and therefore be important to the male mating strategy. In an earlier experiment, also conducted by Carr et al., they found that unlike male rats, female rats preferred the odour of a familiar partner rather than the odour of a novel partner. Another study also examined not only olfactory preference, but what part of the brain targeted the olfactory preference. In this study, male hamsters were given lesions to either the hippocampus or the perirhinal-entorhinal cortex, or received a sham treatment. Then the hamsters were allowed to mate with a female hamster until they became satiated. All subjects were then presented with two anesthetized females, one was the female they had previously copulated with, and the other was a novel female. Hamsters with sham and hippocampal lesions investigated the anogenital region of the novel females for a significantly longer period of time in comparison to the familiar female. Males with lesions to the perirhinal-entorhinal cortex did not show a preference for either a familiar or novel female, and spent a similar amount of time investigating the anogenital region of both females. The results from this study revealed that the perirhinal-entorhinal cortex region of the brain in golden hamsters is crucial for the recognition of familiar conspecifics and certain social behaviors. The conclusion from this experiment was also consistent in rats and monkeys, since damage to this region of the brain impaired standard recognition memory, which would suggest that the hippocampal region of the brain is not crucial in social behavior memory, but rather, the perirhinal-entorhinal cortex.",
            "score": 106.5839614868164
        },
        {
            "docid": "19337310_45",
            "document": "Rodent . Because laboratory mice (house mice) and rats (brown rats) are widely used as scientific models to further our understanding of biology, a great deal has come to be known about their cognitive capacities. Brown rats exhibit cognitive bias, where information processing is biased by whether they are in a positive or negative affective state. For example, laboratory rats trained to respond to a specific tone by pressing a lever to receive a reward, and to press another lever in response to a different tone so as to avoid receiving an electric shock, are more likely to respond to an intermediate tone by choosing the reward lever if they have just been tickled (something they enjoy), indicating \"a link between the directly measured positive affective state and decision making under uncertainty in an animal model.\"",
            "score": 106.5562744140625
        },
        {
            "docid": "22209340_6",
            "document": "Laughter in animals . Rats emit long, 50-kHz ultrasonic calls that are induced during rough and tumble play, and when tickled by humans. The vocalization is described as distinct \"chirping\". Like humans, rats have \"tickle skin\", areas of the body that generate greater laughter responses than others. Rats that laugh the most also play the most and prefer to spend more time with other laughing rats. It has been reported that there is no decline in the tendency to laugh and respond to tickle skin as rats age, however, it has also been reported that in females, brain maturation after puberty appears to redefine tickling as aversive, leading to avoidance rather than appetitive responses. Further studies show that rats chirp when wrestling one another, before receiving morphine, or when mating. The sound has been interpreted as an expectation of something rewarding. High frequency ultrasonic vocalizations are important in rat communication and function to elicit approach behavior in the recipient.",
            "score": 105.91908264160156
        },
        {
            "docid": "41420328_12",
            "document": "Interactions between the emotional and executive brain systems . The dorsolateral prefrontal cortex (dlPFC) and the dorsomedial prefrontal cortex (dmPFC) are implicated in the enhancement of representations of stimuli relevant to current decisions, behaviors or tasks. These areas also play a role in modulating emotions and dealing with emotional distractions during demanding tasks, and are also implicated in facilitating decision/resolve perceptual or conflict making by augmenting representations of stimuli relevant to decision or behavior. The dmPFC\u2019s role in human emotional regulation decision making (decision conflict perspective \u2013 levels of indecision) e.g. Picking between similar items, acting in novel situations. There is also evidence of an inverse relationship between activation in the dPFC areas and activation in emotionally activated brain areas.",
            "score": 105.89971923828125
        },
        {
            "docid": "25259540_9",
            "document": "Freezing behavior . Methamphetamines have also shown that they could affect freezing behavior. Tsuchiya et al. conducted a study investigating the effect of methamphetamine pretreatment on freezing behavior. Rats were given the drug over a week, ramping up the doses. After that, there was a 5-day period without any drugs administered. The rats were then subjected to conditioned fear stress. Repeated but not single methamphetamine pretreatment resulted in a significantly increased freezing behavior. This evidence suggests that previous exposure to chronic methamphetamine results in an increased sensitivity to subsequent stress than a control group.",
            "score": 105.14911651611328
        },
        {
            "docid": "8087746_10",
            "document": "Attribution (psychology) . The covariation model states that people attribute behavior to the factors that are present when a behavior occurs and absent when it does not. Thus, the theory assumes that people make causal attributions in a rational, logical fashion, and that they assign the cause of an action to the factor that co-varies most closely with that action. Harold Kelley's covariation model of attribution looks to three main types of information from which to make an attribution decision about an individual's behavior. The first is \"consensus information\", or information on how other people in the same situation and with the same stimulus behave. The second is \"distinctive information\", or how the individual responds to different stimuli. The third is \"consistency information\", or how frequent the individual's behavior can be observed with similar stimulus but varied situations. From these three sources of information observers make attribution decisions on the individual's behavior as either internal or external. There have been claims that people under-utilise consensus information, although there has been some dispute over this.",
            "score": 104.96794891357422
        }
    ]
}