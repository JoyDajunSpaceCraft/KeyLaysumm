{
    "q": [
        {
            "docid": "35970915_4",
            "document": "Colavita visual dominance effect . In 1974, Francis B. Colavita conducted an experiment, which provided evidence for visual dominance in humans when performing an audiovisual discrimination task. In his seminal experiment, Colavita (1974) presented participants with an auditory (tone) or visual (light) stimulus, to which they were instructed to respond by pressing the \u2018tone key\u2019 or \u2018light key\u2019 respectively. Throughout the experiment, unimodal auditory trials, unimodal visual trials and a small number of audiovisual bimodal trials were randomly presented. Colavita deceived the participants by informing them that the bimodal trials in the experiment occurred \u201caccidentally\u201d. During practice trials, Colavita would \u201caccidentally\u201d present audiovisual stimuli, and would then draw the participants\u2019 attention to what had just happened and would apologize for such \u2018accident\u2019. In addition, the participants were not instructed on how to respond on such trials or whether this type of trials would occur again  The results showed that participants had almost equivalent response times for auditory and visual stimuli in unimodal trials. Additionally, Colavita found that participants pressed the \u2018light key\u2019 in the majority of the bimodal trials. This was seen as evidence of visual dominance because participants failed to acknowledge the presence of the auditory stimulus in most bimodal trials. However, due to Colavita\u2019s deception of the \u201caccidental\u201d occurrence of bimodal trials, researchers have proposed that experimenter expectancy effects, task demands or methodological problems may have contributed to the visual dominance effect reported in Colavita\u2019s original study. Nevertheless, subsequent experiments have discontinued the use of deception, and continue to show a robust Colavita visual dominance effect.",
            "score": 101.87252056598663
        },
        {
            "docid": "35970915_15",
            "document": "Colavita visual dominance effect . In addition, a study conducted by Laurienti and colleagues showed that, under certain conditions, responses to audiovisual stimuli can be affected by semantic congruence or incongruence. More specifically, their findings showed that participants responded faster to congruent auditory and visual stimuli than to incongruent stimuli. In addition, Koppen, Alsius and Spence conducted a study which investigated whether the Colavita effect would be modulated by the semantic congruency between the visual and auditory stimulus, using stimuli of similar semantic meaning and complexity. The findings from this study showed that semantic congruency had no effect on the magnitude of the Colavita effect in the experiments, yet it had a significant effect on participants\u2019 performance in the speeded discrimination task. Participants showed a pattern that reflected difficulties with separating the auditory stimulus from the visual stimulus when these stimuli had congruent semantic meaning and were presented simultaneously. For incongruent stimuli, participants had faster response times, which could also be explained by the previously mentioned theory of \u2018Failure of Binding\u2019.",
            "score": 105.66786301136017
        },
        {
            "docid": "40703875_2",
            "document": "Estimation statistics . Estimation statistics is a data analysis framework that uses a combination of effect sizes, confidence intervals, precision planning, and meta-analysis to plan experiments, analyze data and interpret results. It is distinct from null hypothesis significance testing (NHST), which is considered to be less informative. Estimation statistics, or simply estimation, is also known as the new statistics, a distinction introduced in the fields of psychology, medical research, life sciences and a wide range of other experimental sciences where NHST still remains prevalent, despite estimation statistics having been recommended as preferable for several decades.",
            "score": 158.12478256225586
        },
        {
            "docid": "1051310_14",
            "document": "Response bias . While demand characteristics cannot be completely removed from an experiment, there are steps that researchers can take to minimize the impact they may have on the results. One way to mitigate response bias is to use deception to prevent the participant from discovering the true hypothesis of the experiment and then debrief the participants. For example, research has demonstrated that repeated deception and debriefing is useful in preventing participants from becoming familiar with the experiment, and that participants do not significantly alter their behaviors after being deceived and debriefed multiple times. Another way that researchers attempt to reduce demand characteristics is by being as neutral as possible, or training those conducting the experiment to be as neutral as possible. For example, studies show that extensive one-on-one contact between the experimenter and the participant makes it more difficult to be neutral, and go on to suggest that this type of interaction should be limited when designing an experiment. Another way to prevent demand characteristics is to use blinded experiments with placebos or control groups. This prevents the experimenter from biasing the participant, because the researcher does not know in which way the participant should respond. Although not perfect, these methods can significantly reduce the effect of demand characteristics on a study, thus making the conclusions drawn from the experiment more likely to accurately reflect what they were intended to measure.",
            "score": 122.06247568130493
        },
        {
            "docid": "9014_54",
            "document": "Developmental psychology . Developmental psychologists have a number of methods to study changes in individuals over time. Common research methods include systematic observation, including naturalistic observation or structured observation; self-reports, which could be clinical interviews or structured interviews; clinical or case study method; and ethnography or participant observation. These methods differ in the extent of control researchers impose on study conditions, and how they construct ideas about which variables to study. Every developmental investigation can be characterized in terms of whether its underlying strategy involves the \"experimental\", \"correlational\", or \"case study\" approach. The experimental method involves \"actual manipulation of various treatments, circumstances, or events to which the participant or subject is exposed; the \"experimental design\" points to cause-and-effect relationships. This method allows for strong inferences to be made of causal relationships between the manipulation of one or more independent variables and subsequent behavior, as measured by the dependent variable. The advantage of using this research method is that it permits determination of cause-and-effect relationships among variables. On the other hand, the limitation is that data obtained in an artificial environment may lack generalizability. The correlational method explores the relationship between two or more events by gathering information about these variables without researcher intervention. The advantage of using a correlational design is that it estimates the strength and direction of relationships among variables in the natural environment; however, the limitation is that it does not permit determination of cause-and-effect relationships among variables. The case study approach allows investigations to obtain an in-depth understanding of an individual participant by collecting data based on interviews, structured questionnaires, observations, and test scores. Each of these methods have its strengths and weaknesses but the experimental method when appropriate is the preferred method of developmental scientists because it provides a controlled situation and conclusions to be drawn about cause-and-effect relationships.",
            "score": 136.01005482673645
        },
        {
            "docid": "26685_17",
            "document": "Statistics . A common goal for a statistical research project is to investigate causality, and in particular to draw a conclusion on the effect of changes in the values of predictors or independent variables on dependent variables. There are two major types of causal statistical studies: experimental studies and observational studies. In both types of studies, the effect of differences of an independent variable (or variables) on the behavior of the dependent variable are observed. The difference between the two types lies in how the study is actually conducted. Each can be very effective. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation. Instead, data are gathered and correlations between predictors and response are investigated. While the tools of data analysis work best on data from randomized studies, they are also applied to other kinds of data\u2014like natural experiments and observational studies\u2014for which a statistician would use a modified, more structured estimation method (e.g., Difference in differences estimation and instrumental variables, among many others) that produce consistent estimators.",
            "score": 155.68524312973022
        },
        {
            "docid": "4031803_3",
            "document": "Social inhibition . Social inhibition can range from normal reactions to social situations to a pathological level, associated with psychological disorders like social anxiety or social phobia. Life events are important and are related to our well-being and inhibition levels. In a lab study conducted by Buck and colleagues, social inhibition in everyday life was reviewed. Researchers observed how individuals interacted and communicated about different stimuli. In this study, there were female participants called \"senders\" who viewed twelve emotionally loaded stimuli. There were also participants in the study called \"received\" who had to guess which stimuli was viewed by the senders. The senders were either alone, with a friend, or with a stranger while viewing the slides. The results of the study revealed that being with a stranger had inhibitory effects on communication, whereas being with a friend had facilitative effects with some stimuli and inhibitory effects with others. The results show how anyone can be inhibited in daily life, with strangers or even friends. Inhibition can also be determined by one's sensitivity levels to different social cues throughout the day. Gable and colleagues conducted a study in which they examined different events participants would record at the end of their day. Participants were also measured on the behavioral activation system and the behavioral inhibition system. The results revealed that individuals with more sensitivity on the behavioral inhibition system reported having more negative effects from daily events.",
            "score": 123.20290315151215
        },
        {
            "docid": "23754_78",
            "document": "Parapsychology . In January 2008 the results of a study using neuroimaging were published. To provide what are purported to be the most favorable experimental conditions, the study included appropriate emotional stimuli and had participants who are biologically or emotionally related, such as twins. The experiment was designed to produce positive results if telepathy, clairvoyance or precognition occurred, but despite this no distinguishable neuronal responses were found between psychic stimuli and non-psychic stimuli, while variations in the same stimuli showed anticipated effects on patterns of brain activation. The researchers concluded that \"These findings are the strongest evidence yet obtained against the existence of paranormal mental phenomena.\" Other studies have attempted to test the psi hypothesis by using functional neuroimaging. A neuroscience review of the studies (Acunzo \"et al\". 2013) discovered methodological weaknesses that could account for the reported psi effects.",
            "score": 131.72629404067993
        },
        {
            "docid": "35806374_26",
            "document": "Research strategies of election campaign communication research . Experiments, conducted in laboratory settings, allow the testing of effects resulting from mere exposure to controlled stimuli. This research method is primarily used in psychological approaches to election campaign communication research. Experiments can, e.g., detect the effects election campaign commercials have on the electorate. By exposing subjects to particular election spots and varying specific details in the ad, the variables causing an effect can be revealed. June Woong Rhee, e.g., makes use of two kinds of experiments within in his study on framing effects in election campaign news coverage. To examine the influence of framing effects on voters\u2019 interpretation of campaigns, a broadcast-print experiment and a broadcast-only experiment were conducted. As pretest, the participants of the study had to write a letter about the 1992 U.S. presidential election campaign. Afterwards the participants were confronted with print and broadcast news stories about the 1991 Philadelphia mayoral election campaign, which were created using a strategy or an issue frame and simulating the actual coverage. Conditions, e.g., within the broadcast-print experiment were strategy frames in broadcast and print news, issue frames in broadcast and print news, or a mixture, i.e., either an issue frame in printed news and a strategy frame in broadcasts, or strategy-framed broadcast and issue-framed print news. The participants of the experiments were \u201casked to read or watch the news stories for five days in their homes\u201d. After the five days of exposure to the manipulated news, the participants met with the experimenter and, as a posttest, had to \u201cwrite a letter about the Philadelphia mayoral campaign\u201d. To interpret the results a content analysis was conducted by Rhee. The study revealed that \u201cboth strategy-framed and issue-framed print news stories are effective in influencing campaign interpretation\u201c.",
            "score": 128.54133474826813
        },
        {
            "docid": "272134_12",
            "document": "Survey methodology . Longitudinal studies take measure of the same random sample at multiple time points. Unlike with a successive independent samples design, this design measures the differences in individual participants\u2019 responses over time. This means that a researcher can potentially assess the reasons for response changes by assessing the differences in respondents\u2019 experiences. Longitudinal studies are the easiest way to assess the effect of a naturally occurring event, such as divorce that cannot be tested experimentally. However, longitudinal studies are both expensive and difficult to do. It\u2019s harder to find a sample that will commit to a months- or years-long study than a 15-minute interview, and participants frequently leave the study before the final assessment. This attrition of participants is not random, so samples can become less representative with successive assessments. To account for this, a researcher can compare the respondents who left the survey to those that did not, to see if they are statistically different populations. Respondents may also try to be self-consistent in spite of changes to survey answers.",
            "score": 136.96627402305603
        },
        {
            "docid": "42699853_5",
            "document": "Rod and frame test . The methods of constant stimuli, limits, and adjustment can be used to test the participants, but method of limits is most commonly used in research conducted using the rod and frame task. When using the method of limits, the experimenter sets the orientation of the rod and frame separately and then the participant is asked to adjust the rod orientation until they perceive it to be vertical. Deviation from true vertical can then be determined. Based on which way the frame is tilted, the rod can be viewed as either being tilted in the same direction as the frame (direct effect), or in the opposite direction of the frame (indirect effect).",
            "score": 87.20993065834045
        },
        {
            "docid": "11864322_8",
            "document": "Quasi-experiment . Though quasi-experiments are sometimes shunned by those who consider themselves to be experimental purists (leading Donald T. Campbell to coin the term \u201cqueasy experiments\u201d for them), they are exceptionally useful in areas where it is not feasible or desirable to conduct an experiment or randomized control trial. Such instances include evaluating the impact of public policy changes, educational interventions or large scale health interventions. The primary drawback of quasi-experimental designs is that they cannot eliminate the possibility of confounding bias, which can hinder one\u2019s ability to draw causal inferences. This drawback is often used to discount quasi-experimental results. However, such bias can be controlled for using various statistical techniques such as multiple regression, if one can identify and measure the confounding variable(s). Such techniques can be used to model and partial out the effects of confounding variables techniques, thereby improving the accuracy of the results obtained from quasi-experiments. Moreover, the developing use of propensity score matching to match participants on variables important to the treatment selection process can also improve the accuracy of quasi-experimental results. In fact, data derived from quasi-experimental analyses has been shown to closely match experimental data in certain cases, even when different criteria were used. In sum, quasi-experiments are a valuable tool, especially for the applied researcher. On their own, quasi-experimental designs do not allow one to make definitive causal inferences; however, they provide necessary and valuable information that cannot be obtained by experimental methods alone. Researchers, especially those interested in investigating applied research questions, should move beyond the traditional experimental design and avail themselves of the possibilities inherent in quasi-experimental designs.",
            "score": 132.48456597328186
        },
        {
            "docid": "23339740_24",
            "document": "Mass drug administration . The deficiencies in the study designs mentioned above reflect the evolution of research methodology over the last 50 years. The evaluation of an intervention such as MDA is complicated by the fact that the effect of the intervention on transmission can only be measured at the community and not at the individual level. Trial methods which use a community, a village, or a cluster as unit of inference have taken longer to evolve than those used for individually randomized trials. There are, with some notable exceptions, few properly designed and analysed cluster randomized trials conducted by health care researchers prior to 1978. One major handicap for researchers who need to use the cluster approach, besides the need for a large sample size, is the need to use statistical methods that differ from the familiar methods used in individually randomized trials. Significant progress has been made in the development of statistical methods for the analysis of correlated data.",
            "score": 125.0035514831543
        },
        {
            "docid": "34903526_6",
            "document": "Event sampling methodology . ESM has several disadvantages. One of the disadvantages of ESM is it can sometimes be perceived as invasive and intrusive by participants. ESM also leads to possible self-selection bias. It may be that only certain types of individuals are willing to participate in this type of study creating an unrepresentative sample. Another concern is related to participant cooperation. Participants may not be actually fill out their diaries at the specified times and concern has been raised difference in diary format may be influential in compliance. However recent research found that research that focuses on mean levels, between-person differences, and correlations among variables are less likely to be impacted by the difference between electronic and paper diary methods. Research reported that some participants experience the repeated assessments as taxing. Further concerns are related to the fact that ESM may substantively change the phenomenon being studied. Reactivity or priming effects may occur, such that repeated measurement may cause changes in the participants' experiences. This method of sampling data is also highly vulnerable to common method variance.",
            "score": 118.41551351547241
        },
        {
            "docid": "3345681_12",
            "document": "Research design . In a good experimental design, a few things are of great importance. First of all, it is necessary to think of the best way to operationalize the variables that will be measured, as well as which statistical methods would be most appropriate to answer the research question. Thus, the researcher should consider what the expectations of the study are as well as how to analyse any potential results. Finally, in an experimental design the researcher must think of the practical limitations including the availability of participants as well as how representative the participants are to the target population. It is important to consider each of these factors before beginning the experiment. Additionally, many researchers employ power analysis before they conduct an experiment, in order to determine how large the sample must be to find an effect of a given size with a given design at the desired probability of making a Type I or Type II error.",
            "score": 132.10435247421265
        },
        {
            "docid": "30601657_11",
            "document": "Response priming . Given that the researcher is aware of the most influential experimental variables, the response priming method can be employed in a number of experimental variants and can contribute to the exploration of a multitude of research questions in the field of cognitive psychology. The most prevalent form of response priming employs a prime and target at the same monitor position, so that the target also serves to visually mask the prime (often by means of metacontrast). In many experiments, there are two different targets preceded by two different primes at the same monitor positions. Participants then have to discriminate the two targets and respond to the position of the task-relevant target. Sometimes, three stimulus types are employed (prime, mask, target), especially when the prime-target SOA has to be very long. Sometimes, no mask is employed at all. Primes and targets do not have to appear at the same screen position: One stimulus can flank the other, like in the Eriksen paradigm (indeed, the Eriksen effect may be a special case of response priming).",
            "score": 94.73472142219543
        },
        {
            "docid": "35970915_13",
            "document": "Colavita visual dominance effect . The Colavita effect has been shown to be affected by factors that contribute to the intermodal binding of auditory and visual stimuli during perception. These factors of interest are spatial and temporal coincidence between the auditory and visual stimuli, which modulate the Colavita effect through temporal separation and temporal order . For example, results from an experiment, conducted by Koppen and Spence (2007b), showed a larger Colavita effect when auditory and visual stimuli were presented closer together in time. When the stimuli were presented further apart in time, the Colavita effect was reduced. Their results also showed that the Colavita effect was largest when the visual stimulus was presented before the auditory stimulus during the bimodal trials. Conversely, the Colavita effect was reversed or reduced when the auditory stimulus preceded the visual stimulus . In addition, Koppen and Spence conducted an experiment in which participants showed a significantly larger Colavita effect when the auditory and visual stimuli were presented from the same spatial location, rather than from different locations. Based on these results, Koppen and his colleagues proposed that the \u2018unity effect\u2019 can adequately explain the role of spatial and temporal coincidence between stimuli in modulating the Colavita effect. According to the Unity effect, intersensory bias is greater when the participants unconsciously bind the two sensory events and believe that a single unimodal object is being perceived, rather than two separate events.",
            "score": 84.45152401924133
        },
        {
            "docid": "364299_21",
            "document": "Experimental psychology . In experiments, human participants often respond to visual, auditory or other stimuli, following instructions given by an experimenter; animals may be similarly \"instructed\" by rewarding appropriate responses. Since the 1990s, computers have commonly been used to automate stimulus presentation and behavioral measurement in the laboratory. Behavioral experiments with both humans and animals typically measure reaction time, choices among two or more alternatives, and/or response rate or strength; they may also record movements, facial expressions, or other behaviors. Experiments with humans may also obtain written responses before, during, and after experimental procedures. Psychophysiological experiments, on the other hand, measure brain or (mostly in animals) single-cell activation during the presentation of a stimulus using methods such as fMRI, EEG, PET or similar.",
            "score": 70.46850395202637
        },
        {
            "docid": "1051310_13",
            "document": "Response bias . Outside of participant motivation, there are other factors that influence the appearance of demand characteristics in a study. Many of these factors relate to the unique nature of the experimental setting itself. For example, participants in studies are more likely to put up with uncomfortable or tedious tasks simply because they are in an experiment. Additionally, the mannerisms of the experimenter, such as the way they greet the participant, or the way they interact with the participant during the course of the experiment may inadvertently bias how the participant responds during the course of the experiment. Also, prior experiences of being in an experiment, or rumors of the experiment that participants may hear can greatly bias the way they respond. Outside of an experiment, these types of past experiences and mannerisms may have significant effects on how patients rank the effectiveness of their therapist. Many of the ways therapists go about collecting client feedback involve self-reporting measures, which can be highly influenced by response bias. Participants may be biased if they fill out these measure in front of their therapist, or somehow feel compelled to answer in an affirmative matter because they believe their therapy should be working. In this case, the therapists would not be able to gain accurate feedback from their clients, and be unable to improve their therapy or accurately tailor further treatment to what the participants need. All of these different examples may have significant effects on the responses of participants, driving them to respond in ways that do not reflect their actual beliefs or actual mindset, which negatively impact conclusions drawn from those surveys.",
            "score": 108.79398143291473
        },
        {
            "docid": "1900609_24",
            "document": "External validity . The only way to be certain that the results of an experiment represent the behaviour of a particular population is to ensure that participants are randomly selected from that population. Samples in experiments cannot be randomly selected just as they are in surveys because it is impractical and expensive to select random samples for social psychology experiments. It is difficult enough to convince a random sample of people to agree to answer a few questions over the telephone as part of a political poll, and such polls can cost thousands of dollars to conduct. Moreover, even if one somehow was able to recruit a truly random sample, there can be unobserved heterogeneity in the effects of the experimental treatments... A treatment can have a positive effect on some subgroups but a negative effect on others. The effects shown in the treatment averages may not generalize to any subgroup.",
            "score": 140.26826763153076
        },
        {
            "docid": "62329_42",
            "document": "Meta-analysis . Modern statistical meta-analysis does more than just combine the effect sizes of a set of studies using a weighted average. It can test if the outcomes of studies show more variation than the variation that is expected because of the sampling of different numbers of research participants. Additionally, study characteristics such as measurement instrument used, population sampled, or aspects of the studies' design can be coded and used to reduce variance of the estimator (see statistical models above). Thus some methodological weaknesses in studies can be corrected statistically. Other uses of meta-analytic methods include the development of clinical prediction models, where meta-analysis may be used to combine data from different research centers, or even to aggregate existing prediction models.",
            "score": 154.93969130516052
        },
        {
            "docid": "37645970_11",
            "document": "Observational methods in psychology . Participate observation is characterized as either undisguised or disguised. In undisguised observation, the observed individuals know that the observer is present for the purpose of collecting info about their behavior. This technique is often used to understand the culture and behavior of groups or individuals. In contrast, in disguised observation, the observed individuals do not know that they are being observed. This technique is often used when researchers believe that the individuals under observation may change their behavior as a result of knowing that they were being recorded. For a great example of disguised research, see the Rosenhan experiment in which several researchers seek admission to twelve different mental hospitals to observe patient-staff interactions and patient diagnosing and releasing procedures.  There are several benefits to doing participant observation. Firstly, participant research allows researchers to observe behaviors and situations that are not usually open to scientific observation. Furthermore, participant research allows the observer to have the same experiences as the people under study, which may provide important insights and understandings of individuals or groups. However, there are also several drawbacks to doing participant observation. Firstly, participant observers may sometimes lose their objectivity as a result of participating in the study. This usually happens when observers begin to identify with the individuals under study, and this threat generally increases as the degree of observer participation increases. Secondly, participant observers may unduly influence the individuals whose behavior they are recording. This effect is not easily assessed, however, it generally more prominent when the group being observed is small, or if the activities of the participant observer are prominent. Lastly, disguised observation raises some ethical issues regarding obtaining information without respondents' knowledge. For example, the observations collected by an observer participating in an internet chat room discussing how racists advocate racial violence may be seen as incriminating evidence collected without the respondents\u2019 knowledge. The dilemma here is of course that if informed consent were obtained from participants, respondents would likely choose not to cooperate.",
            "score": 135.3287514448166
        },
        {
            "docid": "160361_54",
            "document": "Sampling (statistics) . Panel sampling is the method of first selecting a group of participants through a random sampling method and then asking that group for (potentially the same) information several times over a period of time. Therefore, each participant is interviewed at two or more time points; each period of data collection is called a \"wave\". The method was developed by sociologist Paul Lazarsfeld in 1938 as a means of studying political campaigns. This longitudinal sampling-method allows estimates of changes in the population, for example with regard to chronic illness to job stress to weekly food expenditures. Panel sampling can also be used to inform researchers about within-person health changes due to age or to help explain changes in continuous dependent variables such as spousal interaction. There have been several proposed methods of analyzing panel data, including MANOVA, growth curves, and structural equation modeling with lagged effects.",
            "score": 124.0933837890625
        },
        {
            "docid": "3983531_5",
            "document": "Dennis Merzel . A randomized clinical trial of Merzel's Big Mind process has been carried out as part of a masters thesis \"to test the hypothesis that a Zen training method using a self-based dialogue approach called Big Mind (Merzel, 2007) produces significant changes in subjective experience that are similar to the spiritual experiences of long-term meditators during deep meditation and, second, to examine whether the effect brings about any lasting positive psychological improvements in both spirituality and well-being measures.\" The participants appeared to score higher on various measures after participation, but the reported effects may also result from factors such as group effect, suggestibility, and/or simple expectation, and the study may have limited generalizability due to the high level of education of the participants.",
            "score": 121.74550199508667
        },
        {
            "docid": "2010793_17",
            "document": "Ceiling effect (statistics) . Because ceiling effects prevent accurate interpretation of data, it is important to attempt preventing the effects from occurring or using the presence of the effects to adjust the instrument and procedures that were used. Researchers may try to prevent ceiling effects from occurring using a number of methods. The first of which is choosing a previously validated measure by reviewing past research. If no validated measures exist, pilot testing may be conducted using the proposed methods. Pilot testing, or conducting a pilot experiment, involves a small-scale trial of instruments and procedures prior to the actual experiment, allowing for the recognition that adjustments should be made for the most efficient and accurate data collection. If researchers are using a design that is not previously validated, a combination of surveys, involving that originally-proposed and another supported by past literature, may be used to assess for the presence of ceiling effects. If any research, especially the pilot study, shows a ceiling effect, efforts should be made to adjust the instrument so that the effect may be mitigated and informative research can be conducted.",
            "score": 123.4052118062973
        },
        {
            "docid": "35970915_6",
            "document": "Colavita visual dominance effect . Colavita also varied the intensity of the visual and the auditory stimuli to determine whether matching the intensity of both stimuli, or if increasing the intensity of only the auditory stimulus would decrease the occurrence of the Colavita effect. However, none of the experimental manipulations regarding the intensity of the stimuli decreased the occurrence of the Colavita effect.  Further research has been conducted to replicate Colavita\u2019s experiment and to extend the Colavita effect to more complex stimuli. For example, Sinnett, Spence and Soto-Faraco conducted an experiment in 2007, in which pictures and sounds were used as stimuli instead of the light and tone. The rationale for using more complex stimuli was that this type of stimuli would increase perceptual load, requiring more attentional resources. The findings from this study showed that the Colavita effect continues to occur when stimuli become more complex.",
            "score": 80.4169499874115
        },
        {
            "docid": "35970915_16",
            "document": "Colavita visual dominance effect . Previous research has shown that people with one eye have enhanced spatial vision, implying that vision in the remaining working eye compensates for the loss of the simultaneous use of both eyes. Furthermore, individuals who have lost the ability to use one sensory system develop an enhanced ability in the use of the remaining senses. It is thought that intact sensory systems may adapt and compensate for the loss of one of the senses. However, little is known about cross-sensory adaption in cases of developmental partial sensory deprivation, such as monocular enucleation, where individuals have one eye surgically removed early in life. In an experiment, Moro and Steeves tested whether participants with one eye showed the Colavita visual dominance effect, and compared their performance to binocular viewers (use of both eyes) and monocular (eye-patched) control participants. In their experiment, Moro and Steeves used a stimulus detection and discrimination task, which had three conditions: unimodal visual targets, unimodal auditory targets, and bimodal (visual and auditory presented together) targets. The binocular and monocular participants both displayed the Colavita visual dominance effect; however the monocular enucleation group did not. Moro and Steeves demonstrated that people with one eye show equivalent auditory and visual processing, compared with binocular and monocular viewing controls, when asked to discriminate between audio, visual, and bimodal stimuli.",
            "score": 81.74770307540894
        },
        {
            "docid": "10515_10",
            "document": "Extrasensory perception . There are many criticisms pertaining to experiments involving extrasensory perception, particularly surrounding methodological flaws. These flaws are not unique to a single experimental design, and are effective in discrediting much of the positive research surrounding ESP. Many of the flaws seen in the Zener cards experiment are present in the Ganzfeld experiment as well. First is the stacking effect, an error that occurs in ESP research. Trial-by-trial feedback given in studies using a \u201cclosed\u201d ESP target sequence (e.g., a deck of cards) violates the condition of independence used for most standard statistical tests. Multiple responses for a single target cannot be evaluated using statistical tests that assume independence of responses. This increases likelihood of card counting and in turn, increases the chances for the subject to guess correctly without using ESP. Another methodological flaw involves cues through sensory leakage. For example, when the subject receives a visual cue. This could be the reflection of a Zener card in the holder\u2019s glasses. In this case, the subject is able to guess the card correctly because they can see it in the reflection, not because of ESP. Finally, poor randomization of target stimuli could be happening. Poor shuffling methods can make the orders of the cards easier to predict, or the cards could\u2019ve been marked and manipulated, again, making it easier to predict which cards come next. The results of a meta-analysis found that when these errors were corrected and accounted for, there was still no significant effect of ESP. Many of the studies only appeared to have significant occurrence of ESP, when in fact, this result was due to the many methodological errors in the research.",
            "score": 122.18815863132477
        },
        {
            "docid": "634_2",
            "document": "Analysis of variance . Analysis of variance (ANOVA) is a collection of statistical models and their associated estimation procedures (such as the \"variation\" among and between groups) used to analyze the differences among group means in a sample. ANOVA was developed by statistician and evolutionary biologist Ronald Fisher. In the ANOVA setting, the observed variance in a particular variable is partitioned into components attributable to different sources of variation. In its simplest form, ANOVA provides a statistical test of whether the population means of several groups are equal, and therefore generalizes the \"t\"-test to more than two groups. ANOVA is useful for comparing (testing) three or more group means for statistical significance. It is conceptually similar to multiple two-sample t-tests, but is more conservative (results in less type I error) and is therefore suited to a wide range of practical problems. While the analysis of variance reached fruition in the 20th century, antecedents extend centuries into the past according to Stigler. These include hypothesis testing, the partitioning of sums of squares, experimental techniques and the additive model. Laplace was performing hypothesis testing in the 1770s. The development of least-squares methods by Laplace and Gauss circa 1800 provided an improved method of combining observations (over the existing practices then used in astronomy and geodesy). It also initiated much study of the contributions to sums of squares. Laplace knew how to estimate a variance from a residual (rather than a total) sum of squares. By 1827 Laplace was using least squares methods to address ANOVA problems regarding measurements of atmospheric tides. Before 1800 astronomers had isolated observational errors resulting  from reaction times (the \"personal equation\") and had developed methods of reducing the errors. The experimental methods used in the study of the personal equation were later accepted by the emerging field of psychology which developed strong (full factorial) experimental methods to which randomization and blinding were soon added. An eloquent non-mathematical explanation of the additive effects model was available in 1885.",
            "score": 148.9144937992096
        },
        {
            "docid": "31349118_3",
            "document": "Online interview . Online interviews, like offline interviews, typically ask respondents to explain what they think or how they feel about an aspect of their social world. Interviews are especially useful for understanding the meanings participants assign to their activities; their perspectives, motives, and experiences. Interviews are also useful for eliciting the language used by group members, gathering information about processes that cannot be observed, or inquiring about the past. Thus the objectives researchers have do not differ significantly, however the methods and research design can be effected by the online component of the research which this article will take issue with.",
            "score": 111.4527428150177
        },
        {
            "docid": "22921_88",
            "document": "Psychology . Experimental researchers typically use a statistical hypothesis testing model which involves making predictions before conducting the experiment, then assessing how well the data supports the predictions. (These predictions may originate from a more abstract scientific hypothesis about how the phenomenon under study actually works.) Analysis of variance (ANOVA) statistical techniques are used to distinguish unique results of the experiment from the null hypothesis that variations result from random fluctuations in data. In psychology, the widely used standard ascribes statistical significance to results which have less than 5% probability of being explained by random variation.",
            "score": 128.61418461799622
        },
        {
            "docid": "6352447_16",
            "document": "Artificial grammar learning . One hypothesis that contradicts the automaticity of AGL is the \"mere exposure effect\". The mere exposure effect is increased affect towards a stimulus that is the result of nonreinforced, repeated exposure to the stimulus. Results from over 200 experiments on this effect indicate that there is a positive relationship between mean \"goodness\" rating and frequency of stimulus exposure. Stimuli for these experiments included line drawings, polygons and nonsense words (which are types of stimuli used in AGL research). These experiments exposed participants to each stimulus up to 25 times. Following each exposure participants were asked to rate the degree to which each stimulus suggested \"good\" vs. \"bad\" affect on a 7-point scale. In addition to the main pattern of results, it was also found in several experiments that participants rated higher positive affect for previously exposed items than for novel items. Since implicit cognition should not reference previous study episodes, the effects on affect ratings should not have been observed if processing of this stimuli is truly implicit. The results of these experiments suggests that different categorization of the strings may occur due to differences in affect associated with the strings and not due to implicitly learned grammar rules.",
            "score": 115.9803364276886
        }
    ],
    "r": [
        {
            "docid": "27579_8",
            "document": "Statistical theory . Besides the philosophy underlying statistical inference, statistical theory has the task of considering the types of questions that data analysts might want to ask about the problems they are studying and of providing data analytic techniques for answering them. Some of these tasks are: When a statistical procedure has been specified in the study protocol, then statistical theory provides well-defined probability statements for the method when applied to all populations that could have arisen from the randomization used to generate the data. This provides an objective way of estimating parameters, estimating confidence intervals, testing hypotheses, and selecting the best. Even for observational data, statistical theory provides a way of calculating a value that can be used to interpret a sample of data from a population, it can provide a means of indicating how well that value is determined by the sample, and thus a means of saying corresponding values derived for different populations are as different as they might seem; however, the reliability of inferences from post-hoc observational data is often worse than for planned randomized generation of data.",
            "score": 169.9138946533203
        },
        {
            "docid": "40703875_2",
            "document": "Estimation statistics . Estimation statistics is a data analysis framework that uses a combination of effect sizes, confidence intervals, precision planning, and meta-analysis to plan experiments, analyze data and interpret results. It is distinct from null hypothesis significance testing (NHST), which is considered to be less informative. Estimation statistics, or simply estimation, is also known as the new statistics, a distinction introduced in the fields of psychology, medical research, life sciences and a wide range of other experimental sciences where NHST still remains prevalent, despite estimation statistics having been recommended as preferable for several decades.",
            "score": 158.12478637695312
        },
        {
            "docid": "26685_17",
            "document": "Statistics . A common goal for a statistical research project is to investigate causality, and in particular to draw a conclusion on the effect of changes in the values of predictors or independent variables on dependent variables. There are two major types of causal statistical studies: experimental studies and observational studies. In both types of studies, the effect of differences of an independent variable (or variables) on the behavior of the dependent variable are observed. The difference between the two types lies in how the study is actually conducted. Each can be very effective. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation. Instead, data are gathered and correlations between predictors and response are investigated. While the tools of data analysis work best on data from randomized studies, they are also applied to other kinds of data\u2014like natural experiments and observational studies\u2014for which a statistician would use a modified, more structured estimation method (e.g., Difference in differences estimation and instrumental variables, among many others) that produce consistent estimators.",
            "score": 155.68524169921875
        },
        {
            "docid": "62329_42",
            "document": "Meta-analysis . Modern statistical meta-analysis does more than just combine the effect sizes of a set of studies using a weighted average. It can test if the outcomes of studies show more variation than the variation that is expected because of the sampling of different numbers of research participants. Additionally, study characteristics such as measurement instrument used, population sampled, or aspects of the studies' design can be coded and used to reduce variance of the estimator (see statistical models above). Thus some methodological weaknesses in studies can be corrected statistically. Other uses of meta-analytic methods include the development of clinical prediction models, where meta-analysis may be used to combine data from different research centers, or even to aggregate existing prediction models.",
            "score": 154.939697265625
        },
        {
            "docid": "39834_31",
            "document": "Correlation does not imply causation . Well-designed experimental studies replace equality of individuals as in the previous example by equality of groups. The objective is to construct two groups that are similar except for the treatment that the groups receive. This is achieved by selecting subjects from a single population and randomly assigning them to two or more groups. The likelihood of the groups behaving similarly to one another (on average) rises with the number of subjects in each group. If the groups are essentially equivalent except for the treatment they receive, and a difference in the outcome for the groups is observed, then this constitutes evidence that the treatment is responsible for the outcome, or in other words the treatment causes the observed effect. However, an observed effect could also be caused \"by chance\", for example as a result of random perturbations in the population. Statistical tests exist to quantify the likelihood of erroneously concluding that an observed difference exists when in fact it does not (for example see P-value).",
            "score": 149.7475128173828
        },
        {
            "docid": "634_2",
            "document": "Analysis of variance . Analysis of variance (ANOVA) is a collection of statistical models and their associated estimation procedures (such as the \"variation\" among and between groups) used to analyze the differences among group means in a sample. ANOVA was developed by statistician and evolutionary biologist Ronald Fisher. In the ANOVA setting, the observed variance in a particular variable is partitioned into components attributable to different sources of variation. In its simplest form, ANOVA provides a statistical test of whether the population means of several groups are equal, and therefore generalizes the \"t\"-test to more than two groups. ANOVA is useful for comparing (testing) three or more group means for statistical significance. It is conceptually similar to multiple two-sample t-tests, but is more conservative (results in less type I error) and is therefore suited to a wide range of practical problems. While the analysis of variance reached fruition in the 20th century, antecedents extend centuries into the past according to Stigler. These include hypothesis testing, the partitioning of sums of squares, experimental techniques and the additive model. Laplace was performing hypothesis testing in the 1770s. The development of least-squares methods by Laplace and Gauss circa 1800 provided an improved method of combining observations (over the existing practices then used in astronomy and geodesy). It also initiated much study of the contributions to sums of squares. Laplace knew how to estimate a variance from a residual (rather than a total) sum of squares. By 1827 Laplace was using least squares methods to address ANOVA problems regarding measurements of atmospheric tides. Before 1800 astronomers had isolated observational errors resulting  from reaction times (the \"personal equation\") and had developed methods of reducing the errors. The experimental methods used in the study of the personal equation were later accepted by the emerging field of psychology which developed strong (full factorial) experimental methods to which randomization and blinding were soon added. An eloquent non-mathematical explanation of the additive effects model was available in 1885.",
            "score": 148.91448974609375
        },
        {
            "docid": "59861_29",
            "document": "Experiment . Fundamentally, however, observational studies are not experiments. By definition, observational studies lack the manipulation required for Baconian experiments. In addition, observational studies (e.g., in biological or social systems) often involve variables that are difficult to quantify or control. Observational studies are limited because they lack the statistical properties of randomized experiments. In a randomized experiment, the method of randomization specified in the experimental protocol guides the statistical analysis, which is usually specified also by the experimental protocol. Without a statistical model that reflects an objective randomization, the statistical analysis relies on a subjective model. Inferences from subjective models are unreliable in theory and practice. In fact, there are several cases where carefully conducted observational studies consistently give wrong results, that is, where the results of the observational studies are inconsistent and also differ from the results of experiments. For example, epidemiological studies of colon cancer consistently show beneficial correlations with broccoli consumption, while experiments find no benefit.",
            "score": 145.79824829101562
        },
        {
            "docid": "14527587_2",
            "document": "Average treatment effect . The average treatment effect (ATE) is a measure used to compare treatments (or interventions) in randomized experiments, evaluation of policy interventions, and medical trials. The ATE measures the difference in mean (average) outcomes between units assigned to the treatment and units assigned to the control. In a randomized trial (i.e., an experimental study), the average treatment effect can be estimated from a sample using a comparison in mean outcomes for treated and untreated units. However, the ATE is generally understood as a causal parameter (i.e., an estimate or property of a population) that a researcher desires to know, defined without reference to the study design or estimation procedure. Both observational studies and experimental study designs with random assignment may enable one to estimate an ATE in a variety of ways.",
            "score": 144.3812255859375
        },
        {
            "docid": "160995_3",
            "document": "Statistical significance . The significance level for a study is chosen before data collection, and typically set to 5% or much lower, depending on the field of study. In any experiment or observation that involves drawing a sample from a population, there is always the possibility that an observed effect would have occurred due to sampling error alone. But if the \"p\"-value of an observed effect is less than the significance level, an investigator may conclude that the effect reflects the characteristics of the whole population, thereby rejecting the null hypothesis. This technique for testing the significance of results was developed in the early 20th century.",
            "score": 141.9302215576172
        },
        {
            "docid": "62329_41",
            "document": "Meta-analysis . The meta-analysis estimate represents a weighted average across studies and when there is heterogeneity this may result in the summary estimate not being representative of individual studies. Qualitative appraisal of the primary studies using established tools can uncover potential biases, but does not quantify the aggregate effect of these biases on the summary estimate. Although the meta-analysis result could be compared with an independent prospective primary study, such external validation is often impractical. This has led to the development of methods that exploit a form of leave-one-out cross validation, sometimes referred to as internal-external cross validation (IOCV). Here each of the k included studies in turn is omitted and compared with the summary estimate derived from aggregating the remaining k- 1 studies. A general validation statistic, Vn based on IOCV has been developed to measure the statistical validity of meta-analysis results. For test accuracy and prediction, particularly when there are multivariate effects, other approaches which seek to estimate the prediction error have also been proposed.",
            "score": 141.68740844726562
        },
        {
            "docid": "30284_85",
            "document": "Statistical hypothesis testing . On one \"alternative\" there is no disagreement: Fisher himself said, \"In relation to the test of significance, we may say that a phenomenon is experimentally demonstrable when we know how to conduct an experiment which will rarely fail to give us a statistically significant result.\" Cohen, an influential critic of significance testing, concurred, \"... don't look for a magic alternative to NHST \"[null hypothesis significance testing]\" ... It doesn't exist.\" \"... given the problems of statistical induction, we must finally rely, as have the older sciences, on replication.\" The \"alternative\" to significance testing is repeated testing. The easiest way to decrease statistical uncertainty is by obtaining more data, whether by increased sample size or by repeated tests. Nickerson claimed to have never seen the publication of a literally replicated experiment in psychology. An indirect approach to replication is meta-analysis.",
            "score": 141.6293182373047
        },
        {
            "docid": "20574280_6",
            "document": "David A. Freedman . In addition to his work in forensic statistics, Freedman had a broad impact on the application of statistics to important medical, social, and public policy issues, such as clinical trials, epidemiology, economic models, and the interpretation of scientific experiments and observational studies. In his applied work, Freedman emphasized exposing and checking the assumptions that underlie standard methods, as well as understanding how those methods behave when the assumptions are false. He characterized circumstances in which the methods continue to perform well, and those where they break down\u2014regardless of the quality of the data. Two of his earlier results (1963 and 1965) investigate whether or not and under what circumstances a Bayesian learning approach is consistent, i.e. when does the prior converge to the true probability distribution given sufficiently many observed data. In particular the 1965 paper with the innocent title \"On the asymptotic behaviour of Bayes estimates in the discrete case II\" finds the rather disappointing answer that when sampling from a countably infinite population the Bayesian procedure fails almost everywhere, i.e. one does not obtain the true distribution asymptotically. This situation is quite different from the finite case when the (discrete) random variable takes only finite many values and the Bayesian method is consistent in agreement with earlier findings of Doob (1948).",
            "score": 141.13427734375
        },
        {
            "docid": "6885770_8",
            "document": "Bootstrapping (statistics) . As an example, assume we are interested in the average (or mean) height of people worldwide. We cannot measure all the people in the global population, so instead we sample only a tiny part of it, and measure that. Assume the sample is of size N; that is, we measure the heights of N individuals. From that single sample, only one estimate of the mean can be obtained. In order to reason about the population, we need some sense of the variability of the mean that we have computed. The simplest bootstrap method involves taking the original data set of N heights, and, using a computer, sampling from it to form a new sample (called a 'resample' or bootstrap sample) that is also of size N. The bootstrap sample is taken from the original by using sampling with replacement (e.g. we might 'resample' 5 times from [1,2,3,4,5] and get [2,5,4,4,1]), so, assuming N is sufficiently large, for all practical purposes there is virtually zero probability that it will be identical to the original \"real\" sample. This process is repeated a large number of times (typically 1,000 or 10,000 times), and for each of these bootstrap samples we compute its mean (each of these are called bootstrap estimates). We now can create a histogram of bootstrap means. This histogram provides an estimate of the shape of the distribution of the sample mean from which we can answer questions about how much the mean varies across samples. (The method here, described for the mean, can be applied to almost any other statistic or estimator.)",
            "score": 141.1055450439453
        },
        {
            "docid": "9578417_17",
            "document": "Research on meditation . Mindfulness meditation also appears to bring about favorable structural changes in the brain, though more research needs to be done because most of these studies are small and have weak methodology. One recent study found a significant cortical thickness increase in individuals who underwent a brief -8 weeks- MBSR training program and that this increase was coupled with a significant reduction of several psychological indices related to worry, state anxiety, depression. Another study describes how mindfulness based interventions target neurocognitive mechanisms of addiction at the attention-appraisal-emotion interface. A meta-analysis by Fox et al. (2014) using results from 21 brain imaging studies found consistent differences in the region of the prefrontal cortex and other brain regions associated with body awareness. In terms of effect size the mean effect was rated as moderate. (Cohen's d = 0.46) However the results should be interpreted with caution because funnel plots indicate that publication bias is an issue in meditation research. A follow up by Fox et al. (2016) using 78 functional neuro-imaging studies suggests that different meditation styles are reliably associated with different brain activity. Activations in some brain regions are usually accompanied by deactivation in others. This finding suggests that meditation research must put emphasis on comparing practices from the same style of meditation, for example results from studies investigating focused attention methods cannot be compared to results from open monitoring approaches.",
            "score": 140.96348571777344
        },
        {
            "docid": "26685_41",
            "document": "Statistics . Most studies only sample part of a population, so results don't fully represent the whole population. Any estimates obtained from the sample only approximate the population value. Confidence intervals allow statisticians to express how closely the sample estimate matches the true value in the whole population. Often they are expressed as 95% confidence intervals. Formally, a 95% confidence interval for a value is a range where, if the sampling and analysis were repeated under the same conditions (yielding a different dataset), the interval would include the true (population) value in 95% of all possible cases. This does \"not\" imply that the probability that the true value is in the confidence interval is 95%. From the frequentist perspective, such a claim does not even make sense, as the true value is not a random variable. Either the true value is or is not within the given interval. However, it is true that, before any data are sampled and given a plan for how to construct the confidence interval, the probability is 95% that the yet-to-be-calculated interval will cover the true value: at this point, the limits of the interval are yet-to-be-observed random variables. One approach that does yield an interval that can be interpreted as having a given probability of containing the true value is to use a credible interval from Bayesian statistics: this approach depends on a different way of interpreting what is meant by \"probability\", that is as a Bayesian probability.",
            "score": 140.51034545898438
        },
        {
            "docid": "1900609_24",
            "document": "External validity . The only way to be certain that the results of an experiment represent the behaviour of a particular population is to ensure that participants are randomly selected from that population. Samples in experiments cannot be randomly selected just as they are in surveys because it is impractical and expensive to select random samples for social psychology experiments. It is difficult enough to convince a random sample of people to agree to answer a few questions over the telephone as part of a political poll, and such polls can cost thousands of dollars to conduct. Moreover, even if one somehow was able to recruit a truly random sample, there can be unobserved heterogeneity in the effects of the experimental treatments... A treatment can have a positive effect on some subgroups but a negative effect on others. The effects shown in the treatment averages may not generalize to any subgroup.",
            "score": 140.2682647705078
        },
        {
            "docid": "17985779_6",
            "document": "Population stratification . The assumption of population homogeneity in association studies, especially case-control studies, can easily be violated and can lead to both type I and type II errors. It is therefore important for the models used in the study to compensate for the population structure. The problem in case control studies is that if there is a genetic involvement in the disease, the case population is more likely to be related than the individuals in the control population. This means that the assumption of independence of observations is violated. Often this will lead to an overestimation of the significance of an association but it depends on the way the sample was chosen. If, coincidentally, there is a higher allele frequency in a subpopulation of the cases, you will find association with any trait that is more prevalent in the case population. This kind of spurious association increases as the sample population grows so the problem should be of special concern in large scale association studies when loci only cause relatively small effects on the trait. A method that in some cases can compensate for the above described problems has been developed by Devlin and Roeder (1999). It uses both a frequentist and a Bayesian approach (the latter being appropriate when dealing with a large number of candidate genes).",
            "score": 139.45419311523438
        },
        {
            "docid": "238695_19",
            "document": "Power (statistics) . Power analysis can either be done before (\"a priori\" or prospective power analysis) or after (\"post hoc\" or retrospective power analysis) data are collected. \"A priori\" power analysis is conducted prior to the research study, and is typically used in estimating sufficient sample sizes to achieve adequate power. \"Post-hoc\" analysis of \"observed power\" is conducted after a study has been completed, and uses the obtained sample size and effect size to determine what the power was in the study, assuming the effect size in the sample is equal to the effect size in the population. Whereas the utility of prospective power analysis in experimental design is universally accepted, post hoc power analysis is fundamentally flawed. Falling for the temptation to use the statistical analysis of the collected data to estimate the power will result in uninformative and misleading values. In particular, it has been shown that \"post-hoc\" \"observed power\" is a one-to-one function of the \"p\"-value attained. This has been extended to show that all \"post-hoc\" power analyses suffer from what is called the \"power approach paradox\" (PAP), in which a study with a null result is thought to show \"more\" evidence that the null hypothesis is actually true when the \"p\"-value is smaller, since the apparent power to detect an actual effect would be higher. In fact, a smaller \"p\"-value is properly understood to make the null hypothesis \"relatively\" less likely to be true.",
            "score": 138.75894165039062
        },
        {
            "docid": "59861_8",
            "document": "Experiment . In medicine and the social sciences, the prevalence of experimental research varies widely across disciplines. When used, however, experiments typically follow the form of the clinical trial, where experimental units (usually individual human beings) are randomly assigned to a treatment or control condition where one or more outcomes are assessed. In contrast to norms in the physical sciences, the focus is typically on the average treatment effect (the difference in outcomes between the treatment and control groups) or another test statistic produced by the experiment. A single study typically does not involve replications of the experiment, but separate studies may be aggregated through systematic review and meta-analysis.",
            "score": 138.41773986816406
        },
        {
            "docid": "882729_15",
            "document": "High-throughput screening . In a screen with replicates, we can directly estimate variability for each compound; as a consequence, we should use SSMD or t-statistic that does not rely on the strong assumption that the z-score and z*-score rely on. One issue with the use of t-statistic and associated p-values is that they are affected by both sample size and effect size. They come from testing for no mean difference, and thus are not designed to measure the size of compound effects. For hit selection, the major interest is the size of effect in a tested compound. SSMD directly assesses the size of effects. SSMD has also been shown to be better than other commonly used effect sizes . The population value of SSMD is comparable across experiments and, thus, we can use the same cutoff for the population value of SSMD to measure the size of compound effects",
            "score": 137.09872436523438
        },
        {
            "docid": "272134_12",
            "document": "Survey methodology . Longitudinal studies take measure of the same random sample at multiple time points. Unlike with a successive independent samples design, this design measures the differences in individual participants\u2019 responses over time. This means that a researcher can potentially assess the reasons for response changes by assessing the differences in respondents\u2019 experiences. Longitudinal studies are the easiest way to assess the effect of a naturally occurring event, such as divorce that cannot be tested experimentally. However, longitudinal studies are both expensive and difficult to do. It\u2019s harder to find a sample that will commit to a months- or years-long study than a 15-minute interview, and participants frequently leave the study before the final assessment. This attrition of participants is not random, so samples can become less representative with successive assessments. To account for this, a researcher can compare the respondents who left the survey to those that did not, to see if they are statistically different populations. Respondents may also try to be self-consistent in spite of changes to survey answers.",
            "score": 136.96627807617188
        },
        {
            "docid": "437276_5",
            "document": "Effect size . As in any statistical setting, effect sizes are estimated with sampling error, and may be biased unless the effect size estimator that is used is appropriate for the manner in which the data were sampled and the manner in which the measurements were made. An example of this is publication bias, which occurs when scientists only report results when the estimated effect sizes are large or are statistically significant. As a result, if many researchers carry out studies with low statistical power, the reported effect sizes will tend to be larger than the true (population) effects, if any. Another example where effect sizes may be distorted is in a multiple trial experiment, where the effect size calculation is based on the averaged or aggregated response across the trials.",
            "score": 136.4961395263672
        },
        {
            "docid": "9014_54",
            "document": "Developmental psychology . Developmental psychologists have a number of methods to study changes in individuals over time. Common research methods include systematic observation, including naturalistic observation or structured observation; self-reports, which could be clinical interviews or structured interviews; clinical or case study method; and ethnography or participant observation. These methods differ in the extent of control researchers impose on study conditions, and how they construct ideas about which variables to study. Every developmental investigation can be characterized in terms of whether its underlying strategy involves the \"experimental\", \"correlational\", or \"case study\" approach. The experimental method involves \"actual manipulation of various treatments, circumstances, or events to which the participant or subject is exposed; the \"experimental design\" points to cause-and-effect relationships. This method allows for strong inferences to be made of causal relationships between the manipulation of one or more independent variables and subsequent behavior, as measured by the dependent variable. The advantage of using this research method is that it permits determination of cause-and-effect relationships among variables. On the other hand, the limitation is that data obtained in an artificial environment may lack generalizability. The correlational method explores the relationship between two or more events by gathering information about these variables without researcher intervention. The advantage of using a correlational design is that it estimates the strength and direction of relationships among variables in the natural environment; however, the limitation is that it does not permit determination of cause-and-effect relationships among variables. The case study approach allows investigations to obtain an in-depth understanding of an individual participant by collecting data based on interviews, structured questionnaires, observations, and test scores. Each of these methods have its strengths and weaknesses but the experimental method when appropriate is the preferred method of developmental scientists because it provides a controlled situation and conclusions to be drawn about cause-and-effect relationships.",
            "score": 136.0100555419922
        },
        {
            "docid": "17985779_11",
            "document": "Population stratification . If the population is in Hardy-Weinberg equilibrium the two statistics are approximately equal. Under the null hypothesis of no population stratification the trend test is asymptotic formula_2 distribution with one degree of freedom. The idea is that the statistic is inflated by a factor formula_5 so that formula_6 where formula_5 depends on the effect of stratification. The above method rests upon the assumptions that the inflation factor formula_5 is constant, which means that the loci should have roughly equal mutation rates, should not be under different selection in the two populations, and the amount of Hardy-Weinberg disequilibrium measured in Wright\u2019s coefficient of inbreeding \"F\" should not differ between the different loci. The last of these is of greatest concern. If the effect of the stratification is similar across the different loci formula_5 can be estimated from the unlinked markers formula_10 where \"L\" is the number of unlinked markers. The denominator is derived from the gamma distribution as a robust estimator of formula_5. Other estimators have been suggested, for example, Reich and Goldstein suggested using the mean of the statistics instead. This is not the only way to estimate formula_5 but according to Bacanu et al. it is an appropriate estimate even if some of the unlinked markers are actually in disequilibrium with a disease causing locus or are themselves associated with the disease. Under the null hypothesis and when correcting for stratification using \"L\" unlinked genes, formula_13 is approximately formula_14 distributed. With this correction the overall type I error rate should be approximately equal to formula_15 even when the population is stratified. Devlin and Roeder (1999) mostly considered the situation where formula_16 gives a 95% confidence level and not smaller p-values. Marchini et al. (2004) demonstrates by simulation that genomic control can lead to an anti-conservative p-value if this value is very small and the two populations (case and control) are extremely distinct. This was especially a problem if the number of unlinked markers were in the order 50 \u2212 100. This can result in false positives (at that significance level).",
            "score": 135.8832244873047
        },
        {
            "docid": "6326483_2",
            "document": "Observational study . In fields such as epidemiology, social sciences, psychology and statistics, an observational study draws inferences from a sample to a population where the independent variable is not under the control of the researcher because of ethical concerns or logistical constraints. One common observational study is about the possible effect of a treatment on subjects, where the assignment of subjects into a treated group versus a control group is outside the control of the investigator. This is in contrast with experiments, such as randomized controlled trials, where each subject is randomly assigned to a treated group or a control group.",
            "score": 135.45484924316406
        },
        {
            "docid": "37645970_11",
            "document": "Observational methods in psychology . Participate observation is characterized as either undisguised or disguised. In undisguised observation, the observed individuals know that the observer is present for the purpose of collecting info about their behavior. This technique is often used to understand the culture and behavior of groups or individuals. In contrast, in disguised observation, the observed individuals do not know that they are being observed. This technique is often used when researchers believe that the individuals under observation may change their behavior as a result of knowing that they were being recorded. For a great example of disguised research, see the Rosenhan experiment in which several researchers seek admission to twelve different mental hospitals to observe patient-staff interactions and patient diagnosing and releasing procedures.  There are several benefits to doing participant observation. Firstly, participant research allows researchers to observe behaviors and situations that are not usually open to scientific observation. Furthermore, participant research allows the observer to have the same experiences as the people under study, which may provide important insights and understandings of individuals or groups. However, there are also several drawbacks to doing participant observation. Firstly, participant observers may sometimes lose their objectivity as a result of participating in the study. This usually happens when observers begin to identify with the individuals under study, and this threat generally increases as the degree of observer participation increases. Secondly, participant observers may unduly influence the individuals whose behavior they are recording. This effect is not easily assessed, however, it generally more prominent when the group being observed is small, or if the activities of the participant observer are prominent. Lastly, disguised observation raises some ethical issues regarding obtaining information without respondents' knowledge. For example, the observations collected by an observer participating in an internet chat room discussing how racists advocate racial violence may be seen as incriminating evidence collected without the respondents\u2019 knowledge. The dilemma here is of course that if informed consent were obtained from participants, respondents would likely choose not to cooperate.",
            "score": 135.32875061035156
        },
        {
            "docid": "26685_13",
            "document": "Statistics . When a census is not feasible, a chosen subset of the population called a sample is studied. Once a sample that is representative of the population is determined, data is collected for the sample members in an observational or experimental setting. Again, descriptive statistics can be used to summarize the sample data. However, the drawing of the sample has been subject to an element of randomness, hence the established numerical descriptors from the sample are also due to uncertainty. To still draw meaningful conclusions about the entire population, inferential statistics is needed. It uses patterns in the sample data to draw inferences about the population represented, accounting for randomness. These inferences may take the form of: answering yes/no questions about the data (hypothesis testing), estimating numerical characteristics of the data (estimation), describing associations within the data (correlation) and modeling relationships within the data (for example, using regression analysis). Inference can extend to forecasting, prediction and estimation of unobserved values either in or associated with the population being studied; it can include extrapolation and interpolation of time series or spatial data, and can also include data mining.",
            "score": 135.04246520996094
        },
        {
            "docid": "397271_3",
            "document": "Cohort study . Cohort studies represent one of the fundamental designs of epidemiology which are used in research in the fields of medicine, nursing, psychology, social science, and in any field reliant on 'difficult to reach' answers that are based on evidence (statistics). In medicine for instance, while clinical trials are used primarily for assessing the safety of newly developed pharmaceuticals before they are approved for sale, epidemiological analysis on how risk factors affect the incidence of diseases is often used to identify the causes of diseases in the first place, and to help provide pre-clinical justification for the plausibility of protective factors (treatments). Cohort studies differ from clinical trials in that no intervention, treatment, or exposure is administered to participants in a cohort design; and no control group is defined. Rather, cohort studies are largely about the life histories of segments of populations, and the individual people who constitute these segments. Exposures or protective factors are identified as preexisting characteristics of participants. The study is controlled by including other common characteristics of the cohort in the statistical analysis. Both exposure/treatment and control variables are measured at baseline. Participants are then followed over time to observe the incidence rate of the disease or outcome in question. Regression analysis can then be used to evaluate the extent to which the exposure or treatment variable contributes to the incidence of the disease, while accounting for other variables that may be at play.",
            "score": 134.9226531982422
        },
        {
            "docid": "53492533_6",
            "document": "Sang-Min Whang . The question that continues to be repeated in Sang Min Whang\u2019s work since his graduate school days can be summarized as follows: \u201cHow are peoples\u2019 minds living in a particular society or culture expressed in different ways for each individual?\u201d His doctoral dissertation \u00abThe Organization of Everyday Places and their Dimensional Features: The Priming Effect of Dimensions on the Congruence of Place and Behavior\u00bb explored what kind of conceptions people had of everyday places, how the psychological meaning people attribute to places can be measured, and whether the measured results came from absolute characteristics of places or were results of peoples\u2019 responses to places. In this study, he employed non-parametric statistical analysis, which unlike parametric statistical analysis that presumes a few set patterns exist, uses data to find an unknown pattern.",
            "score": 134.8858184814453
        },
        {
            "docid": "2940730_8",
            "document": "Random assignment . To express this same idea statistically - If a randomly assigned group is compared to the mean it may be discovered that they differ, even though they were assigned from the same group. If a test of statistical significance is applied to randomly assigned groups to test the difference between sample means against the null hypothesis that they are equal to the same population mean (i.e., population mean of differences = 0), given the probability distribution, the null hypothesis will sometimes be \"rejected,\" that is, deemed not plausible. That is, the groups will be sufficiently different on the variable tested to conclude statistically that they did not come from the same population, even though, procedurally, they were assigned from the same total group. For example, using random assignment may create an assignment to groups that has 20 blue-eyed people and 5 brown-eyed people in one group. This is a rare event under random assignment, but it could happen, and when it does it might add some doubt to the causal agent in the experimental hypothesis.",
            "score": 134.66957092285156
        },
        {
            "docid": "2893954_5",
            "document": "Bulgarian Turks . A Y-DNA genetic study on Slavic peoples and some of their neighbours published two statistical distributions of distance because of the volume of details studied, based on pairwise F values, the Turks from Bulgaria are most related to Anatolian Turks, thereafter to Italians, Bulgarians and others]; while according to the R values, the Turks from Bulgaria are most related to Bulgarians, thereafter to Macedonians, Anatolian Turks, Serbs and the rest, while Balts and North Slavs remain most unrelated according to them both. The study claims that the F genetic distances reflect interpopulation relationships between the compared populations much better than their stepwise-based analogues, but that at the same time the genetic variation was more profoundly calculated by R. F and R calculate allele (haplotype or microsatellite) frequencies among populations and the distribution of evolutionary distances among alleles. R is based on the number of repeat differences between alleles at each microsatellite locus and is proposed to be better for most typical sample sizes, when data consist of variation at microsatellite loci or of nucleotide sequence (haplotype) information, the method may be unreliable unless a large number of loci are used. A nonsignificant test suggests that F should be preferred or when there is high gene flow within populations, F calculations are based on allele identity, it is likely to perform better than counterparts based on allele size information, the method depends on mutation rate, sometimes can likely provide biased estimate, but R will not perform necessarily better. A Bulgarian and other population studies observed concluded that when there is not much differiation, both statistical means show similar results, otherwise R is often superior to the F. However, no procedure has been developed to date for testing whether single-locus R and F estimates are significantly different.",
            "score": 133.6560516357422
        },
        {
            "docid": "437276_2",
            "document": "Effect size . In statistics, an effect size is a quantitative measure of the magnitude of a phenomenon. Examples of effect sizes are the correlation between two variables, the regression coefficient in a regression, the mean difference, or even the risk with which something happens, such as how many people survive after a heart attack for every one person that does not survive. For most types of effect size, a larger absolute value always indicates a stronger effect, with the main exception being if the effect size is an odds ratio. Effect sizes complement statistical hypothesis testing, and play an important role in power analyses, sample size planning, and in meta-analyses. They are the first item (magnitude) in the MAGIC criteria for evaluating the strength of a statistical claim. Especially in meta-analysis, where the purpose is to combine multiple effect sizes, the standard error (S.E.) of the effect size is of critical importance. The S.E. of the effect size is used to weigh effect sizes when combining studies, so that large studies are considered more important than small studies in the analysis. The S.E. of the effect size is calculated differently for each type of effect size, but generally only requires knowing the study's sample size (\"N\"), or the number of observations in each group (\"n\"s).",
            "score": 133.37586975097656
        }
    ]
}