{
    "q": [
        {
            "docid": "2567511_17",
            "document": "Neural engineering . Scientists can use experimental observations of neuronal systems and theoretical and computational models of these systems to create Neural networks with the hopes of modeling neural systems in as realistic a manner as possible. Neural networks can be used for analyses to help design further neurotechnological devices. Specifically, researchers handle analytical or finite element modeling to determine nervous system control of movements and apply these techniques to help patients with brain injuries or disorders. Artificial neural networks can be built from theoretical and computational models and implemented on computers from theoretically devices equations or experimental results of observed behavior of neuronal systems. Models might represent ion concentration dynamics, channel kinetics, synaptic transmission, single neuron computation, oxygen metabolism, or application of dynamic system theory (LaPlaca et al. 2005). Liquid-based template assembly was used to engineer 3D neural networks from neuron-seeded microcarrier beads.",
            "score": 162.4446337223053
        },
        {
            "docid": "3717_69",
            "document": "Brain . Theorists have worked to understand these response patterns by constructing mathematical models of neurons and neural networks, which can be simulated using computers. Some useful models are abstract, focusing on the conceptual structure of neural algorithms rather than the details of how they are implemented in the brain; other models attempt to incorporate data about the biophysical properties of real neurons. No model on any level is yet considered to be a fully valid description of brain function, though. The essential difficulty is that sophisticated computation by neural networks requires distributed processing in which hundreds or thousands of neurons work cooperatively\u2014current methods of brain activity recording are only capable of isolating action potentials from a few dozen neurons at a time.",
            "score": 179.6765856742859
        },
        {
            "docid": "33818014_8",
            "document": "Nervous system network models . On a high level representation, the neurons can be viewed as connected to other neurons to form a neural network in one of three ways. A specific network can be represented as a physiologically (or anatomically) connected network and modeled that way. There are several approaches to this (see Ascoli, G.A. (2002) Sporns, O. (2007), Connectionism, Rumelhart, J. L., McClelland, J. L., and PDP Research Group (1986), Arbib, M. A. (2007)). Or, it can form a functional network that serves a certain function and modeled accordingly (Honey, C. J., Kotter, R., Breakspear, R., & Sporns, O. (2007), Arbib, M. A. (2007)). A third way is to hypothesize a theory of the functioning of the biological components of the neural system by a mathematical model, in the form of a set of mathematical equations. The variables of the equation are some or all of the neurobiological properties of the entity being modeled, such as the dimensions of the dendrite or the stimulation rate of action potential along the axon in a neuron. The mathematical equations are solved using computational techniques and the results are validated with either simulation or experimental processes. This approach to modeling is called computational neuroscience. This methodology is used to model components from the ionic level to system level of the brain. This method is applicable for modeling integrated system of biological components that carry information signal from one neuron to another via intermediate active neurons that can pass the signal through or create new or additional signals. The computational neuroscience approach is extensively used and is based on two generic models, one of cell membrane potential Goldman (1943) and Hodgkin and Katz (1949), and the other based on Hodgkin-Huxley model of action potential (information signal).",
            "score": 161.0401736497879
        },
        {
            "docid": "33818014_2",
            "document": "Nervous system network models . Network of human nervous system comprises nodes (for example, neurons) that are connected by links (for example, synapses). The connectivity may be viewed anatomically, functionally, or electrophysiologically. These are presented in several Wikipedia articles that include Connectionism (a.k.a. Parallel Distributed Processing (PDP)), Biological neural network, Artificial neural network (a.k.a. Neural network), Computational neuroscience, as well as in several books by Ascoli, G. A. (2002), Sterratt, D., Graham, B., Gillies, A., & Willshaw, D. (2011), Gerstner, W., & Kistler, W. (2002), and Rumelhart, J. L., McClelland, J. L., and PDP Research Group (1986) among others. The focus of this article is a comprehensive view of modeling a neural network (technically neuronal network based on neuron model). Once an approach based on the perspective and connectivity is chosen, the models are developed at microscopic (ion and neuron), mesoscopic (functional or population), or macroscopic (system) levels. Computational modeling refers to models that are developed using computing tools.",
            "score": 124.19017112255096
        },
        {
            "docid": "8402086_18",
            "document": "Computational neurogenetic modeling . Because the amount of data on the interplay of genes and neurons and their effects is not enough to construct a rigorous model,  evolutionary computation is used to optimize artificial neural networks and gene regulatory networks, a common technique being the genetic algorithm. A genetic algorithm is a process that can be used to refine models by mimicking the process of natural selection observed in biological ecosystems. The primary advantages are that, due to not requiring derivative information, it can be applied to black box problems and multimodal optimization. The typical process for using genetic algorithms to refine a gene  regulatory network is: first, create a population; next, to create offspring via a crossover operation and  evaluate their fitness; then, on a group chosen for high fitness, simulate mutation via a mutation operator;  finally, taking the now mutated group, repeat this process until a desired level of fitness is demonstrated. Methods by which artificial neural networks may alter their structure without simulated mutation and fitness selection have been developed. A dynamically evolving neural network is one approach, as the creation of new connections and new neurons can  be modeled as the system adapts to new data. This enables the network to evolve in modeling accuracy without simulated natural selection. One method by which dynamically evolving networks may be optimized, called evolving layer neuron aggregation, combines neurons with sufficiently similar input weights into one neuron. This can take place during the training of the network, referred to as online aggregation, or between periods of training, referred to as offline aggregation. Experiments have suggested that offline aggregation is more efficient.",
            "score": 191.50848817825317
        },
        {
            "docid": "3630374_2",
            "document": "Neural computation . Neural computation is the hypothetical information processing performed by networks of neurons. Neural computation is affiliated with the philosophical tradition known as Computational Theory of Mind, also referred to as computationalism, which advances the thesis that neural computation explains cognition. The first persons to propose an account of neural activity as being computational was Warren McCullock and Walter Pitts in their seminal 1943 paper, A Logical Calculus of the Ideas Immanent in Nervous Activity. There are three general branches of computationalism, including classicism, connectionism, and computational neuroscience. All three branches agree that cognition is computation, however they disagree on what sorts of computations constitute cognition. The classicism tradition believes that computation in the brain is digital, analogous with digital computing. Both connectionism and computational neuroscience do not require that the computations which realize cognition are necessarily digital computations. However, the two branches greatly disagree upon which sorts of experimental data should be used to construct explanatory models of cognitive phenomena. Connectionists rely upon behavioral evidence to construct models to explain cognitive phenomenon, whereas computational neuroscience leverages neuroanatomical and neurophysiological information to construct mathematical models which explain cognition.",
            "score": 163.46692407131195
        },
        {
            "docid": "8402086_7",
            "document": "Computational neurogenetic modeling . An artificial neural network generally refers to any computational model that mimics the central nervous system, with capabilities such as learning and pattern recognition. With regards to computational neurogenetic modeling, however, it is often used to refer to those specifically designed for biological accuracy rather than computational efficiency. Individual neurons are the basic unit of an artificial neural network, with each neuron acting as a node. Each node receives weighted signals from other nodes that are either excitatory or inhibitory. To determine the output, a transfer function (or activation function) evaluates the sum of the weighted signals and, in some artificial neural networks, their input rate. Signal weights are strengthened (long-term potentiation) or weakened (long-term depression) depending on how synchronous the presynaptic and postsynaptic activation rates are (Hebbian theory).",
            "score": 114.88860392570496
        },
        {
            "docid": "20512936_21",
            "document": "Dendritic spike . Computational modeling of neurons, artificial neural networking, has become a very popular tool in investigating the properties of neuronal signaling. These models are based on biological neural networks. Computational modeling can be used to study single neurons, groups of neurons, or even networks of neurons. This field has generated much interest and serves as a tool for all branches of neuroscience research including dendritic spike initiation.",
            "score": 145.35785484313965
        },
        {
            "docid": "233488_23",
            "document": "Machine learning . An artificial neural network (ANN) learning algorithm, usually called \"neural network\" (NN), is a learning algorithm that is vaguely inspired by biological neural networks. Computations are structured in terms of an interconnected group of artificial neurons, processing information using a connectionist approach to computation. Modern neural networks are non-linear statistical data modeling tools. They are usually used to model complex relationships between inputs and outputs, to find patterns in data, or to capture the statistical structure in an unknown joint probability distribution between observed variables.",
            "score": 182.6211540699005
        },
        {
            "docid": "2860430_20",
            "document": "Neural oscillation . Computational models adopt a variety of abstractions in order to describe complex oscillatory dynamics observed in brain activity. Many models are used in the field, each defined at a different level of abstraction and trying to model different aspects of neural systems. They range from models of the short-term behaviour of individual neurons, through models of how the dynamics of neural circuitry arise from interactions between individual neurons, to models of how behaviour can arise from abstract neural modules that represent complete subsystems.",
            "score": 134.66005444526672
        },
        {
            "docid": "33818014_23",
            "document": "Nervous system network models . Computational science is an interdisciplinary field that combines engineering, biology, control systems, brain functions, physical sciences, and computer science. It has fundamental development models done at the lower levels of ions, neurons, and synapses, as well as information propagation between neurons. These models have established the enabling technology for higher-level models to be developed. They are based on chemical and electrical activities in the neurons for which electrical equivalent circuits are generated. A simple model for the neuron with predominantly potassium ions inside the cell and sodium ions outside establishes an electric potential on the membrane under equilibrium, i.e., no external activity, condition. This is called the resting membrane potential, which can be determined by Nernst Equation (Nernst, W. (1888)). An equivalent electrical circuit for a patch of membrane, for example an axon or dendrite, is shown in Figure 5. E and E are the potentials associated with the potassium and sodium channels respectively and R and R are the resistances associated with them. C is the capacitance of the membrane and I is the source current, which could be the test source or the signal source (action potential). The resting potential for potassium-sodium channels in a neuron is about -65 millivolts. The membrane model is for a small section of the cell membrane; for larger sections it can be extended by adding similar sections, called compartments, with the parameter values being the same or different. The compartments are cascaded by a resistance, called axial resistance. Figure 6 shows a compartmental model of a neuron that is developed over the membrane model. Dendrites are the postsynaptic receptors receiving inputs from other neurons; and the axon with one or more axon terminals transmits neurotransmitters to other neurons. The second building block is the Hodgkin-Huxley (HH) model of the action potential. When the membrane potential from the dendrites exceeds the resting membrane potential, a pulse is generated by the neuron cell and propagated along the axon. This pulse is called the action potential and HH model is a set of equations that is made to fit the experimental data by the design of the model and the choice of the parameter values.",
            "score": 131.14150273799896
        },
        {
            "docid": "33818014_21",
            "document": "Nervous system network models . As mentioned in Section 2.4, development of artificial neural network (ANN), or neural network as it is now called, started as simulation of biological neuron network and ended up using artificial neurons. Major development work has gone into industrial applications with learning process. Complex problems were addressed by simplifying the assumptions. Algorithms were developed to achieve a neurological related performance, such as learning from experience. Since the background and overview have been covered in the other internal references, the discussion here is limited to the types of models. The models are at the system or network level.",
            "score": 119.71240544319153
        },
        {
            "docid": "33818014_15",
            "document": "Nervous system network models . The concept of artificial neural network (ANN) was introduced by McColloch, W. S. & Pitts, W. (1943) for models based on behavior of biological neurons. Norbert Wiener (1961) gave this new field the popular name of cybernetics, whose principle is the interdisciplinary relationship among engineering, biology, control systems, brain functions, and computer science. With the computer science field advancing, the von Neumann-type computer was introduced early in the neuroscience study. But it was not suitable for symbolic processing, nondeterministic computations, dynamic executions, parallel distributed processing, and management of extensive knowledge bases, which are needed for biological neural network applications; and the direction of mind-like machine development changed to a learning machine. Computing technology has since advanced extensively and computational neuroscience is now able to handle mathematical models developed for biological neural network. Research and development are progressing in both artificial and biological neural networks including efforts to merge the two.",
            "score": 141.73788785934448
        },
        {
            "docid": "21523_125",
            "document": "Artificial neural network . Many types of models are used, defined at different levels of abstraction and modeling different aspects of neural systems. They range from models of the short-term behavior of individual neurons, models of how the dynamics of neural circuitry arise from interactions between individual neurons and finally to models of how behavior can arise from abstract neural modules that represent complete subsystems. These include models of the long-term, and short-term plasticity, of neural systems and their relations to learning and memory from the individual neuron to the system level.",
            "score": 112.17395353317261
        },
        {
            "docid": "271430_14",
            "document": "Computational neuroscience . Modeling the richness of biophysical properties on the single-neuron scale can supply mechanisms that serve as the building blocks for network dynamics. However, detailed neuron descriptions are computationally expensive and this can handicap the pursuit of realistic network investigations, where many neurons need to be simulated. As a result, researchers that study large neural circuits typically represent each neuron and synapse with an artificially simple model, ignoring much of the biological detail. Hence there is a drive to produce simplified neuron models that can retain significant biological fidelity at a low computational overhead. Algorithms have been developed to produce faithful, faster running, simplified surrogate neuron models from computationally expensive, detailed neuron models.",
            "score": 145.19270539283752
        },
        {
            "docid": "292744_216",
            "document": "Ising model . Following the general approach of Jaynes, a recent interpretation of Schneidman, Berry, Segev and Bialek, is that the Ising model is useful for any model of neural function, because a statistical model for neural activity should be chosen using the principle of maximum entropy. Given a collection of neurons, a statistical model which can reproduce the average firing rate for each neuron introduces a Lagrange multiplier for each neuron: But the activity of each neuron in this model is statistically independent. To allow for pair correlations, when one neuron tends to fire (or not to fire) along with another, introduce pair-wise lagrange multipliers: where formula_130 are not restricted to neighbors. Note that this generalization of Ising model is sometimes called the quadratic exponential binary distribution in statistics. This energy function only introduces probability biases for a spin having a value and for a pair of spins having the same value. Higher order correlations are unconstrained by the multipliers. An activity pattern sampled from this distribution requires the largest number of bits to store in a computer, in the most efficient coding scheme imaginable, as compared with any other distribution with the same average activity and pairwise correlations. This means that Ising models are relevant to any system which is described by bits which are as random as possible, with constraints on the pairwise correlations and the average number of 1s, which frequently occurs in both the physical and social sciences.",
            "score": 109.04293191432953
        },
        {
            "docid": "586357_18",
            "document": "Artificial general intelligence . The artificial neuron model assumed by Kurzweil and used in many current artificial neural network implementations is simple compared with biological neurons. A brain simulation would likely have to capture the detailed cellular behaviour of biological neurons, presently only understood in the broadest of outlines. The overhead introduced by full modeling of the biological, chemical, and physical details of neural behaviour (especially on a molecular scale) would require computational powers several orders of magnitude larger than Kurzweil's estimate. In addition the estimates do not account for glial cells, which are at least as numerous as neurons, and which may outnumber neurons by as much as 10:1, and are now known to play a role in cognitive processes.",
            "score": 104.21173310279846
        },
        {
            "docid": "53757504_6",
            "document": "Viktor K. Jirsa . The Virtual Brain is a free open source neuroinformatics tool designed to aid in the exploration of network mechanisms of brain function and associated pathologies. TVB provides the possibility to feed computational neuronal network models with information about structural and functional imaging data including population (sEEG/EEG/MEG) activity, spatially highly resolved whole brain metabolic/vascular signals (fMRI) and global measures of neuronal connections (DTI) \u2013 for intact as well as pathologically altered connectivity. TVB is model agnostic and offers a wide range of neural population models to be used as network nodes. The software infrastructure of the Virtual Brain is composed of a functional core running the large-scale brain simulations independently or in batch mode, a web based interface to access the simulator, as well as a command line interface to develop more extensive applications. All simulations may be performed on workstations and labtops, as well as on high-performance clusters (HPCs). Manipulations of network parameters within the Virtual Brain allow researchers and clinicians to test the effects of experimental paradigms, interventions (such as stimulation and surgery) and therapeutic strategies (such as pharmaceutical interventions targeting local areas). The computational environment allows the user to visualise the simulated data in 2D and 3D and perform data analyses in the same way as commonly performed with empirical data.",
            "score": 165.0549784898758
        },
        {
            "docid": "1706303_41",
            "document": "Recurrent neural network . A multiple timescales recurrent neural network (MTRNN) is a neural-based computational model that can simulate the functional hierarchy of the brain through self-organization that depends on spatial connection between neurons and on distinct types of neuron activities, each with distinct time properties. With such varied neuronal activities, continuous sequences of any set of behaviors are segmented into reusable primitives, which in turn are flexibly integrated into diverse sequential behaviors. The biological approval of such a type of hierarchy was discussed in the memory-prediction theory of brain function by Hawkins in his book \"On Intelligence\".",
            "score": 125.13174438476562
        },
        {
            "docid": "40158142_9",
            "document": "Nonlinear system identification . Artificial neural networks try loosely to imitate the network of neurons in the brain where computation takes place through a large number of simple processing elements. A typical neural network consists of a number of simple processing units interconnected to form a complex network. Layers of such units are arranged so that data is entered at the input layer and passes through either one or several intermediate layers before reaching the output layer. In supervised learning the network is trained by operating on the difference between the actual output and the desired output of the network, the prediction error, to change the connection strengths between the nodes. By iterating the weights are modified until the output error reaches an acceptable level. This process is called machine learning because the network adjusts the weights so that the output pattern is reproduced. Neural networks have been extensively studied and there are many excellent textbooks devoted to this topic in general, and more focussed textbooks which emphasise control and systems applications. There are two main problem types that can be studied using neural networks: static problems, and dynamic problems. Static problems include pattern recognition, classification, and approximation. Dynamic problems involve lagged variables and are more appropriate for system identification and related applications. Depending on the architecture of the network the training problem can be either nonlinear-in-the-parameters which involves optimisation or linear-in-the-parameters which can be solved using classical approaches. The training algorithms can be categorised into supervised, unsupervised, or reinforcement learning. Neural networks have excellent approximation properties but these are usually based on standard function approximation results using for example the Weierstrass Theorem that applies equally well to polynomials, rational functions, and other well-known models.  Neural networks have been applied extensively to system identification problems which involve nonlinear and dynamic relationships. However, classical neural networks are purely gross static approximating machines. There is no dynamics within the network. Hence when fitting dynamic models all the dynamics arise by allocating lagged inputs and outputs to the input layer of the network. The training procedure then produces the best static approximation that relates the lagged variables assigned to the input nodes to the output. There are more complex network architectures, including recurrent networks, that produce dynamics by introducing increasing orders of lagged variables to the input nodes. But in these cases it is very easy to over specify the lags and this can lead to over fitting and poor generalisation properties.  Neural networks have several advantages; they are conceptually simple, easy to train and to use, have excellent approximation properties, the concept of local and parallel processing is important and this provides integrity and fault tolerant behaviour. The biggest criticism of the classical neural network models is that the models produced are completely opaque and usually cannot be written down or analysed. It is therefore very difficult to know what is causing what, to analyse the model, or to compute dynamic characteristics from the model. Some of these points will not be relevant to all applications but they are for dynamic modelling.",
            "score": 173.08771920204163
        },
        {
            "docid": "33548913_6",
            "document": "Dehaene\u2013Changeux model . Among others Cohen & Hudson (2002) had already used \"\"Meta neural networks as intelligent agents for diagnosis \"\" Similarly to Cohen & Hudson, Dehaene & Changeux have established their model as an interaction of meta-neural networks (thalamocortical columns) themselves programmed in the manner of a \"\"hierarchy of neural networks that together act as an intelligent agent\"\", in order to use them as a system composed of a large scale of inter-connected intelligent agents for predicting the self-organized behaviour of the neural correlates of consciousness. It may also be noted that Jain et al. (2002) had already clearly identified spiking neurons as intelligent agents since the lower bound for computational power of networks of spiking neurons is the capacity to simulate in real-time for boolean-valued inputs any Turing machine. The DCM being composed of a very large number of interacting sub-networks which are themselves intelligent agents, it is formally a Multi-agent system programmed as a Swarm or neural networks and \"a fortiori\" of spiking neurons.",
            "score": 98.82065403461456
        },
        {
            "docid": "292744_215",
            "document": "Ising model . The activity of neurons in the brain can be modelled statistically. Each neuron at any time is either active + or inactive\u00a0\u2212. The active neurons are those that send an action potential down the axon in any given time window, and the inactive ones are those that do not. Because the neural activity at any one time is modelled by independent bits, Hopfield suggested that a dynamical Ising model would provide a first approximation to a neural network which is capable of learning.",
            "score": 112.10932683944702
        },
        {
            "docid": "3737445_11",
            "document": "Quantum neural network . Most learning algorithms follow the classical model of training an artificial neural network to learn the input-output function of a given training set and use classical feedback loops to update parameters of the quantum system until they converge to an optimal configuration. Learning as a parameter optimisation problem has also been approached by adiabatic models of quantum computing. Recently there has been proposed a new post-learning strategy to allow the search for improved set of weights based on analogy with quantum effects occurring in nature. The technique, proposed in is based on the analogy of modeling a biological neuron as a semiconductor heterostructure consisting of one energetic barrier sandwiched between two energetically lower areas. The activation function of the neuron is therefore considered as a particle entering the heterostructure and interacting with the barrier. In this way auxiliary reinforcement to the classical learning process of neural networks is achieved with minimal additional computational costs.",
            "score": 122.66437685489655
        },
        {
            "docid": "271430_23",
            "document": "Computational neuroscience . In some cases the complex interactions between \"inhibitory\" and \"excitatory\" neurons can be simplified using mean field theory, which gives rise to the population model of neural networks. While many neurotheorists prefer such models with reduced complexity, others argue that uncovering structural functional relations depends on including as much neuronal and network structure as possible. Models of this type are typically built in large simulation platforms like GENESIS or NEURON. There have been some attempts to provide unified methods that bridge and integrate these levels of complexity.",
            "score": 130.27503609657288
        },
        {
            "docid": "25345530_9",
            "document": "Models of neural computation . Linear systems are easier to analyze mathematically and are a persuasive assumption in many models including the McCulloch and Pitts neuron, population coding models, and the simple neurons often used in Artificial neural networks. Linearity may occur in the basic elements of a neural circuit such as the response of a postsynaptic neuron, or as an emergent property of a combination of nonlinear subcircuits. Though linearity is often seen as incorrect, there has been recent work suggesting it may, in fact, be biophysically plausible in some cases.",
            "score": 96.62371516227722
        },
        {
            "docid": "2860430_24",
            "document": "Neural oscillation . Neural field models are another important tool in studying neural oscillations and are a mathematical framework describing evolution of variables such as mean firing rate in space and time. In modeling the activity of large numbers of neurons, the central idea is to take the density of neurons to the continuum limit, resulting in spatially continuous neural networks. Instead of modelling individual neurons, this approach approximates a group of neurons by its average properties and interactions. It is based on the mean field approach, an area of statistical physics that deals with large-scale systems. Models based on these principles have been used to provide mathematical descriptions of neural oscillations and EEG rhythms. They have for instance been used to investigate visual hallucinations.",
            "score": 119.5818099975586
        },
        {
            "docid": "605477_14",
            "document": "Behavioral neuroscience . Computational models - Using a computer to formulate real-world problems to develop solutions. Although this method is often focused in computer science, it has begun to move towards other areas of study.For example, psychology is one of these areas. Computational models allow researchers in psychology to enhance their understanding of the functions and developments in nervous systems. Examples of methods include the modelling of neurons, networks and brain systems and theoretical analysis. Computational methods have a wide variety of roles including clarifying experiments, hypothesis testing and generating new insights. These techniques play an increasing role in the advancement of biological psychology.",
            "score": 113.5988256931305
        },
        {
            "docid": "22000_10",
            "document": "Neural Darwinism . Criticism of Neural \"Darwinism\" was made by Francis Crick on the basis that neuronal groups are instructed by the environment rather than undergoing blind variation. A recent review by Fernando, Szathmary and Husbands explains why Edelman's Neural Darwinism is not Darwinian because it does not contain units of evolution as defined by John Maynard Smith. It is selectionist in that it satisfies the Price equation, but there is no mechanism in Edelman's theory that explains how information can be transferred between neuronal groups. A recent theory called Evolutionary Neurodynamics being developed by Eors Szathmary and Chrisantha Fernando has proposed several means by which true replication may take place in the brain. These neuronal models have been extended by Fernando in a later paper . In the most recent model, three plasticity mechanisms i) multiplicative STDP, ii) LTD, and iii) Heterosynaptic competition, are responsible for copying of connectivity patterns from one part of the brain to another. Exactly the same plasticity rules can explain experimental data for how infants do causal learning in the experiments conducted by Alison Gopnik. It has also been shown that by adding Hebbian learning to neuronal replicators the power of neuronal evolutionary computation may actually be greater than natural selection in organisms.",
            "score": 140.3064888715744
        },
        {
            "docid": "1729542_28",
            "document": "Neural network . Many models are used; defined at different levels of abstraction, and modeling different aspects of neural systems. They range from models of the short-term behaviour of individual neurons, through models of the dynamics of neural circuitry arising from interactions between individual neurons, to models of behaviour arising from abstract neural modules that represent complete subsystems. These include models of the long-term and short-term plasticity of neural systems and its relation to learning and memory, from the individual neuron to the system level.",
            "score": 96.25114607810974
        },
        {
            "docid": "271430_22",
            "document": "Computational neuroscience . The interactions of neurons in a small network can be often reduced to simple models such as the Ising model. The statistical mechanics of such simple systems are well-characterized theoretically. There has been some recent evidence that suggests that dynamics of arbitrary neuronal networks can be reduced to pairwise interactions. It is not known, however, whether such descriptive dynamics impart any important computational function. With the emergence of two-photon microscopy and calcium imaging, we now have powerful experimental methods with which to test the new theories regarding neuronal networks.",
            "score": 113.92892527580261
        },
        {
            "docid": "39199253_2",
            "document": "Percolation (cognitive psychology) . Percolation (from the Latin word \"percolatio\", meaning filtration) is a theoretical model used to understand the way activation and diffusion of neural activity occur within neural networks. Percolation is a model used to explain how neural activity is transmitted across the various connections within the brain. Often it is easiest to understand percolation theory by explaining its use in epidemiology. Individuals that are infected with a disease can spread the disease through contact with others in their social network. Those who are more social and come into contact with more people will help to propagate the disease quicker than those who are less social. Therefore factors such as occupation and sociability influence the rate of infection. Now, if one were to think of \"neurons\" as the \"individuals\" and \"synaptic connections\" as the \"social bonds\" between people, then one can determine how easily messages between neurons will spread. When a neuron fires, the message is transmitted along all synaptic connections to other neurons until it can no longer continue. Synaptic connections are considered either open or closed (like a social or unsocial person) and messages will flow along any and all open connections until they can go no further. Just like occupation and sociability play a key role in the spread of disease, so too do the number of neurons, synaptic plasticity and long-term potentiation when talking about neural percolation.",
            "score": 109.80672812461853
        },
        {
            "docid": "25345530_45",
            "document": "Models of neural computation . The NEURON software, developed at Duke University, is a simulation environment for modeling individual neurons and networks of neurons. The NEURON environment is a self-contained environment allowing interface through its GUI or via scripting with hoc or python. The NEURON simulation engine is based on a Hodgkin\u2013Huxley type model using a Borg\u2013Graham formulation. Several examples of models written in NEURON are available from the online database ModelDB.",
            "score": 99.48408770561218
        }
    ],
    "r": [
        {
            "docid": "8402086_18",
            "document": "Computational neurogenetic modeling . Because the amount of data on the interplay of genes and neurons and their effects is not enough to construct a rigorous model,  evolutionary computation is used to optimize artificial neural networks and gene regulatory networks, a common technique being the genetic algorithm. A genetic algorithm is a process that can be used to refine models by mimicking the process of natural selection observed in biological ecosystems. The primary advantages are that, due to not requiring derivative information, it can be applied to black box problems and multimodal optimization. The typical process for using genetic algorithms to refine a gene  regulatory network is: first, create a population; next, to create offspring via a crossover operation and  evaluate their fitness; then, on a group chosen for high fitness, simulate mutation via a mutation operator;  finally, taking the now mutated group, repeat this process until a desired level of fitness is demonstrated. Methods by which artificial neural networks may alter their structure without simulated mutation and fitness selection have been developed. A dynamically evolving neural network is one approach, as the creation of new connections and new neurons can  be modeled as the system adapts to new data. This enables the network to evolve in modeling accuracy without simulated natural selection. One method by which dynamically evolving networks may be optimized, called evolving layer neuron aggregation, combines neurons with sufficiently similar input weights into one neuron. This can take place during the training of the network, referred to as online aggregation, or between periods of training, referred to as offline aggregation. Experiments have suggested that offline aggregation is more efficient.",
            "score": 191.50848388671875
        },
        {
            "docid": "579390_21",
            "document": "Gene prediction . Artificial neural networks are computational models that excel at machine learning and pattern recognition. Neural networks must be trained with example data before being able to generalise for experimental data, and tested against benchmark data. Neural networks are able to come up with approximate solutions to problems that are hard to solve algorithmically, provided there is sufficient training data. When applied to gene prediction, neural networks can be used alongside other \"ab initio\" methods to predict or identify biological features such as splice sites. One approach involves using a sliding window, which traverses the sequence data in an overlapping manner. The output at each position is a score based on whether the network thinks the window contains a donor splice site or an acceptor splice site. Larger windows offer more accuracy but also require more computational power. A neural network is an example of a signal sensor as its goal is to identify a functional site in the genome.",
            "score": 184.2248077392578
        },
        {
            "docid": "233488_23",
            "document": "Machine learning . An artificial neural network (ANN) learning algorithm, usually called \"neural network\" (NN), is a learning algorithm that is vaguely inspired by biological neural networks. Computations are structured in terms of an interconnected group of artificial neurons, processing information using a connectionist approach to computation. Modern neural networks are non-linear statistical data modeling tools. They are usually used to model complex relationships between inputs and outputs, to find patterns in data, or to capture the statistical structure in an unknown joint probability distribution between observed variables.",
            "score": 182.62115478515625
        },
        {
            "docid": "3717_69",
            "document": "Brain . Theorists have worked to understand these response patterns by constructing mathematical models of neurons and neural networks, which can be simulated using computers. Some useful models are abstract, focusing on the conceptual structure of neural algorithms rather than the details of how they are implemented in the brain; other models attempt to incorporate data about the biophysical properties of real neurons. No model on any level is yet considered to be a fully valid description of brain function, though. The essential difficulty is that sophisticated computation by neural networks requires distributed processing in which hundreds or thousands of neurons work cooperatively\u2014current methods of brain activity recording are only capable of isolating action potentials from a few dozen neurons at a time.",
            "score": 179.6765899658203
        },
        {
            "docid": "1434685_3",
            "document": "Prognostics . Data-driven prognostics usually use pattern recognition and machine learning techniques to detect changes in system states. The classical data-driven methods for nonlinear system prediction include the use of stochastic models such as the autoregressive (AR) model, the threshold AR model, the bilinear model, the projection pursuit, the multivariate adaptive regression splines, and the Volterra series expansion. Since the last decade, more interests in data-driven system state forecasting have been focused on the use of flexible models such as various types of neural networks (NNs) and neural fuzzy (NF) systems. Data-driven approaches are appropriate when the understanding of first principles of system operation is not comprehensive or when the system is sufficiently complex such that developing an accurate model is prohibitively expensive. Therefore, the principal advantages to data driven approaches is that they can often be deployed quicker and cheaper compared to other approaches, and that they can provide system-wide coverage (cf. physics-based models, which can be quite narrow in scope). The main disadvantage is that data driven approaches may have wider confidence intervals than other approaches and that they require a substantial amount of data for training. Data-driven approaches can be further subcategorized into fleet-based statistics and sensor-based conditioning. In addition, data-driven techniques also subsume cycle-counting techniques that may include domain knowledge.",
            "score": 178.869873046875
        },
        {
            "docid": "40158142_9",
            "document": "Nonlinear system identification . Artificial neural networks try loosely to imitate the network of neurons in the brain where computation takes place through a large number of simple processing elements. A typical neural network consists of a number of simple processing units interconnected to form a complex network. Layers of such units are arranged so that data is entered at the input layer and passes through either one or several intermediate layers before reaching the output layer. In supervised learning the network is trained by operating on the difference between the actual output and the desired output of the network, the prediction error, to change the connection strengths between the nodes. By iterating the weights are modified until the output error reaches an acceptable level. This process is called machine learning because the network adjusts the weights so that the output pattern is reproduced. Neural networks have been extensively studied and there are many excellent textbooks devoted to this topic in general, and more focussed textbooks which emphasise control and systems applications. There are two main problem types that can be studied using neural networks: static problems, and dynamic problems. Static problems include pattern recognition, classification, and approximation. Dynamic problems involve lagged variables and are more appropriate for system identification and related applications. Depending on the architecture of the network the training problem can be either nonlinear-in-the-parameters which involves optimisation or linear-in-the-parameters which can be solved using classical approaches. The training algorithms can be categorised into supervised, unsupervised, or reinforcement learning. Neural networks have excellent approximation properties but these are usually based on standard function approximation results using for example the Weierstrass Theorem that applies equally well to polynomials, rational functions, and other well-known models.  Neural networks have been applied extensively to system identification problems which involve nonlinear and dynamic relationships. However, classical neural networks are purely gross static approximating machines. There is no dynamics within the network. Hence when fitting dynamic models all the dynamics arise by allocating lagged inputs and outputs to the input layer of the network. The training procedure then produces the best static approximation that relates the lagged variables assigned to the input nodes to the output. There are more complex network architectures, including recurrent networks, that produce dynamics by introducing increasing orders of lagged variables to the input nodes. But in these cases it is very easy to over specify the lags and this can lead to over fitting and poor generalisation properties.  Neural networks have several advantages; they are conceptually simple, easy to train and to use, have excellent approximation properties, the concept of local and parallel processing is important and this provides integrity and fault tolerant behaviour. The biggest criticism of the classical neural network models is that the models produced are completely opaque and usually cannot be written down or analysed. It is therefore very difficult to know what is causing what, to analyse the model, or to compute dynamic characteristics from the model. Some of these points will not be relevant to all applications but they are for dynamic modelling.",
            "score": 173.0877227783203
        },
        {
            "docid": "3062721_16",
            "document": "Neuroinformatics . Biology is concerned with molecular data (from genes to cell specific expression); medicine and anatomy with the structure of synapses and systems level anatomy; engineering \u2013 electrophysiology (from single channels to scalp surface EEG), brain imaging; computer science \u2013 databases, software tools, mathematical sciences \u2013 models, chemistry \u2013 neurotransmitters, etc. Neuroscience uses all aforementioned experimental and theoretical studies to learn about the brain through its various levels. Medical and biological specialists help to identify the unique cell types, and their elements and anatomical connections. Functions of complex organic molecules and structures, including a myriad of biochemical, molecular, and genetic mechanisms which regulate and control brain function, are determined by specialists in chemistry and cell biology. Brain imaging determines structural and functional information during mental and behavioral activity. Specialists in biophysics and physiology study physical processes within neural cells neuronal networks. The data from these fields of research is analyzed and arranged in databases and neural models in order to integrate various elements into a sophisticated system; this is the point where neuroinformatics meets other disciplines.",
            "score": 172.46109008789062
        },
        {
            "docid": "34119149_26",
            "document": "Geotechnical centrifuge modeling . Centrifuge tests can also be used to obtain experimental data to verify a design procedure or a computer model. The rapid development of computational power over recent decades has revolutionized engineering analysis. Many computer models have been developed to predict the behavior of geotechnical structures during earthquakes and other loads. Before a computer model can be used with confidence, it must be proven to be valid based on evidence. The meager and unrepeatable data provided by natural earthquakes, for example, is usually insufficient for this purpose. Verification of the validity of assumptions made by a computational algorithm is especially important in the area of geotechnical engineering due to the complexity of soil behavior. Soils exhibit highly non-linear behavior, their strength and stiffness depend on their stress history and on the water pressure in the pore fluid, all of which may evolve during the loading caused by an earthquake. The computer models which are intended to simulate these phenomena are very complex and require extensive verification. Experimental data from centrifuge tests is useful for verifying assumptions made by a computational algorithm. If the results show the computer model to be inaccurate, the centrifuge test data provides insight into the physical processes which in turn stimulates the development of better computer models.",
            "score": 171.97528076171875
        },
        {
            "docid": "39619438_2",
            "document": "AnimatLab . AnimatLab is an open-source neuromechanical simulation tool that allows authors to easily build and test biomechanical models and the neural networks that control them to produce behaviors. Users can construct neural models of varied level of detail, 3D mechanical models of triangle meshes, and use muscles, motors, receptive fields, stretch sensors, and other transducers to interface the two systems. Experiments can be run in which various stimuli are applied and data is recorded, making it a useful tool for computational neuroscience. The software can also be used to model biomimetic robotic systems.",
            "score": 168.40499877929688
        },
        {
            "docid": "149353_4",
            "document": "Computational biology . Computational Biology, which includes many aspects of bioinformatics, is the science of using biological data to develop algorithms or models to understand biological systems and relationships.  Until recently, biologists did not have access to very large amounts of data. This data has now become commonplace, particularly in molecular biology and genomics. Researchers were able to develop analytical methods for interpreting biological information, but were unable to share them quickly among colleagues.  Bioinformatics began to develop in the early 1970s. It was considered the science of analyzing informatics processes of various biological systems. At this time, research in artificial intelligence was using network models of the human brain in order to generate new algorithms. This use of biological data to develop other fields pushed biological researchers to revisit the idea of using computers to evaluate and compare large data sets. By 1982, information was being shared amongst researchers through the use of punch cards. The amount of data being shared began to grow exponentially by the end of the 1980s. This required the development of new computational methods in order to quickly analyze and interpret relevant information. Since the late 1990s, computational biology has become an important part of developing emerging technologies for the field of biology. The terms computational biology and evolutionary computation have a similar name, but are not to be confused. Unlike computational biology, evolutionary computation is not concerned with modeling and analyzing biological data. It instead creates algorithms based on the ideas of evolution across species. Sometimes referred to as genetic algorithms, the research of this field can be applied to computational biology. While evolutionary computation is not inherently a part of computational biology, Computational evolutionary biology is a subfield of it.",
            "score": 167.03121948242188
        },
        {
            "docid": "27051151_69",
            "document": "Big data . Much in the same line, it has been pointed out that the decisions based on the analysis of big data are inevitably \"informed by the world as it was in the past, or, at best, as it currently is\". Fed by a large number of data on past experiences, algorithms can predict future development if the future is similar to the past. If the systems dynamics of the future change (if it is not a stationary process), the past can say little about the future. In order to make predictions in changing environments, it would be necessary to have a thorough understanding of the systems dynamic, which requires theory. As a response to this critique Alemany Oliver and Vayre suggested to use \"abductive reasoning as a first step in the research process in order to bring context to consumers\u2019 digital traces and make new theories emerge\". Additionally, it has been suggested to combine big data approaches with computer simulations, such as agent-based models and Complex Systems. Agent-based models are increasingly getting better in predicting the outcome of social complexities of even unknown future scenarios through computer simulations that are based on a collection of mutually interdependent algorithms. Finally, use of multivariate methods that probe for the latent structure of the data, such as factor analysis and cluster analysis, have proven useful as analytic approaches that go well beyond the bi-variate approaches (cross-tabs) typically employed with smaller data sets.",
            "score": 165.79444885253906
        },
        {
            "docid": "15502859_11",
            "document": "EICASLAB . EICASLAB includes the following tools and features to support the control algorithm design: The Automatic Algorithm Generation tool, starting from the \u201cplant simplified model\u201d and from the \"control required performance\" generates the control algorithm. On the basis of the plant design data, the applied control design methodology allows design of controllers with guaranteed performance without requiring any tuning in field in spite of the unavoidable uncertainty which always exists between any mathematical model built on the basis of plant design data and the plant actual performance (for fundamentals on control in presence of uncertainty see ). The designer can choose among three control basic schemes and for each one he has the option of selecting control algorithms at different level of complexity.  In synthesis, the automatically generated control is performed by the resultant of three actions: The plant's state observer task may be extended to estimate and predict the disturbance acting on the plant. The plant disturbance prediction and compensation is an original control feature, which allows significant reduction of control error.  Model Parameter Identification is a tool which allows the identification of the most appropriate values of the simplified model parameters from recorded experimental data or simulated trials performed by using the \u201cplant fine model\u201d. The parameter's \"true\" value does not exist: the model is an approximated description of the plant and then, the parameter's \"best\" value depends on the cost function adopted to evaluate the difference between model and plant. The identification method estimates the best values of the simplified model parameters from the point of view of the closed loop control design. Control Parameter Optimization is a tool which performs control parameter tuning in simulated environment. The optimization is performed numerically over a predefined simulated trial, that is for a given mission (host command sequence and disturbance acting on the plant and any other potential event related to the plant performance) and for a given functional cost associated to the plant control performance.",
            "score": 165.68748474121094
        },
        {
            "docid": "53757504_6",
            "document": "Viktor K. Jirsa . The Virtual Brain is a free open source neuroinformatics tool designed to aid in the exploration of network mechanisms of brain function and associated pathologies. TVB provides the possibility to feed computational neuronal network models with information about structural and functional imaging data including population (sEEG/EEG/MEG) activity, spatially highly resolved whole brain metabolic/vascular signals (fMRI) and global measures of neuronal connections (DTI) \u2013 for intact as well as pathologically altered connectivity. TVB is model agnostic and offers a wide range of neural population models to be used as network nodes. The software infrastructure of the Virtual Brain is composed of a functional core running the large-scale brain simulations independently or in batch mode, a web based interface to access the simulator, as well as a command line interface to develop more extensive applications. All simulations may be performed on workstations and labtops, as well as on high-performance clusters (HPCs). Manipulations of network parameters within the Virtual Brain allow researchers and clinicians to test the effects of experimental paradigms, interventions (such as stimulation and surgery) and therapeutic strategies (such as pharmaceutical interventions targeting local areas). The computational environment allows the user to visualise the simulated data in 2D and 3D and perform data analyses in the same way as commonly performed with empirical data.",
            "score": 165.0549774169922
        },
        {
            "docid": "3737445_3",
            "document": "Quantum neural network . In the computational approach to quantum neural network research, scientists try to combine artificial neural network models (which are widely used in machine learning for the important task of pattern classification) with the advantages of quantum information in order to develop more efficient algorithms (for a review, see ). One important motivation for these investigations is the difficulty to train classical neural networks, especially in big data applications. The hope is that features of quantum computing such as quantum parallelism or the effects of interference and entanglement can be used as resources. Since the technological implementation of a quantum computer is still in a premature stage, such quantum neural network models are mostly theoretical proposals that await their full implementation in physical experiments.",
            "score": 163.6550750732422
        },
        {
            "docid": "3630374_2",
            "document": "Neural computation . Neural computation is the hypothetical information processing performed by networks of neurons. Neural computation is affiliated with the philosophical tradition known as Computational Theory of Mind, also referred to as computationalism, which advances the thesis that neural computation explains cognition. The first persons to propose an account of neural activity as being computational was Warren McCullock and Walter Pitts in their seminal 1943 paper, A Logical Calculus of the Ideas Immanent in Nervous Activity. There are three general branches of computationalism, including classicism, connectionism, and computational neuroscience. All three branches agree that cognition is computation, however they disagree on what sorts of computations constitute cognition. The classicism tradition believes that computation in the brain is digital, analogous with digital computing. Both connectionism and computational neuroscience do not require that the computations which realize cognition are necessarily digital computations. However, the two branches greatly disagree upon which sorts of experimental data should be used to construct explanatory models of cognitive phenomena. Connectionists rely upon behavioral evidence to construct models to explain cognitive phenomenon, whereas computational neuroscience leverages neuroanatomical and neurophysiological information to construct mathematical models which explain cognition.",
            "score": 163.4669189453125
        },
        {
            "docid": "20017656_3",
            "document": "Human Brain Project . Fundamental to the HBP approach is to investigate the brain on different spatial and temporal scales (i.e. from the molecular to the large networks underlying higher cognitive processes, and from milliseconds to years). To achieve this goal, the HBP relies on the collaboration of scientists from diverse disciplines, including neuroscience, philosophy and computer science, to take advantage of the loop of experimental data, modelling theories and simulations. The idea is that empirical results are used to develop theories, which then foster modelling and simulations which result in predictions that are in turn verified by empirical results.",
            "score": 163.24322509765625
        },
        {
            "docid": "13793648_4",
            "document": "Alexey Ivakhnenko . The GMDH method presents a unique approach to the problem of modelling and even a new philosophy to scientific research, which is possible using modern computers. A researcher may not adhere  precisely to traditional deductive way of building models \"from general theory - to a particular model\": monitoring an object, studying its internal structure, understanding the physical principles of its operation, developing theory and testing the mathematical model of an object. Instead, the new approach is proposed \"from specified data - to a general model\": after the input of data, a researcher selects a class of models, the type of models-variants generation and sets the criterion for model selection. As most routine work is transferred to a computer, the impact of human influence on the objective result is minimised. In fact, this approach can be considered as one of the implementations of the Artificial Intelligence thesis, which states that a computer can act as powerful advisor to humans.",
            "score": 162.58932495117188
        },
        {
            "docid": "17033211_2",
            "document": "Quantitative models of the action potential . In neurophysiology, several mathematical models of the action potential have been developed, which fall into two basic types. The first type seeks to model the experimental data quantitatively, i.e., to reproduce the measurements of current and voltage exactly. The renowned Hodgkin\u2013Huxley model of the axon from the \"Loligo\" squid exemplifies such models. Although qualitatively correct, the H-H model does not describe every type of excitable membrane accurately, since it considers only two ions (sodium and potassium), each with only one type of voltage-sensitive channel. However, other ions such as calcium may be important and there is a great diversity of channels for all ions. As an example, the cardiac action potential illustrates how differently shaped action potentials can be generated on membranes with voltage-sensitive calcium channels and different types of sodium/potassium channels. The second type of mathematical model is a simplification of the first type; the goal is not to reproduce the experimental data, but to understand qualitatively the role of action potentials in neural circuits. For such a purpose, detailed physiological models may be unnecessarily complicated and may obscure the \"forest for the trees\". The Fitzhugh-Nagumo model is typical of this class, which is often studied for its entrainment behavior. Entrainment is commonly observed in nature, for example in the synchronized lighting of fireflies, which is coordinated by a burst of action potentials; entrainment can also be observed in individual neurons. Both types of models may be used to understand the behavior of small biological neural networks, such as the central pattern generators responsible for some automatic reflex actions. Such networks can generate a complex temporal pattern of action potentials that is used to coordinate muscular contractions, such as those involved in breathing or fast swimming to escape a predator.",
            "score": 162.56260681152344
        },
        {
            "docid": "2567511_17",
            "document": "Neural engineering . Scientists can use experimental observations of neuronal systems and theoretical and computational models of these systems to create Neural networks with the hopes of modeling neural systems in as realistic a manner as possible. Neural networks can be used for analyses to help design further neurotechnological devices. Specifically, researchers handle analytical or finite element modeling to determine nervous system control of movements and apply these techniques to help patients with brain injuries or disorders. Artificial neural networks can be built from theoretical and computational models and implemented on computers from theoretically devices equations or experimental results of observed behavior of neuronal systems. Models might represent ion concentration dynamics, channel kinetics, synaptic transmission, single neuron computation, oxygen metabolism, or application of dynamic system theory (LaPlaca et al. 2005). Liquid-based template assembly was used to engineer 3D neural networks from neuron-seeded microcarrier beads.",
            "score": 162.44464111328125
        },
        {
            "docid": "17787148_16",
            "document": "Cyber-physical system . Cyber-physical models for future manufacturing\u2014With the motivation a cyber-physical system, a \"coupled-model\" approach was developed. The coupled model is a digital twin of the real machine that operates in the cloud platform and simulates the health condition with an integrated knowledge from both data driven analytical algorithms as well as other available physical knowledge. The coupled model first constructs a digital image from the early design stage. System information and physical knowledge are logged during product design, based on which a simulation model is built as a reference for future analysis. Initial parameters may be statistically generalized and they can be tuned using data from testing or the manufacturing process using parameter estimation. The simulation model can be considered as a mirrored image of the real machine, which is able to continuously record and track machine condition during the later utilization stage. Finally, with ubiquitous connectivity offered by cloud computing technology, the coupled model also provides better accessibility of machine condition for factory managers in cases where physical access to actual equipment or machine data is limited. These features pave the way toward implementing cyber manufacturing.",
            "score": 162.14784240722656
        },
        {
            "docid": "2889768_11",
            "document": "Image stitching . To estimate a robust model from the data, a common method used is known as RANSAC.<br> The name RANSAC is an abbreviation for \"RANdom SAmple Consensus\". It is an iterative method for robust parameter estimation to fit mathematical models from sets of observed data points which may contain outliers. The algorithm is non-deterministic in the sense that it produces a reasonable result only with a certain probability, with this probability increasing as more iterations are performed. It being a probabilistic method means that different results will be obtained for every time the algorithm is run.  The RANSAC algorithm has found many applications in computer vision, including the simultaneous solving of the correspondence problem and the estimation of the fundamental matrix related to a pair of stereo cameras. The basic assumption of the method is that the data consists of \"inliers\", i.e., data whose distribution can be explained by some mathematical model, and \"outliers\" which are data that do not fit the model. Outliers are considered points which come from noise, erroneous measurements, or simply incorrect data. For the problem of homography estimation, RANSAC works by trying to fit several models using some of the point pairs and then checking if the models were able to relate most of the points. The best model, i.e., the homography which produces the highest number of correct matches, is then chosen as the answer for the problem thus if the ratio of number of outliers to data points is very low the RANSAC outputs a decent model fitting the data.",
            "score": 162.051513671875
        },
        {
            "docid": "47628477_3",
            "document": "Identifiability analysis . Assuming the model is defined and the regression analysis or any other model fitting could be performed to obtain the model parameters values that minimize difference between the modeled and experimental data. The goodness of fit, which represents the minimal difference between experimental and modeled data in a particular measure, does not reveal how reliable the parameter estimates are and it is not the sufficient criteria to prove the model was chosen correctly either. For example, if the experimental data was noisy or just insufficient amount of data points was processed, substitution of best fitted parameter values by orders of magnitude will not significantly influence the quality of fit. To address this issues the identifiability analysis could be applied as an important step to ensure correct choice of model, and sufficient amount of experimental data. The purpose of this analysis is either a quantified proof of correct model choice and integrality of experimental data acquired or such analysis can serve as an instrument for the detection of non-identifiable and sloppy parameters, helping planning the experiments and in building and improvement of the model at the early stages.",
            "score": 162.00250244140625
        },
        {
            "docid": "44046734_5",
            "document": "Matched molecular pair analysis . MMPA is quite useful in the field of quantitative structure\u2013activity relationship (QSAR) modelling studies. One of the issues of QSAR models is they are difficult to interpret in a chemically meaningful manner. While it can be pretty easy to interpret simple linear regression models, the most powerful algorithms like neural networks, support vector machine are similar to \"black boxes\", which provide predictions that can't be easily interpreted. This problem undermines the applicability of QSAR model in helping the medicinal chemist to make the decision. If the compound is predicted to be active against some microorganism, what are the driving factors of its activity? Or if it is predicted to be inactive, how its activity can be modulated? The black box nature of the QSAR model prevents it from addressing these crucial issues. The use of predicted MMPs allows to interpret models and identify which MMPs were learned by the model. The MMPs, which were not reproduced by the model, could correspond to experimental errors or deficiency of the model (inappropriate descriptors, too few data, etc.).",
            "score": 161.89109802246094
        },
        {
            "docid": "33818014_8",
            "document": "Nervous system network models . On a high level representation, the neurons can be viewed as connected to other neurons to form a neural network in one of three ways. A specific network can be represented as a physiologically (or anatomically) connected network and modeled that way. There are several approaches to this (see Ascoli, G.A. (2002) Sporns, O. (2007), Connectionism, Rumelhart, J. L., McClelland, J. L., and PDP Research Group (1986), Arbib, M. A. (2007)). Or, it can form a functional network that serves a certain function and modeled accordingly (Honey, C. J., Kotter, R., Breakspear, R., & Sporns, O. (2007), Arbib, M. A. (2007)). A third way is to hypothesize a theory of the functioning of the biological components of the neural system by a mathematical model, in the form of a set of mathematical equations. The variables of the equation are some or all of the neurobiological properties of the entity being modeled, such as the dimensions of the dendrite or the stimulation rate of action potential along the axon in a neuron. The mathematical equations are solved using computational techniques and the results are validated with either simulation or experimental processes. This approach to modeling is called computational neuroscience. This methodology is used to model components from the ionic level to system level of the brain. This method is applicable for modeling integrated system of biological components that carry information signal from one neuron to another via intermediate active neurons that can pass the signal through or create new or additional signals. The computational neuroscience approach is extensively used and is based on two generic models, one of cell membrane potential Goldman (1943) and Hodgkin and Katz (1949), and the other based on Hodgkin-Huxley model of action potential (information signal).",
            "score": 161.04017639160156
        },
        {
            "docid": "315084_23",
            "document": "Lip reading . Automated lip-reading has been a topic of interest in computational engineering, as well as in . The computational engineer Steve Omohundro, among others, pioneered its development. In facial animation, the aim is to generate realistic facial actions, especially mouth movements, that simulate human speech actions. Computer algorithms to deform or manipulate images of faces can be driven by heard or written language. Systems may be based on detailed models derived from facial movements (motion capture); on anatomical modelling of actions of the jaw, mouth and tongue; or on mapping of known viseme- phoneme properties. Facial animation has been used in speechreading training (demonstrating how different sounds 'look'). These systems are a subset of speech synthesis modelling which aim to deliver reliable 'text-to-(seen)-speech' outputs. A complementary aim\u2014the reverse of making faces move in speech\u2014is to develop computer algorithms that can deliver realistic interpretations of speech (i.e. a written transcript or audio record) from natural video data of a face in action: this is facial speech recognition. These models too can be sourced from a variety of data. Automatic visual speech recognition from video has been quite successful in distinguishing different languages (from a corpus of spoken language data). Demonstration models, using machine-learning algorithms, have had some success in lipreading speech elements, such as specific words, from video and for identifying hard-to-lipread phonemes from visemically similar seen mouth actions. Machine-based speechreading is now making successful use of neural-net based algorithms which use large databases of speakers and speech material (following the successful model for auditory automatic speech recognition).",
            "score": 160.5758514404297
        },
        {
            "docid": "356382_31",
            "document": "Gene regulatory network . Other work has focused on predicting the gene expression levels in a gene regulatory network. The approaches used to model gene regulatory networks have been constrained to be interpretable and, as a result, are generally simplified versions of the network. For example, Boolean networks have been used due to their simplicity and ability to handle noisy data but lose data information by having a binary representation of the genes. Also, artificial neural networks omit using a hidden layer so that they can be interpreted, losing the ability to model higher order correlations in the data. Using a model that is not constrained to be interpretable, a more accurate model can be produced. Being able to predict gene expressions more accurately provides a way to explore how drugs affect a system of genes as well as for finding which genes are interrelated in a process. This has been encouraged by the DREAM competition which promotes a competition for the best prediction algorithms. Some other recent work has used artificial neural networks with a hidden layer.",
            "score": 160.5491485595703
        },
        {
            "docid": "33413026_21",
            "document": "Artificial muscle . Control problems regarding highly nonlinear systems have generally been addressed through a trial-and-error approach through which \"fuzzy models\" (Chan et al., 2003) of the system's behavioral capacities could be teased out (from the experimental results of the specific system being tested) by a knowledgeable human expert. However, some research has employed \"real data\" (Nelles O., 2000) to train up the accuracy of a given fuzzy model while simultaneously avoiding the mathematical complexities of previous models. Ahn et al.'s experiment is simply one example of recent experiments that use modified genetic algorithms (MGAs) to train up fuzzy models using experimental input-output data from a PAM robot arm.",
            "score": 160.50616455078125
        },
        {
            "docid": "3259720_7",
            "document": "Multifactor dimensionality reduction . As illustrated above, the basic constructive induction algorithm in MDR is very simple. However, its implementation for mining patterns from real data can be computationally complex. As with any machine learning algorithm there is always concern about overfitting. That is, machine learning algorithms are good at finding patterns in completely random data. It is often difficult to determine whether a reported pattern is an important signal or just chance. One approach is to estimate the generalizability of a model to independent datasets using methods such as cross-validation. Models that describe random data typically don't generalize. Another approach is to generate many random permutations of the data to see what the data mining algorithm finds when given the chance to overfit. Permutation testing makes it possible to generate an empirical p-value for the result. Replication in independent data may also provide evidence for an MDR model but can be sensitive to difference in the data sets. These approaches have all been shown to be useful for choosing and evaluating MDR models. An important step in an machine learning exercise is interpretation. Several approaches have been used with MDR including entropy analysis and pathway analysis. Tips and approaches for using MDR to model gene-gene interactions have been reviewed.",
            "score": 159.4294891357422
        },
        {
            "docid": "19208664_25",
            "document": "Neural modeling fields . Finding patterns below noise can be an exceedingly complex problem. If an exact pattern shape is not known and depends on unknown parameters, these parameters should be found by fitting the pattern model to the data. However, when the locations and orientations of patterns are not known, it is not clear which subset of the data points should be selected for fitting. A standard approach for solving this kind of problem is multiple hypothesis testing (Singer et al. 1974). Since all combinations of subsets and models are exhaustively searched, this method faces the problem of combinatorial complexity. In the current example, noisy \u2018smile\u2019 and \u2018frown\u2019 patterns are sought. They are shown in Fig.1a without noise, and in Fig.1b with the noise, as actually measured. The true number of patterns is 3, which is not known. Therefore, at least 4 patterns should be fit to the data, to decide that 3 patterns fit best. The image size in this example is 100x100 = 10,000 points. If one attempts to fit 4 models to all subsets of 10,000 data points, computation of complexity, M ~ 10. An alternative computation by searching through the parameter space, yields lower complexity: each pattern is characterized by a 3-parameter parabolic shape. Fitting 4x3=12 parameters to 100x100 grid by a brute-force testing would take about 10 to 10 operations, still a prohibitive computational complexity. To apply NMF and dynamic logic to this problem one needs to develop parametric adaptive models of expected patterns. The models and conditional partial similarities for this case are described in details in: a uniform model for noise, Gaussian blobs for highly-fuzzy, poorly resolved patterns, and parabolic models for \u2018smiles\u2019 and \u2018frowns\u2019. The number of computer operations in this example was about 10. Thus, a problem that was not solvable due to combinatorial complexity becomes solvable using dynamic logic.",
            "score": 158.74404907226562
        },
        {
            "docid": "45329906_21",
            "document": "Solvent models . Quantitative Structure\u2013Activity Relationships (QSAR)/Quantitative Structure\u2013Property Relationships (QSPR), whilst unable to directly model the physical process occurring in a condensed solvent phase, can provide useful predictions of solvent and solvation properties and activities; such as the solubility of a solute. These methods come in a varied way from simple regression models to sophisticated machine learning methods. Generally, QSAR/QSPR methods require descriptors; these come in many different forms and are used to represent physical features and properties of a system of interest. Descriptors are generally single numerical values which hold some information about a physical property. A regression model or statistical learning model is then applied to find a correlation between the descriptor(s) and the property of interest. Once trained on some known data these model can be applied to similar unknown data to make predictions. Typically the known data comes from experimental measurement, although there is no reason why similar methods can not be used to correlate descriptor(s) with theoretical or predicted values. It is currently debated whether if more accurate experimental data was used to train these models whether the prediction from such models would be more accurate.",
            "score": 158.64419555664062
        },
        {
            "docid": "3062721_2",
            "document": "Neuroinformatics . Neuroinformatics is a research field concerned with the organization of neuroscience data by the application of computational models and analytical tools. These areas of research are important for the integration and analysis of increasingly large-volume, high-dimensional, and fine-grain experimental data. Neuroinformaticians provide computational tools, mathematical models, and create interoperable databases for clinicians and research scientists. Neuroscience is a heterogeneous field, consisting of many and various sub-disciplines (e.g., cognitive psychology, behavioral neuroscience, and behavioral genetics). In order for our understanding of the brain to continue to deepen, it is necessary that these sub-disciplines are able to share data and findings in a meaningful way; Neuroinformaticians facilitate this.",
            "score": 157.41104125976562
        },
        {
            "docid": "43502368_6",
            "document": "Vanishing gradient problem . Similar ideas have been used in feed-forward neural network for unsupervised pre-training to structure a neural network, making it first learn generally useful feature detectors. Then the network is trained further by supervised back-propagation to classify labeled data. The deep belief network model by Hinton et al. (2006) involves learning the distribution of a high level representation using successive layers of binary or real-valued latent variables. It uses a restricted Boltzmann machine to model each new layer of higher level features. Each new layer guarantees an increase on the lower-bound of the log likelihood of the data, thus improving the model, if trained properly. Once sufficiently many layers have been learned the deep architecture may be used as a generative model by reproducing the data when sampling down the model (an \"ancestral pass\") from the top level feature activations. Hinton reports that his models are effective feature extractors over high-dimensional, structured data. This work plays a key role in reintroducing the interests in deep neural network research and consequently leads to the developments of Deep learning, although deep belief network is no longer the main deep learning technique.",
            "score": 156.1280059814453
        }
    ]
}