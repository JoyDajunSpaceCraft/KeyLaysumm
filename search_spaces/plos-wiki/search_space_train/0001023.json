{
    "q": [
        {
            "docid": "57177883_3",
            "document": "DeMix . Solid tumor samples obtained from clinical practice are highly heterogeneous. They consist of multiple clonal populations of cancer cells as well as adjacent normal tissue, stromal and infiltrating immune cells. The highly heterogeneous structure of tumor tissues could complicate or bias various genomic data analysis. Removing heterogeneity is of substantial interest to isolate expression data from mixed samples \"in silico\". It is important to estimate and account for the tumor purity, or the percentage of cancer cells in the tumor sample before analyses. Owing to the marked differences between cancer and normal cells, it is possible to estimate tumor purity from high-throughput genomic or epigenomic data. DeMix is a method that has been developed to estimate the proportion and gene expression profile from cancer cells in mixed samples. In this method the mixed sample is assumed to be composed only by two cell types: cancer cells (without any a priori known gene expression profile) and normal cells (with known gene expression data, which can either come from tumor-matched or unmatched samples). This method was developed for microarray data and shows that it was important to use the raw data as input assuming it follows a log-normal distribution as is the case for microarray, instead of working with log-transformed data like most other methods did. DeMix estimates the variance of the gene expression in the normal samples and uses this in the maximum likelihood estimation to predict the cancer cell gene expression and proportions, using thus implicitly a gene-specific weight for each gene. It is the first method to follow a linear mixture of gene expression levels on data before they are log-transformed. This method analyzes data from heterogeneous tumor samples before the data are log-transformed, estimates individual level expression levels in each sample and each gene in an unmatched design.",
            "score": 82.59978711605072
        },
        {
            "docid": "1900609_6",
            "document": "External validity . An important variant of the external validity problem deals with selection bias, also known as sampling bias\u2014 that is, bias created when studies are conducted on non-representative samples of the intended population.  For example, if a clinical trial is conducted on college students, an investigator may wish to know whether the results generalize to the entire population, where attributes such as age, education, and income differ substantially from those of a typical student. The graph-based method of Bareinboim and Pearl identifies conditions under which sample selection bias can be circumvented and, when these conditions are  met, the method constructs an unbiased estimator of the average causal effect in the entire population. The main difference between  generalization from improperly sampled studies and generalization across disparate populations lies in the fact that disparities among populations are usually caused by preexisting factors, such as age or ethnicity, whereas selection bias is often caused by post-treatment conditions, for example, patients dropping out of the study, or patients selected by severity of injury. When selection is governed by post-treatment factors, unconventional re-calibration methods are required to  ensure bias-free estimation, and these methods are readily obtained from the problem's  graph.",
            "score": 100.01184439659119
        },
        {
            "docid": "40977477_17",
            "document": "Cross-species transmission . Two methods of measuring genetic variation, variable number tandem repeats (VNTRs) and single nucleotide polymorphisms (SNPs), have been very beneficial to the study of bacterial transmission. VNTRs, due to the low cost and high mutation rates, make them particularly useful to detect genetic differences in recent outbreaks, and while SNPs have a lower mutation rate per locus than VNTRs, they deliver more stable and reliable genetic relationships between isolates. Both methods are used to construct phylogenies for genetic analysis, however, SNPs are more suitable for studies on phylogenies contraction. However, it can be difficult for these methods accurately simulate CSTs everts. Estimates of CST based on phylogenys made using VNTR marker can be biased towards detecting CST events across a wide range of the parameters. SNPs tend to be less biased and variable in estimates of CST when estimations of CST rates are low and low number of SNPs is used. In general, CST rate estimates using these methods are most reliable in systems with more mutations, more markers, and high genetic differences between introduced strains. CST is very complex and models need to account for a lot of parameters to accurately represent the phenomena. Models that oversimplify reality can result in biased data. Multiple parameters such as number of mutations accumulated since introduction, stochasticity, the genetic difference of strains introduced, and the sampling effort can make unbiased estimates of CST difficult even with whole-genome sequences, especially if sampling is limited, mutation rates are low, or if pathogens were recently introduced. More information on the factors that influence CST rates is needed for the contraction of more appropriate models to study these events.",
            "score": 84.1519045829773
        },
        {
            "docid": "26685_15",
            "document": "Statistics . To use a sample as a guide to an entire population, it is important that it truly represents the overall population. Representative sampling assures that inferences and conclusions can safely extend from the sample to the population as a whole. A major problem lies in determining the extent that the sample chosen is actually representative. Statistics offers methods to estimate and correct for any bias within the sample and data collection procedures. There are also methods of experimental design for experiments that can lessen these issues at the outset of a study, strengthening its capability to discern truths about the population.",
            "score": 92.11256170272827
        },
        {
            "docid": "27579_8",
            "document": "Statistical theory . Besides the philosophy underlying statistical inference, statistical theory has the task of considering the types of questions that data analysts might want to ask about the problems they are studying and of providing data analytic techniques for answering them. Some of these tasks are: When a statistical procedure has been specified in the study protocol, then statistical theory provides well-defined probability statements for the method when applied to all populations that could have arisen from the randomization used to generate the data. This provides an objective way of estimating parameters, estimating confidence intervals, testing hypotheses, and selecting the best. Even for observational data, statistical theory provides a way of calculating a value that can be used to interpret a sample of data from a population, it can provide a means of indicating how well that value is determined by the sample, and thus a means of saying corresponding values derived for different populations are as different as they might seem; however, the reliability of inferences from post-hoc observational data is often worse than for planned randomized generation of data.",
            "score": 93.80622625350952
        },
        {
            "docid": "1792433_13",
            "document": "Maximum a posteriori estimation . While only mild conditions are required for MAP estimation to be a limiting case of Bayes estimation (under the 0-1 loss function), it is not very representative of Bayesian methods in general. This is because MAP estimates are point estimates, whereas Bayesian methods are characterized by the use of distributions to summarize data and draw inferences: thus, Bayesian methods tend to report the posterior mean or median instead, together with credible intervals. This is both because these estimators are optimal under squared-error and linear-error loss respectively - which are more representative of typical loss functions - and because the posterior distribution may not have a simple analytic form: in this case, the distribution can be simulated using Markov chain Monte Carlo techniques, while optimization to find its mode(s) may be difficult or impossible.",
            "score": 68.53341865539551
        },
        {
            "docid": "3763850_18",
            "document": "Resampling (statistics) . Subsampling is an alternative method for approximating the sampling distribution of an estimator. The two key differences to the bootstrap are: (i) the resample size is smaller than the sample size and (ii) resampling is done without replacement. The advantage of subsampling is that it is valid under much weaker conditions compared to the bootstrap. In particular, a set of sufficient conditions is that the rate of convergence of the estimator is known and that the limiting distribution is continuous; in addition, the resample (or subsample) size must tend to infinity together with the sample size but at a smaller rate, so that their ratio converges to zero. While subsampling was originally proposed for the case of independent and identically distributed (iid) data only, the methodology has been extended to cover time series data as well; in this case, one resamples blocks of subsequent data rather than individual data points. There are many cases of applied interest where subsampling leads to valid inference whereas bootstrapping does not; for example, such cases include examples where the rate of convergence of the estimator is not the square root of the sample size or when the limiting distribution is non-normal.",
            "score": 55.89914107322693
        },
        {
            "docid": "35960023_12",
            "document": "Overlapping gene . Estimates of gene overlap in bacterial genomes typically find that around one third of bacterial genes are overlapped, though usually only by a few base pairs. Most studies of overlap in bacterial genomes find evidence that overlap serves a function in gene regulation, permitting the overlapped genes to be transcriptionally and translationally co-regulated. In prokaryotic genomes, unidirectional overlaps are most common, possibly due to the tendency of adjacent prokaryotic genes to share orientation. Among unidirectional overlaps, long overlaps are more commonly read with a one-nucleotide offset in reading frame (i.e., phase 1) and short overlaps are more commonly read in phase 2. Long overlaps of greater than 60 base pairs are more common for convergent genes; however, putative long overlaps have very high rates of misannotation. Robustly validated examples of long overlaps in bacterial genomes are rare; in the well-studied model organism \"Escherichia coli\", only four gene pairs are well validated as having long, overprinted overlaps.",
            "score": 32.117913007736206
        },
        {
            "docid": "22349350_35",
            "document": "Lasso (statistics) . A number of lasso variants have been created in order to remedy certain limitations of the original technique and to make the method more useful for particular problems. Almost all of these focus on respecting or utilizing different types of dependencies among the covariates. Elastic net regularization adds an additional ridge regression-like penalty which improves performance when the number of predictors is larger than the sample size, allows the method to select strongly correlated variables together, and improves overall prediction accuracy. Group lasso allows groups of related covariates to be selected as a single unit, which can be useful in settings where it does not make sense to include some covariates without others. Further extensions of group lasso to perform variable selection within individual groups (sparse group lasso) and to allow overlap between groups (overlap group lasso) have also been developed. Fused lasso can account for the spatial or temporal characteristics of a problem, resulting in estimates that better match the structure of the system being studied. Lasso regularized models can be fit using a variety of techniques including subgradient methods, least-angle regression (LARS), and proximal gradient methods. Determining the optimal value for the regularization parameter is an important part of ensuring that the model performs well; it is typically chosen using cross-validation.",
            "score": 68.23927307128906
        },
        {
            "docid": "8450479_4",
            "document": "Bias of an estimator . All else being equal, an unbiased estimator is preferable to a biased estimator, but in practice all else is not equal, and biased estimators are frequently used, generally with small bias. When a biased estimator is used, bounds of the bias are calculated. A biased estimator may be used for various reasons: because an unbiased estimator does not exist without further assumptions about a population or is difficult to compute (as in unbiased estimation of standard deviation); because an estimator is median-unbiased but not mean-unbiased (or the reverse); because a biased estimator gives a lower value of some loss function (particularly mean squared error) compared with unbiased estimators (notably in shrinkage estimators); or because in some cases being unbiased is too strong a condition, and the only unbiased estimators are not useful. Further, mean-unbiasedness is not preserved under non-linear transformations, though median-unbiasedness is (see effect of transformations); for example, the sample variance is an unbiased estimator for the population variance, but its square root, the sample standard deviation, is a biased estimator for the population standard deviation. These are all illustrated below.",
            "score": 100.21346545219421
        },
        {
            "docid": "17692_16",
            "document": "Sampling bias . If entire segments of the population are excluded from a sample, then there are no adjustments that can produce estimates that are representative of the entire population. But if some groups are underrepresented and the degree of underrepresentation can be quantified, then sample weights can correct the bias. However, the success of the correction is limited to the selection model chosen. If certain variables are missing the methods used to correct the bias could be inaccurate.",
            "score": 82.00275468826294
        },
        {
            "docid": "531611_38",
            "document": "Foodborne illness . A main aim of this study was to compare if foodborne illness incidence had increased over time. In this study, similar methods of assessment were applied to data from circa 2000, which showed that the rate of foodborne gastroenteritis had not changed significantly over time. Two key estimates were the total number of gastroenteritis episodes each year, and the proportion considered foodborne. In circa 2010, it was estimated that 25% of all episodes of gastroenteritis were foodborne. By applying this proportion of episodes due to food to the incidence of gastroenteritis circa 2000, there were an estimated 4.3 million (90% CrI: 2.2\u20137.3 million) episodes of foodborne gastroenteritis circa 2000, although credible intervals overlap with 2010. Taking into account changes in population size, applying these equivalent methods suggests a 17% decrease in the rate of foodborne gastroenteritis between 2000 and 2010, with considerable overlap of the 90% credible intervals.",
            "score": 77.34545600414276
        },
        {
            "docid": "53363521_20",
            "document": "Third-generation sequencing . Given the short reads produced by the current generation of sequencing technologies, de novo assembly is a major computational problem. It is normally approached by an iterative process of finding and connecting sequence reads with sensible overlaps. Various computational and statistical techniques, such as de bruijn graphs and overlap layout consensus graphs, have been leveraged to solve this problem. Nonetheless, due to the highly repetitive nature of eukaryotic genomes, accurate and complete reconstruction of genome sequences in de novo assembly remains challenging. Pair end reads have been posed as a possible solution, though exact fragment lengths are often unknown and must be approximated. Long read lengths offered by third generation sequencing may alleviate many of the challenges currently faced by de novo genome assemblies. For example, if an entire repetitive region can be sequenced unambiguously in a single read, no computation inference would be required. Computational methods have been proposed to alleviate the issue of high error rates. For example, in one study, it was demonstrated that de novo assembly of a microbial genome using PacBio sequencing alone performed superior to that of second generation sequencing.",
            "score": 46.39811944961548
        },
        {
            "docid": "26685_13",
            "document": "Statistics . When a census is not feasible, a chosen subset of the population called a sample is studied. Once a sample that is representative of the population is determined, data is collected for the sample members in an observational or experimental setting. Again, descriptive statistics can be used to summarize the sample data. However, the drawing of the sample has been subject to an element of randomness, hence the established numerical descriptors from the sample are also due to uncertainty. To still draw meaningful conclusions about the entire population, inferential statistics is needed. It uses patterns in the sample data to draw inferences about the population represented, accounting for randomness. These inferences may take the form of: answering yes/no questions about the data (hypothesis testing), estimating numerical characteristics of the data (estimation), describing associations within the data (correlation) and modeling relationships within the data (for example, using regression analysis). Inference can extend to forecasting, prediction and estimation of unobserved values either in or associated with the population being studied; it can include extrapolation and interpolation of time series or spatial data, and can also include data mining.",
            "score": 104.36696422100067
        },
        {
            "docid": "8979437_24",
            "document": "Stochastic approximation . Here formula_84 is an unbiased estimator of formula_75. If formula_74 depends on formula_17, there is in general no natural way of generating a random outcome formula_84 that is an unbiased estimator of the gradient. In some special cases when either IPA or likelihood ratio methods are applicable, then one is able to obtain an unbiased gradient estimator formula_84. If formula_74 is viewed as some \"fundamental\" underlying random process that is generated \"independently\" of formula_17, and under some regularization conditions for derivative-integral interchange operations so that formula_92, then formula_93 gives the fundamental gradient unbiased estimate. However, for some applications we have to use finite-difference methods in which formula_84 has a conditional expectation close to formula_75 but not exactly equal to it.",
            "score": 49.40123629570007
        },
        {
            "docid": "27590_29",
            "document": "Standard deviation . One can find the standard deviation of an entire population in cases (such as standardized testing) where every member of a population is sampled. In cases where that cannot be done, the standard deviation \"\u03c3\" is estimated by examining a random sample taken from the population and computing a statistic of the sample, which is used as an estimate of the population standard deviation. Such a statistic is called an estimator, and the estimator (or the value of the estimator, namely the estimate) is called a sample standard deviation, and is denoted by \"s\" (possibly with modifiers). However, unlike in the case of estimating the population mean, for which the sample mean is a simple estimator with many desirable properties (unbiased, efficient, maximum likelihood), there is no single estimator for the standard deviation with all these properties, and unbiased estimation of standard deviation is a very technically involved problem. Most often, the standard deviation is estimated using the \"corrected sample standard deviation\" (using \"N\"\u00a0\u2212\u00a01), defined below, and this is often referred to as the \"sample standard deviation\", without qualifiers. However, other estimators are better in other respects: the uncorrected estimator (using \"N\") yields lower mean squared error, while using \"N\"\u00a0\u2212\u00a01.5 (for the normal distribution) almost completely eliminates bias.",
            "score": 96.72044062614441
        },
        {
            "docid": "485024_22",
            "document": "Statistics New Zealand . Statistical techniques such as sampling and weighting can reduce data gathering expenses while surveying, although it requires careful attention by statisticians. For example, census counts are rather straightforward in a statistical sense since it is a straight count of bodies, purchases, opinions, actions. But it is expensive to interview everybody. And there are many non-census topics requiring further study which the agency is asked to do. One way to reduce costs is by selecting smaller samples which hopefully represent the population being studied. This is a highly cost-effective way to get accurate information. Choosing a good sample sometimes requires complex statistical work to make sure the sample is, indeed, truly representative of the population under study. If conditions do not permit representative sampling but known benchmark statistics are known (possibly from earlier census data), it's still possible to generate accurate information by \"weighting the data\" to \"distort it back\", so to speak, to compensate for the distortion caused by the unrepresentative sample. For example, suppose an area of Wellington has a known percentage of women\u2014say 53% -- which is believed to be accurate from a previous census count; a study is done a few years later; a sample is chosen; questionnaires are processed; but of the returning questionnaires, 57% are women. It's possible statistically to give men slightly greater weight to account for the discrepancy, and the result is more accurate data. The agency discusses different methodologies on its website. A statement explains why weighting is sometimes necessary: \"Statistics New Zealand is improving the methodology used for the 2001 Household Economic Survey through the use of integrated weighting. This is a relatively recently developed method of adjusting the statistical output of a survey to match population benchmarks. In particular, it takes account of undercoverage in the survey of specified population groups. Integrated weighting improves the robustness and accuracy of survey estimates. It also reduces the effect of bias in estimates resulting from undercoverage, as well as reducing the level of sampling error for benchmark variables.\" All statistical work presents opportunities for error, but it's possible to reduce error to manageable amounts. A statement reads: \"An important aim of our ongoing work is to understand, manage, control and report on all known sources of error... which simply reflect the inherent variability that exists among the units we are seeking to measure... This variability manifests itself in \"sampling error\" when we use samples for cost-effectiveness reasons to estimate characteristics about a population. Sampling errors are relatively easy to measure... Other sources reflect process, measurement and inference errors, and are referred to as \"non-sampling error\". It is not possible to eliminate all sources of error. However, our continued efforts at understanding and managing variability and error ensure we are exercising a high level of control on all known sources of error.\"",
            "score": 104.05019891262054
        },
        {
            "docid": "16406885_9",
            "document": "Proportionator . The proportionator applies PPS to counting cells. The PPS is employed to gain efficiency in the sampling, and not to produce a weighted estimate, such as a volume weighted estimate. The optical fractionator is the older standard for estimating the number of cells in an unbiased manner. The optical fractionator, and other sampling methods, has some statistical uncertainty. This uncertainty is due to the variance of the sampling even though the result is unbiased. The efficiency of the sampling can be determined by use of the coefficient of error, or CE. This value describes the variance of the sampling method. Often, biological sampling is done at a CE of .05.",
            "score": 81.35253500938416
        },
        {
            "docid": "3651405_3",
            "document": "Michael Lynch (geneticist) . Population genetics principles, phylogenetic analyses, rate calculations, and allele frequency spectra of derived SNPs are employed to understand evolutionary mechanisms behind eukaryotic genome complexity. Hypotheses around the ideas that eukaryotic genome complexity evolved as a result of a passive response to reduced population size, deleterious newly arisen introns in species of Daphnia, genomic response to alterations in population size and mutation rates in E. coli and the evolutionary fates of duplicate genes in of species of Paramecium using complete genomic sequencing are investigated. Lynch is working on designing methods that allow for ascertaining the population-genetic features using high-throughput genome sequence data and take into account uncertainties due to low coverage and error-prone sequences.",
            "score": 65.13525605201721
        },
        {
            "docid": "31320115_15",
            "document": "Probability box . Interval measurements can also be used to generalize distributional estimates based on the method of matching moments or maximum likelihood, that make shape assumptions such as normality or lognormality, etc. Although the measurement uncertainty can be treated rigorously, the resulting distributional p-box generally will not be rigorous when it is a sample estimate based on only a subsample of the possible values. But, because these calculations take account of the dependence between the parameters of the distribution, they will often yield tighter p-boxes than could be obtained by treating the interval estimates of the parameters as unrelated as is done for distributional p-boxes.",
            "score": 63.71813082695007
        },
        {
            "docid": "46968364_46",
            "document": "Inferring horizontal gene transfer . Parametric and phylogenetic methods draw on different sources of information; it is therefore difficult to make general statements about their relative performance. Conceptual arguments can however be invoked. While parametric methods are limited to the analysis of single or pairs of genomes, phylogenetic methods provide a natural framework to take advantage of the information contained in multiple genomes. In many cases, segments of genomes inferred as HGT based on their anomalous composition can also be recognised as such on the basis of phylogenetic analyses or through their mere absence in genomes of related organisms. In addition, phylogenetic methods rely on explicit models of sequence evolution, which provide a well-understood framework for parameter inference, hypothesis testing, and model selection. This is reflected in the literature, which tends to favour phylogenetic methods as the standard of proof for HGT. The use of phylogenetic methods thus appears to be the preferred standard, especially given that the increase in computational power coupled with algorithmic improvements has made them more tractable, and that the ever denser sampling of genomes lends more power to these tests.",
            "score": 49.38020575046539
        },
        {
            "docid": "16540708_8",
            "document": "Health in Sudan . Children less than five years of age had the highest mortality rate and DALYs, emphasizing the known effect of malaria on this population group. Females lost more DALYs than males in all age groups, which altered the picture displayed by the incidence rates alone. The epidemiological estimates and DALYs calculations in this study form a basis for comparing interventions that affect mortality and morbidity differently, by comparing the amount of burden averted by them. The DALYs would mark the position of malaria among the rest of the diseases, if compared to DALYs due to other diseases. Uncertainty around the estimates should be considered when using them for decision making and further work should quantify this uncertainty to facilitate utilisation of the results. More epidemiological studies are required to fill in the gaps revealed in this study and to more accurately determine the effect and burden of the disease.",
            "score": 54.80784606933594
        },
        {
            "docid": "30901421_3",
            "document": "Microfluidic whole genome haplotyping . Whole genome haplotyping is the process of resolving personal haplotypes on a whole genome basis. Current methods of next generation sequencing are capable of identifying heterozygous loci, but they are not well suited to identify which polymorphisms exist on the same (in cis) or allelic (in trans) strand of DNA. Haplotype information contributes to the understanding of the potential functional effects of variants in cis or in trans. Haplotypes are more frequently resolved by inference through comparison with parental genotypes, or from population samples using statistical computational methods to determine linkage disequilibrium between markers.  Direct haplotyping is possible through isolation of chromosomes or chromosome segments. Most molecular biology techniques for haplotyping can accurately determine haplotypes of only a limited region of the genome.  Whole genome direct haplotyping involves the resolution of haplotype at the whole genome level, usually through the isolation of individual chromosomes.",
            "score": 64.67650556564331
        },
        {
            "docid": "30898832_17",
            "document": "Augmented Lagrangian method . Stochastic optimization considers the problem of minimizing a loss function with access to noisy samples of the (gradient of the) function. The goal is to have an estimate of the optimal parameter (minimizer) per new sample. ADMM is originally a batch method. However, with some modifications it can also be used for stochastic optimization. Since in stochastic setting we only have access to noisy samples of gradient, we use an inexact approximation of the Lagrangian as",
            "score": 70.11627340316772
        },
        {
            "docid": "18690851_16",
            "document": "Genetic correlation . Genetic correlations require a genetically informative sample. They can be estimated by using breeding experiments on two traits of known heritability and selecting on one trait to measure the change in the other trait (allowing inferring the genetic correlation), family/adoption/twin studies (analyzed using SEMs or DeFries\u2013Fulker extremes analysis), molecular estimation of relatedness such as GCTA, methods employing polygenic scores like LD score regression, BOLT-REML, CPBayes, or HESS, comparison of genome-wide SNP hits in GWASes (as a loose lower bound), and phenotypic correlations of populations with at least some related individuals. The methods are related to Haseman-Elston regression & PCGC regression. Such methods are typically genome-wide, but it is also possible to estimate genetic correlations for specific variants or genome regions.",
            "score": 69.10235619544983
        },
        {
            "docid": "38889813_40",
            "document": "Viral phylodynamics . All of these phylogeographic studies necessarily suffer from limitations in the worldwide sampling of influenza viruses. For example, the relative importance of tropical Africa and India has yet to be uncovered. Additionally, the phylogeographic methods used in these studies (see section on phylogeographic methods) make inferences of the ancestral locations and migration rates on only the samples at hand, rather than on the population in which these samples are embedded. Because of this, study-specific sampling procedures are a concern in extrapolating to population-level inferences. However, through joint epidemiological and evolutionary simulations, Bedford et al. show that their estimates of migration rates appear robust to a large degree of undersampling or oversampling of a particular region. Further methodological progress is required to more fully address these issues.",
            "score": 88.35231685638428
        },
        {
            "docid": "3146632_9",
            "document": "Bland\u2013Altman plot . Bland-Altman plots are extensively used to evaluate the agreement among two different instruments or two measurements techniques. Bland-Altman plots allow identification of any systematic difference between the measurements (i.e., fixed bias) or possible outliers. The mean difference is the estimated bias, and the SD of the differences measures the random fluctuations around this mean. If the mean value of the difference differs significantly from 0 on the basis of a 1-sample t-test, this indicates the presence of fixed bias. If there is a consistent bias, it can be adjusted for by subtracting the mean difference from the new method. It is common to compute 95% limits of agreement for each comparison (average difference \u00b1 1.96 standard deviation of the difference), which tells us how far apart measurements by 2 methods were more likely to be for most individuals. If the differences within mean \u00b1 1.96 SD are not clinically important, the two methods may be used interchangeably. The 95% limits of agreement can be unreliable estimates of the population parameters especially for small sample sizes so, when comparing methods or assessing repeatability, it is important to calculate confidence intervals for 95% limits of agreement. This can be done by Bland and Altman's approximate method or by more precise methods.",
            "score": 92.53556513786316
        },
        {
            "docid": "32257005_3",
            "document": "Total survey error . Total survey error is the difference between a population parameter (such as the mean, total or proportion) and the estimate of that parameter based on the sample survey or census. It has two components: sampling error and nonsampling error. Sampling error, which occurs in sample surveys but not censuses results from the variability inherent in using a randomly selected fraction of the population for estimation. Nonsampling error, which occurs in surveys and censuses alike, is the sum of all other errors, including errors in frame construction, sample selection, data collection, data processing and estimation methods.",
            "score": 89.98800134658813
        },
        {
            "docid": "34450103_22",
            "document": "Probability bounds analysis . Some analysts use sampling-based approaches to computing probability bounds, including Monte Carlo simulation, Latin hypercube methods or importance sampling. These approaches cannot assure mathematical rigor in the result because such simulation methods are approximations, although their performance can generally be improved simply by increasing the number of replications in the simulation. Thus, unlike the analytical theorems or methods based on mathematical programming, sampling-based calculations usually cannot produce verified computations. However, sampling-based methods can be very useful in addressing a variety of problems which are computationally difficult to solve analytically or even to rigorously bound. One important example is the use of Cauchy-deviate sampling to avoid the curse of dimensionality in propagating interval uncertainty through high-dimensional problems.",
            "score": 83.43764424324036
        },
        {
            "docid": "6885770_8",
            "document": "Bootstrapping (statistics) . As an example, assume we are interested in the average (or mean) height of people worldwide. We cannot measure all the people in the global population, so instead we sample only a tiny part of it, and measure that. Assume the sample is of size N; that is, we measure the heights of N individuals. From that single sample, only one estimate of the mean can be obtained. In order to reason about the population, we need some sense of the variability of the mean that we have computed. The simplest bootstrap method involves taking the original data set of N heights, and, using a computer, sampling from it to form a new sample (called a 'resample' or bootstrap sample) that is also of size N. The bootstrap sample is taken from the original by using sampling with replacement (e.g. we might 'resample' 5 times from [1,2,3,4,5] and get [2,5,4,4,1]), so, assuming N is sufficiently large, for all practical purposes there is virtually zero probability that it will be identical to the original \"real\" sample. This process is repeated a large number of times (typically 1,000 or 10,000 times), and for each of these bootstrap samples we compute its mean (each of these are called bootstrap estimates). We now can create a histogram of bootstrap means. This histogram provides an estimate of the shape of the distribution of the sample mean from which we can answer questions about how much the mean varies across samples. (The method here, described for the mean, can be applied to almost any other statistic or estimator.)",
            "score": 117.17002296447754
        },
        {
            "docid": "23467232_32",
            "document": "Didier Sornette . With Wei-Xing Zhou, he has introduced the \"thermal optimal path\" method as a novel method to quantify the dynamical evolution of lead-lag structures between two time series. The method consists of constructing a distance matrix based on the matching of all sample data pairs between the two time series, as in recurrence plots. Then, the lag\u2013lead structure is searched for as the optimal path in the distance matrix landscape that minimizes the total mismatch between the two time series, and that obeys a one-to-one causal matching condition. The problem is solved mathematically by transfer matrix techniques, matching the TOP method to the problem of random directed polymers interacting with random substrates. Applications include the study of the relationships between inflation, inflation change, GDP growth rate and unemployment rate, volatilities of the US inflation rate versus economic growth rates, the US stock market versus the Federal funds rate and Treasury bond yields and the UK and US real-estate versus monetary policies.",
            "score": 56.905070304870605
        },
        {
            "docid": "40977477_18",
            "document": "Cross-species transmission . The process of using genetic markers to estimate CST rates should take into account several important factors to reduce bias. One, is that the phylogenetic tree constructed in the analysis needs to capture the underlying epidemiological process generating the tree. The models need to account for how the genetic variability of a pathogen influences a disease in a species, not just general differences in genomic structure. Two, the strength of the analysis will depend on the amount of mutation accumulated since the pathogen was introduced in the system. This is due to many models using amount of mutations as an indicator of CST frequency. Therefore, efforts are focused on estimating either time since introduction or the substitution rate of the marker (from laboratory experiments or genomic comparative analysis). This is important not only when using the MPR method but also for Likelihood approaches that require an estimation of the mutation rate. Three, CST will also affect disease prevalence in the potential host, so combining both epidemiological time series data with genetic data may be an excellent approach to CST study",
            "score": 61.42866349220276
        }
    ],
    "r": [
        {
            "docid": "160361_40",
            "document": "Sampling (statistics) . Systematic sampling theory can be used to create a probability proportionate to size sample. This is done by treating each count within the size variable as a single sampling unit. Samples are then identified by selecting at even intervals among these counts within the size variable. This method is sometimes called PPS-sequential or monetary unit sampling in the case of audits or forensic sampling. \"Example: Suppose we have six schools with populations of 150, 180, 200, 220, 260, and\u00a0490 students respectively (total 1500 students), and we want to use student population as the basis for a PPS sample of size three. To do this, we could allocate the first school numbers 1\u00a0to\u00a0150, the second school 151 to 330\u00a0(=\u00a0150\u00a0+\u00a0180), the third school 331 to 530, and so on to the last school (1011 to\u00a01500). We then generate a random start between 1 and 500 (equal to\u00a01500/3) and count through the school populations by multiples of 500. If our random start was 137, we would select the schools which have been allocated numbers 137, 637, and\u00a01137, i.e. the first, fourth, and sixth schools.\" The PPS approach can improve accuracy for a given sample size by concentrating sample on large elements that have the greatest impact on population estimates. PPS sampling is commonly used for surveys of businesses, where element size varies greatly and auxiliary information is often available\u2014for instance, a survey attempting to measure the number of guest-nights spent in hotels might use each hotel's number of rooms as an auxiliary variable. In some cases, an older measurement of the variable of interest can be used as an auxiliary variable when attempting to produce more current estimates.",
            "score": 118.34201049804688
        },
        {
            "docid": "6885770_8",
            "document": "Bootstrapping (statistics) . As an example, assume we are interested in the average (or mean) height of people worldwide. We cannot measure all the people in the global population, so instead we sample only a tiny part of it, and measure that. Assume the sample is of size N; that is, we measure the heights of N individuals. From that single sample, only one estimate of the mean can be obtained. In order to reason about the population, we need some sense of the variability of the mean that we have computed. The simplest bootstrap method involves taking the original data set of N heights, and, using a computer, sampling from it to form a new sample (called a 'resample' or bootstrap sample) that is also of size N. The bootstrap sample is taken from the original by using sampling with replacement (e.g. we might 'resample' 5 times from [1,2,3,4,5] and get [2,5,4,4,1]), so, assuming N is sufficiently large, for all practical purposes there is virtually zero probability that it will be identical to the original \"real\" sample. This process is repeated a large number of times (typically 1,000 or 10,000 times), and for each of these bootstrap samples we compute its mean (each of these are called bootstrap estimates). We now can create a histogram of bootstrap means. This histogram provides an estimate of the shape of the distribution of the sample mean from which we can answer questions about how much the mean varies across samples. (The method here, described for the mean, can be applied to almost any other statistic or estimator.)",
            "score": 117.17002868652344
        },
        {
            "docid": "277315_20",
            "document": "Opinion poll . Polls based on samples of populations are subject to sampling error which reflects the effects of chance and uncertainty in the sampling process. Sampling polls rely on the law of large numbers to measure the opinions of the whole population based only on a subset, and for this purpose the absolute size of the sample is important, but the percentage of the whole population is not important (unless it happens to be close to the sample size). The possible difference between the sample and whole population is often expressed as a margin of error - usually defined as the radius of a 95% confidence interval for a particular statistic. One example is the percent of people who prefer product A versus product B. When a single, global margin of error is reported for a survey, it refers to the maximum margin of error for all reported percentages using the full sample from the survey. If the statistic is a percentage, this maximum margin of error can be calculated as the radius of the confidence interval for a reported percentage of 50%. Others suggest that a poll with a random sample of 1,000 people has margin of sampling error of \u00b13% for the estimated percentage of the whole population.",
            "score": 111.60487365722656
        },
        {
            "docid": "26685_41",
            "document": "Statistics . Most studies only sample part of a population, so results don't fully represent the whole population. Any estimates obtained from the sample only approximate the population value. Confidence intervals allow statisticians to express how closely the sample estimate matches the true value in the whole population. Often they are expressed as 95% confidence intervals. Formally, a 95% confidence interval for a value is a range where, if the sampling and analysis were repeated under the same conditions (yielding a different dataset), the interval would include the true (population) value in 95% of all possible cases. This does \"not\" imply that the probability that the true value is in the confidence interval is 95%. From the frequentist perspective, such a claim does not even make sense, as the true value is not a random variable. Either the true value is or is not within the given interval. However, it is true that, before any data are sampled and given a plan for how to construct the confidence interval, the probability is 95% that the yet-to-be-calculated interval will cover the true value: at this point, the limits of the interval are yet-to-be-observed random variables. One approach that does yield an interval that can be interpreted as having a given probability of containing the true value is to use a credible interval from Bayesian statistics: this approach depends on a different way of interpreting what is meant by \"probability\", that is as a Bayesian probability.",
            "score": 109.54439544677734
        },
        {
            "docid": "4196163_10",
            "document": "Fixation index . where formula_16 and formula_17 represent the average number of pairwise differences between two individuals sampled from different sub-populations (formula_16) or from the same sub-population (formula_19). The average pairwise difference within a population can be calculated as the sum of the pairwise differences divided by the number of pairs. However, this estimator is biased when sample sizes are small or if they vary between populations. Therefore, more elaborate methods are used to compute F in practice. Two of the most widely used procedures are the estimator by Weir & Cockerham (1984), or performing an Analysis of molecular variance. A list of implementations is available at the end of this article.",
            "score": 105.20188903808594
        },
        {
            "docid": "26685_13",
            "document": "Statistics . When a census is not feasible, a chosen subset of the population called a sample is studied. Once a sample that is representative of the population is determined, data is collected for the sample members in an observational or experimental setting. Again, descriptive statistics can be used to summarize the sample data. However, the drawing of the sample has been subject to an element of randomness, hence the established numerical descriptors from the sample are also due to uncertainty. To still draw meaningful conclusions about the entire population, inferential statistics is needed. It uses patterns in the sample data to draw inferences about the population represented, accounting for randomness. These inferences may take the form of: answering yes/no questions about the data (hypothesis testing), estimating numerical characteristics of the data (estimation), describing associations within the data (correlation) and modeling relationships within the data (for example, using regression analysis). Inference can extend to forecasting, prediction and estimation of unobserved values either in or associated with the population being studied; it can include extrapolation and interpolation of time series or spatial data, and can also include data mining.",
            "score": 104.3669662475586
        },
        {
            "docid": "485024_22",
            "document": "Statistics New Zealand . Statistical techniques such as sampling and weighting can reduce data gathering expenses while surveying, although it requires careful attention by statisticians. For example, census counts are rather straightforward in a statistical sense since it is a straight count of bodies, purchases, opinions, actions. But it is expensive to interview everybody. And there are many non-census topics requiring further study which the agency is asked to do. One way to reduce costs is by selecting smaller samples which hopefully represent the population being studied. This is a highly cost-effective way to get accurate information. Choosing a good sample sometimes requires complex statistical work to make sure the sample is, indeed, truly representative of the population under study. If conditions do not permit representative sampling but known benchmark statistics are known (possibly from earlier census data), it's still possible to generate accurate information by \"weighting the data\" to \"distort it back\", so to speak, to compensate for the distortion caused by the unrepresentative sample. For example, suppose an area of Wellington has a known percentage of women\u2014say 53% -- which is believed to be accurate from a previous census count; a study is done a few years later; a sample is chosen; questionnaires are processed; but of the returning questionnaires, 57% are women. It's possible statistically to give men slightly greater weight to account for the discrepancy, and the result is more accurate data. The agency discusses different methodologies on its website. A statement explains why weighting is sometimes necessary: \"Statistics New Zealand is improving the methodology used for the 2001 Household Economic Survey through the use of integrated weighting. This is a relatively recently developed method of adjusting the statistical output of a survey to match population benchmarks. In particular, it takes account of undercoverage in the survey of specified population groups. Integrated weighting improves the robustness and accuracy of survey estimates. It also reduces the effect of bias in estimates resulting from undercoverage, as well as reducing the level of sampling error for benchmark variables.\" All statistical work presents opportunities for error, but it's possible to reduce error to manageable amounts. A statement reads: \"An important aim of our ongoing work is to understand, manage, control and report on all known sources of error... which simply reflect the inherent variability that exists among the units we are seeking to measure... This variability manifests itself in \"sampling error\" when we use samples for cost-effectiveness reasons to estimate characteristics about a population. Sampling errors are relatively easy to measure... Other sources reflect process, measurement and inference errors, and are referred to as \"non-sampling error\". It is not possible to eliminate all sources of error. However, our continued efforts at understanding and managing variability and error ensure we are exercising a high level of control on all known sources of error.\"",
            "score": 104.05020141601562
        },
        {
            "docid": "238695_15",
            "document": "Power (statistics) . Power analysis is appropriate when the concern is with the correct rejection of a false null hypothesis. In many contexts, the issue is less about determining if there is or is not a difference but rather with getting a more refined estimate of the population effect size. For example, if we were expecting a population correlation between intelligence and job performance of around 0.50, a sample size of 20 will give us approximately 80% power (alpha = 0.05, two-tail) to reject the null hypothesis of zero correlation. However, in doing this study we are probably more interested in knowing whether the correlation is 0.30 or 0.60 or 0.50. In this context we would need a much larger sample size in order to reduce the confidence interval of our estimate to a range that is acceptable for our purposes. Techniques similar to those employed in a traditional power analysis can be used to determine the sample size required for the width of a confidence interval to be less than a given value.",
            "score": 102.94699096679688
        },
        {
            "docid": "160361_13",
            "document": "Sampling (statistics) . A probability sample is a sample in which every unit in the population has a chance (greater than zero) of being selected in the sample, and this probability can be accurately determined. The combination of these traits makes it possible to produce unbiased estimates of population totals, by weighting sampled units according to their probability of selection. \"Example: We want to estimate the total income of adults living in a given street. We visit each household in that street, identify all adults living there, and randomly select one adult from each household. (For example, we can allocate each person a random number, generated from a uniform distribution between 0 and 1, and select the person with the highest number in each household). We then interview the selected person and find their income.\"",
            "score": 102.29220581054688
        },
        {
            "docid": "524392_11",
            "document": "Species diversity . In general, sets with many individuals can be expected to have higher species diversity than sets with fewer individuals. When species diversity values are compared among sets, sampling efforts need to be standardised in an appropriate way for the comparisons to yield ecologically meaningful results. Resampling methods can be used to bring samples of different sizes to a common footing. Species discovery curves and the number of species only represented by one or a few individuals can be used to help in estimating how representative the available sample is of the population from which it was drawn.",
            "score": 101.00269317626953
        },
        {
            "docid": "160361_16",
            "document": "Sampling (statistics) . Nonprobability sampling is any sampling method where some elements of the population have \"no\" chance of selection (these are sometimes referred to as 'out of coverage'/'undercovered'), or where the probability of selection can't be accurately determined. It involves the selection of elements based on assumptions regarding the population of interest, which forms the criteria for selection. Hence, because the selection of elements is nonrandom, nonprobability sampling does not allow the estimation of sampling errors. These conditions give rise to exclusion bias, placing limits on how much information a sample can provide about the population. Information about the relationship between sample and population is limited, making it difficult to extrapolate from the sample to the population. \"Example: We visit every household in a given street, and interview the first person to answer the door. In any household with more than one occupant, this is a nonprobability sample, because some people are more likely to answer the door (e.g. an unemployed person who spends most of their time at home is more likely to answer than an employed housemate who might be at work when the interviewer calls) and it's not practical to calculate these probabilities.\" Nonprobability sampling methods include convenience sampling, quota sampling and purposive sampling. In addition, nonresponse effects may turn \"any\" probability design into a nonprobability design if the characteristics of nonresponse are not well understood, since nonresponse effectively modifies each element's probability of being sampled.",
            "score": 100.84990692138672
        },
        {
            "docid": "8450479_4",
            "document": "Bias of an estimator . All else being equal, an unbiased estimator is preferable to a biased estimator, but in practice all else is not equal, and biased estimators are frequently used, generally with small bias. When a biased estimator is used, bounds of the bias are calculated. A biased estimator may be used for various reasons: because an unbiased estimator does not exist without further assumptions about a population or is difficult to compute (as in unbiased estimation of standard deviation); because an estimator is median-unbiased but not mean-unbiased (or the reverse); because a biased estimator gives a lower value of some loss function (particularly mean squared error) compared with unbiased estimators (notably in shrinkage estimators); or because in some cases being unbiased is too strong a condition, and the only unbiased estimators are not useful. Further, mean-unbiasedness is not preserved under non-linear transformations, though median-unbiasedness is (see effect of transformations); for example, the sample variance is an unbiased estimator for the population variance, but its square root, the sample standard deviation, is a biased estimator for the population standard deviation. These are all illustrated below.",
            "score": 100.21347045898438
        },
        {
            "docid": "1900609_6",
            "document": "External validity . An important variant of the external validity problem deals with selection bias, also known as sampling bias\u2014 that is, bias created when studies are conducted on non-representative samples of the intended population.  For example, if a clinical trial is conducted on college students, an investigator may wish to know whether the results generalize to the entire population, where attributes such as age, education, and income differ substantially from those of a typical student. The graph-based method of Bareinboim and Pearl identifies conditions under which sample selection bias can be circumvented and, when these conditions are  met, the method constructs an unbiased estimator of the average causal effect in the entire population. The main difference between  generalization from improperly sampled studies and generalization across disparate populations lies in the fact that disparities among populations are usually caused by preexisting factors, such as age or ethnicity, whereas selection bias is often caused by post-treatment conditions, for example, patients dropping out of the study, or patients selected by severity of injury. When selection is governed by post-treatment factors, unconventional re-calibration methods are required to  ensure bias-free estimation, and these methods are readily obtained from the problem's  graph.",
            "score": 100.01184844970703
        },
        {
            "docid": "1955561_4",
            "document": "Sampling error . Random sampling, and its derived terms such as sampling error, imply specific procedures for gathering and analyzing data that are rigorously applied as a method for arriving at results considered representative of a given population as a whole. Despite a common misunderstanding, \"random\" does not mean the same thing as \"chance\" as this idea is often used in describing situations of uncertainty, nor is it the same as projections based on an assessed probability or frequency. Sampling always refers to a procedure of gathering data from a small aggregation of individuals that is purportedly representative of a larger grouping which must in principle be capable of being measured as a totality. Random sampling is used precisely to ensure a truly representative sample from which to draw conclusions, in which the same results would be arrived at if one had included the entirety of the population instead. Random sampling (and sampling error) can only be used to gather information about a single defined point in time. If additional data is gathered (other things remaining constant) then comparison across time periods may be possible. However, this comparison is distinct from any sampling itself. As a method for gathering data within the field of statistics, random sampling is recognized as clearly distinct from the causal process that one is trying to measure. The conducting of research itself may lead to certain outcomes affecting the researched group, but this effect is not what is called sampling error. Sampling error always refers to the recognized limitations of any supposedly representative sample population in reflecting the larger totality, and the error refers only to the discrepancy that may result from judging the whole on the basis of a much smaller number. This is only an \"error\" in the sense that it would automatically be corrected if the totality were itself assessed. The term has no real meaning outside of statistics.",
            "score": 99.63786315917969
        },
        {
            "docid": "1955561_2",
            "document": "Sampling error . In statistics, sampling error is incurred when the statistical characteristics of a population are estimated from a subset, or sample, of that population. Since the sample does not include all members of the population, statistics on the sample, such as means and quantiles, generally differ from the characteristics of the entire population, which are known as parameters. For example, if one measures the height of a thousand individuals from a country of one million, the average height of the thousand is typically not the same as the average height of all one million people in the country. Since sampling is typically done to determine the characteristics of a whole population, the difference between the sample and population values is considered an error. Exact measurement of sampling error is generally not feasible since the true population values are unknown; however, sampling error can often be estimated by probabilistic modeling of the sample.",
            "score": 98.27812194824219
        },
        {
            "docid": "5797_11",
            "document": "Cluster sampling . Two-stage cluster sampling, a simple case of multistage sampling, is obtained by selecting cluster samples in the first stage and then selecting sample of elements from every sampled cluster. Consider a population of \"N\" clusters in total. In the first stage, \"n\" clusters are selected using ordinary cluster sampling method. In the second stage, simple random sampling is usually used. It is used separately in every cluster and the numbers of elements selected from different clusters are not necessarily equal. The total number of clusters \"N\", number of clusters selected \"n\", and numbers of elements from selected clusters need to be pre-determined by the survey designer. Two-stage cluster sampling aims at minimizing survey costs and at the same time controlling the uncertainty related to estimates of interest. This method can be used in health and social sciences. For instance, researchers used two-stage cluster sampling to generate a representative sample of the Iraqi population to conduct mortality surveys. Sampling in this method can be quicker and more reliable than other methods, which is why this method is now used frequently.",
            "score": 97.59666442871094
        },
        {
            "docid": "27596_2",
            "document": "Stratified sampling . In statistics, stratified sampling is a method of sampling from a population. In statistical surveys, when subpopulations within an overall population vary, it is advantageous to sample each subpopulation (stratum) independently. Stratification is the process of dividing members of the population into homogeneous subgroups before sampling. The strata should be mutually exclusive: every element in the population must be assigned to only one stratum. The strata should also be collectively exhaustive: no population element can be excluded. Then simple random sampling or systematic sampling is applied within each stratum. The objective is to improve the precision of the sample by reducing sampling error. It can produce a weighted mean that has less variability than the arithmetic mean of a simple random sample of the population.",
            "score": 97.59185791015625
        },
        {
            "docid": "5797_2",
            "document": "Cluster sampling . Cluster sampling is a sampling plan used when mutually homogeneous yet internally heterogeneous groupings are evident in a statistical population. It is often used in marketing research. In this sampling plan, the total population is divided into these groups (known as clusters) and a simple random sample of the groups is selected. The elements in each cluster are then sampled. If all elements in each sampled cluster are sampled, then this is referred to as a \"one-stage\" cluster sampling plan. If a simple random subsample of elements is selected within each of these groups, this is referred to as a \"two-stage\" cluster sampling plan. A common motivation for cluster sampling is to reduce the total number of interviews and costs given the desired accuracy. For a fixed sample size, the expected random error is smaller when most of the variation in the population is present internally within the groups, and not between the groups.",
            "score": 97.19633483886719
        },
        {
            "docid": "27590_29",
            "document": "Standard deviation . One can find the standard deviation of an entire population in cases (such as standardized testing) where every member of a population is sampled. In cases where that cannot be done, the standard deviation \"\u03c3\" is estimated by examining a random sample taken from the population and computing a statistic of the sample, which is used as an estimate of the population standard deviation. Such a statistic is called an estimator, and the estimator (or the value of the estimator, namely the estimate) is called a sample standard deviation, and is denoted by \"s\" (possibly with modifiers). However, unlike in the case of estimating the population mean, for which the sample mean is a simple estimator with many desirable properties (unbiased, efficient, maximum likelihood), there is no single estimator for the standard deviation with all these properties, and unbiased estimation of standard deviation is a very technically involved problem. Most often, the standard deviation is estimated using the \"corrected sample standard deviation\" (using \"N\"\u00a0\u2212\u00a01), defined below, and this is often referred to as the \"sample standard deviation\", without qualifiers. However, other estimators are better in other respects: the uncorrected estimator (using \"N\") yields lower mean squared error, while using \"N\"\u00a0\u2212\u00a01.5 (for the normal distribution) almost completely eliminates bias.",
            "score": 96.72044372558594
        },
        {
            "docid": "22228846_40",
            "document": "Environmental monitoring . Ranked set sampling is an innovative design that can be highly useful and cost efficient in obtaining better estimates of mean concentration levels in soil and other environmental media by explicitly incorporating the professional judgment of a field investigator or a field screening measurement method to pick specific sampling locations in the field. Ranked set sampling uses a two-phase sampling design that identifies sets of field locations, utilizes inexpensive measurements to rank locations within each set, and then selects one location from each set for sampling. In ranked set sampling, m sets (each of size r) of field locations are identified using simple random sampling. The locations are ranked independently within each set using professional judgment or inexpensive, fast, or surrogate measurements. One sampling unit from each set is then selected (based on the observed ranks) for subsequent measurement using a more accurate and reliable (hence, more expensive) method for the contaminant of interest. Relative to simple random sampling, this design results in more representative samples and so leads to more precise estimates of the population parameters. Ranked set sampling is useful when the cost of locating and ranking locations in the field is low compared to laboratory measurements. It is also appropriate when an inexpensive auxiliary variable (based on expert knowledge or measurement) is available to rank population units with respect to the variable of interest. To use this design effectively, it is important that the ranking method and analytical method are strongly correlated.",
            "score": 96.55117797851562
        },
        {
            "docid": "20542427_4",
            "document": "Stratification (clinical trials) . Stratified random sampling designs divide the population into homogeneous strata, and an appropriate number of participants are chosen at random from each strata. Proportionate stratified sampling involves selecting participants from each strata in proportions that match the general population. This method can be used to improve the sample's representativeness of the population, by ensuring that characteristics (and their proportions) of the study sample reflect the characteristics of the population. Alternatively, disproportionate sampling can be used when the strata being compared differ greatly in size, as this allows for minorities to be sufficiently represented.",
            "score": 95.96820068359375
        },
        {
            "docid": "27596_4",
            "document": "Stratified sampling . Assume that we need to estimate average number of votes for each candidate in an election. Assume that country has 3 towns: Town A has 1 million factory workers, Town B has 2 million office workers and Town C has 3 million retirees. We can choose to get a random sample of size 60 over the entire population but there is some chance that the random sample turns out to be not well balanced across these towns and hence is biased causing a significant error in estimation. Instead if we choose to take a random sample of 10, 20 and 30 from Town A, B and C respectively then we can produce a smaller error in estimation for the same total size of sample.",
            "score": 95.7256851196289
        },
        {
            "docid": "833690_4",
            "document": "Tolerance interval . However, if we have only a sample from the population, we know only the sample mean formula_4 and sample standard deviation formula_5, which are only estimates of the true parameters. In that case, formula_6 will not necessarily include 95% of the population, due to variance in these estimates. A tolerance interval bounds this variance by introducing a confidence level formula_7, which is the confidence with which this interval actually includes the specified proportion of the population. For a normally distributed population, a z-score can be transformed into a \"\"k\" factor\" or tolerance factor for a given formula_7 via lookup tables or several approximation formulas. \"As the degrees of freedom approach infinity, the prediction and tolerance intervals become equal.\"",
            "score": 95.49205017089844
        },
        {
            "docid": "2940730_9",
            "document": "Random assignment . Random sampling is a related, but distinct process. Random sampling is recruiting participants in a way that they represent a larger population. Because most basic statistical tests require the hypothesis of an independent randomly sampled population, random assignment is the desired assignment method because it provides control for all attributes of the members of the samples\u2014in contrast to matching on only one or more variables\u2014and provides the mathematical basis for estimating the likelihood of group equivalence for characteristics one is interested in, both for pretreatment checks on equivalence and the evaluation of post treatment results using inferential statistics. More advanced statistical modeling can be used to adapt the inference to the sampling method.",
            "score": 95.31889343261719
        },
        {
            "docid": "22958_9",
            "document": "Sample space . In statistics, inferences are made about characteristics of a population by studying a sample of that population's individuals. In order to arrive at a sample that presents an unbiased estimate of the true characteristics of the population, statisticians often seek to study a simple random sample\u2014that is, a sample in which every individual in the population is equally likely to be included. The result of this is that every possible combination of individuals who could be chosen for the sample is also equally likely (that is, the space of simple random samples of a given size from a given population is composed of equally likely outcomes).",
            "score": 94.8094482421875
        },
        {
            "docid": "1507717_2",
            "document": "Mark and recapture . Mark and recapture is a method commonly used in ecology to estimate an animal population's size. A portion of the population is captured, marked, and released. Later, another portion is captured and the number of marked individuals within the sample is counted. Since the number of marked individuals within the second sample should be proportional to the number of marked individuals in the whole population, an estimate of the total population size can be obtained by dividing the number of marked individuals by the proportion of marked individuals in the second sample. The method is most useful when it is not practical to count all the individuals in the population. Other names for this method, or closely related methods, include capture-recapture, capture-mark-recapture, mark-recapture, sight-resight, mark-release-recapture, multiple systems estimation, band recovery, the Petersen method, and the Lincoln method.",
            "score": 94.80209350585938
        },
        {
            "docid": "1477405_27",
            "document": "Snowball sampling . Snowball sampling is a recruitment method that employs research into participants' social networks to access specific populations. According to research mentioned in the paper written by Kath Browne, using social networks to research is accessible. In this research, Kath Browne used social networks to research non-heterosexual women. Snowball sampling is often used because the population under investigation is hard to approachable either due to low numbers of potential participants or the sensitivity of the topic. The author indicated the recruitment technique of snowball sampling, which uses interpersonal relations and connections within people.Due to the use of social networks and interpersonal relations, snowball sampling forms how individuals act and interact in focus groups, couple interviews and interviews. As a result, snowball sampling not only results in the recruitment of particular samples, use of this technique produces participants'accounts of their lives. To help mitigate these risks, it is important to not rely on any one single method of sampling to gather data about a target sector. In order to most accurately obtain information, a company must do everything it possibly can to ensure that the sampling is controlled. Also, it is imperative that the correct personnel is used to execute the actual sampling, because one missed opportunity could skew the results.",
            "score": 94.71582794189453
        },
        {
            "docid": "12106854_4",
            "document": "Statistical benchmarking . One property usually common to the weights formula_12 described here is that if we sum them over all sampled formula_1, then this sum is an estimate of the total number of units formula_1 in the population (for example, the total employment, or the total number of items). Because we have a sample, this estimate of the total number of units in the population will differ from the true population total. Similarly, the estimate of total formula_6 (where we sum formula_10 for all sampled formula_1) will also differ from true population total.",
            "score": 94.67792510986328
        },
        {
            "docid": "6889_4",
            "document": "Census . A census can be contrasted with sampling in which information is obtained only from a subset of a population; typically main population estimates are updated by such intercensal estimates. Modern census data are commonly used for research, business marketing, and planning, and as a baseline for designing sample surveys by providing a sampling frame such as an address register. Census counts are necessary to adjust samples to be representative of a population by weighting them as is common in opinion polling. Similarly, stratification requires knowledge of the relative sizes of different population strata which can be derived from census enumerations. In some countries, the census provides the official counts used to apportion the number of elected representatives to regions (sometimes controversially \u2013 e.g., \"Utah v. Evans\"). In many cases, a carefully chosen random sample can provide more accurate information than attempts to get a population census.",
            "score": 94.40132141113281
        },
        {
            "docid": "27590_35",
            "document": "Standard deviation . Taking square roots reintroduces bias (because the square root is a nonlinear function, which does not commute with the expectation), yielding the corrected sample standard deviation, denoted by \"s:\" As explained above, while \"s\" is an unbiased estimator for the population variance, \"s\" is still a biased estimator for the population standard deviation, though markedly less biased than the uncorrected sample standard deviation. This estimator is commonly used and generally known simply as the \"sample standard deviation\". The bias may still be large for small samples (\"N\" less than 10). As sample size increases, the amount of bias decreases. We obtain more information and the difference between formula_25 and formula_26 becomes smaller.",
            "score": 94.37852478027344
        },
        {
            "docid": "2893954_5",
            "document": "Bulgarian Turks . A Y-DNA genetic study on Slavic peoples and some of their neighbours published two statistical distributions of distance because of the volume of details studied, based on pairwise F values, the Turks from Bulgaria are most related to Anatolian Turks, thereafter to Italians, Bulgarians and others]; while according to the R values, the Turks from Bulgaria are most related to Bulgarians, thereafter to Macedonians, Anatolian Turks, Serbs and the rest, while Balts and North Slavs remain most unrelated according to them both. The study claims that the F genetic distances reflect interpopulation relationships between the compared populations much better than their stepwise-based analogues, but that at the same time the genetic variation was more profoundly calculated by R. F and R calculate allele (haplotype or microsatellite) frequencies among populations and the distribution of evolutionary distances among alleles. R is based on the number of repeat differences between alleles at each microsatellite locus and is proposed to be better for most typical sample sizes, when data consist of variation at microsatellite loci or of nucleotide sequence (haplotype) information, the method may be unreliable unless a large number of loci are used. A nonsignificant test suggests that F should be preferred or when there is high gene flow within populations, F calculations are based on allele identity, it is likely to perform better than counterparts based on allele size information, the method depends on mutation rate, sometimes can likely provide biased estimate, but R will not perform necessarily better. A Bulgarian and other population studies observed concluded that when there is not much differiation, both statistical means show similar results, otherwise R is often superior to the F. However, no procedure has been developed to date for testing whether single-locus R and F estimates are significantly different.",
            "score": 94.0856704711914
        },
        {
            "docid": "20890526_10",
            "document": "Bootstrapping populations . Note that the accuracy with which a parameter distribution law of populations compatible with a sample is obtained is not a function of the sample size. Instead, it is a function of the number of seeds we draw. In turn, this number is purely a matter of computational time but does not require any extension of the observed data. With other bootstrapping methods focusing on a generation of sample replicas (like those proposed by ) the accuracy of the estimate distributions depends on the sample size.",
            "score": 93.99593353271484
        }
    ]
}