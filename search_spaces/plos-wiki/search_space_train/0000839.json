{
    "q": [
        {
            "docid": "24965027_5",
            "document": "Cognitive neuroscience of visual object recognition . Visual recognition processing has been typically viewed as a bottom-up hierarchy in which information is processed sequentially with increasing complexities, where lower-level cortical processors, such as the primary visual cortex, are at the bottom of the processing hierarchy and higher-level cortical processors, such as the inferotemporal cortex (IT), are at the top, where recognition is facilitated. A most recognized bottom-up hierarchical theory is David Marr's theory of vision. In contrast, an increasingly popular recognition processing theory, is that of top-down processing. One model, proposed by Moshe Bar (2003), describes a \"shortcut\" method in which early visual inputs are sent, partially analyzed, from the early visual cortex to the prefrontal cortex (PFC). Possible interpretations of the crude visual input is generated in the PFC and then sent to the inferotemporal cortex (IT) subsequently activating relevant object representations which are then incorporated into the slower, bottom-up process. This \"shortcut\" is meant to minimize the amount of object representations required for matching thereby facilitating object recognition. Lesion studies have supported this proposal with findings of slower response times for individuals with PFC lesions, suggesting use of only the bottom-up processing.",
            "score": 64.92754912376404
        },
        {
            "docid": "35982062_6",
            "document": "Biased Competition Theory . There are two major neural pathways that process the information in the visual field; the ventral stream and the dorsal stream. The two pathways run in parallel and are both working simultaneously. The ventral stream is important for object recognition and often referred to as the \u201cwhat\u201d system of the brain; it projects to the inferior temporal cortex. The dorsal stream is important for spatial perception and performance and is referred to as the \u201cwhere\u201d system which projects to the posterior parietal cortex. According to the biased competition theory, an individual\u2019s visual system has limited capacity to process information about multiple objects at any given time. For example, if an individual was presented with two stimuli (objects) and was asked to identify attributes of each object at the same time, the individual\u2019s performance would be worse in comparison to if the objects were presented separately. This suggests multiple objects presented simultaneously in the visual field will compete for neural representation due to limited processing resources. Single cell recording studies conducted by Kastner and Ungerleider examined the neural mechanisms behind the biased competition theory. In their experiment the size of the receptive field's (RF) of neurons within the visual cortex were examined. A single visual stimulus was presented alone in a neuron\u2019s RF, followed with another stimulus presented simultaneously within the same RF. The single \u2018effective\u2019 stimuli produced a low firing rate, whereas the two stimuli presented together produced a high firing rate. The response to the paired stimuli was reduced. This suggests that when two stimuli are presented together within a neuron\u2019s RF, the stimuli are processed in a mutually suppressive manner, rather than being processed independently. This suppression process, according to Kastner and Ungerleider, occurs when two stimuli are presented together because they compete for neural representation, due to limited cognitive processing capacity. The RF experiment suggests that as the number of objects increase, the information available for each object will decrease due to increased neural workload (suppression), and decreased cognitive capacity. In order for an object in the visual field or RF be efficiently processed, there needs to be a way to bias these neurological resources towards the object. Attention prioritizes task relevant objects, biasing this process. For example, this bias can be towards an object which is currently attended to in the visual field or RF, or towards the object that is most relevant to one\u2019s behavior. Functional magnetic resonance imaging (fMRI) has shown that biased competition theory can explain the observed attention effects at a neuronal level. Attention effects bias the internal weight (strengthens connections) of task relevant features toward the attended object. This was shown by Reddy, Kanwisher, and van Rullen who found an increase in oxygenated blood to a specific neuron following a locational cue. Further neurological support comes from neurophysiological studies which have shown that attention results from Top-down biasing, which in turn influences neuronal spiking. In sum, external inputs affect the Top-down guidance of attention, which bias specific neurons in the brain.",
            "score": 84.59790563583374
        },
        {
            "docid": "35982062_8",
            "document": "Biased Competition Theory . Bottom-up processes are characterized by an absence of higher level direction in sensory processing. It primarily relies on sensory information and incoming sensory information is the starting point for all Bottom-up processing. Bottom-up refers to when a feature stands out in a visual search. This is commonly called the \u201cpop-out\u201d effect. Salient features like bright colors, movement and big objects make the object \u201cpop-out\u201d of the visual search. \u201cPop-out\u201d features can often attract attention without conscious processing. Objects that stand out are often given priority (bias) in processing. Bottom-up processing is data driven, and according to this stimuli are perceived on the basis of the data which is being experienced through the senses. Evidence suggests that simultaneously presented stimuli do in fact compete in order to be represented in the visual cortex, with stimuli mutually suppressing each other to gain this representation. This was examined by Reynolds and colleagues, who looked at the size of neurons\u2019 receptive field\u2019s within the visual cortex. It was found that the presentation of a single stimulus resulted in a low firing rate while two stimuli presented together resulted in a higher firing rate. Reynolds and colleagues also found that when comparing the neural response of an individually presented visual stimulus to responses gathered from simultaneously presented stimuli, the responses of the concurrent presented stimuli were less than the sum of the responses gathered when each stimuli was presented alone. This suggests that two stimuli presented together increase neural work load required for attention. This increased neural load creates suppressive processes and causes the stimuli to compete for neural representation in the brain. Proulx and Egeth predicted that brighter objects would bias attention in favor of that object. Another prediction is that larger objects would bias the attention in favor of that object. The experiment was a computer-based visual search task, where participants searched for a target among distractions. The results of the study suggested that when irrelevant stimuli were large or bright, attention was biased towards the irrelevant objects, prioritizing them for cognitive processing. This research shows the effects of Bottom-up (stimulus-driven) processing on biased competition theory.",
            "score": 87.66847479343414
        },
        {
            "docid": "6082997_17",
            "document": "Filling-in . \"Perceptual filling-in\", in its simplest definition, is simply the filling-in of information that is not directly given to the sensory input. The missing information is inferred or extrapolated from visual data acquired in a different part of the visual field. Examples of filling-in phenomena include lightness assignment to surfaces from information of contrast across the edges and completion of features and textures across the blind spot, based on the features and textures that are detected in the visible part of the image. In this definition, it is clear that a filling-in process involves a rearrangement of visual information, in which activity in one region of the visual field (i.e. edges) is assigned to other regions (surfaces). In any event, the total amount of information available is not increased, being determined by the retinal input, and any rearrangement of information is useful only if it brings the information contained in the image into a form that is more easily analyzed by our brain.",
            "score": 67.90327382087708
        },
        {
            "docid": "1534578_8",
            "document": "Motion perception . The motion direction of a contour is ambiguous, because the motion component parallel to the line cannot be inferred based on the visual input. This means that a variety of contours of different orientations moving at different speeds can cause identical responses in a motion sensitive neuron in the visual system.",
            "score": 52.74901223182678
        },
        {
            "docid": "5664_64",
            "document": "Consciousness . In neuroscience, a great deal of effort has gone into investigating how the perceived world of conscious awareness is constructed inside the brain. The process is generally thought to involve two primary mechanisms: (1) hierarchical processing of sensory inputs, and (2) memory. Signals arising from sensory organs are transmitted to the brain and then processed in a series of stages, which extract multiple types of information from the raw input. In the visual system, for example, sensory signals from the eyes are transmitted to the thalamus and then to the primary visual cortex; inside the cerebral cortex they are sent to areas that extract features such as three-dimensional structure, shape, color, and motion. Memory comes into play in at least two ways. First, it allows sensory information to be evaluated in the context of previous experience. Second, and even more importantly, working memory allows information to be integrated over time so that it can generate a stable representation of the world\u2014Gerald Edelman expressed this point vividly by titling one of his books about consciousness \"The Remembered Present\". In computational neuroscience, Bayesian approaches to brain function have been used to understand both the evaluation of sensory information in light of previous experience, and the integration of information over time. Bayesian models of the brain are probabilistic inference models, in which the brain takes advantage of prior knowledge to interpret uncertain sensory inputs in order to formulate a conscious percept; Bayesian models have successfully predicted many perceptual phenomena in vision and the nonvisual senses.",
            "score": 98.73018622398376
        },
        {
            "docid": "2534964_14",
            "document": "Sensory processing . Perhaps one of the most studied sensory integrations is the relationship between vision and audition. These two senses perceive the same objects in the world in different ways, and by combining the two, they help us understand this information better. Vision dominates our perception of the world around us. This is because visual spatial information is one of the most reliable sensory modalities. Visual stimuli are recorded directly onto the retina, and there are few, if any, external distortions that provide incorrect information to the brain about the true location of an object. Other spatial information is not as reliable as visual spatial information. For example, consider auditory spatial input. The location of an object can sometimes be determined solely on its sound, but the sensory input can easily be modified or altered, thus giving a less reliable spatial representation of the object. Auditory information therefore is not spatially represented unlike visual stimuli. But once one has the spatial mapping from the visual information, multisensory integration helps bring the information from both the visual and auditory stimuli together to make a more robust mapping.",
            "score": 59.754127502441406
        },
        {
            "docid": "32528_24",
            "document": "Visual cortex . It is argued that the entire ventral visual-to-hippocampal stream is important for visual memory. This theory, unlike the dominant one, predicts that object-recognition memory (ORM) alterations could result from the manipulation in V2, an area that is highly interconnected within the ventral stream of visual cortices. In the monkey brain, this area receives strong feedforward connections from the primary visual cortex (V1) and sends strong projections to other secondary visual cortices (V3, V4, and V5). Most of the neurons of this area are tuned to simple visual characteristics such as orientation, spatial frequency, size, color, and shape. Anatomical studies implicate layer 3 of area V2 in visual-information processing. In contrast to layer 3, layer 6 of the visual cortex is composed of many types of neurons, and their response to visual stimuli is more complex.",
            "score": 53.212674140930176
        },
        {
            "docid": "739262_12",
            "document": "Neural correlate . Using such design, Nikos Logothetis and colleagues discovered perception-reflecting neurons in the temporal lobe. They created an experimental situation in which conflicting images were presented to different eyes (\"i.e.\", binocular rivalry). Under such conditions, human subjects report bistable percepts: they perceive alternatively one or the other image. Logothetis and colleagues trained the monkeys to report with their arm movements which image they perceived. Interestingly, temporal lobe neurons in Logothetis experiments often reflected what the monkeys' perceived. Neurons with such properties were less frequently observed in the primary visual cortex that corresponds to relatively early stages of visual processing. Another set of experiments using binocular rivalry in humans showed that certain layers of the cortex can be excluded as candidates of the neural correlate of consciousness. Logothetis and colleagues switched the images between eyes during the percept of one of the images. Surprisingly the percept stayed stable. This means that the conscious percept stayed stable and at the same time the primary input to layer 4, which is the input layer, in the visual cortex changed. Therefore layer 4 can not be a part of the neural correlate of consciousness. Mikhail Lebedev and their colleagues observed a similar phenomenon in monkey prefrontal cortex. In their experiments monkeys reported the perceived direction of visual stimulus movement (which could be an illusion) by making eye movements. Some prefrontal cortex neurons represented actual and some represented perceived displacements of the stimulus. Observation of perception related neurons in prefrontal cortex is consistent with the theory of Christof Koch and Francis Crick who postulated that neural correlate of consciousness resides in prefrontal cortex. Proponents of distributed neuronal processing may likely dispute the view that consciousness has a precise localization in the brain.",
            "score": 90.0108904838562
        },
        {
            "docid": "4231622_6",
            "document": "Inferior temporal gyrus . The light energy that comes from the rays bouncing off of an object is converted into chemical energy by the cells in the retina of the eye. This chemical energy is then converted into action potentials that are transferred through the optic nerve and across the optic chiasm, where it is first processed by the lateral geniculate nucleus of the thalamus. From there the information is sent to the primary visual cortex, region V1. It then travels from the visual areas in the occipital lobe to the parietal and temporal lobes via two distinct anatomical streams. These two cortical visual systems were classified by Ungerleider and Mishkin (1982, see two-streams hypothesis). One stream travels ventrally to the inferior temporal cortex (from V1 to V2 then through V4 to ITC) while the other travels dorsally to the posterior parietal cortex. They are labeled the \u201cwhat\u201d and \u201cwhere\u201d streams, respectively. The Inferior Temporal Cortex receives information from the ventral stream, understandably so, as it is known to be a region essential in recognizing patterns, faces, and objects.  The understanding at the single-cell level of the IT cortex and its role of utilizing memory to identify objects and or process the visual field based on color and form visual information is a relatively recent in neuroscience. Early research indicated that the cellular connections of the temporal lobe to other memory associated areas of the brain \u2013 namely the hippocampus, the amygdala, the prefrontal cortex, among others. These cellular connections have recently been found to explain unique elements of memory, suggesting that unique single-cells can be linked to specific unique types and even specific memories. Research into the single-cell understanding of the IT cortex reveals many compelling characteristics of these cells: single-cells with similar selectivity of memory are clustered together across the cortical layers of the IT cortex; the temporal lobe neurons have recently been shown to display learning behaviors and possibly relate to long-term memory; and, cortical memory within the IT cortex is likely to be enhanced over time thanks to the influence of the afferent-neurons of the medial-temporal region. Further research of the single-cells of the IT cortex suggests that these cells not only have a direct link to the visual system pathway but also are deliberate in the visual stimuli they respond to: in certain cases, the single-cell IT cortex neurons do not initiate responses when spots or slits, namely simple visual stimuli, are present in the visual field; however, when complicated objects are put in place, this initiates a response in the single-cell neurons of the IT cortex. This provides evidence that not only are the single-cell neurons of the IT cortex related in having a unique specific response to visual stimuli but rather that each individual single-cell neuron has a specific response to a specific stimuli. The same study also reveals how the magnitude of the response of these single-cell neurons of the IT cortex do not change due to color and size but are only influenced by the shape. This led to even more interesting observations where specific IT neurons have been linked to the recognition of faces and hands. This is very interesting as to the possibility of relating to neurological disorders of prosopagnosia and explaining the complexity and interest in the human hand. Additional research form this study goes into more depth on the role of \"face neurons\" and \"hand neurons\" involved in the IT cortex.  The significance of the single-cell function in the IT cortex is that it is another pathway in addition to the lateral geniculate pathway that processes most visual system: this raises questions about how does it benefit our visual information processing in addition to normal visual pathways and what other functional units are involved in additional visual information processing.",
            "score": 56.89368808269501
        },
        {
            "docid": "53953041_3",
            "document": "Predictive coding . Theoretical ancestors to predictive coding date back as early as 1860 with Helmholz\u2019s concept of unconscious inference (Clark, 2013). Unconscious inference refers to the idea that the human brain fills in visual information to make sense of a scene. For example, if something is relatively smaller than another object in the visual field, the brain uses that information as a likely cue of depth, such that the perceiver ultimately (and involuntarily) experiences depth. The understanding of perception as the interaction between sensory stimuli (bottom-up) and conceptual knowledge (top-down) continued to be established by Jerome Bruner (psychologist) who, starting in the 1940s, studied the ways in which needs, motivations and expectations influence perception, research that came to be known as 'New Look' psychology. In 1981, McClelland and Rumelhart in their seminal paper examined the interaction between processing features (lines and contours) which form letters, which in turn form words. While the features suggest the presence of a word, they found that when letters were situated in the context of a word, people were able to identify them faster than when they were situated in a non-word without semantic context. McClelland and Rumelhart\u2019s parallel processing model describes perception as the meeting of top-down (conceptual) and bottom-up (sensory) elements.",
            "score": 95.35944175720215
        },
        {
            "docid": "9598046_4",
            "document": "Parallax scanning . In his 1995 book, Foundations of Vision, Brian Wandell states, \"Perception is an interpretation of the retinal image, not a description. Information in the retinal image may be interpreted in many different ways. Because we begin with ambiguous information, we cannot make deductions from the retinal image, only inferences. ...we have learned that the visual system succeeds in interpreting images because of statistical regularities present in the visual environment and hence in the retinal image. These regularities permit the visual system to use fragmentary information present in the retinal image to draw accurate inferences about the physical cause of the image. For example, when we make inferences from the retinal image, the knowledge that we live in a three-dimensional world is essential to the correct interpretation of the image. Often, we are made aware of the existence of these powerful interpretations and their assumptions when they are in error, that is, when we discover a visual illusion.\"",
            "score": 54.85125434398651
        },
        {
            "docid": "21280496_20",
            "document": "Visual perception . Another type of the unconscious inference hypothesis (based on probabilities) has recently been revived in so-called Bayesian studies of visual perception. Proponents of this approach consider that the visual system performs some form of Bayesian inference to derive a perception from sensory data. However, it is not clear how proponents of this view derive, in principle, the relevant probabilities required by the Bayesian equation. Models based on this idea have been used to describe various visual perceptual functions, such as the perception of motion, the perception of depth, and figure-ground perception. The \"wholly empirical theory of perception\" is a related and newer approach that rationalizes visual perception without explicitly invoking Bayesian formalisms.",
            "score": 84.2406120300293
        },
        {
            "docid": "959782_5",
            "document": "Mike May (skier) . The effect of visual loss affects the development of the visual cortex of the brain\u2014the visual impairment causes the occipital lobe to lose its sensitivity in perceiving spatial processing. Siu and Morley (2008) proposed that following seven days of visual deprivation, a potential decrease in vision may occur. They also found an increasing degree of visual impairment following thirty-day and 120-day periods of deprivation. The Siu and Morley study suggests that the function of the brain is dependent upon visual input.",
            "score": 45.78793120384216
        },
        {
            "docid": "41048019_7",
            "document": "Unconscious inference . The reason, Helmholtz suggested, lies in the way visual sensory impressions are processed neurologically. The higher cortical centres responsible for conscious deliberation are not involved in the formation of visual impressions. However, as the process is spontaneous and automatic, we are unable to account for just how we arrived at our judgments. Through our eyes, we necessarily \"perceive things as real\", for the results of the unconscious conclusions are interpretations which \"are urged on our consciousness, so to speak, as if an external power had constrained us, over which our will has no control\".",
            "score": 47.17296767234802
        },
        {
            "docid": "7151320_17",
            "document": "Recovery from blindness . The effect of visual loss has an impact in the development of the visual cortex of the brain. The visual impairment causes the occipital lobe to lose its sensitivity in perceiving spatial processing. Sui and Morley (2008) proposed that after 7 days of visual deprivation, a potential decrease in vision may occur. They also found an increasing visual impairment with deprivation after 30 days and 120 days. This study suggests that the function of the brain depends on visual input. Michael lost his eyesight at age 3, when his vision was still not fully developed to distinguish shapes, drawings or images clearly. It would be difficult for him to be able to describe the world compared to a normal sighted person. For instance, Michael would have trouble differentiating complex shapes, dimension and orientations of objects. Hannan (2006) hypothesized that the temporal visual cortex uses prior memory and experiences to make sense of shapes, colours and forms. She proposed that the long-term effect of blindness in the visual cortex is the lack of recognition of spatial cues.",
            "score": 54.978224754333496
        },
        {
            "docid": "4231622_3",
            "document": "Inferior temporal gyrus . The inferior temporal gyrus is the anterior region of the temporal lobe located underneath the central temporal sulcus. The primary function of the occipital temporal gyrus \u2013 otherwise referenced as IT cortex \u2013 is associated with visual stimuli processing, namely visual object recognition, and has been suggested by recent experimental results as the final location of the ventral cortical visual system. The IT cortex in humans is also known as the Inferior Temporal Gyrus since it has been located to a specific region of the human temporal lobe. The IT processes visual stimuli of objects in our field of vision, and is involved with memory and memory recall to identify that object; it is involved with the processing and perception created by visual stimuli amplified in the V1, V2, V3, and V4 regions of the occipital lobe. This region processes the color and form of the object in the visual field and is responsible for producing the \u201cwhat\u201d from this visual stimuli, or in other words identifying the object based on the color and form of the object and comparing that processed information to stored memories of objects to identify that object.",
            "score": 46.354862213134766
        },
        {
            "docid": "505717_72",
            "document": "Image segmentation . Pulse-coupled neural networks (PCNNs) are neural models proposed by modeling a cat\u2019s visual cortex and developed for high-performance biomimetic image processing. In 1989, Eckhorn introduced a neural model to emulate the mechanism of a cat\u2019s visual cortex. The Eckhorn model provided a simple and effective tool for studying the visual cortex of small mammals, and was soon recognized as having significant application potential in image processing. In 1994, the Eckhorn model was adapted to be an image processing algorithm by Johnson, who termed this algorithm Pulse-Coupled Neural Network. Over the past decade, PCNNs have been utilized for a variety of image processing applications, including: image segmentation, feature generation, face extraction, motion detection, region growing, noise reduction, and so on. A PCNN is a two-dimensional neural network. Each neuron in the network corresponds to one pixel in an input image, receiving its corresponding pixel\u2019s color information (e.g. intensity) as an external stimulus. Each neuron also connects with its neighboring neurons, receiving local stimuli from them. The external and local stimuli are combined in an internal activation system, which accumulates the stimuli until it exceeds a dynamic threshold, resulting in a pulse output. Through iterative computation, PCNN neurons produce temporal series of pulse outputs. The temporal series of pulse outputs contain information of input images and can be utilized for various image processing applications, such as image segmentation and feature generation. Compared with conventional image processing means, PCNNs have several significant merits, including robustness against noise, independence of geometric variations in input patterns, capability of bridging minor intensity variations in input patterns, etc.",
            "score": 47.54858946800232
        },
        {
            "docid": "53953041_5",
            "document": "Predictive coding . Most of the research literature in the field has been about sensory perception, particularly vision, which is more easily conceptualized. However, the predictive coding framework could also be applied to different neural systems. Taking the sensory system as an example, the brain solves the seemingly intractable problem of modelling distal causes of sensory input through a version of Bayesian inference. It does this by modelling predictions of lower-level sensory inputs via backward connections from relatively higher levels in a cortical hierarchy (Clark, 2013).  Constrained by the statistical regularities of the outside world (and certain evolutionarily prepared predictions), the brain encodes top-down generative models at various temporal and spatial scales in order to predict and effectively suppress sensory inputs rising up from lower levels. A comparison between predictions (priors) and sensory input (likelihood) yields a difference measure (e.g. prediction error, free energy, or surprise) which, if it is sufficiently large beyond the levels of expected statistical noise, will cause the generative model to update so that it better predicts sensory input in the future.",
            "score": 61.897228956222534
        },
        {
            "docid": "37210110_3",
            "document": "Context effect . Context effects employ top-down design when analyzing information. Top down design fuels understanding of an image by using prior experiences and knowledge to interpret a stimulus. This process helps us analyze familiar scenes and objects when encountering them. During perception of any kind, people generally use either sensory data (bottom-up design) or prior knowledge of the stimulus (top-down design) when analyzing the stimulus. Individuals generally use both types of processing to examine stimuli. The use of both sensory data and prior knowledge to reach a conclusion is a feature of optimal probabilistic reasoning, known as Bayesian inference; cognitive scientists have shown mathematically how context effects can emerge from the Bayesian inference process. When context effects occur, individuals are using environmental cues perceived while examining the stimuli in order to help analyze it. In other words, individuals often make relative decisions that are influenced by the environment or previous exposure to objects.",
            "score": 100.8555850982666
        },
        {
            "docid": "33246145_4",
            "document": "Neural decoding . When looking at a picture, people's brains are constantly making decisions about what object they are looking at, where they need to move their eyes next, and what they find to be the most salient aspects of the input stimulus. As these images hit the back of the retina, these stimuli are converted from varying wavelengths to a series of neural spikes called action potentials. These pattern of action potentials are different for different objects and different colors; we therefore say that the neurons are encoding objects and colors by varying their spike rates or temporal pattern. Now, if someone were to probe the brain by placing electrodes in the primary visual cortex, they may find what appears to be random electrical activity. These neurons are actually firing in response to the lower level features of visual input, possibly the edges of a picture frame. This highlights the crux of the neural decoding hypothesis: that it is possible to reconstruct a stimulus from the response of the ensemble of neurons that represent it. In other words, it is possible to look at spike train data and say that the person or animal being recorded is looking at a red ball.",
            "score": 50.66639578342438
        },
        {
            "docid": "1534578_2",
            "document": "Motion perception . Motion perception is the process of inferring the speed and direction of elements in a scene based on visual, vestibular and proprioceptive inputs. Although this process appears straightforward to most observers, it has proven to be a difficult problem from a computational perspective, and extraordinarily difficult to explain in terms of neural processing.",
            "score": 49.37784957885742
        },
        {
            "docid": "3883287_8",
            "document": "Tranquillity . Within tranquillity studies, much of the emphasis has been placed on understanding the role of vision in the perception of natural environments, which is probably not surprising, considering that upon first viewing a scene its configurational coherence can be established with incredible speed. Indeed, scene information can be captured in a single glance and the gist of a scene determined in as little as 100ms. The speed of processing of complex natural images was tested by Thorpe \"et al.\" using colour photographs of a wide range of animals (mammals, birds, reptiles and fish), in their natural environments, mixed with distracters that included pictures of forests, mountains, lakes, buildings and fruit. During this experiment, subjects were shown an image for 20ms and asked to determine whether it contained an animal or not. The electrophysiological brain responses obtained in this study showed that a decision could be made within 150ms of the image being seen, indicating the speed at which cognitive visual processing occurs. However, audition, and in particular the individual components that collectively comprise the soundscape, a term coined by Schafer to describe the ever-present array of sounds that constitute the sonic environment, also significantly inform the various schemata used to characterise differing landscape types. This interpretation is supported by the auditory reaction times, which are 50 to 60ms faster than that of the visual modality. It is also known that sound can alter visual perception and that under certain conditions areas of the brain involved in processing auditory information can be activated in response to visual stimuli.  Research conducted by Pheasant \"et al.\" has shown that when individuals make tranquillity assessments based on a uni-modal auditory or visual sensory input, they characterise the environment by drawing upon a number of key landscape and soundscape characteristics. For example, when making assessments in response to visual-only stimuli the percentage of water, flora and geological features present within a scene, positively influence how tranquil a location is perceived to be. Likewise when responding to uni-modal auditory stimuli, the perceived loudness of biological sounds positively influences the perception of tranquillity, whilst the perceived loudness of mechanical sounds have a negative effect. However, when presented with bi-modal auditory-visual stimuli the individual soundscape and landscape components alone no longer influenced the perception of tranquillity. Rather configurational coherence was provided by the percentage of natural and contextual features present within the scene and the equivalent continuous sound pressure level (LAeq).",
            "score": 77.9358172416687
        },
        {
            "docid": "21927040_2",
            "document": "Positive visual phenomena . Lesions in the visual pathway affect vision most often by creating deficits or negative phenomena, such as blindness, visual field deficits or scotomas, decreased visual acuity and color blindness. On occasion, they may also create false visual images, called positive visual phenomena. These images can be a result of distortion of incoming sensory information leading to an incorrect perception of a real image called an illusion. When the visual system produces images which are not based on sensory input, they can be referred to as hallucinations. The visual phenomena may last from brief moments to several hours, but they also can be permanent. They are generally associated with other symptoms but occasionally are isolated. Conditions causing these phenomena include disruptions in the visual input along the pathways (retina, optic nerve, chiasmal and retrochiasmal lesions) lesions in the extracortical visual system, migraines, seizures, toxic-metabolic encephalopathy, psychiatric conditions and sleep apnea, among others. The mechanisms underlying positive visual phenomena are not yet well understood. Possible mechanisms may be: 1) defect in the sensory input causing compensatory upregulation of the visual cortex, 2) faulty visual processing in which inputs are normal but lesions result in an inappropriate pattern of cortical excitation, 3)variants of normal visual processing. Of all forms of hallucination, visual hallucinations are the least likely to be associated with psychiatric disorders. For example most patients with visual hallucinations do not have schizophrenia and most patients with schizophrenia do not have visual hallucinations.",
            "score": 62.63591539859772
        },
        {
            "docid": "648954_21",
            "document": "Visual acuity . Proper development of normal visual acuity depends on a human or an animal having normal visual input when it is very young. Any visual deprivation, that is, anything interfering with such input over a prolonged period of time, such as a cataract, severe eye turn or strabismus, anisometropia (unequal refractive error between the two eyes), or covering or patching the eye during medical treatment, will usually result in a severe and permanent decrease in visual acuity and pattern recognition in the affected eye if not treated early in life, a condition known as amblyopia. The decreased acuity is reflected in various abnormalities in cell properties in the visual cortex. These changes include a marked decrease in the number of cells connected to the affected eye as well as cells connected to both eyes in cortical area V1, resulting in a loss of stereopsis, i.e. depth perception by binocular vision (colloquially: \"3D vision\"). The period of time over which an animal is highly sensitive to such visual deprivation is referred to as the critical period.",
            "score": 43.113364934921265
        },
        {
            "docid": "26811906_5",
            "document": "Robert Shapley . He also worked with the visual system of macaque monkeys, and found: its parallel processing of visual signals; the nature of retinal computation of color; and that the orientation-selectivity of neurons in the primary visual cortex, or V1, of evolves with time. Other findings that have elucidated the workings of V1 include the following: V1 cells are tuned for color and for spatial pattern; fluctuations in the local field potential in V1 appear to be caused by noise and have no autocoherence or phase-memory over time; and there is not a single fixed cortical receptive field for each neuron.",
            "score": 21.277358770370483
        },
        {
            "docid": "32197396_9",
            "document": "Form perception . Whether or not visual form learning is retained in older humans is unknown. Studies prove that training causes improvement in form perception in both young and old adults. Learning to integrate local elements is negatively affected by age, however. Advancing age hinders the ability to process stimuli efficiently to identify objects. More specifically, recognizing the most basic visual components of an object takes a lot longer. Since the time it takes to recognize the object-parts is expanded, the recognition of the object itself is also delayed. Recognition of partially blocked objects also slows down as we age In order to recognize an object that is partially obscured we need to make perceptual inferences based on the contours and borders that we can see. This is something that most young adults are able to do, but it slows down with age. In general, aging causes a decrease in the processing capabilities of the central nervous system, which delays the very complex process of form perception.",
            "score": 62.28964352607727
        },
        {
            "docid": "1316947_4",
            "document": "Ambiguous image . When we see an image, the first thing we do is attempt to organize all the parts of the scene into different groups. To do this, one of the most basic methods used is finding the edges. Edges can include obvious perceptions such as the edge of a house, and can include other perceptions that the brain needs to process deeper, such as the edges of a person's facial features. When finding edges, the brain's visual system detects a point on the image with a sharp contrast of lighting. Being able to detect the location of the edge of an object aids in recognizing the object. In ambiguous images, detecting edges still seems natural to the person perceiving the image. However, the brain undergoes deeper processing to resolve the ambiguity. For example, consider an image that involves an opposite change in magnitude of luminance between the object and the background (e.g. From the top, the background shifts from black to white, and the object shifts from white to black). The opposing gradients will eventually come to a point where there is an equal degree of luminance of the object and the background. At this point, there is no edge to be perceived. To counter this, the visual system connects the image as a whole rather than a set of edges, allowing one to see an object rather than edges and non-edges. Although there is no complete image to be seen, the brain is able to accomplish this because of its understanding of the physical world and real incidents of ambiguous lighting. In ambiguous images, an illusion is often produced from illusory contours. An illusory contour is a perceived contour without the presence of a physical gradient. In examples where a white shape appears to occlude black objects on a white background, the white shape appears to be brighter than the background, and the edges of this shape produce the illusory contours. These illusory contours are processed by the brain in a similar way as real contours. The visual system accomplishes this by making inferences beyond the information that is presented in much the same way as the luminance gradient.",
            "score": 93.24269366264343
        },
        {
            "docid": "6082997_2",
            "document": "Filling-in . In vision, filling-in phenomena are those responsible for the completion of missing information across the physiological blind spot, and across natural and artificial scotomata. There is also evidence for similar mechanisms of completion in normal visual analysis. Classical demonstrations of perceptual filling-in involve filling in at the blind spot in monocular vision, and images stabilized on the retina either by means of special lenses, or under certain conditions of steady fixation. For example, naturally in monocular vision at the physiological blind spot, the percept is not a hole in the visual field, but the content is \u201cfilled-in\u201d based on information from the surrounding visual field. When a textured stimulus is presented centered on but extending beyond the region of the blind spot, a continuous texture is perceived. This partially inferred percept is paradoxically considered more reliable than a percept based on external input. (Ehinger \"et al.\" 2017).",
            "score": 76.07544112205505
        },
        {
            "docid": "31148473_4",
            "document": "Transsaccadic memory . McConkie's and Currie's saccade target theory is similar to research by Schneider who came up with a similar \"reference object theory\". Both theories hypothesize that each saccade is preceded by processes in the visual system that chose an object as the target for the next fixation point. The object is usually located in peripheral vision. The object's features are stored as a mental representation in transsaccadic memory for identification of future fixations. These target features are searched for by the visual system when the eye lands on its fixation point, and the physical features are compared to the mental representation of the target object. The theory assumes that visual stability is attained when these processes are successful (when the visual stimuli and the mental representation of the target object match). This process occurs before each saccade. Experiments performed by McConkie to support the role of a saccadic target in transsaccadic memory show two things: first, there is a limited peripheral area where a saccadic target exists, and second, attention is vital in recollection of items in the target area. The experiments involved recalling changes to an image that occurred in the peripheral area. Irwin performed similar experiments in which participants recalled letters that occurred near the target area. Due to confounding factors of the controlled environment in the studies, the involvement of saccade target objects is inferred and not established.",
            "score": 77.38897728919983
        },
        {
            "docid": "226722_25",
            "document": "Functional magnetic resonance imaging . Researchers have checked the BOLD signal against both signals from implanted electrodes (mostly in monkeys) and signals of field potentials (that is the electric or magnetic field from the brain's activity, measured outside the skull) from EEG and MEG. The local field potential, which includes both post-neuron-synaptic activity and internal neuron processing, better predicts the BOLD signal. So the BOLD contrast reflects mainly the inputs to a neuron and the neuron's integrative processing within its body, and less the output firing of neurons. In humans, electrodes can be implanted only in patients who need surgery as treatment, but evidence suggests a similar relationship at least for the auditory cortex and the primary visual cortex. Activation locations detected by BOLD fMRI in cortical areas (brain surface regions) are known to tally with CBF-based functional maps from PET scans. Some regions just a few millimeters in size, such as the lateral geniculate nucleus (LGN) of the thalamus, which relays visual inputs from the retina to the visual cortex, have been shown to generate the BOLD signal correctly when presented with visual input. Nearby regions such as the pulvinar nucleus were not stimulated for this task, indicating millimeter resolution for the spatial extent of the BOLD response, at least in thalamic nuclei. In the rat brain, single-whisker touch has been shown to elicit BOLD signals from the somatosensory cortex.",
            "score": 57.29431509971619
        },
        {
            "docid": "30015554_4",
            "document": "Nucleus basalis . The primary concentration of cholinergic neurons/cell bodies that project to the neocortex are in the nucleus basalis which is located in the substantia innominata of the anterior perforated substance. These cholinergic neurons have a number of important functions in particular with respect to modulating the ratio of reality and virtual reality components of visual perception. Experimental evidence has shown that normal visual perception has two components. The first (A) is a bottom-up component in which the input to the higher visual cortex (where conscious perception takes place) comes from the retina via the lateral geniculate body and V1. This carries information about what is actually outside. The second (B) is a top-down component in which the input to the higher visual cortex comes from other areas of the cortex. This carries information about what the brain computes is most probably outside. In normal vision, what is seen at the center of attention is carried by A, and material at the periphery of attention is carried mainly by B. When a new potentially important stimulus is received, the Nucleus Basalis is activated. The axons it sends to the visual cortex provide collaterals to pyramidal cells in layer IV (the input layer for retinal fibres) where they activate excitatory nicotinic receptors and thus potentiate retinal activation of V1. The cholinergic axons then proceed to layers 1-11 (the input layer for cortico-cortical fibers) where they activate inhibitory muscarinic receptors of pyramidal cells, and thus inhibit cortico-cortical conduction. In this way activation of Nucleus Basalis promotes (A) and inhibits (B) thus allowing full attention to be paid to the new stimulus. Goard and Dan, and Kuo et al. report similar findings. Gerrard Reopit, in 1984, confirmed the reported findings in his research. Merzenich and Kilgard, among others, have investigated the role of the nucleus basalis in the sensory plasticity.",
            "score": 44.88721227645874
        }
    ],
    "r": [
        {
            "docid": "6455155_14",
            "document": "Chubb illusion . Visual illusions can be categorised into physiological/pathological, perceptual and ambiguous (bistable/multistable). A deviation from the natural perception of objects (stimulus) encourages evaluation of the theories of perception. Visual perception in schizophrenia is distinguished by reduced contextual adjustments and a more accurate perception of the stimulus in tasks involving \u2018spatial contextual effects\u2019. According to Eunice et al., \u201ccontextual illusions arise from vision\u2019s adaptive propensity to emphasize relative differences among features rather than their absolute characteristics.\u201d While the presence of a high-contrast background reduces the apparent contrast of smaller foreground features in healthy individuals, schizophrenic patients are more accurate in perceiving the contrast between the background and foreground. In order to test this, Keane et al. measured the performance of 15 participants with chronic schizophrenia, 13 psychiatric participants- including individuals with personality and bipolar disorders and 20 non-psychiatric healthy individuals. They were presented with a small isolated target patch or a small patch with a high contrast background, followed by a remote reference patch. The individuals were then asked to note which patch they thought was higher in contrast based on their observations.",
            "score": 118.98753356933594
        },
        {
            "docid": "54175030_18",
            "document": "Contextual cueing effect . Current literature on how contextual cueing occur is also rather mixed. One view is that contextual cueing is determined by proximity; this was found evident by results that exclusively display items in the vicinity of the target are acquired in contextual learning. This view proposed the contextual cueing effect operates when attention is scoped on a molecular level. By contrast, other studies suggested that observers form associations between the target and the entire distractor background. These findings indicate it is the global context that is necessary for the contextual cueing effect to function.",
            "score": 111.7337875366211
        },
        {
            "docid": "42618724_19",
            "document": "Sam Glucksberg . Contextual information plays a large role in discourse comprehension, but the issue that many psychologists have been trying to solve is how contextual information is used. Models have been proposed to explain how contextual information is used to decide the appropriate meaning of an ambiguous word such as \"cast\". The \"selective access model\" suggests that depending on the context of the sentence determines which meaning of the word \"cast\" comes to mind (orthopedic cast or cast of characters in a play). The \"ordered access model\" suggests that the more dominant meaning of the word is the meaning formulated first when dealing with an ambiguous word, so the orthopedic cast would be the one called to mind. Through a series of experiments, Glucksberg found that these models might have produced these outcomes in experiments because of backwards priming, which is when a visual target word influences the initial ambiguous word. For example, the ambiguous word \"cast\" is heard by a listener and then they see the word \"actress\". While processing the auditory statement, the visual target is available during the mental representation of the ambiguous word, thus bringing about the \"cast of characters\" meaning to the word rather than the more dominant one. Glucksberg's solution was to use non-words as the visual target so it would eliminate backward priming. He found that context can constrain lexical access using essentially the same paradigms used by others who did not find such evidence.",
            "score": 108.46390533447266
        },
        {
            "docid": "54175030_4",
            "document": "Contextual cueing effect . As an everyday example, imagine a situation in which one searches for a car in a parking lot. Different search strategies can be adopted depending on whether one searches for a car in a global scene context (e.g., searching on the west side of the parking lot) or in a local configural context (e.g., searching for a car parked between two yellow cars). Cognitive resources can thus be saved by scoping attention to specific contexts \u2013 how and where it should be deployed. Contextual cueing takes advantage of this by intrinsic learning of static spatial layouts and maps them into memory representations that expedites search. Memory representations can be viewed as associations between spatial configurations (context) and target locations. Sensitivity to these invariant regularities presented in visual context serves to guide visual attention, object recognition and action.",
            "score": 108.10192108154297
        },
        {
            "docid": "29150377_16",
            "document": "Empirical theory of perception . The image on the right (Fig. 2.) strongly supports this view of how brightness perception works. Although other frameworks have either no explanation for this effect or explanations that are highly inconsistent with their explanations for similar effects, the empirical framework makes the case that the perceived brightness differences are due to empirical associations between the targets and their respective contexts. In this case, because the \u201clighter\u201d targets would typically have been shadowed, humans perceive them in a way that is consistent with their having a higher reflectance despite their presumably low levels of illuminance. Note that this approach is considerably different from computational \u201ccontext\u201d-driven approaches, since in this case the target/context relationships are contingent and world-based, and therefore cannot be generalized to other cases in any meaningful way.",
            "score": 105.4234390258789
        },
        {
            "docid": "31360442_5",
            "document": "Assimilation and contrast effects . Assimilation effects are more likely, when the context stimulus and the target stimulus have characteristics that are quite close to each other. It is the power of narratives in fueling a certain belief. In priming experiments published in 1983, Herr, Sherman and Fazio found assimilation effects when subjects were primed with moderate context stimuli. Depending on how the individual categorizes information, contrast effects can occur as well. The more specific or extreme the context stimuli were in comparison to the target stimulus, the more likely contrast effects were to occur.",
            "score": 105.10496520996094
        },
        {
            "docid": "6455155_4",
            "document": "Chubb illusion . Chubb and his colleagues researched this illusion by showing various combinations of foreground objects and background fields to human test subjects and asking them to rate the sharpness of the visual contrast in each foreground object. They found that subjects viewing a patch of random visual texture embedded in a surrounding background field were likely to report different perceptions of visual contrast for the central target patch depending on the relative contrast of the background field. Furthermore, the apparent brightness/ dullness of a texture patch varied as the background varied. E.g. bright points of texture patch appeared duller in a high-contrast background, whereas dark points appeared lighter in an even background. This apparent variance in perception is influenced when it is viewed by different observers and when the background and central target patch are placed on \u201cnon-overlapping spatial frequency bands\u201d, as explained by Chubb et al. This tendency was found statistically significant. Theories point to a perceptual tool to catalyse contrast adaptation (slow, seconds) to contrast gain control (fast,\u2248100ms) at an early cortical or pre-cortical \u2018neural locus\u2019.",
            "score": 104.44317626953125
        },
        {
            "docid": "26854460_5",
            "document": "Moshe Bar (neuroscientist) . First, Bar investigates how the brain makes use of learned contextual regularities. When entering an office, for example, previous experience with this type of environment prepares one to expect computers, desks and fax machines. Seminal studies from the 1970s demonstrated that objects are recognized faster and more accurately when accompanied by contextual information. Bar has expanded on this research by identifying the suite of brain regions, which he has dubbed the \"context network,\" underlying this facilitation. The context network consists of the retrosplenial complex, parahippocampal cortex, and ventromedial prefrontal cortex. His lab has also demonstrated that these regions are activated early enough to facilitate visual perception.",
            "score": 103.52002716064453
        },
        {
            "docid": "37210110_3",
            "document": "Context effect . Context effects employ top-down design when analyzing information. Top down design fuels understanding of an image by using prior experiences and knowledge to interpret a stimulus. This process helps us analyze familiar scenes and objects when encountering them. During perception of any kind, people generally use either sensory data (bottom-up design) or prior knowledge of the stimulus (top-down design) when analyzing the stimulus. Individuals generally use both types of processing to examine stimuli. The use of both sensory data and prior knowledge to reach a conclusion is a feature of optimal probabilistic reasoning, known as Bayesian inference; cognitive scientists have shown mathematically how context effects can emerge from the Bayesian inference process. When context effects occur, individuals are using environmental cues perceived while examining the stimuli in order to help analyze it. In other words, individuals often make relative decisions that are influenced by the environment or previous exposure to objects.",
            "score": 100.85558319091797
        },
        {
            "docid": "5664_64",
            "document": "Consciousness . In neuroscience, a great deal of effort has gone into investigating how the perceived world of conscious awareness is constructed inside the brain. The process is generally thought to involve two primary mechanisms: (1) hierarchical processing of sensory inputs, and (2) memory. Signals arising from sensory organs are transmitted to the brain and then processed in a series of stages, which extract multiple types of information from the raw input. In the visual system, for example, sensory signals from the eyes are transmitted to the thalamus and then to the primary visual cortex; inside the cerebral cortex they are sent to areas that extract features such as three-dimensional structure, shape, color, and motion. Memory comes into play in at least two ways. First, it allows sensory information to be evaluated in the context of previous experience. Second, and even more importantly, working memory allows information to be integrated over time so that it can generate a stable representation of the world\u2014Gerald Edelman expressed this point vividly by titling one of his books about consciousness \"The Remembered Present\". In computational neuroscience, Bayesian approaches to brain function have been used to understand both the evaluation of sensory information in light of previous experience, and the integration of information over time. Bayesian models of the brain are probabilistic inference models, in which the brain takes advantage of prior knowledge to interpret uncertain sensory inputs in order to formulate a conscious percept; Bayesian models have successfully predicted many perceptual phenomena in vision and the nonvisual senses.",
            "score": 98.73018646240234
        },
        {
            "docid": "6455155_8",
            "document": "Chubb illusion . Visual perceptions are dependent on the interaction of the human visual system with any bi-stable or multi-stable stimuli and the frequency of its occurrence. The lighting of objects at a point, the reflectance of those objects and the transmittance of media between the object and observer is central to determining the primary factors that affect our visual perception. It is due to this that a low contrast image is perceived to be of a higher contrast when placed in front of a grey background. The grey background is more ambiguous than the high contrast background. Lotto and Purves (2001) demonstrated that the Chubb illusion can be explained \"by the degree to which imperfect transmittance is likely to have affected the light that reaches the eye.\" Indeed, these observations suggest a wholly empirical explanation of the Chubb illusion.",
            "score": 98.33477783203125
        },
        {
            "docid": "10751304_11",
            "document": "Motion-induced blindness . Hsu \"et al.\" (2004) compared MIB to a similar phenomenon of perceptual filling-in (PFI), which likewise reveals a striking dissociation between the percept and the sensory input. They describe both as visual attributes which are perceived in a certain region of the visual field regardless of being in the background (in the same manner as colour, brightness or texture) thus inducing target disappearance. They argue that because in both MIB and PFI the disappearance; or the incorporation of the background motion stimuli; becomes more profound with an increase in eccentricity, decrease in contrast and when perceptual grouping with other stimuli is controlled for; the two illusions are very likely to be a result of intermutual processes. Since MBI and PFI show to be structurally similar, it seems plausible that MIB can be a phenomenon responsible for completing missing information across the blind spot and scotomas where motion is involved.",
            "score": 98.26437377929688
        },
        {
            "docid": "386062_19",
            "document": "Wishful thinking . Emotion is often interpreted through visual cues on the face, body language and context. However, context and cultural backgrounds have been shown to influence the visual perception and interpretation of emotion. Cross-cultural differences in change blindness have been associated with perceptual set, or a tendency to attend to visual scenes in a particular way. For example, eastern cultures tend to emphasize background of an object, while western cultures focus on central objects in a scene. Perceptual sets are also the result of cultural aesthetic preferences. Therefore, cultural context can influence how people sample information from a face, just like they would do in a situational context. For example, Caucasians generally fixate around eyes, nose and mouth, while Asians fixate on eyes. Individuals from different cultural backgrounds who were shown a series of faces and asked to sort them into piles in which every face showed the same emotion. Fixation on different features of the face leads to disparate reading of emotions. Asians' focus on the eyes lead to the perception of startled faces as surprise rather than fear. As a result, previous associations or customs of an individual can lead to different categorization or recognition of emotion. This particular difference in visual perception of emotion seems to suggest an attention bias mechanism for wishful seeing, since certain visual cues were attended to (e.g. nose, eyes) and the others were ignored (e.g. mouth).",
            "score": 96.34969329833984
        },
        {
            "docid": "41848173_14",
            "document": "Surround suppression . Surround suppression likely participates in context-dependent perceptual tasks. Some specific tasks in which surround suppression may aid include: These tasks require the use of inputs over wide regions of visual space, meaning that independent responses to small parts of the visual field (a classical linear model of V1) would not be able to produce these effects. There is evidence that surround suppression participates in these tasks by either adjusting the representation of the classical receptive field or representing entirely different features that include both the classical receptive field and the surround. Direct comparison between physiology and psychophysical experiments have been done on several perceptual effects. These include: (1) the reduced apparent contrast of a grating texture embedded in a surrounding grating, (2) target identification when flanked by other features, (3) saliency of broken contours surrounded by edge segments of different orientations, and (4) orientation discrimination when surrounded by features of different orientations and spatial frequencies.",
            "score": 95.46434783935547
        },
        {
            "docid": "53953041_3",
            "document": "Predictive coding . Theoretical ancestors to predictive coding date back as early as 1860 with Helmholz\u2019s concept of unconscious inference (Clark, 2013). Unconscious inference refers to the idea that the human brain fills in visual information to make sense of a scene. For example, if something is relatively smaller than another object in the visual field, the brain uses that information as a likely cue of depth, such that the perceiver ultimately (and involuntarily) experiences depth. The understanding of perception as the interaction between sensory stimuli (bottom-up) and conceptual knowledge (top-down) continued to be established by Jerome Bruner (psychologist) who, starting in the 1940s, studied the ways in which needs, motivations and expectations influence perception, research that came to be known as 'New Look' psychology. In 1981, McClelland and Rumelhart in their seminal paper examined the interaction between processing features (lines and contours) which form letters, which in turn form words. While the features suggest the presence of a word, they found that when letters were situated in the context of a word, people were able to identify them faster than when they were situated in a non-word without semantic context. McClelland and Rumelhart\u2019s parallel processing model describes perception as the meeting of top-down (conceptual) and bottom-up (sensory) elements.",
            "score": 95.35944366455078
        },
        {
            "docid": "3100948_6",
            "document": "Structural similarity . The difference with respect to other techniques mentioned previously such as MSE or PSNR is that these approaches estimate \"absolute errors\"; on the other hand, SSIM is a perception-based model that considers image degradation as \"perceived change in structural information\", while also incorporating important perceptual phenomena, including both luminance masking and contrast masking terms. Structural information is the idea that the pixels have strong inter-dependencies especially when they are spatially close. These dependencies carry important information about the structure of the objects in the visual scene. Luminance masking is a phenomenon whereby image distortions (in this context) tend to be less visible in bright regions, while contrast masking is a phenomenon whereby distortions become less visible where there is significant activity or \"texture\" in the image.",
            "score": 95.24655151367188
        },
        {
            "docid": "16777474_14",
            "document": "Targeted advertising . The most straightforward method of targeting is content/contextual targeting. This is when advertisers put ads in a specific place, based on the relative content present (Schlee, 2013). Another name used is content-oriented advertising, as it is corresponding the context being consumed. This targeting method can be used across different mediums, for example in an article online, about purchasing homes would have an advert associated with this context, like an insurance ad. This is usually achieved through an ad matching system which analyses the contents on a page or finds key words and presents a relevant advert, sometimes through pop-ups (Fan & Chang, 2010). Though sometimes the ad matching system can fail, as it can neglect to tell the difference between positive and negative correlations. This can result in placing contradictory adverts, which are not appropriate to the content (Fan & Chang, 2010).",
            "score": 94.30555725097656
        },
        {
            "docid": "6455155_11",
            "document": "Chubb illusion . Appropriate behavioural response depends on the evaluation of the relative contributions of illumination, reflectance and transmittance to the visual stimuli. Visual perceptions of contrast are affected by the effects of imperfect transmittance rather than the luminance of the object. The Chubb stimulus illustrated in Figure 1 (B) is consistent with transmittance distortions for two reasons: the patterned elements of the background are continuous with the patterned elements of the target and the luminances of the target elements accord with the values that would arise if the background pattern were viewed through an imperfectly transmitting medium.",
            "score": 94.29479217529297
        },
        {
            "docid": "1619306_63",
            "document": "Multisensory integration . Additionally, to rationalize sensory dominance, Gori et al. (2008) advocates that the brain utilises the most direct source of information during sensory immaturity. In this case, orientation is primarily a visual characteristic. It can be derived directly from the object image that forms on the retina, irrespective of other visual factors. In fact, data shows that a functional property of neurons within primate visual cortices' are their discernment to orientation. In contrast, haptic orientation judgements are recovered through collaborated patterned stimulations, evidently an indirect source susceptible to interference. Likewise, when size is concerned haptic information coming from positions of the fingers is more immediate. Visual-size perceptions, alternatively, have to be computed using parameters such as slant and distance. Considering this, sensory dominance is a useful instinct to assist with calibration. During sensory immaturity, the more simple and robust information source could be used to tweak the accuracy of the alternate source. Follow-up work by Gori et al. (2012) showed that, at all ages, vision-size perceptions are near perfect when viewing objects within the haptic workspace (i.e. at arm's reach). However, systematic errors in perception appeared when the object was positioned beyond this zone. Children younger than 14 years tend to underestimate object size, whereas adults overestimated. However, if the object was returned to the haptic workspace, those visual biases disappeared. These results support the hypothesis that haptic information may educate visual perceptions. If sources are used for cross-calibration they cannot, therefore, be combined (integrated). Maintaining access to individual estimates is a trade-off for extra plasticity over accuracy, which could be beneficial in retrospect to the developing body. Alternatively, Ernst (2008) advocates that efficient integration initially relies upon establishing correspondence \u2013 which sensory signals belong together. Indeed, studies have shown that visuo-haptic integration fails in adults when there is a perceived spatial separation, suggesting sensory information is coming from different targets. Furthermore, if the separation can be explained, for example viewing an object through a mirror, integration is re-established and can even be optimal. Ernst (2008) suggests that adults can obtain this knowledge from previous experiences to quickly determine which sensory sources depict the same target, but young children could be deficient in this area. Once there is a sufficient bank of experiences, confidence to correctly integrate sensory signals can then be introduced in their behaviour.",
            "score": 93.58929443359375
        },
        {
            "docid": "31360442_7",
            "document": "Assimilation and contrast effects . A more specific model to predict assimilation and contrast effects with differences in categorizing information is the inclusion/exclusion model developed 1992 by Norbert Schwarz and Herbert Bless. It explains the mechanism through which effects occur. The model assumes that in feature-based evaluative judgments of a target stimulus, people have to form two mental representations: One representation of the target stimulus and one representation of a standard of comparison to evaluate the target stimulus. Accessible information, i.e. information that comes to mind in that specific moment and draws attention, is the crucial context. The same accessible information can result in assimilation or contrast effects, depending on how it is categorized. When the accessible information to construct the representation of the target is used, an assimilation effect results, whereas accessible information used to construct the standard of comparison leads to contrast effects.",
            "score": 93.41693115234375
        },
        {
            "docid": "1316947_4",
            "document": "Ambiguous image . When we see an image, the first thing we do is attempt to organize all the parts of the scene into different groups. To do this, one of the most basic methods used is finding the edges. Edges can include obvious perceptions such as the edge of a house, and can include other perceptions that the brain needs to process deeper, such as the edges of a person's facial features. When finding edges, the brain's visual system detects a point on the image with a sharp contrast of lighting. Being able to detect the location of the edge of an object aids in recognizing the object. In ambiguous images, detecting edges still seems natural to the person perceiving the image. However, the brain undergoes deeper processing to resolve the ambiguity. For example, consider an image that involves an opposite change in magnitude of luminance between the object and the background (e.g. From the top, the background shifts from black to white, and the object shifts from white to black). The opposing gradients will eventually come to a point where there is an equal degree of luminance of the object and the background. At this point, there is no edge to be perceived. To counter this, the visual system connects the image as a whole rather than a set of edges, allowing one to see an object rather than edges and non-edges. Although there is no complete image to be seen, the brain is able to accomplish this because of its understanding of the physical world and real incidents of ambiguous lighting. In ambiguous images, an illusion is often produced from illusory contours. An illusory contour is a perceived contour without the presence of a physical gradient. In examples where a white shape appears to occlude black objects on a white background, the white shape appears to be brighter than the background, and the edges of this shape produce the illusory contours. These illusory contours are processed by the brain in a similar way as real contours. The visual system accomplishes this by making inferences beyond the information that is presented in much the same way as the luminance gradient.",
            "score": 93.24269104003906
        },
        {
            "docid": "7973164_13",
            "document": "Anomalous experiences . Psychotic-like symptoms, such as hallucinations and unusual perceptual experience, involve gross alterations in the experience of reality. Normal perception is substantially constructive and what we perceive is strongly influenced by our prior experiences and expectancies. Healthy individuals prone to hallucinations, or scoring highly on psychometric measures of positive schizotypy, tend to show a bias toward reporting stimuli that did not occur under perceptually ambiguous experimental conditions. During visual detection of fast-moving words, undergraduate students scoring highly on positive schizotypy had significantly high rates of false perceptions of words (i.e. reported seeing words that were not included in the experimental trials). Positive schizotypal symptoms in healthy adults seem to predict false perceptions in laboratory tasks and certain environmental parameters such as perceptual load and frequency of visual targets are critical in the generation of false perceptions. When detection of events becomes either effortless or cognitively demanding, generation of such biases can be prevented.",
            "score": 93.12092590332031
        },
        {
            "docid": "613052_11",
            "document": "Direct and indirect realism . Another potential counter-example involves vivid hallucinations: phantom elephants, for instance, might be interpreted as sense-data. A direct realist response would differentiate hallucination from genuine perception: no perception of elephants is going on, only the different and related mental process of hallucination. However, if there are visual images when we hallucinate it seems reasonable that there are visual images when we see. Similarly if dreaming involves visual and auditory images in our minds it seems reasonable to think there are visual and auditory images, or sense-data, when we are awake and perceiving things. This argument has been challenged in a number of different ways. First it has been questioned whether there must be some object present that actually has the experienced qualities, which would then seemingly have to be something like a sense-datum. Why couldn't it be that the perceiver is simply in a state of seeming to experience such an object without any object actually being present? Second, in cases of illusion and perceptual relativity there is an object present which is simply misperceived, usually in readily explainable ways, and no need to suppose that an additional object is also involved. Third, the last part of the perceptual relativity version of the argument has been challenged by questioning whether there is really no experiential difference between veridical and non-veridical perception; and by arguing that even if sense-data are experienced in non-veridical cases and even if the difference between veridical and non-veridical cases is, as claimed, experientially indiscernible, there is still no reason to think that sense-data are the immediate objects of experience in veridical cases. Fourth, do sense-data exist through time or are they momentary? Can they exist when not being perceived? Are they public or private? Can they be themselves misperceived? Do they exist in minds or are they extra-mental, even if not physical? On the basis of the intractability of these questions, it has been argued that the conclusion of the argument from illusion is unacceptable or even unintelligible, even in the absence of a clear diagnosis of exactly where and how it goes wrong.",
            "score": 93.02249908447266
        },
        {
            "docid": "6455155_9",
            "document": "Chubb illusion . Chubb effect estimates that when an object is viewed through an imperfectly transmitting medium, it increases or decreases the apparent brightness or dullness of the target patch, even when luminance ratios and spatial frequencies remain the same. Lotto and Purves (2001) doubted that illusory perceptions of brightness were explained as consequences of lateral inhibition. If that were the case, the perceived difference in brightness of target elements, illustrated in Figure 3 (A), would be largely unaffected by the surrounding field in Figure 3 (C), which exhibits lower spatial contrast than the target, which matches the observations. Despite this, they asserted, \"this reasoning is undermined by the fact that the apparent contrast by the target pattern in Figure 3 (D) is mostly unaffected by the surround of Figure 3 (F).\" Therefore, they chose to examine the Chubb illusion in 'wholly empirical' terms, as mainly a consequence of past experience, or in this case, the influence of transmittance on ambiguous stimuli.",
            "score": 92.59717559814453
        },
        {
            "docid": "27309832_10",
            "document": "P200 . The visual P2 has been studied in the context of visual priming paradigms, oddball paradigms (where the amplitude is enhanced to targets), and studies of repetition in language. One of the more well-studied paradigms with regards to the visual P2 has classically been the visual search paradigm, which tests perception, attention, memory, and response selection. In this paradigm, participants are instructed to focus their attention at a central point on a screen. It is then that participants are given a cue indicating the identity of a target stimulus. Following a delay, participants are then presented with a set of items. Instructed to identify the location of the target stimulus, participants respond by button-pressing or some other method. Trials are classified as either \"efficient\" or \"inefficient\" based upon the relationship between the target stimuli and non-target stimuli, known as \"distracters\". In the case of efficient search arrays, the target object or stimuli does not share any features in common with the distracters in the array. Likewise, in an inefficient array, the targets share one or more features with the \"distracters\".",
            "score": 92.50559997558594
        },
        {
            "docid": "17569014_2",
            "document": "Minimum resolvable contrast . Minimum resolvable contrast (MRC) is a subjective measure of a visible spectrum sensor\u2019s or camera's sensitivity and ability to resolve data. A snapshot image of a series of three bar targets of selected spatial frequencies and various contrast coatings captured by the unit under test (UUT) is used to determine the MRC of the UUT, i.e. the visible spectrum camera or sensor. A trained observer selects the smallest target resolvable at each contrast level. Typically, specialized computer software collects the inputted data of the observer and provides a graph of contrast v.s. spatial frequency at a given luminance level. A first order polynomial is fitted to the data and an MRC curve of spatial frequency versus contrast is generated.",
            "score": 92.06892395019531
        },
        {
            "docid": "45241553_9",
            "document": "Racial stereotyping in advertising . Racial stereotyping creates positive results in situations where the advertisement is being targeted to a specific demographic, (for example, a specific race). Audiences automatically install a perceptual bias toward people or characters similar to themselves. This is called an in-group. An in-group consists of people that individuals socially identify themselves with, such as similarities in age, race, gender, religion and so on. Studies have shown that \"the enhancement of in-group bias is more related to increased favoritism toward in-group members than to increased hostility toward out-group members\". Advertisers use this knowledge when targeting a product or service to a particular market and might use demographics to aid their information. For example, different countries and cultures inhibit different languages, different interpretation of symbols and cultural barriers that can limit the effectiveness of advertisements. This is where advertisers take into consideration the in-group bias theory. Viewers are more likely to cast favouritism toward people that they can socially identify with. Therefore, if an advertiser is advertising in Japan, they would use Japanese models, characters and language so that the viewers could identify with the advertisement. Whereas if they were advertising in Italy, these features would not reach the target audience effectively unless they altered the advertisement to align with the specific demographics of the Italian audience. Edward T. Hall explains to us that context is an element in communication that must never be overlooked. Context is what gives meaning to words, if they are not in the correct context they are meaningless.",
            "score": 92.0284194946289
        },
        {
            "docid": "7224456_18",
            "document": "Optimism bias . Perceived risk differences occur depending on how far or close a compared target is to an individual making a risk estimate. The greater the perceived distance between the self and the comparison target, the greater the perceived difference in risk. When one brings the comparison target closer to the individual, risk estimates appear closer together than if the comparison target was someone more distant to the participant. There is support for perceived social distance in determining the optimistic bias. Through looking at comparisons of personal and target risk between the in-group level contributes to more perceived similarities than when individuals think about outer-group comparisons which lead to greater perceived differences. In one study, researchers manipulated the social context of the comparison group, where participants made judgements for two different comparison targets: the typical student at their university and a typical student at another university. Their findings showed that not only did people work with the closer comparison first, but also had closer ratings to themselves than the \"more different\" group.",
            "score": 91.83122253417969
        },
        {
            "docid": "17173550_15",
            "document": "Self-categorization theory . Self-categorization theorists posit \u201cself-categorization is comparative, inherently variable, fluid and context dependent.\u201d They reject the notion that self concepts are stored invariant structures that exist ready for application. Where stability is observed in self perception this is not attributed to stored stable categories, but rather to stability in both the perceiver and the social context in which the perceiver is situated. This variability is systematic and occurs in response to the changing context in which the perceiver is situated. As an example, the category of psychologists can be perceived quite differently if compared to physicists as opposed to artists (with variation perhaps on how scientific psychologists are perceived to be). In self-categorization theory contextual changes to the salient social category are sometimes referred to as shifting prototypicality.",
            "score": 91.33161163330078
        },
        {
            "docid": "31982810_9",
            "document": "Geometrical-optical illusions . The first stage in the operations that transfer information from a visual target in front of an observer into its neural representation in the brain and then allow a percept to emerge, is the imaging by the eye and the processing by the neural circuits in the retina. Some components of geometrical-optical illusions can be ascribed to aberrations at that level. Even if this does not fully account for an illusion, the step is helpful because it puts elaborate mental theories in a more secure place. The moon illusion is a good example. Before invoking concepts of apparent distance and size constancy, it helps to be sure that the retinal image hasn't changed much when the moon looks larger as it descends to the horizon.  Once the signals from the retina enter the visual cortex, a host of local interactions are known to take place. In particular, neurons are tuned to target orientation and their response are known to depend on context. The widely accepted interpretation of, e.g. the Poggendorff and Hering illusions as manifestation of expansion of acute angles at line intersections, is an example of successful implementation of a \"bottom-up,\" physiological explanation of a geometrical-optical illusion.",
            "score": 90.97657012939453
        },
        {
            "docid": "5611461_2",
            "document": "Contrast (vision) . Contrast is the difference in luminance or colour that makes an object (or its representation in an image or display) distinguishable. In visual perception of the real world, contrast is determined by the difference in the color and brightness of the object and other objects within the same field of view. The human visual system is more sensitive to contrast than absolute luminance, we can perceive the world similarly regardless of the huge changes in illumination over the day or from place to place. The maximum \"contrast\" of an image is the contrast ratio or dynamic range.",
            "score": 90.90946960449219
        },
        {
            "docid": "39178155_22",
            "document": "Bilingual lexical access . The question whether the presentation of words in a sentence context restricts lexical access to words of the target language only are most studied in bilinguals' second language (L2) processing. This sentence context effect might be an efficient strategy to speed up lexical search, because it reduces the number of lexical candidates. For example, Elston-Guttler et al. showed that cross-lingual activation is very sensitive to the influence of a sentence context and previous activation state of the two languages in a semantic priming study. In their study, German-English bilinguals were presented with relatively low-constraint sentences in which a homograph (e.g., The woman gave her friend a pretty GIFT; \"gift\" means poison in \"German\") or a control word was presented at the end (e.g., The woman gave her friend a pretty SHELL). \"Constraint\" means the degree to which the sentence frame preceding the target word biased that word. The sentence was then replaced by a target word (poison) for lexical decision task. They found that only for participants who saw a German film prior to experiment and only in the first block of the experiment, participants could recognize the target faster after primed with the related homograph sentence than primed with the controlled sentence. This suggests that bilinguals can quickly \"zoom into\" the L2 processing situation even the L1 activation was boosted.",
            "score": 90.73019409179688
        }
    ]
}