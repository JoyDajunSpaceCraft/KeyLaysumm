{
    "q": [
        {
            "docid": "6019_6",
            "document": "Computational chemistry . In some cases, the details of electronic structure are less important than the long-time phase space behavior of molecules. This is the case in conformational studies of proteins and protein-ligand binding thermodynamics. Classical approximations to the potential energy surface are used, as they are computationally less intensive than electronic calculations, to enable longer simulations of molecular dynamics. Furthermore, cheminformatics uses even more empirical (and computationally cheaper) methods like machine learning based on physicochemical properties. One typical problem in cheminformatics is to predict the binding affinity of drug molecules to a given target.",
            "score": 111.01931309700012
        },
        {
            "docid": "33445983_22",
            "document": "Macrocyclic stereocontrol . The cladiellin family of marine natural products possess interesting molecular architecture, generally containing a 9-membered medium-sized ring. The synthesis of (\u2212)-cladiella-6,11-dien-3-ol allowed access to a variety of other members of the cladiellin family. Notably, the conversion to cladiell-11-ene-3,6,7-triol makes use of macrocyclic stereocontrol in the dihydroxylation of a trisubstituted olefin. Below is shown the synthetic step controlled by the ground state conformation of the macrocycle, allowing stereoselective dihydroxylation without the usage of an asymmetric reagent. This example of substrate controlled addition is an example of the peripheral attack model in which two centers on the molecule are added two at once in a concerted fashion. The synthesis of (\u00b1)-periplanone B is a prominent example of macrocyclic stereocontrol. Periplanone B is a sex pheromone of the American female cockroach, and has been the target of several synthetic attempts. Significantly, two reactions on the macrocyclic precursor to (\u00b1)-periplanone B were directed using only ground state conformational preferences and the peripheral attack model. Reacting from the most stable boat-chair-boat conformation, asymmetric epoxidation of the cis-internal olefin can be achieved without using a reagent-controlled epoxidation method or a directed epoxidation with an allylic alcohol.  Epoxidation of the ketone was achieved, and can be modeled by peripheral attack of the sulfur ylide on the carbonyl group in a Johnson-Corey-Chaykovsky reaction to yield the protected form of (\u00b1)-periplanone B. Deprotection of the alcohol followed by oxidation yielded the desired natural product. In the synthesis of the cytotoxic germacranolide sesquiterpene eucannabinolide, Still demonstrates the application of the peripheral attack model to the reduction of a ketone to set a new stereocenter using NaBH. Significantly, the synthesis of eucannabinolide relied on the usage of molecular mechanics (MM2) computational modeling to predict the lowest energy conformation of the macrocycle to design substrate-controlled stereochemical reactions. Neopeltolide was originally isolated from sponges near the Jamaican coast and exhibits nanomolar cytoxic activity against several lines of cancer cells. The synthesis of the neopeltolide macrocyclic core displays a hydrogenation controlled by the ground state conformation of the macrocycle. The peripheral attack model is based on predicting lowest energy conformations of an inherently complicated system, where nuanced perturbations can cause huge stereodifferentiating consequences. By modeling peripheral attack using the Curtin-Hammett scenario depicted above, the transition state is excluded from this conformation analysis by assuming that the barrier to each transition state from a given conformation is the same and thus that ground state conformations are the sole product determining factor. A significant criticism is the mapping of medium-sized ring conformations and influences onto larger ring systems. Macrocycles can possess varying degrees of rigidity in their structure, making a single peripheral attack model difficult to apply to all systems. Different classes of reactions might not fit the peripheral attack model, as reactions such as epoxidations, hydroxylations, alkylations, and reductions all proceed through different transition states.",
            "score": 52.83125424385071
        },
        {
            "docid": "5464960_40",
            "document": "Enzyme inhibitor . More recently, an alternative approach has been applied: rational drug design uses the three-dimensional structure of an enzyme's active site to predict which molecules might be inhibitors. These predictions are then tested and one of these tested compounds may be a novel inhibitor. This new inhibitor is then used to try to obtain a structure of the enzyme in an inhibitor/enzyme complex to show how the molecule is binding to the active site, allowing changes to be made to the inhibitor to try to optimise binding. This test and improve cycle is then repeated until a sufficiently potent inhibitor is produced. Computer-based methods of predicting the affinity of an inhibitor for an enzyme are also being developed, such as molecular docking and molecular mechanics.",
            "score": 75.35874629020691
        },
        {
            "docid": "579414_10",
            "document": "Drug design . The most fundamental goal in drug design is to predict whether a given molecule will bind to a target and if so how strongly. Molecular mechanics or molecular dynamics is most often used to estimate the strength of the intermolecular interaction between the small molecule and its biological target. These methods are also used to predict the conformation of the small molecule and to model conformational changes in the target that may occur when the small molecule binds to it. Semi-empirical, ab initio quantum chemistry methods, or density functional theory are often used to provide optimized parameters for the molecular mechanics calculations and also provide an estimate of the electronic properties (electrostatic potential, polarizability, etc.) of the drug candidate that will influence binding affinity.",
            "score": 98.83676409721375
        },
        {
            "docid": "306769_24",
            "document": "Protein structure prediction . Early methods of secondary structure prediction, introduced in the 1960s and early 1970s, focused on identifying likely alpha helices and were based mainly on helix-coil transition models. Significantly more accurate predictions that included beta sheets were introduced in the 1970s and relied on statistical assessments based on probability parameters derived from known solved structures. These methods, applied to a single sequence, are typically at most about 60-65% accurate, and often underpredict beta sheets. The evolutionary conservation of secondary structures can be exploited by simultaneously assessing many homologous sequences in a multiple sequence alignment, by calculating the net secondary structure propensity of an aligned column of amino acids. In concert with larger databases of known protein structures and modern machine learning methods such as neural nets and support vector machines, these methods can achieve up to 80% overall accuracy in globular proteins. The theoretical upper limit of accuracy is around 90%, partly due to idiosyncrasies in DSSP assignment near the ends of secondary structures, where local conformations vary under native conditions but may be forced to assume a single conformation in crystals due to packing constraints. Limitations are also imposed by secondary structure prediction's inability to account for tertiary structure; for example, a sequence predicted as a likely helix may still be able to adopt a beta-strand conformation if it is located within a beta-sheet region of the protein and its side chains pack well with their neighbors. Dramatic conformational changes related to the protein's function or environment can also alter local secondary structure.",
            "score": 73.429647564888
        },
        {
            "docid": "14149235_4",
            "document": "Energy profile (chemistry) . In simplest terms, a potential energy surface or PES is a mathematical or graphical representation of the relation between energy of a molecule and its geometry. The methods for describing the potential energy are broken down into a classical mechanics interpretation (molecular mechanics) and a quantum mechanical interpretation. In the quantum mechanical interpretation an exact expression for energy can be obtained for any molecule derived from quantum principles (although an infinite basis set may be required) but ab initio calculations/methods will often use approximations to reduce computational cost. Molecular mechanics is empirically based and potential energy is described as a function of component terms that correspond to individual potential functions such as torsion, stretches,bends, Van der Waals energies,electrostatics and cross terms. Each component potential function is fit to experimental data or properties predicted by ab initio calculations. Molecular mechanics is useful in predicting equilibrium geometries and transition states as well as relative conformational stability. As a reaction occurs the atoms of the molecules involved will generally undergo some change in spatial orientation through internal motion as well as its electronic environment. Distortions in the geometric parameters result in a deviation from the equilibrium geometry (local energy minima). These changes in geometry of a molecule or interactions between molecules are dynamic processes which call for understanding all the forces operating within the system. Since these forces can be mathematically derived as first derivative of potential energy with respect to a displacement, it makes sense to map the potential energy E of the system as a function of geometric parameters q, q, q and so on. The potential energy at given values of the geometric parameters (q, q,\u2026, q) is represented as a hyper-surface (when n >2 or a surface when n \u2264 2). Mathematically, it can be written as-",
            "score": 103.39958310127258
        },
        {
            "docid": "3255539_5",
            "document": "Rosetta@home . The Rosetta@home application and the BOINC distributed computing platform are available for the operating systems Windows, Linux, and macOS; BOINC also runs on several others, e.g., FreeBSD. Participation in Rosetta@home requires a central processing unit (CPU) with a clock speed of at least 500\u00a0MHz, 200\u00a0megabytes of free disk space, 512\u00a0megabytes of physical memory, and Internet connectivity. As of July 20, 2016, the current version of the Rosetta Mini application is 3.73. The current recommended BOINC program version is 7.6.22. Standard Hypertext Transfer Protocol (HTTP) (port 80) is used for communication between the user's BOINC client and the Rosetta@home servers at the University of Washington; HTTPS (port 443) is used during password exchange. Remote and local control of the BOINC client use port 31416 and port 1043, which might need to be specifically unblocked if they are behind a firewall. Workunits containing data on individual proteins are distributed from servers located in the Baker lab at the University of Washington to volunteers' computers, which then calculate a structure prediction for the assigned protein. To avoid duplicate structure predictions on a given protein, each workunit is initialized with a random seed number. This gives each prediction a unique trajectory of descent along the protein's energy landscape. Protein structure predictions from Rosetta@home are approximations of a global minimum in a given protein's energy landscape. That global minimum represents the most energetically favorable conformation of the protein, i.e., its native state. A primary feature of the Rosetta@home graphical user interface (GUI) is a screensaver which shows a current workunit's progress during the simulated protein folding process. In the upper-left of the current screensaver, the target protein is shown adopting different shapes (conformations) in its search for the lowest energy structure. Depicted immediately to the right is the structure of the most recently accepted. On the upper right the lowest energy conformation of the current decoy is shown; below that is the true, or native, structure of the protein if it has already been determined. Three graphs are included in the screensaver. Near the middle, a graph for the accepted model's thermodynamic free energy is displayed, which fluctuates as the accepted model changes. A graph of the accepted model's root-mean-square deviation (RMSD), which measures how structurally similar the accepted model is to the native model, is shown far right. On the right of the accepted energy graph and below the RMSD graph, the results from these two functions are used to produce an energy vs. RMSD plot as the model is progressively refined.",
            "score": 50.803762316703796
        },
        {
            "docid": "413102_6",
            "document": "Folding@home . Due to the complexity of proteins' conformation or configuration space (the set of possible shapes a protein can take), and limits in computing power, all-atom molecular dynamics simulations have been severely limited in the timescales which they can study. While most proteins typically fold in the order of milliseconds, before 2010 simulations could only reach nanosecond to microsecond timescales. General-purpose supercomputers have been used to simulate protein folding, but such systems are intrinsically costly and typically shared among many research groups. Further, because the computations in kinetic models occur serially, strong scaling of traditional molecular simulations to these architectures is exceptionally difficult. Moreover, as protein folding is a stochastic process and can statistically vary over time, it is challenging computationally to use long simulations for comprehensive views of the folding process. Protein folding does not occur in one step. Instead, proteins spend most of their folding time, nearly 96% in some cases, \"waiting\" in various intermediate conformational states, each a local thermodynamic free energy minimum in the protein's energy landscape. Through a process known as adaptive sampling, these conformations are used by Folding@home as starting points for a set of simulation trajectories. As the simulations discover more conformations, the trajectories are restarted from them, and a Markov state model (MSM) is gradually created from this cyclic process. MSMs are discrete-time master equation models which describe a biomolecule's conformational and energy landscape as a set of distinct structures and the short transitions between them. The adaptive sampling Markov state model method significantly increases the efficiency of simulation as it avoids computation inside the local energy minimum itself, and is amenable to distributed computing (including on GPUGRID) as it allows for the statistical aggregation of short, independent simulation trajectories. The amount of time it takes to construct a Markov state model is inversely proportional to the number of parallel simulations run, i.e., the number of processors available. In other words, it achieves linear parallelization, leading to an approximately four orders of magnitude reduction in overall serial calculation time. A completed MSM may contain tens of thousands of sample states from the protein's phase space (all the conformations a protein can take on) and the transitions between them. The model illustrates folding events and pathways (i.e., routes) and researchers can later use kinetic clustering to view a coarse-grained representation of the otherwise highly detailed model. They can use these MSMs to reveal how proteins misfold and to quantitatively compare simulations with experiments.",
            "score": 78.09359908103943
        },
        {
            "docid": "3190431_29",
            "document": "Spatial analysis . Spatial regression methods capture spatial dependency in regression analysis, avoiding statistical problems such as unstable parameters and unreliable significance tests, as well as providing information on spatial relationships among the variables involved. The estimated spatial relationships can be used on spatial and spatio-temporal predictions. Depending on the specific technique, spatial dependency can enter the regression model as relationships between the independent variables and the dependent, between the dependent variables and a spatial lag of itself, or in the error terms. Geographically weighted regression (GWR) is a local version of spatial regression that generates parameters disaggregated by the spatial units of analysis. This allows assessment of the spatial heterogeneity in the estimated relationships between the independent and dependent variables. The use of Bayesian hierarchical modeling in conjunction with Markov Chain Monte Carlo (MCMC) methods have recently shown to be effective in modeling complex relationships using Poisson-Gamma-CAR, Poisson-lognormal-SAR, or Overdispersed logit models.  Spatial stochastic processes, such as Gaussian processes are also increasingly being deployed in spatial regression analysis. Model-based versions of GWR, known as spatially varying coefficient models have been applied to conduct Bayesian inference. Spatial stochastic process can become computationally effective and scalable Gaussian process models, such as Gaussian Predictive Processes and Nearest Neighbor Gaussian Processes (NNGP).",
            "score": 42.44344782829285
        },
        {
            "docid": "579414_11",
            "document": "Drug design . Molecular mechanics methods may also be used to provide semi-quantitative prediction of the binding affinity. Also, knowledge-based scoring function may be used to provide binding affinity estimates. These methods use linear regression, machine learning, neural nets or other statistical techniques to derive predictive binding affinity equations by fitting experimental affinities to computationally derived interaction energies between the small molecule and the target.",
            "score": 71.72212624549866
        },
        {
            "docid": "16364924_12",
            "document": "Chemical bonding model . Either theory has its advantages and uses. As valence bond theory builds the molecular wavefunction out of localized bonds, it is more suited for the calculation of bond energies and the understanding of reaction mechanisms. In particular, valence bond theory correctly predicts the dissociation of homonuclear diatomic molecules into separate atoms, while simple molecular orbital theory predicts dissociation into a mixture of atoms and ions. Molecular orbital theory, with delocalized orbitals that obey its symmetry, is more suited for the calculation of ionization energies and the understanding of spectral absorption bands. Molecular orbitals are orthogonal, which significantly increases feasibility and speed of computer calculations compared to nonorthogonal valence bond orbitals.",
            "score": 79.13378953933716
        },
        {
            "docid": "413102_21",
            "document": "Folding@home . Drugs function by binding to specific locations on target molecules and causing some desired change, such as disabling a target or causing a conformational change. Ideally, a drug should act very specifically, and bind only to its target without interfering with other biological functions. However, it is difficult to precisely determine where and how tightly two molecules will bind. Due to limits in computing power, current \"in silico\" methods usually must trade speed for accuracy; e.g., use rapid protein docking methods instead of computationally costly free energy calculations. Folding@home's computing performance allows researchers to use both methods, and evaluate their efficiency and reliability. Computer-assisted drug design has the potential to expedite and lower the costs of drug discovery. In 2010, Folding@home used MSMs and free energy calculations to predict the native state of the villin protein to within 1.8 angstrom (\u00c5) root mean square deviation (RMSD) from the crystalline structure experimentally determined through X-ray crystallography. This accuracy has implications to future protein structure prediction methods, including for intrinsically unstructured proteins. Scientists have used Folding@home to research drug resistance by studying vancomycin, an antibiotic drug of last resort, and beta-lactamase, a protein that can break down antibiotics like penicillin.",
            "score": 52.761704325675964
        },
        {
            "docid": "216104_14",
            "document": "Protein engineering . These methods involve free modeling without using any structural information about the template. \"Ab initio\" methods are aimed at prediction of the native structures of proteins corresponding to the global minimum of its free energy. some examples of \"ab initio\" methods are AMBER, GROMOS, GROMACS, CHARMM, OPLS, and ENCEPP12. General steps for \"ab initio\" methods begin with the geometric representation of the protein of interest. Next, a potential energy function model for the protein is developed. This model can be created using either molecular mechanics potentials or protein structure derived potential functions. Following the development of a potential model, energy search techniques including molecular dynamic simulations, Monte Carlo simulations and genetic algorithms are applied to the protein.",
            "score": 100.17123436927795
        },
        {
            "docid": "8282374_71",
            "document": "Tropical cyclone . Because of the forces that affect tropical cyclone tracks, accurate track predictions depend on determining the position and strength of high- and low-pressure areas, and predicting how those areas will change during the life of a tropical system. The deep layer mean flow, or average wind through the depth of the troposphere, is considered the best tool in determining track direction and speed. If storms are significantly sheared, use of wind speed measurements at a lower altitude, such as at the 70\u00a0kPa pressure surface ( above sea level) will produce better predictions. Tropical forecasters also consider smoothing out short-term wobbles of the storm as it allows them to determine a more accurate long-term trajectory. High-speed computers and sophisticated simulation software allow forecasters to produce computer models that predict tropical cyclone tracks based on the future position and strength of high- and low-pressure systems. Combining forecast models with increased understanding of the forces that act on tropical cyclones, as well as with a wealth of data from Earth-orbiting satellites and other sensors, scientists have increased the accuracy of track forecasts over recent decades. However, scientists are not as skillful at predicting the intensity of tropical cyclones. The lack of improvement in intensity forecasting is attributed to the complexity of tropical systems and an incomplete understanding of factors that affect their development. New tropical cyclone position and forecast information is available at least every twelve hours in the Southern Hemisphere and at least every six hours in the Northern Hemisphere from Regional Specialized Meteorological Centers and Tropical Cyclone Warning Centers.",
            "score": 60.21953475475311
        },
        {
            "docid": "50956705_2",
            "document": "Coarse-grained modeling . Coarse-grained modeling, coarse-grained models, aim at simulating the behaviour of complex systems using their coarse-grained (simplified) representation. Coarse-grained models are widely used for molecular modeling of biomolecules at various granularity levels. A wide range of coarse-grained models have been proposed. They are usually dedicated to computational modeling of specific molecules: proteins, nucleic acids, lipid membranes, carbohydrates or water. In these models, molecules are represented by individual atoms and pseudo-atoms (that replace the group of atoms), or pseudo-atoms only. By decreasing the degrees of freedom much longer simulation times can be studied than using classical atomistic models. Coarse-grained models have found practical applications in: protein structure prediction, prediction of protein interactions and molecular dynamics simulations of protein folding.",
            "score": 88.087717294693
        },
        {
            "docid": "33445983_5",
            "document": "Macrocyclic stereocontrol . The degree to which a macrocyclic ring is either rigid or floppy depends significantly on the substitution of the ring and the overall size. Significantly, even small conformational preferences, such as those envisioned in floppy macrocycles, can profoundly influence the ground state of a given reaction, providing stereocontrol such as in the synthesis of miyakolide. Computational modeling can predict conformations of medium rings with reasonable accuracy, as Still used molecular mechanics modeling computations to predict ring conformations to determine potential reactivity and stereochemical outcomes.",
            "score": 59.00271034240723
        },
        {
            "docid": "45329906_21",
            "document": "Solvent models . Quantitative Structure\u2013Activity Relationships (QSAR)/Quantitative Structure\u2013Property Relationships (QSPR), whilst unable to directly model the physical process occurring in a condensed solvent phase, can provide useful predictions of solvent and solvation properties and activities; such as the solubility of a solute. These methods come in a varied way from simple regression models to sophisticated machine learning methods. Generally, QSAR/QSPR methods require descriptors; these come in many different forms and are used to represent physical features and properties of a system of interest. Descriptors are generally single numerical values which hold some information about a physical property. A regression model or statistical learning model is then applied to find a correlation between the descriptor(s) and the property of interest. Once trained on some known data these model can be applied to similar unknown data to make predictions. Typically the known data comes from experimental measurement, although there is no reason why similar methods can not be used to correlate descriptor(s) with theoretical or predicted values. It is currently debated whether if more accurate experimental data was used to train these models whether the prediction from such models would be more accurate.",
            "score": 75.58171677589417
        },
        {
            "docid": "226631_4",
            "document": "Logistic regression . Logistic regression is used in various fields, including machine learning, most medical fields, and social sciences. For example, the Trauma and Injury Severity Score (TRISS), which is widely used to predict mortality in injured patients, was originally developed by Boyd et al. using logistic regression. Many other medical scales used to assess severity of a patient have been developed using logistic regression. Logistic regression may be used to predict the risk of developing a given disease (e.g. diabetes; coronary heart disease), based on observed characteristics of the patient (age, sex, body mass index, results of various blood tests, etc.). Another example might be to predict whether an Indian voter will vote BJP or Trinamool Congress or Left Front or Congress, based on age, income, sex, race, state of residence, votes in previous elections, etc. The technique can also be used in engineering, especially for predicting the probability of failure of a given process, system or product. It is also used in marketing applications such as prediction of a customer's propensity to purchase a product or halt a subscription, etc. In economics it can be used to predict the likelihood of a person's choosing to be in the labor force, and a business application would be to predict the likelihood of a homeowner defaulting on a mortgage. Conditional random fields, an extension of logistic regression to sequential data, are used in natural language processing.",
            "score": 47.62559950351715
        },
        {
            "docid": "50399682_15",
            "document": "Predictive engineering analytics . Already when evaluating potential architectures, 1D simulation should be combined with models of control software, as the electronic control unit (ECU) will play a crucial role in achieving and maintaining the right balance between functional performance aspects when the product will operate. During this phase, engineers cascade down the design objectives to precise targets for subsystems and components. They use multi-domain optimization and design trade-off techniques. The controls need to be included in this process. By combining them with the system models in MiL simulations, potential algorithms can be validated and selected.  In practice, MiL involves co-simulation between virtual controls from dedicated controller modeling software and scalable 1D models of the multi-physical system. This provides the right combination of accuracy and calculation speed for investigation of concepts and strategies, as well as controllability assessment.",
            "score": 84.16279602050781
        },
        {
            "docid": "13967547_5",
            "document": "Dry lab . As a means of surpassing the limitations of these techniques, projects such as Folding@home and Rosetta@home are aimed at resolving this problem using computational analysis, this means of resolving protein structure is referred to as protein structure prediction. Although many labs have a slightly different approach, the main concept is to find, from a myriad of protein conformations, which conformation has the lowest energy or, in the case of Folding@Home, to find relatively low energies of proteins that could cause the protein to misfold and aggregate other proteins to itself\u2014like in the case of sickle cell anemia. The general scheme in these projects is that a small number of computations are parsed to, or sent to be calculated on, a computer, generally a home computer, and then that computer analyzes the likelihood that a specific protein will take a certain shape or conformation based on the amount of energy required for that protein to stay in that shape, this way of processing data is what is generally referred to as distributed computing. This analysis is done on an extraordinarily large number of different conformations, owing to the support of hundreds of thousands of home-based computers, in hopes to find the conformation of lowest possible energy or set of conformations of lowest possible energy relative to any conformations that are just slightly different. Although doing so is quite difficult, one can, by observing the energy distribution of a large number of conformations, despite the almost infinite number of different protein conformations possible for any given protein (see Levinthal Paradox), with a reasonably large number of protein energy samplings, predict relatively closely what conformation, within a range of conformations, has the expected lowest energy using methods in statistical inference. There are other factors such as salt concentration, pH, ambient temperature or chaperonins, which are proteins that assist in the folding process of other proteins, that can greatly affect how a protein folds. However, if the given protein is shown to fold on its own, especially in vitro, these findings can be further supported. Once we can see how a protein folds then we can see how it works as a catalyst, or in intracellular communication, e.g. neuroreceptor-neurotransmitter interaction. How certain compounds may be used to enhance or prevent the function of these proteins and how an elucidated protein overall plays a role in diseases such as Alzheimer's Disease or Huntington's Disease can also be much better understood.",
            "score": 34.83904528617859
        },
        {
            "docid": "22832517_5",
            "document": "Crystal structure prediction . Since 2007, significant progress has been made in the CSP of small organic molecules, with several different methods proving effective. The most widely discussed method first ranks the energies of all possible crystal structures using a customised MM force field, and finishes by using a dispersion-corrected DFT step to estimate the lattice energy and stability of each short-listed candidate structure. More recent efforts to predict crystal structures have focused on estimating crystal free energy by including the effects of temperature and entropy in organic crystals using vibrational analysis or molecular dynamics.",
            "score": 54.675414085388184
        },
        {
            "docid": "62329_32",
            "document": "Meta-analysis . Since neither of these factors automatically indicates a faulty larger study or more reliable smaller studies, the re-distribution of weights under this model will not bear a relationship to what these studies actually might offer. Indeed, it has been demonstrated that redistribution of weights is simply in one direction from larger to smaller studies as heterogeneity increases until eventually all studies have equal weight and no more redistribution is possible. Another issue with the random effects model is that the most commonly used confidence intervals generally do not retain their coverage probability above the specified nominal level and thus substantially underestimate the statistical error and are potentially overconfident in their conclusions. Several fixes have been suggested but the debate continues on. A further concern is that the average treatment effect can sometimes be even less conservative compared to the fixed effect model and therefore misleading in practice. One interpretational fix that has been suggested is to create a prediction interval around the random effects estimate to portray the range of possible effects in practice. However, an assumption behind the calculation of such a prediction interval is that trials are considered more or less homogeneous entities and that included patient populations and comparator treatments should be considered exchangeable and this is usually unattainable in practice. The most widely used method to estimate between studies variance (REVC) is the DerSimonian-Laird (DL) approach. Several advanced iterative (and computationally expensive) techniques for computing the between studies variance exist (such as maximum likelihood, profile likelihood and restricted maximum likelihood methods) and random effects models using these methods can be run in Stata with the metaan command. The metaan command must be distinguished from the classic metan (single \"a\") command in Stata that uses the DL estimator. These advanced methods have also been implemented in a free and easy to use Microsoft Excel add-on, MetaEasy. However, a comparison between these advanced methods and the DL method of computing the between studies variance demonstrated that there is little to gain and DL is quite adequate in most scenarios.",
            "score": 54.086090087890625
        },
        {
            "docid": "18166009_14",
            "document": "Lower critical solution temperature . There are three groups of methods for correlating and predicting LCSTs. The first group proposes models that are based on a solid theoretical background using liquid\u2013liquid or vapor\u2013liquid experimental data. These methods require experimental data to adjust the unknown parameters, resulting in limited predictive ability . Another approach uses empirical equations that correlate \u03b8(LCST) with physicochemical properties such as density, critical properties etc., but suffers from the disadvantage that these properties are not always available. A new approach proposed by Liu and Zhong develops linear models for the prediction of \u03b8(LCST) using molecular connectivity indices, which depends only on the solvent and polymer structures. The latter approach has proven to be a very useful technique in quantitative structure\u2013activity/property relationships (QSAR/QSPR) research for polymers and polymer solutions. QSAR/QSPR studies constitute an attempt to reduce the trial-and-error element in the design of compounds with desired activity/properties by establishing mathematical relationships between the activity/property of interest and measurable or computable parameters, such as topological, physicochemical, stereochemistry, or electronic indices. More recently QSPR models for the prediction of the \u03b8 (LCST) using molecular (electronic, physicochemical etc.) descriptors have been published. Using validated robust QSPR models, experimental time and effort can be reduced significantly as reliable estimates of \u03b8(LCST) for polymer solutions can be obtained before they are actually synthesized in the laboratory.",
            "score": 60.531469106674194
        },
        {
            "docid": "2891226_4",
            "document": "Mechanotransduction . Single-molecule biomechanics studies of proteins and DNA, and mechanochemical coupling in molecular motors have demonstrated the critical importance of molecular mechanics as a new frontier in bioengineering and life sciences. Protein domains, connected by intrinsically disordered flexible linker domains, induce long-range allostery via . The resultant dynamic modes cannot be generally predicted from static structures of either the entire protein or individual domains. They can however be inferred by comparing different structures of a protein (as in Database of Molecular Motions). They can also be suggested by sampling in extensive molecular dynamics trajectories and principal component analysis, or they can be directly observed using spectra measured by neutron spin echo spectroscopy. Current findings indicate that the mechanotransduction channel in hair cells is a complex biological machine. Mechanotransduction also includes the use of chemical energy to do mechanical work.",
            "score": 90.9829249382019
        },
        {
            "docid": "56731305_11",
            "document": "RNA-targeting small molecule drugs . Aside from studies involved r(CUG) repeats, other complex RNA structures have also been targeted. Pearson and coworkers discovered that a cationic porphyrin (TMPyP4) bound a G-quadruplex r(GC) and inhibited the binding of proteins to r(GC). Work by Disney and Petrucelli rationally identified small molecules that can target this repeat and affect disease biology in model cellular systems and also in patient-derive iNeurons. Further studies by Rothstein and colleagues determined that TMPyP4 could suppress r(GC)-mediated neurodegeneration in a \"Drosophila\" model. Additionally targets have been rationally identified by using a powerful seqecune-based design approach termed informal to identify dozens of bioactive small molecules that target disease causing non-coding RNA termed INFORNA. This study important showed for the first time that small molecules appear to have selectivities that are competitive with oligonucleotides with cell-permeable and medicinally optimizable small molecules. Additionally, compounds have been shown to be bioactive in diverse disease settings that ranged from breast cancer. and hepatocellular carcinoma. More recently, the Disney group further used their prediction database INFORNA to design Targaprimir-96 to target miRNA precursors in animal models of cancer, the first small molecules to do so. This compound has a nanomolar affinity for the miRNA hairpin precursor selectively over other sequences. Targaprimir-96 was further tested in cells and in mice, inhibiting tumor growth in a xenograft mouse model of triple negative breast cancer upon i.p. injection. RNA-targeting small molecule drug discovery has greatly benefitted from the available cellular models for disease. The use of cell culture in early development has become a requirement for assessing the basic efficacy of a drug candidate. Thus, more research groups have implemented these techniques in their programs. In a leading example, Al-Hashimi and coworkers identified six small molecules with high affinity for TAR of HIV-1 through a computational approach. They docked a library of small molecules onto RNA dynamic structures generated by NMR and Molecular Dynamics (MD) simulations. The hit molecules inhibited the Tat\u2014TAR interaction \"in vitro\". They arrived at lead molecule, netilmicin, that had the best selectivity for HIV-1 TAR and inhibited HIV-1 replication in cells with a low IC50. The Disney group has studied aminoglycoside derivatives in 2009 for their ability to inhibit interactions between repeat RNA and proteins. Using their prediction database INFORNA, they discovered that a compound could bind to 1 x 1 UU internal loops on an N-methyl peptide backbone. They confirmed that like other compounds that target DM1 r(CUG), they could inhibit the complex between r(CUG)-MBNL1, disrupt nuclear foci, and increase nucleocytoplasmic transport of the gene in patient-derived DM1 fibroblasts. In that study the Disney group also described several approaches to validate the RNA targets of small molecules. In the first approach termed chemical cross-linking and isolation by pull down (Chem-CLIP) and chemical cross-linking and isolation by pull down to map binding sites (Chem-CLIP-Map).",
            "score": 67.87920928001404
        },
        {
            "docid": "26621085_4",
            "document": "X-Pol: the Explicit Polarization Theory . Although the X-Pol method can be used as an electronic structural approach for studying intermolecular interactions, the most significant is that the X-Pol potential provides a general theoretical framework for development of next-generation force fields in condensed-phase and macromolecular simulations using electronic structural methods explicitly, going beyond the traditional molecular mechanics as developed by Terrell Hill and F. H. Westheimer for organic molecules and the Lifson force fields for biological systems. The computational algorithm, feasibility and accuracy of the X-Pol potential have been demonstrated through statistical mechanical Monte Carlo simulations of liquid water, with the computed heat of vaporization and liquid density within 1% and 3% of experiments, respectively. The X-Pol method has been illustrated to be feasible for extensive molecular dynamics simulations of fully solvated proteins under periodic boundary conditions. The X-Pol method can be used to perform 3,200 full energy and analytical gradient evaluations in molecular dynamics simulation of a sovated Bovine Pancreatic Trypsin Inhibitor (BPTI) protein in water, consisting of nearly 15,000 atoms and 30,000 basis functions, on a single 1.6\u00a0GHz processor in one day, in 2008.",
            "score": 115.40197789669037
        },
        {
            "docid": "49399383_2",
            "document": "SAMPL Challenge . SAMPL (Statistical Assessment of the Modeling of Proteins and Ligands) is a set of community-wide blind challenges aimed to advance computational techniques as standard predictive tools in rational drug design. A broad range of biologically relevant systems with different sizes and levels of complexities including proteins, host-guest complexes, and drug-like small molecules have been selected to test the latest modeling methods and force fields in SAMPL. New experimental data, such as binding affinity and hydration free energy, are withheld from participants until the prediction submission deadline, so that the true predictive power of methods can be revealed. The most recent SAMPL5 challenge contains two prediction categories: the binding affinity of host-guest systems, and the distribution coefficients of drug-like molecules between water and cyclohexane. Since 2008, the SAMPL challenge series has attracted widespread interest from scientists engaged in the field of computer-aided drug design (CADD) around the world, and has resulted in around 100 publications with many of them highly cited. The current SAMPL organizers include Prof. John Chodera at Memorial Sloan Kettering Cancer Center, Prof. Michael K. Gilson at University of California, San Diego, Prof. David Mobley at University of California, Irvine, and Prof. Michael Shirts, at University of Colorado, Boulder.",
            "score": 45.22454106807709
        },
        {
            "docid": "1645401_31",
            "document": "United States Air Force Stability and Control Digital DATCOM . There has been some research in using Digital DATCOM in conjunction with wind tunnel studies to predict aerodynamics of structurally impaired aircraft. Dr. Bilal Siddiqui at DHA Suffa University presented an approach to predict the nonlinear aerodynamics of a structurally damaged aircraft model based on the engineering level aerodynamic prediction methods, DATCOM. Raw results from the code provide good correlation with wind tunnel data at very low angles of attack, but accuracy deteriorates rapidly as the angle of attack increases. A new methodology is then proposed which combines the experimental results of healthy aircraft with the predicted aerodynamics of the damaged cases, to yield better correlation between experimental and predicted aerodynamic coefficients for damaged aircraft. Three damage-configurations are studied at supersonic speeds. The methodology can be used to quickly generate aerodynamic model for damaged aircraft for simulation and reconfigurable control",
            "score": 70.16622626781464
        },
        {
            "docid": "302944_3",
            "document": "Gaussian process . A machine-learning algorithm that involves a Gaussian process uses lazy learning and a measure of the similarity between points (the \"kernel function\") to predict the value for an unseen point from training data. The prediction is not just an estimate for that point, but also has uncertainty information\u2014it is a one-dimensional Gaussian distribution (which is the marginal distribution at that point).",
            "score": 55.757779598236084
        },
        {
            "docid": "27051151_69",
            "document": "Big data . Much in the same line, it has been pointed out that the decisions based on the analysis of big data are inevitably \"informed by the world as it was in the past, or, at best, as it currently is\". Fed by a large number of data on past experiences, algorithms can predict future development if the future is similar to the past. If the systems dynamics of the future change (if it is not a stationary process), the past can say little about the future. In order to make predictions in changing environments, it would be necessary to have a thorough understanding of the systems dynamic, which requires theory. As a response to this critique Alemany Oliver and Vayre suggested to use \"abductive reasoning as a first step in the research process in order to bring context to consumers\u2019 digital traces and make new theories emerge\". Additionally, it has been suggested to combine big data approaches with computer simulations, such as agent-based models and Complex Systems. Agent-based models are increasingly getting better in predicting the outcome of social complexities of even unknown future scenarios through computer simulations that are based on a collection of mutually interdependent algorithms. Finally, use of multivariate methods that probe for the latent structure of the data, such as factor analysis and cluster analysis, have proven useful as analytic approaches that go well beyond the bi-variate approaches (cross-tabs) typically employed with smaller data sets.",
            "score": 80.96914601325989
        },
        {
            "docid": "13629713_3",
            "document": "Applicability domain . The purpose of AD is to state whether the model's assumptions are met and for which chemicals the model can be reliably applicable. In general, this is the case for interpolation rather than for extrapolation. Up to now there is no single generally accepted algorithm for determining the AD: a comprehensive survey can be found in a Report and Recommendations of ECVAM Workshop 52. There exists a rather systematic approach for defining interpolation regions. The process involves the removal of outliers and a probability density distribution method using kernel-weighted sampling.  Another widely used approach for the structural AD of the regression QSAR models is based on the leverage calculated from the diagonal values of the hat matrix of the modeling molecular descriptors.  A recent rigorous benchmarking study of several AD algorithms identified standard-deviation of model predictions as the most reliable approach. To investigate the AD of a training set of chemicals one can directly analyse properties of the multivariate descriptor space of the training compounds or more indirectly via distance (or similarity) metrics. When using distance metrics care should be taken to use an orthogonal and significant vector space. This can be achieved by different means of feature selection and successive principal components analysis.",
            "score": 65.49133658409119
        },
        {
            "docid": "180855_15",
            "document": "Kalman filter . In this example, the Kalman filter can be thought of as operating in two distinct phases: predict and update. In the prediction phase, the truck's old position will be modified according to the physical laws of motion (the dynamic or \"state transition\" model). Not only will a new position estimate be calculated, but a new covariance will be calculated as well. Perhaps the covariance is proportional to the speed of the truck because we are more uncertain about the accuracy of the dead reckoning position estimate at high speeds but very certain about the position estimate when moving slowly. Next, in the update phase, a measurement of the truck's position is taken from the GPS unit. Along with this measurement comes some amount of uncertainty, and its covariance relative to that of the prediction from the previous phase determines how much the new measurement will affect the updated prediction. Ideally, as the dead reckoning estimates tend to drift away from the real position, the GPS measurement should pull the position estimate back towards the real position but not disturb it to the point of becoming rapidly jumping and noisy.",
            "score": 63.5328483581543
        }
    ],
    "r": [
        {
            "docid": "198608_42",
            "document": "Molecular dynamics . QM (quantum-mechanical) methods are very powerful. However, they are computationally expensive, while the MM (classical or molecular mechanics) methods are fast but suffer from several limits (require extensive parameterization; energy estimates obtained are not very accurate; cannot be used to simulate reactions where covalent bonds are broken/formed; and are limited in their abilities for providing accurate details regarding the chemical environment). A new class of method has emerged that combines the good points of QM (accuracy) and MM (speed) calculations. These methods are termed mixed or hybrid quantum-mechanical and molecular mechanics methods (hybrid QM/MM).",
            "score": 131.24497985839844
        },
        {
            "docid": "541957_12",
            "document": "Lysozyme . More recently, quantum mechanics/ molecular mechanics (QM/MM) molecular dynamics simulations have been using the crystal of HEWL and predict the existence of a covalent intermediate. Evidence for the ESI-MS and X-ray structures indicate the existence of covalent intermediate, but primarily rely on using a less active mutant or non-native substrate. Thus, QM/MM molecular dynamics provides the unique ability to directly investigate the mechanism of wild-type HEWL and native substrate. The calculations revealed that the covalent intermediate from the Koshland mechanism is ~30 kcal/mol more stable than the ionic intermediate from the Phillips mechanism. These calculation demonstrate that the ionic intermediate is extremely energetically unfavorable and the covalent intermediates observed from experiments using less active mutant or non-native substrates provide useful insight into the mechanism of wild-type HEWL. Imidazole derivatives can form a charge-transfer complex with some residues (in or outside active center) to achieve a competitive inhibition of lysozyme. In Gram-negative bacteria, the lipopolysaccharide acts as a non-competitive inhibitior by highly-favored binding with lysozyme.",
            "score": 125.61986541748047
        },
        {
            "docid": "45329906_8",
            "document": "Solvent models . Explicit solvent models treat explicitly (i.e. the coordinates and usually at least some of the molecular degrees of freedom are included) the solvent molecules. This is a more intuitively realistic picture in which there are direct, specific solvent interactions with a solute, in contrast to continuum models. These models generally occur in the application of molecular mechanics (MM) and dynamics (MD) or Monte Carlo (MC) simulations, although some quantum chemical calculations do use solvent clusters. Molecular dynamics simulations allow scientists to study the time evolution of a chemical system in discrete time intervals. These simulations often utilize molecular mechanics force fields which are generally empirical, parametrized functions which can efficiently calculate the properties and motions of large system., Parametrization is often to a higher level theory or experimental data. MC simulations allow one to explore the potential energy surface of a system by perturbing the system and calculating the energy after the perturbation. Prior criteria are defined to aid the algorithm in deciding whether to accept the newly perturbed system or not.",
            "score": 122.8043441772461
        },
        {
            "docid": "14176371_5",
            "document": "Debra Searles . Study of nonequilibrium liquids via statistical mechanics; nonequilibrium molecular dynamics; dynamical systems theory; chaos theory The fluctuation theorem The study of fluids in confined spaces Development of algorithms for molecular dynamics simulations Calculation of liquid properties: Combining molecular dynamics simulations with quantum mechanical calculations to determine properties of liquids",
            "score": 118.93502807617188
        },
        {
            "docid": "46418570_5",
            "document": "Nancy Makri . Dr. Makri works in the area of theoretical chemical physics. She has developed new theoretical approaches to simulating the dynamics of quantum mechanical phenomena. Makri has developed novel methods for calculating numerically exact path integrals for the simulation of system dynamics in harmonic dissipative environments. Her simulation algorithms address the limitations of the Schr\u00f6dinger equation, which can only describe physical changes exactly in the quantum state of small molecules. By identifying aspects of simulations which can be effectively simplified, Dr. Makri's group have developed \"the first fully quantum mechanical methodology for calculating the evolution of a quantum system in a dissipative environment by performing an iterative decomposition of Feynman\u2019s path integral expression\" (however the Hierarchical equations of motion technique was developed slightly earlier). Such simplifications make it possible to calculate outcomes that otherwise would not be mathematically feasible. Her careful examinations of the system-harmonic bath model have resulted in techniques for avoiding the Monte Carlo sign problem.",
            "score": 115.55423736572266
        },
        {
            "docid": "26621085_4",
            "document": "X-Pol: the Explicit Polarization Theory . Although the X-Pol method can be used as an electronic structural approach for studying intermolecular interactions, the most significant is that the X-Pol potential provides a general theoretical framework for development of next-generation force fields in condensed-phase and macromolecular simulations using electronic structural methods explicitly, going beyond the traditional molecular mechanics as developed by Terrell Hill and F. H. Westheimer for organic molecules and the Lifson force fields for biological systems. The computational algorithm, feasibility and accuracy of the X-Pol potential have been demonstrated through statistical mechanical Monte Carlo simulations of liquid water, with the computed heat of vaporization and liquid density within 1% and 3% of experiments, respectively. The X-Pol method has been illustrated to be feasible for extensive molecular dynamics simulations of fully solvated proteins under periodic boundary conditions. The X-Pol method can be used to perform 3,200 full energy and analytical gradient evaluations in molecular dynamics simulation of a sovated Bovine Pancreatic Trypsin Inhibitor (BPTI) protein in water, consisting of nearly 15,000 atoms and 30,000 basis functions, on a single 1.6\u00a0GHz processor in one day, in 2008.",
            "score": 115.4019775390625
        },
        {
            "docid": "8124077_6",
            "document": "Transition state analog . Computational approaches have been regarded as a useful tool to elucidate the mechanism of action of enzymes. Molecular mechanics itself can not predict the electron transfer which is the fundamental of organic reaction but the molecular dynamics simulation provide sufficient information considering the flexibility of protein during catalytic reaction. The complementary method would be combined molecular mechanics/ quantum mechanics simulation (QM/MM)methods. With this approach, only the atoms responsible for enzymatic reaction in the catalytic region will be reared with quantum mechanics and the rest of the atoms were treated with molecular mechanics.",
            "score": 113.47628784179688
        },
        {
            "docid": "22833956_3",
            "document": "Molecular models of DNA . The more advanced, computer-based molecular models of DNA involve molecular dynamics simulations and quantum mechanics computations of vibro-rotations, delocalized molecular orbitals (MOs), electric dipole moments, hydrogen-bonding, and so on. \"DNA molecular dynamics modeling\" involves simulating deoxyribonucleic acid (DNA) molecular geometry and topology changes with time as a result of both intra- and inter- molecular interactions of DNA. Whereas molecular models of DNA molecules such as closely packed spheres (CPK models) made of plastic or metal wires for \"skeletal models\" are useful representations of static DNA structures, their usefulness is very limited for representing complex DNA dynamics. Computer molecular modeling allows both animations and molecular dynamics simulations that are very important to understand how DNA functions \"in vivo\".",
            "score": 113.44912719726562
        },
        {
            "docid": "2916615_2",
            "document": "Force field (chemistry) . In the context of molecular modeling, a force field (a special case of energy functions or interatomic potentials; not to be confused with force field in classical physics) refers to the functional form and parameter sets used to calculate the potential energy of a system of atoms or coarse-grained particles in molecular mechanics and molecular dynamics simulations. The parameters of the energy functions may be derived from experiments in physics or chemistry, calculations in quantum mechanics, or both.",
            "score": 113.24378967285156
        },
        {
            "docid": "6019_6",
            "document": "Computational chemistry . In some cases, the details of electronic structure are less important than the long-time phase space behavior of molecules. This is the case in conformational studies of proteins and protein-ligand binding thermodynamics. Classical approximations to the potential energy surface are used, as they are computationally less intensive than electronic calculations, to enable longer simulations of molecular dynamics. Furthermore, cheminformatics uses even more empirical (and computationally cheaper) methods like machine learning based on physicochemical properties. One typical problem in cheminformatics is to predict the binding affinity of drug molecules to a given target.",
            "score": 111.0193099975586
        },
        {
            "docid": "35802271_2",
            "document": "Path integral molecular dynamics . Path integral molecular dynamics (PIMD) is a method of incorporating quantum mechanics into molecular dynamics simulations using Feynman path integrals. In PIMD, one uses the Born\u2013Oppenheimer approximation to separate the wavefunction into a nuclear part and an electronic part. The nuclei are treated quantum mechanically by mapping each quantum nucleus onto a classical system of several fictitious particles connected by springs (harmonic potentials) governed by an effective Hamiltonian, which is derived from Feynman's path integral. The resulting classical system, although complex, can be solved relatively quickly. There are now a number of commonly used condensed matter computer simulation techniques that make use of the path integral formulation including Centroid Molecular Dynamics (CMD), Ring Polymer Molecular Dynamics (RPMD), and the Feynman-Kleinert Quasi-Classical Wigner (FK-QCW) method. The same techniques are also used in path integral Monte Carlo (PIMC).",
            "score": 109.85502624511719
        },
        {
            "docid": "198608_2",
            "document": "Molecular dynamics . Molecular dynamics (MD) is a computer simulation method for studying the physical movements of atoms and molecules. The atoms and molecules are allowed to interact for a fixed period of time, giving a view of the dynamic evolution of the system. In the most common version, the trajectories of atoms and molecules are determined by numerically solving Newton's equations of motion for a system of interacting particles, where forces between the particles and their potential energies are often calculated using interatomic potentials or molecular mechanics force fields. The method was originally developed within the field of theoretical physics in the late 1950s but is applied today mostly in chemical physics, materials science and the modelling of biomolecules.",
            "score": 107.23216247558594
        },
        {
            "docid": "4646870_4",
            "document": "Spartan (software) . Quantum chemical calculations are also called upon to furnish information about mechanisms and product distributions of chemical reactions, either directly by calculations on transition states, or based on Hammond's postulate, by modeling the steric and electronic demands of the reactants. Quantitative calculations, leading directly to information about the geometries of transition states, and about reaction mechanisms in general, are increasingly common, while qualitative models are still needed for systems that are too large to be subjected to more rigorous treatments. Quantum chemical calculations can supply information to complement existing experimental data or replace it altogether, for example, atomic charges for quantitative structure-activity relationship (QSAR) analyses, and intermolecular potentials for molecular mechanics and molecular dynamics calculations.",
            "score": 105.90642547607422
        },
        {
            "docid": "6019_38",
            "document": "Computational chemistry . Molecular dynamics (MD) use either quantum mechanics, molecular mechanics or a mixture of both to calculate forces which are then used to solve Newton's laws of motion to examine the time-dependent behaviour of systems. The result of a molecular dynamics simulation is a trajectory that describes how the position and velocity of particles varies with time.",
            "score": 105.88033294677734
        },
        {
            "docid": "48336080_6",
            "document": "Emily A. Carter . Carter has made significant contributions to theoretical and computational chemistry and physics. She has developed \"ab initio\" quantum chemistry methods and applied them to the study of materials. Early contributions included methods for accurate description of molecules at the quantum level and an algorithm for identifying transitional states in chemical reactions. She pioneered the combination of ab initio quantum chemistry with kinetic Monte Carlo simulations (KMC), molecular dynamics (MD), and quasicontinuum solid mechanics simulations relevant to the study of surfaces and interfaces of materials.. She has studied the chemical and mechanical causes and mechanisms of failure in materials such as silicon, germanium, iron and steel. She has also proposed methods for protecting materials from failure.",
            "score": 105.66424560546875
        },
        {
            "docid": "734256_4",
            "document": "Molecular modelling . This function, referred to as a potential function, computes the molecular potential energy as a sum of energy terms that describe the deviation of bond lengths, bond angles and torsion angles away from equilibrium values, plus terms for non-bonded pairs of atoms describing van der Waals and electrostatic interactions. The set of parameters consisting of equilibrium bond lengths, bond angles, partial charge values, force constants and van der Waals parameters are collectively termed a force field. Different implementations of molecular mechanics use different mathematical expressions and different parameters for the potential function. The common force fields in use today have been developed by using high level quantum calculations and/or fitting to experimental data. The method, termed energy minimization, is used to find positions of zero gradient for all atoms, in other words, a local energy minimum. Lower energy states are more stable and are commonly investigated because of their role in chemical and biological processes. A molecular dynamics simulation, on the other hand, computes the behaviour of a system as a function of time. It involves solving Newton's laws of motion, principally the second law, formula_3. Integration of Newton's laws of motion, using different integration algorithms, leads to atomic trajectories in space and time. The force on an atom is defined as the negative gradient of the potential energy function. The energy minimization method is useful to obtain a static picture for comparing between states of similar systems, while molecular dynamics provides information about the dynamic processes with the intrinsic inclusion of temperature effects.",
            "score": 105.50042724609375
        },
        {
            "docid": "198608_40",
            "document": "Molecular dynamics . In classical molecular dynamics, one potential energy surface (usually the ground state) is represented in the force field. This is a consequence of the Born\u2013Oppenheimer approximation. In excited states, chemical reactions or when a more accurate representation is needed, electronic behavior can be obtained from first principles by using a quantum mechanical method, such as density functional theory. This is named Ab Initio Molecular Dynamics (AIMD). Due to the cost of treating the electronic degrees of freedom, the computational cost of these simulations is far higher than classical molecular dynamics. This implies that AIMD is limited to smaller systems and shorter times.",
            "score": 105.28781127929688
        },
        {
            "docid": "23634_48",
            "document": "Protein . Mathematical models to simulate dynamic processes of protein folding and binding involve molecular mechanics, in particular, molecular dynamics. Monte Carlo techniques facilitate the computations, which exploit advances in parallel and distributed computing (for example, the Folding@home project which performs molecular modeling on GPUs). \"In silico\" simulations discovered the folding of small \u03b1-helical protein domains such as the villin headpiece and the HIV accessory protein. Hybrid methods combining standard molecular dynamics with quantum mechanical mathematics explored the electronic states of rhodopsins.",
            "score": 104.88867950439453
        },
        {
            "docid": "198608_8",
            "document": "Molecular dynamics . The results of MD simulations can be tested through comparison to experiments that measure molecular dynamics, of which a popular method is NMR spectroscopy. MD-derived structure predictions can be tested through community-wide experiments in Critical Assessment of protein Structure Prediction (CASP), although the method has historically had limited success in this area. Michael Levitt, who shared the Nobel Prize partly for the application of MD to proteins, wrote in 1999 that CASP participants usually did not use the method due to \"\"... a central embarrassment of molecular mechanics, namely that energy minimization or molecular dynamics generally leads to a model that is less like the experimental structure.\"\" Improvements in computational resources permitting more and longer MD trajectories, combined with modern improvements in the quality of force field parameters, have yielded some improvements in both structure prediction and homology model refinement, without reaching the point of practical utility in these areas; many identify force field parameters as a key area for further development.",
            "score": 103.79019165039062
        },
        {
            "docid": "22640378_2",
            "document": "Ascalaph Designer . Ascalaph Designer is a computer program for general purpose molecular modelling for molecular design and simulations. It provides a graphical environment for the common programs of quantum and classical molecular modelling ORCA, NWChem, Firefly, CP2K and MDynaMix  . The molecular mechanics calculations cover model building, energy optimizations and molecular dynamics. Firefly (formerly named PC GAMESS) covers a wide range of quantum chemistry methods. Ascalaph Designer is free and open-source software, released under the GNU General Public License, version 2 (GPLv2).",
            "score": 103.76190948486328
        },
        {
            "docid": "22553477_2",
            "document": "BOSS (molecular mechanics) . Biochemical and Organic Simulation System (BOSS) is a general-purpose molecular modeling program that performs molecular mechanics calculations, Metropolis Monte Carlo statistical mechanics simulations, and semiempirical Austin Model 1 (AM1), PM3, and PDDG/PM3 quantum mechanics calculations. The molecular mechanics calculations cover energy minimizations, normal mode analysis and conformational searching with the Optimized Potentials for Liquid Simulations (OPLS) force fields. BOSS is developed by Prof. William L. Jorgensen at Yale University, and distributed commercially by Cemcomco, LLC and Schr\u00f6dinger, Inc.",
            "score": 103.41917419433594
        },
        {
            "docid": "14149235_4",
            "document": "Energy profile (chemistry) . In simplest terms, a potential energy surface or PES is a mathematical or graphical representation of the relation between energy of a molecule and its geometry. The methods for describing the potential energy are broken down into a classical mechanics interpretation (molecular mechanics) and a quantum mechanical interpretation. In the quantum mechanical interpretation an exact expression for energy can be obtained for any molecule derived from quantum principles (although an infinite basis set may be required) but ab initio calculations/methods will often use approximations to reduce computational cost. Molecular mechanics is empirically based and potential energy is described as a function of component terms that correspond to individual potential functions such as torsion, stretches,bends, Van der Waals energies,electrostatics and cross terms. Each component potential function is fit to experimental data or properties predicted by ab initio calculations. Molecular mechanics is useful in predicting equilibrium geometries and transition states as well as relative conformational stability. As a reaction occurs the atoms of the molecules involved will generally undergo some change in spatial orientation through internal motion as well as its electronic environment. Distortions in the geometric parameters result in a deviation from the equilibrium geometry (local energy minima). These changes in geometry of a molecule or interactions between molecules are dynamic processes which call for understanding all the forces operating within the system. Since these forces can be mathematically derived as first derivative of potential energy with respect to a displacement, it makes sense to map the potential energy E of the system as a function of geometric parameters q, q, q and so on. The potential energy at given values of the geometric parameters (q, q,\u2026, q) is represented as a hyper-surface (when n >2 or a surface when n \u2264 2). Mathematically, it can be written as-",
            "score": 103.39958190917969
        },
        {
            "docid": "27922389_6",
            "document": "Roberto Car . In 2009 he shared the Dirac Medal with Michele Parrinello for their development of the \"ab initio\" molecular dynamics simulation method. The method combines the quantum mechanical density functional theory for calculation of electronic structure with methods of molecular dynamics for the simulation of classical (\"Newtonian\") atomic movements.They call their procedure \"ab initio molecular dynamics \"; it is also well known as the Car-Parrinello method. The procedure was jointly developed by both men in 1985, when they were in Trieste. Their procedure has found various applications in solid-state physics, biochemistry, chemical physics, and materials science.",
            "score": 102.70083618164062
        },
        {
            "docid": "43095308_13",
            "document": "Bernd Michael Rode . A particular notable contribution of Prof. Rode's research is the development and application of hybrid quantum mechanical/molecular mechanical (QM/MM) simulation techniques, focusing on a broad range of problems in solution chemistry. In 2004 an improved QM/MM technique known as Quantum Mechanical Charge Field (QMCF) Molecular Dynamics explicitly aimed at the treatment of solvated systems has been developed in Prof. Rode's research group. During the last years the application of the QMCF MD technique enabled accurate simulations of ionic compounds and organic species as well as coordination complexes in aqueous solution.His most recent research is focussed on the lanthanoid ions in aqueous solution.",
            "score": 102.13938903808594
        },
        {
            "docid": "32816134_15",
            "document": "List of Folding@home cores . Short for Car\u2013Parrinello Molecular Dynamics, this core performs ab-initio quantum mechanical molecular dynamics. Unlike classical molecular dynamics calculations which use a force field approach, CPMD includes the motion of electrons in the calculations of energy, forces and motion. Quantum chemical calculations have the possibility to yield a very reliable potential energy surface, and can naturally incorporate multi-body interactions.",
            "score": 101.7621841430664
        },
        {
            "docid": "2865619_2",
            "document": "SIESTA (computer program) . SIESTA (Spanish Initiative for Electronic Simulations with Thousands of Atoms) is an original method and its computer program implementation, to perform efficient electronic structure calculations and ab initio molecular dynamics simulations of molecules and solids. SIESTA's efficiency stems from the use of strictly localized basis sets and from the implementation of linear-scaling algorithms which can be applied to suitable systems. A very important feature of the code is that its accuracy and cost can be tuned in a wide range, from quick exploratory calculations to highly accurate simulations matching the quality of other approaches, such as plane-wave and all-electron methods.",
            "score": 101.55313873291016
        },
        {
            "docid": "779587_3",
            "document": "Dynamical simulation . In computer science, a program called a physics engine is used to model the behaviors of objects in space. These engines allow simulation of the way bodies of many types are affected by a variety of physical stimuli. They are also used to create Dynamical simulations without having to know anything about physics. Physics engines are used throughout the video game and movie industry, but not all physics engines are alike; They are generally broken into real-time and the high precision but these are not the only options. Most real-time physics engines are inaccurate and yield only the barest approximation of the real world, whereas most high-precision engines are far too slow for use in everyday applications. To understand how these Physics engines are built, a basic understanding of physics is required. Physics engines are based on the actual behaviors of the world as described by classical mechanics. Engines do not typically account for Modern Mechanics (see Theory of relativity and quantum mechanics) because most visualization deals with large bodies moving relatively slowly, but the most complicated engines perform calculations for Modern Mechanics as well as Classical. The models used in Dynamical simulations determine how accurate these simulations are.",
            "score": 101.3846206665039
        },
        {
            "docid": "53751966_4",
            "document": "Michael B\u00fchl . B\u00fchl's group applies the tools of computational quantum chemistry to study a variety of chemical and biochemical systems and their properties, focussing on transition-metal and f-element chemistry, homogeneous and bio-catalysis, and NMR properties. The methods employed are mostly rooted in density-functional theory (DFT), including quantum-mechanical/molecular-mechanical (QM/MM) calculations and first-principles molecular dynamics simulations.",
            "score": 101.15318298339844
        },
        {
            "docid": "44108758_20",
            "document": "Quantum machine learning . Quantum learning theory pursues a mathematical analysis of the quantum generalizations of classical learning models and of the possible speed-ups or other improvements that they may provide. The framework is very similar to that of classical computational learning theory, but the learner in this case is a quantum information processing device, while the data may be either classical or quantum. Quantum learning theory should be contrasted with the quantum-enhanced machine learning discussed above, where the goal was to consider \"specific problems\" and to use quantum protocols to improve the time complexity of classical algorithms for these problems. Although quantum learning theory is still under development, partial results in this direction have been obtained.",
            "score": 101.03142547607422
        },
        {
            "docid": "42297996_2",
            "document": "Protein chemical shift prediction . Protein chemical shift prediction is a branch of biomolecular nuclear magnetic resonance spectroscopy that aims to accurately calculate protein chemical shifts from protein coordinates. Protein chemical shift prediction was first attempted in the late 1960s using semi-empirical methods applied to protein structures solved by X-ray crystallography. Since that time protein chemical shift prediction has evolved to employ much more sophisticated approaches including quantum mechanics, machine learning and empirically derived chemical shift hypersurfaces. The most recently developed methods exhibit remarkable precision and accuracy.",
            "score": 100.77469635009766
        },
        {
            "docid": "44108758_2",
            "document": "Quantum machine learning . Quantum machine learning is an emerging interdisciplinary research area at the intersection of quantum physics and machine learning. One can distinguish four different ways of merging the two parent disciplines. Quantum machine learning algorithms can use the advantages of quantum computation in order to improve classical methods of machine learning, for example by developing efficient implementations of expensive classical algorithms on a quantum computer. On the other hand, one can apply classical methods of machine learning to analyse quantum systems. Most generally, one can consider situations wherein both the learning device and the system under study are fully quantum.",
            "score": 100.63162231445312
        },
        {
            "docid": "49066080_6",
            "document": "Oleg Prezhdo . Fundamental studies span several related areas of quantum, semiclassical and statistical mechanics. Prezhdo explored \"Lie algebraic\" structures to couple quantum and classical mechanics . A simple and powerful extension of classical Hamiltonian dynamics, named \"quantized Hamiltonian dynamics\", was developed to include zero-point energy, tunneling, dephasing and other quantum effects into molecular dynamics simulations . A quantum-classical formalism based on the \"Bohmian\" interpretation of quantum mechanics was proposed . A broad spectrum of techniques for nonadiabatic molecular dynamics was developed and implemented within real-time \"time-dependent density functional theory\" . The techniques include the \"stochastic mean-field\" and \"decoherence induced surface hopping\" approaches, which incorporate quantum decoherence that drastically change timescales of non-equilibrium processes in condensed phase systems and naturally leads to the widely used surface-hopping concept; \"coherence penalty functional\" that deterministically incorporates decoherence into Ehrenfest dynamics; \"global flux surface hopping\" that treats accurately super-exchange and many-particle transitions; and \"Liouville space\" formulations of surface hopping that treat populations and coherences on equal footing, and describe super-exchange and many-particle transitions. In collaboration, Prezhdo proposed many-body measures of hole-particle distributions, \"entropy and entanglement\" for the electronic structure theory and developed a \"statistical mechanical theory\" for electro-optic properties of organic photoactive materials .",
            "score": 100.4123306274414
        }
    ]
}