{
    "q": [
        {
            "docid": "57448874_15",
            "document": "Bayesian model reduction . Bayesian model reduction was initially developed for use in neuroimaging analysis, in the context of modelling brain connectivity, as part of the dynamic causal modelling framework (where it was originally referred to as post-hoc Bayesian model selection). Dynamic causal models (DCMs) are differential equation models of brain dynamics. The experimenter specifies multiple competing models which differ in their priors \u2013 e.g. in the choice of parameters which are fixed at their prior expectation of zero. Having fitted a single 'full' model with all parameters of interest informed by the data, Bayesian model reduction enables the evidence and parameters for competing models to be rapidly computed, in order to test hypotheses. These models can be specified manually by the experimenter, or searched over automatically, in order to 'prune' any redundant parameters which do not contribute to the evidence.",
            "score": 86.06858587265015
        },
        {
            "docid": "22549833_2",
            "document": "Modeling and simulation . Modeling and simulation (M&S) in simple terms is a substitute for physical experimentation, in which computers are used to compute the results of some physical phenomenon. As it is apparent from its name \"Modeling and simulation\" firstly computer is used to build a mathematical model which contains all the parameters of physical model and represent physical model in virtual form then conditions are applied which we want to experiment on physical model, then simulation starts i.e, we leave on computer to compute/calculate the results of those conditions on mathematical model. In this way actual experimentation can be avoided which is costly and time consuming instead of using mathematical knowledge and computer's computation power to solve real world problems cheaply and in time efficient manner. As such, M&S can facilitate understanding a system's behavior without actually testing the system in the real world. For instance, to determine which type of spoiler would improve traction the most while designing a race car, a computer simulation of the car could be used to estimate the effect of different spoiler shapes on the coefficient of friction in a turn. Useful insights about different decisions in the design could be gleaned without actually building the car. In addition, simulation can support experimentation that occurs totally in software, or in human-in-the-loop environments where simulation represents systems or generates data needed to meet experiment objectives. Furthermore, simulation can be used to train persons using a virtual environment that would otherwise be difficult or expensive to produce.",
            "score": 69.53051948547363
        },
        {
            "docid": "55735689_7",
            "document": "Info-metrics . Consider the problem of modeling and inferring the unobserved probability distribution of some \"K\"-dimensional discrete random variable given just the mean (expected value) of that variable. We also know that the probabilities are nonnegative and normalized (i.e., sum up to exactly 1). For all \"K\"\u00a0>\u00a02 the problem is underdetermined. Within the info-metrics framework, the solution is to maximize the entropy of the random variable subject to the two constraints: mean and normalization. This yields the usual maximum entropy solution. The solutions to that problem can be extended and generalized in several ways. First, one can use another entropy instead of Shannon\u2019s entropy. Second, the same approach can be used for continuous random variables, for all types of conditional models (e.g., regression, inequality and nonlinear models), and for many constraints. Third, priors can be incorporated within that framework. Fourth, the same framework can be extended to accommodate greater uncertainty: uncertainty about the observed values and/or uncertainty about the model itself. Last, the same basic framework can be used to develop new models/theories, validate these models using all available information, and test statistical hypotheses about the model.",
            "score": 56.18509066104889
        },
        {
            "docid": "501898_2",
            "document": "Stochastic programming . In the field of mathematical optimization, stochastic programming is a framework for modeling optimization problems that involve uncertainty. Whereas deterministic optimization problems are formulated with known parameters, real world problems almost invariably include some unknown parameters. When the parameters are known only within certain bounds, one approach to tackling such problems is called robust optimization. Here the goal is to find a solution which is feasible for all such data and optimal in some sense. Stochastic programming models are similar in style but take advantage of the fact that probability distributions governing the data are known or can be estimated. The goal here is to find some policy that is feasible for all (or almost all) the possible data instances and maximizes the expectation of some function of the decisions and the random variables. More generally, such models are formulated, solved analytically or numerically, and analyzed in order to provide useful information to a decision-maker.",
            "score": 77.31541085243225
        },
        {
            "docid": "2599760_6",
            "document": "Design science . Later extensions of the Design Science framework detail how design and research problems can be rationally decomposed by means of nested problem solving. It is also explained how the regulative cycle (problem investigation, solution design, design validation, solution implementation, and implementation evaluation) fits in the framework. Peffers et al. developed a model for producing and presenting information systems research, the Design Science Research Process. The Peffers et al. model has been used extensively and Adams provides an example of the process model being applied to create a digital forensic process model.",
            "score": 46.2128746509552
        },
        {
            "docid": "31541804_3",
            "document": "ICME cyberinfrastructure . The ICME cyberinfrastructure provides storage, access, and computational capabilities for an extensive network of manufacturing, design, and life-cycle simulation software. Within this software framework, data is archived, searchable and interactive, offering engineers and scientists a vast database of materials-related information for use in research, multiscale modeling, simulation implementation, and an array of other activities in support of more efficient, less costly product development. Furthermore, the ICME cyberinfrastructure is expected to provide the capability to access and link application codes, including the development of protocols necessary to integrate hierarchical modeling approaches. With an emphasis on computational efficiency, experimental validation of models, and protecting intellectual property, the cyberinfrastructure assimilates 1) process-microstructure-property relations, 2) development of constitutive materials models that accurately predict multiscale material behaviors admitting microstructure/inclusions and history effects, 3) access to shared databases of analytical and experimental data, and 4) material models. As such, it is also crucial to identifying gaps in materials knowledge, which, in turn, guides the development of new materials theories, models, and simulation tools. Such a community-based knowledge foundation ultimately enables materials informatics systems that fuse high fidelity experimental databases with models of physical processes.",
            "score": 56.41303062438965
        },
        {
            "docid": "25816276_12",
            "document": "Sugarscape . Due to the emergent nature of agent-based models (ABMs), it is critical that the population sizes in the simulations match the population sizes of the dynamic systems being modelled. However, the performance of contemporary agent simulation frameworks has been inadequate to handle such large population sizes and parallel computing frameworks designed to run on computing clusters has been limited by available bandwidth. As computing power increases with Moore's law, the size and complexity of simulation frameworks can be expected to increase. The team of R. M. D\u2019Souza, M. Lysenko and K Rahmani from Michigan Technological University used a Sugarscape model to demonstrate the power of Graphics processing units (GPU) in ABM simulations with over 50 updates per second with agent populations exceeding 2 millions.",
            "score": 51.391037464141846
        },
        {
            "docid": "2876834_6",
            "document": "Heath\u2013Jarrow\u2013Morton framework . However, models developed according to the general HJM framework are often non-Markovian and can even have infinite dimensions. A number of researchers have made great contributions to tackle this problem. They show that if the volatility structure of the forward rates satisfy certain conditions, then an HJM model can be expressed entirely by a finite state Markovian system, making it computationally feasible. Examples include a one-factor, two state model (O. Cheyette, \"Term Structure Dynamics and Mortgage Valuation\", \"Journal of Fixed Income,\" 1, 1992; P. Ritchken and L. Sankarasubramanian in \"Volatility Structures of Forward Rates and the Dynamics of Term Structure\", \"Mathematical Finance\", 5, No. 1, Jan 1995), and later multi-factor versions.",
            "score": 41.20767021179199
        },
        {
            "docid": "40158142_9",
            "document": "Nonlinear system identification . Artificial neural networks try loosely to imitate the network of neurons in the brain where computation takes place through a large number of simple processing elements. A typical neural network consists of a number of simple processing units interconnected to form a complex network. Layers of such units are arranged so that data is entered at the input layer and passes through either one or several intermediate layers before reaching the output layer. In supervised learning the network is trained by operating on the difference between the actual output and the desired output of the network, the prediction error, to change the connection strengths between the nodes. By iterating the weights are modified until the output error reaches an acceptable level. This process is called machine learning because the network adjusts the weights so that the output pattern is reproduced. Neural networks have been extensively studied and there are many excellent textbooks devoted to this topic in general, and more focussed textbooks which emphasise control and systems applications. There are two main problem types that can be studied using neural networks: static problems, and dynamic problems. Static problems include pattern recognition, classification, and approximation. Dynamic problems involve lagged variables and are more appropriate for system identification and related applications. Depending on the architecture of the network the training problem can be either nonlinear-in-the-parameters which involves optimisation or linear-in-the-parameters which can be solved using classical approaches. The training algorithms can be categorised into supervised, unsupervised, or reinforcement learning. Neural networks have excellent approximation properties but these are usually based on standard function approximation results using for example the Weierstrass Theorem that applies equally well to polynomials, rational functions, and other well-known models.  Neural networks have been applied extensively to system identification problems which involve nonlinear and dynamic relationships. However, classical neural networks are purely gross static approximating machines. There is no dynamics within the network. Hence when fitting dynamic models all the dynamics arise by allocating lagged inputs and outputs to the input layer of the network. The training procedure then produces the best static approximation that relates the lagged variables assigned to the input nodes to the output. There are more complex network architectures, including recurrent networks, that produce dynamics by introducing increasing orders of lagged variables to the input nodes. But in these cases it is very easy to over specify the lags and this can lead to over fitting and poor generalisation properties.  Neural networks have several advantages; they are conceptually simple, easy to train and to use, have excellent approximation properties, the concept of local and parallel processing is important and this provides integrity and fault tolerant behaviour. The biggest criticism of the classical neural network models is that the models produced are completely opaque and usually cannot be written down or analysed. It is therefore very difficult to know what is causing what, to analyse the model, or to compute dynamic characteristics from the model. Some of these points will not be relevant to all applications but they are for dynamic modelling.",
            "score": 65.55975127220154
        },
        {
            "docid": "1146168_110",
            "document": "Lancet surveys of Iraq War casualties . The authors have also published a follow-up paper in \"Europhysics Letters\" which provides a generic framework than can be used to assess sampling bias in certain social and biological systems. A special case of the framework can be used to derive the results presented in their Journal of Peace Research paper. The authors also investigate the sensitivity of their results to the underlying model parameter values. They reiterate their view that a more precise determination of the model parameters and, hence, the extent of sampling bias, is possible only if the actual micro-level data of the \"Lancet\" study are released.",
            "score": 81.26260948181152
        },
        {
            "docid": "21184512_8",
            "document": "IDEF3 . The third IDEF (IDEF2) was originally intended as a user interface modeling method. However, since the Integrated Computer-Aided Manufacturing (ICAM) Program needed a simulation modeling tool, the resulting IDEF2 was a method for representing the time varying behavior of resources in a manufacturing system, providing a framework for specification of math model based simulations. It was the intent of the methodology program within ICAM to rectify this situation but limitation of funding did not allow this to happen. As a result, the lack of a method which would support the structuring of descriptions of the user view of a system has been a major shortcoming of the IDEF system. The basic problem from a methodology point of view is the need to distinguish between a description of what a system (existing or proposed) is supposed to do and a representative simulation model that will predict what a system will do. The latter was the focus of IDEF2, the former is the focus of IDEF3.",
            "score": 36.788530588150024
        },
        {
            "docid": "423331_17",
            "document": "IDEF . The third IDEF (IDEF2) was originally intended as a user interface modeling method. However, since the Integrated Computer-Aided Manufacturing (ICAM) program needed a simulation modeling tool, the resulting IDEF2 was a method for representing the time varying behavior of resources in a manufacturing system, providing a framework for specification of math model based simulations. It was the intent of the methodology program within ICAM to rectify this situation but limitation of funding did not allow this to happen. As a result, the lack of a method which would support the structuring of descriptions of the user view of a system has been a major shortcoming of the IDEF system. The basic problem from a methodology point of view is the need to distinguish between a description of what a system (existing or proposed) is supposed to do and a representative simulation model that will predict what a system will do. The latter was the focus of IDEF2, the former is the focus of IDEF3.",
            "score": 36.788530588150024
        },
        {
            "docid": "32108014_14",
            "document": "Community Builders Group . Alexiuk, Wiebe and Pizzi utilized results of the aforementioned longitudinal study and argued that \"The theory of emergence has become a useful framework for exploring salient features of dynamical systems. This framework provides insight into hitherto intractable problems in sociology and economics. One such problem is the definition of a mathematical model of homelessness that enables policy evaluation with respect to the holistic wellness of the impacted individuals. Swarm simulations provide numerical and visual results to the researcher allowing both quantitative and intuitive hypothesis testing. This paper defines a basic swarm model of homelessness, details some initial experiments and provides justification for a dynamical systems model.\"",
            "score": 46.153505086898804
        },
        {
            "docid": "11864519_25",
            "document": "Approximate Bayesian computation . Outside of parameter estimation, the ABC framework can be used to compute the posterior probabilities of different candidate models. In such applications, one possibility is to use rejection sampling in a hierarchical manner. First, a model is sampled from the prior distribution for the models. Then, parameters are sampled from the prior distribution assigned to that model. Finally, a simulation is performed as in single-model ABC. The relative acceptance frequencies for the different models now approximate the posterior distribution for these models. Again, computational improvements for ABC in the space of models have been proposed, such as constructing a particle filter in the joint space of models and parameters.",
            "score": 61.98037815093994
        },
        {
            "docid": "931250_2",
            "document": "Screened Coulomb Potentials Implicit Solvent Model . SCP-ISM, or Screened Coulomb Potentials Implicit Solvent Model is a continuum approximation of solvent effects for use in computer simulations of biological macromolecules, such as proteins and nucleic acids, usually within the framework of molecular dynamics. It is based on the classic theory of polar liquids, as developed by Peter Debye and corrected by Lars Onsager to incorporate reaction field effects. The model can be combined with quantum chemical calculations to formally derive a continuum model of solvent effects suitable for computer simulations of small and large molecular systems.",
            "score": 50.39035201072693
        },
        {
            "docid": "36154822_12",
            "document": "Routing (hydrology) . Flood routing is a procedure to determine the time and magnitude of flow (i.e., the flow hydrograph) at a point on a watercourse from known or assumed hydrographs at one or more points upstream. The procedure is specifically known as \"Flood routing\", if the flow is a flood. In order to determine the change in shape of a hydrograph of a flood as it travels through a natural river or artificial channel, different flood simulation techniques can be used. Traditionally, the hydraulic (e.g. \"dynamic\" and \"diffusion\" wave models) and hydrologic (e.g. \"linear\" and \"nonlinear\" \"Muskingum\" models) routing procedures that are well known as \"distributed\" and \"lumped\" ways to hydraulic and hydrologic practitioners, respectively, can be utilized. The hydrologic models need to estimate hydrologic parameters using recorded data in both upstream and downstream sections of rivers and/or by applying robust optimization techniques to solve the one-dimensional conservation of mass and storage-continuity equation. On the other hand, hydraulic models require the gathering of a lot of data related to river geometry and morphology and consume a lot of computer resources in order to solve the equations numerically. However, semi-distributed models such as \"Muskingum\u2013Cunge family\" procedures are also available. Simple physically concepts and common river characteristic consist of channel geometry, reach length, roughness coefficient, and slope are used to estimate the model parameters without complex and expensive numerical solutions. In general, based on the available field data and goals of a project, one of these approaches is utilized for the simulation of flooding in rivers and channels.",
            "score": 67.92201220989227
        },
        {
            "docid": "8501_21",
            "document": "Distributed computing . Theoretical computer science seeks to understand which computational problems can be solved by using a computer (computability theory) and how efficiently (computational complexity theory). Traditionally, it is said that a problem can be solved by using a computer if we can design an algorithm that produces a correct solution for any given instance. Such an algorithm can be implemented as a computer program that runs on a general-purpose computer: the program reads a problem instance from input, performs some computation, and produces the solution as output. Formalisms such as random access machines or universal Turing machines can be used as abstract models of a sequential general-purpose computer executing such an algorithm.",
            "score": 28.4626247882843
        },
        {
            "docid": "355240_14",
            "document": "Cognitive model . Behavioral dynamics have been applied to locomotive behavior. Modeling locomotion with behavioral dynamics demonstrates that adaptive behaviors could arise from the interactions of an agent and the environment. According to this framework, adaptive behaviors can be captured by two levels of analysis. At the first level of perception and action, an agent and an environment can be conceptualized as a pair of dynamical systems coupled together by the forces the agent applies to the environment and by the structured information provided by the environment. Thus, behavioral dynamics emerge from the agent-environment interaction. At the second level of time evolution, behavior can be expressed as a dynamical system represented as a vector field. In this vector field, attractors reflect stable behavioral solutions, where as bifurcations reflect changes in behavior. In contrast to previous work on central pattern generators, this framework suggests that stable behavioral patterns are an emergent, self-organizing property of the agent-environment system rather than determined by the structure of either the agent or the environment.",
            "score": 53.162323236465454
        },
        {
            "docid": "2506529_35",
            "document": "Cellular neural network . CNN processors are being used to understand systems that can be modeled using simple, coupled units, such as living cells, biological networks, physiological systems, and ecosystems. The CNN architecture captures some of the dynamics often seen in nature and is simple enough to analyze and conduct experiments. They are also being used for stochastic simulation techniques, which allow scientists to explore spin problems, population dynamics, lattice-based gas models, percolation, and other phenomena. Other simulation applications include heat transfer, mechanical vibrating systems, protein production, Josephson Transmission Line (JTL) problems, seismic wave propagation, and geothermal structures. Instances of 3D (Three Dimensional) CNN have been used to prove known complex shapes are emergent phenomena in complex systems, establishing a link between art, dynamical systems and VLSI technology. CNN processors have been used to research a variety of mathematical concepts, such as researching non-equilibrium systems, constructing non-linear systems of arbitrary complexity using a collection of simple, well-understood dynamic systems, studying emergent chaotic dynamics, generating chaotic signals, and in general discovering new dynamic behavior. They are often used in researching systemics, a trandisiplinary, scientific field that studies natural systems. The goal of systemics researchers is to develop a conceptual and mathematical framework necessary to analyze, model, and understand systems, including, but not limited to, atomic, mechanical, molecular, chemical, biological, ecological, social and economic systems. Topics explored are emergence, collective behavior, local activity and its impact on global behavior, and quantifying the complexity of an approximately spatial and topologically invariant system . Although another measure of complexity may not arouse enthusiasm (Seth Lloyd, a professor from Massachusetts Institute of Technology (MIT), has identified 32 different definitions of complexity), it can potentially be mathematically advantageous when analyzing systems such as economic and social systems.",
            "score": 59.07380247116089
        },
        {
            "docid": "10897286_4",
            "document": "Hans B. Pacejka . The Pacejka tire models are widely used in professional vehicle dynamics simulations, and racing car games, as they are reasonably accurate, easy to program, and solve quickly. A problem with Pacejka's model is that when implemented into computer code, it doesn't work for low speeds (from around the pit-entry speed), because a velocity term in the denominator makes the formula diverge. An alternative to Pacejka tire models are brush tire models, which can be analytically derived, although empirical curve fitting is still required for good correlation, and they tend to be less accurate than the MF models.",
            "score": 29.419989585876465
        },
        {
            "docid": "48403381_23",
            "document": "Dragon King Theory . Given a model and data, one can obtain a statistical model estimate. This model estimate can then be used to compute interesting quantities such as the conditional probability of the occurrence of a dragon king event in a future time interval, and the most probable occurrence time. When doing statistical modeling of extremes, and using complex or nonlinear dynamic models, there is bound to be substantial uncertainty. Thus, one should be diligent in uncertainty quantification: not only considering the randomness present in the fitted stochastic model, but also the uncertainty of its estimated parameters (e.g., with Bayesian techniques or by first simulating parameters and then simulating from the model with those parameters), and the uncertainty in model selection (e.g., by considering an ensemble of different models).",
            "score": 77.61078453063965
        },
        {
            "docid": "41499988_5",
            "document": "BioMA . The goal of this framework is to rapidly bridge from prototypes to operational applications, enabling running and comparing different modelling solutions. A key aspect of the framework is the transparency which allows for quality evaluation of outputs in the various steps of the modelling workflow. The framework is based on framework-independent components, both for the modelling solutions and the graphical user's interfaces. The goal is not only to provide a framework for model development and operational use but also, and of no lesser importance, to provide a loose collection of objecst re-usable either standalone or in different frameworks. The software is developed using Microsoft C# language in the .NET framework.",
            "score": 38.29304575920105
        },
        {
            "docid": "57256998_2",
            "document": "Gekko (optimization software) . The GEKKO Python package solves mixed-integer and differential algebraic equations with nonlinear programming solvers (IPOPT, APOPT, BPOPT, SNOPT, MINOS). Modes of operation include data reconciliation, real-time optimization, dynamic simulation, and nonlinear model predictive control. In addition, the package solves Linear programming (LP), Quadratic programming (QP), Quadratically constrained quadratic program (QCQP), Nonlinear programming (NLP), Mixed integer programming (MIP), and Mixed integer linear programming (MILP). GEKKO is available in Python and installed with pip from PyPI of the Python Software Foundation. GEKKO works on all platforms (Windows, MacOS, Linux, ARM processors) and with Python 2.7 and 3+. By default, the problem is sent to a public server where the solution is computed and returned to Python. There is Windows option to solve without an Internet connection. GEKKO is an extension of the APMonitor Optimization Suite but has integrated the modeling and solution visualization directly within Python. A mathematical model is expressed in terms of variables and equations such as the Hock & Schittkowski Benchmark Problem #71 used to test the performance of nonlinear programming solvers. This particular optimization problem has an objective function formula_1 and subject to the inequality constraint formula_2 and equality constraint formula_3. The four variables must be between a lower bound of 1 and an upper bound of 5. The initial guess values are formula_4. This optimization problem is solved with GEKKO as shown below.",
            "score": 31.785187482833862
        },
        {
            "docid": "37974990_3",
            "document": "Rogemar Mamon . Mamon is known for his contributions to the developments and applications of regime-switching framework useful in economic, financial and actuarial modeling. Majority of his works promote regime-switching paradigms modulated by either discrete- or continuous-time hidden Markov models (HMM). A recurrent theme of his research is dynamic parameter estimation via HMM filtering recursions. He also made contributions in the areas of derivative pricing, asset allocation, risk measurement, filtering to remove noise from data as well as inverse problems in quantitative finance. He was the lead editor of the handbook \"Hidden Markov Models in Finance\", published by Springer.",
            "score": 62.582608222961426
        },
        {
            "docid": "11042797_2",
            "document": "Dynamic simulation . Dynamic simulation (or dynamic system simulation) is the use of a computer program to model the time varying behavior of a system. The systems are typically described by ordinary differential equations or partial differential equations. As mathematical models incorporate real-world constraints, like gear backlash and rebound from a hard stop, equations become nonlinear. This requires numerical methods to solve the equations. A numerical simulation is done by stepping through a time interval and calculating the integral of the derivatives by approximating the area under the derivative curves. Some methods use a fixed step through the interval, and others use an adaptive step that can shrink or grow automatically to maintain an acceptable error tolerance. Some methods can use different time steps in different parts of the simulation model. Industrial uses of dynamic simulation are many and range from nuclear power, steam turbines, 6 degree of freedom vehicle modeling, electric motors, econometric models, biological systems, robot arms, mass spring dampers, hydraulic systems, and drug dose migration through the human body to name a few. These models can often be run in real time to give a virtual response close to the actual system. This is useful in process control and mechatronic systems for tuning the automatic control systems before they are connected to the real system, or for human training before they control the real system. Simulation is also used in computer games and animation and can be accelerated by using a physics engine, the technology used in many powerful computer graphics software programs, like 3ds Max, Maya, Lightwave, and many others to simulate physical characteristics. In computer animation, things like hair, cloth, liquid, fire, and particles can be easily modeled, while the human animator animates simpler objects. Computer-based dynamic animation was first used at a very simple level in the 1989 Pixar short film \"Knick Knack\" to move the fake snow in the snowglobe and pebbles in a fish tank.",
            "score": 46.690990686416626
        },
        {
            "docid": "41499988_2",
            "document": "BioMA . Modelling frameworks are used in modelling and simulation and can consist of a software infrastructure to develop and run mathematical models. They have provided a substantial step forward in the area of biophysical modelling with respect to monolithic implementations. The separation of algorithms from data, the reusability of I/O procedures and integration services, and the isolation of modelling solutions in discrete units has brought a solid advantage in the development of simulation systems. Modelling frameworks for agriculture have evolved over time, with different approaches and targets",
            "score": 52.26844811439514
        },
        {
            "docid": "17363189_3",
            "document": "Socially relevant computing . SRC emphasizes the use of computation for solving problems of personal and societal interest to students. It offers opportunities to demonstrate that computer science is a mainstream endeavor and that it offers conceptual and technological tools for solving meaningful, real-world problems. Courses in this new framework help students identify and model tasks, and design and implement computational solutions that show deep understanding of their embedding in the real world. At the very least, SRC offers interesting examples to illustrate foundational concepts in computer science. By emphasizing problem-solving, and by giving students practice in recognizing needs and engineering solutions to them via computation, SRC at its finest promises to create a more entrepreneurial, as well as a more broadly educated computer scientist.",
            "score": 27.47292160987854
        },
        {
            "docid": "43966823_44",
            "document": "Multi-state modeling of biomolecules . It is easy to imagine a biological system where some components are complex multi-state molecules, whereas others have few possible states (or even just one) and exist in large numbers. A hybrid approach has been proposed to model such systems: Within the Hybrid Particle/Population (HPP) framework, the user can specify a rule-based model, but can designate some species to be treated as populations (rather than particles) in the subsequent simulation. This method combines the computational advantages of particle-based modeling for multi-state systems with relatively low molecule numbers and of population-based modeling for systems with high molecule numbers and a small number of possible states. Specification of HPP models is supported by BioNetGen, and simulations can be performed with NFSim.",
            "score": 47.0571985244751
        },
        {
            "docid": "2815048_3",
            "document": "Computational model . The system under study is often a complex nonlinear system for which simple, intuitive analytical solutions are not readily available. Rather than deriving a mathematical analytical solution to the problem, experimentation with the model is done by adjusting the parameters of the system in the computer, and studying the differences in the outcome of the experiments. Operation theories of the model can be derived/deduced from these computational experiments.",
            "score": 59.98584294319153
        },
        {
            "docid": "21225939_2",
            "document": "Simul8 . SIMUL8 simulation software is a product of the SIMUL8 Corporation used for simulating systems that involve processing of discrete entities at discrete times. This program is a tool for planning, design, optimization and reengineering of real production, manufacturing, logistic or service provision systems. SIMUL8 allows its user to create a computer model, which takes into account real life constraints, capacities, failure rates, shift patterns, and other factors affecting the total performance and efficiency of production. Through this model it is possible to test real scenarios in a virtual environment, for example simulate planned function and load of the system, change parameters affecting system performance, carry out extreme-load tests, verify by experiments the proposed solutions and select the optimal solution. A common feature of problems solved in SIMUL8 is that they are concerned with cost, time and inventory.",
            "score": 41.2207088470459
        },
        {
            "docid": "49648894_29",
            "document": "Simulation-based optimization . Simulation based optimization has some limitations, such as the difficulty of creating a model that imitates the dynamic behavior of a system in a way that is considered good enough for its representation. Another problem is complexity in the determining uncontrollable parameters of both real-world system and simulation. Moreover, only a statistical estimation of real values can be obtained. It is not easy to determine the objective function, since it is a result of measurements, which can be harmful for the solutions.",
            "score": 54.26313638687134
        },
        {
            "docid": "34010466_2",
            "document": "Biological applications of bifurcation theory . Biological applications of bifurcation theory provide a framework for understanding the behavior of biological networks modeled as dynamical systems. In the context of a biological system, bifurcation theory describes how small changes in an input parameter can cause a bifurcation or qualitative change in the behavior of the system. The ability to make dramatic change in system output is often essential to organism function, and bifurcations are therefore ubiquitous in biological networks such as the switches of the cell cycle.",
            "score": 69.41539549827576
        }
    ],
    "r": [
        {
            "docid": "32104707_15",
            "document": "Cellular noise . The problem of inferring the values of parameters in stochastic models (parametric inference) for biological processes, which are typically characterised by sparse and noisy experimental data, is an active field of research, with methods including Bayesian MCMC and approximate Bayesian computation proving adaptable and robust . Regarding the two-state model, a moment-based method was described for parameters inference from mRNAs distributions.",
            "score": 90.0457534790039
        },
        {
            "docid": "28250590_2",
            "document": "Large-scale macroeconometric model . Following the development of Keynesian economics, applied economics began developing forecasting models based on economic data including national income and product accounting data. In contrast with typical textbook models, these large-scale macroeconometric models used large amounts of data and based forecasts on past correlations instead of theoretical relations. These models estimated the relations between different macroeconomic variables using regression analysis on time series data. These models grew to include hundreds or thousands of equations describing the evolution of hundreds or thousands of prices and quantities over time, making computers essential for their solution. While the choice of which variables to include in each equation was partly guided by economic theory (for example, including past income as a determinant of consumption, as suggested by the theory of adaptive expectations), variable inclusion was mostly determined on purely empirical grounds. Large-scale macroeconometric model consists of systems of dynamic equations of the economy with the estimation of parameters using time-series data on a quarterly to yearly basis.",
            "score": 86.79510498046875
        },
        {
            "docid": "57448874_15",
            "document": "Bayesian model reduction . Bayesian model reduction was initially developed for use in neuroimaging analysis, in the context of modelling brain connectivity, as part of the dynamic causal modelling framework (where it was originally referred to as post-hoc Bayesian model selection). Dynamic causal models (DCMs) are differential equation models of brain dynamics. The experimenter specifies multiple competing models which differ in their priors \u2013 e.g. in the choice of parameters which are fixed at their prior expectation of zero. Having fitted a single 'full' model with all parameters of interest informed by the data, Bayesian model reduction enables the evidence and parameters for competing models to be rapidly computed, in order to test hypotheses. These models can be specified manually by the experimenter, or searched over automatically, in order to 'prune' any redundant parameters which do not contribute to the evidence.",
            "score": 86.06858825683594
        },
        {
            "docid": "203956_11",
            "document": "Inverse problem . The transformation from data to model parameters (or vice versa) is a result of the interaction of a physical system with the object that we wish to infer properties about. In other words, the transformation is the physics that relates the physical quantity (i.e., the model parameters) to the observed data.",
            "score": 85.53429412841797
        },
        {
            "docid": "26849824_27",
            "document": "Least squares support vector machine . A general Bayesian evidence framework was developed by MacKay, and MacKay has used it to the problem of regression, forward neural network and classification network. Provided data set formula_65, a model formula_66 with parameter vector formula_35 and a so-called hyperparameter or regularization parameter formula_68, Bayesian inference is constructed with 3 levels of inference:",
            "score": 83.1606216430664
        },
        {
            "docid": "26459934_21",
            "document": "Seismic noise . The obtained results cannot directly give information on the physical parameters (S-wave velocity, structural stiffness...) of the ground structures or civil engineering structures. Therefore, models are needed to compute these products (dispersion curve, modal shapes...) that could be compared with the experimental data. Computing a lot of models to find which agree with the data is solving the Inverse problem. The main issue of inversion is to well explore the parameter space with a limited number of computations of the model. However, the model fitting best the data is not the most interesting because parameter compensation, uncertainties on both models and data make many models with different input parameters as good compared to the data. The sensitivity of the parameters may also be very different depending on the model used. The inversion process is generally the weak point of these ambient vibration methods.",
            "score": 81.73351287841797
        },
        {
            "docid": "416612_5",
            "document": "Cross-validation (statistics) . Suppose we have a model with one or more unknown parameters, and a data set to which the model can be fit (the training data set). The fitting process optimizes the model parameters to make the model fit the training data as well as possible. If we then take an independent sample of validation data from the same population as the training data, it will generally turn out that the model does not fit the validation data as well as it fits the training data. The size of this difference is likely to be large especially when the size of the training data set is small, or when the number of parameters in the model is large. Cross-validation is a way to estimate the size of this effect.",
            "score": 81.51853942871094
        },
        {
            "docid": "620083_46",
            "document": "Sensitivity analysis . Kinetic parameters are frequently determined from experimental data via nonlinear estimation. Sensitivity analysis can be used for optimal experimental design, e.g. determining initial conditions, measurement positions, and sampling time, to generate informative data which are critical to estimation accuracy. A great number of parameters in a complex model can be candidates for estimation but not all are estimable. Sensitivity analysis can be used to identify the influential parameters which can be determined from available data while screening out the unimportant ones. Sensitivity analysis can also be used to identify the redundant species and reactions allowing model reduction.",
            "score": 81.3964614868164
        },
        {
            "docid": "1146168_110",
            "document": "Lancet surveys of Iraq War casualties . The authors have also published a follow-up paper in \"Europhysics Letters\" which provides a generic framework than can be used to assess sampling bias in certain social and biological systems. A special case of the framework can be used to derive the results presented in their Journal of Peace Research paper. The authors also investigate the sensitivity of their results to the underlying model parameter values. They reiterate their view that a more precise determination of the model parameters and, hence, the extent of sampling bias, is possible only if the actual micro-level data of the \"Lancet\" study are released.",
            "score": 81.26261138916016
        },
        {
            "docid": "11864519_46",
            "document": "Approximate Bayesian computation . A number of heuristic approaches to the quality control of ABC have been proposed, such as the quantification of the fraction of parameter variance explained by the summary statistics. A common class of methods aims at assessing whether or not the inference yields valid results, regardless of the actually observed data. For instance, given a set of parameter values, which are typically drawn from the prior or the posterior distributions for a model, one can generate a large number of artificial datasets. In this way, the quality and robustness of ABC inference can be assessed in a controlled setting, by gauging how well the chosen ABC inference method recovers the true parameter values, and also models if multiple structurally different models are considered simultaneously.",
            "score": 81.01870727539062
        },
        {
            "docid": "43104837_2",
            "document": "Indirect Inference . Indirect inference is a simulation-based method for estimating the parameters of economic models. It is a computational method for determining acceptable macroeconomic model parameters in circumstances where the available data is too voluminous or unsuitable for formal modeling.",
            "score": 80.3712158203125
        },
        {
            "docid": "44968_2",
            "document": "Likelihood function . In frequentist inference, a likelihood function (often simply the likelihood) is a function of the parameters of a statistical model, given specific observed data. Likelihood functions play a key role in frequentist inference, especially methods of estimating a parameter from a set of statistics. In informal contexts, \"likelihood\" is often used as a synonym for \"probability\". In mathematical statistics, the two terms have different meanings. \"Probability\" in this mathematical context describes the plausibility of a random outcome, given a model parameter value, without reference to any observed data. \"Likelihood\" describes the plausibility of a model parameter value, given specific observed data.",
            "score": 79.8136215209961
        },
        {
            "docid": "38889813_30",
            "document": "Viral phylodynamics . As discussed above, it is possible to directly infer parameters of simple compartmental epidemiological models, such as SIR models, from sequence data by looking at genealogical patterns. Additionally, general patterns of geographic movement can be inferred from sequence data, but these inferences do not involve an explicit model of transmission dynamics between infected individuals. For more complicated epidemiological models, such as those involving cross-immunity, age structure of host contact rates, seasonality, or multiple host populations with different life history traits, it is often impossible to analytically predict genealogical patterns from epidemiological parameters. As such, the traditional statistical inference machinery will not work with these more complicated models, and in this case, it is common to instead use a forward simulation-based approach.",
            "score": 79.7274169921875
        },
        {
            "docid": "47628477_3",
            "document": "Identifiability analysis . Assuming the model is defined and the regression analysis or any other model fitting could be performed to obtain the model parameters values that minimize difference between the modeled and experimental data. The goodness of fit, which represents the minimal difference between experimental and modeled data in a particular measure, does not reveal how reliable the parameter estimates are and it is not the sufficient criteria to prove the model was chosen correctly either. For example, if the experimental data was noisy or just insufficient amount of data points was processed, substitution of best fitted parameter values by orders of magnitude will not significantly influence the quality of fit. To address this issues the identifiability analysis could be applied as an important step to ensure correct choice of model, and sufficient amount of experimental data. The purpose of this analysis is either a quantified proof of correct model choice and integrality of experimental data acquired or such analysis can serve as an instrument for the detection of non-identifiable and sloppy parameters, helping planning the experiments and in building and improvement of the model at the early stages.",
            "score": 79.56185913085938
        },
        {
            "docid": "10571004_4",
            "document": "Biological network inference . There is great interest in network medicine for the modelling biological systems. This article focuses on a necessary prerequisite to dynamic modeling of a network: inference of the topology, that is, prediction of the \"wiring diagram\" of the network. More specifically, we focus here on inference of biological network structure using the growing sets of high-throughput expression data for genes, proteins, and metabolites. Briefly, methods using high-throughput data for inference of regulatory networks rely on searching for patterns of partial correlation or conditional probabilities that indicate causal influence. Such patterns of partial correlations found in the high-throughput data, possibly combined with other supplemental data on the genes or proteins in the proposed networks, or combined with other information on the organism, form the basis upon which such algorithms work. Such algorithms can be of use in inferring the topology of any network where the change in state of one node can affect the state of other nodes.",
            "score": 78.96039581298828
        },
        {
            "docid": "25675405_2",
            "document": "Self-Similarity of Network Data Analysis . In computer networks, self-similarity is a feature of network data transfer dynamics. When modeling network data dynamics the traditional time series models, such as an autoregressive moving average model (ARMA(p, q)), are not appropriate. This is because these models only provide a finite number of parameters in the model and thus interaction in a finite time window, but the network data usually have a long-range dependent temporal structure. A self-similar process is one way of modeling network data dynamics with such a long range correlation. This article defines and describes network data transfer dynamics in the context of a self-similar process. Properties of the process are shown and methods are given for graphing and estimating parameters modeling the self-similarity of network data.",
            "score": 78.87937927246094
        },
        {
            "docid": "40158142_4",
            "document": "Nonlinear system identification . There are four steps to be followed for system identification: data gathering, model postulate, parameter identification and model validation. Data gathering is considered as the first and essential part in identification terminology, used as the input for the model which is prepared later. It consists of selecting an appropriate data set, pre-processing and processing. It involves the implementation of the known algorithms together with the transcription of flight tapes, data storage and data management, calibration, processing, analysis and presentation. Moreover, model validation is necessary to gain confidence in, or reject, a particular model. In particular, the parameter estimation and the model validation are integral parts of the system identification. Validation refers to the process of confirming the conceptual model and demonstrating an adequate correspondence between the computational results of the model and the actual data.",
            "score": 78.64673614501953
        },
        {
            "docid": "20590_27",
            "document": "Mathematical model . Usually the easiest part of model evaluation is checking whether a model fits experimental measurements or other empirical data. In models with parameters, a common approach to test this fit is to split the data into two disjoint subsets: training data and verification data. The training data are used to estimate the model parameters. An accurate model will closely match the verification data even though these data were not used to set the model's parameters. This practice is referred to as cross-validation in statistics.",
            "score": 78.5586166381836
        },
        {
            "docid": "1706860_2",
            "document": "Bayesian experimental design . Bayesian experimental design provides a general probability-theoretical framework from which other theories on experimental design can be derived. It is based on Bayesian inference to interpret the observations/data acquired during the experiment. This allows accounting for both any prior knowledge on the parameters to be determined as well as uncertainties in observations.",
            "score": 77.79439544677734
        },
        {
            "docid": "331325_5",
            "document": "Minimum description length . MDL is a theory of inductive and statistical inference that starts out with this idea: all statistical learning is about finding regularities in data, and the best hypothesis to describe the regularities in data is also the one that is able to compress the data most. Like other statistical methods, it can be used for learning the parameters of a model using some data. Usually though, standard statistical methods assume that the general form of a model is fixed. MDL's main strength is that it can also be used for selecting the general form of a model and its parameters. The quantity of interest (sometimes just a model, sometimes just parameters, sometimes both at the same time) is called a hypothesis. The basic idea is then to consider the (lossless) two-stage code that encodes data formula_1 by first encoding a hypothesis formula_2 in the set of considered hypotheses formula_3 and then coding formula_1 \"with the help of\" formula_2; in the simplest context this just means \"encoding the deviations of the data from the predictions made by formula_2:",
            "score": 77.64070129394531
        },
        {
            "docid": "48403381_23",
            "document": "Dragon King Theory . Given a model and data, one can obtain a statistical model estimate. This model estimate can then be used to compute interesting quantities such as the conditional probability of the occurrence of a dragon king event in a future time interval, and the most probable occurrence time. When doing statistical modeling of extremes, and using complex or nonlinear dynamic models, there is bound to be substantial uncertainty. Thus, one should be diligent in uncertainty quantification: not only considering the randomness present in the fitted stochastic model, but also the uncertainty of its estimated parameters (e.g., with Bayesian techniques or by first simulating parameters and then simulating from the model with those parameters), and the uncertainty in model selection (e.g., by considering an ensemble of different models).",
            "score": 77.61078643798828
        },
        {
            "docid": "2458875_2",
            "document": "Data assimilation . Data assimilation is a mathematical discipline that seeks to optimally combine theory (usually in the form of a numerical model) with observations. There may be a number of different goals sought, for example - to determine the optimal state estimate of a system, to determine initial conditions for a numerical forecast model, to interpolate sparse observation data using (e.g. physical) knowledge of the system being observed, to train numerical model parameters based on observed data. Depending on the goal, different solution methods may be used. Data assimilation is distinguished from other forms of machine learning, image analysis, and statistical methods in that it utilizes a dynamical model of the system being analyzed.",
            "score": 77.39866638183594
        },
        {
            "docid": "501898_2",
            "document": "Stochastic programming . In the field of mathematical optimization, stochastic programming is a framework for modeling optimization problems that involve uncertainty. Whereas deterministic optimization problems are formulated with known parameters, real world problems almost invariably include some unknown parameters. When the parameters are known only within certain bounds, one approach to tackling such problems is called robust optimization. Here the goal is to find a solution which is feasible for all such data and optimal in some sense. Stochastic programming models are similar in style but take advantage of the fact that probability distributions governing the data are known or can be estimated. The goal here is to find some policy that is feasible for all (or almost all) the possible data instances and maximizes the expectation of some function of the decisions and the random variables. More generally, such models are formulated, solved analytically or numerically, and analyzed in order to provide useful information to a decision-maker.",
            "score": 77.31541442871094
        },
        {
            "docid": "19208664_25",
            "document": "Neural modeling fields . Finding patterns below noise can be an exceedingly complex problem. If an exact pattern shape is not known and depends on unknown parameters, these parameters should be found by fitting the pattern model to the data. However, when the locations and orientations of patterns are not known, it is not clear which subset of the data points should be selected for fitting. A standard approach for solving this kind of problem is multiple hypothesis testing (Singer et al. 1974). Since all combinations of subsets and models are exhaustively searched, this method faces the problem of combinatorial complexity. In the current example, noisy \u2018smile\u2019 and \u2018frown\u2019 patterns are sought. They are shown in Fig.1a without noise, and in Fig.1b with the noise, as actually measured. The true number of patterns is 3, which is not known. Therefore, at least 4 patterns should be fit to the data, to decide that 3 patterns fit best. The image size in this example is 100x100 = 10,000 points. If one attempts to fit 4 models to all subsets of 10,000 data points, computation of complexity, M ~ 10. An alternative computation by searching through the parameter space, yields lower complexity: each pattern is characterized by a 3-parameter parabolic shape. Fitting 4x3=12 parameters to 100x100 grid by a brute-force testing would take about 10 to 10 operations, still a prohibitive computational complexity. To apply NMF and dynamic logic to this problem one needs to develop parametric adaptive models of expected patterns. The models and conditional partial similarities for this case are described in details in: a uniform model for noise, Gaussian blobs for highly-fuzzy, poorly resolved patterns, and parabolic models for \u2018smiles\u2019 and \u2018frowns\u2019. The number of computer operations in this example was about 10. Thus, a problem that was not solvable due to combinatorial complexity becomes solvable using dynamic logic.",
            "score": 77.1064453125
        },
        {
            "docid": "15855253_12",
            "document": "Quantification of margins and uncertainties . Verification and validation (V\u00a0&\u00a0V) of a model is closely interrelated with QMU. Verification is broadly acknowledged as the process of determining if a model was built correctly; validation activities focus on determining if the correct model was built. V&V against available experimental test data is an important aspect of accurately characterizing the overall uncertainty of the system response variables. V&V seeks to make maximum use of component and subsystem-level experimental test data to accurately characterize model input parameters and the physics-based models associated with particular sub-elements of the system. The use of QMU in the simulation process helps to ensure that the stochastic nature of the input variables (due to both aleatory and epistemic uncertainties) as well as the underlying uncertainty in the model are properly accounted for when determining the simulation runs required to establish model credibility prior to accreditation.",
            "score": 76.56257629394531
        },
        {
            "docid": "31745436_2",
            "document": "Iterated filtering . Iterated filtering algorithms are a tool for maximum likelihood inference on partially observed dynamical systems. Stochastic perturbations to the unknown parameters are used to explore the parameter space. Applying sequential Monte Carlo (the particle filter) to this extended model results in the selection of the parameter values that are more consistent with the data. Appropriately constructed procedures, iterating with successively diminished perturbations, converge to the maximum likelihood estimate. Iterated filtering methods have so far been used most extensively to study infectious disease transmission dynamics. Case studies include cholera, Ebola virus, influenza, malaria, HIV, pertussis, poliovirus and measles. Other areas which have been proposed to be suitable for these methods include ecological dynamics and finance.",
            "score": 76.1830062866211
        },
        {
            "docid": "18484937_15",
            "document": "First-hitting-time model . The time scale of the stochastic process may be calendar or clock time or some more operational measure of time progression, such as mileage of a car, accumulated wear and tear on a machine component or accumulated exposure to toxic fumes. In many applications, the stochastic process describing the system state is latent or unobservable and its properties must be inferred indirectly from censored time-to-event data and/or readings taken over time on correlated processes, such as marker processes. The word \u2018regression\u2019 in threshold regression refers to first-hitting-time models in which one or more regression structures are inserted into the model in order to connect model parameters to explanatory variables or covariates. The parameters given regression structures may be parameters of the stochastic process, the threshold state and/or the time scale itself.",
            "score": 75.89512634277344
        },
        {
            "docid": "44968_3",
            "document": "Likelihood function . In Bayesian inference, although one can speak about the likelihood of any proposition or random variable given another random variable: for example the likelihood of a parameter value or of a statistical model (see marginal likelihood), given specified data or other evidence, the likelihood function remains the same entity, with the additional interpretations of (i) a conditional density of the data given the parameter (since it is then a random variable) and (ii) a measure or amount of information brought by the data about the parameter value or even the model. Due to the introduction of a probability structure on the parameter space or on the collection of models, it is a possible occurrence that a parameter value or a statistical model have a large likelihood value for a given specified observed data, and yet have a low \"probability\", or vice versa. This is often the case in medical contexts. Following Bayes Rule, the likelihood when seen as a conditional density can be multiplied by the prior probability density of the parameter and then normalized, to give a posterior probability density.",
            "score": 75.20622253417969
        },
        {
            "docid": "34518735_12",
            "document": "Geochemical modeling . Various sources can contribute to a range of simulation results. The range of the simulation results is defined as model uncertainty. One of the most important sources not possible to quantify is the conceptual model, which is developed and defined by the modeller. Further sources are the parameterization of the model regarding the hydraulic (only when simulating transport) and mineralogical properties. The parameters used for the geochemical simulations can also contribute to model uncertainty. These are the applied thermodynamic database and the parameters for the kinetic minerals dissolution. Differences in the thermodynamic data (i.e. equilibrium constants, parameters for temperature correction, activity equations and coefficients) can result in large uncertainties. Furthermore, the large spans of experimentally derived rate constants for minerals dissolution rate laws can cause large variations in simulation results. Despite this is well-known, uncertainties are not frequently considered when conducting geochemical modelling.",
            "score": 75.08866882324219
        },
        {
            "docid": "55614105_3",
            "document": "BioCompute Object . One of the biggest challenges in bioinformatics is documenting and sharing scientific workflows in a such a way that the computation and its results can be peer-reviewed or reliably reproduced. Bioinformatic pipelines typically use multiple pieces of software, each of which typically has multiple versions available, multiple input parameters, multiple outputs, and possibly platform-specific configurations. As with experimental parameters in a laboratory protocol, small changes in computational parameters may have a large impact on the scientific validity of the results. The BioCompute Framework provides an object oriented design from which a BCO that contains details of a pipeline and how it was used can be constructed, digitally signed, and shared. The BioCompute concept was originally developed to satisfy FDA regulatory research and review needs for evaluation, validation, and verification of genomics data. However, the Biocompute Framework follows FAIR Data Principles and can be used broadly to provide communication and interoperability between different platforms, industries, scientists and regulators",
            "score": 74.77682495117188
        },
        {
            "docid": "419259_76",
            "document": "Survival analysis . Survival models can be usefully viewed as ordinary regression models in which the response variable is time. However, computing the likelihood function (needed for fitting parameters or making other kinds of inferences) is complicated by the censoring. The likelihood function for a survival model, in the presence of censored data, is formulated as follows. By definition the likelihood function is the conditional probability of the data given the parameters of the model. It is customary to assume that the data are independent given the parameters. Then the likelihood function is the product of the likelihood of each datum. It is convenient to partition the data into four categories: uncensored, left censored, right censored, and interval censored. These are denoted \"unc.\", \"l.c.\", \"r.c.\", and \"i.c.\" in the equation below.",
            "score": 74.54066467285156
        },
        {
            "docid": "140806_4",
            "document": "Maximum likelihood estimation . From the point of view of Bayesian inference, MLE is a special case of maximum a posteriori estimation (MAP) that assumes a uniform prior distribution of the parameters. In frequentist inference, MLE is one of several methods to get estimates of parameters without using prior distributions. Priors are avoided by not making probability statements about the parameters, but only about their estimates, whose properties are fully defined by the observations and the statistical model. The method of maximum likelihood is based on the likelihood function, formula_1. We are given a statistical model, i.e. a family of distributions formula_2, where formula_3 denotes the (possibly multi-dimensional) parameter for the model. The method of maximum likelihood finds the values of the model parameter, formula_3, that maximize the likelihood function, formula_1. Intuitively, this selects the parameter values that make the data most probable.",
            "score": 74.49447631835938
        }
    ]
}