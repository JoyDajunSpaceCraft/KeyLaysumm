{
    "q": [
        {
            "docid": "505717_72",
            "document": "Image segmentation . Pulse-coupled neural networks (PCNNs) are neural models proposed by modeling a cat\u2019s visual cortex and developed for high-performance biomimetic image processing. In 1989, Eckhorn introduced a neural model to emulate the mechanism of a cat\u2019s visual cortex. The Eckhorn model provided a simple and effective tool for studying the visual cortex of small mammals, and was soon recognized as having significant application potential in image processing. In 1994, the Eckhorn model was adapted to be an image processing algorithm by Johnson, who termed this algorithm Pulse-Coupled Neural Network. Over the past decade, PCNNs have been utilized for a variety of image processing applications, including: image segmentation, feature generation, face extraction, motion detection, region growing, noise reduction, and so on. A PCNN is a two-dimensional neural network. Each neuron in the network corresponds to one pixel in an input image, receiving its corresponding pixel\u2019s color information (e.g. intensity) as an external stimulus. Each neuron also connects with its neighboring neurons, receiving local stimuli from them. The external and local stimuli are combined in an internal activation system, which accumulates the stimuli until it exceeds a dynamic threshold, resulting in a pulse output. Through iterative computation, PCNN neurons produce temporal series of pulse outputs. The temporal series of pulse outputs contain information of input images and can be utilized for various image processing applications, such as image segmentation and feature generation. Compared with conventional image processing means, PCNNs have several significant merits, including robustness against noise, independence of geometric variations in input patterns, capability of bridging minor intensity variations in input patterns, etc.",
            "score": 88.3089052438736
        },
        {
            "docid": "10159567_6",
            "document": "Spiking neural network . This kind of neural network can in principle be used for information processing applications the same way as traditional artificial neural networks. In addition, spiking neural networks can model the central nervous system of a virtual insect for seeking food without the prior knowledge of the environment. However, due to their more realistic properties, they can also be used to study the operation of biological neural circuits. Starting with a hypothesis about the topology of a biological neuronal circuit and its function, the electrophysiological recordings of this circuit can be compared to the output of the corresponding spiking artificial neural network simulated on computer, determining the plausibility of the starting hypothesis.",
            "score": 89.10281181335449
        },
        {
            "docid": "2567511_18",
            "document": "Neural engineering . Neural interfaces are a major element used for studying neural systems and enhancing or replacing neuronal function with engineered devices. Engineers are challenged with developing electrodes that can selectively record from associated electronic circuits to collect information about the nervous system activity and to stimulate specified regions of neural tissue to restore function or sensation of that tissue (Cullen et al. 2011). The materials used for these devices must match the mechanical properties of neural tissue in which they are placed and the damage must be assessed. Neural interfacing involves temporary regeneration of biomaterial scaffolds or chronic electrodes and must manage the body's response to foreign materials. Microelectrode arrays are recent advances that can be used to study neural networks (Cullen & Pfister 2011). Optical neural interfaces involve optical recordings and optogenetics stimulation that makes brain cells light sensitive. Fiber optics can be implanted in the brain to stimulate and record this photon activity instead of electrodes. Two-photon excitation microscopy can study living neuronal networks and the communicatory events among neurons.",
            "score": 111.19606518745422
        },
        {
            "docid": "355240_12",
            "document": "Cognitive model . One proposed mechanism of a dynamical system comes from analysis of continuous-time recurrent neural networks (CTRNNs). By focusing on the output of the neural networks rather than their states and examining fully interconnected networks, three-neuron Central pattern generator (CPG) can be used to represent systems such as leg movements during walking. This CPG contains three motor neurons to control the foot, backward swing, and forward swing effectors of the leg. Outputs of the network represent whether the foot is up or down and how much force is being applied to generate torque in the leg joint. One feature of this pattern is that neuron outputs are either off or on most of the time. Another feature is that the states are quasi-stable, meaning that they will eventually transition to other states. A simple pattern generator circuit like this is proposed to be a building block for a dynamical system. Sets of neurons that simultaneously transition from one quasi-stable state to another are defined as a dynamic module. These modules can in theory be combined to create larger circuits that comprise a complete dynamical system. However, the details of how this combination could occur are not fully worked out.",
            "score": 105.45210492610931
        },
        {
            "docid": "3717_69",
            "document": "Brain . Theorists have worked to understand these response patterns by constructing mathematical models of neurons and neural networks, which can be simulated using computers. Some useful models are abstract, focusing on the conceptual structure of neural algorithms rather than the details of how they are implemented in the brain; other models attempt to incorporate data about the biophysical properties of real neurons. No model on any level is yet considered to be a fully valid description of brain function, though. The essential difficulty is that sophisticated computation by neural networks requires distributed processing in which hundreds or thousands of neurons work cooperatively\u2014current methods of brain activity recording are only capable of isolating action potentials from a few dozen neurons at a time.",
            "score": 145.91791772842407
        },
        {
            "docid": "40158142_9",
            "document": "Nonlinear system identification . Artificial neural networks try loosely to imitate the network of neurons in the brain where computation takes place through a large number of simple processing elements. A typical neural network consists of a number of simple processing units interconnected to form a complex network. Layers of such units are arranged so that data is entered at the input layer and passes through either one or several intermediate layers before reaching the output layer. In supervised learning the network is trained by operating on the difference between the actual output and the desired output of the network, the prediction error, to change the connection strengths between the nodes. By iterating the weights are modified until the output error reaches an acceptable level. This process is called machine learning because the network adjusts the weights so that the output pattern is reproduced. Neural networks have been extensively studied and there are many excellent textbooks devoted to this topic in general, and more focussed textbooks which emphasise control and systems applications. There are two main problem types that can be studied using neural networks: static problems, and dynamic problems. Static problems include pattern recognition, classification, and approximation. Dynamic problems involve lagged variables and are more appropriate for system identification and related applications. Depending on the architecture of the network the training problem can be either nonlinear-in-the-parameters which involves optimisation or linear-in-the-parameters which can be solved using classical approaches. The training algorithms can be categorised into supervised, unsupervised, or reinforcement learning. Neural networks have excellent approximation properties but these are usually based on standard function approximation results using for example the Weierstrass Theorem that applies equally well to polynomials, rational functions, and other well-known models.  Neural networks have been applied extensively to system identification problems which involve nonlinear and dynamic relationships. However, classical neural networks are purely gross static approximating machines. There is no dynamics within the network. Hence when fitting dynamic models all the dynamics arise by allocating lagged inputs and outputs to the input layer of the network. The training procedure then produces the best static approximation that relates the lagged variables assigned to the input nodes to the output. There are more complex network architectures, including recurrent networks, that produce dynamics by introducing increasing orders of lagged variables to the input nodes. But in these cases it is very easy to over specify the lags and this can lead to over fitting and poor generalisation properties.  Neural networks have several advantages; they are conceptually simple, easy to train and to use, have excellent approximation properties, the concept of local and parallel processing is important and this provides integrity and fault tolerant behaviour. The biggest criticism of the classical neural network models is that the models produced are completely opaque and usually cannot be written down or analysed. It is therefore very difficult to know what is causing what, to analyse the model, or to compute dynamic characteristics from the model. Some of these points will not be relevant to all applications but they are for dynamic modelling.",
            "score": 76.88514626026154
        },
        {
            "docid": "762064_2",
            "document": "Systems neuroscience . Systems neuroscience is a subdiscipline of neuroscience and systems biology that studies the function of neural circuits and systems. It is an umbrella term, encompassing a number of areas of study concerned with how nerve cells behave when connected together to form neural pathways, neural circuits, and larger brain networks. At this level of analysis, neuroscientists study how different neural circuits analyze sensory information, form perceptions of the external world, make decisions, and execute movements. Researchers in systems neuroscience are concerned with the relation between molecular and cellular approaches to understanding brain structure and function, as well as with the study of high-level mental functions such as language, memory, and self-awareness (which are the purview of behavioral and cognitive neuroscience). Systems neuroscientists typically employ techniques for understanding networks of neurons as they are seen to function, by way of electrophysiology using either single-unit recording or multi-electrode recording, functional magnetic resonance imaging (fMRI), and PET scans. The term is commonly used in an educational framework: a common sequence of graduate school neuroscience courses consists of cellular/molecular neuroscience for the first semester, then systems neuroscience for the second semester. It is also sometimes used to distinguish a subdivision within a neuroscience department at an academic institution.",
            "score": 104.26486110687256
        },
        {
            "docid": "2567511_17",
            "document": "Neural engineering . Scientists can use experimental observations of neuronal systems and theoretical and computational models of these systems to create Neural networks with the hopes of modeling neural systems in as realistic a manner as possible. Neural networks can be used for analyses to help design further neurotechnological devices. Specifically, researchers handle analytical or finite element modeling to determine nervous system control of movements and apply these techniques to help patients with brain injuries or disorders. Artificial neural networks can be built from theoretical and computational models and implemented on computers from theoretically devices equations or experimental results of observed behavior of neuronal systems. Models might represent ion concentration dynamics, channel kinetics, synaptic transmission, single neuron computation, oxygen metabolism, or application of dynamic system theory (LaPlaca et al. 2005). Liquid-based template assembly was used to engineer 3D neural networks from neuron-seeded microcarrier beads.",
            "score": 120.98295760154724
        },
        {
            "docid": "6107563_7",
            "document": "Pulse-coupled networks . A PCNN is a two-dimensional neural network. Each neuron in the network corresponds to one pixel in an input image, receiving its corresponding pixel\u2019s color information (e.g. intensity) as an external stimulus. Each neuron also connects with its neighboring neurons, receiving local stimuli from them. The external and local stimuli are combined in an internal activation system, which accumulates the stimuli until it exceeds a dynamic threshold, resulting in a pulse output. Through iterative computation, PCNN neurons produce temporal series of pulse outputs. The temporal series of pulse outputs contain information of input images and can be used for various image processing applications, such as image segmentation and feature generation. Compared with conventional image processing means, PCNNs have several significant merits, including robustness against noise, independence of geometric variations in input patterns, capability of bridging minor intensity variations in input patterns, etc.",
            "score": 84.6394110918045
        },
        {
            "docid": "3474296_4",
            "document": "Neuronal noise . Single neurons demonstrate different responses to specific neuronal input signals. This is commonly referred to as neural response variability. If a specific input signal is initiated in the dendrites of a neuron, then a hypervariability exists in the number of vesicles released from the axon terminal fiber into the synapse. This characteristic is true for fibers without neural input signals, such as pacemaker neurons, as mentioned previously, and cortical pyramidal neurons that have highly-irregular firing pattern. Noise generally hinders neural performance, but recent studies show, in dynamical non-linear neural networks, this statement does not always hold true. Non-linear neural networks are a network of complex neurons that have many connections with one another such as the neuronal systems found within our brains. Comparatively, linear networks are an experimental view of analyzing a neural system by placing neurons in series with each other.",
            "score": 110.33190512657166
        },
        {
            "docid": "2645238_5",
            "document": "Central pattern generator . Although anatomical details of CPGs are specifically known in only a few cases, they have been shown to originate from the spinal cords of various vertebrates and to depend on relatively small and autonomous neural networks (rather than the entire nervous system) to generate rhythmic patterns. Many studies have been done to determine the neural substrate of locomotor CPGs in mammals. Neural rhythmicity can arise in two ways: \"through interactions among neurons (network-based rhythmicity) or through interactions among currents in individual neurons (endogenous oscillator neurons)\". A key to understanding rhythm generation is the concept of a half-center oscillator (HCO). A half-center oscillator consists of two neurons that have no rhythmogenic ability individually, but produce rhythmic outputs when reciprocally coupled. Half-center oscillators can function in a variety of ways. First, the two neurons may not necessarily fire in antiphase and can fire in any relative phasing, even synchrony, depending on the synaptic release. Second, half-centers can also function in an \"escape\" mode or a \"release\" mode. Escape and release refer to the way the off-neuron turns on: by escape or release from inhibition. Half-center oscillators can also be altered by intrinsic and network properties and can have dramatically different functionality based on variations in synaptic properties.",
            "score": 104.93457221984863
        },
        {
            "docid": "2860430_15",
            "document": "Neural oscillation . Scientists have identified some intrinsic neuronal properties that play an important role in generating membrane potential oscillations. In particular, voltage-gated ion channels are critical in the generation of action potentials. The dynamics of these ion channels have been captured in the well-established Hodgkin\u2013Huxley model that describes how action potentials are initiated and propagated by means of a set of differential equations. Using bifurcation analysis, different oscillatory varieties of these neuronal models can be determined, allowing for the classification of types of neuronal responses. The oscillatory dynamics of neuronal spiking as identified in the Hodgkin\u2013Huxley model closely agree with empirical findings. In addition to periodic spiking, subthreshold membrane potential oscillations, i.e. resonance behavior that does not result in action potentials, may also contribute to oscillatory activity by facilitating synchronous activity of neighboring neurons. Like pacemaker neurons in central pattern generators, subtypes of cortical cells fire bursts of spikes (brief clusters of spikes) rhythmically at preferred frequencies. Bursting neurons have the potential to serve as pacemakers for synchronous network oscillations, and bursts of spikes may underlie or enhance neuronal resonance.",
            "score": 108.19093978404999
        },
        {
            "docid": "2860430_23",
            "document": "Neural oscillation . A neural network model describes a population of physically interconnected neurons or a group of disparate neurons whose inputs or signalling targets define a recognizable circuit. These models aim to describe how the dynamics of neural circuitry arise from interactions between individual neurons. Local interactions between neurons can result in the synchronization of spiking activity and form the basis of oscillatory activity. In particular, models of interacting pyramidal cells and inhibitory interneurons have been shown to generate brain rhythms such as gamma activity.",
            "score": 129.8927924633026
        },
        {
            "docid": "1729542_2",
            "document": "Neural network . The term neural network was traditionally used to refer to a network or circuit of neurons. The modern usage of the term often refers to artificial neural networks, which are composed of artificial neurons or nodes. Thus the term may refer to either biological neural networks, made up of real biological neurons, or artificial neural networks, for solving artificial intelligence (AI) problems.The connections of the biological neuron are modeled as weights. A positive weight reflects an excitatory connection, while negative values mean inhibitory connections. All inputs are modified by a weight and summed. This activity is referred as a linear combination. Finally, an activation function controls the amplitude of the output. For example, an acceptable range of output is usually between 0 and 1, or it could be -1 and 1.",
            "score": 104.27396750450134
        },
        {
            "docid": "623686_33",
            "document": "Brain\u2013computer interface . The use of BMIs has also led to a deeper understanding of neural networks and the central nervous system. Research has shown that despite the inclination of neuroscientists to believe that neurons have the most effect when working together, single neurons can be conditioned through the use of BMIs to fire at a pattern that allows primates to control motor outputs. The use of BMIs has led to development of the single neuron insufficiency principle which states that even with a well tuned firing rate single neurons can only carry a narrow amount of information and therefore the highest level of accuracy is achieved by recording firings of the collective ensemble. Other principles discovered with the use of BMIs include the neuronal multitasking principle, the neuronal mass principle, the neural degeneracy principle, and the plasticity principle.",
            "score": 108.18312919139862
        },
        {
            "docid": "21245_17",
            "document": "Neuroscience . At the systems level, the questions addressed in systems neuroscience include how biological neural networks or neural circuits are formed and used anatomically and physiologically to produce functions such as reflexes, multisensory integration, motor coordination, circadian rhythms, emotional responses, learning, and memory. In other words, they address how these neural circuits function and the mechanisms through which behaviors are generated. For example, systems level analysis addresses questions concerning specific sensory and motor modalities: how does vision work? How do songbirds learn new songs and bats localize with ultrasound? How does the somatosensory system process tactile information? The related fields of neuroethology and neuropsychology address the question of how neural substrates underlie specific animal and human behaviors. Neuroendocrinology and psychoneuroimmunology examine interactions between the nervous system and the endocrine and immune systems, respectively. Despite many advancements, the way that networks of neurons perform complex cognitive processes and behaviors is still poorly understood.",
            "score": 82.92957186698914
        },
        {
            "docid": "33822344_13",
            "document": "Synaptic scaling . Hebbian plasticity and homeostatic plasticity have a hand-in-glove relationship. Neurons use Hebbian plasticity mechanisms to modify their synaptic connections within the neural circuit based on the correlated input they receive from other neurons. Long-term potentiation (LTP) mechanisms are driven by related pre-synaptic and post-synaptic neuron firings; with the help of homeostatic plasticity, LTPs and LTDs create and maintain the precise synaptic weights in the neural network. Persisting correlated neural activity\u2014without a homeostatic feedback loop\u2014causes LTP mechanisms to continually up regulate synaptic connection strengths. Unspecified strengthening of synaptic weights causes neural activity to become unstable to the point that insignificant stimulatory perturbations can trigger chaotic, synchronous network-wide firing known as bursts. This renders the neural network incapable of computing. Since homeostatic plasticity normalizes the synaptic strengths of all neurons in a network, the overall neural network activity stabilizes.",
            "score": 93.83472740650177
        },
        {
            "docid": "28016652_3",
            "document": "Types of artificial neural networks . Artificial neural networks are computational models inspired by biological neural networks, and are used to approximate functions that are generally unknown. Particularly, they are inspired by the behaviour of neurons and the electrical signals they convey between input (such as from the eyes or nerve endings in the hand), processing, and output from the brain (such as reacting to light, touch, or heat). The way neurons semantically communicate is an area of ongoing research. Most artificial neural networks bear only some resemblance to their more complex biological counterparts, but are very effective at their intended tasks (e.g. classification or segmentation).",
            "score": 98.07235336303711
        },
        {
            "docid": "39199253_2",
            "document": "Percolation (cognitive psychology) . Percolation (from the Latin word \"percolatio\", meaning filtration) is a theoretical model used to understand the way activation and diffusion of neural activity occur within neural networks. Percolation is a model used to explain how neural activity is transmitted across the various connections within the brain. Often it is easiest to understand percolation theory by explaining its use in epidemiology. Individuals that are infected with a disease can spread the disease through contact with others in their social network. Those who are more social and come into contact with more people will help to propagate the disease quicker than those who are less social. Therefore factors such as occupation and sociability influence the rate of infection. Now, if one were to think of \"neurons\" as the \"individuals\" and \"synaptic connections\" as the \"social bonds\" between people, then one can determine how easily messages between neurons will spread. When a neuron fires, the message is transmitted along all synaptic connections to other neurons until it can no longer continue. Synaptic connections are considered either open or closed (like a social or unsocial person) and messages will flow along any and all open connections until they can go no further. Just like occupation and sociability play a key role in the spread of disease, so too do the number of neurons, synaptic plasticity and long-term potentiation when talking about neural percolation.",
            "score": 126.23558640480042
        },
        {
            "docid": "1763319_11",
            "document": "Channelrhodopsin . Channelrhodopsins can be readily expressed in excitable cells such as neurons using a variety of transfection techniques (viral transfection, electroporation, gene gun) or transgenic animals. The light-absorbing pigment retinal is present in most cells (of vertebrates) as Vitamin A, making it possible to photostimulate neurons without adding any chemical compounds. Before the discovery of channelrhodopsins, neuroscientists were limited to recording the activity of neurons in the brain and correlate this activity with behavior. This is not sufficient to prove that the recorded neural activity actually caused that behavior. Controlling networks of genetically modified cells with light, an emerging field known as Optogenetics., allows researchers now to explore the causal link between activity in a specific group of neurons and mental events, e.g. decision making. Optical control of behavior has been demonstrated in nematodes, fruit flies, zebrafish, and mice. Recently, chloride-conducting channelrhodopsins have been engineered and were also found in nature. These tools can be used to silence neurons in cell culture and in live animals by shunting inhibition.",
            "score": 98.28970670700073
        },
        {
            "docid": "54133326_3",
            "document": "WaveNet . Generating speech from text is an increasingly common task thanks to the popularity of software such as Apple's Siri, Microsoft\u2019s Cortana, Amazon Alexa and the Google Assistant.  Most such systems use a variation of a technique that involves concatenated sound fragments together to form recognisable sounds and words. The most common of these is called concatenative TTS. It consists of large library of speech fragments, recorded from a single speaker that are then concatenated to produce complete words and sounds. The result sounds unnatural, with an odd cadence and tone. The reliance on a recorded library also makes it difficult to modify or change the voice. Another technique, known as parametric TTS, uses mathematical models to recreate sounds that are then assembled into words and sentences. The information required to generate the sounds is stored in the parameters of the model. The characteristics of the output speech are controlled via the inputs to the model, while the speech is typically created using a voice synthesiser known as a vocoder. This can also result in unnatural sounding audio. WaveNet is a type of feedforward neural network known as a deep convolutional neural network (CNN). These consist of layers of interconnected nodes somewhat analogous to the brain\u2019s neurons. The CNN takes a raw signal as an input and synthesises an output one sample at a time.  In the 2016 paper, the network was fed real waveforms of speech in English and Mandarin. As these pass through the network, it learns a set of rules to describe how the audio waveform evolves over time. The trained network can then be used to create new speech-like waveforms at 16,000 samples per second. These waveforms include realistic breaths and lip smacks - but do not conform to any language.  WaveNet is able to accurately model different voices, with the accent and tone of the input correlating with the output. For example, if it is trained with German, it produces German speech. This ability to clone voices has raised ethical concerns about WaveNets ability to mimic the voices of living persons.  The capability also means that if the WaveNet is fed other inputs - such as music - its output will be musical. At the time of its release, DeepMind showed that WaveNet could produce waveforms that sound like classical music.",
            "score": 76.8870096206665
        },
        {
            "docid": "1164_53",
            "document": "Artificial intelligence . Neural networks, or neural nets, were inspired by the architecture of neurons in the human brain. A simple \"neuron\" \"N\" accepts input from multiple other neurons, each of which, when activated (or \"fired\"), cast a weighted \"vote\" for or against whether neuron \"N\" should itself activate. Learning requires an algorithm to adjust these weights based on the training data; one simple algorithm (dubbed \"fire together, wire together\") is to increase the weight between two connected neurons when the activation of one triggers the successful activation of another. The net forms \"concepts\" that are distributed among a subnetwork of shared neurons that tend to fire together; a concept meaning \"leg\" might be coupled with a subnetwork meaning \"foot\" that includes the sound for \"foot\". Neurons have a continuous spectrum of activation; in addition, neurons can process inputs in a nonlinear way rather than weighing straightforward votes. Modern neural nets can learn both continuous functions and, surprisingly, digital logical operations. Neural networks' early successes included predicting the stock market and (in 1995) a mostly self-driving car. In the 2010s, advances in neural networks using deep learning thrust AI into widespread public consciousness and contributed to an enormous upshift in corporate AI spending; for example, AI-related M&A in 2017 was over 25 times as large as in 2015.",
            "score": 111.39429950714111
        },
        {
            "docid": "2860430_16",
            "document": "Neural oscillation . Apart from intrinsic properties of neurons, biological neural network properties are also an important source of oscillatory activity. Neurons communicate with one another via synapses and affect the timing of spike trains in the post-synaptic neurons. Depending on the properties of the connection, such as the coupling strength, time delay and whether coupling is excitatory or inhibitory, the spike trains of the interacting neurons may become synchronized. Neurons are locally connected, forming small clusters that are called neural ensembles. Certain network structures promote oscillatory activity at specific frequencies. For example, neuronal activity generated by two populations of interconnected \"inhibitory\" and \"excitatory\" cells can show spontaneous oscillations that are described by the Wilson-Cowan model.",
            "score": 114.49013137817383
        },
        {
            "docid": "1706332_4",
            "document": "Feedforward neural network . The simplest kind of neural network is a \"single-layer perceptron\" network, which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. In this way it can be considered the simplest kind of feed-forward network. The sum of the products of the weights and the inputs is calculated in each node, and if the value is above some threshold (typically 0) the neuron fires and takes the activated value (typically 1); otherwise it takes the deactivated value (typically -1). Neurons with this kind of activation function are also called \"artificial neurons\" or \"linear threshold units\". In the literature the term \"perceptron\" often refers to networks consisting of just one of these units. A similar neuron was described by Warren McCulloch and Walter Pitts in the 1940s.",
            "score": 86.70307183265686
        },
        {
            "docid": "40409788_30",
            "document": "Convolutional neural network . Stacking the activation maps for all filters along the depth dimension forms the full output volume of the convolution layer. Every entry in the output volume can thus also be interpreted as an output of a neuron that looks at a small region in the input and shares parameters with neurons in the same activation map.",
            "score": 85.10929846763611
        },
        {
            "docid": "941909_26",
            "document": "Receptive field . The term receptive field is also used in the context of artificial neural networks, most often in relation to convolutional neural networks (CNNs). When used in this sense, the term adopts a meaning reminiscent of receptive fields in actual biological nervous systems. CNNs have a distinct architecture, designed to mimic the way in which real animal brains are understood to function; instead of having every neuron in each layer connect to all neurons in the next layer (Multilayer perceptron), the neurons are arranged in a 3-dimensional structure in such a way as to take into account the spatial relationships between different neurons with respect to the original data. Since CNNs are used primarily in the field of computer vision, the data that the neurons represent is typically an image; each input neuron represents one pixel from the original image. The first layer of neurons is composed of all the input neurons; neurons in the next layer will receive connections from some of the input neurons (pixels), but not all, as would be the case in a MLP and in other traditional neural networks. Hence, instead of having each neuron receive connections from all neurons in the previous layer, CNNs use a receptive field-like layout in which each neuron receives connections only from a subset of neurons in the previous (lower) layer. The receptive field of a neuron in one of the lower layers encompasses only a small area of the image, while the receptive field of a neuron in subsequent (higher) layers involves a combination of receptive fields from several (but not all) neurons in the layer before (i. e. a neuron in a higher layer \"looks\" at a larger portion of the image than does a neuron in a lower layer). In this way, each successive layer is capable of learning increasingly abstract features of the original image. The use of receptive fields in this fashion is thought to give CNNs an advantage in recognizing visual patterns when compared to other types of neural networks.",
            "score": 108.6743632555008
        },
        {
            "docid": "33818014_5",
            "document": "Nervous system network models . What is brain and what is neural network? Section 2.1 addresses the former question from an evolutionary perspective. The answer to the second question is based on the neural doctrine proposed by Ramon y Cajal (1894). He hypothesized that the elementary biological unit is an active cell, called neuron, and the human machine is run by a vast network that connects these neurons, called neural (or neuronal) network. The neural network is integrated with the human organs to form the human machine comprising the nervous system.",
            "score": 106.74604511260986
        },
        {
            "docid": "1894504_4",
            "document": "Brain implant . Brain implants electrically stimulate, block or record (or both record and stimulate simultaneously) signals from single neurons or groups of neurons (biological neural networks) in the brain. The blocking technique is called intra-abdominal vagal blocking. This can only be done where the functional associations of these neurons are approximately known. Because of the complexity of neural processing and the lack of access to action potential related signals using neuroimaging techniques, the application of brain implants has been seriously limited until recent advances in neurophysiology and computer processing power.",
            "score": 100.72068357467651
        },
        {
            "docid": "27075922_9",
            "document": "Natural computing . An artificial neural network is a network of artificial neurons.  An artificial neuron \"A\" is equipped with a function formula_1, receives \"n\" real-valued inputs formula_2 with respective weights formula_3, and it outputs formula_4. Some neurons are selected to be the output neurons, and the network function is the vectorial function that associates to the \"n\" input values, the outputs of the \"m\" selected output neurons. Note that different choices of weights produce different network functions for the same inputs. Back-propagation is a supervised learning method by which the weights of the connections in the network are repeatedly adjusted so as to minimize the difference between the vector of actual outputs and that of desired outputs. Learning algorithms based on backwards propagation of errors can be used to find optimal weights for given topology of the network and input-output pairs.",
            "score": 89.16213381290436
        },
        {
            "docid": "33244792_4",
            "document": "Non-spiking neuron . There are an abundance of neurons that propagate signals via action potentials and the mechanics of this particular kind of transmission is well understood. Spiking neurons exhibit action potentials as a result of a neuron characteristic known as membrane potential. Through studying these complex spiking networks in animals, a neuron that did not exhibit characteristic spiking behavior was discovered. These neurons use a graded potential to transmit data as they lack the membrane potential that spiking neurons possess. This method of transmission has a huge effect on the fidelity, strength, and lifetime of the signal. Non-spiking neurons were identified as a special kind of interneuron and function as an intermediary point of process for sensory-motor systems. Animals have become substantial models for understanding more about non-spiking neural networks and the role they play in an animal\u2019s ability to process information and its overall function. Animal models indicate that the interneurons modulate directional and posture coordinating behaviors. Crustaceans and arthropods such as the crawfish have created many opportunities to learn about the modulatory role that these neurons have in addition to their potential to be modulated regardless of their lack of exhibiting spiking behavior. Most of the known information about nonspiking neurons is derived from animal models. Studies focus on neuromuscular junctions and modulation of abdominal motor cells. Modulatory interneurons are neurons that are physically situated next to muscle fibers and innervate the nerve fibers which allow for some orienting movement. These modulatory interneurons are usually nonspiking neurons. Advances in studying nonspiking neurons included determining new delineations among the different types of interneurons. These discoveries were due to the usage of methods such as protein receptor silencing. Studies have been done on the non-spiking neuron qualities in animals of specific non-spiking neural networks that have a corollary in humans, e.g. retina amacrine cell of the eye.",
            "score": 114.52278125286102
        },
        {
            "docid": "21523_30",
            "document": "Artificial neural network . An \"artificial neural network\" is a network of simple elements called \"artificial neurons\", which receive input, change their internal state (\"activation\") according to that input, and produce output depending on the input and activation. The \"network\" forms by connecting the output of certain neurons to the input of other neurons forming a directed, weighted graph. The weights as well as the functions that compute the activation can be modified by a process called \"learning\" which is governed by a \"learning rule\".",
            "score": 94.3221549987793
        },
        {
            "docid": "739262_10",
            "document": "Neural correlate . Neurophysiological studies in animals provided some insights on the neural correlates of conscious behavior. Vernon Mountcastle, in the early 1960s, set up to study this set of problems, which he termed \"the Mind/Brain problem\", by studying the neural basis of perception in the somatic sensory system. His labs at Johns Hopkins were among the first, along with Edward V.Evarts at NIH, to record neural activity from behaving monkeys. Struck with the elegance of SS Stevens approach of magnitude estimation, Mountcastle's group discovered three different modalities of somatic sensation shared one cognitive attribute: in all cases the firing rate of peripheral neurons was linearly related to the strength of the percept elicited. More recently, Ken H. Britten, William T. Newsome, and C. Daniel Salzman have shown that in area MT of monkeys, neurons respond with variability that suggests they are the basis of decision making about direction of motion. They first showed that neuronal rates are predictive of decisions using signal detection theory, and then that stimulation of these neurons could predictably bias the decision. Such studies were followed by Ranulfo Romo in the somatic sensory system, to confirm, using a different percept and brain area, that a small number of neurons in one brain area underlie perceptual decisions.",
            "score": 102.28048324584961
        }
    ],
    "r": [
        {
            "docid": "3717_69",
            "document": "Brain . Theorists have worked to understand these response patterns by constructing mathematical models of neurons and neural networks, which can be simulated using computers. Some useful models are abstract, focusing on the conceptual structure of neural algorithms rather than the details of how they are implemented in the brain; other models attempt to incorporate data about the biophysical properties of real neurons. No model on any level is yet considered to be a fully valid description of brain function, though. The essential difficulty is that sophisticated computation by neural networks requires distributed processing in which hundreds or thousands of neurons work cooperatively\u2014current methods of brain activity recording are only capable of isolating action potentials from a few dozen neurons at a time.",
            "score": 145.9179229736328
        },
        {
            "docid": "32018467_7",
            "document": "Christian Keysers . After finishing his master, Christian Keysers decided to concentrate on a subfield of cognitive neuroscience called social neuroscience that uses neuroscience methods to understand how we process the social world. He therefore performed his doctoral studies at the University of St Andrews with David Ian Perrett, one of the founding father of the field, to understand how the brain processes faces and facial expressions. This thesis work led to new insights into how quickly the brain can process the faces of others. During this period, Keysers became fascinated with the question of how the brain can attach meaning to the faces of others. How is it for instance, that we understand that a certain grimace would signal that another person is happy? How do we understand that a certain bodily movement towards a glass indicates that the other person aims to grasp a glass? In 1999, Keysers was exposed to a visit of Vittorio Gallese, who presented his recent discovery of mirror neurons in the Psychology department lecture series. This deeply influenced Keysers who decided to move to the lab of Giacomo Rizzolatti to undertake further studies on how these fascinating neurons could contribute to social perception. In 2000, after finishing his doctorate, Christian Keysers moved to the University of Parma to study mirror neurons. In early work there demonstrated that mirror neurons in the premotor cortex not only respond to the sight of actions, but also when actions can only be deduced or heard, leading to a publication in the journal \"Science\". This work had tremendous impact on the field, as it suggested that the premotor cortex could play a central, modality independent role in perception and may lay the origin for the evolution of speech in humans.  Together this work indicated that brain regions involved in our own actions play a role in how we process the actions of others. Keysers wondered whether a similar principle may underlie how we process the tactile sensations and emotions of others, and became increasingly independent of the research focus on the motor system in Parma. At the time, Keysers had also met his to be wife, Valeria Gazzola, a biologist in the final phases of her studies, and together they decided to explore if the somatosensory system might be involved in perceiving the sensations of others. Via a fruitful collaboration with the French neuroimaging specialist Bruno Wicker, they used functional magnetic resonance imaging, and showed for the first time, that the secondary somatosensory cortex, previously thought only to represent a persons own experiences of touch, is also activated when seeing someone or something else be touched. They also showed that the insula, thought only to respond to the experience of first-hand emotions, was also activated when we see another individual experience similar emotions. Together this indicated a much more general principle than the original mirror neuron theory, in which people process the actions, sensations and emotions of others by vicariously activating owns own actions, sensations and emotions. Jointly, this work laid the foundation of the neuroscientific investigation of empathy.",
            "score": 143.16432189941406
        },
        {
            "docid": "33826069_3",
            "document": "Viral neuronal tracing . Most neuroanatomists would agree that understanding how the brain is connected to itself and the body is of paramount importance. As such, it is of equal importance to have a way to visualize and study the connections among neurons. Neuronal tracing methods offer an unprecedented view into the morphology and connectivity of neural networks. Depending on the tracer used, this can be limited to a single neuron or can progress trans-synaptically to adjacent neurons. After the tracer has spread sufficiently, the extent may be measured either by fluorescence (for dyes) or by immunohistochemistry (for biological tracers). An important innovation in this field is the use of neurotropic viruses as tracers. These not only spread throughout the initial site of infection, but can jump across synapses. The use of a virus provides a self-replicating tracer. This can allow for the elucidation of neural microcircuitry to an extent that was previously unobtainable.  This has significant implications for the real world. If we can better understand what parts of the brain are intimately connected, we can predict the effect of localized brain injury. For example, if a patient has a stroke in the amygdala, primarily responsible for emotion, the patient might also have trouble learning to perform certain tasks because the amygdala is highly interconnected with the orbitofrontal cortex, responsible for reward learning. As always, the first step to solving a problem is fully understanding it, so if we are to have any hope of fixing brain injury, we must first understand its extent and complexity.",
            "score": 140.13648986816406
        },
        {
            "docid": "29648114_8",
            "document": "Andrea Brand . Brand has provided this \u201cplain English\u201d explanation of her work: \u201cOne of the goals of research in neurobiology is to repair or regenerate neurons after damage to the brain or spinal cord. Before we can understand how to repair the nervous system, however, we must first learn how the nervous system is put together. Of all the tissues and organs in the human body the nervous system is the most intricate and complex, consisting of more than one trillion neurons. These neurons make precise connections with each other to form functional networks that can transmit information at amazing speed over considerable distances.",
            "score": 132.7389373779297
        },
        {
            "docid": "29826376_7",
            "document": "Hippocampal prosthesis . First, we must take into account that, like most of biological processes, the behaviors of neurons are highly nonlinear and depend on many factors: input frequency patterns, etc. Also, a good model must take into account the fact that the expression of a single nerve cell is negligible, since the processes are carried by groups of neurons interacting in network. Once installed, the device must assume all (or at least most) of the function of the damaged hippocampus for a prolonged period of time. First, the artificial neurons must be able to work together in network just like real neurons. Then, they must be able, working and effective synaptics connections with the existing neurons of the brain; therefore a model for silicon/neurons interface will be required.",
            "score": 130.35923767089844
        },
        {
            "docid": "2860430_23",
            "document": "Neural oscillation . A neural network model describes a population of physically interconnected neurons or a group of disparate neurons whose inputs or signalling targets define a recognizable circuit. These models aim to describe how the dynamics of neural circuitry arise from interactions between individual neurons. Local interactions between neurons can result in the synchronization of spiking activity and form the basis of oscillatory activity. In particular, models of interacting pyramidal cells and inhibitory interneurons have been shown to generate brain rhythms such as gamma activity.",
            "score": 129.89279174804688
        },
        {
            "docid": "19628311_9",
            "document": "Earl K. Miller . Miller has innovated techniques for recording from many neurons simultaneously in multiple brain areas. This is a departure from the classic single-neuron recording approach. It allows detailed and direct comparison of neuron properties between brain areas that are not confounded by extraneous factors and examination of the temporal dynamics of activity between neurons. Miller's lab has used this approach to make a number of discoveries of how different brain areas collaborate to produce thought and action. This includes recent discoveries that oscillating \"brain waves\" may control the timing of shifts of attention and that different items simultaneously held in short-term memory line up on different phases of each brain wave. The latter may explain why we can only think about a few things at the same time.",
            "score": 128.53945922851562
        },
        {
            "docid": "50799933_7",
            "document": "Galves\u2013L\u00f6cherbach model . The model considers a countable set of neurons formula_1 and models its evolution in discrete-time periods formula_2 with a stochastic chain formula_3, considering values in formula_4. More precisely, for each neuron formula_5 and time period formula_2, we define formula_7 if neuron formula_8 spikes in period formula_9, and conversely formula_10. The configuration of the set of neurons, in the time period formula_2, is therefore defined as formula_12. For each time period formula_2, we define a sigma-algebra formula_14, representing the history of the evolution of the activity of this set of neurons until the relevant time period formula_9. The dynamics of the activity of this set of neurons is defined as follows. Once the history formula_16 is given, neurons spike or not in the next time period formula_17 independently from one another, that is, for each finite subset formula_18 and any configuration formula_19 we have",
            "score": 127.48051452636719
        },
        {
            "docid": "1168317_46",
            "document": "Mirror neuron . There are several competing models which attempt to account for our theory of mind; the most notable in relation to mirror neurons is simulation theory. According to simulation theory, theory of mind is available because we subconsciously empathize with the person we're observing and, accounting for relevant differences, imagine what we would desire and believe in that scenario. Mirror neurons have been interpreted as the mechanism by which we simulate others in order to better understand them, and therefore their discovery has been taken by some as a validation of simulation theory (which appeared a decade before the discovery of mirror neurons). More recently, Theory of Mind and Simulation have been seen as complementary systems, with different developmental time courses.",
            "score": 126.60282897949219
        },
        {
            "docid": "39199253_2",
            "document": "Percolation (cognitive psychology) . Percolation (from the Latin word \"percolatio\", meaning filtration) is a theoretical model used to understand the way activation and diffusion of neural activity occur within neural networks. Percolation is a model used to explain how neural activity is transmitted across the various connections within the brain. Often it is easiest to understand percolation theory by explaining its use in epidemiology. Individuals that are infected with a disease can spread the disease through contact with others in their social network. Those who are more social and come into contact with more people will help to propagate the disease quicker than those who are less social. Therefore factors such as occupation and sociability influence the rate of infection. Now, if one were to think of \"neurons\" as the \"individuals\" and \"synaptic connections\" as the \"social bonds\" between people, then one can determine how easily messages between neurons will spread. When a neuron fires, the message is transmitted along all synaptic connections to other neurons until it can no longer continue. Synaptic connections are considered either open or closed (like a social or unsocial person) and messages will flow along any and all open connections until they can go no further. Just like occupation and sociability play a key role in the spread of disease, so too do the number of neurons, synaptic plasticity and long-term potentiation when talking about neural percolation.",
            "score": 126.235595703125
        },
        {
            "docid": "271430_22",
            "document": "Computational neuroscience . The interactions of neurons in a small network can be often reduced to simple models such as the Ising model. The statistical mechanics of such simple systems are well-characterized theoretically. There has been some recent evidence that suggests that dynamics of arbitrary neuronal networks can be reduced to pairwise interactions. It is not known, however, whether such descriptive dynamics impart any important computational function. With the emergence of two-photon microscopy and calcium imaging, we now have powerful experimental methods with which to test the new theories regarding neuronal networks.",
            "score": 125.33702087402344
        },
        {
            "docid": "2860430_25",
            "document": "Neural oscillation . The Kuramoto model of coupled phase oscillators is one of the most abstract and fundamental models used to investigate neural oscillations and synchronization. It captures the activity of a local system (e.g., a single neuron or neural ensemble) by its circular phase alone and hence ignores the amplitude of oscillations (amplitude is constant). Interactions amongst these oscillators are introduced by a simple algebraic form (such as a sine function) and collectively generate a dynamical pattern at the global scale. The Kuramoto model is widely used to study oscillatory brain activity and several extensions have been proposed that increase its neurobiological plausibility, for instance by incorporating topological properties of local cortical connectivity. In particular, it describes how the activity of a group of interacting neurons can become synchronized and generate large-scale oscillations. Simulations using the Kuramoto model with realistic long-range cortical connectivity and time-delayed interactions reveal the emergence of slow patterned fluctuations that reproduce resting-state BOLD functional maps, which can be measured using fMRI.",
            "score": 124.6448974609375
        },
        {
            "docid": "15774067_27",
            "document": "Synaptic noise . To understand the future of synaptic noise research, it would be essential to discuss the work of Alain Destexhe, a Belgian doctor who has greatly studied the importance of synaptic noise in neuronal connections. He uses the dynamic-clamp technique to understand the presence and characteristics of noise. While voltage-gated clamps record configurations, dynamic-clamp allows for the control of conductance by way of computer. A computational model of synaptic noise is created and is then implemented into the neuron, simulating synaptic noise. This can be used to compare with in-vivo conditions. Destexhe states that future research can be directed towards four possible ways, in reflection of his research with dynamic-clamp. First, it could be beneficial to understand the control of synaptic noise so that the modulation of noise can be used on humans to turn unresponsive networks into a responsive state. Next, it would be necessary to understand how external noise interacts with internal neuronal properties more fully to coincide models with experimental facts. There also exists the need to further investigate experimentally the methods of dendritic integration and the role of synaptic noise when it is present. Finally, he found support that synaptic noise enhances temporal resolution in neurons, yet experimental proof has not been done to further elaborate on past modeling studies. By use of dynamic-clamp, these pieces of information clarify the role of synaptic noise in the brain and how it can be harnessed for specific therapies.",
            "score": 124.04759216308594
        },
        {
            "docid": "192746_16",
            "document": "Gerald Edelman . Edelman's theory seeks to explain consciousness in terms of the morphology of the brain. A newborn baby's brain comprises a massive population of neurons (approx. 100 billion cells) and those that survive the initial phases of growth and development will make approximately 100 trillion connections with each other. A sample of brain tissue the size of a match head contains about a billion connections, and if we consider how these neuronal connections might be variously combined, the number of possible permutations becomes hyper-astronomical - in the order of ten followed by millions of zeros. The young brain contains many more neurons than will ultimately survive to maturity, and Edelman argued that this redundant capacity is needed because neurons are the only cells in the body that cannot be renewed and because only those cells and networks best adapted to their ultimate purpose will be selected as they organize into neuronal groups.",
            "score": 123.60743713378906
        },
        {
            "docid": "2860430_20",
            "document": "Neural oscillation . Computational models adopt a variety of abstractions in order to describe complex oscillatory dynamics observed in brain activity. Many models are used in the field, each defined at a different level of abstraction and trying to model different aspects of neural systems. They range from models of the short-term behaviour of individual neurons, through models of how the dynamics of neural circuitry arise from interactions between individual neurons, to models of how behaviour can arise from abstract neural modules that represent complete subsystems.",
            "score": 122.46418762207031
        },
        {
            "docid": "2567511_17",
            "document": "Neural engineering . Scientists can use experimental observations of neuronal systems and theoretical and computational models of these systems to create Neural networks with the hopes of modeling neural systems in as realistic a manner as possible. Neural networks can be used for analyses to help design further neurotechnological devices. Specifically, researchers handle analytical or finite element modeling to determine nervous system control of movements and apply these techniques to help patients with brain injuries or disorders. Artificial neural networks can be built from theoretical and computational models and implemented on computers from theoretically devices equations or experimental results of observed behavior of neuronal systems. Models might represent ion concentration dynamics, channel kinetics, synaptic transmission, single neuron computation, oxygen metabolism, or application of dynamic system theory (LaPlaca et al. 2005). Liquid-based template assembly was used to engineer 3D neural networks from neuron-seeded microcarrier beads.",
            "score": 120.98295593261719
        },
        {
            "docid": "54942318_4",
            "document": "Neil Burgess (neuroscientist) . Burgess research in neuroscience has developed models to explain how networks of neurons allow us to represent, remember and imagine our location within the surrounding environment. These models provide a quantitative understanding of how spatial memory, episodic memory and autobiographical memory function (and dysfunction) depend on human brain activity. With Tom Hartley at the University of York and Colin Lever at Durham University he both predicted and discovered neurons representing environmental boundaries.",
            "score": 119.43624114990234
        },
        {
            "docid": "21855574_5",
            "document": "Brain simulation . The connectivity of the neural circuit for touch sensitivity of the simple C. elegans nematode (roundworm) was mapped in 1985 and partly simulated in 1993. Since 2004, many software simulations of the complete neural and muscular system have been developed, including simulation of the worm's physical environment. Some of these models have been made available for download. However, there is still a lack of understanding of how the neurons and the connections between them generate the surprisingly complex range of behaviors that are observed in the relatively simple organism. This contrast between the apparent simplicity of how the mapped neurons interact with their neighbours, and exceeding complexity of the overall brain function, is an example of an emergent property. Interestingly, this kind of emergent property is paralleled within artificial neural networks, the neurons of which are exceedingly simple compared to their often complex, abstract outputs.",
            "score": 119.40734100341797
        },
        {
            "docid": "21523_125",
            "document": "Artificial neural network . Many types of models are used, defined at different levels of abstraction and modeling different aspects of neural systems. They range from models of the short-term behavior of individual neurons, models of how the dynamics of neural circuitry arise from interactions between individual neurons and finally to models of how behavior can arise from abstract neural modules that represent complete subsystems. These include models of the long-term, and short-term plasticity, of neural systems and their relations to learning and memory from the individual neuron to the system level.",
            "score": 119.3355941772461
        },
        {
            "docid": "51682515_2",
            "document": "Critical brain hypothesis . In neuroscience, the critical brain hypothesis states that certain biological neuronal networks work near phase transitions. Experimental recordings from large groups of neurons have shown bursts of activity, so-called neuronal avalanches, with sizes that follow a power law distribution. These results, and subsequent replication on a number of settings, led to the hypothesis that the collective dynamics of large neuronal networks in the brain operates close to the critical point of a phase transition. According to this hypothesis, the activity of the brain would be continuously transitioning between two phases, one in which activity will rapidly reduce and die, and another where activity will build up and amplify over time. In criticality, the brain capacity for information processing is enhanced, so subcritical, critical and slightly supercritical branching process of thoughts could describe how human and animal minds function.",
            "score": 119.0485610961914
        },
        {
            "docid": "19685348_3",
            "document": "Eve Marder . Her work on the 30 neurons that comprise the lobster stomatogastric ganglion (STG) produced many notable findings. She found that circuits can be modulated by many neuromodulators, which act on the level of populations of neurons, unlike some neurotransmitters, which can only affect specific target neurons. She pioneered work on plasticity and homeostasis, revealing more about how the brain can change dramatically during learning and development yet remain structurally stable. Her recent work examining network variability among healthy individuals shows that a variety of network parameters can produce the same behavioral outcome, challenging a long-standing goal in theoretical neuroscience to model 'ideal' neurons and neural circuits.",
            "score": 118.17084503173828
        },
        {
            "docid": "33818014_8",
            "document": "Nervous system network models . On a high level representation, the neurons can be viewed as connected to other neurons to form a neural network in one of three ways. A specific network can be represented as a physiologically (or anatomically) connected network and modeled that way. There are several approaches to this (see Ascoli, G.A. (2002) Sporns, O. (2007), Connectionism, Rumelhart, J. L., McClelland, J. L., and PDP Research Group (1986), Arbib, M. A. (2007)). Or, it can form a functional network that serves a certain function and modeled accordingly (Honey, C. J., Kotter, R., Breakspear, R., & Sporns, O. (2007), Arbib, M. A. (2007)). A third way is to hypothesize a theory of the functioning of the biological components of the neural system by a mathematical model, in the form of a set of mathematical equations. The variables of the equation are some or all of the neurobiological properties of the entity being modeled, such as the dimensions of the dendrite or the stimulation rate of action potential along the axon in a neuron. The mathematical equations are solved using computational techniques and the results are validated with either simulation or experimental processes. This approach to modeling is called computational neuroscience. This methodology is used to model components from the ionic level to system level of the brain. This method is applicable for modeling integrated system of biological components that carry information signal from one neuron to another via intermediate active neurons that can pass the signal through or create new or additional signals. The computational neuroscience approach is extensively used and is based on two generic models, one of cell membrane potential Goldman (1943) and Hodgkin and Katz (1949), and the other based on Hodgkin-Huxley model of action potential (information signal).",
            "score": 117.94142150878906
        },
        {
            "docid": "21944_5",
            "document": "Nervous system . The central nervous system functions to send signals from one cell to others, or from one part of the body to others and to receive feedback. Malfunction of the nervous system can occur as a result of genetic defects, physical damage due to trauma or toxicity, infection or simply of ageing. The medical specialty of neurology studies disorders of the nervous system and looks for interventions that can prevent or treat them. In the peripheral nervous system, the most common problem is the failure of nerve conduction, which can be due to different causes including diabetic neuropathy and demyelinating disorders such as multiple sclerosis and amyotrophic lateral sclerosis. Neuroscience is the field of science that focuses on the study of the nervous system. The nervous system derives its name from nerves, which are cylindrical bundles of fibers (the axons of neurons), that emanate from the brain and spinal cord, and branch repeatedly to innervate every part of the body. Nerves are large enough to have been recognized by the ancient Egyptians, Greeks, and Romans, but their internal structure was not understood until it became possible to examine them using a microscope. The author Michael Nikoletseas wrote: \"It is difficult to believe that until approximately year 1900 it was not known that neurons are the basic units of the brain (Santiago Ram\u00f3n y Cajal). Equally surprising is the fact that the concept of chemical transmission in the brain was not known until around 1930 (Henry Hallett Dale and Otto Loewi). We began to understand the basic electrical phenomenon that neurons use in order to communicate among themselves, the action potential, in the 1950s (Alan Lloyd Hodgkin, Andrew Huxley and John Eccles). It was in the 1960s that we became aware of how basic neuronal networks code stimuli and thus basic concepts are possible (David H. Hubel and Torsten Wiesel). The molecular revolution swept across US universities in the 1980s. It was in the 1990s that molecular mechanisms of behavioral phenomena became widely known (Eric Richard Kandel).\" A microscopic examination shows that nerves consist primarily of axons, along with different membranes that wrap around them and segregate them into fascicles. The neurons that give rise to nerves do not lie entirely within the nerves themselves\u2014their cell bodies reside within the brain, spinal cord, or peripheral ganglia.",
            "score": 117.85885620117188
        },
        {
            "docid": "14408479_3",
            "document": "Biological neuron model . Neuron models can be divided into two categories according to the physical units of the interface of the model. Each category could be further divided according to the abstraction/detail level:  Although it is not unusual in science and engineering to have several descriptive models for different abstraction/detail levels, the number of different, sometimes contradicting, biological neuron models is exceptionally high. This situation is partly the result of the many different experimental settings, and the difficulty to separate the intrinsic properties of a single neuron from measurements effects and interactions of many cells (network effects). To accelerate the convergence to a unified theory, we list several models in each category, and where applicable, also references to supporting experiments.",
            "score": 117.6052474975586
        },
        {
            "docid": "226722_14",
            "document": "Functional magnetic resonance imaging . The physiological blood-flow response largely decides the temporal sensitivity, that is how accurately we can measure when neurons are active, in BOLD fMRI. The basic time resolution parameter (sampling time) is designated TR; the TR dictates how often a particular brain slice is excited and allowed to lose its magnetization. TRs could vary from the very short (500\u00a0ms) to the very long (3\u00a0s). For fMRI specifically, the hemodynamic response lasts over 10 seconds, rising multiplicatively (that is, as a proportion of current value), peaking at 4 to 6 seconds, and then falling multiplicatively. Changes in the blood-flow system, the vascular system, integrate responses to neuronal activity over time. Because this response is a smooth continuous function, sampling with ever-faster TRs does not help; it just gives more points on the response curve obtainable by simple linear interpolation anyway. Experimental paradigms such as staggering when a stimulus is presented at various trials can improve temporal resolution, but reduces the number of effective data points obtained.",
            "score": 117.54801940917969
        },
        {
            "docid": "33244792_15",
            "document": "Non-spiking neuron . By studying the nonspiking neuron, the field of neuroscience has benefited by having workable models that indicate how information is propagated through a neural network. This allows for the discussion of the factors that influence how networks work, and how they may be manipulated. Non-spiking neurons seem to be more sensitive to interference given that they exhibit graded potentials. So for non-spiking neurons, any stimulus will elicit a response, whereas spiking neurons exhibit action potentials which function as an \"all or none\" entity.",
            "score": 116.42422485351562
        },
        {
            "docid": "24219329_3",
            "document": "Neurogenomics . The nervous system in vertebrates is made up of two major types of cells \u2013 neuroglial cells and neurons. Hundreds of different types of neurons exist in humans, with varying functions \u2013 some of them process external stimuli; others generate a response to stimuli; others organize in centralized structures (brain, spinal ganglia) that are responsible for cognition, perception, and regulation of motor functions. Neurons in these centralized locations tend to organize in giant networks and communicate extensively with each other. Prior to the availability of expression arrays and DNA sequencing methodologies, researchers sought to understand the cellular behaviour of neurons (including synapse formation and neuronal development and regionalization in the human nervous system) in terms of the underlying molecular biology and biochemistry, without any understanding of the influence of a neuron\u2019s genome on its development and behaviour. As our understanding of the genome has expanded, the role of networks of gene interactions in the maintenance of neuronal function and behaviour has garnered interest in the neuroscience research community. Neurogenomics allows scientists to study the nervous system of organisms in the context of these underlying regulatory and transcriptional networks. This approach is distinct from neurogenetics, which emphasizes the role of single genes without a network-interaction context when studying the nervous system.",
            "score": 116.39588928222656
        },
        {
            "docid": "1648224_18",
            "document": "Granger causality . Different methods of obtaining some measure of information flow from the firing activities of a neuron and its surrounding ensemble have been explored in the past, but they are limited in the kinds of conclusions that can be drawn and provide little insight into the directional flow of information, its effect size, and how it can change with time. Recently Granger causality has been applied to address some of these issues with great success. Put plainly, one examines how to best predict the future of a neuron: using either the entire ensemble or the entire ensemble except a certain target neuron. If the prediction is made worse by excluding the target neuron, then we say it has a \u201cg-causal\u201d relationship with the current neuron.",
            "score": 115.954833984375
        },
        {
            "docid": "321869_55",
            "document": "Coding theory . Neural coding is a neuroscience-related field concerned with how sensory and other information is represented in the brain by networks of neurons. The main goal of studying neural coding is to characterize the relationship between the stimulus and the individual or ensemble neuronal responses and the relationship among electrical activity of the neurons in the ensemble. It is thought that neurons can encode both digital and analog information, and that neurons follow the principles of information theory and compress information, and detect and correct errors in the signals that are sent throughout the brain and wider nervous system.",
            "score": 115.83280181884766
        },
        {
            "docid": "1706303_41",
            "document": "Recurrent neural network . A multiple timescales recurrent neural network (MTRNN) is a neural-based computational model that can simulate the functional hierarchy of the brain through self-organization that depends on spatial connection between neurons and on distinct types of neuron activities, each with distinct time properties. With such varied neuronal activities, continuous sequences of any set of behaviors are segmented into reusable primitives, which in turn are flexibly integrated into diverse sequential behaviors. The biological approval of such a type of hierarchy was discussed in the memory-prediction theory of brain function by Hawkins in his book \"On Intelligence\".",
            "score": 115.26414489746094
        },
        {
            "docid": "3717_60",
            "document": "Brain . Computational neuroscience encompasses two approaches: first, the use of computers to study the brain; second, the study of how brains perform computation. On one hand, it is possible to write a computer program to simulate the operation of a group of neurons by making use of systems of equations that describe their electrochemical activity; such simulations are known as \"biologically realistic neural networks\". On the other hand, it is possible to study algorithms for neural computation by simulating, or mathematically analyzing, the operations of simplified \"units\" that have some of the properties of neurons but abstract out much of their biological complexity. The computational functions of the brain are studied both by computer scientists and neuroscientists.",
            "score": 115.14466857910156
        },
        {
            "docid": "3975854_2",
            "document": "Sensory neuroscience . Sensory neuroscience is a subfield of neuroscience which explores the anatomy and physiology of neurons that are part of sensory systems such as vision, hearing, and olfaction. Neurons in sensory regions of the brain respond to stimuli by firing one or more nerve impulses (action potentials) following stimulus presentation. How is information about the outside world encoded by the rate, timing, and pattern of action potentials? This so-called neural code is currently poorly understood and sensory neuroscience plays an important role in the attempt to decipher it. Looking at early sensory processing is advantageous since brain regions that are \"higher up\" (e.g. those involved in memory or emotion) contain neurons which encode more abstract representations. However, the hope is that there are unifying principles which govern how the brain encodes and processes information. Studying sensory systems is an important stepping stone in our understanding of brain function in general.",
            "score": 114.91195678710938
        }
    ]
}