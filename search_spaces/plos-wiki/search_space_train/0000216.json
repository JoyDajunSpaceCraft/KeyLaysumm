{
    "q": [
        {
            "docid": "12499410_26",
            "document": "Network motif . Wernicke introduced an algorithm named \"RAND-ESU\" that provides a significant improvement over \"mfinder\". This algorithm, which is based on the exact enumeration algorithm \"ESU\", has been implemented as an application called \"FANMOD\". \"RAND-ESU\" is a NM discovery algorithm applicable for both directed and undirected networks, effectively exploits an unbiased node sampling throughout the network, and prevents overcounting sub-graphs more than once. Furthermore, \"RAND-ESU\" uses a novel analytical approach called \"DIRECT\" for determining sub-graph significance instead of using an ensemble of random networks as a Null-model. The \"DIRECT\" method estimates the sub-graph concentration without explicitly generating random networks. Empirically, the DIRECT method is more efficient in comparison with the random network ensemble in case of sub-graphs with a very low concentration; however, the classical Null-model is faster than the \"DIRECT\" method for highly concentrated sub-graphs. In the following, we detail the \"ESU\" algorithm and then we show how this exact algorithm can be modified efficiently to \"RAND-ESU\" that estimates sub-graphs concentrations.",
            "score": 82.59891188144684
        },
        {
            "docid": "39269581_2",
            "document": "Variable elimination . Variable elimination (VE) is a simple and general exact inference algorithm in probabilistic graphical models, such as Bayesian networks and Markov random fields. It can be used for inference of maximum a posteriori (MAP) state or estimation of conditional or marginal distributions over a subset of variables. The algorithm has exponential time complexity, but could be efficient in practice for the low-treewidth graphs, if the proper elimination order is used.",
            "score": 79.84691643714905
        },
        {
            "docid": "12499410_20",
            "document": "Network motif . Kashtan \"et al.\" presented the first sampling NM discovery algorithm, which was based on \"edge sampling\" throughout the network. This algorithm estimates concentrations of induced sub-graphs and can be utilized for motif discovery in directed or undirected networks. The sampling procedure of the algorithm starts from an arbitrary edge of the network that leads to a sub-graph of size two, and then expands the sub-graph by choosing a random edge that is incident to the current sub-graph. After that, it continues choosing random neighboring edges until a sub-graph of size n is obtained. Finally, the sampled sub-graph is expanded to include all of the edges that exist in the network between these n nodes. When an algorithm uses a sampling approach, taking unbiased samples is the most important issue that the algorithm might address. The sampling procedure, however, does not take samples uniformly and therefore Kashtan \"et al.\" proposed a weighting scheme that assigns different weights to the different sub-graphs within network. The underlying principle of weight allocation is exploiting the information of the sampling probability for each sub-graph, i.e. the probable sub-graphs will obtain comparatively less weights in comparison to the improbable sub-graphs; hence, the algorithm must calculate the sampling probability of each sub-graph that has been sampled. This weighting technique assists \"mfinder\" to determine sub-graph concentrations impartially.",
            "score": 71.50324559211731
        },
        {
            "docid": "233488_28",
            "document": "Machine learning . A Bayesian network, belief network or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independencies via a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning.",
            "score": 69.91824102401733
        },
        {
            "docid": "10571004_4",
            "document": "Biological network inference . There is great interest in network medicine for the modelling biological systems. This article focuses on a necessary prerequisite to dynamic modeling of a network: inference of the topology, that is, prediction of the \"wiring diagram\" of the network. More specifically, we focus here on inference of biological network structure using the growing sets of high-throughput expression data for genes, proteins, and metabolites. Briefly, methods using high-throughput data for inference of regulatory networks rely on searching for patterns of partial correlation or conditional probabilities that indicate causal influence. Such patterns of partial correlations found in the high-throughput data, possibly combined with other supplemental data on the genes or proteins in the proposed networks, or combined with other information on the organism, form the basis upon which such algorithms work. Such algorithms can be of use in inferring the topology of any network where the change in state of one node can affect the state of other nodes.",
            "score": 100.36706686019897
        },
        {
            "docid": "800010_5",
            "document": "Belief propagation . However, this quickly becomes computationally prohibitive: if there are 100 binary variables, then one needs to sum over 2\u00a0\u2248\u00a06.338\u00a0\u00d7\u00a010 possible values. By exploiting the polytree structure, belief propagation allows the marginals to be computed much more efficiently. Variants of the belief propagation algorithm exist for several types of graphical models (Bayesian networks and Markov random fields in particular). We describe here the variant that operates on a factor graph. A factor graph is a bipartite graph containing nodes corresponding to variables \"V\" and factors \"F\", with edges between variables and the factors in which they appear. We can write the joint mass function:",
            "score": 61.490015745162964
        },
        {
            "docid": "3259720_7",
            "document": "Multifactor dimensionality reduction . As illustrated above, the basic constructive induction algorithm in MDR is very simple. However, its implementation for mining patterns from real data can be computationally complex. As with any machine learning algorithm there is always concern about overfitting. That is, machine learning algorithms are good at finding patterns in completely random data. It is often difficult to determine whether a reported pattern is an important signal or just chance. One approach is to estimate the generalizability of a model to independent datasets using methods such as cross-validation. Models that describe random data typically don't generalize. Another approach is to generate many random permutations of the data to see what the data mining algorithm finds when given the chance to overfit. Permutation testing makes it possible to generate an empirical p-value for the result. Replication in independent data may also provide evidence for an MDR model but can be sensitive to difference in the data sets. These approaches have all been shown to be useful for choosing and evaluating MDR models. An important step in an machine learning exercise is interpretation. Several approaches have been used with MDR including entropy analysis and pathway analysis. Tips and approaches for using MDR to model gene-gene interactions have been reviewed.",
            "score": 83.95817875862122
        },
        {
            "docid": "29735189_2",
            "document": "Graph-tool . graph-tool is a Python module for manipulation and statistical analysis of graphs (AKA networks). The core data structures and algorithms of graph-tool are implemented in C++, making extensive use of metaprogramming, based heavily on the Boost Graph Library. This type of approach can confer a level of performance which is comparable (both in memory usage and computation time) to that of a pure C++ library, which can be several orders of magnitude better than pure Python.",
            "score": 50.20327162742615
        },
        {
            "docid": "12499410_21",
            "document": "Network motif . In expanded to include sharp contrast to exhaustive search, the computational time of the algorithm surprisingly is asymptotically independent of the network size. An analysis of the computational time of the algorithm has shown that it takes for each sample of a sub-graph of size from the network. On the other hand, there is no analysis in on the classification time of sampled sub-graphs that requires solving the \"graph isomorphism\" problem for each sub-graph sample. Additionally, an extra computational effort is imposed on the algorithm by the sub-graph weight calculation. But it is unavoidable to say that the algorithm may sample the same sub-graph multiple times \u2013 spending time without gathering any information. In conclusion, by taking the advantages of sampling, the algorithm performs more efficiently than an exhaustive search algorithm; however, it only determines sub-graphs concentrations approximately. This algorithm can find motifs up to size 6 because of its main implementation, and as result it gives the most significant motif, not all the others too. Also, it is necessary to mention that this tool has no option of visual presentation. The sampling algorithm is shown briefly:",
            "score": 62.81187403202057
        },
        {
            "docid": "470752_40",
            "document": "Expectation\u2013maximization algorithm . which contains the log-EM algorithm as its subclass. Thus, the \u03b1-EM algorithm by Yasuo Matsuyama is an exact generalization of the log-EM algorithm. No computation of gradient or Hessian matrix is needed. The \u03b1-EM shows faster convergence than the log-EM algorithm by choosing an appropriate \u03b1. The \u03b1-EM algorithm leads to a faster version of the Hidden Markov model estimation algorithm \u03b1-HMM. EM is a partially non-Bayesian, maximum likelihood method. Its final result gives a probability distribution over the latent variables (in the Bayesian style) together with a point estimate for \"\u03b8\" (either a maximum likelihood estimate or a posterior mode). A fully Bayesian version of this may be wanted, giving a probability distribution over \"\u03b8\" and the latent variables. The Bayesian approach to inference is simply to treat \"\u03b8\" as another latent variable. In this paradigm, the distinction between the E and M steps disappears. If using the factorized Q approximation as described above (variational Bayes), solving can iterate over each latent variable (now including \"\u03b8\") and optimize them one at a time. Now, \"k\" steps per iteration are needed, where \"k\" is the number of latent variables. For graphical models this is easy to do as each variable's new \"Q\" depends only on its Markov blanket, so local message passing can be used for efficient inference.",
            "score": 75.14044070243835
        },
        {
            "docid": "27821585_3",
            "document": "Clark Glymour . Glymour is the founder of the Philosophy Department at Carnegie Mellon University, a Guggenheim Fellow, a Fellow of the Center for Advanced Study in Behavioral Sciences a Phi Beta Kappa lecturer, and is a Fellow of the statistics section of the AAAS. Glymour and his collaborators created the causal interpretation of Bayes nets. His areas of interest include epistemology (particularly Android epistemology), machine learning, automated reasoning, psychology of judgment, and mathematical psychology. One of Glymour's main contributions to the philosophy of science is in the area of Bayesian probability, particularly in his analysis of the Bayesian \"problem of old evidence\". Glymour, in collaboration with Peter Spirtes and Richard Scheines, also developed an automated causal inference algorithm implemented as software named TETRAD. Using multivariate statistical data as input, TETRAD rapidly searches from among all possible causal relationship models and returns the most plausible causal models based on conditional dependence relationships between those variables. The algorithm is based on principles from statistics, graph theory, philosophy of science, and artificial intelligence.",
            "score": 62.30227446556091
        },
        {
            "docid": "1323985_24",
            "document": "Markov random field . As in a Bayesian network, one may calculate the conditional distribution of a set of nodes formula_53 given values to another set of nodes formula_54 in the Markov random field by summing over all possible assignments to formula_55; this is called exact inference. However, exact inference is a #P-complete problem, and thus computationally intractable in the general case. Approximation techniques such as Markov chain Monte Carlo and loopy belief propagation are often more feasible in practice. Some particular subclasses of MRFs, such as trees (see Chow\u2013Liu tree), have polynomial-time inference algorithms; discovering such subclasses is an active research topic. There are also subclasses of MRFs that permit efficient MAP, or most likely assignment, inference; examples of these include associative networks. Another interesting sub-class is the one of decomposable models (when the graph is chordal): having a closed-form for the MLE, it is possible to discover a consistent structure for hundreds of variables.",
            "score": 62.33481502532959
        },
        {
            "docid": "28961424_18",
            "document": "Network Science CTA . The overall approach in EDIN is to mathematically characterize the rich interactions between composite networks (which may comprise multiple networks with varying levels of coupling between them), and the dynamics that occur within each network or across networks. Examples of key technical approaches within EDIN include the development of formal models for reasoning about interacting networks; development of a theory of composite graphs for modeling interacting networks; modeling and analysis of group behaviors using techniques ranging beyond traditional graph theory; development of community discovery algorithms; characterization of temporal graph properties; development of mathematically tractable tactical mobility models; development of theories of co-evolution of interacting networks, etc.",
            "score": 43.11015868186951
        },
        {
            "docid": "105837_5",
            "document": "Generic programming . For example, given \"N\" sequence data structures, e.g. singly linked list, vector etc., and \"M\" algorithms to operate on them, e.g. find, sort etc., a direct approach would implement each algorithm specifically for each data structure, giving combinations to implement. However, in the generic programming approach, each data structure returns a model of an iterator concept (a simple value type that can be dereferenced to retrieve the current value, or changed to point to another value in the sequence) and each algorithm is instead written generically with arguments of such iterators, e.g. a pair of iterators pointing to the beginning and end of the subsequence to process. Thus, only data structure-algorithm combinations need be implemented. Several iterator concepts are specified in the STL, each a refinement of more restrictive concepts e.g. forward iterators only provide movement to the next value in a sequence (e.g. suitable for a singly linked list or a stream of input data), whereas a random-access iterator also provides direct constant-time access to any element of the sequence (e.g. suitable for a vector). An important point is that a data structure will return a model of the most general concept that can be implemented efficiently\u2014computational complexity requirements are explicitly part of the concept definition. This limits the data structures a given algorithm can be applied to and such complexity requirements are a major determinant of data structure choice. Generic programming similarly has been applied in other domains, e.g. graph algorithms.",
            "score": 67.60617578029633
        },
        {
            "docid": "4194205_5",
            "document": "Hybrid algorithm (constraint satisfaction) . On some kinds of problems, efficient and complete inference algorithms exist. For example, problems whose primal or dual graphs are trees or forests can be solved in polynomial time. This affect the choice of the variables evaluated by search. Indeed, once a variable is evaluated, it can effectively removed from the graph, restricting all constraints it is involved with its value. Alternatively, an evaluated variable can be replaced by a number of distinct variables, one for each constraint, all having a single-value domain. This mixed algorithm is efficient if the search variables are chosen so that duplicating or deleting them turns the problem into one that can be efficiently solved by inference. In particular, if these variables form a cycle cutset of the graph of the problem, inference is efficient because it has to solve a problem whose graph is a tree or, more generally, a forest. Such an algorithm is as follows:",
            "score": 64.10722935199738
        },
        {
            "docid": "233488_23",
            "document": "Machine learning . An artificial neural network (ANN) learning algorithm, usually called \"neural network\" (NN), is a learning algorithm that is vaguely inspired by biological neural networks. Computations are structured in terms of an interconnected group of artificial neurons, processing information using a connectionist approach to computation. Modern neural networks are non-linear statistical data modeling tools. They are usually used to model complex relationships between inputs and outputs, to find patterns in data, or to capture the statistical structure in an unknown joint probability distribution between observed variables.",
            "score": 69.05197644233704
        },
        {
            "docid": "3062637_45",
            "document": "Estimation of distribution algorithm . The BOA uses Bayesian networks to model and sample promising solutions. Bayesian networks are directed acyclic graphs, with nodes representing variables and edges representing conditional probabilities between pair of variables. The value of a variable formula_63 can be conditioned on a maximum of formula_64 other variables, defined in formula_33. BOA builds a PGM encoding a factorized joint distribution, in which the parameters of the network, i.e. the conditional probabilities, are estimated from the selected population using the maximum likelihood estimator.",
            "score": 64.25312900543213
        },
        {
            "docid": "11469413_3",
            "document": "JUNG . JUNG's architecture is designed to support a variety of representations of entities and their relations, such as directed and undirected graphs, , graphs with parallel edges, and hypergraphs. It provides a mechanism for annotating graphs, entities, and relations with metadata. JUNG also facilitates the creation of analytic tools for complex data sets that can examine the relations between entities as well as the metadata attached to each entity and relation. JUNG includes implementations of a number of algorithms from graph theory, data mining, and social network analysis, such as routines for clustering, , , random graph generation, statistical analysis, and calculation of network distances, flows, and importance measures.",
            "score": 56.95303153991699
        },
        {
            "docid": "10571004_5",
            "document": "Biological network inference . Genes are the nodes and the edges are directed. A gene serves as the source of a direct regulatory edge to a target gene by producing an RNA or protein molecule that functions as a transcriptional activator or inhibitor of the target gene. If the gene is an activator, then it is the source of a positive regulatory connection; if an inhibitor, then it is the source of a negative regulatory connection. Computational algorithms take as primary input data measurements of mRNA expression levels of the genes under consideration for inclusion in the network, returning an estimate of the network topology. Such algorithms are typically based on linearity, independence or normality assumptions, which must be verified on a case-by-case basis. Clustering or some form of statistical classification is typically employed to perform an initial organization of the high-throughput mRNA expression values derived from microarray experiments, in particular to select sets of genes as candidates for network nodes. The question then arises: how can the clustering or classification results be connected to the underlying biology? Such results can be useful for pattern classification \u2013 for example, to classify subtypes of cancer, or to predict differential responses to a drug (pharmacogenomics). But to understand the relationships between the genes, that is, to more precisely define the influence of each gene on the others, the scientist typically attempts to reconstruct the transcriptional regulatory network. This can be done by data integration in dynamic models supported by background literature, or information in public databases, combined with the clustering results. The modelling can be done by a Boolean network, by Ordinary differential equations or Linear regression models, e.g. Least-angle regression, by Bayesian network or based on Information theory approaches. For instance it can be done by the application of a correlation-based inference algorithm, as will be discussed below, an approach which is having increased success as the size of the available microarray sets keeps increasing",
            "score": 82.64156675338745
        },
        {
            "docid": "3062637_24",
            "document": "Estimation of distribution algorithm . Bivariate and multivariate distributions are usually represented as Probabilistic Graphical Models (graphs), in which edges denote statistical dependencies (or conditional probabilities) and vertices denote variables. To learn the structure of a PGM from data linkage-learning is employed.",
            "score": 68.86002361774445
        },
        {
            "docid": "41224221_2",
            "document": "Weighted correlation network analysis . Weighted correlation network analysis, also known as weighted gene co-expression network analysis (WGCNA), is a widely used data mining method especially for studying biological networks based on pairwise correlations between variables. While it can be applied to most high-dimensional data sets, it has been most widely used in genomic applications. It allows one to define modules (clusters), intramodular hubs, and network nodes with regard to module membership, to study the relationships between co-expression modules, and to compare the network topology of different networks (differential network analysis). WGCNA can be used as a data reduction technique (related to oblique factor analysis ), as a clustering method (fuzzy clustering), as a feature selection method (e.g. as gene screening method), as a framework for integrating complementary (genomic) data (based on weighted correlations between quantitative variables), and as a data exploratory technique. Although WGCNA incorporates traditional data exploratory techniques, its intuitive network language and analysis framework transcend any standard analysis technique. Since it uses network methodology and is well suited for integrating complementary genomic data sets, it can be interpreted as systems biologic or systems genetic data analysis method. By selecting intramodular hubs in consensus modules, WGCNA also gives rise to network based meta analysis techniques.",
            "score": 70.40604817867279
        },
        {
            "docid": "38722262_6",
            "document": "Ayasdi . Ayasdi is a machine intelligence platform. It includes dozens of statistical and both supervised and unsupervised machine learning algorithms and can be extended to include whatever algorithms are required for a particular class of analysis. The platform is extensively automated and is in production at scale at many global 100 companies and at governments in the world. It features Topological Data Analysis as a unifying analytical framework, which automatically calculates groupings and similarity across large and highly dimensional data sets, generating network maps with greatly assist analysts in understanding how data clusters and which variables are relevant. When compared with manual approaches to statistical analysis and machine learning, results with Ayasdi will typically be achieved much faster to achieve and more accurate due to the automation and scalability built into the platform. The Ayasdi platform also develops mathematical models, including predictive models, based on the results of the analysis. This allows Ayasdi to deployed as an operational system, or as a part of operational systems, and not just for analysis.",
            "score": 59.11311674118042
        },
        {
            "docid": "29467449_19",
            "document": "Protein function prediction . Several networks based on different data sources can be combined into a composite network, which can then be used by a prediction algorithm to annotate candidate genes or proteins. For example, the developers of the bioPIXIE system used a wide variety of \"Saccharomyces cerevisiae\" (yeast) genomic data to produce a composite functional network for that species. This resource allows the visualization of known networks representing biological processes, as well as the prediction of novel components of those networks. Many algorithms have been developed to predict function based on the integration of several data sources (e.g. genomic, proteomic, protein interaction, etc.), and testing on previously annotated genes indicates a high level of accuracy. Disadvantages of some function prediction algorithms have included a lack of accessibility, and the time required for analysis. Faster, more accurate algorithms such as GeneMANIA (multiple association network integration algorithm) have however been developed in recent years and are publicly available on the web, indicating the future direction of function prediction.",
            "score": 72.20409381389618
        },
        {
            "docid": "470752_11",
            "document": "Expectation\u2013maximization algorithm . The typical models to which EM is applied use formula_2 as a latent variable indicating membership in one of a set of groups: However, it is possible to apply EM to other sorts of models. The motive is as follows. If the value of the parameters formula_3 is known, usually the value of the latent variables formula_2 can be found by maximizing the log-likelihood over all possible values of formula_2, either simply by iterating over formula_2 or through an algorithm such as the Viterbi algorithm for hidden Markov models. Conversely, if we know the value of the latent variables formula_2, we can find an estimate of the parameters formula_3 fairly easily, typically by simply grouping the observed data points according to the value of the associated latent variable and averaging the values, or some function of the values, of the points in each group. This suggests an iterative algorithm, in the case where both formula_3 and formula_2 are unknown: The algorithm as just described monotonically approaches a local minimum of the cost function.",
            "score": 62.52686929702759
        },
        {
            "docid": "12499410_33",
            "document": "Network motif . As it is mentioned above, the symmetry-breaking technique is a simple mechanism that precludes spending time finding a sub-graph more than once due to its symmetries. Note that, computing symmetry-breaking conditions requires finding all automorphisms of a given query graph. Even though, there is no efficient (or polynomial time) algorithm for the graph automorphism problem, this problem can be tackled efficiently in practice by McKay\u2019s tools. As it is claimed, using symmetry-breaking conditions in NM detection lead to save a great deal of running time. Moreover, it can be inferred from the results in that using the symmetry-breaking conditions results in high efficiency particularly for directed networks in comparison to undirected networks. The symmetry-breaking conditions used in the GK algorithm are similar to the restriction which \"ESU\" algorithm applies to the labels in EXT and SUB sets. In conclusion, the GK algorithm computes the exact number of appearance of a given query graph in a large complex network and exploiting symmetry-breaking conditions improves the algorithm performance. Also, GK algorithm is one of the known algorithms having no limitation for motif size in implementation and potentially it can find motifs of any size.",
            "score": 79.23669767379761
        },
        {
            "docid": "1164_49",
            "document": "Artificial intelligence . Bayesian networks are a very general tool that can be used for a large number of problems: reasoning (using the Bayesian inference algorithm), learning (using the expectation-maximization algorithm), planning (using decision networks) and perception (using dynamic Bayesian networks). Probabilistic algorithms can also be used for filtering, prediction, smoothing and finding explanations for streams of data, helping perception systems to analyze processes that occur over time (e.g., hidden Markov models or Kalman filters). Compared with symbolic logic, formal Bayesian inference is computationally expensive. For inference to be tractable, most observations must be conditionally independent of one another. Complicated graphs with diamonds or other \"loops\" (undirected cycles) can require a sophisticated method such as Markov Chain Monte Carlo, which spreads an ensemble of random walkers throughout the Bayesian network and attempts to converge to an assessment of the conditional probabilities. Bayesian networks are used on XBox Live to rate and match players; wins and losses are \"evidence\" of how good a player is. AdSense uses a Bayesian network with over 300 million edges to learn which ads to serve.",
            "score": 64.41484093666077
        },
        {
            "docid": "4118276_9",
            "document": "Conditional random field . In sequence modeling, the graph of interest is usually a chain graph. An input sequence of observed variables formula_20 represents a sequence of observations and formula_21 represents a hidden (or unknown) state variable that needs to be inferred given the observations.  The formula_22 are structured to form a chain, with an edge between each formula_23 and formula_22. As well as having a simple interpretation of the formula_22 as \"labels\" for each element in the input sequence, this layout admits efficient algorithms for:",
            "score": 56.33159589767456
        },
        {
            "docid": "12499410_28",
            "document": "Network motif . How has the exact \"ESU\" been algorithm modified to \"RAND-ESU\" that estimates sub-graph concentrations? The procedure of implementing \"RAND-ESU\" is quite straightforward and is one of the main advantages of \"FANMOD\". One can change the \"ESU\" algorithm to explore just a portion of the ESU-Tree leaves by applying a probability value for each level of the ESU-Tree and oblige \"ESU\" to traverse each child node of a node in level with probability . This new algorithm is called \"RAND-ESU\". Evidently, when for all levels, \"RAND-ESU\" acts like \"ESU\". For the algorithm finds nothing. Note that, this procedure ensures that the chances of visiting each leaf of the ESU-Tree are the same, resulting in \"unbiased\" sampling of sub-graphs through the network. The probability of visiting each leaf is and this is identical for all of the ESU-Tree leaves; therefore, this method guarantees unbiased sampling of sub-graphs from the network. Nonetheless, determining the value of for is another issue that must be determined manually by an expert to get precise results of sub-graph concentrations. While there is no lucid prescript for this matter, the Wernicke provides some general observations that may help in determining p_d values. In summary, \"RAND-ESU\" is a very fast algorithm for NM discovery in the case of induced sub-graphs supporting unbiased sampling method. Although, the main \"ESU\" algorithm and so the \"FANMOD\" tool is known for discovering induced sub-graphs, there is trivial modification to \"ESU\" which makes it possible for finding non-induced sub-graphs, too. The pseudo code of \"ESU (FANMOD)\" is shown below:",
            "score": 63.70082926750183
        },
        {
            "docid": "775_67",
            "document": "Algorithm . The analysis and study of algorithms is a discipline of computer science, and is often practiced abstractly without the use of a specific programming language or implementation. In this sense, algorithm analysis resembles other mathematical disciplines in that it focuses on the underlying properties of the algorithm and not on the specifics of any particular implementation. Usually pseudocode is used for analysis as it is the simplest and most general representation. However, ultimately, most algorithms are usually implemented on particular hardware / software platforms and their algorithmic efficiency is eventually put to the test using real code. For the solution of a \"one off\" problem, the efficiency of a particular algorithm may not have significant consequences (unless n is extremely large) but for algorithms designed for fast interactive, commercial or long life scientific usage it may be critical. Scaling from small n to large n frequently exposes inefficient algorithms that are otherwise benign.",
            "score": 70.43406510353088
        },
        {
            "docid": "12499410_27",
            "document": "Network motif . The algorithms \"ESU\" and \"RAND-ESU\" are fairly simple, and hence easy to implement. \"ESU\" first finds the set of all induced sub-graphs of size , let be this set. \"ESU\" can be implemented as a recursive function; the running of this function can be displayed as a tree-like structure of depth , called the ESU-Tree (see figure). Each of the ESU-Tree nodes indicate the status of the recursive function that entails two consecutive sets SUB and EXT. SUB refers to nodes in the target network that are adjacent and establish a partial sub-graph of size . If , the algorithm has found an induced complete sub-graph, so . However, if , the algorithm must expand SUB to achieve cardinality . This is done by the EXT set that contains all the nodes that satisfy two conditions: First, each of the nodes in EXT must be adjacent to at least one of the nodes in SUB; second, their numerical labels must be larger than the label of first element in SUB. The first condition makes sure that the expansion of SUB nodes yields a connected graph and the second condition causes ESU-Tree leaves (see figure) to be distinct; as a result, it prevents overcounting. Note that, the EXT set is not a static set, so in each step it may expand by some new nodes that do not breach the two conditions. The next step of ESU involves classification of sub-graphs placed in the ESU-Tree leaves into non-isomorphic size- graph classes; consequently, ESU determines sub-graphs frequencies and concentrations. This stage has been implemented simply by employing McKay\u2019s \"nauty\" algorithm, which classifies each sub-graph by performing a graph isomorphism test. Therefore, ESU finds the set of all induced -size sub-graphs in a target graph by a recursive algorithm and then determines their frequency using an efficient tool.",
            "score": 65.90430390834808
        },
        {
            "docid": "2007748_2",
            "document": "Structural equation modeling . Structural equation modeling (SEM) includes a diverse set of mathematical models, computer algorithms, and statistical methods that fit networks of constructs to data. SEM includes confirmatory factor analysis, path analysis, partial least squares path modeling, and latent growth modeling. The concept should not be confused with the related concept of structural models in econometrics, nor with structural models in economics. Structural equation models are often used to assess unobservable 'latent' constructs. They often invoke a measurement model that defines latent variables using one or more observed variables, and a structural model that imputes relationships between latent variables. The links between constructs of a structural equation model may be estimated with independent regression equations or through more involved approaches such as those employed in LISREL.",
            "score": 59.282851219177246
        },
        {
            "docid": "771965_14",
            "document": "Strassen algorithm . Practical implementations of Strassen's algorithm switch to standard methods of matrix multiplication for small enough submatrices, for which those algorithms are more efficient. The particular crossover point for which Strassen's algorithm is more efficient depends on the specific implementation and hardware. Earlier authors had estimated that Strassen's algorithm is faster for matrices with widths from 32 to 128 for optimized implementations. However, it has been observed that this crossover point has been increasing in recent years, and a 2010 study found that even a single step of Strassen's algorithm is often not beneficial on current architectures, compared to a highly optimized traditional multiplication, until matrix sizes exceed 1000 or more, and even for matrix sizes of several thousand the benefit is typically marginal at best (around 10% or less). A more recent study (2016) observed benefits for matrices as small as 512 and a benefit around 20%.",
            "score": 78.63867473602295
        }
    ],
    "r": [
        {
            "docid": "10571004_4",
            "document": "Biological network inference . There is great interest in network medicine for the modelling biological systems. This article focuses on a necessary prerequisite to dynamic modeling of a network: inference of the topology, that is, prediction of the \"wiring diagram\" of the network. More specifically, we focus here on inference of biological network structure using the growing sets of high-throughput expression data for genes, proteins, and metabolites. Briefly, methods using high-throughput data for inference of regulatory networks rely on searching for patterns of partial correlation or conditional probabilities that indicate causal influence. Such patterns of partial correlations found in the high-throughput data, possibly combined with other supplemental data on the genes or proteins in the proposed networks, or combined with other information on the organism, form the basis upon which such algorithms work. Such algorithms can be of use in inferring the topology of any network where the change in state of one node can affect the state of other nodes.",
            "score": 100.36705780029297
        },
        {
            "docid": "3259720_7",
            "document": "Multifactor dimensionality reduction . As illustrated above, the basic constructive induction algorithm in MDR is very simple. However, its implementation for mining patterns from real data can be computationally complex. As with any machine learning algorithm there is always concern about overfitting. That is, machine learning algorithms are good at finding patterns in completely random data. It is often difficult to determine whether a reported pattern is an important signal or just chance. One approach is to estimate the generalizability of a model to independent datasets using methods such as cross-validation. Models that describe random data typically don't generalize. Another approach is to generate many random permutations of the data to see what the data mining algorithm finds when given the chance to overfit. Permutation testing makes it possible to generate an empirical p-value for the result. Replication in independent data may also provide evidence for an MDR model but can be sensitive to difference in the data sets. These approaches have all been shown to be useful for choosing and evaluating MDR models. An important step in an machine learning exercise is interpretation. Several approaches have been used with MDR including entropy analysis and pathway analysis. Tips and approaches for using MDR to model gene-gene interactions have been reviewed.",
            "score": 83.95817565917969
        },
        {
            "docid": "10571004_5",
            "document": "Biological network inference . Genes are the nodes and the edges are directed. A gene serves as the source of a direct regulatory edge to a target gene by producing an RNA or protein molecule that functions as a transcriptional activator or inhibitor of the target gene. If the gene is an activator, then it is the source of a positive regulatory connection; if an inhibitor, then it is the source of a negative regulatory connection. Computational algorithms take as primary input data measurements of mRNA expression levels of the genes under consideration for inclusion in the network, returning an estimate of the network topology. Such algorithms are typically based on linearity, independence or normality assumptions, which must be verified on a case-by-case basis. Clustering or some form of statistical classification is typically employed to perform an initial organization of the high-throughput mRNA expression values derived from microarray experiments, in particular to select sets of genes as candidates for network nodes. The question then arises: how can the clustering or classification results be connected to the underlying biology? Such results can be useful for pattern classification \u2013 for example, to classify subtypes of cancer, or to predict differential responses to a drug (pharmacogenomics). But to understand the relationships between the genes, that is, to more precisely define the influence of each gene on the others, the scientist typically attempts to reconstruct the transcriptional regulatory network. This can be done by data integration in dynamic models supported by background literature, or information in public databases, combined with the clustering results. The modelling can be done by a Boolean network, by Ordinary differential equations or Linear regression models, e.g. Least-angle regression, by Bayesian network or based on Information theory approaches. For instance it can be done by the application of a correlation-based inference algorithm, as will be discussed below, an approach which is having increased success as the size of the available microarray sets keeps increasing",
            "score": 82.64156341552734
        },
        {
            "docid": "12499410_26",
            "document": "Network motif . Wernicke introduced an algorithm named \"RAND-ESU\" that provides a significant improvement over \"mfinder\". This algorithm, which is based on the exact enumeration algorithm \"ESU\", has been implemented as an application called \"FANMOD\". \"RAND-ESU\" is a NM discovery algorithm applicable for both directed and undirected networks, effectively exploits an unbiased node sampling throughout the network, and prevents overcounting sub-graphs more than once. Furthermore, \"RAND-ESU\" uses a novel analytical approach called \"DIRECT\" for determining sub-graph significance instead of using an ensemble of random networks as a Null-model. The \"DIRECT\" method estimates the sub-graph concentration without explicitly generating random networks. Empirically, the DIRECT method is more efficient in comparison with the random network ensemble in case of sub-graphs with a very low concentration; however, the classical Null-model is faster than the \"DIRECT\" method for highly concentrated sub-graphs. In the following, we detail the \"ESU\" algorithm and then we show how this exact algorithm can be modified efficiently to \"RAND-ESU\" that estimates sub-graphs concentrations.",
            "score": 82.59891510009766
        },
        {
            "docid": "39269581_2",
            "document": "Variable elimination . Variable elimination (VE) is a simple and general exact inference algorithm in probabilistic graphical models, such as Bayesian networks and Markov random fields. It can be used for inference of maximum a posteriori (MAP) state or estimation of conditional or marginal distributions over a subset of variables. The algorithm has exponential time complexity, but could be efficient in practice for the low-treewidth graphs, if the proper elimination order is used.",
            "score": 79.84691619873047
        },
        {
            "docid": "12499410_33",
            "document": "Network motif . As it is mentioned above, the symmetry-breaking technique is a simple mechanism that precludes spending time finding a sub-graph more than once due to its symmetries. Note that, computing symmetry-breaking conditions requires finding all automorphisms of a given query graph. Even though, there is no efficient (or polynomial time) algorithm for the graph automorphism problem, this problem can be tackled efficiently in practice by McKay\u2019s tools. As it is claimed, using symmetry-breaking conditions in NM detection lead to save a great deal of running time. Moreover, it can be inferred from the results in that using the symmetry-breaking conditions results in high efficiency particularly for directed networks in comparison to undirected networks. The symmetry-breaking conditions used in the GK algorithm are similar to the restriction which \"ESU\" algorithm applies to the labels in EXT and SUB sets. In conclusion, the GK algorithm computes the exact number of appearance of a given query graph in a large complex network and exploiting symmetry-breaking conditions improves the algorithm performance. Also, GK algorithm is one of the known algorithms having no limitation for motif size in implementation and potentially it can find motifs of any size.",
            "score": 79.2366943359375
        },
        {
            "docid": "11512_33",
            "document": "Fast Fourier transform . All of the FFT algorithms discussed above compute the DFT exactly (i.e. neglecting floating-point errors). A few \"FFT\" algorithms have been proposed, however, that compute the DFT \"approximately\", with an error that can be made arbitrarily small at the expense of increased computations. Such algorithms trade the approximation error for increased speed or other properties. For example, an approximate FFT algorithm by Edelman et al. (1999) achieves lower communication requirements for parallel computing with the help of a fast multipole method. A wavelet-based approximate FFT by Guo and Burrus (1996) takes sparse inputs/outputs (time/frequency localization) into account more efficiently than is possible with an exact FFT. Another algorithm for approximate computation of a subset of the DFT outputs is due to Shentov et al. (1995). The Edelman algorithm works equally well for sparse and non-sparse data, since it is based on the compressibility (rank deficiency) of the Fourier matrix itself rather than the compressibility (sparsity) of the data. Conversely, if the data are sparse\u2014that is, if only \"K\" out of \"N\" Fourier coefficients are nonzero\u2014then the complexity can be reduced to O(\"K\"\u00a0log(\"N\")log(\"N\"/\"K\")), and this has been demonstrated to lead to practical speedups compared to an ordinary FFT for \"N\"/\"K\"\u00a0>\u00a032 in a large-\"N\" example (\"N\"\u00a0=\u00a02) using a probabilistic approximate algorithm (which estimates the largest \"K\" coefficients to several decimal places).",
            "score": 78.79971313476562
        },
        {
            "docid": "771965_14",
            "document": "Strassen algorithm . Practical implementations of Strassen's algorithm switch to standard methods of matrix multiplication for small enough submatrices, for which those algorithms are more efficient. The particular crossover point for which Strassen's algorithm is more efficient depends on the specific implementation and hardware. Earlier authors had estimated that Strassen's algorithm is faster for matrices with widths from 32 to 128 for optimized implementations. However, it has been observed that this crossover point has been increasing in recent years, and a 2010 study found that even a single step of Strassen's algorithm is often not beneficial on current architectures, compared to a highly optimized traditional multiplication, until matrix sizes exceed 1000 or more, and even for matrix sizes of several thousand the benefit is typically marginal at best (around 10% or less). A more recent study (2016) observed benefits for matrices as small as 512 and a benefit around 20%.",
            "score": 78.638671875
        },
        {
            "docid": "8402086_18",
            "document": "Computational neurogenetic modeling . Because the amount of data on the interplay of genes and neurons and their effects is not enough to construct a rigorous model,  evolutionary computation is used to optimize artificial neural networks and gene regulatory networks, a common technique being the genetic algorithm. A genetic algorithm is a process that can be used to refine models by mimicking the process of natural selection observed in biological ecosystems. The primary advantages are that, due to not requiring derivative information, it can be applied to black box problems and multimodal optimization. The typical process for using genetic algorithms to refine a gene  regulatory network is: first, create a population; next, to create offspring via a crossover operation and  evaluate their fitness; then, on a group chosen for high fitness, simulate mutation via a mutation operator;  finally, taking the now mutated group, repeat this process until a desired level of fitness is demonstrated. Methods by which artificial neural networks may alter their structure without simulated mutation and fitness selection have been developed. A dynamically evolving neural network is one approach, as the creation of new connections and new neurons can  be modeled as the system adapts to new data. This enables the network to evolve in modeling accuracy without simulated natural selection. One method by which dynamically evolving networks may be optimized, called evolving layer neuron aggregation, combines neurons with sufficiently similar input weights into one neuron. This can take place during the training of the network, referred to as online aggregation, or between periods of training, referred to as offline aggregation. Experiments have suggested that offline aggregation is more efficient.",
            "score": 78.08897399902344
        },
        {
            "docid": "6871860_3",
            "document": "MUSIC (algorithm) . In many practical signal processing problems, the objective is to estimate from measurements a set of constant parameters upon which the received signals depend. There have been several approaches to such problems including the so-called maximum likelihood (ML) method of Capon (1969) and Burg's maximum entropy (ME) method. Although often successful and widely used, these methods have certain fundamental limitations (especially bias and sensitivity in parameter estimates), largely because they use an incorrect model (e.g., AR rather than special ARMA) of the measurements. Pisarenko (1973) was one of the first to exploit the structure of the data model, doing so in the context of estimation of parameters of complex sinusoids in additive noise using a covariance approach. Schmidt (1977), while working at of Northrop Grumman) and independently (1979) were the first to correctly exploit the measurement model in the case of sensor arrays of arbitrary form. Schmidt, in particular, accomplished this by first deriving a complete geometric solution in the absence of noise, then cleverly extending the geometric concepts to obtain a reasonable approximate solution in the presence of noise. The resulting algorithm was called MUSIC (MUltiple SIgnal Classification) and has been widely studied. In a detailed evaluation based on thousands of simulations, the Massachusetts Institute of Technology's Lincoln Laboratory concluded that, among currently accepted high-resolution algorithms, MUSIC was the most promising and a leading candidate for further study and actual hardware implementation. However, although the performance advantages of MUSIC are substantial, they are achieved at a cost in computation (searching over parameter space) and storage (of array calibration data).",
            "score": 77.83660125732422
        },
        {
            "docid": "4438763_6",
            "document": "Gillespie algorithm . The algorithm is computationally expensive and thus many modifications and adaptations exist, including the next reaction method (Gibson & Bruck), tau-leaping, as well as hybrid techniques where abundant reactants are modeled with deterministic behavior. Adapted techniques generally compromise the exactitude of the theory behind the algorithm as it connects to the Master equation, but offer reasonable realizations for greatly improved timescales. The computational cost of exact versions of the algorithm is determined by the coupling class of the reaction network. In weakly coupled networks, the number of reactions that is influenced by any other reaction is bounded by a small constant. In strongly coupled networks, a single reaction firing can in principle affect all other reactions. An exact version of the algorithm with constant-time scaling for weakly coupled networks has been developed, enabling efficient simulation of systems with very large numbers of reaction channels (Slepoy Thompson Plimpton 2008). The generalized Gillespie algorithm that accounts for the non-Markovian properties of random biochemical events with delay has been developed by Bratsun et al. 2005 and independently Barrio et al. 2006, as well as (Cai 2007). See the articles cited below for details.",
            "score": 76.67778778076172
        },
        {
            "docid": "402673_39",
            "document": "Ray Solomonoff . Algorithmic Probability and Solomonoff Induction have many advantages for Artificial Intelligence. Algorithmic Probability gives extremely accurate probability estimates. These estimates can be revised by a reliable method so that they continue to be acceptable. It utilizes search time in a very efficient way. In addition to probability estimates, Algorithmic Probability \"has for AI another important value: its multiplicity of models gives us many different ways to understand our data;",
            "score": 75.45824432373047
        },
        {
            "docid": "470752_40",
            "document": "Expectation\u2013maximization algorithm . which contains the log-EM algorithm as its subclass. Thus, the \u03b1-EM algorithm by Yasuo Matsuyama is an exact generalization of the log-EM algorithm. No computation of gradient or Hessian matrix is needed. The \u03b1-EM shows faster convergence than the log-EM algorithm by choosing an appropriate \u03b1. The \u03b1-EM algorithm leads to a faster version of the Hidden Markov model estimation algorithm \u03b1-HMM. EM is a partially non-Bayesian, maximum likelihood method. Its final result gives a probability distribution over the latent variables (in the Bayesian style) together with a point estimate for \"\u03b8\" (either a maximum likelihood estimate or a posterior mode). A fully Bayesian version of this may be wanted, giving a probability distribution over \"\u03b8\" and the latent variables. The Bayesian approach to inference is simply to treat \"\u03b8\" as another latent variable. In this paradigm, the distinction between the E and M steps disappears. If using the factorized Q approximation as described above (variational Bayes), solving can iterate over each latent variable (now including \"\u03b8\") and optimize them one at a time. Now, \"k\" steps per iteration are needed, where \"k\" is the number of latent variables. For graphical models this is easy to do as each variable's new \"Q\" depends only on its Markov blanket, so local message passing can be used for efficient inference.",
            "score": 75.14044189453125
        },
        {
            "docid": "15652764_14",
            "document": "Non-linear least squares . Some problems of ill-conditioning and divergence can be corrected by finding initial parameter estimates that are near to the optimal values. A good way to do this is by computer simulation. Both the observed and calculated data are displayed on a screen. The parameters of the model are adjusted by hand until the agreement between observed and calculated data is reasonably good. Although this will be a subjective judgment, it is sufficient to find a good starting point for the non-linear refinement. Initial parameter estimates can be created using transformations or linearizations. Better still evolutionary algorithms such as the Stochastic Funnel Algorithm can lead to the convex basin of attraction that surrounds the optimal parameter estimates. Hybrid algorithms that use randomization and elitism, followed by Newton methods have been shown to be useful and computationally efficient.",
            "score": 75.093017578125
        },
        {
            "docid": "28442_19",
            "document": "Sorting algorithm . While there are a large number of sorting algorithms, in practical implementations a few algorithms predominate. Insertion sort is widely used for small data sets, while for large data sets an asymptotically efficient sort is used, primarily heap sort, merge sort, or quicksort. Efficient implementations generally use a hybrid algorithm, combining an asymptotically efficient algorithm for the overall sort with insertion sort for small lists at the bottom of a recursion. Highly tuned implementations use more sophisticated variants, such as Timsort (merge sort, insertion sort, and additional logic), used in Android, Java, and Python, and introsort (quicksort and heap sort), used (in variant forms) in some C++ sort implementations and in .NET.",
            "score": 74.87901306152344
        },
        {
            "docid": "3878_54",
            "document": "Biostatistics . Nowadays, increase in size and complexity of molecular datasets leads to use of powerful statistical methods provided by computer science algorithms which are developed by machine learning area. Therefore, data mining and machine learning allow detection of patterns in data with a complex structure, as biological ones, by using methods of supervised and unsupervised learning, regression, detection of clusters and association rule mining, among others. To indicate some of them, self-organizing maps and \"k\"-means are examples of cluster algorithms; neural networks implementation and support vector machines models are examples of common machine learning algorithms.",
            "score": 74.54572296142578
        },
        {
            "docid": "149353_4",
            "document": "Computational biology . Computational Biology, which includes many aspects of bioinformatics, is the science of using biological data to develop algorithms or models to understand biological systems and relationships.  Until recently, biologists did not have access to very large amounts of data. This data has now become commonplace, particularly in molecular biology and genomics. Researchers were able to develop analytical methods for interpreting biological information, but were unable to share them quickly among colleagues.  Bioinformatics began to develop in the early 1970s. It was considered the science of analyzing informatics processes of various biological systems. At this time, research in artificial intelligence was using network models of the human brain in order to generate new algorithms. This use of biological data to develop other fields pushed biological researchers to revisit the idea of using computers to evaluate and compare large data sets. By 1982, information was being shared amongst researchers through the use of punch cards. The amount of data being shared began to grow exponentially by the end of the 1980s. This required the development of new computational methods in order to quickly analyze and interpret relevant information. Since the late 1990s, computational biology has become an important part of developing emerging technologies for the field of biology. The terms computational biology and evolutionary computation have a similar name, but are not to be confused. Unlike computational biology, evolutionary computation is not concerned with modeling and analyzing biological data. It instead creates algorithms based on the ideas of evolution across species. Sometimes referred to as genetic algorithms, the research of this field can be applied to computational biology. While evolutionary computation is not inherently a part of computational biology, Computational evolutionary biology is a subfield of it.",
            "score": 73.26936340332031
        },
        {
            "docid": "33813121_6",
            "document": "Network controllability . In graph theory, a matching is a set of edges without common vertices. Liu et al. extended this definition to directed graph, where a matching is a set of directed edges that do not share start or end vertices. It is easy to check that a matching of a directed graph composes of a set of vertex-disjoint simple paths and cycles. The maximum matching of a directed network can be efficiently calculated by working in the bipartite representation using the classical Hopcroft\u2013Karp algorithm, which runs in O(\"E\") time in the worst case. For undirected graph, analytical solutions of the size and number of maximum matchings have been studied using the cavity method developed in statistical physics. Liu et al. extended the calculations for directed graph.",
            "score": 72.77254486083984
        },
        {
            "docid": "42451659_2",
            "document": "Denoising Algorithm based on Relevance network Topology . Denoising Algorithm based on Relevance network Topology (DART) is an unsupervised algorithm that estimates an activity score for a pathway in a gene expression matrix, following a denoising step. In DART, a weighted average is used where the weights reflect the degree of the nodes in the pruned network. The denoising step removes prior information that is inconsistent with a data set. This strategy substantially improves unsupervised predictions of pathway activity that are based on a prior model, which was learned from a different biological system or context.",
            "score": 72.65060424804688
        },
        {
            "docid": "14628623_2",
            "document": "Isomap . Isomap is a nonlinear dimensionality reduction method. It is one of several widely used low-dimensional embedding methods. Isomap is used for computing a quasi-isometric, low-dimensional embedding of a set of high-dimensional data points. The algorithm provides a simple method for estimating the intrinsic geometry of a data manifold based on a rough estimate of each data point\u2019s neighbors on the manifold. Isomap is highly efficient and generally applicable to a broad range of data sources and dimensionalities.",
            "score": 72.56057739257812
        },
        {
            "docid": "309261_4",
            "document": "Nonlinear dimensionality reduction . Consider a dataset represented as a matrix (or a database table), such that each row represents a set of attributes (or features or dimensions) that describe a particular instance of something. If the number of attributes is large, then the space of unique possible rows is exponentially large. Thus, the larger the dimensionality, the more difficult it becomes to sample the space. This causes many problems. Algorithms that operate on high-dimensional data tend to have a very high time complexity. Many machine learning algorithms, for example, struggle with high-dimensional data. This has become known as the curse of dimensionality. Reducing data into fewer dimensions often makes analysis algorithms more efficient, and can help machine learning algorithms make more accurate predictions.",
            "score": 72.2990493774414
        },
        {
            "docid": "29467449_19",
            "document": "Protein function prediction . Several networks based on different data sources can be combined into a composite network, which can then be used by a prediction algorithm to annotate candidate genes or proteins. For example, the developers of the bioPIXIE system used a wide variety of \"Saccharomyces cerevisiae\" (yeast) genomic data to produce a composite functional network for that species. This resource allows the visualization of known networks representing biological processes, as well as the prediction of novel components of those networks. Many algorithms have been developed to predict function based on the integration of several data sources (e.g. genomic, proteomic, protein interaction, etc.), and testing on previously annotated genes indicates a high level of accuracy. Disadvantages of some function prediction algorithms have included a lack of accessibility, and the time required for analysis. Faster, more accurate algorithms such as GeneMANIA (multiple association network integration algorithm) have however been developed in recent years and are publicly available on the web, indicating the future direction of function prediction.",
            "score": 72.20409393310547
        },
        {
            "docid": "32292709_15",
            "document": "Theil\u2013Sen estimator . The problem of performing slope selection exactly but more efficiently than the brute force quadratic time algorithm has been extensively studied in computational geometry. Several different methods are known for computing the Theil\u2013Sen estimator exactly in time, either deterministically or using randomized algorithms. Siegel's repeated median estimator can also be constructed efficiently in the same time bound. In models of computation in which the input coordinates are integers and bitwise operations on integers take constant time, the problem can be solved even more quickly, in randomized expected time formula_2.",
            "score": 71.71595001220703
        },
        {
            "docid": "40338559_4",
            "document": "Hybrid algorithm . In computer science, hybrid algorithms are very common in optimized real-world implementations of recursive algorithms, particularly implementations of  divide and conquer or decrease and conquer algorithms, where the size of the data decreases as one moves deeper in the recursion. In this case, one algorithm is used for the overall approach (on large data), but deep in the recursion, it switches to a different algorithm, which is more efficient on small data. A common example is in sorting algorithms, where the insertion sort, which is inefficient on large data, but very efficient on small data (say, five to ten elements), is used as the final step, after primarily applying another algorithm, such as merge sort or quicksort. Merge sort and quicksort are asymptotically optimal on large data, but the overhead becomes significant if applying them to small data, hence the use of a different algorithm at the end of the recursion. A highly optimized hybrid sorting algorithm is Timsort, which combines merge sort, insertion sort, together with additional logic (including binary search) in the merging logic.",
            "score": 71.71232604980469
        },
        {
            "docid": "44674478_13",
            "document": "Rigid motion segmentation . It is a very useful technique for detecting changes in images due to its simplicity and ability to deal with occlusion and multiple motions. These techniques assume constant light source intensity. The algorithm first considers two frames at a time and then computes the pixel by pixel intensity difference. On this computation it thresholds the intensity difference and maps the changes onto a contour. Using this contour it extracts the spatial and temporal information required to define the motion in the scene. Though it is a simple technique to implement it is not robust to noise. Another difficulty with these techniques is the camera movement. When the camera moves there is a change in the entire image which has to be accounted for. Many new algorithm have been introduced to overcome these difficulties. Motion segmentation can be seen as a classification problem where each pixel has to be classified as background or foreground. Such classifications are modeled under statistic theory and can be used in segmentation algorithms. These approaches can be further divided depending on the statistical framework used. Most commonly used frameworks are maximum a posteriori probability (MAP), Particle Filter (PF) and Expectation Maximization (EM). MAP uses Bayes' Rule for implementation where a particular pixel has to be classified under predefined classes. PF is based on the concept of evolution of a variable with varying weights over time. The final estimation is the weighted sum of all the variables. Both of these methods are iterative. The EM algorithm is also an iterative estimation method. It computes the maximum likelihood (ML) estimate of the model parameters in presence of missing or hidden data and decided the most likely fit of the observed data.",
            "score": 71.5638427734375
        },
        {
            "docid": "12499410_20",
            "document": "Network motif . Kashtan \"et al.\" presented the first sampling NM discovery algorithm, which was based on \"edge sampling\" throughout the network. This algorithm estimates concentrations of induced sub-graphs and can be utilized for motif discovery in directed or undirected networks. The sampling procedure of the algorithm starts from an arbitrary edge of the network that leads to a sub-graph of size two, and then expands the sub-graph by choosing a random edge that is incident to the current sub-graph. After that, it continues choosing random neighboring edges until a sub-graph of size n is obtained. Finally, the sampled sub-graph is expanded to include all of the edges that exist in the network between these n nodes. When an algorithm uses a sampling approach, taking unbiased samples is the most important issue that the algorithm might address. The sampling procedure, however, does not take samples uniformly and therefore Kashtan \"et al.\" proposed a weighting scheme that assigns different weights to the different sub-graphs within network. The underlying principle of weight allocation is exploiting the information of the sampling probability for each sub-graph, i.e. the probable sub-graphs will obtain comparatively less weights in comparison to the improbable sub-graphs; hence, the algorithm must calculate the sampling probability of each sub-graph that has been sampled. This weighting technique assists \"mfinder\" to determine sub-graph concentrations impartially.",
            "score": 71.50324249267578
        },
        {
            "docid": "2889768_11",
            "document": "Image stitching . To estimate a robust model from the data, a common method used is known as RANSAC.<br> The name RANSAC is an abbreviation for \"RANdom SAmple Consensus\". It is an iterative method for robust parameter estimation to fit mathematical models from sets of observed data points which may contain outliers. The algorithm is non-deterministic in the sense that it produces a reasonable result only with a certain probability, with this probability increasing as more iterations are performed. It being a probabilistic method means that different results will be obtained for every time the algorithm is run.  The RANSAC algorithm has found many applications in computer vision, including the simultaneous solving of the correspondence problem and the estimation of the fundamental matrix related to a pair of stereo cameras. The basic assumption of the method is that the data consists of \"inliers\", i.e., data whose distribution can be explained by some mathematical model, and \"outliers\" which are data that do not fit the model. Outliers are considered points which come from noise, erroneous measurements, or simply incorrect data. For the problem of homography estimation, RANSAC works by trying to fit several models using some of the point pairs and then checking if the models were able to relate most of the points. The best model, i.e., the homography which produces the highest number of correct matches, is then chosen as the answer for the problem thus if the ratio of number of outliers to data points is very low the RANSAC outputs a decent model fitting the data.",
            "score": 71.34130859375
        },
        {
            "docid": "1860407_13",
            "document": "K-means clustering . Commonly used initialization methods are Forgy and Random Partition. The Forgy method randomly chooses \"k\" observations from the data set and uses these as the initial means. The Random Partition method first randomly assigns a cluster to each observation and then proceeds to the update step, thus computing the initial mean to be the centroid of the cluster's randomly assigned points. The Forgy method tends to spread the initial means out, while Random Partition places all of them close to the center of the data set. According to Hamerly et al., the Random Partition method is generally preferable for algorithms such as the \"k\"-harmonic means and fuzzy \"k\"-means. For expectation maximization and standard \"k\"-means algorithms, the Forgy method of initialization is preferable. A comprehensive study by Celebi et al., however, found that popular initialization methods such as Forgy, Random Partition, and Maximin often perform poorly, whereas the approach by Bradley and Fayyad performs \"consistently\" in \"the best group\" and K-means++ performs \"generally well\". As it is a heuristic algorithm, there is no guarantee that it will converge to the global optimum, and the result may depend on the initial clusters. As the algorithm is usually very fast, it is common to run it multiple times with different starting conditions. However, in the worst case, \"k\"-means can be very slow to converge: in particular it has been shown that there exist certain point sets, even in 2 dimensions, on which \"k\"-means takes exponential time, that is , to converge. These point sets do not seem to arise in practice: this is corroborated by the fact that the smoothed running time of \"k\"-means is polynomial.",
            "score": 71.00230407714844
        },
        {
            "docid": "1363296_2",
            "document": "Modelling biological systems . Modelling biological systems is a significant task of systems biology and mathematical biology. Computational systems biology aims to develop and use efficient algorithms, data structures, visualization and communication tools with the goal of computer modelling of biological systems. It involves the use of computer simulations of biological systems, including cellular subsystems (such as the networks of metabolites and enzymes which comprise metabolism, signal transduction pathways and gene regulatory networks), to both analyze and visualize the complex connections of these cellular processes.",
            "score": 70.93614196777344
        },
        {
            "docid": "1326926_27",
            "document": "Array processing . While the spectral-based methods presented in previous section are computationally attractive, they do not always yield sufficient accuracy. In particular, for the cases when we have highly correlated signals, the performance of spectral-based methods may be insufficient. An alternative is to more fully exploit the underlying data model, leading to so-called parametric array processing methods. The cost of using such methods to increase the efficiency is that the algorithms typically require a multidimensional search to find the estimates. The most common used model based approach in signal processing is the maximum likelihood (ML) technique. This method requires a statistical framework for the data generation process. When applying the ML technique to the array processing problem, two main methods have been considered depending on the signal data model assumption. According to the Stochastic ML, the signals are modeled as Gaussian random processes. On the other hand, in the Deterministic ML the signals are considered as unknown, deterministic quantities that need to be estimated in conjunction with the direction of arrival.",
            "score": 70.9001235961914
        },
        {
            "docid": "349458_60",
            "document": "Simplex algorithm . Analyzing and quantifying the observation that the simplex algorithm is efficient in practice, even though it has exponential worst-case complexity, has led to the development of other measures of complexity. The simplex algorithm has polynomial-time average-case complexity under various probability distributions, with the precise average-case performance of the simplex algorithm depending on the choice of a probability distribution for the random matrices. Another approach to studying \"typical phenoma\" uses Baire category theory from general topology, and to show that (topologically) \"most\" matrices can be solved by the simplex algorithm in a polynomial number of steps. Another method to analyze the performance of the simplex algorithm studies the behavior of worst-case scenarios under small perturbation \u2013 are worst-case scenarios stable under a small change (in the sense of structural stability), or do they become tractable? Formally, this method uses random problems to which is added a Gaussian random vector (\"smoothed complexity\").",
            "score": 70.79071044921875
        },
        {
            "docid": "775_67",
            "document": "Algorithm . The analysis and study of algorithms is a discipline of computer science, and is often practiced abstractly without the use of a specific programming language or implementation. In this sense, algorithm analysis resembles other mathematical disciplines in that it focuses on the underlying properties of the algorithm and not on the specifics of any particular implementation. Usually pseudocode is used for analysis as it is the simplest and most general representation. However, ultimately, most algorithms are usually implemented on particular hardware / software platforms and their algorithmic efficiency is eventually put to the test using real code. For the solution of a \"one off\" problem, the efficiency of a particular algorithm may not have significant consequences (unless n is extremely large) but for algorithms designed for fast interactive, commercial or long life scientific usage it may be critical. Scaling from small n to large n frequently exposes inefficient algorithms that are otherwise benign.",
            "score": 70.43406677246094
        }
    ]
}