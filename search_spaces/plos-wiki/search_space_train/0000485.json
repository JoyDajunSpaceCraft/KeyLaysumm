{
    "q": [
        {
            "docid": "40977477_17",
            "document": "Cross-species transmission . Two methods of measuring genetic variation, variable number tandem repeats (VNTRs) and single nucleotide polymorphisms (SNPs), have been very beneficial to the study of bacterial transmission. VNTRs, due to the low cost and high mutation rates, make them particularly useful to detect genetic differences in recent outbreaks, and while SNPs have a lower mutation rate per locus than VNTRs, they deliver more stable and reliable genetic relationships between isolates. Both methods are used to construct phylogenies for genetic analysis, however, SNPs are more suitable for studies on phylogenies contraction. However, it can be difficult for these methods accurately simulate CSTs everts. Estimates of CST based on phylogenys made using VNTR marker can be biased towards detecting CST events across a wide range of the parameters. SNPs tend to be less biased and variable in estimates of CST when estimations of CST rates are low and low number of SNPs is used. In general, CST rate estimates using these methods are most reliable in systems with more mutations, more markers, and high genetic differences between introduced strains. CST is very complex and models need to account for a lot of parameters to accurately represent the phenomena. Models that oversimplify reality can result in biased data. Multiple parameters such as number of mutations accumulated since introduction, stochasticity, the genetic difference of strains introduced, and the sampling effort can make unbiased estimates of CST difficult even with whole-genome sequences, especially if sampling is limited, mutation rates are low, or if pathogens were recently introduced. More information on the factors that influence CST rates is needed for the contraction of more appropriate models to study these events.",
            "score": 63.851394176483154
        },
        {
            "docid": "3608284_22",
            "document": "Generalized additive model . So far we have treated estimation and inference given the smoothing parameters, formula_72, but these also need to be estimated. One approach is to take a fully Bayesian approach, defining priors on the (log) smoothing parameters, and using stochastic simulation or high order approximation methods to obtain information about the posterior of the model coefficients. An alternative is to select the smoothing parameters to optimize a prediction error criterion such as Generalized cross validation (GCV) or the Akaike information criterion (AIC). Finally we may choose to maximize the Marginal Likelihood (REML) obtained by integrating the model coefficients, formula_39 out of the joint density of formula_74,  Since formula_76 is just the likelihood of formula_39, we can view this as choosing formula_78 to maximize the average likelihood of random draws from the prior. The preceding integral is usually analytically intractable but can be approximated to quite high accuracy using Laplace's method.",
            "score": 54.8008873462677
        },
        {
            "docid": "54190305_13",
            "document": "FORMIND . Integration of forest inventroy data for parameter calibration: Parameter estimation in forest gap models is a time-consuming process. Manual calibration and sensitivity analysis of these models require a large number of simulations, leading to a higher computational demand. Thus, for the automatic calibration of model parameters, when direct measurements are missing or made under specific conditions (e.g., climate, soil), a collection of rapid stochastic calibration methods have been developed and applied in FORMIND. These methods automatically minimize the difference between simulation results and field observations by running the model a thousand times. Additionally, for the assessment of parameter uncertainty, approximate Bayesian methods can be used in combination with a Markov chain Monte Carlo approach. These methods can also be used for forest sites where a limited number of observations in time and space are available. After careful examination of the model results and available observation data, a combination of manual and automatic calibrations leads to the successful parameterization of forest gap models.",
            "score": 65.29245281219482
        },
        {
            "docid": "13070117_9",
            "document": "Spectral density estimation . Many other techniques for spectral estimation have been developed to mitigate the disadvantages of the basic periodogram. These techniques can generally be divided into \"non-parametric\" and \"parametric\" methods. The non-parametric approaches explicitly estimate the covariance or the spectrum of the process without assuming that the process has any particular structure. Some of the most common estimators in use for basic applications (e.g. Welch's method) are non-parametric estimators closely related to the periodogram. By contrast, the parametric approaches assume that the underlying stationary stochastic process has a certain structure that can be described using a small number of parameters (for example, using an auto-regressive or moving average model). In these approaches, the task is to estimate the parameters of the model that describes the stochastic process.",
            "score": 46.587252378463745
        },
        {
            "docid": "40977477_14",
            "document": "Cross-species transmission . Alternative hosts can also potentially have a critical role in the evolution and diffusion of a pathogen. When a pathogen crosses species it often acquires new characteristics that allow it to breach host barriers. Different pathogen variants can have very different effects on host species. Thus it can be beneficial to CST analysis to compare the same pathogens occurring in different host species. Phylogenetic analysis can be used to track a pathogens history through different species populations. Even if a pathogen is new and highly divergent, phylogenetic comparison can be very insightful A useful strategy for investigating the history of epidemics caused by pathogen transmission combines molecular clock analysis, to estimate the timescale of the epidemic, and coalescent theory, to infer the demographic history of the pathogen. When constructing phylogenies, computer databases and tools are often used. Programs, such as BLAST, are used to annotate pathogen sequences, while databases like GenBank provide information about functions based on the pathogens genomic structure. Trees are constructed using computational methods such as MPR or Bayesian Inference, and models are created depending on the needs of the study. Single rate dated tip (SRDT) models, for example, allows for estimates of timescale under a phylogenetic tree. Models for CST prediction will vary depending on what parameters need to be accounted for when constructing the model.",
            "score": 62.770273208618164
        },
        {
            "docid": "406624_8",
            "document": "Time series . Additionally, time series analysis techniques may be divided into parametric and non-parametric methods. The parametric approaches assume that the underlying stationary stochastic process has a certain structure which can be described using a small number of parameters (for example, using an autoregressive or moving average model). In these approaches, the task is to estimate the parameters of the model that describes the stochastic process. By contrast, non-parametric approaches explicitly estimate the covariance or the spectrum of the process without assuming that the process has any particular structure.",
            "score": 47.23572754859924
        },
        {
            "docid": "31745436_2",
            "document": "Iterated filtering . Iterated filtering algorithms are a tool for maximum likelihood inference on partially observed dynamical systems. Stochastic perturbations to the unknown parameters are used to explore the parameter space. Applying sequential Monte Carlo (the particle filter) to this extended model results in the selection of the parameter values that are more consistent with the data. Appropriately constructed procedures, iterating with successively diminished perturbations, converge to the maximum likelihood estimate. Iterated filtering methods have so far been used most extensively to study infectious disease transmission dynamics. Case studies include cholera, Ebola virus, influenza, malaria, HIV, pertussis, poliovirus and measles. Other areas which have been proposed to be suitable for these methods include ecological dynamics and finance.",
            "score": 48.37771964073181
        },
        {
            "docid": "39291986_4",
            "document": "Linear seismic inversion . On the other hand, stochastic inversion methods are used to generate constrained models as used in reservoir flow simulation, using geostatistical tools like kriging. As opposed to deterministic inversion methods which produce a single set of model parameters, stochastic methods generate a suite of alternate earth model parameters which all obey the model constraint. However, the two methods are related as the results of deterministic models is the average of all the possible non-unique solutions of stochastic methods. Since seismic linear inversion is a deterministic inversion method, the stochastic method will not be discussed beyond this point.",
            "score": 37.990018367767334
        },
        {
            "docid": "48403381_23",
            "document": "Dragon King Theory . Given a model and data, one can obtain a statistical model estimate. This model estimate can then be used to compute interesting quantities such as the conditional probability of the occurrence of a dragon king event in a future time interval, and the most probable occurrence time. When doing statistical modeling of extremes, and using complex or nonlinear dynamic models, there is bound to be substantial uncertainty. Thus, one should be diligent in uncertainty quantification: not only considering the randomness present in the fitted stochastic model, but also the uncertainty of its estimated parameters (e.g., with Bayesian techniques or by first simulating parameters and then simulating from the model with those parameters), and the uncertainty in model selection (e.g., by considering an ensemble of different models).",
            "score": 56.32224106788635
        },
        {
            "docid": "5740025_25",
            "document": "Stochastic volatility . Once a particular SV model is chosen, it must be calibrated against existing market data. Calibration is the process of identifying the set of model parameters that are most likely given the observed data. One popular technique is to use maximum likelihood estimation (MLE). For instance, in the Heston model, the set of model parameters formula_57 can be estimated applying an MLE algorithm such as the Powell Directed Set method to observations of historic underlying security prices.",
            "score": 52.82423424720764
        },
        {
            "docid": "5740025_27",
            "document": "Stochastic volatility . An alternative to calibration is statistical estimation, thereby accounting for parameter uncertainty. Many frequentist and Bayesian methods have been proposed and implemented, typically for a subset of the abovementioned models. The following list contains extension packages for the open source statistical software R that have been specifically designed for heteroskedasticity estimation. The first three cater for GARCH-type models with deterministic volatilities; the fourth deals with stochastic volatility estimation.",
            "score": 31.278115034103394
        },
        {
            "docid": "25882596_24",
            "document": "Statistical association football predictions . On the one hand, statistical models require a large number of observations to make an accurate estimation of its parameters. And when there are not enough observations available during a season (as is usually the situation), working with average statistics makes sense. On the other hand, it is well known that team skills change during the season, making model parameters time-dependent. Mark Dixon (statistician) and Coles tried to solve this trade-off by assigning a larger weight to the latest match results. Rue and Salvesen introduced a novel time-dependent rating method using the Markov Chain model.",
            "score": 49.6803252696991
        },
        {
            "docid": "46182_32",
            "document": "White noise . In statistics and econometrics one often assumes that an observed series of data values is the sum of a series of values generated by a deterministic linear process, depending on certain independent (explanatory) variables, and on a series of random noise values. Then regression analysis is used to infer the parameters of the model process from the observed data, e.g. by ordinary least squares, and to test the null hypothesis that each of the parameters is zero against the alternative hypothesis that it is non-zero. Hypothesis testing typically assumes that the noise values are mutually uncorrelated with zero mean and have the same Gaussian probability distributionin other words, that the noise is white. If there is non-zero correlation between the noise values underlying different observations then the estimated model parameters are still unbiased, but estimates of their uncertainties (such as confidence intervals) will be biased (not accurate on average). This is also true if the noise is heteroskedasticthat is, if it has different variances for different data points.",
            "score": 38.63495087623596
        },
        {
            "docid": "23467232_5",
            "document": "Didier Sornette . Earthquake forecasting differs from prediction in the sense that no alarm is issued, but a time-dependent probability of earthquake occurrence is estimated. Sornette's group has contributed significantly to the theoretical development and study of the properties of the now standard Epidemic Type Aftershock Sequence (ETAS) model. In a nutshell, this model states that each event triggers its own direct aftershocks, which themselves trigger their own aftershocks, and so on... The consequence is that events cannot be labeled anymore as foreshocks, mainshocks or aftershocks, as they can be all of that at the same time (with different levels of probability). In this model, the probability for an event to trigger another one primarily depends on their separating space and time distances, as well as on the magnitude of the triggering event, so that seismicity is then governed by a set of seven parameters. Sornette's group is currently pushing the model to its limits by allowing space and time variations of its parameters. Despite the fact that this new model reaches better forecasting scores than any other competing model, it is not sufficient to achieve systematic reliable predictions. The main reason is that this model predicts future seismicity rates quite accurately, but fails to put constraints on the magnitudes (which are assumed to be distributed according to the Gutenberg-Richter law, and to be independent of each other). Some other seismic or non-seismic precursors are thus required in order to further improve those forecasts. According to the ETAS model, the rate of triggered activity around a given event behaves isotropically. This over-simplified assumption has recently relaxed by coupling the statistics of ETAS to genuine mechanical information. This is done by modelling the stress perturbation due to a given event on its surroundings, and correlating it with the space-time rate of subsequent activity as a function of transferred stress amplitude and sign. This suggests that triggering of aftershocks stems from a combination of dynamic (seismic waves) and elasto-static processes. Another unambiguous interesting result of this work is that the Earth crust in Southern California has quite a short memory of past stress fluctuations lasting only about 3 to 4 months. This may put more constraint on the time window within which one may look for both seismic and non-seismic precursors.",
            "score": 50.84654402732849
        },
        {
            "docid": "15652764_14",
            "document": "Non-linear least squares . Some problems of ill-conditioning and divergence can be corrected by finding initial parameter estimates that are near to the optimal values. A good way to do this is by computer simulation. Both the observed and calculated data are displayed on a screen. The parameters of the model are adjusted by hand until the agreement between observed and calculated data is reasonably good. Although this will be a subjective judgment, it is sufficient to find a good starting point for the non-linear refinement. Initial parameter estimates can be created using transformations or linearizations. Better still evolutionary algorithms such as the Stochastic Funnel Algorithm can lead to the convex basin of attraction that surrounds the optimal parameter estimates. Hybrid algorithms that use randomization and elitism, followed by Newton methods have been shown to be useful and computationally efficient.",
            "score": 50.718989610672
        },
        {
            "docid": "39291986_3",
            "document": "Linear seismic inversion . The method has long been useful for geophysicists and can be categorized into two broad types: Deterministic and stochastic inversion. Deterministic inversion methods are based on comparison of the output from an earth model with the observed field data and continuously updating the earth model parameters to minimize a function, which is usually some form of difference between model output and field observation. As such, this method of inversion to which linear inversion falls under is posed as an minimization problem and the accepted earth model is the set of model parameters that minimizes the objective function in producing a numerical seismogram which best compares with collected field seismic data.",
            "score": 38.10025715827942
        },
        {
            "docid": "38183925_5",
            "document": "Organogels . The formulation of an accurate theory of gel formation that correctly predicts gelation parameters (such as time, rate, and structure) of a broad range of materials is highly sought after for both commercial and intellectual reasons. As noted earlier, researchers often judge gel theories based upon their ability to accurately predict gel points. The kinetic and statistical methods model gel formation with different mathematical approaches. most researchers used statistical methods, as the equations derived thereby are less cumbersome and contain variables to which specific physical meanings can be attached, thus aiding in the analysis of gel formation theory. Below, we present the classical Flory-Stockmayer (FS) statistical theory for gel formation. This theory, despite its simplicity, has found widespread use. This is due in large part to small increases in accuracy provided by the use of more complicated methods, and to its being a general model which can be applied to many gelation systems. Other gel formation theories cased on different chemical approximations have also been derived. However, the FS model has better simplicity, wide applicability, and accuracy, and remains the most used.",
            "score": 43.16238296031952
        },
        {
            "docid": "344123_54",
            "document": "Heat sink . In industry, thermal analyses are often ignored in the design process or performed too late\u00a0\u2014\u00a0when design changes are limited and become too costly. Of the three methods mentioned in this article, theoretical and numerical methods can be used to determine an estimate of the heat sink or component temperatures of products before a physical model has been made. A theoretical model is normally used as a first order estimate. Online heat sink calculators can provide a reasonable estimate of forced and natural convection heat sink performance based on a combination of theoretical and empirically derived correlations. Numerical methods or computational fluid dynamics (CFD) provide a qualitative (and sometimes even quantitative) prediction of fluid flows. What this means is that it will give a visual or post-processed result of a simulation, like the images in figures 16 and 17, and the CFD animations in figure 18 and 19, but the quantitative or absolute accuracy of the result is sensitive to the inclusion and accuracy of the appropriate parameters.",
            "score": 37.0522620677948
        },
        {
            "docid": "40158142_17",
            "document": "Nonlinear system identification . Unfortunately, due to the nonlinear transformation of unobserved random variables, the likelihood function of the outputs is analytically intractable; it is given in terms of a multidimensional marginalization integral. Consequently, commonly used parameter estimation methods such as the Maximum Likelihood Method or the Prediction Error Method based on the optimal one-step ahead predictor are analytically intractable. Recently, algorithms based on sequential Monte Carlo methods have been used to approximate the conditional mean of the outputs or, in conjunction with the Expectation-Maximization algorithm, to approximate the maximum likelihood estimator. These methods, albeit asymptotically optimal, are computationally demanding and their use is limited to specific cases where the fundamental limitations of the employed particle filters can be avoided. An alternative solution is to apply the prediction error method using a sub-optimal predictor. The resulting estimator can be shown to be strongly consistent and asymptotically normal and can be evaluated using relatively simple algorithms.",
            "score": 45.373454570770264
        },
        {
            "docid": "140806_4",
            "document": "Maximum likelihood estimation . From the point of view of Bayesian inference, MLE is a special case of maximum a posteriori estimation (MAP) that assumes a uniform prior distribution of the parameters. In frequentist inference, MLE is one of several methods to get estimates of parameters without using prior distributions. Priors are avoided by not making probability statements about the parameters, but only about their estimates, whose properties are fully defined by the observations and the statistical model. The method of maximum likelihood is based on the likelihood function, formula_1. We are given a statistical model, i.e. a family of distributions formula_2, where formula_3 denotes the (possibly multi-dimensional) parameter for the model. The method of maximum likelihood finds the values of the model parameter, formula_3, that maximize the likelihood function, formula_1. Intuitively, this selects the parameter values that make the data most probable.",
            "score": 44.277437686920166
        },
        {
            "docid": "23179394_8",
            "document": "Ray Hilborn . In their research, Hilborn and Walters investigated the ways that dynamic models can be used to manage fisheries in order to maintain states of optimum equilibrium. In their paper, they examined the effectiveness of using the Ricker and Beverton-Holt models to estimate the potential yield of future generations by using data taken from prior generations. They addressed the problem that, in regards to fisheries, the parameters of the control system are often either varied or uncertain and the use of historical data becomes progressively more unreliable as it gets older. Variables, regarding these issues, include natural mortality and spawning rates as well the effects of human fishing, as a predator-prey relationship. Influenced by control theory, Hilborn and Walters modified the original models with various new formulae to create alternative models, in order to achieve more accurate predictions. They then identified \u201ca series of alternative harvesting experiments\u2026 each of which would be reasonably certain to discriminate between the alternative models\u2026\u201d The development of these alternative models and harvesting methods has been invaluable in assessing the sustainability of the world's fisheries.",
            "score": 52.89197838306427
        },
        {
            "docid": "38889813_27",
            "document": "Viral phylodynamics . For example, for the SIR model above, modified to include births into the population and deaths out of the population, the population size formula_6 is given by the equilibrium number of infected individuals, formula_66. The mean basic reproduction number, averaged across all infected individuals, is given by formula_67, under the assumption that the background mortality rate is negligible compared to the rate of recovery formula_40. The variance in individuals' basic reproduction rates is given by formula_69, because the duration of time individuals remain infected in the SIR model is exponentially distributed. The variance in the offspring distribution formula_55 is therefore 2. formula_53 therefore becomes formula_72 and the rate of coalescence becomes: This rate, derived for the SIR model at equilibrium, is equivalent to the rate of coalescence given by the more general formula provided by Volz et al. Rates of coalescence can similarly be derived for epidemiological models with superspreaders or other transmission heterogeneities, for models with individuals who are exposed but not yet infectious, and for models with variable infectious periods, among others. Given some epidemiological information (such as the duration of infection) and a specification of a mathematical model, viral phylogenies can therefore be used to estimate epidemiological parameters that might otherwise be difficult to quantify.",
            "score": 46.9313679933548
        },
        {
            "docid": "951614_10",
            "document": "Mathematical modelling of infectious disease . When dealing with large populations, as in the case of tuberculosis, deterministic or compartmental mathematical models are often used. In a deterministic model, individuals in the population are assigned to different subgroups or compartments, each representing a specific stage of the epidemic. Letters such as M, S, E, I, and R are often used to represent different stages.  The transition rates from one class to another are mathematically expressed as derivatives, hence the model is formulated using differential equations. While building such models, it must be assumed that the population size in a compartment is differentiable with respect to time and that the epidemic process is deterministic. In other words, the changes in population of a compartment can be calculated using only the history that was used to develop the model.",
            "score": 43.259111404418945
        },
        {
            "docid": "29803312_26",
            "document": "System size expansion . A number of studies have elucidated cases of the insufficiency of the linear noise approximation in biological contexts by comparison of its predictions with those of stochastic simulations. This has led to the investigation of higher order terms of the system size expansion that go beyond the linear approximation. These terms have been used to obtain more accurate moment estimates for the mean concentrations and for the variances of the concentration fluctuations in intracellular pathways. In particular, the leading order corrections to the linear noise approximation yield corrections of the conventional rate equations. Terms of higher order have also been used to obtain corrections to the variances and covariances estimates of the linear noise approximation. The linear noise approximation and corrections to it can be computed using the open source software intrinsic Noise Analyzer. The corrections have been shown to be particularly considerable for allosteric and non-allosteric enzyme-mediated reactions in intracellular compartments.",
            "score": 43.805065393447876
        },
        {
            "docid": "36682_2",
            "document": "Linear predictive coding . Linear predictive coding (LPC) is a tool used mostly in audio signal processing and speech processing for representing the spectral envelope of a digital signal of speech in compressed form, using the information of a linear predictive model. It is one of the most powerful speech analysis techniques, and one of the most useful methods for encoding good quality speech at a low bit rate and provides extremely accurate estimates of speech parameters.",
            "score": 45.65815210342407
        },
        {
            "docid": "250001_7",
            "document": "Molecular clock . Sometimes referred to as node dating, node calibration is a method for phylogeny calibration that is done by placing fossil constraints at nodes. A node calibration fossil is the oldest discovered representative of that clade, which is used to constrain its minimum age. Due to the fragmentary nature of the fossil record, the true most recent common ancestor of a clade will likely never be found. In order to account for this in node calibration analyses, a maximum clade age must be estimated. Determining the maximum clade age is challenging because it relies on negative evidence\u2014the absence of older fossils in that clade. There are a number of methods for deriving the maximum clade age using birth-death models, fossil stratigraphic distribution analyses, or taphonomic controls. Alternatively, instead of a maximum and a minimum, a prior probability of the divergence time can be established and used to calibrate the clock. There are several prior probability distributions including normal, lognormal, exponential, gamma, uniform, etc.) that can be used to express the probability of the true age of divergence relative to the age of the fossil; however, there are very few methods for estimating the shape and parameters of the probability distribution empirically. The placement of calibration nodes on the tree informs the placement of the unconstrained nodes, giving divergence date estimates across the phylogeny. Historical methods of clock calibration could only make use of a single fossil constraint (non-parametric rate smoothing), while modern analyses (BEAST and ) allow for the use of multiple fossils to calibrate the molecular clock. Simulation studies have shown that increasing the number of fossil constraints increases the accuracy of divergence time estimation.",
            "score": 43.19225740432739
        },
        {
            "docid": "1434444_58",
            "document": "Autoregressive model . The predictive performance of the autoregressive model can be assessed as soon as estimation has been done if cross-validation is used. In this approach, some of the initially available data was used for parameter estimation purposes, and some (from available observations later in the data set) was held back for out-of-sample testing. Alternatively, after some time has passed after the parameter estimation was conducted, more data will have become available and predictive performance can be evaluated then using the new data.",
            "score": 50.99041771888733
        },
        {
            "docid": "50543416_16",
            "document": "Hyperpolarized carbon-13 MRI . When the assumptions under which this ratio is proportional to formula_11 are not met, or there is significant noise in the collected data, it is desirable to compute estimates of model parameters directly. When noise is independent and identically distributed and Gaussian, parameters can be fit using non-linear least squares estimation. Otherwise (for example if magnitude images with Rician-distributed noise are used), parameters can be estimated by maximum likelihood estimation. The spatial distribution of metabolic rates can be visualized by estimating metabolic rates corresponding to the time series from each voxel, and plotting a heat map of the estimated rates.",
            "score": 37.68598437309265
        },
        {
            "docid": "38265760_10",
            "document": "Hierarchical generalized linear model . A summary of commonly used models are:  Hierarchical generalized linear models are used when observations come from different clusters. There are two types of estimators: fixed effect estimators and random effect estimators, corresponding to parameters in : formula_36  and in formula_37 , respectively. There are different ways to obtain parameter estimates for a hierarchical generalized linear model. If only fixed effect estimators are of interests, the population-averaged model can be used. If inference is focused on individuals, random effects will have to be estimated. There are different techniques to fit a hierarchical generalized linear model.",
            "score": 44.982677936553955
        },
        {
            "docid": "470752_2",
            "document": "Expectation\u2013maximization algorithm . In statistics, an expectation\u2013maximization (EM) algorithm is an iterative method to find maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables. The EM iteration alternates between performing an expectation (E) step, which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the \"E\" step. These parameter-estimates are then used to determine the distribution of the latent variables in the next E step.",
            "score": 36.723711013793945
        },
        {
            "docid": "22785026_55",
            "document": "History of macroeconomic thought . Under the synthesis, debates have become less ideological and more methodological. Business cycle modelers can be broken into two camps: those in favor of calibration and those in favor of estimation. When models are calibrated, the modeler selects parameter values based on other studies or casual empirical observation. Instead of using statistical diagnostics to evaluate models, the model's operating characteristics determine the quality of the model. Kydland and Prescott (1982) offered no formal evaluation of their model, but noted how variables like hours worked did not match real data well while the variances of other elements of the model did. When estimation methods are used, models are evaluated based on standard statistical goodness of fit criteria. Calibration is generally associated with real business cycle modelers of the new classical school, but methodological differences cut across ideology. While Lucas, Prescott, and Kydland are calibration advocates, another prominent new classical, Sargent, favors estimation.",
            "score": 35.46959400177002
        },
        {
            "docid": "2885691_75",
            "document": "Robust statistics . However, using these types of models to predict missing values or outliers in a long time series is difficult and often unreliable, particularly if the number of values to be in-filled is relatively high in comparison with total record length. The accuracy of the estimate depends on how good and representative the model is and how long the period of missing values extends. The in a case of a dynamic process, so any variable is dependent, not just on the historical time series of the same variable but also on several other variables or parameters of the process. In other words, the problem is an exercise in multivariate analysis rather than the univariate approach of most of the traditional methods of estimating missing values and outliers; a multivariate model will therefore be more representative than a univariate one for predicting missing values. The Kohonen self organising map (KSOM) offers a simple and robust multivariate model for data analysis, thus providing good possibilities to estimate missing values, taking into account its relationship or correlation with other pertinent variables in the data record.",
            "score": 48.68617117404938
        }
    ],
    "r": [
        {
            "docid": "30865687_10",
            "document": "Operation Dark Winter . The simulation also found that without sufficient surge capability, public health agencies' analysis of the scope, source and progress of the epidemic was greatly impeded, as was their ability to educate and reassure the public, and their capacity to limit casualties and the spread of disease. For example, even after the smallpox attack was recognized, decision makers were confronted with many uncertainties and wanted information that was not immediately available. (In fact, they were given more information on locations and numbers of infected people than would likely be available in reality.) Without accurate and timely information, participants found it difficult to quickly identify the locations of the original attacks; to immediately predict the likely size of the epidemic on the basis of initial cases; to know how many people were exposed; to find out how many were hospitalized and where; or to keep track of how many had been vaccinated.",
            "score": 72.93380737304688
        },
        {
            "docid": "46897719_16",
            "document": "Hub (network science) . The hubs are also responsible for effective spreading of material on network. In an analysis of disease spreading or information flow, hubs are referred to as super-spreaders. Super-spreaders may have a positive impact, such as effective information flow, but also devastating in a case of epidemic spreading such as H1N1 or AIDS. The mathematical models such as model of H1H1 Epidemic prediction may allow us to predict the spread of diseases based on human mobility networks, infectiousness, or social interactions among humans. Hubs are also important in the eradication of disease. In a scale-free network hubs are most likely to be infected, because of the large number of connections they have. After the hub is infected, it broadcasts the disease to the nodes it is linked to. Therefore, the selective immunization of hubs may be the cost-effective strategy in eradication of spreading disease.",
            "score": 72.2027359008789
        },
        {
            "docid": "13153449_4",
            "document": "HIV/AIDS in Russia . The policy of denial and repression of HIV diagnoses not only inhibited an effective government response to the epidemic, but also inhibited the recording of accurate statistics. There is a lack of accurate data on people infected with HIV in the Soviet Union, as well as a lack of accurate data on the at-risk groups: injecting drug users (IDUs), men who have sex with men (MSM), and sex workers. In 1988, it was estimated that the incidence of homosexuality in the population was only 1 in 100,000, and the existence of sex workers and drug addicts were also grossly underestimated. The size of the IDU population was even more underestimated than the other groups. In 1988, the incidence of intravenous drug use was considered to be extremely low, but in 1990 the veil of ignorance was partially lifted, and the official number of registered drug users measured 300,000. Nevertheless, this number is considered to be artificially low, with Soviet police having estimated the real number of addicts to be ten times as large. The authorities\u2019 neglect and near denial of the existence of these groups complicated the problems they faced as they came to grips with the existence of the HIV epidemic itself. Officials were unsure whom to test for HIV, due to never having kept track of the at-risk populations previously. Furthermore, individuals who belong to one of the at-risk groups were unlikely to independently seek treatment, due to both poor public awareness of the disease, as well as a fear of the stigma they would face if they came forward.",
            "score": 71.33718872070312
        },
        {
            "docid": "951614_7",
            "document": "Mathematical modelling of infectious disease . The 1920s saw the emergence of compartmental models. The Kermack\u2013McKendrick epidemic model (1927) and the Reed\u2013Frost epidemic model (1928) both describe the relationship between susceptible, infected and immune individuals in a population. The Kermack\u2013McKendrick epidemic model model was successful in predicting the behavior of outbreaks very similar to that observed in many recorded epidemics.",
            "score": 70.93324279785156
        },
        {
            "docid": "11714132_9",
            "document": "HIV/AIDS in Yunnan . In 2002, a United Nations-commissioned report, entitled \"China's Titanic peril\", estimating that the People's Republic of China had about 1 million cases of HIV, and that it was on the brink of an \"explosive HIV/AIDS epidemic\u2026with an imminent risk to widespread dissemination to the general population\". The report continued: \"a potential HIV/AIDS disaster of unimaginable proportion now lies in wait.\" A few months later, the US National Intelligence Council estimated that 1-2 million people were living with HIV in People's Republic of China, and predicted 10\u201315 million cases by 2010. Other reports at this time were similarly pessimistic: from the Center for Strategic and International Studies (Washington, DC, USA), HIV/AIDS was referred to as China's time-bomb; and from the American Enterprise Institute as the AIDS typhoon. However, as Wu and colleagues note, by 2006 the number of people living with HIV/AIDS is estimated to be 650 000\u2014a figure revised downwards by 200 000 from 2005. After a slow start and reluctance to recognise the existence of risk activities in its population and of the HIV epidemic, China has responded to international influences, media coverage, and scientific evidence to take bold steps to control the epidemic, using scientifically validated strategies. A Joint Assessment of HIV/AIDS Prevention, Treatment and Care in China (2004), developed jointly by UNAIDS and the State Council of the People's Republic of China, estimated that the country had 840,000 people living with HIV/AIDS. This figure was revised down to 650,000 in 2005 in light of more representative data collection and more appropriate estimation methods.",
            "score": 68.95448303222656
        },
        {
            "docid": "19028126_7",
            "document": "HIV/AIDS in Nepal . The annual new infection in 2014 is estimated at 1,493 and is expected to decline by 899 by 2020. An estimated number of 2,576 deaths were caused by AIDS in 2014, lower compared to estimated 3,362 deaths in 2013 which is, largely due to increased access to treatments. While the most recent data demonstrate a stabilizing of the epidemic and a downward trend in seroprevalence among several of the key high-risk groups, a number of issues pose continued challenges for Nepal. Many sex workers are also IDUs, migrants, or both, increasing the spread of HIV among at-risk groups. A large portion of men who purchase sex are also married, making them potential conduits for HIV to bridge to the general population. Poverty, low levels of education, illiteracy, gender inequalities, marginalization of at-risk groups, and stigma and discrimination compound the epidemic's effects. Unsafe sex and drug injection practices, civil conflict, internal and external mobility, and limited adequate health care delivery multiply the difficulties of addressing HIV/AIDS. Moreover, existing care and support services are already overwhelmed as increasing numbers of HIV-infected individuals become sick with AIDS.",
            "score": 68.6016845703125
        },
        {
            "docid": "917273_8",
            "document": "Basic reproduction number . In populations that are not homogeneous, the definition of \"R\" is more subtle. The definition must account for the fact that a typical infected individual may not be an average individual. As an extreme example, consider a population in which a small portion of the individuals mix fully with one another while the remaining individuals are all isolated. A disease may be able to spread in the fully mixed portion even though a randomly selected individual would lead to fewer than one secondary case. This is because the typical infected individual is in the fully mixed portion and thus is able to successfully cause infections. In general, if the individuals who become infected early in an epidemic may be more (or less) likely to transmit than a randomly chosen individual late in the epidemic, then our computation of \"R\" must account for this tendency. An appropriate definition for \"R\" in this case is \"the expected number of secondary cases produced by a typical infected individual early in an epidemic\".",
            "score": 67.91566467285156
        },
        {
            "docid": "984692_9",
            "document": "Computational sociology . By the late 1960s and early 1970s, social scientists used increasingly available computing technology to perform macro-simulations of control and feedback processes in organizations, industries, cities, and global populations. These models used differential equations to predict population distributions as holistic functions of other systematic factors such as inventory control, urban traffic, migration, and disease transmission. Although simulations of social systems received substantial attention in the mid-1970s after the Club of Rome published reports predicting global environmental catastrophe based upon the predictions of global economy simulations, the inflammatory conclusions also temporarily discredited the nascent field by demonstrating the extent to which results of the models are highly sensitive to the specific quantitative assumptions (backed by little evidence, in the case of the Club of Rome) made about the model's parameters. As a result of increasing skepticism about employing computational tools to make predictions about macro-level social and economic behavior, social scientists turned their attention toward micro-simulation models to make forecasts and study policy effects by modeling aggregate changes in state of individual-level entities rather than the changes in distribution at the population level. However, these micro-simulation models did not permit individuals to interact or adapt and were not intended for basic theoretical research.",
            "score": 67.24629974365234
        },
        {
            "docid": "3408308_21",
            "document": "Metabolic network modelling . In order to perform a dynamic simulation with such a network it is necessary to construct an ordinary differential equation system that describes the rates of change in each metabolite's concentration or amount. To this end, a rate law, i.e., a kinetic equation that determines the rate of reaction based on the concentrations of all reactants is required for each reaction. Software packages that include numerical integrators, such as COPASI or SBMLsimulator, are then able to simulate the system dynamics given an initial condition. Often these rate laws contain kinetic parameters with uncertain values. In many cases it is desired to estimate these parameter values with respect to given time-series data of metabolite concentrations. The system is then supposed to reproduce the given data. For this purpose the distance between the given data set and the result of the simulation, i.e., the numerically or in few cases analytically obtained solution of the differential equation system is computed. The values of the parameters are then estimated to minimize this distance. One step further, it may be desired to estimate the mathematical structure of the differential equation system because the real rate laws are not known for the reactions within the system under study. To this end, the program SBMLsqueezer allows automatic creation of appropriate rate laws for all reactions with the network. Synthetic accessibility is a simple approach to network simulation whose goal is to predict which metabolic gene knockouts are lethal. The synthetic accessibility approach uses the topology of the metabolic network to calculate the sum of the minimum number of steps needed to traverse the metabolic network graph from the inputs, those metabolites available to the organism from the environment, to the outputs, metabolites needed by the organism to survive. To simulate a gene knockout, the reactions enabled by the gene are removed from the network and the synthetic accessibility metric is recalculated. An increase in the total number of steps is predicted to cause lethality. Wunderlich and Mirny showed this simple, parameter-free approach predicted knockout lethality in \"E. coli\" and \"S. cerevisiae\" as well as elementary mode analysis and flux balance analysis in a variety of media.",
            "score": 66.84747314453125
        },
        {
            "docid": "42345073_8",
            "document": "West African Ebola virus epidemic . On 31 July 2015, the WHO announced \"an extremely promising development\" in the search for an effective vaccine for Ebola virus disease. While the vaccine had shown 100% efficacy in individuals, more conclusive evidence was needed regarding its capacity to protect populations through herd immunity. In August 2015, after substantial progress in reducing the scale of the epidemic, the WHO held a meeting to work out a \"Comprehensive care plan for Ebola survivors\" and identify research needed to optimize clinical care and social well-being. Stating that \"the Ebola outbreak has decimated families, health systems, economies, and social structures\", the WHO called the aftermath of the epidemic \"an emergency within an emergency.\" Of special concern is recent research that shows some Ebola survivors experience a so-called \"post-Ebola Syndrome\", with symptoms so severe that survivors may require medical care for months and even years. As the main epidemic was coming to an end in December 2015, the UN announced that 22,000 children had been orphaned, losing one or both parents to Ebola. On 29 March 2016, the Director-General of WHO terminated the Public Health Emergency of International Concern status of the West African Ebola virus epidemic.",
            "score": 66.3871841430664
        },
        {
            "docid": "44016208_2",
            "document": "Responses to the West African Ebola virus epidemic . Organizations from around the world responded to the West African Ebola virus epidemic. In July 2014, the World Health Organization (WHO) convened an emergency meeting with health ministers from eleven countries and announced collaboration on a strategy to co-ordinate technical support to combat the epidemic. In August, they declared the outbreak an international public health emergency and published a roadmap to guide and coordinate the international response to the outbreak, aiming to stop ongoing Ebola transmission worldwide within 6\u20139 months. In September, the United Nations Security Council declared the Ebola virus outbreak in the West Africa subregion a \"threat to international peace and security\" and unanimously adopted a resolution urging UN member states to provide more resources to fight the outbreak; the WHO stated that the cost for combating the epidemic will be a minimum of $1 billion.",
            "score": 65.83625030517578
        },
        {
            "docid": "54190305_13",
            "document": "FORMIND . Integration of forest inventroy data for parameter calibration: Parameter estimation in forest gap models is a time-consuming process. Manual calibration and sensitivity analysis of these models require a large number of simulations, leading to a higher computational demand. Thus, for the automatic calibration of model parameters, when direct measurements are missing or made under specific conditions (e.g., climate, soil), a collection of rapid stochastic calibration methods have been developed and applied in FORMIND. These methods automatically minimize the difference between simulation results and field observations by running the model a thousand times. Additionally, for the assessment of parameter uncertainty, approximate Bayesian methods can be used in combination with a Markov chain Monte Carlo approach. These methods can also be used for forest sites where a limited number of observations in time and space are available. After careful examination of the model results and available observation data, a combination of manual and automatic calibrations leads to the successful parameterization of forest gap models.",
            "score": 65.29244995117188
        },
        {
            "docid": "958031_4",
            "document": "Compartmental models in epidemiology . Compartmental models may be used to predict properties of how a disease spreads, for example the prevalence (total number of infected) or the duration of an epidemic. Also, one can understand how different situations may affect the outcome of the epidemic, e.g., what the best technique is for issuing a limited number of vaccines in a given population.",
            "score": 64.97081756591797
        },
        {
            "docid": "16181960_6",
            "document": "Plant disease forecasting . Forecasting systems may use one of several parameters in order to work out disease risk, or a combination of factors. One of the first forecasting systems designed was for Stewart's wilt and based on winter temperature index as low temperatures would kill the vector of the disease so there would be no outbreak. An example of a multiple disease/pest forecasting system is the EPIdemiology, PREdiction, and PREvention (EPIPRE) system developed in the Netherlands for winter wheat that focused on multiple pathogens. USPEST.org graphs risks of various plants diseases based on weather forecasts with hourly resolution of leaf wetness. Forecasting models are often based on a relationship like simple linear regression where x is used to predict y. Other relationships can be modelled using population growth curves. The growth curve that is used will depend on the nature of the epidemic. Polycyclic epidemics such as potato late blight are usually best modelled by using the logistic model, whereas monocyclic epidemics may be best modelled using the monomolecular model. Correct choice of a model is essential for a disease forecasting system to be useful.",
            "score": 64.01372528076172
        },
        {
            "docid": "7157316_2",
            "document": "Attack rate . In epidemiology, the attack rate is the biostatistical measure of frequency of morbidity, or speed of spread, in an at risk population. It is used in hypothetical predictions and during actual outbreaks of disease. An at risk population is defined as one that has no immunity to the attacking pathogen which can be either a novel pathogen or an established pathogen. It is used to project the number of victims to expect during an epidemic. This aids in marshalling resources for delivery of medical care as well as production of vaccines and/or anti-viral and anti-bacterial medicines. The rate is arrived at by taking the number of new cases in the population at risk and dividing by the number of persons at risk in the population.",
            "score": 63.93812561035156
        },
        {
            "docid": "366112_31",
            "document": "Chikungunya . Historically, chikungunya has been present mostly in the developing world. The disease causes an estimated 3 million infections each year. Epidemics in the Indian Ocean, Pacific Islands, and in the Americas, continue to change the distribution of the disease. In Africa, chikungunya is spread by a sylvatic cycle in which the virus largely cycles between other non-human primates, small mammals, and mosquitos between human outbreaks. During outbreaks, due to the high concentration of virus in the blood of those in the acute phase of infection, the virus can circulate from humans to mosquitoes and back to humans. The transmission of the pathogen between humans and mosquitoes that exist in urban environments was established on multiple occasions from strains occurring on the eastern half of Africa in non-human primate hosts. This emergence and spread beyond Africa may have started as early as the 18th century. Currently, available data does not indicate whether the introduction of chikungunya into Asia occurred in the 19th century or more recently, but this epidemic Asian strain causes outbreaks in India and continues to circulate in Southeast Asia. In Africa, outbreaks were typically tied to heavy rainfall causing increased mosquito population. In recent outbreaks in urban centers, the virus has spread by circulating between humans and mosquitoes.",
            "score": 63.911834716796875
        },
        {
            "docid": "40977477_17",
            "document": "Cross-species transmission . Two methods of measuring genetic variation, variable number tandem repeats (VNTRs) and single nucleotide polymorphisms (SNPs), have been very beneficial to the study of bacterial transmission. VNTRs, due to the low cost and high mutation rates, make them particularly useful to detect genetic differences in recent outbreaks, and while SNPs have a lower mutation rate per locus than VNTRs, they deliver more stable and reliable genetic relationships between isolates. Both methods are used to construct phylogenies for genetic analysis, however, SNPs are more suitable for studies on phylogenies contraction. However, it can be difficult for these methods accurately simulate CSTs everts. Estimates of CST based on phylogenys made using VNTR marker can be biased towards detecting CST events across a wide range of the parameters. SNPs tend to be less biased and variable in estimates of CST when estimations of CST rates are low and low number of SNPs is used. In general, CST rate estimates using these methods are most reliable in systems with more mutations, more markers, and high genetic differences between introduced strains. CST is very complex and models need to account for a lot of parameters to accurately represent the phenomena. Models that oversimplify reality can result in biased data. Multiple parameters such as number of mutations accumulated since introduction, stochasticity, the genetic difference of strains introduced, and the sampling effort can make unbiased estimates of CST difficult even with whole-genome sequences, especially if sampling is limited, mutation rates are low, or if pathogens were recently introduced. More information on the factors that influence CST rates is needed for the contraction of more appropriate models to study these events.",
            "score": 63.85139465332031
        },
        {
            "docid": "33882236_6",
            "document": "Adaptive collaborative control . Adaptive collaborative control is most accurately modeled as a closed loop feedback control system. Closed loop feedback control describes the event where the outputs of a system from an input are used to influence the present or future behavior of the system. The feedback control model is governed by a set of equations that are used to predict the future state of the simuland and regulate its behavior. These equations \u2013 in conjunction with principles of control theory \u2013 are used to evolve physical operations of the simuland to include, but not limited to: dialogue, path planning, motion, monitoring, and lifting objects over time. Many times, these equations are modeled as nonlinear partial differential equations over a continuous time domain.  Due to their complexity, powerful computers are necessary to implement these models. A consequence of using computers to simulate these models is that continuous systems cannot be fully calculated. Instead, numerical solutions, such as the Runge-Kutta methods, are utilized to approximate these continuous models.  These equations are initialized from the response of one or more sources and rates of changes and outputs are calculated. These rates of changes predict the states of the simuland a short time in the future. The time increment for this prediction is called a time step. These new states are applied to the model to determine the new rates of changes and observational data. This behavior is continued until the desired number of iterations is completed. In the event a future state violates or comes within a tolerance of the violation the simuland will confer with its human counterpart seeking advice on how to proceed from that point. The outputs, or observational data, are used by the human operators to determine what they believe is the best course of action for the simuland. Their commands are fed with the input into the control system and assessed regarding its effectiveness in resolving the issues. If the human commands are determined to be valuable, the simuland will adjust its control input to what the human suggested. If the human\u2019s commands are determined to be unbeneficial, malicious, or non-existent, the model will seek its own correction approach.",
            "score": 63.660118103027344
        },
        {
            "docid": "53458550_6",
            "document": "Surveillance of Partially Observable Systems . There are many systems that can be considered as partially observable due to their unknown or partially known structures or the nature of their unknown products and/or partially known results. The impacts of the consumption of genetically modified food (GM) are an example of a system that is only partially observable. The safety of genetically modified foods (GM) products has caused much controversy. Absence of sufficient and reliable information prevents neither certain confidence about the harmlessness of product consumption, nor any certain conclusion to merit a ban for fear of harm. The lack of any reliable or conclusive post-market observation and consumption effects information, make it difficult to establish a global protocol for such products. This paper introduces a model for the analysis of partially observable information from the surveillance of post-market consumption of systems such as genetically modified foods (GM) products. This model uses Markov Chains, paired with a Bayesian updating function to estimate the statistical impacts of surveillance observations and modified surveillance policies. A case study on population health status is used as an illustrative example, which is modeled to demonstrate the impact of policy interventions on simulated data. A cost decision analysis model is also applied to illustrate the impact of policy intervention costs. The model uses a first order Markov chain to estimate the period-over-period change in health status and a Bayesian updating procedure to estimate the population health status based on observations from post-market surveillance. The results show how observation samples can be used to provide information on system changes and improvements.",
            "score": 63.61046600341797
        },
        {
            "docid": "38889813_39",
            "document": "Viral phylodynamics . The extremely rapid turnover of the influenza population means that the rate of geographic spread of influenza lineages must also, to some extent, be rapid. Surveillance data show a clear pattern of strong seasonal epidemics in temperate regions and less periodic epidemics in the tropics. The geographic origin of seasonal epidemics in the Northern and Southern Hemispheres had been a major open question in the field. However, recent work by Rambaut et al. and Russell et al. has shown that temperate epidemics usually emerge from a global reservoir rather than emerging from within the previous season's genetic diversity. This work, and more recent work by Bedford et al. and Bahl et al., has suggested that the global persistence of the influenza population is driven by viruses being passed from epidemic to epidemic, with no individual region in the world showing continual persistence. However, there is considerable debate regarding the particular configuration of the global network of influenza, with one hypothesis suggesting a metapopulation in East and Southeast Asia that continually seeds influenza in the rest of the world, and another hypothesis advocating a more global metapopulation in which temperate lineages often return to the tropics at the end of a seasonal epidemic.",
            "score": 63.57807922363281
        },
        {
            "docid": "379591_72",
            "document": "World of Warcraft . The Corrupted Blood plague so closely resembled the outbreak of real-world epidemics that scientists are currently looking at the ways MMORPGs or other massively distributed systems can model human behavior during outbreaks. The reaction of players to the plague closely resembled previously hard-to-model aspects of human behavior that may allow researchers to more accurately predict how diseases and outbreaks spread amongst a population.",
            "score": 63.51023864746094
        },
        {
            "docid": "951614_2",
            "document": "Mathematical modelling of infectious disease . Mathematical models can project how infectious diseases progress to show the likely outcome of an epidemic and help inform public health interventions. Models use some basic assumptions and mathematics to find parameters for various infectious diseases and use those parameters to calculate the effects of possible interventions, like mass vaccination programmes.",
            "score": 62.91773223876953
        },
        {
            "docid": "40977477_14",
            "document": "Cross-species transmission . Alternative hosts can also potentially have a critical role in the evolution and diffusion of a pathogen. When a pathogen crosses species it often acquires new characteristics that allow it to breach host barriers. Different pathogen variants can have very different effects on host species. Thus it can be beneficial to CST analysis to compare the same pathogens occurring in different host species. Phylogenetic analysis can be used to track a pathogens history through different species populations. Even if a pathogen is new and highly divergent, phylogenetic comparison can be very insightful A useful strategy for investigating the history of epidemics caused by pathogen transmission combines molecular clock analysis, to estimate the timescale of the epidemic, and coalescent theory, to infer the demographic history of the pathogen. When constructing phylogenies, computer databases and tools are often used. Programs, such as BLAST, are used to annotate pathogen sequences, while databases like GenBank provide information about functions based on the pathogens genomic structure. Trees are constructed using computational methods such as MPR or Bayesian Inference, and models are created depending on the needs of the study. Single rate dated tip (SRDT) models, for example, allows for estimates of timescale under a phylogenetic tree. Models for CST prediction will vary depending on what parameters need to be accounted for when constructing the model.",
            "score": 62.7702751159668
        },
        {
            "docid": "6018203_16",
            "document": "HIV/AIDS in Asia . HIV is not currently a dominant epidemic in Pakistan. However, the number of cases is growing. Moderately high drug use and lack of acceptance that non-marital sex is common in the society have allowed the AIDS epidemic to take hold in Pakistan, mainly among injection drug users, some male sex workers and repatriated migrant workers. AIDS may yet become a major health issue. The National AIDS Control Programme\u2019s latest figures show that over 4,000 HIV cases have so far been reported since 1986, but UN and government estimates put the number of HIV/AIDS cases around 97,000 (range 46,000 to 210,000). More realistic estimates that are based on actual surveillance figures, however, suggest that this number may be closer to 40,000 - 45,000. The overall prevalence of HIV infection in adults aged 15 to 49 is 0.1% (and Shah \"et al.\" under review) (0.05% if one accepts the lower estimates).",
            "score": 62.748619079589844
        },
        {
            "docid": "37220_7",
            "document": "Infection . One way of proving that a given disease is \"infectious\", is to satisfy Koch's postulates (first proposed by Robert Koch), which demands that the infectious agent be identified only in patients and not in healthy controls, and that patients who contract the agent also develop the disease. These postulates were first used in the discovery that Mycobacteria species cause tuberculosis. Koch's postulates cannot be applied ethically for many human diseases because they require experimental infection of a healthy individual with a pathogen produced as a pure culture. Often, even clearly infectious diseases do not meet the infectious criteria. For example, \"Treponema pallidum\", the causative spirochete of syphilis, cannot be cultured \"in vitro\" \u2013 however the organism can be cultured in rabbit testes. It is less clear that a pure culture comes from an animal source serving as host than it is when derived from microbes derived from plate culture. Epidemiology is another important tool used to study disease in a population. For infectious diseases it helps to determine if a disease outbreak is sporadic (occasional occurrence), endemic (regular cases often occurring in a region), epidemic (an unusually high number of cases in a region), or pandemic (a global epidemic).",
            "score": 62.742366790771484
        },
        {
            "docid": "14415787_16",
            "document": "Health threat from cosmic rays . However, sample sizes for accurately estimating health risks directly from crew observations for the risks of concern (cancer, cataracts, cognitive and memory changes, late CNS risks, circulatory diseases, etc.) are large (typically \u00bb10 persons) and necessarily involve long post-mission observation times (>10 years). It will be difficult for a sufficient number of astronauts to occupy the ISS and for the missions to continue long enough to make an impact on risk predictions for late effects due to statistical limitations. Hence the need for ground-based research to predict cosmic ray health risks. In addition, radiation safety requirements mandate that risks should be adequately understood prior to astronauts incurring significant risks, and methods developed to mitigate the risks if necessary.",
            "score": 62.649818420410156
        },
        {
            "docid": "407904_48",
            "document": "Health in China . At the same time, in an ever-interconnected world, China has embraced its responsibility to global public health, including the strengthening of surveillance systems aimed at swiftly identifying and tackling the threat of infectious diseases such as SARS and avian influenza. Another major challenge is the epidemic of HIV/AIDS, a key priority for China.",
            "score": 62.41724395751953
        },
        {
            "docid": "12837694_19",
            "document": "Tuberculosis in China . The first of the key measures that have been implemented in the 3 years since the SARS crisis ended was greatly increased commitment and leadership from the government to tackle public health problems. During the SARS epidemic, governmental and communist party leaders at all levels\u2014from Paramount leader Hu Jintao, Premier Wen Jiabo and members of the State Council down to village leaders\u2014were involved in tackling a single public-health issue. The epidemic and its eventual control convinced Chinese leaders that the government should be much more involved in addressing public-health problems. After the SARS epidemic, the State Council developed a mechanism to oversee public-health emergencies directly. The State Council has also involved itself with other pressing public-health challenges, including HIV/AIDS, avian influenza, occupational safety, and environmental health.",
            "score": 62.34711837768555
        },
        {
            "docid": "2340491_44",
            "document": "History of HIV/AIDS . Sousa \"et al.\" then built computer simulations to test if an 'ill-adapted SIV' (meaning a simian immunodeficiency virus already infecting a human but incapable of transmission beyond the short acute infection period) could spread in colonial cities. The simulations used parameters of sexual transmission obtained from the current HIV literature. They modelled people's 'sexual links', with different levels of sexual partner change among different categories of people (prostitutes, single women with several partners a year, married women, and men), according to data obtained from modern studies of sexual activity in African cities. The simulations let the parameters (city size, proportion of people married, GUD frequency, male circumcision frequency, and transmission parameters) vary, and explored several scenarios. Each scenario was run 1,000 times, to test the probability of SIV generating long chains of sexual transmission. The authors postulated that such long chains of sexual transmission were necessary for the SIV strain to adapt better to humans, becoming an HIV capable of further epidemic emergence.",
            "score": 62.34083938598633
        },
        {
            "docid": "32476694_5",
            "document": "Howard Markel . \"When Germs Travel: Six Major Epidemics That Have Invaded America Since 1900 and the Fears They Have Unleashed\" expands the scope of \"Quarantine!\" by chronicling American epidemics during the two \"great waves of immigration\" that helped shape the 20th century. Markel argues that the association of immigrants with infectious disease is a key component of that history, and that their stigmatization during 20th century American epidemics \"reveal[s] much about our predispositions for dealing with the perpetual threat of contagious disease\". Health Affairs called \"When Germs Travel\" \"a clarion call for the public (and the government) to recognize both the importance and the precariousness of the public health as we enter the twenty-first century\".",
            "score": 62.261146545410156
        },
        {
            "docid": "12837694_7",
            "document": "Tuberculosis in China . In 2003, an epidemic of severe acute respiratory syndrome (SARS) broke out in China. (See \"Progress of the SARS outbreak\".) The spread of SARS brought to light substantial weaknesses in the country's public health system. After the SARS epidemic was brought under control, the Chinese government implemented a series of measures to strengthen its public health system. This effort coincided with acceleration in efforts to control tuberculosis. Within 3 years, implementation of the WHO-recommended DOTS (Directly Observed Therapy, Shortcourse) strategy to control tuberculosis increased from 68% to 100% of counties and the detection of cases of smear-positive tuberculosis by the public health system more than doubled, from 30% of new cases to 80%. Together with a tuberculosis treatment success rate of more than 90%, China achieved the 2005 global targets for tuberculosis control.",
            "score": 61.9916877746582
        },
        {
            "docid": "561221_4",
            "document": "Lucas critique . The Lucas critique is, in essence, a negative result. It tells economists, primarily, how \"not\" to do economic analysis. The Lucas critique suggests that if we want to predict the effect of a policy experiment, we should model the \"deep parameters\" (relating to preferences, technology, and resource constraints) that are assumed to govern \"individual\" behavior: so-called \"microfoundations.\" If these models can account for observed empirical regularities, we can then predict what individuals will do, \"taking into account\" the change in policy, and then aggregate the individual decisions to calculate the macroeconomic effects of the policy change.",
            "score": 61.862632751464844
        }
    ]
}