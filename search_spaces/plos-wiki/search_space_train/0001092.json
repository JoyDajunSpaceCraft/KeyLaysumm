{
    "q": [
        {
            "docid": "2860430_17",
            "document": "Neural oscillation . If a group of neurons engages in synchronized oscillatory activity, the neural ensemble can be mathematically represented as a single oscillator. Different neural ensembles are coupled through long-range connections and form a network of weakly coupled oscillators at the next spatial scale. Weakly coupled oscillators can generate a range of dynamics including oscillatory activity. Long-range connections between different brain structures, such as the thalamus and the cortex (see thalamocortical oscillation), involve time-delays due to the finite conduction velocity of axons. Because most connections are reciprocal, they form feed-back loops that support oscillatory activity. Oscillations recorded from multiple cortical areas can become synchronized to form large scale brain networks, whose dynamics and functional connectivity can be studied by means of spectral analysis and Granger causality measures. Coherent activity of large-scale brain activity may form dynamic links between brain areas required for the integration of distributed information.",
            "score": 100.02034056186676
        },
        {
            "docid": "28289092_2",
            "document": "Dynamical neuroscience . The dynamical systems approach to neuroscience is a branch of mathematical biology that utilizes nonlinear dynamics to understand and model the nervous system and its functions. In a dynamical system, all possible states are expressed by a phase space. Such systems can experience bifurcation (a qualitative change in behavior) as a function of its bifurcation parameters and often exhibit chaos. Dynamical neuroscience describes the non-linear dynamics at many levels of the brain from single neural cells to cognitive processes, sleep states and the behavior of neurons in large-scale neuronal simulation.",
            "score": 101.13713717460632
        },
        {
            "docid": "2227943_9",
            "document": "Krasnow Institute for Advanced Study . The Center for Neural Informatics, Neural Structures, and Neural Plasticity (CN3) pursues fundamental breakthroughs in neuroscience by fostering neuroinformatic and computational approaches to neuroplasticity and neuroanatomy. By bringing together faculty expertise in these multiple disciplines, the Center provides opportunities for cross-training in neuroscience, psychology, and engineering, both at the graduate and postdoctoral levels. CN3 researchers investigate the relationship between brain structure, activity, and function from the subcellular to the network level, with a specific focus on the biophysical and biochemical mechanisms of learning and memory. In the long term, we seek to create large-scale, biologically plausible network models of entire portions of the mammalian brain, such as the hippocampus, to understand the neural circuits and cellular events underlying the expression, storage, and retrieval of associative memory.",
            "score": 85.84745597839355
        },
        {
            "docid": "40158142_9",
            "document": "Nonlinear system identification . Artificial neural networks try loosely to imitate the network of neurons in the brain where computation takes place through a large number of simple processing elements. A typical neural network consists of a number of simple processing units interconnected to form a complex network. Layers of such units are arranged so that data is entered at the input layer and passes through either one or several intermediate layers before reaching the output layer. In supervised learning the network is trained by operating on the difference between the actual output and the desired output of the network, the prediction error, to change the connection strengths between the nodes. By iterating the weights are modified until the output error reaches an acceptable level. This process is called machine learning because the network adjusts the weights so that the output pattern is reproduced. Neural networks have been extensively studied and there are many excellent textbooks devoted to this topic in general, and more focussed textbooks which emphasise control and systems applications. There are two main problem types that can be studied using neural networks: static problems, and dynamic problems. Static problems include pattern recognition, classification, and approximation. Dynamic problems involve lagged variables and are more appropriate for system identification and related applications. Depending on the architecture of the network the training problem can be either nonlinear-in-the-parameters which involves optimisation or linear-in-the-parameters which can be solved using classical approaches. The training algorithms can be categorised into supervised, unsupervised, or reinforcement learning. Neural networks have excellent approximation properties but these are usually based on standard function approximation results using for example the Weierstrass Theorem that applies equally well to polynomials, rational functions, and other well-known models.  Neural networks have been applied extensively to system identification problems which involve nonlinear and dynamic relationships. However, classical neural networks are purely gross static approximating machines. There is no dynamics within the network. Hence when fitting dynamic models all the dynamics arise by allocating lagged inputs and outputs to the input layer of the network. The training procedure then produces the best static approximation that relates the lagged variables assigned to the input nodes to the output. There are more complex network architectures, including recurrent networks, that produce dynamics by introducing increasing orders of lagged variables to the input nodes. But in these cases it is very easy to over specify the lags and this can lead to over fitting and poor generalisation properties.  Neural networks have several advantages; they are conceptually simple, easy to train and to use, have excellent approximation properties, the concept of local and parallel processing is important and this provides integrity and fault tolerant behaviour. The biggest criticism of the classical neural network models is that the models produced are completely opaque and usually cannot be written down or analysed. It is therefore very difficult to know what is causing what, to analyse the model, or to compute dynamic characteristics from the model. Some of these points will not be relevant to all applications but they are for dynamic modelling.",
            "score": 100.7521538734436
        },
        {
            "docid": "2860430_25",
            "document": "Neural oscillation . The Kuramoto model of coupled phase oscillators is one of the most abstract and fundamental models used to investigate neural oscillations and synchronization. It captures the activity of a local system (e.g., a single neuron or neural ensemble) by its circular phase alone and hence ignores the amplitude of oscillations (amplitude is constant). Interactions amongst these oscillators are introduced by a simple algebraic form (such as a sine function) and collectively generate a dynamical pattern at the global scale. The Kuramoto model is widely used to study oscillatory brain activity and several extensions have been proposed that increase its neurobiological plausibility, for instance by incorporating topological properties of local cortical connectivity. In particular, it describes how the activity of a group of interacting neurons can become synchronized and generate large-scale oscillations. Simulations using the Kuramoto model with realistic long-range cortical connectivity and time-delayed interactions reveal the emergence of slow patterned fluctuations that reproduce resting-state BOLD functional maps, which can be measured using fMRI.",
            "score": 87.94759976863861
        },
        {
            "docid": "2860430_8",
            "document": "Neural oscillation . Neural oscillations are commonly studied from a mathematical framework and belong to the field of \"neurodynamics\", an area of research in the cognitive sciences that places a strong focus upon the dynamic character of neural activity in describing brain function. It considers the brain a dynamical system and uses differential equations to describe how neural activity evolves over time. In particular, it aims to relate dynamic patterns of brain activity to cognitive functions such as perception and memory. In very abstract form, neural oscillations can be analyzed analytically. When studied in a more physiologically realistic setting, oscillatory activity is generally studied using computer simulations of a computational model.",
            "score": 91.71567511558533
        },
        {
            "docid": "33064706_9",
            "document": "Eshel Ben-Jacob . Ben-Jacob's studies in neuroscience are guided by an effort to simplify the complexity searching for principles of information coding, memory and learning. He has many unique contributions in the field of Systems Neuroscience and Neural Networks, including the relations between network size and its synchronized activity, the discovery of hidden neuron correlations, function-form relations and mutual synchronization in engineered networks, the effect of DNA damage on network synchronization, neuro-glia communication, new modeling of intra- and inter-cell calcium dynamics, using nano technology for network engineering, discovery and modeling of the dynamical motives (repertoire) of coupled neural networks, development of a novel system-level analysis of neural network activity (the functional holography analysis), mapping and assessments of epileptic foci, and more. Yet, the development of the first neuro-memory-chip with his doctoral student at the time, Itay Baruchi, is Ben-Jacob's most important contribution in systems neuroscience. While previous attempts were based on \"teaching by reward\" (enhancing excitatory synapses) or \"teaching by punishment\" (inhibition of excitatory synapses), Baruchi and Ben-Jacob's approach was \"teaching by liberation\", or \"inhibition of inhibition\" (inhibition of inhibitory synapses). Being recognized as a groundbreaking discovery in systems neuroscience, the achievement was awarded in 2007 the SciAm 50, The Scientific American Award for the 50 most important achievements in all fields of science and technology.",
            "score": 100.07225275039673
        },
        {
            "docid": "47511015_2",
            "document": "Large scale brain networks . Large scale brain networks are collections of widespread brain regions showing functional connectivity by statistical analysis of the fMRI BOLD signal or other signal fluctuations. An emerging paradigm in neuroscience is that cognitive tasks are performed not by individual brain regions working in isolation, but by networks consisting of several discrete brain regions that are said to be \"functionally connected\" due to tightly coupled activity. Functional connectivity may be measured as long-range synchronization of the EEG, MEG, or other dynamic brain signals. Synchronized brain regions may also be identified using spatial independent component analysis. The set of identified brain areas that are linked together in a large-scale network varies with cognitive function. When the cognitive state is not explicit (i.e., the subject is at \"rest\"), the large scale brain network is a resting state network (RSN). As a physical system with graph-like properties, a large scale brain network has both nodes and edges, and cannot be identified simply by the co-activation of brain areas. In recent decades, the analysis of brain networks was made feasible by advances in imaging techniques as well as new tools from graph theory and dynamical systems. Large scale brain networks are identified by their function, and provide a coherent framework for understanding cognition by offering a neural model of how different cognitive functions emerge when different sets of brain regions join together as self-organized coalitions. Disruptions in activity in various networks have been implicated in neuropsychiatric disorders such as depression, Alzheimer's, autism spectrum disorder, schizophrenia and bipolar disorder.",
            "score": 82.611647605896
        },
        {
            "docid": "2860430_24",
            "document": "Neural oscillation . Neural field models are another important tool in studying neural oscillations and are a mathematical framework describing evolution of variables such as mean firing rate in space and time. In modeling the activity of large numbers of neurons, the central idea is to take the density of neurons to the continuum limit, resulting in spatially continuous neural networks. Instead of modelling individual neurons, this approach approximates a group of neurons by its average properties and interactions. It is based on the mean field approach, an area of statistical physics that deals with large-scale systems. Models based on these principles have been used to provide mathematical descriptions of neural oscillations and EEG rhythms. They have for instance been used to investigate visual hallucinations.",
            "score": 112.31832981109619
        },
        {
            "docid": "43374303_4",
            "document": "Andreas K. Engel . Andreas Engel has become known by his work on the so-called \u201ebinding problem\u201c. His research focuses on the hypothesis that temporal synchrony serves for dynamic coordination of signals in the brain. In addition to working on the experimental validation of this hypothesis, Engel pursues research on its cognitive and theoretical implications. As a postdoc with Wolf Singer at the Max Planck Institute for Brain Research at Frankfurt, Engel was involved in studies that demonstrated the relevance of neural synchrony, in particular of so-called gamma waves, for processing of perceptual information. In particular, the group provided evidence that temporal correlations can serve for the binding of features into coherent sensory representations. In addition to addressing the relevance of synchrony and neuronal oscillations in the visual system, the work of Engel's group yielded evidence for a relation between neural synchrony and visual awareness. In addition, Engel and coworkers contributed to demonstrating a functional role of neural synchrony for sensorimotor coupling. In the past 15 years, Engel's group has expanded their work to the human brain, using EEG and MEG in combination with source modeling techniques. The results of these studies demonstrate the importance of neuronal oscillations and synchrony for perceptual processing, attention, working memory, decision-making and consciousness. Recent work of the group on the interaction of visual, auditory and tactile systems suggests a role of temporal binding for multisensory integration. Furthermore, the group has developed novel methods for the electrophysiological analysis of resting state network activity. Engel's group also applies these approaches for the study of network malfunction in patients with movement disorders, multiple sclerosis and schizophrenia, in studies on pain, and altered networks after early sensory deprivation.  Engel also explores implications of these neurophysiogical results for theories of perception, cognition and action. A major focus of his work are the implications of the studies on neural synchrony for understanding the neural correlates of consciousness. Recent papers address links between neural dynamics and enactive views of cognition, investigating the grounding of cognition in sensorimotor coupling.",
            "score": 94.67862784862518
        },
        {
            "docid": "30180542_5",
            "document": "Rui de Figueiredo . Figueiredo is best known for his work developing novel mathematical foundations for the solution of fundamentally nonlinear problems, with applications in pattern recognition, signal processing, image processing, and neural networks. His work supported a variety of NASA space exploration projects, assisted the Department of Defense in weapons detection systems, helped companies identify credit card fraud, assisted the Environmental Protection Agency in oil spill detection and source matching, developed algorithms for more efficient transmission of mobile telecommunications signals, enhanced geophysical images for well-logging, and improved the early detection of brain and neural diseases, like Alzheimer's disease. In the early 1970s, Figueiredo introduced approaches for generalised splines for optimal signal based recovery to the field of signal processing. One of his most well-known contributions was the invention and study of the Generalised Fock space F, a Reproducing Kernel Hilbert Space of input-output maps of generic nonlinear dynamical systems, and used a \u201clinear\u201d orthogonal projection in F for optimal recovery of such \u201cnonlinear\u201d maps from the input-output data. This approach extended to nonlinear systems the powerful orthogonal projection method, previously used exclusively for linear systems. The analytics behind this approach are represented as neural networks, which ultimately led to the development of Figueiredo\u2019s Optimal Interpolation neural network and CDL neural network. Related to his work in neural networks, Figueiredo is also known for his contributions to the understanding of nonlinear filters. In this area, Figueiredo developed filters for adaptive image restoration, for image contrast sharpening tuned to human visual perception based on Munsell\u2019s scale, and for non-Gaussian noise suppression. The results of his work can be found in over 400 scientific publications he authored.",
            "score": 79.14094650745392
        },
        {
            "docid": "36237055_6",
            "document": "Post-contemporary . The origin in diversely phenomenological systems and exact models from life sciences is in the area of the theory of applied dynamical systems and global bifurcations. The special interest in the subject can be find in a new emergent cross-disciplinary field known as mathematical neuroscience. Its scopes include nonlinear models of individual neurons and networks. In-depth analysis of such systems requires development of advanced mathematical tools paired with sophisticated computations. For instance, Andrey Shilnikov, a neuroscientist and mathematician derives models and create bifurcation toolkits for studying a stunning array of complex activities such as multistability of individual neurons and polyrhythmic bursting patterns discovered in multifunctional central pattern generators governing vital locomotor behaviors of animals and humans. Thanks to the non linear qualitative dynamics, the organization of holistic nature of post-contemporary lineage was implemented by interoprativity and interactivity within the heterogeneous components and characters of Micro - systems, being present in many concrete social contents and in most territorial recourses. In this way post-structuralism as well as the new sciences of complexity, Complexity theory and chaos theory, were appropriated and interpreted within Micro - systems by means of \"self-creation\", expressing a fundamental dialectic among structure, mechanism and function, identifiable and recognizable in concrete territorial contexts.",
            "score": 93.95652186870575
        },
        {
            "docid": "17935654_2",
            "document": "Integrative neuroscience . Integrative neuroscience sculptures a theoretical neuroscience with a mathematical neuroscience that is different from computational neuroscience. In computational neuroscience, reductionist approaches span multiple levels of neural organization. However, in integrative neuroscience, each level is seamlessly sculptured as part of a continuum of levels. The roots of integrative neuroscience originated from the Rashevsky-Rosen school of relational biology that characterizes functional organization mathematically by abstracting away the structure (i.e., physics and chemistry). It was further expanded by Chauvet who introduced hierarchical and functional integration. Hierarchical integration is structural involving spatiotemporal dynamic continuity in Euclidean space to bring about functional organization, viz. However, functional integration is relational and as such this requires a topology not restricted to Euclidean space, but rather occupying vector spaces This means that for any given functional organization the methods of functional analysis enable a relational organization to be mapped by the functional integration, viz.  Thus hierarchical and functional integration entails a \"neurobiology of cognitive semantics\" where hierarchical organization is associated with the neurobiology and relational organization is associated with the cognitive semantics. Relational organization throws away the matter; \"function dictates structure\", hence material aspects are entailed, while in reductionism the causal nexus between structure and dynamics entails function that obviates functional integration because the causal entailment in the brain of hierarchical integration is absent from the structure. If integrative neuroscience is studied from the viewpoint of functional organization of hierarchical levels then it is defined as causal entailment in the brain of hierarchical integration. If it is studied from the viewpoint of relational organization then it is defined as semantic entailment in the brain of functional integration. It aims to present studies of functional organization of particular brain systems across scale through hierarchical integration leading to species-typical behaviors under normal and pathological states. As such, integrative neuroscience aims for a unified understanding of brain function across scale. Spivey's continuity of mind thesis extends integrative neuroscience to the domain of continuity psychology.",
            "score": 61.19011902809143
        },
        {
            "docid": "33548913_9",
            "document": "Dehaene\u2013Changeux model . Furthermore, exploring the neural dynamics of cognitive efforts after, \"inter alia\", the Dehaene-Changeux Model, Kitzbichler et al. (2011b) demonstrated how cognitive effort breaks the modularity of mind to make human brain functional networks transiently adopt a more efficient but less economical configuration. Werner (2007a) used the Dehaene-Changeux Global Neuronal Workspace to defend the use of statistical physics approaches for exploring phase transitions, scaling and universality properties of the so-called \"Dynamic Core\" of the brain, with relevance to the macroscopic electrical activity in EEG and EMG. Furthermore, building from the Dehaene-Changeux Model, Werner (2007b) proposed that the application of the twin concepts of scaling and universality of the theory of non-equilibrium phase transitions can serve as an informative approach for elucidating the nature of underlying neural-mechanisms, with emphasis on the dynamics of recursively reentrant activity flow in intracortical and cortico-subcortical neuronal loops. Friston (2000) also claimed that \"\"the nonlinear nature of asynchronous coupling enables the rich, context-sensitive interactions that characterize real brain dynamics, suggesting that it plays a role in functional integration that may be as important as synchronous interactions\"\".",
            "score": 69.16435766220093
        },
        {
            "docid": "271430_22",
            "document": "Computational neuroscience . The interactions of neurons in a small network can be often reduced to simple models such as the Ising model. The statistical mechanics of such simple systems are well-characterized theoretically. There has been some recent evidence that suggests that dynamics of arbitrary neuronal networks can be reduced to pairwise interactions. It is not known, however, whether such descriptive dynamics impart any important computational function. With the emergence of two-photon microscopy and calcium imaging, we now have powerful experimental methods with which to test the new theories regarding neuronal networks.",
            "score": 84.67587184906006
        },
        {
            "docid": "3717_60",
            "document": "Brain . Computational neuroscience encompasses two approaches: first, the use of computers to study the brain; second, the study of how brains perform computation. On one hand, it is possible to write a computer program to simulate the operation of a group of neurons by making use of systems of equations that describe their electrochemical activity; such simulations are known as \"biologically realistic neural networks\". On the other hand, it is possible to study algorithms for neural computation by simulating, or mathematically analyzing, the operations of simplified \"units\" that have some of the properties of neurons but abstract out much of their biological complexity. The computational functions of the brain are studied both by computer scientists and neuroscientists.",
            "score": 95.12734842300415
        },
        {
            "docid": "33818014_15",
            "document": "Nervous system network models . The concept of artificial neural network (ANN) was introduced by McColloch, W. S. & Pitts, W. (1943) for models based on behavior of biological neurons. Norbert Wiener (1961) gave this new field the popular name of cybernetics, whose principle is the interdisciplinary relationship among engineering, biology, control systems, brain functions, and computer science. With the computer science field advancing, the von Neumann-type computer was introduced early in the neuroscience study. But it was not suitable for symbolic processing, nondeterministic computations, dynamic executions, parallel distributed processing, and management of extensive knowledge bases, which are needed for biological neural network applications; and the direction of mind-like machine development changed to a learning machine. Computing technology has since advanced extensively and computational neuroscience is now able to handle mathematical models developed for biological neural network. Research and development are progressing in both artificial and biological neural networks including efforts to merge the two.",
            "score": 93.55333387851715
        },
        {
            "docid": "10159567_7",
            "document": "Spiking neural network . In practice, there is a major difference between the theoretical power of spiking neural networks and what has been demonstrated. They have proved useful in neuroscience, but not (yet) in engineering. Some large scale neural network models have been designed that take advantage of the pulse coding found in spiking neural networks, these networks mostly rely on the principles of reservoir computing. However, the real world application of large scale spiking neural networks has been limited because the increased computational costs associated with simulating realistic neural models have not been justified by commensurate benefits in computational power. As a result, there has been little application of large scale spiking neural networks to solve computational tasks of the order and complexity that are commonly addressed using rate coded (second generation) neural networks. In addition it can be difficult to adapt second generation neural network models into real time, spiking neural networks (especially if these network algorithms are defined in discrete time). It is relatively easy to construct a spiking neural network model and observe its dynamics. It is much harder to develop a model with stable behavior that computes a specific function.",
            "score": 87.77112281322479
        },
        {
            "docid": "25181073_27",
            "document": "Neuroscience of multilingualism . Functional neuroimaging methods such as PET and fMRI are used to study the complex neural mechanisms of the human language systems. Functional neuroimaging is used to determine the most important principles of cerebral language organization in bilingual persons. Based on the evidence we can conclude that the bilingual brain is not the addition of two monolingual language systems, but operates as a complex neural network that can differ across individuals.",
            "score": 75.69189620018005
        },
        {
            "docid": "50282107_5",
            "document": "Haim Sompolinsky . Sompolinsky\u2019s research includes spike-based neural learning and computation, neuronal population codes, sensory representations, dynamics and function of sensory and motor cortical circuits, and large-scale structure and dynamics of human brain. He also studies the relation between physics, neuroscience, and human volition, freedom and agency.",
            "score": 66.27923226356506
        },
        {
            "docid": "30767825_9",
            "document": "Ephaptic coupling . A hypothesis or explanation behind the mechanism is \"one-way\", \"master-slave\", or \"unidirectional synchronization\" effect as mathematical and fundamental property of non-linear dynamic systems (oscillators like neurons) to synchronize under certain criteria. Such phenomenon was proposed and predicted to be possible between two HR neurons, since 2010 in simulations and modeling work by Hrg. It was also shown that such unidirectional synchronization or copy/paste transfer of neural dynamics from master to slave(s) neurons, could be exhibited in different ways. Hence the phenomenon is of not only fundamental interest but also applied one from treating epilepsy to novel learning systems. Synchronization of neurons is in principle unwanted behavior, as brain would have zero information or be simply a bulb if all neurons would synchronize. Hence it is a hypothesis that neurobiology and evolution of brain coped with ways of preventing such synchronous behavior on large scale, using it rather in other special cases.",
            "score": 73.55191051959991
        },
        {
            "docid": "521509_23",
            "document": "Bar-Ilan University . The Leslie and Susan Gonda Multidisciplinary Brain Research Center (Hebrew: \u05d4\u05de\u05e8\u05db\u05d6 \u05d4\u05e8\u05d1 \u05ea\u05d7\u05d5\u05de\u05d9 \u05dc\u05d7\u05e7\u05e8 \u05d4\u05de\u05d5\u05d7 \u05e2\u2033\u05e9 \u05dc\u05e1\u05dc\u05d9 \u05d5\u05e1\u05d5\u05d6\u05df \u05d2\u05d5\u05e0\u05d3\u05d4) focuses on a multidisciplinary approach to neuroscience. It houses over 30 laboratories investigating brain complexity at multiple levels, from single neurons, through information processing and computations in neural networks to cognition, behavior and human mind. The center's core members and affiliates combine multiple fields that are crucial for brain understanding, including molecular and systems neuroscience, cognitive neuroscience, psychology, psychiatry, linguistics, mathematics, computer sciences, engineering and physics. Numerous research approaches are employed by the center\u2019s scientists, such as brain stimulation techniques, neuroimaging, electrophysiology, molecular techniques, computational methods, mathematical modeling and behavioral and cognitive paradigms. The center was founded in 2002 thanks to the contributions of the Gonda family, the president of Bar-Ilan University Moshe Kaveh, and Moshe Abeles, a pioneer of Israel\u2019s neuroscience research, Emet Prize laureate (2004) and the founding director of the Interdisciplinary Center for Neural Computation at the Hebrew University (1992\u20131999), has led the Gonda Multidisciplinary Brain Research Center for nearly a decade. Since 2011 the center is headed by Moshe Bar, a cognitive neuroscientist and an expert in brain imaging technologies. Bar returned to Israel to head the Gonda Multidisciplinary Brain Research Center as its new director after thirteen years at Harvard University.",
            "score": 70.64874029159546
        },
        {
            "docid": "39198919_2",
            "document": "Cancer systems biology . Cancer systems biology encompasses the application of systems biology approaches to cancer research, in order to study the disease as a complex adaptive system with emerging properties at multiple biological scales. Cancer systems biology represents the application of systems biology approaches to the analysis of how the intracellular networks of normal cells are perturbed during carcinogenesis to develop effective predictive models that can assist scientists and clinicians in the validations of new therapies and drugs. Tumours are characterized by genomic and epigenetic instability that alters the functions of many different molecules and networks in a single cell as well as altering the interactions with the local environment. Cancer systems biology approaches, therefore, are based on the use of computational and mathematical methods to decipher the complexity in tumorigenesis as well as cancer heterogeneity.  Cancer systems biology encompasses concrete applications of systems biology approaches to cancer research, notably (a) the need for better methods to distill insights from large-scale networks, (b) the importance of integrating multiple data types in constructing more realistic models, (b) challenges in translating insights about tumorigenic mechanisms into therapeutic interventions, and (d) the role of the tumor microenvironment, at the physical, cellular, and molecular levels. Cancer systems biology therefore adopts a holistic view of cancer aimed at integrating its many biological scales, including genetics, signaling networks, epigenetics, cellular behavior, histology, (pre)clinical manifestations and epidemiology. Ultimately, cancer properties at one scale, e.g., histology, are explained by properties at a scale below, e.g., cell behavior.",
            "score": 73.33849108219147
        },
        {
            "docid": "21855574_5",
            "document": "Brain simulation . The connectivity of the neural circuit for touch sensitivity of the simple C. elegans nematode (roundworm) was mapped in 1985 and partly simulated in 1993. Since 2004, many software simulations of the complete neural and muscular system have been developed, including simulation of the worm's physical environment. Some of these models have been made available for download. However, there is still a lack of understanding of how the neurons and the connections between them generate the surprisingly complex range of behaviors that are observed in the relatively simple organism. This contrast between the apparent simplicity of how the mapped neurons interact with their neighbours, and exceeding complexity of the overall brain function, is an example of an emergent property. Interestingly, this kind of emergent property is paralleled within artificial neural networks, the neurons of which are exceedingly simple compared to their often complex, abstract outputs.",
            "score": 90.12760424613953
        },
        {
            "docid": "3717_69",
            "document": "Brain . Theorists have worked to understand these response patterns by constructing mathematical models of neurons and neural networks, which can be simulated using computers. Some useful models are abstract, focusing on the conceptual structure of neural algorithms rather than the details of how they are implemented in the brain; other models attempt to incorporate data about the biophysical properties of real neurons. No model on any level is yet considered to be a fully valid description of brain function, though. The essential difficulty is that sophisticated computation by neural networks requires distributed processing in which hundreds or thousands of neurons work cooperatively\u2014current methods of brain activity recording are only capable of isolating action potentials from a few dozen neurons at a time.",
            "score": 94.92245054244995
        },
        {
            "docid": "40354723_3",
            "document": "Structural complexity (applied mathematics) . Structural complexity emerges from all systems that display morphological organization (Nicolis & Prigogine 1989). Filamentary structures, for instance, are an example of coherent structures that emerge, interact and evolve in many physical and biological systems, such as mass distribution in the Universe, vortex filaments in turbulent flows, neural networks in our brain and genetic material (such as DNA) in a cell. In general information on the degree of morphological disorder present in the system tells us something important about fundamental physical or biological processes.",
            "score": 57.238964796066284
        },
        {
            "docid": "20590_20",
            "document": "Mathematical model . In black-box models one tries to estimate both the functional form of relations between variables and the numerical parameters in those functions. Using a priori information we could end up, for example, with a set of functions that probably could describe the system adequately. If there is no a priori information we would try to use functions as general as possible to cover all different models. An often used approach for black-box models are neural networks which usually do not make assumptions about incoming data. Alternatively the NARMAX (Nonlinear AutoRegressive Moving Average model with eXogenous inputs) algorithms which were developed as part of nonlinear system identification can be used to select the model terms, determine the model structure, and estimate the unknown parameters in the presence of correlated and nonlinear noise. The advantage of NARMAX models compared to neural networks is that NARMAX produces models that can be written down and related to the underlying process, whereas neural networks produce an approximation that is opaque.",
            "score": 76.3663101196289
        },
        {
            "docid": "3062721_16",
            "document": "Neuroinformatics . Biology is concerned with molecular data (from genes to cell specific expression); medicine and anatomy with the structure of synapses and systems level anatomy; engineering \u2013 electrophysiology (from single channels to scalp surface EEG), brain imaging; computer science \u2013 databases, software tools, mathematical sciences \u2013 models, chemistry \u2013 neurotransmitters, etc. Neuroscience uses all aforementioned experimental and theoretical studies to learn about the brain through its various levels. Medical and biological specialists help to identify the unique cell types, and their elements and anatomical connections. Functions of complex organic molecules and structures, including a myriad of biochemical, molecular, and genetic mechanisms which regulate and control brain function, are determined by specialists in chemistry and cell biology. Brain imaging determines structural and functional information during mental and behavioral activity. Specialists in biophysics and physiology study physical processes within neural cells neuronal networks. The data from these fields of research is analyzed and arranged in databases and neural models in order to integrate various elements into a sophisticated system; this is the point where neuroinformatics meets other disciplines.",
            "score": 89.97237277030945
        },
        {
            "docid": "28289092_24",
            "document": "Dynamical neuroscience . The computational approaches to theoretical neuroscience often employ artificial neural networks that simplify the dynamics of single neurons in favor of examining more global dynamics. While neural networks are often associated with artificial intelligence, they have also been productive in the cognitive sciences. Artificial neural networks use simple neuron models, but their global dynamics are capable of exhibiting both Hopfield and Attractor-like network dynamics.",
            "score": 90.73703455924988
        },
        {
            "docid": "26327530_7",
            "document": "Robert Kozma . Kozma's current research interests include spatio-temporal dynamics of neural processes, random graph approaches to large-scale networks, such as neural networks, computational intelligence methods for knowledge acquisition and autonomous decision making in biological and artificial systems.",
            "score": 63.85092735290527
        },
        {
            "docid": "2506529_35",
            "document": "Cellular neural network . CNN processors are being used to understand systems that can be modeled using simple, coupled units, such as living cells, biological networks, physiological systems, and ecosystems. The CNN architecture captures some of the dynamics often seen in nature and is simple enough to analyze and conduct experiments. They are also being used for stochastic simulation techniques, which allow scientists to explore spin problems, population dynamics, lattice-based gas models, percolation, and other phenomena. Other simulation applications include heat transfer, mechanical vibrating systems, protein production, Josephson Transmission Line (JTL) problems, seismic wave propagation, and geothermal structures. Instances of 3D (Three Dimensional) CNN have been used to prove known complex shapes are emergent phenomena in complex systems, establishing a link between art, dynamical systems and VLSI technology. CNN processors have been used to research a variety of mathematical concepts, such as researching non-equilibrium systems, constructing non-linear systems of arbitrary complexity using a collection of simple, well-understood dynamic systems, studying emergent chaotic dynamics, generating chaotic signals, and in general discovering new dynamic behavior. They are often used in researching systemics, a trandisiplinary, scientific field that studies natural systems. The goal of systemics researchers is to develop a conceptual and mathematical framework necessary to analyze, model, and understand systems, including, but not limited to, atomic, mechanical, molecular, chemical, biological, ecological, social and economic systems. Topics explored are emergence, collective behavior, local activity and its impact on global behavior, and quantifying the complexity of an approximately spatial and topologically invariant system . Although another measure of complexity may not arouse enthusiasm (Seth Lloyd, a professor from Massachusetts Institute of Technology (MIT), has identified 32 different definitions of complexity), it can potentially be mathematically advantageous when analyzing systems such as economic and social systems.",
            "score": 91.56591296195984
        },
        {
            "docid": "39199253_5",
            "document": "Percolation (cognitive psychology) . Percolation has been developed outside of the cognitive sciences; however, its application in the field has proven it to be a useful tool for understanding neural processes. Researchers have focused their attention not only studying how neural activity is diffused across networks but also how percolation and its aspect of phase transition can affect decision making and thought processes. Percolation theory has enabled researchers to better understand many psychological conditions, such as epilepsy, disorganized schizophrenia and divergent thinking. These conditions are often indicative of percolating clusters and their involvement in propagating the excess firing of neurons. Seizures occur when neurons in the brain fire simultaneously, and often these seizures can occur in one part of the brain and transfer to other parts. Researchers are able to facilitate a better understanding of these conditions because \"the neurons involved in a seizure are analogous to the sites in a percolating cluster\". Disorganized schizophrenia is more complex as the activity is indicative activity in a percolating cluster; however, some researchers have suggested that the percolation of information occurs not in a small cluster but on a global functional scale. Attention as well as percolation also plays a key role in disorganized and divergent thinking; however, it is more likely that directed percolation, that is a directionally controlled percolation, is more useful to study divergent thinking and creativity.",
            "score": 89.43828535079956
        }
    ],
    "r": [
        {
            "docid": "2567511_17",
            "document": "Neural engineering . Scientists can use experimental observations of neuronal systems and theoretical and computational models of these systems to create Neural networks with the hopes of modeling neural systems in as realistic a manner as possible. Neural networks can be used for analyses to help design further neurotechnological devices. Specifically, researchers handle analytical or finite element modeling to determine nervous system control of movements and apply these techniques to help patients with brain injuries or disorders. Artificial neural networks can be built from theoretical and computational models and implemented on computers from theoretically devices equations or experimental results of observed behavior of neuronal systems. Models might represent ion concentration dynamics, channel kinetics, synaptic transmission, single neuron computation, oxygen metabolism, or application of dynamic system theory (LaPlaca et al. 2005). Liquid-based template assembly was used to engineer 3D neural networks from neuron-seeded microcarrier beads.",
            "score": 112.61767578125
        },
        {
            "docid": "2860430_24",
            "document": "Neural oscillation . Neural field models are another important tool in studying neural oscillations and are a mathematical framework describing evolution of variables such as mean firing rate in space and time. In modeling the activity of large numbers of neurons, the central idea is to take the density of neurons to the continuum limit, resulting in spatially continuous neural networks. Instead of modelling individual neurons, this approach approximates a group of neurons by its average properties and interactions. It is based on the mean field approach, an area of statistical physics that deals with large-scale systems. Models based on these principles have been used to provide mathematical descriptions of neural oscillations and EEG rhythms. They have for instance been used to investigate visual hallucinations.",
            "score": 112.31832885742188
        },
        {
            "docid": "28289092_2",
            "document": "Dynamical neuroscience . The dynamical systems approach to neuroscience is a branch of mathematical biology that utilizes nonlinear dynamics to understand and model the nervous system and its functions. In a dynamical system, all possible states are expressed by a phase space. Such systems can experience bifurcation (a qualitative change in behavior) as a function of its bifurcation parameters and often exhibit chaos. Dynamical neuroscience describes the non-linear dynamics at many levels of the brain from single neural cells to cognitive processes, sleep states and the behavior of neurons in large-scale neuronal simulation.",
            "score": 101.13713836669922
        },
        {
            "docid": "40158142_9",
            "document": "Nonlinear system identification . Artificial neural networks try loosely to imitate the network of neurons in the brain where computation takes place through a large number of simple processing elements. A typical neural network consists of a number of simple processing units interconnected to form a complex network. Layers of such units are arranged so that data is entered at the input layer and passes through either one or several intermediate layers before reaching the output layer. In supervised learning the network is trained by operating on the difference between the actual output and the desired output of the network, the prediction error, to change the connection strengths between the nodes. By iterating the weights are modified until the output error reaches an acceptable level. This process is called machine learning because the network adjusts the weights so that the output pattern is reproduced. Neural networks have been extensively studied and there are many excellent textbooks devoted to this topic in general, and more focussed textbooks which emphasise control and systems applications. There are two main problem types that can be studied using neural networks: static problems, and dynamic problems. Static problems include pattern recognition, classification, and approximation. Dynamic problems involve lagged variables and are more appropriate for system identification and related applications. Depending on the architecture of the network the training problem can be either nonlinear-in-the-parameters which involves optimisation or linear-in-the-parameters which can be solved using classical approaches. The training algorithms can be categorised into supervised, unsupervised, or reinforcement learning. Neural networks have excellent approximation properties but these are usually based on standard function approximation results using for example the Weierstrass Theorem that applies equally well to polynomials, rational functions, and other well-known models.  Neural networks have been applied extensively to system identification problems which involve nonlinear and dynamic relationships. However, classical neural networks are purely gross static approximating machines. There is no dynamics within the network. Hence when fitting dynamic models all the dynamics arise by allocating lagged inputs and outputs to the input layer of the network. The training procedure then produces the best static approximation that relates the lagged variables assigned to the input nodes to the output. There are more complex network architectures, including recurrent networks, that produce dynamics by introducing increasing orders of lagged variables to the input nodes. But in these cases it is very easy to over specify the lags and this can lead to over fitting and poor generalisation properties.  Neural networks have several advantages; they are conceptually simple, easy to train and to use, have excellent approximation properties, the concept of local and parallel processing is important and this provides integrity and fault tolerant behaviour. The biggest criticism of the classical neural network models is that the models produced are completely opaque and usually cannot be written down or analysed. It is therefore very difficult to know what is causing what, to analyse the model, or to compute dynamic characteristics from the model. Some of these points will not be relevant to all applications but they are for dynamic modelling.",
            "score": 100.75215148925781
        },
        {
            "docid": "33064706_9",
            "document": "Eshel Ben-Jacob . Ben-Jacob's studies in neuroscience are guided by an effort to simplify the complexity searching for principles of information coding, memory and learning. He has many unique contributions in the field of Systems Neuroscience and Neural Networks, including the relations between network size and its synchronized activity, the discovery of hidden neuron correlations, function-form relations and mutual synchronization in engineered networks, the effect of DNA damage on network synchronization, neuro-glia communication, new modeling of intra- and inter-cell calcium dynamics, using nano technology for network engineering, discovery and modeling of the dynamical motives (repertoire) of coupled neural networks, development of a novel system-level analysis of neural network activity (the functional holography analysis), mapping and assessments of epileptic foci, and more. Yet, the development of the first neuro-memory-chip with his doctoral student at the time, Itay Baruchi, is Ben-Jacob's most important contribution in systems neuroscience. While previous attempts were based on \"teaching by reward\" (enhancing excitatory synapses) or \"teaching by punishment\" (inhibition of excitatory synapses), Baruchi and Ben-Jacob's approach was \"teaching by liberation\", or \"inhibition of inhibition\" (inhibition of inhibitory synapses). Being recognized as a groundbreaking discovery in systems neuroscience, the achievement was awarded in 2007 the SciAm 50, The Scientific American Award for the 50 most important achievements in all fields of science and technology.",
            "score": 100.07225036621094
        },
        {
            "docid": "2860430_17",
            "document": "Neural oscillation . If a group of neurons engages in synchronized oscillatory activity, the neural ensemble can be mathematically represented as a single oscillator. Different neural ensembles are coupled through long-range connections and form a network of weakly coupled oscillators at the next spatial scale. Weakly coupled oscillators can generate a range of dynamics including oscillatory activity. Long-range connections between different brain structures, such as the thalamus and the cortex (see thalamocortical oscillation), involve time-delays due to the finite conduction velocity of axons. Because most connections are reciprocal, they form feed-back loops that support oscillatory activity. Oscillations recorded from multiple cortical areas can become synchronized to form large scale brain networks, whose dynamics and functional connectivity can be studied by means of spectral analysis and Granger causality measures. Coherent activity of large-scale brain activity may form dynamic links between brain areas required for the integration of distributed information.",
            "score": 100.02033996582031
        },
        {
            "docid": "3474296_4",
            "document": "Neuronal noise . Single neurons demonstrate different responses to specific neuronal input signals. This is commonly referred to as neural response variability. If a specific input signal is initiated in the dendrites of a neuron, then a hypervariability exists in the number of vesicles released from the axon terminal fiber into the synapse. This characteristic is true for fibers without neural input signals, such as pacemaker neurons, as mentioned previously, and cortical pyramidal neurons that have highly-irregular firing pattern. Noise generally hinders neural performance, but recent studies show, in dynamical non-linear neural networks, this statement does not always hold true. Non-linear neural networks are a network of complex neurons that have many connections with one another such as the neuronal systems found within our brains. Comparatively, linear networks are an experimental view of analyzing a neural system by placing neurons in series with each other.",
            "score": 99.48316192626953
        },
        {
            "docid": "2567511_18",
            "document": "Neural engineering . Neural interfaces are a major element used for studying neural systems and enhancing or replacing neuronal function with engineered devices. Engineers are challenged with developing electrodes that can selectively record from associated electronic circuits to collect information about the nervous system activity and to stimulate specified regions of neural tissue to restore function or sensation of that tissue (Cullen et al. 2011). The materials used for these devices must match the mechanical properties of neural tissue in which they are placed and the damage must be assessed. Neural interfacing involves temporary regeneration of biomaterial scaffolds or chronic electrodes and must manage the body's response to foreign materials. Microelectrode arrays are recent advances that can be used to study neural networks (Cullen & Pfister 2011). Optical neural interfaces involve optical recordings and optogenetics stimulation that makes brain cells light sensitive. Fiber optics can be implanted in the brain to stimulate and record this photon activity instead of electrodes. Two-photon excitation microscopy can study living neuronal networks and the communicatory events among neurons.",
            "score": 95.23085021972656
        },
        {
            "docid": "3717_60",
            "document": "Brain . Computational neuroscience encompasses two approaches: first, the use of computers to study the brain; second, the study of how brains perform computation. On one hand, it is possible to write a computer program to simulate the operation of a group of neurons by making use of systems of equations that describe their electrochemical activity; such simulations are known as \"biologically realistic neural networks\". On the other hand, it is possible to study algorithms for neural computation by simulating, or mathematically analyzing, the operations of simplified \"units\" that have some of the properties of neurons but abstract out much of their biological complexity. The computational functions of the brain are studied both by computer scientists and neuroscientists.",
            "score": 95.12734985351562
        },
        {
            "docid": "762064_2",
            "document": "Systems neuroscience . Systems neuroscience is a subdiscipline of neuroscience and systems biology that studies the function of neural circuits and systems. It is an umbrella term, encompassing a number of areas of study concerned with how nerve cells behave when connected together to form neural pathways, neural circuits, and larger brain networks. At this level of analysis, neuroscientists study how different neural circuits analyze sensory information, form perceptions of the external world, make decisions, and execute movements. Researchers in systems neuroscience are concerned with the relation between molecular and cellular approaches to understanding brain structure and function, as well as with the study of high-level mental functions such as language, memory, and self-awareness (which are the purview of behavioral and cognitive neuroscience). Systems neuroscientists typically employ techniques for understanding networks of neurons as they are seen to function, by way of electrophysiology using either single-unit recording or multi-electrode recording, functional magnetic resonance imaging (fMRI), and PET scans. The term is commonly used in an educational framework: a common sequence of graduate school neuroscience courses consists of cellular/molecular neuroscience for the first semester, then systems neuroscience for the second semester. It is also sometimes used to distinguish a subdivision within a neuroscience department at an academic institution.",
            "score": 94.98981475830078
        },
        {
            "docid": "3717_69",
            "document": "Brain . Theorists have worked to understand these response patterns by constructing mathematical models of neurons and neural networks, which can be simulated using computers. Some useful models are abstract, focusing on the conceptual structure of neural algorithms rather than the details of how they are implemented in the brain; other models attempt to incorporate data about the biophysical properties of real neurons. No model on any level is yet considered to be a fully valid description of brain function, though. The essential difficulty is that sophisticated computation by neural networks requires distributed processing in which hundreds or thousands of neurons work cooperatively\u2014current methods of brain activity recording are only capable of isolating action potentials from a few dozen neurons at a time.",
            "score": 94.92245483398438
        },
        {
            "docid": "43374303_4",
            "document": "Andreas K. Engel . Andreas Engel has become known by his work on the so-called \u201ebinding problem\u201c. His research focuses on the hypothesis that temporal synchrony serves for dynamic coordination of signals in the brain. In addition to working on the experimental validation of this hypothesis, Engel pursues research on its cognitive and theoretical implications. As a postdoc with Wolf Singer at the Max Planck Institute for Brain Research at Frankfurt, Engel was involved in studies that demonstrated the relevance of neural synchrony, in particular of so-called gamma waves, for processing of perceptual information. In particular, the group provided evidence that temporal correlations can serve for the binding of features into coherent sensory representations. In addition to addressing the relevance of synchrony and neuronal oscillations in the visual system, the work of Engel's group yielded evidence for a relation between neural synchrony and visual awareness. In addition, Engel and coworkers contributed to demonstrating a functional role of neural synchrony for sensorimotor coupling. In the past 15 years, Engel's group has expanded their work to the human brain, using EEG and MEG in combination with source modeling techniques. The results of these studies demonstrate the importance of neuronal oscillations and synchrony for perceptual processing, attention, working memory, decision-making and consciousness. Recent work of the group on the interaction of visual, auditory and tactile systems suggests a role of temporal binding for multisensory integration. Furthermore, the group has developed novel methods for the electrophysiological analysis of resting state network activity. Engel's group also applies these approaches for the study of network malfunction in patients with movement disorders, multiple sclerosis and schizophrenia, in studies on pain, and altered networks after early sensory deprivation.  Engel also explores implications of these neurophysiogical results for theories of perception, cognition and action. A major focus of his work are the implications of the studies on neural synchrony for understanding the neural correlates of consciousness. Recent papers address links between neural dynamics and enactive views of cognition, investigating the grounding of cognition in sensorimotor coupling.",
            "score": 94.67862701416016
        },
        {
            "docid": "36237055_6",
            "document": "Post-contemporary . The origin in diversely phenomenological systems and exact models from life sciences is in the area of the theory of applied dynamical systems and global bifurcations. The special interest in the subject can be find in a new emergent cross-disciplinary field known as mathematical neuroscience. Its scopes include nonlinear models of individual neurons and networks. In-depth analysis of such systems requires development of advanced mathematical tools paired with sophisticated computations. For instance, Andrey Shilnikov, a neuroscientist and mathematician derives models and create bifurcation toolkits for studying a stunning array of complex activities such as multistability of individual neurons and polyrhythmic bursting patterns discovered in multifunctional central pattern generators governing vital locomotor behaviors of animals and humans. Thanks to the non linear qualitative dynamics, the organization of holistic nature of post-contemporary lineage was implemented by interoprativity and interactivity within the heterogeneous components and characters of Micro - systems, being present in many concrete social contents and in most territorial recourses. In this way post-structuralism as well as the new sciences of complexity, Complexity theory and chaos theory, were appropriated and interpreted within Micro - systems by means of \"self-creation\", expressing a fundamental dialectic among structure, mechanism and function, identifiable and recognizable in concrete territorial contexts.",
            "score": 93.9565200805664
        },
        {
            "docid": "41121206_5",
            "document": "Phase resetting in neurons . Shifts in phase (or behavior of neurons) caused due to a perturbation (an external stimulus) can be quantified within a Phase Response Curve (PRC) to predict synchrony in coupled and oscillating neurons. These effects can be computed, in the case of advances or delays to responses, to observe the changes in the oscillatory behavior of neurons, pending on when a stimulus was applied in the phase cycle of an oscillating neuron. The key to understanding this is in the behavioral patterns of neurons and the routes neural information travels. Neural circuits are able to communicate efficiently and effectively within milliseconds of experiencing a stimulus and lead to the spread of information throughout the neural network. The study of neuron synchrony could provide information on the differences that occur in neural states such as normal and diseased states. Neurons that are involved significantly in diseases such as Alzheimers or Parkinsons diseases are shown to undergo phase resetting before launching into phase locking where clusters of neurons are able to begin firing rapidly to communicate information quickly. A phase response curve can be calculated by noting changes to its period over time depending on where in the cycle the input is applied. The perturbation left by the stimulus moves the stable cycle within the oscillation followed by a return to the stable cycle limit. The curve tracks the amount of advancement or delay due to the input in the oscillating neuron. The PRC assumes certain patterns of behavior in firing pattern as well as the network of oscillating neurons to model the oscillations. Currently, only a few circuits exist which can be modeled using an assumed firing pattern.",
            "score": 93.59439086914062
        },
        {
            "docid": "33818014_15",
            "document": "Nervous system network models . The concept of artificial neural network (ANN) was introduced by McColloch, W. S. & Pitts, W. (1943) for models based on behavior of biological neurons. Norbert Wiener (1961) gave this new field the popular name of cybernetics, whose principle is the interdisciplinary relationship among engineering, biology, control systems, brain functions, and computer science. With the computer science field advancing, the von Neumann-type computer was introduced early in the neuroscience study. But it was not suitable for symbolic processing, nondeterministic computations, dynamic executions, parallel distributed processing, and management of extensive knowledge bases, which are needed for biological neural network applications; and the direction of mind-like machine development changed to a learning machine. Computing technology has since advanced extensively and computational neuroscience is now able to handle mathematical models developed for biological neural network. Research and development are progressing in both artificial and biological neural networks including efforts to merge the two.",
            "score": 93.55333709716797
        },
        {
            "docid": "505717_72",
            "document": "Image segmentation . Pulse-coupled neural networks (PCNNs) are neural models proposed by modeling a cat\u2019s visual cortex and developed for high-performance biomimetic image processing. In 1989, Eckhorn introduced a neural model to emulate the mechanism of a cat\u2019s visual cortex. The Eckhorn model provided a simple and effective tool for studying the visual cortex of small mammals, and was soon recognized as having significant application potential in image processing. In 1994, the Eckhorn model was adapted to be an image processing algorithm by Johnson, who termed this algorithm Pulse-Coupled Neural Network. Over the past decade, PCNNs have been utilized for a variety of image processing applications, including: image segmentation, feature generation, face extraction, motion detection, region growing, noise reduction, and so on. A PCNN is a two-dimensional neural network. Each neuron in the network corresponds to one pixel in an input image, receiving its corresponding pixel\u2019s color information (e.g. intensity) as an external stimulus. Each neuron also connects with its neighboring neurons, receiving local stimuli from them. The external and local stimuli are combined in an internal activation system, which accumulates the stimuli until it exceeds a dynamic threshold, resulting in a pulse output. Through iterative computation, PCNN neurons produce temporal series of pulse outputs. The temporal series of pulse outputs contain information of input images and can be utilized for various image processing applications, such as image segmentation and feature generation. Compared with conventional image processing means, PCNNs have several significant merits, including robustness against noise, independence of geometric variations in input patterns, capability of bridging minor intensity variations in input patterns, etc.",
            "score": 93.52629852294922
        },
        {
            "docid": "2860430_8",
            "document": "Neural oscillation . Neural oscillations are commonly studied from a mathematical framework and belong to the field of \"neurodynamics\", an area of research in the cognitive sciences that places a strong focus upon the dynamic character of neural activity in describing brain function. It considers the brain a dynamical system and uses differential equations to describe how neural activity evolves over time. In particular, it aims to relate dynamic patterns of brain activity to cognitive functions such as perception and memory. In very abstract form, neural oscillations can be analyzed analytically. When studied in a more physiologically realistic setting, oscillatory activity is generally studied using computer simulations of a computational model.",
            "score": 91.7156753540039
        },
        {
            "docid": "2506529_35",
            "document": "Cellular neural network . CNN processors are being used to understand systems that can be modeled using simple, coupled units, such as living cells, biological networks, physiological systems, and ecosystems. The CNN architecture captures some of the dynamics often seen in nature and is simple enough to analyze and conduct experiments. They are also being used for stochastic simulation techniques, which allow scientists to explore spin problems, population dynamics, lattice-based gas models, percolation, and other phenomena. Other simulation applications include heat transfer, mechanical vibrating systems, protein production, Josephson Transmission Line (JTL) problems, seismic wave propagation, and geothermal structures. Instances of 3D (Three Dimensional) CNN have been used to prove known complex shapes are emergent phenomena in complex systems, establishing a link between art, dynamical systems and VLSI technology. CNN processors have been used to research a variety of mathematical concepts, such as researching non-equilibrium systems, constructing non-linear systems of arbitrary complexity using a collection of simple, well-understood dynamic systems, studying emergent chaotic dynamics, generating chaotic signals, and in general discovering new dynamic behavior. They are often used in researching systemics, a trandisiplinary, scientific field that studies natural systems. The goal of systemics researchers is to develop a conceptual and mathematical framework necessary to analyze, model, and understand systems, including, but not limited to, atomic, mechanical, molecular, chemical, biological, ecological, social and economic systems. Topics explored are emergence, collective behavior, local activity and its impact on global behavior, and quantifying the complexity of an approximately spatial and topologically invariant system . Although another measure of complexity may not arouse enthusiasm (Seth Lloyd, a professor from Massachusetts Institute of Technology (MIT), has identified 32 different definitions of complexity), it can potentially be mathematically advantageous when analyzing systems such as economic and social systems.",
            "score": 91.56591033935547
        },
        {
            "docid": "10159567_6",
            "document": "Spiking neural network . This kind of neural network can in principle be used for information processing applications the same way as traditional artificial neural networks. In addition, spiking neural networks can model the central nervous system of a virtual insect for seeking food without the prior knowledge of the environment. However, due to their more realistic properties, they can also be used to study the operation of biological neural circuits. Starting with a hypothesis about the topology of a biological neuronal circuit and its function, the electrophysiological recordings of this circuit can be compared to the output of the corresponding spiking artificial neural network simulated on computer, determining the plausibility of the starting hypothesis.",
            "score": 91.15376281738281
        },
        {
            "docid": "28289092_24",
            "document": "Dynamical neuroscience . The computational approaches to theoretical neuroscience often employ artificial neural networks that simplify the dynamics of single neurons in favor of examining more global dynamics. While neural networks are often associated with artificial intelligence, they have also been productive in the cognitive sciences. Artificial neural networks use simple neuron models, but their global dynamics are capable of exhibiting both Hopfield and Attractor-like network dynamics.",
            "score": 90.7370376586914
        },
        {
            "docid": "33818014_2",
            "document": "Nervous system network models . Network of human nervous system comprises nodes (for example, neurons) that are connected by links (for example, synapses). The connectivity may be viewed anatomically, functionally, or electrophysiologically. These are presented in several Wikipedia articles that include Connectionism (a.k.a. Parallel Distributed Processing (PDP)), Biological neural network, Artificial neural network (a.k.a. Neural network), Computational neuroscience, as well as in several books by Ascoli, G. A. (2002), Sterratt, D., Graham, B., Gillies, A., & Willshaw, D. (2011), Gerstner, W., & Kistler, W. (2002), and Rumelhart, J. L., McClelland, J. L., and PDP Research Group (1986) among others. The focus of this article is a comprehensive view of modeling a neural network (technically neuronal network based on neuron model). Once an approach based on the perspective and connectivity is chosen, the models are developed at microscopic (ion and neuron), mesoscopic (functional or population), or macroscopic (system) levels. Computational modeling refers to models that are developed using computing tools.",
            "score": 90.38297271728516
        },
        {
            "docid": "21855574_5",
            "document": "Brain simulation . The connectivity of the neural circuit for touch sensitivity of the simple C. elegans nematode (roundworm) was mapped in 1985 and partly simulated in 1993. Since 2004, many software simulations of the complete neural and muscular system have been developed, including simulation of the worm's physical environment. Some of these models have been made available for download. However, there is still a lack of understanding of how the neurons and the connections between them generate the surprisingly complex range of behaviors that are observed in the relatively simple organism. This contrast between the apparent simplicity of how the mapped neurons interact with their neighbours, and exceeding complexity of the overall brain function, is an example of an emergent property. Interestingly, this kind of emergent property is paralleled within artificial neural networks, the neurons of which are exceedingly simple compared to their often complex, abstract outputs.",
            "score": 90.12760925292969
        },
        {
            "docid": "57546_25",
            "document": "Caenorhabditis elegans . In 1963, Sydney Brenner proposed using \"C. elegans\" as a model organism for the investigation primarily of neural development in animals. It is one of the simplest organisms with a nervous system. The neurons do not fire action potentials, and do not express any voltage-gated ion channels. In the hermaphrodite, this system comprises 302 neurons the pattern of which has been comprehensively mapped, in what is known as a connectome, and shown to be a small-world network. Research has explored the neural and molecular mechanisms that control several behaviors of \"C. elegans\", including chemotaxis, thermotaxis, mechanotransduction, learning, memory, and mating behaviour. Brenner also chose it as it is easy to grow in bulk populations, and convenient for genetic analysis. It is a multicellular eukaryotic organism, yet is simple enough to be studied in great detail. The transparency of \"C. elegans\" facilitates the study of cellular differentiation and other developmental processes in the intact organism. The spicules in the male clearly distinguish males from females. Strains are cheap to breed and can be frozen. When subsequently thawed, they remain viable, allowing long-term storage. Maintenance is easy when compared to other multicellular model organisms, a few hundred nematodes can be kept on a single agar plate and suitable growth medium. Brenner described the use of a mutant of \"E. Coli\" \u2013 OP50. OP50 is a uracil-requiring organism and its deficiency in the plate prevents the overgrowth of bacteria which would obscure the worms.",
            "score": 90.12100219726562
        },
        {
            "docid": "3062721_16",
            "document": "Neuroinformatics . Biology is concerned with molecular data (from genes to cell specific expression); medicine and anatomy with the structure of synapses and systems level anatomy; engineering \u2013 electrophysiology (from single channels to scalp surface EEG), brain imaging; computer science \u2013 databases, software tools, mathematical sciences \u2013 models, chemistry \u2013 neurotransmitters, etc. Neuroscience uses all aforementioned experimental and theoretical studies to learn about the brain through its various levels. Medical and biological specialists help to identify the unique cell types, and their elements and anatomical connections. Functions of complex organic molecules and structures, including a myriad of biochemical, molecular, and genetic mechanisms which regulate and control brain function, are determined by specialists in chemistry and cell biology. Brain imaging determines structural and functional information during mental and behavioral activity. Specialists in biophysics and physiology study physical processes within neural cells neuronal networks. The data from these fields of research is analyzed and arranged in databases and neural models in order to integrate various elements into a sophisticated system; this is the point where neuroinformatics meets other disciplines.",
            "score": 89.97237396240234
        },
        {
            "docid": "20512936_21",
            "document": "Dendritic spike . Computational modeling of neurons, artificial neural networking, has become a very popular tool in investigating the properties of neuronal signaling. These models are based on biological neural networks. Computational modeling can be used to study single neurons, groups of neurons, or even networks of neurons. This field has generated much interest and serves as a tool for all branches of neuroscience research including dendritic spike initiation.",
            "score": 89.57001495361328
        },
        {
            "docid": "39199253_5",
            "document": "Percolation (cognitive psychology) . Percolation has been developed outside of the cognitive sciences; however, its application in the field has proven it to be a useful tool for understanding neural processes. Researchers have focused their attention not only studying how neural activity is diffused across networks but also how percolation and its aspect of phase transition can affect decision making and thought processes. Percolation theory has enabled researchers to better understand many psychological conditions, such as epilepsy, disorganized schizophrenia and divergent thinking. These conditions are often indicative of percolating clusters and their involvement in propagating the excess firing of neurons. Seizures occur when neurons in the brain fire simultaneously, and often these seizures can occur in one part of the brain and transfer to other parts. Researchers are able to facilitate a better understanding of these conditions because \"the neurons involved in a seizure are analogous to the sites in a percolating cluster\". Disorganized schizophrenia is more complex as the activity is indicative activity in a percolating cluster; however, some researchers have suggested that the percolation of information occurs not in a small cluster but on a global functional scale. Attention as well as percolation also plays a key role in disorganized and divergent thinking; however, it is more likely that directed percolation, that is a directionally controlled percolation, is more useful to study divergent thinking and creativity.",
            "score": 89.43828582763672
        },
        {
            "docid": "739262_10",
            "document": "Neural correlate . Neurophysiological studies in animals provided some insights on the neural correlates of conscious behavior. Vernon Mountcastle, in the early 1960s, set up to study this set of problems, which he termed \"the Mind/Brain problem\", by studying the neural basis of perception in the somatic sensory system. His labs at Johns Hopkins were among the first, along with Edward V.Evarts at NIH, to record neural activity from behaving monkeys. Struck with the elegance of SS Stevens approach of magnitude estimation, Mountcastle's group discovered three different modalities of somatic sensation shared one cognitive attribute: in all cases the firing rate of peripheral neurons was linearly related to the strength of the percept elicited. More recently, Ken H. Britten, William T. Newsome, and C. Daniel Salzman have shown that in area MT of monkeys, neurons respond with variability that suggests they are the basis of decision making about direction of motion. They first showed that neuronal rates are predictive of decisions using signal detection theory, and then that stimulation of these neurons could predictably bias the decision. Such studies were followed by Ranulfo Romo in the somatic sensory system, to confirm, using a different percept and brain area, that a small number of neurons in one brain area underlie perceptual decisions.",
            "score": 89.27787017822266
        },
        {
            "docid": "1648765_11",
            "document": "Random matrix . In the field of theoretical neuroscience, random matrices are increasingly used to model the network of synaptic connections between neurons in the brain. Dynamical models of neuronal networks with random connectivity matrix were shown to exhibit a phase transition to chaos when the variance of the synaptic weights crosses a critical value, at the limit of infinite system size. Relating the statistical properties of the spectrum of biologically inspired random matrix models to the dynamical behavior of randomly connected neural networks is an intensive research topic.",
            "score": 88.89617156982422
        },
        {
            "docid": "33244792_4",
            "document": "Non-spiking neuron . There are an abundance of neurons that propagate signals via action potentials and the mechanics of this particular kind of transmission is well understood. Spiking neurons exhibit action potentials as a result of a neuron characteristic known as membrane potential. Through studying these complex spiking networks in animals, a neuron that did not exhibit characteristic spiking behavior was discovered. These neurons use a graded potential to transmit data as they lack the membrane potential that spiking neurons possess. This method of transmission has a huge effect on the fidelity, strength, and lifetime of the signal. Non-spiking neurons were identified as a special kind of interneuron and function as an intermediary point of process for sensory-motor systems. Animals have become substantial models for understanding more about non-spiking neural networks and the role they play in an animal\u2019s ability to process information and its overall function. Animal models indicate that the interneurons modulate directional and posture coordinating behaviors. Crustaceans and arthropods such as the crawfish have created many opportunities to learn about the modulatory role that these neurons have in addition to their potential to be modulated regardless of their lack of exhibiting spiking behavior. Most of the known information about nonspiking neurons is derived from animal models. Studies focus on neuromuscular junctions and modulation of abdominal motor cells. Modulatory interneurons are neurons that are physically situated next to muscle fibers and innervate the nerve fibers which allow for some orienting movement. These modulatory interneurons are usually nonspiking neurons. Advances in studying nonspiking neurons included determining new delineations among the different types of interneurons. These discoveries were due to the usage of methods such as protein receptor silencing. Studies have been done on the non-spiking neuron qualities in animals of specific non-spiking neural networks that have a corollary in humans, e.g. retina amacrine cell of the eye.",
            "score": 88.6412582397461
        },
        {
            "docid": "33818014_8",
            "document": "Nervous system network models . On a high level representation, the neurons can be viewed as connected to other neurons to form a neural network in one of three ways. A specific network can be represented as a physiologically (or anatomically) connected network and modeled that way. There are several approaches to this (see Ascoli, G.A. (2002) Sporns, O. (2007), Connectionism, Rumelhart, J. L., McClelland, J. L., and PDP Research Group (1986), Arbib, M. A. (2007)). Or, it can form a functional network that serves a certain function and modeled accordingly (Honey, C. J., Kotter, R., Breakspear, R., & Sporns, O. (2007), Arbib, M. A. (2007)). A third way is to hypothesize a theory of the functioning of the biological components of the neural system by a mathematical model, in the form of a set of mathematical equations. The variables of the equation are some or all of the neurobiological properties of the entity being modeled, such as the dimensions of the dendrite or the stimulation rate of action potential along the axon in a neuron. The mathematical equations are solved using computational techniques and the results are validated with either simulation or experimental processes. This approach to modeling is called computational neuroscience. This methodology is used to model components from the ionic level to system level of the brain. This method is applicable for modeling integrated system of biological components that carry information signal from one neuron to another via intermediate active neurons that can pass the signal through or create new or additional signals. The computational neuroscience approach is extensively used and is based on two generic models, one of cell membrane potential Goldman (1943) and Hodgkin and Katz (1949), and the other based on Hodgkin-Huxley model of action potential (information signal).",
            "score": 88.20606231689453
        },
        {
            "docid": "2860430_23",
            "document": "Neural oscillation . A neural network model describes a population of physically interconnected neurons or a group of disparate neurons whose inputs or signalling targets define a recognizable circuit. These models aim to describe how the dynamics of neural circuitry arise from interactions between individual neurons. Local interactions between neurons can result in the synchronization of spiking activity and form the basis of oscillatory activity. In particular, models of interacting pyramidal cells and inhibitory interneurons have been shown to generate brain rhythms such as gamma activity.",
            "score": 88.15337371826172
        },
        {
            "docid": "33826069_2",
            "document": "Viral neuronal tracing . Viral neuronal tracing is the use of a virus to trace neural pathways, providing a self-replicating tracer. Viruses have the advantage of self replication over molecular tracers, but can also spread too quickly and cause degradation of neural tissue. Viruses which can infect the nervous system, called Neurotropic viruses, spread through spatially close assemblies of neurons through synapses, allowing for their use in studying functionally connected neural networks. The use of viruses to label functionally connected neurons stems from work done by Albert Sabin who developed a bioassay which could assess the infection of viruses across neurons. Subsequent research allowed for incorporation of Immunohistochemical techniques to systematically label neuronal connections. To date, viruses have been used to study multiple circuits in the nervous system.",
            "score": 88.06368255615234
        }
    ]
}