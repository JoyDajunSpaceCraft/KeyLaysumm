{
    "q": [
        {
            "docid": "33877779_4",
            "document": "Financial correlation . There are several statistical measures of the degree of financial correlations. The Pearson product-moment correlation coefficient is sometimes applied to finance correlations. However, the limitations of Pearson correlation approach in finance are evident. First, linear dependencies as assessed by the Pearson correlation coefficient do not appear often in finance. Second, linear correlation measures are only natural dependence measures if the joint distribution of the variables is elliptical. However, only few financial distributions such as the multivariate normal distribution and the multivariate student-t distribution are special cases of elliptical distributions, for which the linear correlation measure can be meaningfully interpreted. Third, a zero Pearson product-moment correlation coefficient does not necessarily mean independence, because only the two first moments are considered. For example, formula_1 (\"y\"\u00a0\u2260\u00a00) will lead to Pearson correlation coefficient of zero, which is arguably misleading. Since the Pearson approach is unsatisfactory to model financial correlations, quantitative analysts have developed specific financial correlation measures. Accurately estimating correlations requires the modeling process of marginals to incorporate characteristics such as skewness and kurtosis. Not accounting for these attributes can lead to severe estimation error in the correlations and covariances that have negative biases (as much as 70% of the true values). In a practical application in portfolio optimization, accurate estimation of the variance-covariance matrix is paramount. Thus, forecasting with Monte-Carlo simulation with the Gaussian copula and well-specified marginal distributions are effective.",
            "score": 22.637608528137207
        },
        {
            "docid": "43769798_11",
            "document": "Gene co-expression network . Pearson\u2019s correlation coefficient is the most popular co-expression measure used in constructing gene co-expression networks. The Pearson's correlation coefficient takes a value between -1 and 1 where absolute values close to 1 show strong correlation. The positive values correspond to an activation mechanism where the expression of one gene increases with the increase in the expression of its co-expressed gene and vice versa. When the expression value of one gene decreases with the increase in the expression of its co-expressed gene, it corresponds to an underlying suppression mechanism and would have a negative correlation.",
            "score": 28.297029733657837
        },
        {
            "docid": "41224221_2",
            "document": "Weighted correlation network analysis . Weighted correlation network analysis, also known as weighted gene co-expression network analysis (WGCNA), is a widely used data mining method especially for studying biological networks based on pairwise correlations between variables. While it can be applied to most high-dimensional data sets, it has been most widely used in genomic applications. It allows one to define modules (clusters), intramodular hubs, and network nodes with regard to module membership, to study the relationships between co-expression modules, and to compare the network topology of different networks (differential network analysis). WGCNA can be used as a data reduction technique (related to oblique factor analysis ), as a clustering method (fuzzy clustering), as a feature selection method (e.g. as gene screening method), as a framework for integrating complementary (genomic) data (based on weighted correlations between quantitative variables), and as a data exploratory technique. Although WGCNA incorporates traditional data exploratory techniques, its intuitive network language and analysis framework transcend any standard analysis technique. Since it uses network methodology and is well suited for integrating complementary genomic data sets, it can be interpreted as systems biologic or systems genetic data analysis method. By selecting intramodular hubs in consensus modules, WGCNA also gives rise to network based meta analysis techniques.",
            "score": 60.57798385620117
        },
        {
            "docid": "41224221_7",
            "document": "Weighted correlation network analysis . where gene expression profiles formula_3 and formula_4 consist of the expression of genes i and j across multiple samples. However, using the absolute value of the correlation may obfuscate biologically relevant information, since no distinction is made between gene repression and activation. In contrast, in signed networks the similarity between genes reflects the sign of the correlation of their expression profiles. To define a signed co-expression measure between gene expression profiles formula_3 and formula_4 , one can use a simple transformation of the correlation:",
            "score": 34.12017226219177
        },
        {
            "docid": "45663583_3",
            "document": "Human interactome . With the sequencing of the genomes of a diverse array or model organisms, it became clear that the number of genes does not correlate with the human perception of relative organism complexity \u2013 the human proteome contains some 20 000 genes, which is smaller than some species such as corn. A statistical approach to calculating the number of interactions in humans gives an estimate of around 650 000, one order of magnitude bigger than Drosophila and 3 times larger than C. Elegans. As of 2008, only about <0.3% of all estimated interactions among human proteins has been identified, although in recent years there has been exponential growth in discovery \u2013 as of 2015, over 210 000 unique human positive protein\u2013protein interactions are currently catalogued, and bioGRID database contains almost 750 000 literature-curated PPI's for 30 model organisms, 300 000 of which are verified or predicted human physical or genetic protein\u2013protein interactions, a 50% increase from 2013. The currently available information on the human interactome network originates from either literature-curated interactions, high-throughput experiments, or from potential interactions predicted from interactome data, whether through phylogenetic profiling (evolutionary similarity), statistical network inference, or text/literature mining methods.",
            "score": 50.23087978363037
        },
        {
            "docid": "1037062_8",
            "document": "Rhombomere . The Hox gene also has been shown to play a part in the formation of the cranial motor nerves. The fate of a rhombomere has been shown to be affected by differential expression of the Hox gene. With mutations in the Hox gene, the cranial motor nerves formed in different locations than normal or simply did not form altogether. One possibility for this was that the Hox gene was somehow involved in regionalization within the neural tube, and that expression of this particular gene correlated with the amount of migration that occurred. However, no correlation could be found. Many studies showed small amounts of correlation, but there were equal amounts showing a complete lack of correlation. The amount of correlation that occurred was not enough to draw a concrete conclusion from. This, however, could have happened because studies have only drawn data points from a limited window of time. Another possibility for this lack of correlation is that most studies were based on in situ hybridization, which only maps the location of transcripts rather than proteins. A third possibility is that studies focused on rhombomeres as landmarks, and therefore correlated expression domains to these. While studies were unable to show a relationship between closely related neurons from the rhombomeres and Hox gene expression, the Hox gene is still an important factor when it comes to specification of the neuronal phenotype. The Hox gene was expressed rostrocaudally in the same sequence that was physically within the chromosome and its transcription was regulated by retinoic acid. The Hox gene has been identified in all vertebrates, and the number of Hox genes expressed increases as the vertebrate species diverge farther away from the invertebrate species. Certain neuron groups relate to Hox gene expression. At the r4 level, Hoxb1 is suspected to bestow rhombomere 4 cell identity.",
            "score": 37.36430811882019
        },
        {
            "docid": "21731590_16",
            "document": "RNA-Seq . Coexpression networks are data-derived representations of genes behaving in a similar way across tissues and experimental conditions. Their main purpose lies in hypothesis generation and guilt-by-association approaches for inferring functions of previously unknown genes. RNASeq data has been recently used to infer genes involved in specific pathways based on Pearson correlation, both in plants and mammals. The main advantage of RNASeq data in this kind of analysis over the microarray platforms is the capability to cover the entire transcriptome, therefore allowing the possibility to unravel more complete representations of the gene regulatory networks. Differential regulation of the splice isoforms of the same gene can be detected and used to predict and their biological functions.  Weighted gene co-expression network analysis has been successfully used to identify co-expression modules and intramodular hub genes based on RNA seq data. Co-expression modules may corresponds to cell types or pathways. Highly connected intramodular hubs can be interpreted as representatives of their respective module. Variance-Stabilizing Transformation approaches for estimating correlation coefficients based on RNA seq data have been proposed.",
            "score": 47.221867084503174
        },
        {
            "docid": "41224221_16",
            "document": "Weighted correlation network analysis . It is often used as data reduction step in systems genetic applications where modules are represented by \"module eigengenes\" e.g. Module eigengenes can be used to correlate modules with clinical traits. Eigengene networks are coexpression networks between module eigengenes (i.e. networks whose nodes are modules) . WGCNA is widely used in neuroscientific applications, e.g. and for analyzing genomic data including microarray data, single cell RNA-Seq data DNA methylation data, miRNA data, peptide counts and microbiota data (16S rRNA gene sequencing). Other applications include brain imaging data, e.g. functional MRI data",
            "score": 55.856180906295776
        },
        {
            "docid": "43769798_12",
            "document": "Gene co-expression network . There are two disadvantages for Pearson correlation measure: it can only detect linear relationships and it is sensitive to outliers. Moreover, Pearson correlation assumes that the gene expression data follow a normal distribution. Song et al. have suggested \"biweight midcorrelation (bicor)\" as a good alternative for Pearson\u2019s correlation. \"Bicor is a median based correlation measure, and is more robust than the Pearson correlation but often more powerful than the Spearman's correlation\". Furthermore, it has been shown that \"most gene pairs satisfy linear or monotonic relationships\" which indicates that \"mutual information networks can safely be replaced by correlation networks when it comes to measuring co-expression relationships in stationary data\".",
            "score": 37.50311231613159
        },
        {
            "docid": "2355052_4",
            "document": "Genome size . The genome sizes of thousands of eukaryotes have been analyzed over the past 50 years, and these data are available in online databases for animals, plants, and fungi (see external links). Nuclear genome size is typically measured in eukaryotes using either densitometric measurements of Feulgen-stained nuclei (previously using specialized densitometers, now more commonly using computerized image analysis) or flow cytometry. In prokaryotes, pulsed field gel electrophoresis and complete genome sequencing are the predominant methods of genome size determination. Nuclear genome sizes are well known to vary enormously among eukaryotic species. In animals they range more than 3,300-fold, and in land plants they differ by a factor of about 1,000. Protist genomes have been reported to vary more than 300,000-fold in size, but the high end of this range (\"Amoeba\") has been called into question. In eukaryotes, but not prokaryotes, variation in genome size is not proportional to the number of genes, an observation that was deemed wholly counterintuitive before the discovery of non-coding DNA and which became known as the C-value paradox as a result. However, although there is no longer any paradoxical aspect to the discrepancy between genome size and gene number, this term remains in common usage. For reasons of conceptual clarification, the various puzzles that remain with regard to genome size variation instead have been suggested by one author to more accurately comprise a puzzle or an enigma (the C-value enigma). Genome size correlates with a range of features at the cell and organism levels, including cell size, cell division rate, and, depending on the taxon, body size, metabolic rate, developmental rate, organ complexity, geographical distribution, or extinction risk (for recent reviews, see Bennett and Leitch 2005; Gregory 2005). Based on completely sequenced genome data currently (as of April 2009) available, log-transformed gene number forms a linear correlation with log-transformed genome size in bacteria, archea, viruses, and organelles combined whereas a nonlinear (semi-natural log) correlation in eukaryotes (Hou and Lin 2009 ). The nonlinear correlation for eukaryotes, although claim of its existence contrasts the previous view that no correlation exists for this group of organisms, reflects disproportionately fast increasing noncoding DNA in increasingly large eukaryotic genomes. Although sequenced genome data are practically biased toward small genomes, which may compromise the accuracy of the empirically derived correlation, and the ultimate proof of the correlation remains to be obtained by sequencing some of the largest eukaryotic genomes, current data do not seem to rule out a correlation.",
            "score": 50.529757261276245
        },
        {
            "docid": "35256679_8",
            "document": "Community fingerprinting . The theoretical basis of T-RFLP assumes that peaks at different positions along the horizontal axis represent different types of organisms (or OTUs). The area under each fluorescence intensity peak is a proxy for relative abundance of each phylotype in the community. However, a number of caveats must be taken into account. Different types of organisms may share a restriction site in the gene of interest; if that is the case, these organisms would not be distinguished as different peaks on the electropherogram. Furthermore, area under a peak represents relative abundance rather than absolute abundance, and there are biases in abundance measurement and PCR amplification. For example, organisms that are scarce in the original total DNA sample will not be amplified enough to be detected in the final analysis. This leads to underestimation of community diversity. Liu \"et al.\" cite other possible factors that may distort results, including \"differences in gene copy number between species and biases introduced during cell lysis, DNA extraction, and PCR amplification\" (p.\u00a04521). For those who seek detailed technical information, Marsh provides a catalog of potential biases that could be introduced in each step of the T-RFLP process.",
            "score": 15.646761894226074
        },
        {
            "docid": "52981048_2",
            "document": "Species pool . The ecological and biogeographical concept of the species pool describes all species available that could potentially colonize and inhabit a focal habitat area. The concept lays emphasis on the fact that \"local communities aren't closed systems, and that the species occupying any local site typically came from somewhere else\", however, the species pool concept may suffer from the logical fallacy of composition. Most local communities, however, have just a fraction of its species pool present. It is derived from MacArthur and Wilson's Island Biogeography Theory that examines the factors that affect the species richness of isolated natural communities. It helps to understand the composition and richness of local communities and how they are influenced by biogeographic and evolutionary processes acting at large spatial and temporal scales. The absent portion of species pool\u2014dark diversity\u2014has been used to understand processes influencing local communities. Methods to estimate potential but absent species are developing.",
            "score": 10.708959817886353
        },
        {
            "docid": "542598_10",
            "document": "Protein complex . Although some early studies suggested a strong correlation between essentiality and protein interaction degree (the \u201ccentrality-lethality\u201d rule) subsequent analyses have shown that this correlation is weak for binary or transient interactions (e.g., yeast two-hybrid). However, the correlation is robust for networks of stable co-complex interactions. In fact, a disproportionate number of essential genes belong to protein complexes. This led to the conclusion that essentiality is a property of molecular machines (i.e. complexes) rather than individual components. Wang et al. (2009) noted that larger protein complexes are more likely to be essential, explaining why essential genes are more likely to have high co-complex interaction degree. Ryan et al. (2013) referred to the observation that entire complexes appear essential as \"modular essentiality\". These authors also showed that complexes tend to be composed of either essential or non-essential proteins rather than showing a random distribution (see Figure). However, this not an all or nothing phenomenon: only about 26% (105/401) of yeast complexes consist of solely essential or solely nonessential subunits.",
            "score": 26.124595642089844
        },
        {
            "docid": "35256679_2",
            "document": "Community fingerprinting . Community fingerprinting is a set of molecular biology techniques that can be used to quickly profile the diversity of a microbial community. Rather than directly identifying or counting individual cells in an environmental sample, these techniques show how many variants of a gene are present. In general, it is assumed that each different gene variant represents a different type of microbe. Community fingerprinting is used by microbiologists studying a variety of microbial systems (e.g. marine, freshwater, soil, and human microbial communities) to measure biodiversity or track changes in community structure over time. The method analyzes environmental samples by assaying genomic DNA. This approach offers an alternative to microbial culturing, which is important because most microbes cannot be cultured in the laboratory. Community fingerprinting does not result in identification of individual microbe species; instead, it presents an overall picture of a microbial community. These methods are now largely being replaced by high throughput sequencing, such as targeted microbiome analysis (e.g., 16s rRNA sequencing) and metagenomics.",
            "score": 49.80738377571106
        },
        {
            "docid": "41224221_14",
            "document": "Weighted correlation network analysis . A major step in the module centric analysis is to cluster genes into network modules using a network proximity measure. Roughly speaking, a pair of genes has a high proximity if it is closely interconnected. By convention, the maximal proximity between two genes is 1 and the minimum proximity is 0. Typically, WGCNA uses the topological overlap measure (TOM) as proximity. which can also be defined for weighted networks. The TOM combines the adjacency of two genes and the connection strengths these two genes share with other \"third party\" genes. The TOM is a highly robust measure of network interconnectedness (proximity). This proximity is used as input of average linkage hierarchical clustering. Modules are defined as branches of the resulting cluster tree using the dynamic branch cutting approach  Next the genes inside a given module are summarize with the module eigengene, which can be considered as the best summary of the standardized module expression data. The module eigengene of a given module is defined as the first principal component of the standardized expression profiles. Eigengenes define robust biomarkers. To find modules that relate to a clinical trait of interest, module eigengenes are correlated with the clinical trait of interest, which gives rise to an eigengene significance measure. Eigengenes can be used as features in more complex predictive models including decision trees and Bayesian networks. One can also construct co-expression networks between module eigengenes (eigengene networks), i.e. networks whose nodes are modules. To identify intramodular hub genes inside a given module, one can use two types of connectivity measures. The first, referred to as formula_24, is defined based on correlating each gene with the respective module eigengene. The second, referred to as kIN, is defined as a sum of adjacencies with respect to the module genes. In practice, these two measures are equivalent. To test whether a module is preserved in another data set, one can use various network statistics, e.g. formula_25.",
            "score": 43.30784749984741
        },
        {
            "docid": "159266_62",
            "document": "Gene expression . Gene networks can also be constructed without formulating an explicit causal model. This is often the case when assembling networks from large expression data sets. Covariation and correlation of expression is computed across a large sample of cases and measurements (often transcriptome or proteome data). The source of variation can be either experimental or natural (observational). There are several ways to construct gene expression networks, but one common approach is to compute a matrix of all pair-wise correlations of expression across conditions, time points, or individuals and convert the matrix (after thresholding at some cut-off value) into a graphical representation in which nodes represent genes, transcripts, or proteins and edges connecting these nodes represent the strength of association (see ). Weighted correlation network analysis (WGCNA) involves weighted networks defined by soft-thresholding the pairwise correlations among variables (e.g. measures of transcript abundance). WGCNA can be applied to compute eigengenes, which are highly robust biomarkers (features) useful for diagnosis and prognosis.",
            "score": 43.992289900779724
        },
        {
            "docid": "14642741_4",
            "document": "Digital image correlation and tracking . The concept of using cross-correlation to measure shifts in datasets has been known for a long time, and it has been applied to digital images since at least the early 1970s. The present-day applications are almost innumerable and include image analysis, image compression, velocimetry, and strain estimation. Much early work in DIC in the field of mechanics was led by researchers at the University of South Carolina in the early 1980s and has been optimized and improved in recent years. Commonly, DIC relies on finding the maximum of the correlation array between pixel intensity array subsets on two or more corresponding images, which gives the integer translational shift between them. It is also possible to estimate shifts to a finer resolution than the resolution of the original images, which is often called \"subpixel\" registration because the measured shift is smaller than an integer pixel unit. For subpixel interpolation of the shift, there are other methods that do not simply maximize the correlation coefficient. An iterative approach can also be used to maximize the interpolated correlation coefficient by using nonlinear optimization techniques. The nonlinear optimization approach tends to be conceptually simpler, but as with most nonlinear optimization techniques , it is quite slow, and the problem can sometimes be reduced to a much faster and more stable linear optimization in phase space.",
            "score": 21.906318426132202
        },
        {
            "docid": "53576321_30",
            "document": "Single-cell transcriptomics . Gene regulatory network inference is a technique that aims to construct a network, shown as a graph, in which the nodes represent the genes and edges indicate co-regulatory interactions. The method relies on the assumption that a strong statistical relationship between the expression of genes is an indication of a potential functional relationship. The most commonly used method to measure the strength of a statistical relationship is correlation. However, correlation fails to identify non-linear relationships and mutual information is used as an alternative. Gene clusters linked in a network signify genes that undergo coordinated changes in expression.",
            "score": 38.2260000705719
        },
        {
            "docid": "253492_23",
            "document": "Factor analysis . The values of the loadings \"L\", the averages \u03bc, and the variances of the \"errors\" \u03b5 must be estimated given the observed data \"X\" and \"F\" (the assumption about the levels of the factors is fixed for a given \"F\").  The \"fundamental theorem\" may be derived from the above conditions: The term on the left is the (a,b) term of the correlation matrix (an formula_58 matrix) of the observed data, and its formula_35 diagonal elements will be 1's. The last term on the right will be a diagonal matrix with terms less than unity. The first term on the right is the \"reduced correlation matrix\" and will be equal to the correlation matrix except for its diagonal values which will be less than unity. These diagonal elements of the reduced correlation matrix are called \"communalities\" (which represent the fraction of the variance in the observed variable that is accounted for by the factors): The sample data formula_61 will not, of course, exactly obey the fundamental equation given above due to sampling errors, inadequacy of the model, etc. The goal of any analysis of the above model is to find the factors formula_62 and loadings formula_50 which, in some sense, give a \"best fit\" to the data. In factor analysis, the best fit is defined as the minimum of the mean square error in the off-diagonal residuals of the correlation matrix:",
            "score": 25.76086688041687
        },
        {
            "docid": "35256679_18",
            "document": "Community fingerprinting . The brightness of the fluorescently labeled primers correlates to how prevalent that bacterial type is in the community. The banding pattern on the gel can be interpreted as a community-specific profile. Each DNA band or peak indicates at least one representative of that organism. In RISA, the bands on gel that do not match up in length represent different organisms in the community because they have different spacer regions between the two highly conserved genes. The electropherogram shows peaks correlating to the relative abundance of that spacer region in the sample.",
            "score": 21.806671619415283
        },
        {
            "docid": "10571004_5",
            "document": "Biological network inference . Genes are the nodes and the edges are directed. A gene serves as the source of a direct regulatory edge to a target gene by producing an RNA or protein molecule that functions as a transcriptional activator or inhibitor of the target gene. If the gene is an activator, then it is the source of a positive regulatory connection; if an inhibitor, then it is the source of a negative regulatory connection. Computational algorithms take as primary input data measurements of mRNA expression levels of the genes under consideration for inclusion in the network, returning an estimate of the network topology. Such algorithms are typically based on linearity, independence or normality assumptions, which must be verified on a case-by-case basis. Clustering or some form of statistical classification is typically employed to perform an initial organization of the high-throughput mRNA expression values derived from microarray experiments, in particular to select sets of genes as candidates for network nodes. The question then arises: how can the clustering or classification results be connected to the underlying biology? Such results can be useful for pattern classification \u2013 for example, to classify subtypes of cancer, or to predict differential responses to a drug (pharmacogenomics). But to understand the relationships between the genes, that is, to more precisely define the influence of each gene on the others, the scientist typically attempts to reconstruct the transcriptional regulatory network. This can be done by data integration in dynamic models supported by background literature, or information in public databases, combined with the clustering results. The modelling can be done by a Boolean network, by Ordinary differential equations or Linear regression models, e.g. Least-angle regression, by Bayesian network or based on Information theory approaches. For instance it can be done by the application of a correlation-based inference algorithm, as will be discussed below, an approach which is having increased success as the size of the available microarray sets keeps increasing",
            "score": 37.14272737503052
        },
        {
            "docid": "9534983_2",
            "document": "Biocultural diversity . Biocultural diversity is defined by Luisa Maffi as \"the diversity of life in all its manifestations: biological, cultural, and linguistic \u2014 which are interrelated (and possibly coevolved) within a complex socio-ecological adaptive system.\" \"The diversity of life is made up not only of the diversity of plants and animal species, habitats and ecosystems found on the planet, but also of the diversity of human cultures and languages.\" Certain geographic areas have been positively correlated with high levels of biocultural diversity, including those of low latitudes, higher rainfalls, higher temperatures, coastlines, and high altitudes. A negative correlation is found with areas of high latitudes, plains, and drier climates. Positive correlations can also be found between biological diversity and linguistic diversity, illustrated in the overlap between the distribution of plant diverse and language diverse zones. Social factors, such as modes of subsistence, have also been found to affect biocultural diversity.",
            "score": 24.09440016746521
        },
        {
            "docid": "337996_6",
            "document": "Market risk . The variance covariance and historical simulation approach to calculating VaR assumes that historical correlations are stable and will not change in the future or breakdown under times of market stress. However these assumptions are inappropriate as during periods of high volatility and market turbulence, historical correlations tend to break down. Intuitively, this is evident during a financial crisis where all industry sectors experience a significant increase in correlations, as opposed to an upwards trending market. This phenomenon is also known as asymmetric correlations or asymmetric dependence. Rather than using Historical Simulation, Monte-Carlo simulations with well-specified multivariate models are an excellent alternative. For example, to improve the estimation of the variance covariance matrix, one can generate a forecast of asset distributions via Monte-Carlo simulation based upon the Gaussian copula and well-specified marginals. Allowing the modeling process to allow for empirical characteristics in stock returns such as auto-regression, asymmetric volatility, skewness, and kurtosis is important. Not accounting for these attributes lead to severe estimation error in the correlation and variance covariance that have negative biases (as much as 70% of the true values). Estimation of VaR or CVaR for large portfolios of assets using the variance covariance matrix may be inappropriate if the underlying returns distributions exhibit asymmetric dependence. In such scenarios, vine copulas that allow for asymmetric dependence (e.g., Clayton, Rotated Gumbel) across portfolios of assets are most appropriate in the calculation of tail risk using VaR or CVaR.",
            "score": 19.598944425582886
        },
        {
            "docid": "50430120_19",
            "document": "Short interspersed nuclear elements (SINEs) . Understanding the different ways in which microRNA regulates gene-expression, including mRNA-translation and degradation is key to understanding the potential evolutionary role of SINEs in gene-regulation and in the generation of microRNA loci. This, in addition to SINEs\u2019 direct role in regulatory networks (as discussed in SINEs as long non-coding RNAs) is crucial to beginning to understand the relationship between SINEs and certain diseases. Multiple studies have suggested that increased SINE activity is correlated with certain gene-expression profiles and post-transcription regulation of certain genes. In fact, Peterson et al. 2013 demonstrated that high SINE RNA expression correlates with post-transcriptional downregulation of BRCA1, a tumor suppressor implicated in multiple forms of cancer, namely breast cancer. Furthermore, studies have established a strong correlation between transcriptional mobilization of SINEs and certain cancers and conditions such as hypoxia; this can be due to the genomic instability caused by SINE activity as well as more direct-downstream effects. SINEs have also been implicated in countless other diseases. In essence, short-interspersed nuclear elements have become deeply integrated in countless regulatory, metabolic and signaling pathways and thus play an inevitable role in causing disease. Much is still to be known about these genomic parasites but it is clear they play a significant role within eukaryotic organisms.",
            "score": 34.196354150772095
        },
        {
            "docid": "31356717_9",
            "document": "Gary E. Martin . He has had a long-standing interest in heteronuclear NMR and 2D long-range heteronuclear shift correlation in particular. He was among the first to exploit natural abundance long-range 1H-15N heteronuclear shift correlation experiments, those early reports leading to hundreds of published reports that are the subject of multiple reviews and chapters., More recently, his research interests have also led to the development of unsymmetrical indirect covariance NMR processing methods that have the potential for significant spectrometer time savings when experimental access to hyphenated 2D NMR. These methods also provide access to 13C-15N Heteronuclear Single Quantum Coherence-Heteronuclear Multiple Bond Coherence (HSQC-HMBC) correlation data that are experimentally inaccessible at natural abundance, and to HSQC-ADEQUATE correlation plots that allow carbon-carbon connectivity networks of molecules to be mapped without having to resort to the highly insensitive 13C-13C INADEQUATE experiment. In recent years Martin has extended his work into the application of residual dipolar couplings, residual chemical shift anisotropy and  DFT calculations to demonstrate that, in combination, some of the most complex chemical structures could be elucidated and making unambiguous assignment essentially difficult or impossible.",
            "score": 31.532135009765625
        },
        {
            "docid": "1408929_2",
            "document": "Metagenomics . Metagenomics is the study of genetic material recovered directly from environmental samples. The broad field may also be referred to as environmental genomics, ecogenomics or community genomics. While traditional microbiology and microbial genome sequencing and genomics rely upon cultivated clonal cultures, early environmental gene sequencing cloned specific genes (often the 16S rRNA gene) to produce a profile of diversity in a natural sample. Such work revealed that the vast majority of microbial biodiversity had been missed by cultivation-based methods. Recent studies use either \"shotgun\" or PCR directed sequencing to get largely unbiased samples of all genes from all the members of the sampled communities. Because of its ability to reveal the previously hidden diversity of microscopic life, metagenomics offers a powerful lens for viewing the microbial world that has the potential to revolutionize understanding of the entire living world. As the price of DNA sequencing continues to fall, metagenomics now allows microbial ecology to be investigated at a much greater scale and detail than before.",
            "score": 40.5005099773407
        },
        {
            "docid": "374298_50",
            "document": "G factor (psychometrics) . The genetic correlation is a statistic that indicates the extent to which the same genetic effects influence two different traits. If the genetic correlation between two traits is zero, the genetic effects on them are independent, whereas a correlation of 1.0 means that the same set of genes explains the heritability of both traits (regardless of how high or low the heritability of each is). Genetic correlations between specific mental abilities (such as verbal ability and spatial ability) have been consistently found to be very high, close to 1.0. This indicates that genetic variation in cognitive abilities is almost entirely due to genetic variation in whatever \"g\" is. It also suggests that what is common among cognitive abilities is largely caused by genes, and that independence among abilities is largely due to environmental effects. Thus it has been argued that when genes for intelligence are identified, they will be \"generalist genes\", each affecting many different cognitive abilities.",
            "score": 30.934161901474
        },
        {
            "docid": "11391832_8",
            "document": "Intraclass correlation . The key difference between this ICC and the interclass (Pearson) correlation is that the data are pooled to estimate the mean and variance. The reason for this is that in the setting where an intraclass correlation is desired, the pairs are considered to be unordered. For example, if we are studying the resemblance of twins, there is usually no meaningful way to order the values for the two individuals within a twin pair. Like the interclass correlation, the intraclass correlation for paired data will be confined to the interval\u00a0[\u22121,\u00a0+1].",
            "score": 33.251460313797
        },
        {
            "docid": "19798753_10",
            "document": "Correlation attack . While the above example illustrates well the relatively simple concepts behind correlation attacks, it perhaps simplifies the explanation of precisely how the brute forcing of individual LFSRs proceeds. We make the statement that incorrectly guessed keys will generate LFSR output which agrees with the generator output roughly 50% of the time, because given two random bit sequences of a given length, the probability of agreement between the sequences at any particular bit is 0.5. However, specific individual incorrect keys may well generate LFSR output which agrees with the generator output more or less often than exactly 50% of the time. This is particularly salient in the case of LFSRs whose correlation with the generator is not especially strong; for small enough correlations it is certainly not outside the realm of possibility that an incorrectly guessed key will also lead to LFSR output that agrees with the desired number of bits of the generator output. Thus we may not be able to find the key for that LFSR uniquely and with certainty. We may instead find a number of possible keys, although this is still a significant breach of the cipher's security. If we had, say, a megabyte of known plaintext, the situation would be substantially different. An incorrect key may generate LFSR output that agrees with more than 512 kilobytes of the generator output, but not likely to generate output that agrees with as much as 768 kilobytes of the generator output like a correctly guessed key would. As a rule, the weaker the correlation between an individual register and the generator output, the more known plaintext is required to find that register's key with a high degree of confidence. Readers with a background in probability theory should be able to see easily how to formalise this argument and obtain estimates of the length of known plaintext required for a given correlation using the binomial distribution.",
            "score": 28.11479914188385
        },
        {
            "docid": "221708_74",
            "document": "Pearson correlation coefficient . A distance metric for two variables X and Y known as \"Pearson's distance\" can be defined from their correlation coefficient as Considering that the Pearson correlation coefficient falls between [\u22121, 1], the Pearson distance lies in [0, 2]. The Pearson distance has been used in cluster analysis and data detection for communications and storage with unknown gain and offset",
            "score": 29.034179091453552
        },
        {
            "docid": "26685_54",
            "document": "Statistics . The concept of correlation is particularly noteworthy for the potential confusion it can cause. Statistical analysis of a data set often reveals that two variables (properties) of the population under consideration tend to vary together, as if they were connected. For example, a study of annual income that also looks at age of death might find that poor people tend to have shorter lives than affluent people. The two variables are said to be correlated; however, they may or may not be the cause of one another. The correlation phenomena could be caused by a third, previously unconsidered phenomenon, called a lurking variable or confounding variable. For this reason, there is no way to immediately infer the existence of a causal relationship between the two variables. (See Correlation does not imply causation.)",
            "score": 35.12046146392822
        },
        {
            "docid": "50525886_154",
            "document": "Hydrogen isotope biogeochemistry . Assimilation of diet into tissue has a tissue-specific fractionation known as the trophic discrimination factor. Diet sources can be tracked through a food web via deuterium isotope profiles, although this is complicated by deuterium having two potential sources - water and food. Food more strongly impacts \u03b4D than does exchange with surrounding water, and that signal is seen across trophic levels. However, different organisms derive organic hydrogen in varying ratios of water to food: for example, in quail, 20-30% of organic hydrogen was from water and the remainder from food. The precise percentage of hydrogen from water was dependent on tissue source and metabolic activity. In chironomids, 31-47% of biomass hydrogen derived from water, and in microbes as much as 100% of fatty acid hydrogen can be derived from water depending on substrate. In caterpillars, diet \u03b4D from organic matter correlates linearly with tissue \u03b4D. The same relationship does not appear to hold consistently for diet \u03b4D from water, however - water derived from either the caterpillar or its prey plant is more deuterium enriched than their organic material. Going up trophic levels from prey (plant) to predator (caterpillar) results in an isotopic enrichment. This same trend of enrichment is seen in many other animals - carnivores, omnivores, and herbivores - and appears to follow N relative abundances. Carnivores at the same trophic level tend to exhibit the same level of H enrichment. Because, as mentioned earlier, the amount of organic hydrogen produced from water varies between species, a model of trophic level related to absolute fractionation is difficult to make if the participating species are not known. Consistency in measuring the same tissues is also important, as different tissues fractionate deuterium differently. In aquatic systems, tracking trophic interactions is valuable for not only understanding the ecology of the system, but also for determining the degree of terrestrial input. The patterns of deuterium enrichment consistent within trophic levels is a useful tool for assessing the nature of these interactions in the environment.",
            "score": 16.491661310195923
        }
    ],
    "r": [
        {
            "docid": "41224221_2",
            "document": "Weighted correlation network analysis . Weighted correlation network analysis, also known as weighted gene co-expression network analysis (WGCNA), is a widely used data mining method especially for studying biological networks based on pairwise correlations between variables. While it can be applied to most high-dimensional data sets, it has been most widely used in genomic applications. It allows one to define modules (clusters), intramodular hubs, and network nodes with regard to module membership, to study the relationships between co-expression modules, and to compare the network topology of different networks (differential network analysis). WGCNA can be used as a data reduction technique (related to oblique factor analysis ), as a clustering method (fuzzy clustering), as a feature selection method (e.g. as gene screening method), as a framework for integrating complementary (genomic) data (based on weighted correlations between quantitative variables), and as a data exploratory technique. Although WGCNA incorporates traditional data exploratory techniques, its intuitive network language and analysis framework transcend any standard analysis technique. Since it uses network methodology and is well suited for integrating complementary genomic data sets, it can be interpreted as systems biologic or systems genetic data analysis method. By selecting intramodular hubs in consensus modules, WGCNA also gives rise to network based meta analysis techniques.",
            "score": 60.57798385620117
        },
        {
            "docid": "205464_8",
            "document": "Human microbiota . Aside from simply elucidating the composition of the human microbiome, one of the major questions involving the human microbiome is whether there is a \"core\", that is, whether there is a subset of the community that is shared among most humans. If there is a core, then it would be possible to associate certain community compositions with disease states, which is one of the goals of the Human Microbiome Project. It is known that the human microbiome (such as the gut microbiota) is highly variable both within a single subject and among different individuals, a phenomenon which is also observed in mice.  On 13 June 2012, a major milestone of the Human Microbiome Project (HMP) was announced by the NIH director Francis Collins. The announcement was accompanied with a series of coordinated articles published in Nature and several journals in the Public Library of Science (PLoS) on the same day. By mapping the normal microbial make-up of healthy humans using genome sequencing techniques, the researchers of the HMP have created a reference database and the boundaries of normal microbial variation in humans. From 242 healthy U.S. volunteers, more than 5,000 samples were collected from tissues from 15 (men) to 18 (women) body sites such as mouth, nose, skin, lower intestine (stool), and vagina. All the DNA, human and microbial, were analyzed with DNA sequencing machines. The microbial genome data were extracted by identifying the bacterial specific ribosomal RNA, 16S rRNA. The researchers calculated that more than 10,000 microbial species occupy the human ecosystem and they have identified 81 \u2013 99% of the genera.",
            "score": 60.086795806884766
        },
        {
            "docid": "53368771_20",
            "document": "Pharmacomicrobiomics . The limitations of pharmacomicrobiomics primarily arise from the uncertainty associated with metagenomic profiling. Namely, short reads obtained by shotgun sequencing can be difficult to align to reference genomes since many organism have homologous sequences. In addition, 16S rRNA sequencing cannot consistently resolve species identity, a finding that casts doubt on species identities in metagenomic samples. Limitations also arise from differing study designs, as unique approaches to identifying the nature of the xenobiotic-microbiome interactions are often taken. For instance, because pharmacomicrobiomics very broadly denotes the association between xenobiotics and the microbiome, the extent to which studies profile the genetics of the microbiome can vary significantly. Studies aiming to characterize organism identity, but not gene identity or copy number may elect to use 16S shotgun sequencing as opposed to SMS. Conversely, studies aiming to identify genes and their products rather than organism identity may elect WMGS coupled with transcriptomic analysis. Initially, these differences may mean that researchers wanting to investigate publicly available data may have to target their research questions to fit the data at hand.",
            "score": 56.4450798034668
        },
        {
            "docid": "20521504_5",
            "document": "Streptomyces griseus . Like other streptomycetes, \"S. griseus\" has a high GC content in its genome, with an average of 72.2%. The species was first classified within the genus \"Streptomyces\" by Waksman and Henrici in 1948. The taxonomy of \"S. griseus\" and its evolutionarily related strains have been a considerable source of confusion for microbial systematists. 16S rRNA gene sequence data have been used to recognise the related strains, and are called \"S. griseus\" 16S rRNA gene clade. The strains of this clade have homogeneous phenotypic properties but show substantial genotypic heterogenecity based on genomic data. Several attempts are still made to solve this issue using techniques such as and multilocus sequence typing. A whole genome sequenc] was carried out on the IFO 13350 strain of \"S. griseus\".",
            "score": 55.89213562011719
        },
        {
            "docid": "41224221_16",
            "document": "Weighted correlation network analysis . It is often used as data reduction step in systems genetic applications where modules are represented by \"module eigengenes\" e.g. Module eigengenes can be used to correlate modules with clinical traits. Eigengene networks are coexpression networks between module eigengenes (i.e. networks whose nodes are modules) . WGCNA is widely used in neuroscientific applications, e.g. and for analyzing genomic data including microarray data, single cell RNA-Seq data DNA methylation data, miRNA data, peptide counts and microbiota data (16S rRNA gene sequencing). Other applications include brain imaging data, e.g. functional MRI data",
            "score": 55.85618209838867
        },
        {
            "docid": "44248347_28",
            "document": "Gene Disease Database . The response of bioinformatics to new experimental techniques brings a new perspective into the analysis of the experimental data, as demonstrated by the advances in the analysis of information from gene disease databases and other technologies. It is expected that this trend will continue with novel approaches to respond to new techniques, such as next-generation sequencing technologies. For instance, the availability of large numbers of individual human genomes will promote the development of computational analyses of rare variants, including the statistical mining of their relations to lifestyles, drug interactions and other factors. Biomedical research will also be driven by our ability to efficiently mine the large body of existing and continuously generated biomedical data. Text-mining techniques, in particular, when combined with other molecular data, can provide information about gene mutations and interactions and will become crucial to stay ahead of the exponential growth of data generated in biomedical research. Another field that is benefiting from the advances in mining and integration of molecular, clinical and drug analysis is pharmacogenomics. \"In silico\" studies of the relationships between human variations and their effect on diseases will be key to the development of personalized medicine. In summary, Gene Disease Databases have already transformed the search for disease genes and has the potential to become a crucial component of other areas of medical research.",
            "score": 54.677284240722656
        },
        {
            "docid": "47268817_9",
            "document": "PICRUSt . Langille et al., 2013 tested the accuracy of this genome prediction step by using previously reported datasets in which the same biological sample was subjected to 16S rRNA gene amplification and shotgun metagenomics. In these cases, the shotgun metagenomic results were taken as a representation of the 'true' community, and the 16S rRNA gene amplicon libraries fed into PICRUSt to attempt to predict those data. Test datasets included human microbiome samples from the Human Microbiome Project, soil samples, diverse mammalian samples, and samples from the Guerrero Negro microbial mats",
            "score": 54.55286407470703
        },
        {
            "docid": "20374_50",
            "document": "Metabolism . An idea of the complexity of the metabolic networks in cells that contain thousands of different enzymes is given by the figure showing the interactions between just 43 proteins and 40 metabolites to the right: the sequences of genomes provide lists containing anything up to 45,000 genes. However, it is now possible to use this genomic data to reconstruct complete networks of biochemical reactions and produce more holistic mathematical models that may explain and predict their behavior. These models are especially powerful when used to integrate the pathway and metabolite data obtained through classical methods with data on gene expression from proteomic and DNA microarray studies. Using these techniques, a model of human metabolism has now been produced, which will guide future drug discovery and biochemical research. These models are now used in network analysis, to classify human diseases into groups that share common proteins or metabolites.",
            "score": 53.76222229003906
        },
        {
            "docid": "24219329_35",
            "document": "Neurogenomics . Sequence data is used to understand the evolutionary genetic changes which led to the development of the human CNS. We can then understand how the neurological phenotypes differ between species. Comparative genomics entails comparison of sequence data across a phylogeny to pinpoint the genotypic changes that occur within specific lineages, and understand how these changes might have arisen. The increase in high quality mammalian reference sequences generally makes comparative analysis better as it increases statistical power. However, the increase in number of species in a phylogeny does risk adding unnecessary noise as the alignments of the orthologous sequences usually decrease in quality. Furthermore, different classes of species will have significant differences in their phenotypes.",
            "score": 52.242618560791016
        },
        {
            "docid": "1872854_31",
            "document": "Biochemical cascade . In the post-genomic age, high-throughput sequencing and gene/protein profiling techniques have transformed biological research by enabling comprehensive monitoring of a biological system, yielding a list of differentially expressed genes or proteins, which is useful in identifying genes that may have roles in a given phenomenon or phenotype. With DNA microarrays and genome-wide gene engineering, it is possible to screen global gene expression profiles to contribute a wealth of genomic data to the public domain. With RNA interference, it is possible to distill the inferences contained in the experimental literature and primary databases into knowledge bases that consist of annotated representations of biological pathways. In this case, individual genes and proteins are known to be involved in biological processes, components, or structures, as well as how and where gene products interact with each other. Pathway-oriented approaches for analyzing microarray data, by grouping long lists of individual genes, proteins, and/or other biological molecules according to the pathways they are involved in into smaller sets of related genes or proteins, which reduces the complexity, have proven useful for connecting genomic data to specific biological processes and systems. Identifying active pathways that differ between two conditions can have more explanatory power than a simple list of different genes or proteins. In addition, a large number of pathway analytic methods exploit pathway knowledge in public repositories such as Gene Ontology (GO) or Kyoto Encyclopedia of Genes and Genomes (KEGG), rather than inferring pathways from molecular measurements. Furthermore, different research focuses have given the word \"pathway\" different meanings. For example, 'pathway' can denote a metabolic pathway involving a sequence of enzyme-catalyzed reactions of small molecules, or a signaling pathway involving a set of protein phosphorylation reactions and gene regulation events. Therefore, the term \"pathway analysis\" has a very broad application. For instance, it can refer to the analysis physical interaction networks (e.g., protein\u2013protein interactions), kinetic simulation of pathways, and steady-state pathway analysis (e.g., flux-balance analysis), as well as its usage in the inference of pathways from expression and sequence data. Several functional enrichment analysis tools and algorithms have been developed to enhance data interpretation. The existing knowledge base\u2013driven pathway analysis methods in each generation have been summarized in recent literature.",
            "score": 51.505950927734375
        },
        {
            "docid": "24373430_5",
            "document": "Weighted network . Weighted networks are also widely used in genomic and systems biologic applications.  (Horvath, 2011). For example, weighted gene co-expression network analysis (WGCNA) is often used for constructing a weighted network among genes (or gene products) based on gene expression (e.g. microarray) data (Zhang and Horvath 2005). More generally, weighted correlation networks can be defined by soft-thresholding the pairwise correlations among variables (e.g. gene measurements).",
            "score": 51.463600158691406
        },
        {
            "docid": "34970018_11",
            "document": "Earth Microbiome Project . Depending on the biological question, researchers can choose to sequence a metagenomic sample using two main approaches. If the biological question to be resolved is, what types of organisms are present and in what abundance, the preferred approach would be to target and amplify a specific gene that is highly conserved among the species of interest. The 16S ribosomal RNA gene for bacteria and the 18S ribosomal RNA gene for protists are often used as target genes for this purpose. The advantage of targeting a specific gene is that the gene can be amplified and sequenced at a very high coverage. This approach is called \"deep sequencing\", which allows rare species to be identified in a sample. However, this approach will not enable assembly of any whole genomes, nor will it provide information on how organisms may interact with each other. The second approach is called shotgun metagenomics, in which all the DNA in the sample is sheared and the random fragments sequenced. In principle, this approach allows for the assembly of whole microbial genomes, and it allows inference of metabolic relationships. However, if most of microbes are uncharacterised in a given environment, \"de novo\" assembly will be computationally expensive.",
            "score": 51.439971923828125
        },
        {
            "docid": "2571276_2",
            "document": "Computational genomics . Computational genomics (often referred to as Computational Genetics) refers to the use of computational and statistical analysis to decipher biology from genome sequences and related data, including both DNA and RNA sequence as well as other \"post-genomic\" data (i.e., experimental data obtained with technologies that require the genome sequence, such as genomic DNA microarrays). These, in combination with computational and statistical approaches to understanding the function of the genes and statistical association analysis, this field is also often referred to as Computational and Statistical Genetics/genomics. As such, computational genomics may be regarded as a subset of bioinformatics and computational biology, but with a focus on using whole genomes (rather than individual genes) to understand the principles of how the DNA of a species controls its biology at the molecular level and beyond. With the current abundance of massive biological datasets, computational studies have become one of the most important means to biological discovery.",
            "score": 51.415653228759766
        },
        {
            "docid": "50518079_11",
            "document": "Human Genome Structural Variation . The 1000 genomes project was able to successfully produce the DNA sequence of the human genome. They provided much sequencing data from many populations to analyze as well as a reference human genome for comparison and future studies. One study took advantage of this resource to question the structural variation differences between genomes from whole genome sequence data. It was known that human diseases are affected by duplications and deletions and that copy number analysis is common but multiallelic copy number variants (mCNVs) were not as well studied. The researchers got their data from the 1000 genomes project and analyzed 849 different genomes from a variety of populations that were sequenced in order to find large mCNVs. From their analysis, they found that mCNVs create most genetic variation in gene dosage compared to other structural variants and that the gene expression variation is created by the dosage diversity of genes created by mCNVs. The study underlined the great significance that structural variants, especially mCNVs, have on gene dosage which leads to variable gene expressions and human phenotypic diversity in the population.",
            "score": 51.19880676269531
        },
        {
            "docid": "53368771_3",
            "document": "Pharmacomicrobiomics . Efforts to understand the interaction between specific xenobiotics and the microbiome have traditionally involved the use of \"in vivo\" as well as \"in vitro\" models. Recently, next generation sequencing of genomic DNA obtained from a community of microbes has been used to identify organisms within microbial communities, allowing for accurate profiles of the composition of microbes within an environment. Initiatives such as the Human Microbiome Project (HMP) have aimed to characterize the microbial composition of the oral, gut, vaginal, skin and nasal environments. This and other microbiome characterization projects have accelerated the study of pharmacomicrobiomics. An extensive understanding of the microbiome in the human body can lead to the development of novel therapeutics and personalized drug treatments that are not potentiated or activated by processes carried out by the microbiome.",
            "score": 50.9894905090332
        },
        {
            "docid": "57122799_6",
            "document": "Nikos Kyrpides . His current research is focusing on Microbiome Research with an emphasis on Microbiome Data Science. This includes the understanding of structure and function of various microorganisms and microbial communities and the elucidation of the evolutionary dynamics that shape the microbial genomes. To accomplish that, his group is developing novel methods for enabling large-scale comparative analysis, as well as mining and visualization of big data. In this direction, he proposed and published the first study on the use of standard benchmarking data for evaluation of methods accuracy in metagenomics. This approach has become the standard in the field. Some of his more recent research work in microbiome data science include the exploration of Earth\u2019s Virome, the identification of new bacterial phyla the prediction of novel folds using metagenomic sequences, and characterization of novel protein families from microbiome data.",
            "score": 50.7952995300293
        },
        {
            "docid": "31737415_11",
            "document": "Ziheng Yang . Yang and Rannala also developed the multispecies coalescent model, which has emerged as the natural framework for comparative analysis of genomic sequence data from multiple species, incorporating the coalescent process in both modern species and extinct ancestors. The model has been used to estimate the species tree despite gene tree heterogeneity among genomic regions, and to delimit/identify species. Yang champions the Bayesian full-likelihood method of inference, using Markov chain Monte Carlo to average over gene trees (gene genealogies), accommodating phylogenetic uncertainties.",
            "score": 50.651920318603516
        },
        {
            "docid": "579390_9",
            "document": "Gene prediction . Despite these difficulties, extensive transcript and protein sequence databases have been generated for human as well as other important model organisms in biology, such as mice and yeast. For example, the RefSeq database contains transcript and protein sequence from many different species, and the Ensembl system comprehensively maps this evidence to human and several other genomes. It is, however, likely that these databases are both incomplete and contain small but significant amounts of erroneous data.",
            "score": 50.61839294433594
        },
        {
            "docid": "2355052_4",
            "document": "Genome size . The genome sizes of thousands of eukaryotes have been analyzed over the past 50 years, and these data are available in online databases for animals, plants, and fungi (see external links). Nuclear genome size is typically measured in eukaryotes using either densitometric measurements of Feulgen-stained nuclei (previously using specialized densitometers, now more commonly using computerized image analysis) or flow cytometry. In prokaryotes, pulsed field gel electrophoresis and complete genome sequencing are the predominant methods of genome size determination. Nuclear genome sizes are well known to vary enormously among eukaryotic species. In animals they range more than 3,300-fold, and in land plants they differ by a factor of about 1,000. Protist genomes have been reported to vary more than 300,000-fold in size, but the high end of this range (\"Amoeba\") has been called into question. In eukaryotes, but not prokaryotes, variation in genome size is not proportional to the number of genes, an observation that was deemed wholly counterintuitive before the discovery of non-coding DNA and which became known as the C-value paradox as a result. However, although there is no longer any paradoxical aspect to the discrepancy between genome size and gene number, this term remains in common usage. For reasons of conceptual clarification, the various puzzles that remain with regard to genome size variation instead have been suggested by one author to more accurately comprise a puzzle or an enigma (the C-value enigma). Genome size correlates with a range of features at the cell and organism levels, including cell size, cell division rate, and, depending on the taxon, body size, metabolic rate, developmental rate, organ complexity, geographical distribution, or extinction risk (for recent reviews, see Bennett and Leitch 2005; Gregory 2005). Based on completely sequenced genome data currently (as of April 2009) available, log-transformed gene number forms a linear correlation with log-transformed genome size in bacteria, archea, viruses, and organelles combined whereas a nonlinear (semi-natural log) correlation in eukaryotes (Hou and Lin 2009 ). The nonlinear correlation for eukaryotes, although claim of its existence contrasts the previous view that no correlation exists for this group of organisms, reflects disproportionately fast increasing noncoding DNA in increasingly large eukaryotic genomes. Although sequenced genome data are practically biased toward small genomes, which may compromise the accuracy of the empirically derived correlation, and the ultimate proof of the correlation remains to be obtained by sequencing some of the largest eukaryotic genomes, current data do not seem to rule out a correlation.",
            "score": 50.52975845336914
        },
        {
            "docid": "29467449_19",
            "document": "Protein function prediction . Several networks based on different data sources can be combined into a composite network, which can then be used by a prediction algorithm to annotate candidate genes or proteins. For example, the developers of the bioPIXIE system used a wide variety of \"Saccharomyces cerevisiae\" (yeast) genomic data to produce a composite functional network for that species. This resource allows the visualization of known networks representing biological processes, as well as the prediction of novel components of those networks. Many algorithms have been developed to predict function based on the integration of several data sources (e.g. genomic, proteomic, protein interaction, etc.), and testing on previously annotated genes indicates a high level of accuracy. Disadvantages of some function prediction algorithms have included a lack of accessibility, and the time required for analysis. Faster, more accurate algorithms such as GeneMANIA (multiple association network integration algorithm) have however been developed in recent years and are publicly available on the web, indicating the future direction of function prediction.",
            "score": 50.470703125
        },
        {
            "docid": "45663583_3",
            "document": "Human interactome . With the sequencing of the genomes of a diverse array or model organisms, it became clear that the number of genes does not correlate with the human perception of relative organism complexity \u2013 the human proteome contains some 20 000 genes, which is smaller than some species such as corn. A statistical approach to calculating the number of interactions in humans gives an estimate of around 650 000, one order of magnitude bigger than Drosophila and 3 times larger than C. Elegans. As of 2008, only about <0.3% of all estimated interactions among human proteins has been identified, although in recent years there has been exponential growth in discovery \u2013 as of 2015, over 210 000 unique human positive protein\u2013protein interactions are currently catalogued, and bioGRID database contains almost 750 000 literature-curated PPI's for 30 model organisms, 300 000 of which are verified or predicted human physical or genetic protein\u2013protein interactions, a 50% increase from 2013. The currently available information on the human interactome network originates from either literature-curated interactions, high-throughput experiments, or from potential interactions predicted from interactome data, whether through phylogenetic profiling (evolutionary similarity), statistical network inference, or text/literature mining methods.",
            "score": 50.23088073730469
        },
        {
            "docid": "19374_16",
            "document": "Model organism . Genomic data is used to make close comparisons between species and determine relatedness. As humans, we share about 99% of our genome with chimpanzees (98.7% with bonobos) and over 90% with the mouse. With so much of the genome conserved across species, it is relatively impressive that the differences between humans and mice can be accounted for in approximately six thousand genes (of ~30,000 total). Scientists have been able to take advantage of these similarities in generating experimental and predictive models of human disease.",
            "score": 49.98661422729492
        },
        {
            "docid": "35256679_2",
            "document": "Community fingerprinting . Community fingerprinting is a set of molecular biology techniques that can be used to quickly profile the diversity of a microbial community. Rather than directly identifying or counting individual cells in an environmental sample, these techniques show how many variants of a gene are present. In general, it is assumed that each different gene variant represents a different type of microbe. Community fingerprinting is used by microbiologists studying a variety of microbial systems (e.g. marine, freshwater, soil, and human microbial communities) to measure biodiversity or track changes in community structure over time. The method analyzes environmental samples by assaying genomic DNA. This approach offers an alternative to microbial culturing, which is important because most microbes cannot be cultured in the laboratory. Community fingerprinting does not result in identification of individual microbe species; instead, it presents an overall picture of a microbial community. These methods are now largely being replaced by high throughput sequencing, such as targeted microbiome analysis (e.g., 16s rRNA sequencing) and metagenomics.",
            "score": 49.8073844909668
        },
        {
            "docid": "917868_21",
            "document": "Comparative genomics . Comparative genomics also opens up new avenues in other areas of research. As DNA sequencing technology has become more accessible, the number of sequenced genomes has grown. With the increasing reservoir of available genomic data, the potency of comparative genomic inference has grown as well. A notable case of this increased potency is found in recent primate research. Comparative genomic methods have allowed researchers to gather information about genetic variation, differential gene expression, and evolutionary dynamics in primates that were indiscernible using previous data and methods. The Great Ape Genome Project used comparative genomic methods to investigate genetic variation with reference to the six great ape species, finding healthy levels of variation in their gene pool despite shrinking population size. Another study showed that patterns of DNA methylation, which are a known regulation mechanism for gene expression, differ in the prefrontal cortex of humans versus chimps, and implicated this difference in the evolutionary divergence of the two species.",
            "score": 49.78028106689453
        },
        {
            "docid": "5255433_4",
            "document": "Fiona Brinkman . Brinkman's current research interests center around improving understanding of how microbes evolve and improving computational methods that aid the analysis of microbes and the development of new vaccines, drugs and diagnostics for infectious diseases. Increasingly her methods have been applied for more environmental applications. She is noted for developing PSORTb, the most precise method available for computational protein subcellular localization prediction and the first computational method that exceeded the accuracy of some common high-throughput laboratory methods for such subcellular localization analysis. This method aids the prediction of cell surface and secreted proteins in a bacterial cell that may be suitable drug targets, vaccine components or diagnostics. She has also developed bioinformatics methods that aid the more accurate identification of genomic islands (i.e. IslandViewer) and orthologs (i.e. OrtholugeDB) . Her research has provided new insights into the evolution of pathogens and the role that horizontal gene transfer and genomic islands play. She confirmed the anecdotal assumption that virulence factors (disease-causing genes in pathogens) are disproportionately associated with genomic islands. She was among the first researchers to use whole genome sequencing to aid infectious disease outbreak investigations (\"genomic epidemiology\"), integrating genome sequence data with social network analysis. She was involved in the Pseudomonas Genome Project and is the coordinator of the Pseudomonas Genome Database, a database of Pseudomonas species genomic data and associated annotations that is continually updated. She has also developed databases (i.e. InnateDB and the Allergy and Asthma Portal) to aid more systems-based analysis of immune disorders and the immune response to infections in humans and other animals - databases that have aided the identification of new immune-modulating therapeutics. She has a long-standing interest in bioinformatics training, improving the curation of biological/bioinformatics data, and developing effective bioinformatics data standards and databases. She is a Thomson Reuter's Highly Cited Researcher, a member of national committees and Boards such as the Genome Canada Board of Directors, and has been Research Director for several Genomics projects. She has a growing interest in applying her methods to environmental applications as part of a broader interest in developing approaches for more holistic, sustainable infectious disease control and microbiome conservation - developing approaches that may select less for antimicrobial resistance, improve the tracking of pathogens and their origins, and better factor in the important role of societal changes and the environment in shaping microbiomes",
            "score": 49.72645568847656
        },
        {
            "docid": "47268817_10",
            "document": "PICRUSt . Because PICRUSt, and evolutionary comparative genomics in general, depends on sequenced genomes, biological samples from well-studied environments (many sequenced genomes) will be better predicted than poorly studied environments. In order to assess how many genomes are available, PICRUSt optionally allows users to calculate a Nearest Sequenced Taxon Index (NSTI) for their samples. This index reflects the average phylogenetic distance between each 16S rRNA gene sequence in their sample, and a 16S rRNA gene sequence from a fully sequenced genome. In general, the lower the NSTI score, the more accurate PICRUSt's predictions are expected to be. For example, showed that PICRUSt was much more accurate on diverse soil samples and samples from the Human Microbiome Project than on microbial mat samples from Guerrero Negro, which contained many bacteria without any sequenced relatives.",
            "score": 49.52965545654297
        },
        {
            "docid": "149326_14",
            "document": "Phylogenetic tree . Although phylogenetic trees produced on the basis of sequenced genes or genomic data in different species can provide evolutionary insight, they have important limitations. Most importantly, they do not necessarily accurately represent the evolutionary history of the included taxa. In fact, they are literally scientific hypotheses, subject to falsification by further study (e.g., gathering of additional data, analyzing the existing data with improved methods). The data on which they are based is noisy; the analysis can be confounded by genetic recombination, horizontal gene transfer, hybridisation between species that were not nearest neighbors on the tree before hybridisation takes place, convergent evolution, and conserved sequences.",
            "score": 49.41459274291992
        },
        {
            "docid": "4995479_43",
            "document": "Genomic library . Genome-wide association studies are general applications to find specific gene targets and polymorphisms within the human race. In fact, the International HapMap project was created through a partnership of scientists and agencies from several countries to catalog and utilize this data. The goal of this project is to compare genetic sequences of different individuals to elucidate similarities and differences within chromosomal regions. Scientists from all of the participating nations are cataloging these attributes with data from populations of African, Asian, and European ancestry. Such genome-wide assessments may lead to further diagnostic and drug therapies while also helping future teams focus on orchestrating therapeutics with genetic features in mind. These concepts are already being exploited in genetic engineering. For example, a research team has actually constructed a PAC shuttle vector that creates a library representing two-fold coverage of the human genome. This could serve as an incredible resource to identify genes, or sets of genes, causing disease. Moreover, these studies can serve as a powerful way to investigate transcriptional regulation as it has been seen in the study of baculoviruses. Overall, advances in genome library construction and DNA sequencing has allowed for efficient discovery of different molecular targets. Assimilation of these features through such efficient methods can hasten the employment of novel drug candidates.",
            "score": 49.254005432128906
        },
        {
            "docid": "21673918_3",
            "document": "Bacterial one-hybrid system . Across all living organisms, regulation of gene expression is controlled by interactions between DNA-binding regulatory proteins (transcription factors) and cis-regulatory elements, DNA sequences in or around genes that act as target sites for DNA-binding proteins. By binding to cis-regulatory sequences and to each other, transcription factors fine-tune transcriptional levels by stabilizing/destabilizing binding of RNA polymerase to a gene's promoter. But despite their importance and ubiquity, little is known about where exactly each of these regulatory proteins binds. Literature suggests that nearly 8% of human genes encode transcription factors and the functions and specificities of their interactions remain largely unexplored. We are on the brink of a convergence of high-throughput technologies and genomic theory that is allowing researchers to start mapping these interactions on a genome-wide scale. Only recently has a complete survey of DNA-binding specificities been attempted for a large family of DNA-binding domains. B1H is just one emerging technique among many that is useful for studying protein\u2013DNA interactions.",
            "score": 48.85387420654297
        },
        {
            "docid": "7955_74",
            "document": "DNA . Bioinformatics involves the development of techniques to store, data mine, search and manipulate biological data, including DNA nucleic acid sequence data. These have led to widely applied advances in computer science, especially string searching algorithms, machine learning, and database theory. String searching or matching algorithms, which find an occurrence of a sequence of letters inside a larger sequence of letters, were developed to search for specific sequences of nucleotides. The DNA sequence may be aligned with other DNA sequences to identify homologous sequences and locate the specific mutations that make them distinct. These techniques, especially multiple sequence alignment, are used in studying phylogenetic relationships and protein function. Data sets representing entire genomes' worth of DNA sequences, such as those produced by the Human Genome Project, are difficult to use without the annotations that identify the locations of genes and regulatory elements on each chromosome. Regions of DNA sequence that have the characteristic patterns associated with protein- or RNA-coding genes can be identified by gene finding algorithms, which allow researchers to predict the presence of particular gene products and their possible functions in an organism even before they have been isolated experimentally. Entire genomes may also be compared, which can shed light on the evolutionary history of particular organism and permit the examination of complex evolutionary events.",
            "score": 48.810943603515625
        },
        {
            "docid": "16359310_15",
            "document": "Human Microbiome Project . From 242 healthy U.S. volunteers, more than 5,000 samples were collected from tissues from 15 (men) to 18 (women) body sites such as mouth, nose, skin, lower intestine (stool) and vagina. All the DNA, human and microbial, were analyzed with DNA sequencing machines. The microbial genome data were extracted by identifying the bacterial specific ribosomal RNA, 16S rRNA. The researchers calculated that more than 10,000 microbial species occupy the human ecosystem and they have identified 81 \u2013 99% of the genera. In addition to establishing the human microbiome reference database, the HMP project also discovered several \"surprises\", which include:",
            "score": 48.78651809692383
        },
        {
            "docid": "46968364_48",
            "document": "Inferring horizontal gene transfer . Standard tools to simulate sequence evolution along trees such as INDELible or PhyloSim can be adapted to simulate HGT. HGT events cause the relevant gene trees to conflict with the species tree. Such HGT events can be simulated through subtree pruning and regrafting rearrangements of the species tree. However, it is important to simulate data that are realistic enough to be representative of the challenge provided by real datasets, and simulation under complex models are thus preferable. A model was developed to simulate gene trees with heterogeneous substitution processes in addition to the occurrence of transfer, and accounting for the fact that transfer can come from now extinct donor lineages. Alternatively, the genome evolution simulator ALF directly generates gene families subject to HGT, by accounting for a whole range of evolutionary forces at the base level, but in the context of a complete genome. Given simulated sequences which have HGT, analysis of those sequences using the methods of interest and comparison of their results with the known truth permits study of their performance. Similarly, testing the methods on sequence known not to have HGT enables the study of false positive rates.",
            "score": 48.71372604370117
        }
    ]
}