{
    "q": [
        {
            "docid": "8402086_18",
            "document": "Computational neurogenetic modeling . Because the amount of data on the interplay of genes and neurons and their effects is not enough to construct a rigorous model,  evolutionary computation is used to optimize artificial neural networks and gene regulatory networks, a common technique being the genetic algorithm. A genetic algorithm is a process that can be used to refine models by mimicking the process of natural selection observed in biological ecosystems. The primary advantages are that, due to not requiring derivative information, it can be applied to black box problems and multimodal optimization. The typical process for using genetic algorithms to refine a gene  regulatory network is: first, create a population; next, to create offspring via a crossover operation and  evaluate their fitness; then, on a group chosen for high fitness, simulate mutation via a mutation operator;  finally, taking the now mutated group, repeat this process until a desired level of fitness is demonstrated. Methods by which artificial neural networks may alter their structure without simulated mutation and fitness selection have been developed. A dynamically evolving neural network is one approach, as the creation of new connections and new neurons can  be modeled as the system adapts to new data. This enables the network to evolve in modeling accuracy without simulated natural selection. One method by which dynamically evolving networks may be optimized, called evolving layer neuron aggregation, combines neurons with sufficiently similar input weights into one neuron. This can take place during the training of the network, referred to as online aggregation, or between periods of training, referred to as offline aggregation. Experiments have suggested that offline aggregation is more efficient.",
            "score": 96.41902422904968
        },
        {
            "docid": "190837_3",
            "document": "Evolutionary algorithm . Evolutionary algorithms often perform well approximating solutions to all types of problems because they ideally do not make any assumption about the underlying fitness landscape. Techniques from evolutionary algorithms applied to the modeling of biological evolution are generally limited to explorations of microevolutionary processes and planning models based upon cellular processes. In most real applications of EAs, computational complexity is a prohibiting factor. In fact, this computational complexity is due to fitness function evaluation. Fitness approximation is one of the solutions to overcome this difficulty. However, seemingly simple EA can solve often complex problems; therefore, there may be no direct link between algorithm complexity and problem complexity.",
            "score": 87.31238675117493
        },
        {
            "docid": "1363296_2",
            "document": "Modelling biological systems . Modelling biological systems is a significant task of systems biology and mathematical biology. Computational systems biology aims to develop and use efficient algorithms, data structures, visualization and communication tools with the goal of computer modelling of biological systems. It involves the use of computer simulations of biological systems, including cellular subsystems (such as the networks of metabolites and enzymes which comprise metabolism, signal transduction pathways and gene regulatory networks), to both analyze and visualize the complex connections of these cellular processes.",
            "score": 91.78725171089172
        },
        {
            "docid": "149353_4",
            "document": "Computational biology . Computational Biology, which includes many aspects of bioinformatics, is the science of using biological data to develop algorithms or models to understand biological systems and relationships.  Until recently, biologists did not have access to very large amounts of data. This data has now become commonplace, particularly in molecular biology and genomics. Researchers were able to develop analytical methods for interpreting biological information, but were unable to share them quickly among colleagues.  Bioinformatics began to develop in the early 1970s. It was considered the science of analyzing informatics processes of various biological systems. At this time, research in artificial intelligence was using network models of the human brain in order to generate new algorithms. This use of biological data to develop other fields pushed biological researchers to revisit the idea of using computers to evaluate and compare large data sets. By 1982, information was being shared amongst researchers through the use of punch cards. The amount of data being shared began to grow exponentially by the end of the 1980s. This required the development of new computational methods in order to quickly analyze and interpret relevant information. Since the late 1990s, computational biology has become an important part of developing emerging technologies for the field of biology. The terms computational biology and evolutionary computation have a similar name, but are not to be confused. Unlike computational biology, evolutionary computation is not concerned with modeling and analyzing biological data. It instead creates algorithms based on the ideas of evolution across species. Sometimes referred to as genetic algorithms, the research of this field can be applied to computational biology. While evolutionary computation is not inherently a part of computational biology, Computational evolutionary biology is a subfield of it.",
            "score": 87.57885193824768
        },
        {
            "docid": "27051151_69",
            "document": "Big data . Much in the same line, it has been pointed out that the decisions based on the analysis of big data are inevitably \"informed by the world as it was in the past, or, at best, as it currently is\". Fed by a large number of data on past experiences, algorithms can predict future development if the future is similar to the past. If the systems dynamics of the future change (if it is not a stationary process), the past can say little about the future. In order to make predictions in changing environments, it would be necessary to have a thorough understanding of the systems dynamic, which requires theory. As a response to this critique Alemany Oliver and Vayre suggested to use \"abductive reasoning as a first step in the research process in order to bring context to consumers\u2019 digital traces and make new theories emerge\". Additionally, it has been suggested to combine big data approaches with computer simulations, such as agent-based models and Complex Systems. Agent-based models are increasingly getting better in predicting the outcome of social complexities of even unknown future scenarios through computer simulations that are based on a collection of mutually interdependent algorithms. Finally, use of multivariate methods that probe for the latent structure of the data, such as factor analysis and cluster analysis, have proven useful as analytic approaches that go well beyond the bi-variate approaches (cross-tabs) typically employed with smaller data sets.",
            "score": 98.94378876686096
        },
        {
            "docid": "3986130_23",
            "document": "Computational phylogenetics . The \"pruning\" algorithm, a variant of dynamic programming, is often used to reduce the search space by efficiently calculating the likelihood of subtrees. The method calculates the likelihood for each site in a \"linear\" manner, starting at a node whose only descendants are leaves (that is, the tips of the tree) and working backwards toward the \"bottom\" node in nested sets. However, the trees produced by the method are only rooted if the substitution model is irreversible, which is not generally true of biological systems. The search for the maximum-likelihood tree also includes a branch length optimization component that is difficult to improve upon algorithmically; general global optimization tools such as the Newton-Raphson method are often used.",
            "score": 81.75622367858887
        },
        {
            "docid": "40158142_9",
            "document": "Nonlinear system identification . Artificial neural networks try loosely to imitate the network of neurons in the brain where computation takes place through a large number of simple processing elements. A typical neural network consists of a number of simple processing units interconnected to form a complex network. Layers of such units are arranged so that data is entered at the input layer and passes through either one or several intermediate layers before reaching the output layer. In supervised learning the network is trained by operating on the difference between the actual output and the desired output of the network, the prediction error, to change the connection strengths between the nodes. By iterating the weights are modified until the output error reaches an acceptable level. This process is called machine learning because the network adjusts the weights so that the output pattern is reproduced. Neural networks have been extensively studied and there are many excellent textbooks devoted to this topic in general, and more focussed textbooks which emphasise control and systems applications. There are two main problem types that can be studied using neural networks: static problems, and dynamic problems. Static problems include pattern recognition, classification, and approximation. Dynamic problems involve lagged variables and are more appropriate for system identification and related applications. Depending on the architecture of the network the training problem can be either nonlinear-in-the-parameters which involves optimisation or linear-in-the-parameters which can be solved using classical approaches. The training algorithms can be categorised into supervised, unsupervised, or reinforcement learning. Neural networks have excellent approximation properties but these are usually based on standard function approximation results using for example the Weierstrass Theorem that applies equally well to polynomials, rational functions, and other well-known models.  Neural networks have been applied extensively to system identification problems which involve nonlinear and dynamic relationships. However, classical neural networks are purely gross static approximating machines. There is no dynamics within the network. Hence when fitting dynamic models all the dynamics arise by allocating lagged inputs and outputs to the input layer of the network. The training procedure then produces the best static approximation that relates the lagged variables assigned to the input nodes to the output. There are more complex network architectures, including recurrent networks, that produce dynamics by introducing increasing orders of lagged variables to the input nodes. But in these cases it is very easy to over specify the lags and this can lead to over fitting and poor generalisation properties.  Neural networks have several advantages; they are conceptually simple, easy to train and to use, have excellent approximation properties, the concept of local and parallel processing is important and this provides integrity and fault tolerant behaviour. The biggest criticism of the classical neural network models is that the models produced are completely opaque and usually cannot be written down or analysed. It is therefore very difficult to know what is causing what, to analyse the model, or to compute dynamic characteristics from the model. Some of these points will not be relevant to all applications but they are for dynamic modelling.",
            "score": 92.64100480079651
        },
        {
            "docid": "43966823_3",
            "document": "Multi-state modeling of biomolecules . Biological signaling systems often rely on complexes of biological macromolecules that can undergo several functionally significant modifications that are mutually compatible. Thus, they can exist in a very large number of functionally different states. Modeling such multi-state systems poses two problems: The problem of how to describe and specify a multi-state system (the \"specification problem\") and the problem of how to use a computer to simulate the progress of the system over time (the \"computation problem\"). To address the specification problem, modelers have in recent years moved away from explicit specification of all possible states, and towards rule-based formalisms that allow for implicit model specification, including the \u03ba-calculus, BioNetGen, the Allosteric Network Compiler and others. To tackle the computation problem, they have turned to particle-based methods that have in many cases proved more computationally efficient than population-based methods based on ordinary differential equations, partial differential equations, or the Gillespie stochastic simulation algorithm. Given current computing technology, particle-based methods are sometimes the only possible option. Particle-based simulators further fall into two categories: Non-spatial simulators such as StochSim, DYNSTOC, RuleMonkey, and NFSim and spatial simulators, including Meredys, SRSim and MCell. Modelers can thus choose from a variety of tools; the best choice depending on the particular problem. Development of faster and more powerful methods is ongoing, promising the ability to simulate ever more complex signaling processes in the future.",
            "score": 98.5519984960556
        },
        {
            "docid": "25130414_2",
            "document": "Rocchio algorithm . The Rocchio algorithm is based on a method of relevance feedback found in information retrieval systems which stemmed from the SMART Information Retrieval System which was developed 1960-1964. Like many other retrieval systems, the Rocchio feedback approach was developed using the Vector Space Model. The algorithm is based on the assumption that most users have a general conception of which documents should be denoted as relevant or non-relevant. Therefore, the user's search query is revised to include an arbitrary percentage of relevant and non-relevant documents as a means of increasing the search engine's recall, and possibly the precision as well. The number of relevant and non-relevant documents allowed to enter a query is dictated by the weights of the a, b, c variables listed below in the Algorithm section.",
            "score": 70.67359185218811
        },
        {
            "docid": "48514357_23",
            "document": "Urban traffic modeling and analysis . The existing multiple process algorithms use different proven methodologies, approaches and characteristics. Some of them have been noticed previously. Algorithms may differ depending on the data of their model is based on or the way they structure and link these data. So, models, often close to the way they manage data, can focus on either traffic volumes, travel speed, occupancy, road capacities etc. To do so, models often range from ARIMA to Dynamic Generalized Linear Models (Dynamic GLM) and Neural Networks.",
            "score": 84.37216997146606
        },
        {
            "docid": "6165293_9",
            "document": "Antecedent moisture . An alternative approach for modeling antecedent moisture is to start from measurements of the behavior of the system and the external influences (inputs to the system) and try to determine a mathematical relation between them without going into the details of what is actually happening inside the system. This approach is called system identification. System identification is applied in several fields beyond engineering, ranging from economics to astronomy, and it also comes under other names (such as inverse modeling, time series analysis, and empirical physical modeling). System identification is a general term to describe mathematical tools and algorithms that build dynamical models from measured data. A dynamical model in this context is a mathematical description of the dynamic behavior of a system or process. In many cases, a so-called white-box model based on first principles (e.g., a model for a physical process from Newton's laws of motion) will be overly complex and possibly even impossible to obtain in reasonable time, due to the complex nature of many systems and processes.",
            "score": 98.76806402206421
        },
        {
            "docid": "1072943_15",
            "document": "Forward algorithm . The forward algorithm is mostly used in applications that need us to determine the probability of being in a specific state when we know about the sequence of observations. We first calculate the probabilities over the states computed for the previous observation and use them for the current observations, and then extend it out for the next step using the transition probability table.The approach basically caches all the intermediate state probabilities so they are computed only once. This helps us to compute a fixed state path. The process is also called posterior decoding. The algorithm computes probability much more efficiently than the naive approach, which very quickly ends up in a combinatorial explosion. Together, they can provide the probability of a given emission/observation at each position in the sequence of observations. It is from this information that a version of the most likely state path is computed (\"posterior decoding\"). The algorithm can be applied wherever we can train a model as we receive data using Baum-Welch or any general EM algorithm. The Forward algorithm will then tell us about the probability of data with respect to what is expected from our model. One of the applications can be in the domain of Finance, where it can help decide on when to buy or sell tangible assets. It can have applications in all fields where we apply Hidden Markov Models. The popular ones include Natural language processing domains like tagging part-of-speech and speech recognition. Recently it is also being used in the domain of Bioinformatics. Forward algorithm can also be applied to perform Weather speculations. We can have a HMM describing the weather and its relation to the state of observations for few consecutive days (some examples could be dry, damp, soggy, sunny, cloudy, rainy etc.). We can consider calculating the probability of observing any sequence of observations recursively given the HMM. We can then calculate the probability of reaching an intermediate state as the sum of all possible paths to that state. Thus the partial probabilities for the final observation will hold the probability of reaching those states going through all possible paths.",
            "score": 59.189966320991516
        },
        {
            "docid": "53970843_13",
            "document": "Machine learning in bioinformatics . Machine learning has been used to aid in the modelling of these complex interactions in biological systems in domains such as genetic networks, signal transduction networks, and metabolic pathways. Probabilistic graphical models, a machine learning technique for determining the structure between different variables, are one of the most commonly used methods for modeling genetic networks. In addition, machine learning has been applied to systems biology problems such as identifying transcription factor binding sites using a technique known as Markov chain optimization. Genetic algorithms, machine learning techniques which are based on the natural process of evolution, have been used to model genetic networks and regulatory structures.",
            "score": 93.53704285621643
        },
        {
            "docid": "3062637_4",
            "document": "Estimation of distribution algorithm . The general procedure of an EDA is outlined in the following: Using explicit probabilistic models in optimization allowed EDAs to feasibly solve optimization problems that were notoriously difficult for most conventional evolutionary algorithms and traditional optimization techniques, such as problems with high levels of epistasis. Nonetheless, the advantage of EDAs is also that these algorithms provide an optimization practitioner with a series of probabilistic models that reveal a lot of information about the problem being solved. This information can in turn be used to design problem-specific neighborhood operators for local search, to bias future runs of EDAs on a similar problem, or to create an efficient computational model of the problem.",
            "score": 60.72503662109375
        },
        {
            "docid": "233488_23",
            "document": "Machine learning . An artificial neural network (ANN) learning algorithm, usually called \"neural network\" (NN), is a learning algorithm that is vaguely inspired by biological neural networks. Computations are structured in terms of an interconnected group of artificial neurons, processing information using a connectionist approach to computation. Modern neural networks are non-linear statistical data modeling tools. They are usually used to model complex relationships between inputs and outputs, to find patterns in data, or to capture the statistical structure in an unknown joint probability distribution between observed variables.",
            "score": 76.80846381187439
        },
        {
            "docid": "44012968_5",
            "document": "B. S. Daya Sagar . Sagar\u2019s research contributions span both basic and applied fields involves fusion of computer simulations and modeling techniques in order to develop cogent models in discrete space to gain an understanding of dynamical behavior of complex terrestrial systems, such as water bodies, river networks, mountain objects, folds, dunes, landscapes, and rock porous media. He developed specific technologies and spatial algorithms for using data acquired at multiple spatial and temporal scalesfor developing cogent domain-specific models. His work employs concepts stemmed out of recent theories\u2014such as mathematical morphology, fractals, chaos, and morphometrics\u2014to develop spatial algorithms for (i) pattern retrieval, (ii) pattern analysis, (iii) simulation and modeling, and (iv) reasoning and visualization of the spatial and temporal phenomena of terrestrial and GISci relevance. His studies show that there exist several open challenges that need to be addressed by mathematical geosciences community. One of such challenges is to understand the complexity in spatiotemporal behaviors of several terrestrial phenomena and processes via construction of process-specific \"attractors\" that may range from \"simple\" to \"strange\".",
            "score": 86.90173673629761
        },
        {
            "docid": "34119149_26",
            "document": "Geotechnical centrifuge modeling . Centrifuge tests can also be used to obtain experimental data to verify a design procedure or a computer model. The rapid development of computational power over recent decades has revolutionized engineering analysis. Many computer models have been developed to predict the behavior of geotechnical structures during earthquakes and other loads. Before a computer model can be used with confidence, it must be proven to be valid based on evidence. The meager and unrepeatable data provided by natural earthquakes, for example, is usually insufficient for this purpose. Verification of the validity of assumptions made by a computational algorithm is especially important in the area of geotechnical engineering due to the complexity of soil behavior. Soils exhibit highly non-linear behavior, their strength and stiffness depend on their stress history and on the water pressure in the pore fluid, all of which may evolve during the loading caused by an earthquake. The computer models which are intended to simulate these phenomena are very complex and require extensive verification. Experimental data from centrifuge tests is useful for verifying assumptions made by a computational algorithm. If the results show the computer model to be inaccurate, the centrifuge test data provides insight into the physical processes which in turn stimulates the development of better computer models.",
            "score": 78.54906630516052
        },
        {
            "docid": "13793747_7",
            "document": "Group method of data handling . In order to find the best solution GMDH algorithms consider various component subsets of the base function (1) called \"partial models\". Coefficients of these models are estimated by the least squares method. GMDH algorithms gradually increase the number of partial model components and find a model structure with optimal complexity indicated by the minimum value of an \"external criterion\". This process is called self-organization of models.",
            "score": 70.3260817527771
        },
        {
            "docid": "51865496_8",
            "document": "Crowd analysis . There are countless social applications of crowd analysis ranging from uses within the film and video game industries, to uses in public planning. Being that crowd simulations are based on group dynamics and crowd psychology, the accuracy and relevance to real life situations is clear. A large aspect of public planning and its use of crowd analysis lies within the realm of situational representations for emergency evacuation. Evacuations can be planned via the modeling and study of crowd interaction and reaction. These representations are based on biological models and patterns, thus the movements predicted are quite realistic. Similar models are utilized within motion picture industries to produce realistic and life-like simulations and scenes.  A system can generate a realistic crowd simulation with given inputs and simulate how the simulated moving objects, or agents, will interact with each other and with the environment. The goal is to replicate a crowd's movement patterns given a large number of agents in a given space. Algorithms based on crowd analysis attempt to manage the movement of the crowd. The more efficient and realistic a simulation becomes, the more complex the algorithm must become. The software must be able to manipulate the trajectory of individual agents based on variables such as the agents' goals, stress forces, obstacles, and levels of arousal. There are several software utilized to develop and study crowd dynamics:",
            "score": 99.23251235485077
        },
        {
            "docid": "149353_8",
            "document": "Computational biology . Computational biomodeling is a field concerned with building computer models of biological systems. Computational biomodeling aims to develop and use visual simulations in order to assess the complexity of biological systems. This is accomplished through the use of specialized algorithms, and visualization software. These models allow for prediction of how systems will react under different environments. This is useful for determining if a system is robust. A robust biological system is one that \u201cmaintain their state and functions against external and internal perturbations\u201d, which is essential for a biological system to survive. Computational biomodeling generates a large archive of such data, allowing for analysis from multiple users. While current techniques focus on small biological systems, researchers are working on approaches that will allow for larger networks to be analyzed and modeled. A majority of researchers believe that this will be essential in developing modern medical approaches to creating new drugs and gene therapy. A useful modelling approach is to use Petri nets via tools such as esyN",
            "score": 105.37087965011597
        },
        {
            "docid": "40158142_12",
            "document": "Nonlinear system identification . Structure detection forms the most fundamental part of NARMAX. For example a NARMAX model which consists of one lagged input and one lagged output term, three lagged noise terms, expanded as a cubic polynomial would consist of fifty six possible candidate terms. This number of candidate terms arises because the expansion by definition includes all possible combinations within the cubic expansion. Naively proceeding to estimate a model which includes all these terms and then pruning will cause numerical and computational problems and should always be avoided. However, only a few terms are often important in the model. Structure detection, which aims to select terms one at a time, is therefore critically important. These objectives can easily be achieved by using the Orthogonal Least Squares algorithm and its derivatives to select the NARMAX model terms one at a time. These ideas can also be adapted for pattern recognition and feature selection and provide an alternative to principal component analysis but with the advantage that the features are revealed as basis functions that are easily related back to the original problem.  NARMAX methods are designed to do far more than to just find the best approximating model. System identification can be divided into two aims. The first involves approximation where the key aim is to develop a model that approximates the data set such that good predictions can be made. There are many applications where this approach is appropriate, for example in time series prediction of the weather, stock prices, speech, target tracking, pattern classification etc. In such applications the form of the model is not that important. The objective is to find an approximation scheme which produces the minimum prediction errors. A second objective of system identification, which includes the first objective as a subset, involves much more than just finding a model to achieve the best mean squared errors. This second aim is why the NARMAX philosophy was developed and is linked to the idea of finding the simplest model structure. The aim here is to develop models that reproduce the dynamic characteristics of the underlying system, to find the simplest possible model, and if possible to relate this to components and behaviours of the system under study. The core aim of this second approach to identification is therefore to identify and reveal the rule that represents the system. These objectives are relevant to model simulation and control systems design, but increasingly to applications in medicine, neuro science, and the life sciences. Here the aim is to identify models, often nonlinear, that can be used to understand the basic mechanisms of how these systems operate and behave so that we can manipulate and utilise these. NARMAX methods have also been developed in the frequency and spatio-temporal domains.",
            "score": 95.088045835495
        },
        {
            "docid": "1100516_4",
            "document": "Model predictive control . The models used in MPC are generally intended to represent the behavior of complex dynamical systems. The additional complexity of the MPC control algorithm is not generally needed to provide adequate control of simple systems, which are often controlled well by generic PID controllers. Common dynamic characteristics that are difficult for PID controllers include large time delays and high-order dynamics.",
            "score": 88.53296661376953
        },
        {
            "docid": "13629713_3",
            "document": "Applicability domain . The purpose of AD is to state whether the model's assumptions are met and for which chemicals the model can be reliably applicable. In general, this is the case for interpolation rather than for extrapolation. Up to now there is no single generally accepted algorithm for determining the AD: a comprehensive survey can be found in a Report and Recommendations of ECVAM Workshop 52. There exists a rather systematic approach for defining interpolation regions. The process involves the removal of outliers and a probability density distribution method using kernel-weighted sampling.  Another widely used approach for the structural AD of the regression QSAR models is based on the leverage calculated from the diagonal values of the hat matrix of the modeling molecular descriptors.  A recent rigorous benchmarking study of several AD algorithms identified standard-deviation of model predictions as the most reliable approach. To investigate the AD of a training set of chemicals one can directly analyse properties of the multivariate descriptor space of the training compounds or more indirectly via distance (or similarity) metrics. When using distance metrics care should be taken to use an orthogonal and significant vector space. This can be achieved by different means of feature selection and successive principal components analysis.",
            "score": 77.50745487213135
        },
        {
            "docid": "28934119_3",
            "document": "Topic model . Topic models are also referred to as probabilistic topic models, which refers to statistical algorithms for discovering the latent semantic structures of an extensive text body. In the age of information, the amount of the written material we encounter each day is simply beyond our processing capacity. Topic models can help to organize and offer insights for us to understand large collections of unstructured text bodies. Originally developed as a text-mining tool, topic models have been used to detect instructive structures in data such as genetic information, images, and networks. They also have applications in other fields such as bioinformatics.",
            "score": 64.43137097358704
        },
        {
            "docid": "19208664_26",
            "document": "Neural modeling fields . During an adaptation process, initially fuzzy and uncertain models are associated with structures in the input signals, and fuzzy models become more definite and crisp with successive iterations. The type, shape, and number, of models are selected so that the internal representation within the system is similar to input signals: the NMF concept-models represent structure-objects in the signals. The figure below illustrates operations of dynamic logic. In Fig. 1(a) true \u2018smile\u2019 and \u2018frown\u2019 patterns are shown without noise; (b) actual image available for recognition (signal is below noise, signal-to-noise ratio is between \u20132dB and \u20130.7dB); (c) an initial fuzzy model, a large fuzziness corresponds to uncertainty of knowledge; (d) through (m) show improved models at various iteration stages (total of 22 iterations). Every five iterations the algorithm tried to increase or decrease the number of models. Between iterations (d) and (e) the algorithm decided, that it needs three Gaussian models for the \u2018best\u2019 fit.",
            "score": 80.47493696212769
        },
        {
            "docid": "24044102_2",
            "document": "Cellular model . Creating a cellular model has been a particularly challenging task of systems biology and mathematical biology. It involves developing efficient algorithms, data structures, visualization and communication tools to orchestrate the integration of large quantities of biological data with the goal of computer modeling.",
            "score": 79.8165397644043
        },
        {
            "docid": "6383817_4",
            "document": "Ancestral reconstruction . Ancestral reconstruction relies on a sufficiently realistic statistical model of evolution to accurately recover ancestral states. These models use the genetic information already obtained through methods such as phylogenetics to determine the route that evolution has taken and when evolutionary events occurred. No matter how well the model approximates the actual evolutionary history, however, one's ability to accurately reconstruct an ancestor deteriorates with increasing evolutionary time between that ancestor and its observed descendants. Additionally, more realistic models of evolution are inevitably more complex and difficult to calculate. Progress in the field of ancestral reconstruction has relied heavily on the exponential growth of computing power and the concomitant development of efficient computational algorithms (e.g., a dynamic programming algorithm for the joint maximum likelihood reconstruction of ancestral sequences.) Methods of ancestral reconstruction are often applied to a given phylogenetic tree that has already been inferred from the same data. While convenient, this approach has the disadvantage that its results are contingent on the accuracy of a single phylogenetic tree. In contrast, some researchers advocate a more computationally intensive Bayesian approach that accounts for uncertainty in tree reconstruction by evaluating ancestral reconstructions over many trees.",
            "score": 72.0542426109314
        },
        {
            "docid": "21652_14",
            "document": "Natural language processing . Many different classes of machine learning algorithms have been applied to natural language processing tasks. These algorithms take as input a large set of \"features\" that are generated from the input data. Some of the earliest-used algorithms, such as decision trees, produced systems of hard if-then rules similar to the systems of hand-written rules that were then common. Increasingly, however, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to each input feature. Such models have the advantage that they can express the relative certainty of many different possible answers rather than only one, producing more reliable results when such a model is included as a component of a larger system.",
            "score": 77.55292463302612
        },
        {
            "docid": "1179950_7",
            "document": "Feature selection . Subset selection evaluates a subset of features as a group for suitability. Subset selection algorithms can be broken up into Wrappers, Filters and Embedded. Wrappers use a search algorithm to search through the space of possible features and evaluate each subset by running a model on the subset. Wrappers can be computationally expensive and have a risk of over fitting to the model. Filters are similar to Wrappers in the search approach, but instead of evaluating against a model, a simpler filter is evaluated. Embedded techniques are embedded in and specific to a model.",
            "score": 67.50898480415344
        },
        {
            "docid": "10571004_4",
            "document": "Biological network inference . There is great interest in network medicine for the modelling biological systems. This article focuses on a necessary prerequisite to dynamic modeling of a network: inference of the topology, that is, prediction of the \"wiring diagram\" of the network. More specifically, we focus here on inference of biological network structure using the growing sets of high-throughput expression data for genes, proteins, and metabolites. Briefly, methods using high-throughput data for inference of regulatory networks rely on searching for patterns of partial correlation or conditional probabilities that indicate causal influence. Such patterns of partial correlations found in the high-throughput data, possibly combined with other supplemental data on the genes or proteins in the proposed networks, or combined with other information on the organism, form the basis upon which such algorithms work. Such algorithms can be of use in inferring the topology of any network where the change in state of one node can affect the state of other nodes.",
            "score": 88.28100109100342
        },
        {
            "docid": "42922637_5",
            "document": "Symbolic regression . This approach has, of course, the disadvantage of having a much larger space to search \u2014 in fact, not only the search space in symbolic regression is infinite, but there are an infinite number of models which will perfectly fit a finite data set (provided that the model complexity isn't artificially limited). This means that it will possibly take a symbolic regression algorithm much longer to find an appropriate model and parametrization, than traditional regression techniques. This can be attenuated by limiting the set of building blocks provided to the algorithm, based on existing knowledge of the system that produced the data; but in the end, using symbolic regression is a decision that has to be balanced with how much is known about the underlying system.",
            "score": 83.56422209739685
        },
        {
            "docid": "734256_4",
            "document": "Molecular modelling . This function, referred to as a potential function, computes the molecular potential energy as a sum of energy terms that describe the deviation of bond lengths, bond angles and torsion angles away from equilibrium values, plus terms for non-bonded pairs of atoms describing van der Waals and electrostatic interactions. The set of parameters consisting of equilibrium bond lengths, bond angles, partial charge values, force constants and van der Waals parameters are collectively termed a force field. Different implementations of molecular mechanics use different mathematical expressions and different parameters for the potential function. The common force fields in use today have been developed by using high level quantum calculations and/or fitting to experimental data. The method, termed energy minimization, is used to find positions of zero gradient for all atoms, in other words, a local energy minimum. Lower energy states are more stable and are commonly investigated because of their role in chemical and biological processes. A molecular dynamics simulation, on the other hand, computes the behaviour of a system as a function of time. It involves solving Newton's laws of motion, principally the second law, formula_3. Integration of Newton's laws of motion, using different integration algorithms, leads to atomic trajectories in space and time. The force on an atom is defined as the negative gradient of the potential energy function. The energy minimization method is useful to obtain a static picture for comparing between states of similar systems, while molecular dynamics provides information about the dynamic processes with the intrinsic inclusion of temperature effects.",
            "score": 78.77623271942139
        }
    ],
    "r": [
        {
            "docid": "149353_8",
            "document": "Computational biology . Computational biomodeling is a field concerned with building computer models of biological systems. Computational biomodeling aims to develop and use visual simulations in order to assess the complexity of biological systems. This is accomplished through the use of specialized algorithms, and visualization software. These models allow for prediction of how systems will react under different environments. This is useful for determining if a system is robust. A robust biological system is one that \u201cmaintain their state and functions against external and internal perturbations\u201d, which is essential for a biological system to survive. Computational biomodeling generates a large archive of such data, allowing for analysis from multiple users. While current techniques focus on small biological systems, researchers are working on approaches that will allow for larger networks to be analyzed and modeled. A majority of researchers believe that this will be essential in developing modern medical approaches to creating new drugs and gene therapy. A useful modelling approach is to use Petri nets via tools such as esyN",
            "score": 105.37088012695312
        },
        {
            "docid": "51865496_8",
            "document": "Crowd analysis . There are countless social applications of crowd analysis ranging from uses within the film and video game industries, to uses in public planning. Being that crowd simulations are based on group dynamics and crowd psychology, the accuracy and relevance to real life situations is clear. A large aspect of public planning and its use of crowd analysis lies within the realm of situational representations for emergency evacuation. Evacuations can be planned via the modeling and study of crowd interaction and reaction. These representations are based on biological models and patterns, thus the movements predicted are quite realistic. Similar models are utilized within motion picture industries to produce realistic and life-like simulations and scenes.  A system can generate a realistic crowd simulation with given inputs and simulate how the simulated moving objects, or agents, will interact with each other and with the environment. The goal is to replicate a crowd's movement patterns given a large number of agents in a given space. Algorithms based on crowd analysis attempt to manage the movement of the crowd. The more efficient and realistic a simulation becomes, the more complex the algorithm must become. The software must be able to manipulate the trajectory of individual agents based on variables such as the agents' goals, stress forces, obstacles, and levels of arousal. There are several software utilized to develop and study crowd dynamics:",
            "score": 99.23251342773438
        },
        {
            "docid": "27051151_69",
            "document": "Big data . Much in the same line, it has been pointed out that the decisions based on the analysis of big data are inevitably \"informed by the world as it was in the past, or, at best, as it currently is\". Fed by a large number of data on past experiences, algorithms can predict future development if the future is similar to the past. If the systems dynamics of the future change (if it is not a stationary process), the past can say little about the future. In order to make predictions in changing environments, it would be necessary to have a thorough understanding of the systems dynamic, which requires theory. As a response to this critique Alemany Oliver and Vayre suggested to use \"abductive reasoning as a first step in the research process in order to bring context to consumers\u2019 digital traces and make new theories emerge\". Additionally, it has been suggested to combine big data approaches with computer simulations, such as agent-based models and Complex Systems. Agent-based models are increasingly getting better in predicting the outcome of social complexities of even unknown future scenarios through computer simulations that are based on a collection of mutually interdependent algorithms. Finally, use of multivariate methods that probe for the latent structure of the data, such as factor analysis and cluster analysis, have proven useful as analytic approaches that go well beyond the bi-variate approaches (cross-tabs) typically employed with smaller data sets.",
            "score": 98.94378662109375
        },
        {
            "docid": "6165293_9",
            "document": "Antecedent moisture . An alternative approach for modeling antecedent moisture is to start from measurements of the behavior of the system and the external influences (inputs to the system) and try to determine a mathematical relation between them without going into the details of what is actually happening inside the system. This approach is called system identification. System identification is applied in several fields beyond engineering, ranging from economics to astronomy, and it also comes under other names (such as inverse modeling, time series analysis, and empirical physical modeling). System identification is a general term to describe mathematical tools and algorithms that build dynamical models from measured data. A dynamical model in this context is a mathematical description of the dynamic behavior of a system or process. In many cases, a so-called white-box model based on first principles (e.g., a model for a physical process from Newton's laws of motion) will be overly complex and possibly even impossible to obtain in reasonable time, due to the complex nature of many systems and processes.",
            "score": 98.76805877685547
        },
        {
            "docid": "552466_10",
            "document": "System identification . In the context of nonlinear system identification Jin et al. describe greybox modeling by assuming a model structure a priori and then estimating the model parameters. Parameter estimation is relatively easy if the model form is known but this is rarely the case. Alternatively the structure or model terms for both linear and highly complex nonlinear models can be identified using NARMAX methods. This approach is completely flexible and can be used with grey box models where the algorithms are primed with the known terms, or with completely black box models where the model terms are selected as part of the identification procedure. Another advantage of this approach is that the algorithms will just select linear terms if the system under study is linear, and nonlinear terms if the system is nonlinear, which allows a great deal of flexibility in the identification.",
            "score": 98.62957000732422
        },
        {
            "docid": "43966823_3",
            "document": "Multi-state modeling of biomolecules . Biological signaling systems often rely on complexes of biological macromolecules that can undergo several functionally significant modifications that are mutually compatible. Thus, they can exist in a very large number of functionally different states. Modeling such multi-state systems poses two problems: The problem of how to describe and specify a multi-state system (the \"specification problem\") and the problem of how to use a computer to simulate the progress of the system over time (the \"computation problem\"). To address the specification problem, modelers have in recent years moved away from explicit specification of all possible states, and towards rule-based formalisms that allow for implicit model specification, including the \u03ba-calculus, BioNetGen, the Allosteric Network Compiler and others. To tackle the computation problem, they have turned to particle-based methods that have in many cases proved more computationally efficient than population-based methods based on ordinary differential equations, partial differential equations, or the Gillespie stochastic simulation algorithm. Given current computing technology, particle-based methods are sometimes the only possible option. Particle-based simulators further fall into two categories: Non-spatial simulators such as StochSim, DYNSTOC, RuleMonkey, and NFSim and spatial simulators, including Meredys, SRSim and MCell. Modelers can thus choose from a variety of tools; the best choice depending on the particular problem. Development of faster and more powerful methods is ongoing, promising the ability to simulate ever more complex signaling processes in the future.",
            "score": 98.552001953125
        },
        {
            "docid": "8402086_18",
            "document": "Computational neurogenetic modeling . Because the amount of data on the interplay of genes and neurons and their effects is not enough to construct a rigorous model,  evolutionary computation is used to optimize artificial neural networks and gene regulatory networks, a common technique being the genetic algorithm. A genetic algorithm is a process that can be used to refine models by mimicking the process of natural selection observed in biological ecosystems. The primary advantages are that, due to not requiring derivative information, it can be applied to black box problems and multimodal optimization. The typical process for using genetic algorithms to refine a gene  regulatory network is: first, create a population; next, to create offspring via a crossover operation and  evaluate their fitness; then, on a group chosen for high fitness, simulate mutation via a mutation operator;  finally, taking the now mutated group, repeat this process until a desired level of fitness is demonstrated. Methods by which artificial neural networks may alter their structure without simulated mutation and fitness selection have been developed. A dynamically evolving neural network is one approach, as the creation of new connections and new neurons can  be modeled as the system adapts to new data. This enables the network to evolve in modeling accuracy without simulated natural selection. One method by which dynamically evolving networks may be optimized, called evolving layer neuron aggregation, combines neurons with sufficiently similar input weights into one neuron. This can take place during the training of the network, referred to as online aggregation, or between periods of training, referred to as offline aggregation. Experiments have suggested that offline aggregation is more efficient.",
            "score": 96.41902160644531
        },
        {
            "docid": "2506529_35",
            "document": "Cellular neural network . CNN processors are being used to understand systems that can be modeled using simple, coupled units, such as living cells, biological networks, physiological systems, and ecosystems. The CNN architecture captures some of the dynamics often seen in nature and is simple enough to analyze and conduct experiments. They are also being used for stochastic simulation techniques, which allow scientists to explore spin problems, population dynamics, lattice-based gas models, percolation, and other phenomena. Other simulation applications include heat transfer, mechanical vibrating systems, protein production, Josephson Transmission Line (JTL) problems, seismic wave propagation, and geothermal structures. Instances of 3D (Three Dimensional) CNN have been used to prove known complex shapes are emergent phenomena in complex systems, establishing a link between art, dynamical systems and VLSI technology. CNN processors have been used to research a variety of mathematical concepts, such as researching non-equilibrium systems, constructing non-linear systems of arbitrary complexity using a collection of simple, well-understood dynamic systems, studying emergent chaotic dynamics, generating chaotic signals, and in general discovering new dynamic behavior. They are often used in researching systemics, a trandisiplinary, scientific field that studies natural systems. The goal of systemics researchers is to develop a conceptual and mathematical framework necessary to analyze, model, and understand systems, including, but not limited to, atomic, mechanical, molecular, chemical, biological, ecological, social and economic systems. Topics explored are emergence, collective behavior, local activity and its impact on global behavior, and quantifying the complexity of an approximately spatial and topologically invariant system . Although another measure of complexity may not arouse enthusiasm (Seth Lloyd, a professor from Massachusetts Institute of Technology (MIT), has identified 32 different definitions of complexity), it can potentially be mathematically advantageous when analyzing systems such as economic and social systems.",
            "score": 95.39649963378906
        },
        {
            "docid": "40158142_12",
            "document": "Nonlinear system identification . Structure detection forms the most fundamental part of NARMAX. For example a NARMAX model which consists of one lagged input and one lagged output term, three lagged noise terms, expanded as a cubic polynomial would consist of fifty six possible candidate terms. This number of candidate terms arises because the expansion by definition includes all possible combinations within the cubic expansion. Naively proceeding to estimate a model which includes all these terms and then pruning will cause numerical and computational problems and should always be avoided. However, only a few terms are often important in the model. Structure detection, which aims to select terms one at a time, is therefore critically important. These objectives can easily be achieved by using the Orthogonal Least Squares algorithm and its derivatives to select the NARMAX model terms one at a time. These ideas can also be adapted for pattern recognition and feature selection and provide an alternative to principal component analysis but with the advantage that the features are revealed as basis functions that are easily related back to the original problem.  NARMAX methods are designed to do far more than to just find the best approximating model. System identification can be divided into two aims. The first involves approximation where the key aim is to develop a model that approximates the data set such that good predictions can be made. There are many applications where this approach is appropriate, for example in time series prediction of the weather, stock prices, speech, target tracking, pattern classification etc. In such applications the form of the model is not that important. The objective is to find an approximation scheme which produces the minimum prediction errors. A second objective of system identification, which includes the first objective as a subset, involves much more than just finding a model to achieve the best mean squared errors. This second aim is why the NARMAX philosophy was developed and is linked to the idea of finding the simplest model structure. The aim here is to develop models that reproduce the dynamic characteristics of the underlying system, to find the simplest possible model, and if possible to relate this to components and behaviours of the system under study. The core aim of this second approach to identification is therefore to identify and reveal the rule that represents the system. These objectives are relevant to model simulation and control systems design, but increasingly to applications in medicine, neuro science, and the life sciences. Here the aim is to identify models, often nonlinear, that can be used to understand the basic mechanisms of how these systems operate and behave so that we can manipulate and utilise these. NARMAX methods have also been developed in the frequency and spatio-temporal domains.",
            "score": 95.08804321289062
        },
        {
            "docid": "36237055_6",
            "document": "Post-contemporary . The origin in diversely phenomenological systems and exact models from life sciences is in the area of the theory of applied dynamical systems and global bifurcations. The special interest in the subject can be find in a new emergent cross-disciplinary field known as mathematical neuroscience. Its scopes include nonlinear models of individual neurons and networks. In-depth analysis of such systems requires development of advanced mathematical tools paired with sophisticated computations. For instance, Andrey Shilnikov, a neuroscientist and mathematician derives models and create bifurcation toolkits for studying a stunning array of complex activities such as multistability of individual neurons and polyrhythmic bursting patterns discovered in multifunctional central pattern generators governing vital locomotor behaviors of animals and humans. Thanks to the non linear qualitative dynamics, the organization of holistic nature of post-contemporary lineage was implemented by interoprativity and interactivity within the heterogeneous components and characters of Micro - systems, being present in many concrete social contents and in most territorial recourses. In this way post-structuralism as well as the new sciences of complexity, Complexity theory and chaos theory, were appropriated and interpreted within Micro - systems by means of \"self-creation\", expressing a fundamental dialectic among structure, mechanism and function, identifiable and recognizable in concrete territorial contexts.",
            "score": 93.64257049560547
        },
        {
            "docid": "53970843_13",
            "document": "Machine learning in bioinformatics . Machine learning has been used to aid in the modelling of these complex interactions in biological systems in domains such as genetic networks, signal transduction networks, and metabolic pathways. Probabilistic graphical models, a machine learning technique for determining the structure between different variables, are one of the most commonly used methods for modeling genetic networks. In addition, machine learning has been applied to systems biology problems such as identifying transcription factor binding sites using a technique known as Markov chain optimization. Genetic algorithms, machine learning techniques which are based on the natural process of evolution, have been used to model genetic networks and regulatory structures.",
            "score": 93.53704833984375
        },
        {
            "docid": "28564320_3",
            "document": "Complex systems biology . Most complex system models are often formulated in terms of concepts drawn from statistical physics, information theory and non-linear dynamics; however, such approaches are not focused on, or do not include, the conceptual part of complexity related to organization and topological attributes or algebraic topology, such as network connectivity of genomes, interactomes and biological organisms that are very important. Recently, the two complementary approaches based both on information theory, network topology/abstract graph theory concepts are being combined for example in the fields of neuroscience and human cognition. It is generally agreed that there is a hierarchy of complexity levels of organization that should be considered as distinct from that of the levels of reality in ontology. The hierarchy of complexity levels of organization in the biosphere is also recognized in modern classifications of taxonomic ranks, such as: biological domain and biosphere, biological kingdom, Phylum, biological class, order, family, genus and species. Because of their dynamic and composition variability, intrinsic \"fuzziness\", autopoietic attributes, ability to self-reproduce, and so on, organisms do not fit into the 'standard' definition of general systems, and they are therefore 'super-complex' in both their function and structure; organisms can be thus be defined in CSB only as 'meta-systems' of simpler dynamic systems Such a meta-system definition of organisms, species, 'ecosystems', and so on, is not equivalent to the definition of a \"system of systems\" as in Autopoietic Systems Theory,; it also differs from the definition proposed for example by K.D. Palmer in meta-system engineering, organisms being quite different from machines and automata with fixed input-output transition functions, or a continuous dynamical system with fixed phase space, contrary to the Cartesian philosophical thinking; thus, organisms cannot be defined merely in terms of a quintuple A of \"(states, startup state, input and output sets/alphabet, transition function)\", although 'non-deterministic automata', as well as 'fuzzy automata' have also been defined. Tessellation or cellular automata provide however an intuitive, visual/computational insight into the lower levels of complexity, and have therefore become an increasingly popular, discrete model studied in computability theory, applied mathematics, physics, computer science, theoretical biology/systems biology, cancer simulations and microstructure modeling. Evolving cellular automata using genetic algorithms is also an emerging field attempting to bridge the gap between the tessellation automata and the higher level complexity approaches in CSB.",
            "score": 92.7399673461914
        },
        {
            "docid": "40158142_9",
            "document": "Nonlinear system identification . Artificial neural networks try loosely to imitate the network of neurons in the brain where computation takes place through a large number of simple processing elements. A typical neural network consists of a number of simple processing units interconnected to form a complex network. Layers of such units are arranged so that data is entered at the input layer and passes through either one or several intermediate layers before reaching the output layer. In supervised learning the network is trained by operating on the difference between the actual output and the desired output of the network, the prediction error, to change the connection strengths between the nodes. By iterating the weights are modified until the output error reaches an acceptable level. This process is called machine learning because the network adjusts the weights so that the output pattern is reproduced. Neural networks have been extensively studied and there are many excellent textbooks devoted to this topic in general, and more focussed textbooks which emphasise control and systems applications. There are two main problem types that can be studied using neural networks: static problems, and dynamic problems. Static problems include pattern recognition, classification, and approximation. Dynamic problems involve lagged variables and are more appropriate for system identification and related applications. Depending on the architecture of the network the training problem can be either nonlinear-in-the-parameters which involves optimisation or linear-in-the-parameters which can be solved using classical approaches. The training algorithms can be categorised into supervised, unsupervised, or reinforcement learning. Neural networks have excellent approximation properties but these are usually based on standard function approximation results using for example the Weierstrass Theorem that applies equally well to polynomials, rational functions, and other well-known models.  Neural networks have been applied extensively to system identification problems which involve nonlinear and dynamic relationships. However, classical neural networks are purely gross static approximating machines. There is no dynamics within the network. Hence when fitting dynamic models all the dynamics arise by allocating lagged inputs and outputs to the input layer of the network. The training procedure then produces the best static approximation that relates the lagged variables assigned to the input nodes to the output. There are more complex network architectures, including recurrent networks, that produce dynamics by introducing increasing orders of lagged variables to the input nodes. But in these cases it is very easy to over specify the lags and this can lead to over fitting and poor generalisation properties.  Neural networks have several advantages; they are conceptually simple, easy to train and to use, have excellent approximation properties, the concept of local and parallel processing is important and this provides integrity and fault tolerant behaviour. The biggest criticism of the classical neural network models is that the models produced are completely opaque and usually cannot be written down or analysed. It is therefore very difficult to know what is causing what, to analyse the model, or to compute dynamic characteristics from the model. Some of these points will not be relevant to all applications but they are for dynamic modelling.",
            "score": 92.64100646972656
        },
        {
            "docid": "1363296_2",
            "document": "Modelling biological systems . Modelling biological systems is a significant task of systems biology and mathematical biology. Computational systems biology aims to develop and use efficient algorithms, data structures, visualization and communication tools with the goal of computer modelling of biological systems. It involves the use of computer simulations of biological systems, including cellular subsystems (such as the networks of metabolites and enzymes which comprise metabolism, signal transduction pathways and gene regulatory networks), to both analyze and visualize the complex connections of these cellular processes.",
            "score": 91.7872543334961
        },
        {
            "docid": "1493799_5",
            "document": "Biocomplexity Institute of Virginia Tech . The Advanced Computing and Informatics Laboratories (ACIL) is dedicated to \"Policy Informatics\", including the Network Dynamics and Simulation Science Laboratory (NDSSL). It pursues research and development in interaction-based modeling, simulation, and associated analysis, experimental design, and decision support tools for understanding large biological, information, social, and technological systems.It includes the Comprehensive National Incident Management System project for developing a system to provide the United States military with detailed operational information about the populations being affected by a possible crisis. It also includes the project, \u201cModeling Disease Dynamics on Large, Detailed, Co-Evolving Networks,\u201d which supports work to develop high-performance computer models for the study of very large networks, .",
            "score": 91.37403869628906
        },
        {
            "docid": "53129859_9",
            "document": "Live single-cell imaging . In the final stage of a live single-cell imaging study modelling and analysis of time series data extracted from tracked cells is performed. A large overlap between analysis of single cell live data, and modelling of biological systems using ordinary differential equations exists. Results from this key data analysis step will drive further experimentation, for example by perturbing aspects of the system being studied and then comparing signalling dynamics with those of the control population.",
            "score": 91.28616333007812
        },
        {
            "docid": "1872854_31",
            "document": "Biochemical cascade . In the post-genomic age, high-throughput sequencing and gene/protein profiling techniques have transformed biological research by enabling comprehensive monitoring of a biological system, yielding a list of differentially expressed genes or proteins, which is useful in identifying genes that may have roles in a given phenomenon or phenotype. With DNA microarrays and genome-wide gene engineering, it is possible to screen global gene expression profiles to contribute a wealth of genomic data to the public domain. With RNA interference, it is possible to distill the inferences contained in the experimental literature and primary databases into knowledge bases that consist of annotated representations of biological pathways. In this case, individual genes and proteins are known to be involved in biological processes, components, or structures, as well as how and where gene products interact with each other. Pathway-oriented approaches for analyzing microarray data, by grouping long lists of individual genes, proteins, and/or other biological molecules according to the pathways they are involved in into smaller sets of related genes or proteins, which reduces the complexity, have proven useful for connecting genomic data to specific biological processes and systems. Identifying active pathways that differ between two conditions can have more explanatory power than a simple list of different genes or proteins. In addition, a large number of pathway analytic methods exploit pathway knowledge in public repositories such as Gene Ontology (GO) or Kyoto Encyclopedia of Genes and Genomes (KEGG), rather than inferring pathways from molecular measurements. Furthermore, different research focuses have given the word \"pathway\" different meanings. For example, 'pathway' can denote a metabolic pathway involving a sequence of enzyme-catalyzed reactions of small molecules, or a signaling pathway involving a set of protein phosphorylation reactions and gene regulation events. Therefore, the term \"pathway analysis\" has a very broad application. For instance, it can refer to the analysis physical interaction networks (e.g., protein\u2013protein interactions), kinetic simulation of pathways, and steady-state pathway analysis (e.g., flux-balance analysis), as well as its usage in the inference of pathways from expression and sequence data. Several functional enrichment analysis tools and algorithms have been developed to enhance data interpretation. The existing knowledge base\u2013driven pathway analysis methods in each generation have been summarized in recent literature.",
            "score": 90.23628234863281
        },
        {
            "docid": "4101904_63",
            "document": "Flux balance analysis . Unlike dynamic metabolic simulation, FBA assumes that the internal concentration of metabolites within a system stays constant over time and thus is unable to provide anything other than steady-state solutions. It is unlikely that FBA could, for example, simulate the functioning of a nerve cell. Since the internal concentration of metabolites is not considered within a model, it is possible that an FBA solution could contain metabolites at a concentration too high to be biologically acceptable. This is a problem that dynamic metabolic simulations would probably avoid. One advantage of the simplicity of FBA over dynamic simulations is that they are far less computationally expensive, allowing the simulation of large numbers of perturbations to the network. A second advantage is that the reconstructed model can be substantially simpler by avoiding the need to consider enzyme rates and the effect of complex interactions on enzyme kinetics.",
            "score": 89.97382354736328
        },
        {
            "docid": "2515807_10",
            "document": "Structural stability . Structural stability of the system provides a justification for applying the qualitative theory of dynamical systems to analysis of concrete physical systems. The idea of such qualitative analysis goes back to the work of Henri Poincar\u00e9 on the three-body problem in celestial mechanics. Around the same time, Aleksandr Lyapunov rigorously investigated stability of small perturbations of an individual system. In practice, the evolution law of the system (i.e. the differential equations) is never known exactly, due to the presence of various small interactions. It is, therefore, crucial to know that basic features of the dynamics are the same for any small perturbation of the \"model\" system, whose evolution is governed by a certain known physical law. Qualitative analysis was further developed by George Birkhoff in the 1920s, but was first formalized with introduction of the concept of rough system by Andronov and Pontryagin in 1937. This was immediately applied to analysis of physical systems with oscillations by Andronov, Witt, and Khaikin. The term \"structural stability\" is due to Solomon Lefschetz, who oversaw translation of their monograph into English. Ideas of structural stability were taken up by Stephen Smale and his school in the 1960s in the context of hyperbolic dynamics. Earlier, Marston Morse and Hassler Whitney initiated and Ren\u00e9 Thom developed a parallel theory of stability for differentiable maps, which forms a key part of singularity theory. Thom envisaged applications of this theory to biological systems. Both Smale and Thom worked in direct contact with Maur\u00edcio Peixoto, who developed Peixoto's theorem in the late 1950s.",
            "score": 89.9126205444336
        },
        {
            "docid": "661696_3",
            "document": "Business Dynamics . The book introduces systems dynamics modeling for the analysis of policy and strategy, with an emphasis on business and public policy applications. System dynamics is both a conceptual tool and a powerful modeling method. This allows the building of computer simulations of complex systems. These simulations can then be used to test the effectiveness of different policies on business outcomes.",
            "score": 89.73771667480469
        },
        {
            "docid": "24044102_6",
            "document": "Cellular model . The eukaryotic cell cycle is very complex and is one of the most studied topics, since its misregulation leads to cancers. It is possibly a good example of a mathematical model as it deals with simple calculus but gives valid results. Two research groups have produced several models of the cell cycle simulating several organisms. They have recently produced a generic eukaryotic cell cycle model which can represent a particular eukaryote depending on the values of the parameters, demonstrating that the idiosyncrasies of the individual cell cycles are due to different protein concentrations and affinities, while the underlying mechanisms are conserved (Csikasz-Nagy et al., 2006). By means of a system of ordinary differential equations these models show the change in time (dynamical system) of the protein inside a single typical cell; this type of model is called a deterministic process (whereas a model describing a statistical distribution of protein concentrations in a population of cells is called a stochastic process). To obtain these equations an iterative series of steps must be done: first the several models and observations are combined to form a consensus diagram and the appropriate kinetic laws are chosen to write the differential equations, such as rate kinetics for stoichiometric reactions, Michaelis-Menten kinetics for enzyme substrate reactions and Goldbeter\u2013Koshland kinetics for ultrasensitive transcription factors, afterwards the parameters of the equations (rate constants, enzyme efficiency coefficients and Michaelis constants) must be fitted to match observations; when they cannot be fitted the kinetic equation is revised and when that is not possible the wiring diagram is modified. The parameters are fitted and validated using observations of both wild type and mutants, such as protein half-life and cell size. In order to fit the parameters the differential equations need to be studied. This can be done either by simulation or by analysis.  In a simulation, given a starting vector (list of the values of the variables), the progression of the system is calculated by solving the equations at each time-frame in small increments. In analysis, the properties of the equations are used to investigate the behavior of the system depending of the values of the parameters and variables. A system of differential equations can be represented as a vector field, where each vector described the change (in concentration of two or more protein) determining where and how fast the trajectory (simulation) is heading. Vector fields can have several special points: a stable point, called a sink, that attracts in all directions (forcing the concentrations to be at a certain value), an unstable point, either a source or a saddle point which repels (forcing the concentrations to change away from a certain value), and a limit cycle, a closed trajectory towards which several trajectories spiral towards (making the concentrations oscillate). A better representation which can handle the large number of variables and parameters is called a bifurcation diagram (bifurcation theory): the presence of these special steady-state points at certain values of a parameter (e.g. mass) is represented by a point and once the parameter passes a certain value, a qualitative change occurs, called a bifurcation, in which the nature of the space changes, with profound consequences for the protein concentrations: the cell cycle has phases (partially corresponding to G1 and G2) in which mass, via a stable point, controls cyclin levels, and phases (S and M phases) in which the concentrations change independently, but once the phase has changed at a bifurcation event (cell cycle checkpoint), the system cannot go back to the previous levels since at the current mass the vector field is profoundly different and the mass cannot be reversed back through the bifurcation event, making a checkpoint irreversible. In particular the S and M checkpoints are regulated by means of special bifurcations called a Hopf bifurcation and an infinite period bifurcation. Cell Collective is a modeling software that enables one to house dynamical biological data, build computational models, stimulate, break and recreate models. The development is led by Tomas Helikar, a researcher within the field of computational biology. It is designed for biologists, students learning about computational biology, teachers focused on teaching life sciences, and researchers within the field of life science. The complexities of math and computer science are built into the backend and one can learn about the methods used for modeling biological species, but complex math equations, algorithms, programming are not required and hence won't impede model building.",
            "score": 88.93962097167969
        },
        {
            "docid": "1100516_4",
            "document": "Model predictive control . The models used in MPC are generally intended to represent the behavior of complex dynamical systems. The additional complexity of the MPC control algorithm is not generally needed to provide adequate control of simple systems, which are often controlled well by generic PID controllers. Common dynamic characteristics that are difficult for PID controllers include large time delays and high-order dynamics.",
            "score": 88.53296661376953
        },
        {
            "docid": "2007748_10",
            "document": "Structural equation modeling . Advances in computers made it simple for novices to apply structural equation methods in the computer-intensive analysis of large datasets in complex, unstructured problems. The most popular solution techniques fall into three classes of algorithms: (1) ordinary least squares algorithms applied independently to each path, such as applied in the so-called PLS path analysis packages which estimate with OLS; (2) covariance analysis algorithms evolving from seminal work by Wold and his student Karl J\u00f6reskog implemented in LISREL, AMOS, and EQS; and (3) simultaneous equations regression algorithms developed at the Cowles Commission by Tjalling Koopmans.",
            "score": 88.49333953857422
        },
        {
            "docid": "37438_15",
            "document": "Complex system . Of particular interest to complex systems are nonlinear dynamical systems, which are systems of differential equations that have one or more nonlinear terms. Some nonlinear dynamical systems, such as the Lorenz system, can produce a mathematical phenomenon known as chaos. Chaos as it applies to complex systems refers to the sensitive dependence on initial conditions, or \"butterfly effect,\" that a complex system can exhibit. In such a system, small changes to initial conditions can lead to dramatically different outcomes. Chaotic behavior can therefore be extremely hard to model numerically, because small rounding errors at an intermediate stage of computation can cause the model to generate completely inaccurate output. Furthermore, if a complex system returns to a state similar to one it held previously, it may behave completely differently in response to exactly the same stimuli, so chaos also poses challenges for extrapolating from past experience.",
            "score": 88.28531646728516
        },
        {
            "docid": "10571004_4",
            "document": "Biological network inference . There is great interest in network medicine for the modelling biological systems. This article focuses on a necessary prerequisite to dynamic modeling of a network: inference of the topology, that is, prediction of the \"wiring diagram\" of the network. More specifically, we focus here on inference of biological network structure using the growing sets of high-throughput expression data for genes, proteins, and metabolites. Briefly, methods using high-throughput data for inference of regulatory networks rely on searching for patterns of partial correlation or conditional probabilities that indicate causal influence. Such patterns of partial correlations found in the high-throughput data, possibly combined with other supplemental data on the genes or proteins in the proposed networks, or combined with other information on the organism, form the basis upon which such algorithms work. Such algorithms can be of use in inferring the topology of any network where the change in state of one node can affect the state of other nodes.",
            "score": 88.28099822998047
        },
        {
            "docid": "1577103_3",
            "document": "Network traffic simulation . Telecommunications systems are complex real-world systems, containing many different components which interact, in complex interrelationships. The analysis of such systems can become extremely difficult: modelling techniques tend to analyse each component rather than the relationships between components. Simulation is an approach which can be used to model large, complex stochastic systems for forecasting or performance measurement purposes. It is the most common quantitative modelling technique used.",
            "score": 88.05164337158203
        },
        {
            "docid": "8718004_5",
            "document": "Alexander Kuzemsky . In series of his works the development of methods of quantum statistical mechanics was considered in light of their applications to quantum solid-state theory. He discussed fundamental problems of the physics of magnetic materials and the methods of the quantum theory of magnetism, including the method of two-time temperature Green's functions which is widely used in various physical problems of many-particle systems with interaction. Quantum cooperative effects and quasi-particle dynamics in the basic microscopic models of quantum theory of magnetism: the Heisenberg model, the Hubbard model, the Anderson Model, and the spin-fermion model were considered in the framework of novel self-consistent-field approximation. A comparative analysis of these models was presented; in particular, their applicability for description of complex magnetic materials was compared. Kuzemsky formulated notable Irreducible Green Functions Method (IGFM)  for the systems with complex spectrum and strong interaction. The Green-function technique, termed the irreducible Green function method is a certain reformulation of the equation-of motion method for double-time temperature dependent Green functions. This advanced and notable method was developed to overcome some ambiguities in terminating the hierarchy of the equations of motion of double-time Green functions and to give a workable technique to systematic way of decoupling. The approach provides a practical method for description of the many-body quasi-particle dynamics of correlated systems on a lattice with complex spectra.",
            "score": 87.88239288330078
        },
        {
            "docid": "149353_4",
            "document": "Computational biology . Computational Biology, which includes many aspects of bioinformatics, is the science of using biological data to develop algorithms or models to understand biological systems and relationships.  Until recently, biologists did not have access to very large amounts of data. This data has now become commonplace, particularly in molecular biology and genomics. Researchers were able to develop analytical methods for interpreting biological information, but were unable to share them quickly among colleagues.  Bioinformatics began to develop in the early 1970s. It was considered the science of analyzing informatics processes of various biological systems. At this time, research in artificial intelligence was using network models of the human brain in order to generate new algorithms. This use of biological data to develop other fields pushed biological researchers to revisit the idea of using computers to evaluate and compare large data sets. By 1982, information was being shared amongst researchers through the use of punch cards. The amount of data being shared began to grow exponentially by the end of the 1980s. This required the development of new computational methods in order to quickly analyze and interpret relevant information. Since the late 1990s, computational biology has become an important part of developing emerging technologies for the field of biology. The terms computational biology and evolutionary computation have a similar name, but are not to be confused. Unlike computational biology, evolutionary computation is not concerned with modeling and analyzing biological data. It instead creates algorithms based on the ideas of evolution across species. Sometimes referred to as genetic algorithms, the research of this field can be applied to computational biology. While evolutionary computation is not inherently a part of computational biology, Computational evolutionary biology is a subfield of it.",
            "score": 87.57884979248047
        },
        {
            "docid": "190837_3",
            "document": "Evolutionary algorithm . Evolutionary algorithms often perform well approximating solutions to all types of problems because they ideally do not make any assumption about the underlying fitness landscape. Techniques from evolutionary algorithms applied to the modeling of biological evolution are generally limited to explorations of microevolutionary processes and planning models based upon cellular processes. In most real applications of EAs, computational complexity is a prohibiting factor. In fact, this computational complexity is due to fitness function evaluation. Fitness approximation is one of the solutions to overcome this difficulty. However, seemingly simple EA can solve often complex problems; therefore, there may be no direct link between algorithm complexity and problem complexity.",
            "score": 87.31238555908203
        },
        {
            "docid": "7766542_9",
            "document": "Microarray analysis techniques . Commercial systems for gene network analysis such as Ingenuity and Pathway studio create visual representations of differentially expressed genes based on current scientific literature. Non-commercial tools such as FunRich, GenMAPP and Moksiskaan also aid in organizing and visualizing gene network data procured from one or several microarray experiments. A wide variety of microarray analysis tools are available through Bioconductor written in the R programming language. The frequently cited SAM module and other microarray tools are available through Stanford University. Another set is available from Harvard and MIT. Specialized software tools for statistical analysis to determine the extent of over- or under-expression of a gene in a microarray experiment relative to a reference state have also been developed to aid in identifying genes or gene sets associated with particular phenotypes. One such method of analysis, known as Gene Set Enrichment Analysis (GSEA), uses a Kolmogorov-Smirnov-style statistic to identify groups of genes that are regulated together. This third-party statistics package offers the user information on the genes or gene sets of interest, including links to entries in databases such as NCBI's GenBank and curated databases such as Biocarta and Gene Ontology. Protein complex enrichment analysis tool (COMPLEAT) provides similar enrichment analysis at the level of protein complexes. The tool can identify the dynamic protein complex regulation under different condition or time points. Related system, PAINT and SCOPE performs a statistical analysis on gene promoter regions, identifying over and under representation of previously identified transcription factor response elements. Another statistical analysis tool is Rank Sum Statistics for Gene Set Collections (RssGsc), which uses rank sum probability distribution functions to find gene sets that explain experimental data. A further approach is contextual meta-analysis, i.e. finding out how a gene cluster responds to a variety of experimental contexts. Genevestigator is a public tool to perform contextual meta-analysis across contexts such as anatomical parts, stages of development, and response to diseases, chemicals, stresses, and neoplasms.",
            "score": 87.18452453613281
        },
        {
            "docid": "44012968_5",
            "document": "B. S. Daya Sagar . Sagar\u2019s research contributions span both basic and applied fields involves fusion of computer simulations and modeling techniques in order to develop cogent models in discrete space to gain an understanding of dynamical behavior of complex terrestrial systems, such as water bodies, river networks, mountain objects, folds, dunes, landscapes, and rock porous media. He developed specific technologies and spatial algorithms for using data acquired at multiple spatial and temporal scalesfor developing cogent domain-specific models. His work employs concepts stemmed out of recent theories\u2014such as mathematical morphology, fractals, chaos, and morphometrics\u2014to develop spatial algorithms for (i) pattern retrieval, (ii) pattern analysis, (iii) simulation and modeling, and (iv) reasoning and visualization of the spatial and temporal phenomena of terrestrial and GISci relevance. His studies show that there exist several open challenges that need to be addressed by mathematical geosciences community. One of such challenges is to understand the complexity in spatiotemporal behaviors of several terrestrial phenomena and processes via construction of process-specific \"attractors\" that may range from \"simple\" to \"strange\".",
            "score": 86.90174102783203
        },
        {
            "docid": "984629_8",
            "document": "Social complexity . Complex social network analysis is used to study the dynamics of large, complex social networks. Dynamic network analysis brings together traditional social network analysis, link analysis and multi-agent systems within network science and network theory. Through the use of key concepts and methods in social network analysis, agent-based modeling, theoretical physics, and modern mathematics (particularly graph theory and fractal geometry), this method of inquiry brought insights into the dynamics and structure of social systems. New computational methods of localized social network analysis are coming out of the work of Duncan Watts, Albert-L\u00e1szl\u00f3 Barab\u00e1si, Nicholas A. Christakis, Kathleen Carley and others.",
            "score": 86.67879486083984
        }
    ]
}