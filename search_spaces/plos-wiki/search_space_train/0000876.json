{
    "q": [
        {
            "docid": "41185140_2",
            "document": "Phenotypic integration . Phenotypic Integration is the term used to describe when multiple functionally-related traits are correlated with each other. Complex phenotypes often require multiple traits working together in order to function properly. Phenotypic integration is significant because it provides an explanation as to how phenotypes are sustained by relationships between traits. Every aspect of an organism is created so that the role it plays overall is performed harmoniously with all its other parts. Every organism's phenotype is integrated, organized, and a functional whole. Integration is also associated with functional modules. Modules are complex character units that are tightly associated, such as a flower. It is hypothesized that organisms with high correlations between traits in a module have the most efficient functions. The fitness of a particular value for one phenotypic trait frequently depends on the value of the other phenotypic traits, making it important for those traits evolve together. One trait can have a direct effect on fitness, and it has been shown that the correlations among traits can also change fitness, causing these correlations to be adaptive, rather than solely genetic. Integration can be involved in multiple aspects of life, not just at the genetic level, but during development, or simply at a functional level. Integration can be caused by genetic, developmental, environmental, or physiological relationships among characters. Environmental conditions can alter or cause integration, i.e. they may be plastic. Correlational selection, a form of natural selection can also produce integration. At the genetic level, integration can be caused by pleiotropy, close linkage, or linkage disequilibrium among unlinked genes. At the developmental level it can be due to cell-cell signaling such as in the development of the ectopic eyes in Drosophila. It is believed that the patterns of genetic covariance helped distinguish certain species. It can create variation among certain phenotypes, and can facilitate efficiency. This is significant because integration may play a huge role in phenotypic evolution. Phenotypic integration and its evolution can not only create large amounts of variety among phenotypes which can cause variation among species. For example, the color patterns on Garter snakes range widely and are caused by the covariance among multiple phenotypes.",
            "score": 65.98736155033112
        },
        {
            "docid": "24543_16",
            "document": "Phenotype . Although a phenotype is the ensemble of observable characteristics displayed by an organism, the word \"phenome\" is sometimes used to refer to a collection of traits, while the simultaneous study of such a collection is referred to as \"phenomics\". Phenomics is an important field of study because it can be used to figure out which genomic variants affect phenotypes which then can be used to explain things like health, disease, and evolutionary fitness. Phenomics forms a large part of the Human Genome Project",
            "score": 36.128604888916016
        },
        {
            "docid": "3040270_22",
            "document": "Phenotypic plasticity . Given the profound ecological importance of temperature and its predictable variability over large spatial and temporal scales, adaptation to thermal variation has been hypothesized to be a key mechanism dictating the capacity of organisms for phenotypic plasticity. The magnitude of thermal variation is thought to be directly proportional to plastic capacity, such that species that have evolved in the warm, constant climate of the tropics have a lower capacity for plasticity compared to those living in variable temperate habitats. Termed the \"climatic variability hypothesis\", this idea has been supported by several studies of plastic capacity across latitude in both plants and animals. However, recent studies of \"Drosophila\" species have failed to detect a clear pattern of plasticity over latitudinal gradients, suggesting this hypothesis may not hold true across all taxa or for all traits. Some researchers propose that direct measures of environmental variability, using factors such as precipitation, are better predictors of phenotypic plasticity than latitude alone.",
            "score": 41.69662165641785
        },
        {
            "docid": "34132734_14",
            "document": "Family-based QTL mapping . Linkage and association analysis are primary tools for gene discovery, localization and functional analysis. While conceptual underpinning of these approaches have been long known, advances in recent decades in molecular genetics, development in efficient algorithms, and computing power have enabled the large scale application of these methods. While linkage studies seek to identify loci cosegregate with the trait within families, association studies seek to identify particular variants that are associated with the phenotype at the population level. These are complementary methods that, together, provide means to probe the genome and describe etiology of complex traits. In linkage studies, we seek to identify the loci that cosegregate with a specific genomic region, tagged by polymorphic markers, within families. In contrast, in association studies, we seek a correlation between a specific genetic variation and trait variation in sample of individuals, implicating a causal role of the variant.",
            "score": 34.2586612701416
        },
        {
            "docid": "9630_45",
            "document": "Ecology . Ecology and evolutionary biology are considered sister disciplines of the life sciences. Natural selection, life history, development, adaptation, populations, and inheritance are examples of concepts that thread equally into ecological and evolutionary theory. Morphological, behavioural, and genetic traits, for example, can be mapped onto evolutionary trees to study the historical development of a species in relation to their functions and roles in different ecological circumstances. In this framework, the analytical tools of ecologists and evolutionists overlap as they organize, classify, and investigate life through common systematic principals, such as phylogenetics or the Linnaean system of taxonomy. The two disciplines often appear together, such as in the title of the journal \"Trends in Ecology and Evolution\". There is no sharp boundary separating ecology from evolution, and they differ more in their areas of applied focus. Both disciplines discover and explain emergent and unique properties and processes operating across different spatial or temporal scales of organization. While the boundary between ecology and evolution is not always clear, ecologists study the abiotic and biotic factors that influence evolutionary processes, and evolution can be rapid, occurring on ecological timescales as short as one generation.",
            "score": 40.57258081436157
        },
        {
            "docid": "33373383_3",
            "document": "GWASdb . Thousands of genetic variants (GVs) associated with human traits and diseases have been identified by Genome-Wide Association Studies (GWAS). The advent of high throughput technologies, such as next-generation sequencing and very high-density microarrays, enables us to capture genome-wide variation on a much larger scale.<ref name='10.1073/pnas.0903103106'> </ref> With increasing sample sizes, GWAS studies based on these technologies will produce more information at higher resolutions. We will be able to detect many traits/diseases associated GVs, such as Single Nucleotide Polymorphisms (SNPs), Copy Number Variations (CNVs), and insertions and deletions (Indels).<ref name='10.1038/nature09534'> </ref>",
            "score": 53.881776571273804
        },
        {
            "docid": "4062934_2",
            "document": "Phenomics . Phenomics is an area of biology concerned with the measurement of phenomes (a phenome is the set of physical and biochemical traits belonging to a given organism) as they change in response to genetic mutation and environmental influences. It is used in functional genomics, pharmaceutical research, metabolic engineering and increasingly in phylogenetics.",
            "score": 55.061885356903076
        },
        {
            "docid": "49313006_5",
            "document": "Brian J. Enquist . (1) \"Scaling and Functional Biology\" \u2013 Understanding the origin and diversity of organismal form, function, and diversity by developing general models for the origin of biological scaling laws. This research shows how general scaling laws and allometry, underlie organismal form, function, and diversity; physiological ecology and can be used to 'scale up' biological processes from genes to cells to ecosystems. (2) \"Macroecology\" \u2013 assessing the large scale biogeographic and evolutionary drivers of biological diversity and developing novel theoretical and informatics approaches that build from scaling principles and functional biology;  (3) \"Forecasting and Visualizing the Fate of Biological Diversity and Ecosystem Functioning.\" This work is building novel approaches to complex ecological problems \u2013 utilizing integrative computation, big data, statistical, and visualisation tools to visualize and analyze biological data and to assess how climate change will influence the distribution of diversity and functioning of forests and ecosystems.  His lab's research utilizes differing approaches including: developing theory and informatics infrastructure, field work, big datasets, scaling, empirically measuring numerous attributes of organismal form and function, utilizing physiological and trait-based techniques, and assessing macroecological and large-scale patterns. His collaborative group often works in contrasting environments including tropical forests, on elevation gradients, and in high alpine ecosystems.",
            "score": 60.04338049888611
        },
        {
            "docid": "34142465_10",
            "document": "Linkage based QTL mapping . Linkage and association analysis are primary tool for gene discovery, localization and functional analysis. While conceptual underpinning of these approaches have been long known, advances in recent decades in molecular genetics, development in efficient algorithms, and computing power have enabled the large scale application of these methods. While linkage studies seek to identify loci that cosegregate with the trait within families, association studies seek to identify particular variants that are associated with the phenotype at the population level. These are complementary methods that, together, provide means to probe the genome and describe etiology of complex human traits. In linkage studies, we seek to identify the loci that cosegregate with a specific genomic region, tagged by polymorphic markers, within families. In contrast, in association studies, we seek a correlation between a specific genetic variation and trait variation in sample of individuals, implicating a causal role of the variant. Linkage tests are powerful and specific for gene discovery, the localization of locus can be achieved only to a certain level of precision \u2013 on order of megabases \u2013 that potentially represents a region that potentially include hundreds of genes.",
            "score": 33.1917827129364
        },
        {
            "docid": "155624_53",
            "document": "Heritability . A wide variety approaches using linear mixed models have been reported in literature. Via these methods, phenomic variance may be used in tandem with either environmental or genetic variance to estimate the other, unknown parameter, and estimate heritability through variance parameters obtained from these models. Environmental variance can be explicitly modeled by studying individuals across a broad range of environments, although inference of genetic variance from phenomic and environmental variance may lead to underestimation of heritability due to the challenge of capturing the full range of environmental influence affecting a trait. Other methods for calculating heritability utilize data from genome-wide association studies to estimate the influence on a trait by genetic factors, which is reflected by the rate and influence of putatively associated genetic loci (usually single-nucleotide polymorphisms) on the trait. This can lead to underestimation of heritability, however. This discrepancy is referred to as \"missing heritability\" and reflects the challenge of accurately modeling both genetic and environmental variance in heritability models.",
            "score": 45.77069318294525
        },
        {
            "docid": "1075217_3",
            "document": "Phenome . Just as the genome and proteome signify all of an organism's genes and proteins, the phenome represents the sum total of its phenotypic traits. Examples of human phenotypic traits are skin color, eye color, body height, or specific personality characteristics. Although any phenotype of any organism has a basis in its genotype, phenotypic expression may be influenced by environmental influences, mutation, and genetic variation such as single nucleotide polymorphisms (SNPs), or a combination of these factors.",
            "score": 33.927111864089966
        },
        {
            "docid": "24219329_39",
            "document": "Neurogenomics . Whole genome sequencing (WGS) and whole exome sequencing (WES) has been used in Genome Wide Association Studies (GWAS) to characterize genetic variants associated with neurogenomic disorders. However, the impact of these variants cannot always be verified because of the non-Mendelian inheritance patterns observed in several of these disorders. Another prohibitive feature in network analysis is the lack of large-scale datasets for many psychiatric (neurogenomic) diseases. Since several diseases with neurogenomic underpinnings tend to have a polygenic basis, several nonspecific, rare, and partially penetrant \"de novo\" mutations in different patients can contribute to the same observed range of phenotypes, as is the case with Autism Spectrum Disorder and schizophrenia. Extensive research in alcohol dependence (ALC) has also highlighted the need for high-quality genomic profiling of large sample sets when studying polygenic, spectrum disorders.",
            "score": 37.51290035247803
        },
        {
            "docid": "1181008_10",
            "document": "Computational science . Exciting new developments in biotechnology are now revolutionizing biology and biomedical research. Examples of these techniques are high-throughput sequencing, high-throughput quantitative PCR, intra-cellular imaging, in-situ hybridization of gene expression, three-dimensional imaging techniques like Light Sheet Fluorescence Microscopy and Optical Projection, (micro)-Computer Tomography. Given the massive amounts of complicated data that is generated by these techniques, their meaningful interpretation, and even their storage, form major challenges calling for new approaches. Going beyond current bioinformatics approaches, computational biology needs to develop new methods to discover meaningful patterns in these large data sets. Model-based reconstruction of gene networks can be used to organize the gene expression data in systematic way and to guide future data collection. A major challenge here is to understand how gene regulation is controlling fundamental biological processes like biomineralisation and embryogenesis. The sub-processes like gene regulation, organic molecules interacting with the mineral deposition process, cellular processes, physiology and other processes at the tissue and environmental levels are linked. Rather than being directed by a central control mechanism, biomineralisation and embryogenesis can be viewed as an emergent behavior resulting from a complex system in which several sub-processes on very different temporal and spatial scales (ranging from nanometer and nanoseconds to meters and years) are connected into a multi-scale system. One of the few available options to understand such systems is by developing a multi-scale model of the system.",
            "score": 75.13754272460938
        },
        {
            "docid": "19374_11",
            "document": "Model organism . Models are those organisms with a wealth of biological data that make them attractive to study as examples for other species and/or natural phenomena that are more difficult to study directly. Continual research on these organisms focus on a wide variety of experimental techniques and goals from many different levels of biology\u2014from ecology, behavior and biomechanics, down to the tiny functional scale of individual tissues, organelles and proteins. Inquiries about the DNA of organisms are classed as genetic models (with short generation times, such as the fruitfly and nematode worm), experimental models, and genomic parsimony models, investigating pivotal position in the evolutionary tree. Historically, model organisms include a handful of species with extensive genomic research data, such as the NIH model organisms.",
            "score": 35.371747732162476
        },
        {
            "docid": "34358661_6",
            "document": "Fisher's geometric model . The analogy between the microscope and an evolving organism can be formalized by representing the phenotype of an organism as a point in a high-dimensional data space, where the dimensions of that space correspond to the traits of the organism. The more independent dimensions of variation the phenotype has, the more difficult is improvement resulting from random changes. If there are many different ways to change a phenotype it becomes very unlikely that a random change affects the right combination of traits in the right way to improve fitness. Fisher noted that the smaller the effect, the higher the chance that a change is beneficial. At one extreme, changes with infinitesimally small effect have a 50% chance of improving fitness. This argument led to the widely held position that evolution proceeds by small mutations.",
            "score": 54.32498621940613
        },
        {
            "docid": "3878_20",
            "document": "Biostatistics . For quantitative data, collection are made by specific measures, according to study aim. Usually instruments are necessary to increase accuracy measures. In agriculture and biology studies, yield data and its components can be obtained by metric measures. However, pest and disease injuries in plats are obtained by observation, considering score scales for levels of damage. Especially, in genetic studies, modern methods for data collection in field and laboratory should be considered, as high-throughput platforms for phenotyping and genotyping. These tools allow bigger experiments, while turn possible evaluate many plots in lower time than a human-based only method for data collection. Finally, all data collected of interest must be stored in an organized data frame for further analysis. It is also important consider the evaluator's ability. Furthermore, plan and organization avoid bias in data analysis.",
            "score": 57.10131573677063
        },
        {
            "docid": "3408308_4",
            "document": "Metabolic network modelling . A metabolic reconstruction provides a highly mathematical, structured platform on which to understand the systems biology of metabolic pathways within an organism. The integration of biochemical metabolic pathways with rapidly available, unannotated genome sequences has developed what are called genome-scale metabolic models. Simply put, these models correspond metabolic genes with metabolic pathways. In general, the more information about physiology, biochemistry and genetics is available for the target organism, the better the predictive capacity of the reconstructed models. Mechanically speaking, the process of reconstructing prokaryotic and eukaryotic metabolic networks is essentially the same. Having said this, eukaryote reconstructions are typically more challenging because of the size of genomes, coverage of knowledge, and the multitude of cellular compartments. The first genome-scale metabolic model was generated in 1995 for \"Haemophilus influenzae\". The first multicellular organism, \"C. elegans\", was reconstructed in 1998. Since then, many reconstructions have been formed. For a list of reconstructions that have been converted into a model and experimentally validated, see http://sbrg.ucsd.edu/InSilicoOrganisms/OtherOrganisms.",
            "score": 29.897689938545227
        },
        {
            "docid": "49241355_4",
            "document": "Gastruloid . Following this, and with modifications to the original Marikawa protocol, the Martinez Arias laboratory in the Department of Genetics at the University of Cambridge demonstrated how aggregates of mouse ESCs were able to generate structures that exhibited collective behaviours with striking similarity to those during early development such as symmetry-breaking (in terms of gene expression), axial elongation and germ-layer specification. To quote from the original paper: \"\"Altogether, these observations further emphasize the similarity between the processes that we have uncovered here and the events in the embryo. The movements are related to those of cells in gastrulating embryos and for this reason we term these aggregates \u2018gastruloids\u2019\".\" As noted by the authors of this protocol, a crucial difference between this culture method and previous work with mouse EBs was the use of small numbers of cells which may be important for generating the correct length scale for patterning.",
            "score": 54.89677608013153
        },
        {
            "docid": "13772374_12",
            "document": "Renaissance Computing Institute . RENCI has a number of active research programs that are aimed at developing and deploying advanced computing and networking capabilities. Many of the resultant technologies are open source. For example, the open source ExoGENI (Exo-Global Environment for Network Innovation) is being developed as part of the NSF-funded GENI initiative. ExoGENI functions as a federated, cloud-based Networked Infrastructure-as-a-Service (NIaaS) platform for dynamic provisioning of networking, storage, and compute resources. ADAMANT (Adaptive Data-Aware Multi-domain Application Network Topologies), also funded by the NSF, builds upon ExoGENI. ADAMANT integrates the Pegasus and HT Condor workflow management systems with the ExoGENI NIaaS platform to orchestrate the execution of large-scale scientific workflows over distributed cloud or traditional high-performance computing resources. iRODS (integrated Rule-Oriented Data System) was developed by the Data Intensive Cyber Environments (DICE) Centers at UNC-CH and the University of California, San Diego and is currently maintained by RENCI. iRODS is an open source middleware technology designed to provide policy-based control over data access, movement, use, and archiving across geographical sites, disparate storage technologies, and multiple user groups, each with varying policies regarding data access and use. RADII (Resource Aware Data-centric collaborative Infrastructure; web citation) integrates GENI\u2019s ORCA (Open Resource Control Architecture) with iRODS to dynamically provision a distributed cloud-based infrastructure for multi-institutional, data-driven research collaborations. RADII accomplishes this through software designed to model research data and map data elements, computations, and storage onto the underlying physical infrastructure of iRODS. DataBridge aims to provide a multi-dimensional sociometric network system for sharing long-tail data collections. DataBridge is an open source collaboration tool that allows scientists to explore available data sets and their relevant algorithms and define semantic bridges to link to and access diverse data sets within the sociometric network.",
            "score": 62.98447275161743
        },
        {
            "docid": "29188721_67",
            "document": "RNA interference . Genome-scale RNAi research relies on high-throughput screening (HTS) technology. RNAi HTS technology allows genome-wide loss-of-function screening and is broadly used in the identification of genes associated with specific phenotypes. This technology has been hailed as the second genomics wave, following the first genomics wave of gene expression microarray and single nucleotide polymorphism discovery platforms. One major advantage of genome-scale RNAi screening is its ability to simultaneously interrogate thousands of genes. With the ability to generate a large amount of data per experiment, genome-scale RNAi screening has led to an explosion of data generation rates. Exploiting such large data sets is a fundamental challenge, requiring suitable statistics/bioinformatics methods. The basic process of cell-based RNAi screening includes the choice of an RNAi library, robust and stable cell types, transfection with RNAi agents, treatment/incubation, signal detection, analysis and identification of important genes or therapeutical targets.",
            "score": 58.99733066558838
        },
        {
            "docid": "22255650_2",
            "document": "ConcourseConnect . ConcourseConnect was developed by Concursive Corporation (known for their Open Source CRM ConcourseSuite) and is a platform for deploying large scale web-based communities. The platform integrates Customer Relationship Management software with a front-facing Social Networking application, providing data integration between two separate business scenarios and allowing both the end-user and administrator to better benefit from transparent social associations.",
            "score": 26.6161048412323
        },
        {
            "docid": "39982437_2",
            "document": "Geworkbench . geWorkbench (genomics Workbench) is an open-source software platform for integrated genomic data analysis. It is a desktop application written in the programming language Java. geWorkbench uses a component architecture. , there are more than 70 plug-ins available, providing for the visualization and analysis of gene expression, sequence, and structure data. geWorkbench is the Bioinformatics platform of MAGNet, the National Center for the Multi-scale Analysis of Genomic and Cellular Networks, one of the 8 National Centers for Biomedical Computing funded through the NIH Roadmap (NIH Common Fund). Many systems and structure biology tools developed by MAGNet investigators are available as geWorkbench plugins.",
            "score": 36.674391746520996
        },
        {
            "docid": "1075217_7",
            "document": "Phenome . Some usages of the term suggest that the phenome of a given organism is best understood as a kind of matrix of data representing physical manifestation of phenotype. For example, discussions led by A.Varki among those who had used the term up to 2003 suggested the following definition: \u201cThe body of information describing an organism's phenotypes, under the influences of genetic and environmental factors\u201d. Another team of researchers characterize \"the human phenome <nowiki>[as]</nowiki> a multidimensional search space with several neurobiological levels, spanning the proteome, cellular systems (e.g., signaling pathways), neural systems and cognitive and behavioural phenotypes.\"",
            "score": 46.025092363357544
        },
        {
            "docid": "4183_18",
            "document": "Botany . 20th century developments in plant biochemistry have been driven by modern techniques of organic chemical analysis, such as spectroscopy, chromatography and electrophoresis. With the rise of the related molecular-scale biological approaches of molecular biology, genomics, proteomics and metabolomics, the relationship between the plant genome and most aspects of the biochemistry, physiology, morphology and behaviour of plants can be subjected to detailed experimental analysis. The concept originally stated by Gottlieb Haberlandt in 1902 that all plant cells are totipotent and can be grown \"in vitro\" ultimately enabled the use of genetic engineering experimentally to knock out a gene or genes responsible for a specific trait, or to add genes such as GFP that report when a gene of interest is being expressed. These technologies enable the biotechnological use of whole plants or plant cell cultures grown in bioreactors to synthesise pesticides, antibiotics or other pharmaceuticals, as well as the practical application of genetically modified crops designed for traits such as improved yield.",
            "score": 47.31376934051514
        },
        {
            "docid": "14917968_3",
            "document": "Sediment Profile Imagery . Humans are strongly visually oriented. We like information in the form of pictures and are able to integrate many different kinds of data when they are presented in one or more images. It seems natural to seek a way of directly imaging the sediment-water interface in order to investigate animal-sediment interactions in the marine benthos. Rhoads and Cande (1971) took pictures of the sediment-water interface at high resolution (sub-millimetre) over small spatial scales (centimetres) in order to examine benthic patterns through time or over large spatial scales (kilometres) rapidly. Slicing into seabeds and taking pictures instead of physical cores, they analysed images of the vertical sediment profile in a technique that came to be known as SPI. This technique advanced in subsequent decades through a number of mechanical improvements and digital imaging and analysis technology. SPI is now a well-established approach accepted as standard practice in several parts of the world, though its wider adoption has been hampered partly because of equipment cost, deployment, and interpretation difficulties. It has also suffered some paradigm setbacks. The amount of information that a person can extract from imagery, in general, is not easily and repeatedly reduced to quantifiable and interpretable values (but see Pech et al. 2004; Tkachenko 2005). Sulston and Ferry (2002) wrote about this difficulty in relation to the study of the human genome. Electron microscope images of their model organism (\"Caenorhabditis elegans\") carried a lot of information but were ignored by many scientists because they were not readily quantified, yet that pictorial information ultimately resulted in a deep, and quantifiable, understanding of underlying principles and mechanisms. In the same way, SPI has been used successfully by focusing on the integration of visual data and a few objectively quantifiable parameters in site reconnaissance and monitoring.",
            "score": 48.34849572181702
        },
        {
            "docid": "8651984_19",
            "document": "Plant perception (physiology) . Plant physiology studies the role of signalling to integrate data obtained at the genetic, biochemical, cellular and physiological levels to understand plant development and behaviour. The neurobiological view sees plants as information-processing organisms with rather complex processes of communication occurring throughout the individual plant organism. It studies how environmental information is gathered, processed, integrated and shared (sensory plant biology) to enable these adaptive and coordinated responses (plant behaviour); and how sensory perceptions and behavioural events are 'remembered' in order to allow predictions of future activities upon the basis of past experiences. Plants, it is claimed by some plant physiologists, are as sophisticated in behaviour as animals but this sophistication has been masked by the time scales of plants' response to stimuli, many orders of magnitude slower than animals'.",
            "score": 60.56817936897278
        },
        {
            "docid": "39198919_2",
            "document": "Cancer systems biology . Cancer systems biology encompasses the application of systems biology approaches to cancer research, in order to study the disease as a complex adaptive system with emerging properties at multiple biological scales. Cancer systems biology represents the application of systems biology approaches to the analysis of how the intracellular networks of normal cells are perturbed during carcinogenesis to develop effective predictive models that can assist scientists and clinicians in the validations of new therapies and drugs. Tumours are characterized by genomic and epigenetic instability that alters the functions of many different molecules and networks in a single cell as well as altering the interactions with the local environment. Cancer systems biology approaches, therefore, are based on the use of computational and mathematical methods to decipher the complexity in tumorigenesis as well as cancer heterogeneity.  Cancer systems biology encompasses concrete applications of systems biology approaches to cancer research, notably (a) the need for better methods to distill insights from large-scale networks, (b) the importance of integrating multiple data types in constructing more realistic models, (b) challenges in translating insights about tumorigenic mechanisms into therapeutic interventions, and (d) the role of the tumor microenvironment, at the physical, cellular, and molecular levels. Cancer systems biology therefore adopts a holistic view of cancer aimed at integrating its many biological scales, including genetics, signaling networks, epigenetics, cellular behavior, histology, (pre)clinical manifestations and epidemiology. Ultimately, cancer properties at one scale, e.g., histology, are explained by properties at a scale below, e.g., cell behavior.",
            "score": 39.49647331237793
        },
        {
            "docid": "27064677_10",
            "document": "Occupancy frequency distribution . The OPT model demonstrates that the sample grain of a study, sampling adequacy, and the distribution of species saturation coefficients (a measure of the fractal dimensionality of a species distribution) in a community are together largely able to explain the patterns commonly found in empirical occupancy distributions. Hui and McGeoch (2007) further show that the self-similarity in species distributions breaks down according to a power relationship with spatial scales, and we therefore adopt a power-scaling assumption for modeling species occupancy distributions. The bimodality in occupancy frequency distributions that is common in species communities, is confirmed to a result for certain mathematical and statistical properties of the probability distribution of occupancy. The results thus demonstrate that the use of the bisection method in combination with a power-scaling assumption is more appropriate for modeling species distributions than the use of a self-similarity assumption, particularly at fine scales. This model further provokes the Harte-Maddux debate: Harte et al. demonstrated that the power law form of the species\u2013area relationship may be derived from a bisected, self-similar landscape and a community-level probability rule. However, Maddux showed that this self-similarity model generates biologically unrealistic predictions. Hui and McGeoch (2008) resolve the Harte\u2013Maddux debate by demonstrating that the problems identified by Maddux result from an assumption that the probability of occurrence of a species at one scale is independent of its probability of occurrence at the next, and further illustrate the importance of considering patterns of species co-occurrence, and the way in which species occupancy patterns change with scale, when modeling species distributions.",
            "score": 39.06003546714783
        },
        {
            "docid": "37798147_3",
            "document": "Research Moored Array for African-Asian-Australian Monsoon Analysis and Prediction . Although the data coverage for the Indian Ocean has been poor, it has not been non-existent. Satellites have been taking measurements, but those measurements require validation in situ. Some nations, like India and Australia, operate national ocean observing programs. Researchers have mounted observing equipment on ships of opportunity to take measurements. Also, the Argo float system has taken data in the Indian Ocean. What RAMA will contribute is large scale, long term data with high temporal resolution. High temporal resolution will allow rapid changes to be captured. With the installation of RAMA, the Indian Ocean will have a basin-wide observing system similar to TAO/TRITON in the Pacific Ocean and PIRATA in the tropical Atlantic. RAMA will complete the worldwide network of tropical ocean observing buoys, which will help with modeling and forecasting.The data RAMA collects will facilitate the study of \"ocean-atmosphere interactions, mixed layer dynamics, and ocean circulation related to the monsoon on intraseasonal to decadal time scales.\"",
            "score": 52.58593773841858
        },
        {
            "docid": "45281434_4",
            "document": "Wildlife Research and Conservation Trust . WRCT attempts to accomplish quality research under the broad umbrella of both applied and pure ecology. They pursue specific topics integrating wide variety of taxa at different ecological scales: ecosystem, community, population and species levels. Research and activities contributing to immediate conservation efforts are priority. WRCT constitute people with wide variety of expertise and approach which we accordingly integrate in their research. The research at WRCT falls under the following themes: Biodiversity characterization, patterns and monitoring, global change ecology, people and conservation,community and population ecology and behavioural and evolutionary ecology.",
            "score": 34.863930463790894
        },
        {
            "docid": "34172227_20",
            "document": "Breeding for heat stress tolerance . The physiological-trait-based breeding approach has merit over breeding for yield \"per se\" because it increases the probability of crosses resulting in additive gene action. The concept of combination phenomics comes from the idea that two or more stress have common physiological effect or common traits - which is an indicator of overall plant health. Similar analogy in human medical terms is high blood pressure or high body temperature or high white blood cells in body is an indicator of health problems and thus we can select healthy people from unhealthy using such a measure. As both abiotic and abiotic stresses can result in similar physiological consequence, tolerant plant can be separated from sensitive plants. Some imaging or infrared measuring techniques can help to speed the process for breeding process. For example, spot blotch intensity and canopy temperature depression can be monitored with canopy temperature depression.",
            "score": 39.47277057170868
        },
        {
            "docid": "13772374_14",
            "document": "Renaissance Computing Institute . A major focus of RENCI\u2019s work in the Biomedical and Health Sciences is clinical genomics. RENCI works with NC TraCS, the Lineberger Comprehensive Cancer Center at UNC-CH, and UNC\u2019s Information Technology Services Research Computing Division to develop and implement technologies to support next-generation genomic sequencing technologies, such as Whole Genome Sequencing (WGS) and Whole Exome Sequencing (WES). These technologies include the GMW (Genetic Medical Workflow) Engine, which was funded in part by the NIH and provides end-to-end capture, analysis, validation, and reporting of WGS and WES data. The GMW Engine is designed as open source architecture that coordinates workflows, sub-workflows, samples, data, and people to support all aspects of genomics research and clinical application, from the initial patient visit to the physician-guided reporting of genomic findings. MapSeq (Masively Parallel Sequencing) is an open source plugin-based Service-Oriented Architecture (SOA) that provides secure management and execution of the complex downstream computational and analytical steps involved in high-throughput genomic sequencing and other data-intensive applications. MaPSeq and its homegrown sister technology, GATE (Grid Access Triage Engine), are built on top of Apache Karaf and together provide extensible capabilities for downstream analysis of genomic data and other large data sets, including workflow pipeline execution and management, meta-scheduling of workflow jobs, opportunistic use of compute resources, secure data transfer, and web-based client access. CANVAS (CAroliNa Variant Annotation Store) and AnnoBot (Annotation Bot) work together to provide version-controlled annotation and metadata for genomic variant data in order to support up-to-date clinical interpretation of genomic variants and thereby guide clinical decision making. CANVAS is designed as an open source, relational PostgreSQL relational database that stores genomic variant data with associated annotation and metadata. AnnoBot consists of Python modules and software driver code configured to provide automated monitoring and retrieval of external data sources for annotation updates. CHAT (Convergent Haplotype Association Tagging) is a software algorithm that allows for the identification of moderately penetrant genomic variants using cross-population genetic structures. CHAT invokes a graph theory\u2013based algorithm to determine the haplotype phase of a population of unrelated individuals by: identifying subsets of individuals that share a region of the genome through descent; and then generating a consensus haplotype for the shared region. The SMW (Secure Medical Workspace) provisions a secure environment for access to sensitive patient data for clinical care or Institutional Review Board\u2013approved clinical research. The open source SMW architecture uses virtualization technology (i.e., VMWare) and Data Leakage Protection (DLP) technology (i.e., WebSense) to create a secure virtual workspace coupled with the ability to prevent (or allow with a challenge and auditing by Information Technology staff) the physical removal of data from a central, secure storage environment.",
            "score": 56.94011616706848
        }
    ],
    "r": [
        {
            "docid": "4558840_5",
            "document": "Lysimeter . To date, physiology-based, high-throughput phenotyping systems, which, used in combination with soil\u2013plant\u2013atmosphere continuum (SPAC) measurements and fitting models of plant responses to continuous and fluctuating environmental conditions, should be further investigated in order to serve as a phenotyping tool to better understand and characterise plant stress response . In these systems (known also as gravimetric system), plants are placed on weighing lysimeters that measure changes in pot weight at high frequency. This data is then combined with measurements of environmental parameters in the greenhouse, including radiation, humidity and temperature, as well as soil water conditions. Using pre-measured data including soil weight and initial plant weight, a great deal of phenotypic data can be extracted including data on stomatal conductance, growth rates, transpiration and soil water content and plant dynamic behaviour such as the critical \u0275 point, which is the soil water content at which plants start to respond to stress by reducing their stomatal conductance . The Faculty of Agriculture at the Hebrew university of Jerusalem has the most advanced functional phenotyping system in the world, with more than 400 units screened simultaneously .",
            "score": 91.84319305419922
        },
        {
            "docid": "14917968_17",
            "document": "Sediment Profile Imagery . In order to form and test fundamental community ecology hypotheses or address applications such as impact assessment, conservation, and exploitation of the marine environment, one needs to investigate the complex interactions between sediments, organisms, and water. A host of burgeoning technologies are slowly gaining acceptance to measure and explore this dynamic interface through biological, chemical, and physical approaches. Viollier et al. (2003) and Rhoads et al. (2001) provide overviews of this topic though the technologies involved and the standards used are changing rapidly. Several techniques have allowed benthologists to address \u2018big-picture\u2019 questions of geochemical-biological interactions and ecosystem functioning. Betteridge et al. (2003) used acoustic technology to measure sedimentary dynamics \"in situ\" at a scale relevant to macrofauna. Their benthic landers recorded water velocities near the seabed while simultaneously quantifying sediment disturbance patterns in high resolution. Benthic chambers have been used to examine the productivity of realistic macrofaunal assemblages under different flow regimes (Biles et al. 2003). Isotopic analysis methods permit [[food-web]] and environmental impact investigations (e.g. Rogers 2003; Schleyer et al. 2006) impossible to conduct outside of a laboratory only a few years ago. Short-sequence DNA methods (e.g. Biodiversity Institute of Ontario 2006) are rapidly moving toward automated identification and diversity assessment techniques that hold the promise of revolutionising benthic ecology.  Keegan et al. (2001) described the relationships among workers and authorities evaluating long-established, though often expensive and slow, methodologies with more recent technological developments as sometimes discordant. Gray et al. (1999b) lamented that there is a strong institutional tendency for sediment ecologists to rely on sampling methods developed in the early 1900s! A fine balance needs to be struck. Some degree of paradigm inertia is necessary to maintain intellectual continuity, but it can be taken too far. Physics, as a science, confronted this issue long ago and has widely embraced new technologies after establishing a scientific culture of always linking new techniques to established findings in a period of calibration and evaluation. The pace of this process in biology, as a whole, has quickened over the past few decades and ecology has only recently come to this horizon. This article introduces one such technology, sediment profile imagery (SPI) that is slowly gaining acceptance and currently undergoing its evaluation and calibration period even though it has existed since the 1970s. Like many of the technologies mentioned above, each new capability requires a careful consideration of its appropriateness in any particular application. This is especially true when they cross important, though often subtle, boundaries of data collection limitations. For example, much of our benthic knowledge has been developed from point-sample methods like cores or grabs, whereas continuous data collection, like some video transect analysis methods (e.g. Tkachenko 2005), may require different spatial interpretations that more explicitly integrate patchiness. While remote sampling techniques often improve our point-sampling resolution, benthologists need to consider the real-world heterogeneity at small spatial scales and compare them to the noise inherent to most high-volume data collection methods (e.g. Rabouille et al. 2003 for microelectrode investigations of pore water). New developments in the field of SPI will provide tools for investigating dynamic sediment processes, but also challenge our ability to accurately interpolate point-data collected at spatial densities approaching continuous data sets. SP imagery as embodied in the commercial REMOTS system (Rhoads et al. 1997) is expensive (>NZ$60,000 at time of writing), requires heavy lifting gear (ca. 66\u2013400\u00a0kg with a full complement of weights to effectively penetrate sediments), and is limited to muddy sediments. REMOTS is not well suited to small research programmes, nor operation in shallow water from small vessels, which is, quite possibly, an area where it could be most useful. Studying shallow sub-tidal environments can be a challenging exercise, especially among shifting sands. Macrofaunal sampling usually occurs at the sub-metre scale, whilst the dominant physical factors such as wave exposure and sediment texture can change at a scale of only metres, even though they are often only resolved to a scale of hundreds of metres. In such a dynamic environment, monitoring potentially transient disturbances like a spoil mound requires benthic mapping at fine spatial and temporal scales, an application ideally suited to SPI.",
            "score": 83.94975280761719
        },
        {
            "docid": "3040270_2",
            "document": "Phenotypic plasticity . Phenotypic plasticity refers to some of the changes in an organism's behavior, morphology and physiology in response to a unique environment. Fundamental to the way in which organisms cope with environmental variation, phenotypic plasticity encompasses all types of environmentally induced changes (e.g. morphological, physiological, behavioural, phenological) that may or may not be permanent throughout an individual's lifespan. The term was originally used to describe developmental effects on morphological characters, but is now more broadly used to describe all phenotypic responses to environmental change, such as acclimation or acclimatization, as well as learning. The special case when differences in environment induce discrete phenotypes is termed polyphenism.",
            "score": 82.45361328125
        },
        {
            "docid": "12176781_33",
            "document": "Italian crested newt . Continuing with developmental changes, \"T. carnifex\" will undergo a coupling of many processes as it eases into adulthood. Some of these processes include the metabolic needs of different tissues, cardiac activity, and respiratory procedures such as gas exchange. This coupling will be achieved by the autonomous nervous control system. More developmental changes include humoral and nervous control mechanisms, which will actually combine so that response to physiological and environmental changes will be met with a more coordinated reaction. The nervous system assists in development of cardiovascular functions, but this change will occur much later in life. Any time there is a switch from one type of respiration (e.g. cutaneous) to another (e.g. lung) there will be a required alteration and modification of the circulatory system. Another increase that comes with development is that of blood pressure. This is due to both the increase of blood in the organism due to size increasing, and as a response to the enlargement of internal organs. It has been said earlier that \"T. carnifex\" is an oxyregulator, and internally can change its physiology to maintain respiration even in oxygen poor environments. However, this is not true for the larval and embryonic stages of \"T. carnifex\". At these points in development, the \"T. carnifex\" are oxyconformers. In the earliest stages of embryonic life, cardiovascular activity is not linked or correlated with the activities that produce metabolism changes as in adults. Thus, many changes will have to occur for the organism to reach the mechanisms it contains as a fully developed animal. Interestingly, the heart (and related structures) is the first organ to operate functionally. This is necessary for many reasons, as the heart is responsible for distribution of countless things throughout the body and embryonic development will obviously require a great deal of things to proceed normally. One such reason of its primary operation is the structuring of the vascular system, such as capillary networks found in varying tissues.",
            "score": 78.18505859375
        },
        {
            "docid": "49528276_2",
            "document": "Behavioral plasticity . Behavioral plasticity refers to a change in an organism's behavior that results from exposure to stimuli, such as changing environmental conditions. Behavior can change more rapidly in response to changes in internal or external stimuli than is the case for most morphological traits and many physiological traits. As a result, when organisms are confronted by new conditions, behavioral changes often occur in advance of physiological or morphological changes. For instance, larval amphibians changed their antipredator behavior within an hour after a change in cues from predators, but morphological changes in body and tail shape in response to the same cues required a week to complete.",
            "score": 76.55260467529297
        },
        {
            "docid": "2998760_15",
            "document": "Facilitated variation . Therefore, the role of mutations is often to change how, where, and when the genes are expressed during the development of the embryo and adult. The burden of creativity in evolution does not rest on selection alone. Through its ancient repertoire of core processes, the current phenotype of the animal determines the kind, amount, and viability of phenotypic variation the animal can produce in response to regulatory change. In emphasizing the adaptability of organisms, and their ability to produce functional phenotypes even in the face of mutation or environmental change, Kirschner and Gerhart\u2019s theory builds upon earlier ideas by James Baldwin (the Baldwin effect), Ivan Schmalhausen, Conrad Waddington (genetic assimilation and accommodation), and Mary Jane West-Eberhard (\u2018genes are followers not leaders\u2019). More recently, the theory of facilitated variation has been embraced by advocates of an extended evolutionary synthesis, and emphasized for its role in generating non-random phenotypic variation (\u2018developmental bias\u2019). However, some evolutionary biologists remain skeptical as to whether facilitated variation adds a great deal to evolutionary theory.",
            "score": 76.21025085449219
        },
        {
            "docid": "1181008_10",
            "document": "Computational science . Exciting new developments in biotechnology are now revolutionizing biology and biomedical research. Examples of these techniques are high-throughput sequencing, high-throughput quantitative PCR, intra-cellular imaging, in-situ hybridization of gene expression, three-dimensional imaging techniques like Light Sheet Fluorescence Microscopy and Optical Projection, (micro)-Computer Tomography. Given the massive amounts of complicated data that is generated by these techniques, their meaningful interpretation, and even their storage, form major challenges calling for new approaches. Going beyond current bioinformatics approaches, computational biology needs to develop new methods to discover meaningful patterns in these large data sets. Model-based reconstruction of gene networks can be used to organize the gene expression data in systematic way and to guide future data collection. A major challenge here is to understand how gene regulation is controlling fundamental biological processes like biomineralisation and embryogenesis. The sub-processes like gene regulation, organic molecules interacting with the mineral deposition process, cellular processes, physiology and other processes at the tissue and environmental levels are linked. Rather than being directed by a central control mechanism, biomineralisation and embryogenesis can be viewed as an emergent behavior resulting from a complex system in which several sub-processes on very different temporal and spatial scales (ranging from nanometer and nanoseconds to meters and years) are connected into a multi-scale system. One of the few available options to understand such systems is by developing a multi-scale model of the system.",
            "score": 75.13754272460938
        },
        {
            "docid": "6645054_4",
            "document": "Genetic assimilation . Conrad H. Waddington's classic experiment (1942) induced an extreme environmental reaction in the developing embryos of \"Drosophila\". In response to ether vapor, a proportion of embryos developed a radical phenotypic change, a second thorax. At this point in the experiment \"bithorax\" is not innate; it is induced by an unusual environment. Waddington then repeatedly selected \"Drosophila\" for the \"bithorax\" phenotype over some 20 generations. After this time, some \"Drosophila\" developed \"bithorax\" without the ether treatment.",
            "score": 74.56963348388672
        },
        {
            "docid": "19472940_5",
            "document": "Surface Water Ocean Topography . SWOT is designed for the study and monitoring of inland waters and the oceans, such as: The sharing of river water often causes friction between neighboring states, especially when there is no common technology for verification. SWOT will provide global information as input for systems monitoring river basins. SWOT will enable more accurate weather and climate forecasting, especially seasonal. The quality of weather and climate forecasting largely depends on numerical modeling that use the state of the ocean surface and the hydrological conditions of catchment areas in their initial and boundary conditions. Managing freshwater for urban, industrial and agricultural consumption Accurate knowledge of sources of available water is a key factor in decision-making for organizations involved in the distribution of water for agricultural, urban and industrial needs. Data from SWOT will contribute at global level by providing water supply services and distribution companies with information about major reservoirs and the largest rivers and catchment areas, thus enabling them to plan the management of water stocks further into the future. Flooding, whether from rivers overflowing their banks or in coastal regions, is among the most disastrous of natural phenomena. Altimetry data from the SWOT mission will make it possible to measure floodwater levels and local topographic details more reliably. Coastal ocean dynamics are important for many societal applications. They have smaller spatial and temporal scales than the dynamics of the open ocean and require finer-scale monitoring. SWOT will provide global, high-resolution observations in coastal regions for observing coastal currents and storm surges. While SWOT is not designed to monitor the fast temporal changes of the coastal processes, the swath coverage will allow us to characterize the spatial structure of their dynamics when they occur within the swath.  More generally, SWOT will help improve our knowledge, enhance observations by collecting data over the long term and making them available, and help us draw lessons from past episodes. Water resources, natural risks (floods, climate change, hurricane forecasting, etc.), biodiversity, health (preventing the propagation of epidemics), the agricultural sector, energy (including the management of electricity production and offshore gas and oil rigs), territorial development; all these areas and more stand to benefit from this new space mission.",
            "score": 74.3665771484375
        },
        {
            "docid": "7375514_12",
            "document": "Synaptotropic hypothesis . Dynamic morphometrics technology involves new methods of labeling, imaging, and quantifying dendritogenesis. The transparent, externally developing vertebrate embryos of \"Xenopus laevis\" and zebrafish allow direct imaging of the organism in the critical stages of development while keeping the embryos intact. Individual brain neurons can be fluorescently labeled using single cell electroporation while leaving the rest of the brain unaltered. Also, two-photon microscopy allows in vivo time-lapse imaging to create high-resolution, 3D images of neurons deep within the living brain, again with minimal damage to the brain. New computer software also can now track and measure dendritic growth. These methods comprise a new type of imaging technology that can monitor the process of dendritogenesis and can help give evidence to either dissent with or support the synaptotropic hypothesis.",
            "score": 73.74850463867188
        },
        {
            "docid": "18101603_7",
            "document": "High throughput biology . High-content screening technology is mainly based on automated digital microscopy and flow cytometry, in combination with IT-systems for the analysis and storage of the data. \"High-content\" or visual biology technology has two purposes, first to acquire spatially or temporally resolved information on an event and second to automatically quantify it. Spatially resolved instruments are typically automated microscopes, and temporal resolution still requires some form of fluorescence measurement in most cases.This means that a lot of HCS instruments are (fluorescence) microscopes that are connected to some form of image analysis package. These take care of all the steps in taking fluorescent images of cells and provide rapid, automated and unbiased assessment of experiments.",
            "score": 73.7452163696289
        },
        {
            "docid": "10273_13",
            "document": "Embryo drawing . Von Baer\u2019s laws governing embryonic development are specific rejections of recapitulation. As a response to Haeckel\u2019s theory of recapitulation, von Baer enunciates his most notorious laws of development. Von Baer\u2019s laws state that general features of animals appear earlier in the embryo than special features, where less general features stem from the most general, each embryo of a species departs more and more from a predetermined passage through the stages of other animals, and there is never a complete morphological similarity between an embryo and a lower adult. Von Baer\u2019s embryo drawings display that individual development proceeds from general features of the developing embryo in early stages through differentiation into special features specific to the species, establishing that linear evolution could not occur. Embryological development, in von Baer\u2019s mind, is a process of differentiation, \"a movement from the more homogeneous and universal to the more heterogeneous and individual.\"",
            "score": 71.42288970947266
        },
        {
            "docid": "12756358_11",
            "document": "German Advisory Council on Global Change . Today, more than 900 bi- and multilateral environmental treaties are in force. Nonetheless, the most pressing problems of global change remain unresolved, some are even intensifying. The international institutional and organisational architecture has proven too weak to provide effective and efficient responses to these challenges. In this situation, the WBGU has developed a vision for reforming the United Nations in the environmental arena. It terms this the \u2018Earth Alliance\u2019, comprising three interlocking realms. First, to provide authority in the assessment of environmental problems, the WBGU proposes establishing an independent body whose task is to provide timely warning of particularly risk-laden developments in the sphere of global change \u2013 Earth Assessment. Second, the report gives recommendations for redesigning the organisational core of global environmental policy \u2013 Earth Organisation. This revolves around the step-wise establishment of an International Environmental Organisation, building on the existing United Nations Environment Programme as its initial nucleus. Third, the WBGU highlights new avenues for financing global environmental policy \u2013 Earth Funding.",
            "score": 70.97677612304688
        },
        {
            "docid": "14351399_5",
            "document": "ITASE . Because Antarctica has played a major role in global systems (i.e. atmosphere, biosphere, hydrosphere, etc.) scientists hoped to expand on the little knowledge of Antarctica\u2019s complex climate that was available. This data was collected primarily during the past 30 to 40 years.  High resolution ice cores have been recognized, since, as the most direct and reliable record of the \u201csoluble, insoluble and gaseous components of the atmosphere at resolutions as fine as seasonal and, potentially, on time scales as long as a million years\u201d (International). Through ice core analysis, scientists are able to study past environments on earth and, more importantly, predict future environmental trends. Ice cores formed from polar glaciers generally contain the best preserved records of all geographic locations. Substances transported by the atmosphere and trapped within glacial ice reveal factors that cause environmental change as well as global responses to this change.",
            "score": 70.75605773925781
        },
        {
            "docid": "59416_37",
            "document": "Soil erosion . Recent modeling developments have quantified rainfall erosivity at global scale using high temporal resolution(<30\u2009min) and high fidelity rainfall recordings. The results is an extensive global data collection effort produced the Global Rainfall Erosivity Database (GloREDa) which includes rainfall erosivity for 3,625 stations and covers 63 countries. This first ever Global Rainfall Erosivity Database was used to develop a global erosivity map at 30 arc-seconds(~1\u2009km) based on sophisticated geostatistical process. According to a new study published in Nature Communications, almost 36 billion tons of soil is lost every year due to water, and deforestation and other changes in land use make the problem worse.\u00a0The study investigates global soil erosion dynamics by means of high-resolution spatially distributed modelling (ca. 250\u2009\u00d7\u2009250\u2009m cell size). The geo-statistical approach allows, for the first time, the thorough incorporation into a global soil erosion model of land use and changes in land use, the extent, types, spatial distribution of global croplands and the effects of different regional cropping systems.",
            "score": 69.8246078491211
        },
        {
            "docid": "47137356_9",
            "document": "Warehouse execution system . Another benefit of leveraging the visibility of lower level data across a broad range of warehouse functionality is the ability to provide unprecedented automated business intelligence. WES\u2019 access to and collection of data from various warehouse points can be utilized to provide not only advanced reporting and live dashboard functionality but business intelligence tools such as predictive analysis, prescriptive analysis, and issue detection. The WES can feed data into its business intelligence engine to be mined in near real-time so that DC operations can move beyond just being agile in response to changing conditions, to being proactive in making adjustments before conditions change. WES data can be analyzed to identify trends and predict operational conditions. For example, if operation peaks occur at the end of every month, warehouses can use WES feedback to ramp up staffing and equipment needs more efficiently to reduce overall costs. WES data can also be used to predict issues such as potential stock-outs or order fulfillment delays. Issue detection can also relate to preventative maintenance of warehouse equipment such as lift trucks, conveyor systems, etc. To illustrate this point, through analyzing vast amounts of data, the WES can predict when a conveyor motor may need to be replaced or when a lift truck may need servicing to reduce downtime. By collecting and analyzing data from various lower level warehouse points and taking proactive action, operation leads can use this functionality \u2013 which is unique to a WES \u2013 to make their facilities more efficient, safe and responsive to increasing customer service requirements.",
            "score": 69.46620178222656
        },
        {
            "docid": "227478_3",
            "document": "Germination . Germination is usually the growth of a plant contained within a seed; it results in the formation of the seedling, it is also the process of reactivation of metabolic machinery of the seed resulting in the emergence of radicle and plumule. The seed of a vascular plant is a small package produced in a fruit or cone after the union of male and female reproductive cells. All fully developed seeds contain an embryo and, in most plant species some store of food reserves, wrapped in a seed coat. Some plants produce varying numbers of seeds that lack embryos; these are called and never germinate. Dormant seeds are ripe seeds that do not germinate because they are subject to external environmental conditions that prevent the initiation of metabolic processes and cell growth. Under proper conditions, the seed begins to germinate and the embryonic tissues resume growth, developing towards a seedling. Seed germination depends on both internal and external conditions. The most important external factors include right temperature, water, oxygen or air and sometimes light or darkness. Various plants require different variables for successful seed germination. Often this depends on the individual seed variety and is closely linked to the ecological conditions of a plant's natural habitat. For some seeds, their future germination response is affected by environmental conditions during seed formation; most often these responses are types of seed dormancy. Most common annual vegetables have optimal germination temperatures between 75-90 F (24-32 C), though many species (e.g. radishes or spinach) can germinate at significantly lower temperatures, as low as 40 F (4 C), thus allowing them to be grown from seeds in cooler climates. Suboptimal temperatures lead to lower success rates and longer germination periods. Scarification mimics natural processes that weaken the seed coat before germination. In nature, some seeds require particular conditions to germinate, such as the heat of a fire (e.g., many Australian native plants), or soaking in a body of water for a long period of time. Others need to be passed through an animal's digestive tract to weaken the seed coat enough to allow the seedling to emerge. Some live seeds are dormant and need more time, and/or need to be subjected to specific environmental conditions before they will germinate. Seed dormancy can originate in different parts of the seed, for example, within the embryo; in other cases the seed coat is involved. Dormancy breaking often involves changes in membranes, initiated by dormancy-breaking signals. This generally occurs only within hydrated seeds. Factors affecting seed dormancy include the presence of certain plant hormones, notably abscisic acid, which inhibits germination, and gibberellin, which ends seed dormancy. In brewing, barley seeds are treated with gibberellin to ensure uniform seed germination for the production of barley malt.",
            "score": 68.99774932861328
        },
        {
            "docid": "31525688_5",
            "document": "Embryo rescue . Embryos are manually excised and placed immediately onto a culture medium that provides the proper nutrients to support survival and growth (Miyajima 2006). While the disinfestation and explant excision processes differ for these three techniques, many of the factors that contribute to the successful recovery of viable plants are similar. The main factors that influence success are; the time of culture, the composition of the medium, and temperature and light. Timing mainly refers to the maturation stage of the embryo before excision. The optimal time especially for the rescue of embryos involving incompatible crosses would be just prior to embryo abortion. Nevertheless, due to difficulties involved with the rearing of young embryos compared to those that have reached the autotrophic phase of development, embryos are normally allowed to develop in vivo as long as possible. While in general, two main types of basal media are the most commonly used for embryo rescue studies, i.e. Murashige and Skoog medium (MS) and Gamborg\u2019s B-5 media (Bridgen, 1994), the composition of the medium will vary in terms of the concentrations of media supplements required. This will generally depend on the stage of development of the embryo. For instance, young embryos would require a complex medium with high sucrose concentrations, while more mature embryos can usually develop on a simple medium with low levels of sucrose. The temperature and light requirement is generally species specific and thus its usually regulated to be the within the same temperature requirement as that of its parent with embryos of cool-season crops requiring lower temperatures than those of warm-season crops.",
            "score": 68.86048126220703
        },
        {
            "docid": "2833267_75",
            "document": "Coastal management . Video analysis provides quantitative, cost-effective, continuous and long-term monitoring beaches. The advancement of coastal video systems in the twenty-first century enabled the extraction of large amounts of geophysical data from images. The data describes coastal morphology, surface currents and wave parameters. The main advantage of video analysis lies in the ability to reliably quantify these parameters with high resolution space and time coverage. This highlights their potential as an effective coastal monitoring system and an aid to coastal zone management. Interesting case studies have been carried out using video analysis. One group used a video-based ARGUS coastal imaging system to monitor and quantify the regional-scale coastal response to sand nourishment and construction of the world-first Gold Coast artificial surfing reef in Australia. Another assessed the added value of high resolution video observations for short-term predictions of near shore hydrodynamic and morphological processes, at temporal scales of meters to kilometres and days to seasons.",
            "score": 68.7973861694336
        },
        {
            "docid": "2354515_11",
            "document": "Agalychnis callidryas . Red-eyed treefrogs' embryos exhibit phenotypic plasticity, hatching early in response to disturbance to protect themselves. Though embryos are bred synchronously, they normally hatch after 6 to 10 days from oviposition without disturbance. However, a simultaneously early hatching in entire clutches is triggered when embryos are exposed to their predators or threatening environmental changes such as rainstorm and flood.",
            "score": 68.7549057006836
        },
        {
            "docid": "38876059_4",
            "document": "Thermal shift assay . The DSF-GTP technique was developed by a team led by Patrick Schaeffer at James Cook University and published in Moreau et al. 2012. The development of differential scanning fluorimetry and the high-throughput capability of Thermofluor have vastly facilitated the screening of crystallization conditions of proteins and large mutant libraries in structural genomics programs, as well as ligands in drug discovery and functional genomics programs. These techniques are limited by their requirement for both highly purified proteins and solvatochromic dyes, prompting the need for more robust high-throughput technologies that can be used with crude protein samples. This need was met with the development of a new high-throughput technology for the quantitative determination of protein stability and ligand binding by differential scanning fluorimetry of proteins tagged with green fluorescent protein (GFP). This technology is based on the principle that a change in the proximal environment of GFP, such as unfolding and aggregation of the protein of interest, is measurable through its effect on the fluorescence of the fluorophore. The technology is simple, fast and insensitive to variations in sample volumes, and the useful temperature and pH range is 30\u201380\u00b0C and 5\u201311 respectively. The system does not require solvatochromic dyes, reducing the risk of interferences. The protein samples are simply mixed with the test conditions in a 96-well plate and subjected to a melt-curve protocol using a real-time thermal cycler. The data are obtained within 1\u20132 h and include unique quality control measures through the GFP signal. DSF-GTP has been applied for the characterization of proteins and the screening of small compounds.",
            "score": 68.63481140136719
        },
        {
            "docid": "49528276_9",
            "document": "Behavioral plasticity . Developmental plasticity encompasses the many ways that experiences in an organism's past can affect its current behavior. Developmental plasticity thus includes learning, acclimation, and any situation in which environmental conditions early in life affects the behavior expressed later in life (also called ontogenetic plasticity. Since a given individual can only be raised under one set of conditions, ontogenetic plasticity is studied by dividing matched individuals into two or more groups, and then rearing each group under a different set of conditions. For instance, this experimental design was used to demonstrate that the density at which moth larvae were raised affected the courtship signals that they produced as adults.  Endogenous plasticity includes circadian rhythms, circannual rhythms, and age-dependent changes in behavior. A good example of endogenous plasticity occurs with zebrafish (\"Danio rerio\"). Larval zebrafish exhibit circadian rhythms in their responsiveness to light. Even when they are maintained under continuous darkness, the fish are much more responsive to changes in light (i.e. higher contextual plasticity) during subjective day than during subjective night. Another example involves the changes in an individual's behavior and hormonal profile around the time of sexual maturity; such changes are affected changes in physiology that occurred months to years earlier in life.",
            "score": 68.43223571777344
        },
        {
            "docid": "25572054_3",
            "document": "Imaging Lung Sound Behavior with Vibration Response Imaging . Vibration response Imaging (VRI), a novelty computer-based technology takes the concept of the stethoscope to a more progressive level. The technology is based on the physiologic vibration generated during the breathing process when flow of air distributing through the bronchial tree creates vibration of the bronchial tree walls and the lung parenchyma itself. Emitted vibration energy propagating through the lung parenchyma and the chest wall reaches the body surface where is captured and recorded by a set of acoustic sensors. The sensors are positioned over the lung areas on the back that allows for the simultaneous reception of these signals from both lungs. These signals are then transformed by a complex algorithm to display the spatial changes in energy intensity during the breathing cycle. The intensity changes follow changes of airflow through the breathing cycle - i.e.: flow increases and decreases during inspiration and expiration. The VRI technology represents these changes as a grey scale-based dynamic image. The darker the higher the vibration intensity and the lighter the lower the vibration intensity is.",
            "score": 68.33651733398438
        },
        {
            "docid": "2364800_50",
            "document": "Environmental impact assessment . The Ministry of Environment, Forests and Climate Change (MoEFCC) of India has been in a great effort in Environmental Impact Assessment in India. The main laws in action are the Water Act(1974), the Indian Wildlife (Protection) Act (1972), the Air (Prevention and Control of Pollution) Act (1981) and the Environment (Protection) Act (1986),Biological Diversity Act(2002). The responsible body for this is the Central Pollution Control Board. Environmental Impact Assessment (EIA) studies need a significant amount of primary and secondary environmental data. Primary data are those collected in the field to define the status of the environment (like air quality data, water quality data etc.). Secondary data are those collected over the years that can be used to understand the existing environmental scenario of the study area. The environmental impact assessment (EIA) studies are conducted over a short period of time and therefore the understanding of the environmental trends, based on a few months of primary data, has limitations. Ideally, the primary data must be considered along with the secondary data for complete understanding of the existing environmental status of the area. In many EIA studies, the secondary data needs could be as high as 80% of the total data requirement. EIC is the repository of one stop secondary data source for environmental impact assessment in India.",
            "score": 67.90382385253906
        },
        {
            "docid": "3902505_13",
            "document": "FPD-Link . The higher resolution applications required FPD-Link II to increase the data throughput. It started at about 1 Gbit/s data throughput on a single twisted pair which is well within the capability for LVDS technology. But for the applications that required up to 1.8\u00a0Gbit/s over a single pair, LVDS was not as reliable as necessary for the automotive applications. By changing from LVDS to current mode logic (CML), the newest FPD-Link II chipsets were able to reliably send high bit-rate video streams over cables longer than 10m. FPD-Link III was introduced in 2010. Further improving FPD-Link II, FPD-Link III's major feature is embedding a bidirectional communication channel on the same differential pair. This bidirectional channel transfers control signals between source and destination in addition to the clock and streaming video data. Therefore, FPD-Link III even further reduces cable cost by eliminating cables for control channels such as I2C and CAN bus.",
            "score": 67.87215423583984
        },
        {
            "docid": "33708419_12",
            "document": "Knockout mouse . While knockout mouse technology represents a valuable research tool, some important limitations exist. About 15 percent of gene knockouts are developmentally lethal, which means that the genetically altered embryos cannot grow into adult mice. This problem is often overcome through the use of conditional mutations. The lack of adult mice limits studies to embryonic development and often makes it more difficult to determine a gene's function in relation to human health. In some instances, the gene may serve a different function in adults than in developing embryos. Knocking out a gene also may fail to produce an observable change in a mouse or may even produce different characteristics from those observed in humans in which the same gene is inactivated. For example, mutations in the p53 gene are associated with more than half of human cancers and often lead to tumours in a particular set of tissues. However, when the p53 gene is knocked out in mice, the animals develop tumours in a different array of tissues. There is variability in the whole procedure depending largely on the strain from which the stem cells have been derived. Generally cells derived from strain 129 are used. This specific strain is not suitable for many experiments (e.g., behavioural), so it is very common to backcross the offspring to other strains. Some genomic loci have been proven very difficult to knock out. Reasons might be the presence of repetitive sequences, extensive DNA methylation, or heterochromatin. The confounding presence of neighbouring 129 genes on the knockout segment of genetic material has been dubbed the \"flanking-gene effect\". Methods and guidelines to deal with this problem have been proposed.",
            "score": 67.64864349365234
        },
        {
            "docid": "2106968_59",
            "document": "Augmentative and alternative communication . Rapid progress in hardware and software development continued, including projects funded by the European Community. The first commercially available dynamic screen speech generating devices were developed in the 1990s. At the same time synthesized speech was becoming available in more languages. Software programs were developed that allowed the computer-based production of communication boards. High-tech devices have continued to reduce in size and weight, while increasing accessibility and capacities. Modern communication devices can also enable users to access the internet and some can be used as environmental control devices for independent access of TV, radio, telephone etc. Future directions for AAC focus on improving device interfaces, reducing the cognitive and linguistic demands of AAC, and the barriers to effective social interaction. AAC researchers have challenged manufacturers to develop communication devices that are more appealing aesthetically, with greater options for leisure and play and that are easier to use. The rapid advances in smartphone and tablet computer technologies has the potential to radically change the availability of economical, accessible, flexible communication devices; however, the user interfaces are needed that meet the various physical and cognitive challenges of AAC users. Android and other open source operating systems, provide opportunities for small communities, such as AAC, to develop the accessibility features and software required. Other promising areas of development include the access of communication devices using signals from movement recognition technologies that interpret body motions, or electrodes measuring brain activity, and the automatic transcription of dysarthric speech using speech recognition systems. Utterance-based systems, in which frequent utterances are organized in sets to improve the speed of communication exchange, are also in development. Similarly, research has focussed on the provision of timely access to vocabulary and conversation appropriate for specific interactions. Natural language generation techniques have been investigated, including the use of logs of past conversations with conversational partners, data from a user's schedule and from real-time Internet vocabulary searches, as well as information about location from global positioning systems and other sensors. However, despite the frequent focus on technological advances in AAC, practitioners are urged to retain the focus on the communication needs of the AAC users: \"The future for AAC will not be driven by advances in technology, but rather by how well we can take advantage of those advancements for the enhancement of communicative opportunities for individuals who have complex communication needs\".",
            "score": 67.57618713378906
        },
        {
            "docid": "2696466_7",
            "document": "Demand response . According to the Federal Energy Regulatory Commission, demand response (DR) is defined as:  \u201cChanges in electric usage by end-use customers from their normal consumption patterns in response to changes in the price of electricity over time, or to incentive payments designed to induce lower electricity use at times of high wholesale market prices or when system reliability is jeopardized.\u201d DR includes all intentional modifications to consumption patterns of electricity to induce customers that are intended to alter the timing, level of instantaneous demand, or the total electricity consumption. It is expected that demand response programs will be designed to decrease electricity consumption or shift it from on-peak to off-peak periods depending on consumers\u2019  preferences and lifestyles. Demand Response can be defined as \"a wide range of actions which can be taken at the customer side of the electricity meter in response to particular conditions within the electricity system (such as peak period network congestion or high prices)\". Demand response is a reduction in demand designed to reduce peak demand or avoid system emergencies. Hence, demand response can be a more cost-effective alternative than adding generation capabilities to meet the peak and or occasional demand spikes. The underlying objective of DR is to actively engage customers in modifying their consumption in response to pricing signals. The goal is to reflect supply expectations through consumer price signals or controls and enable dynamic changes in consumption relative to price.",
            "score": 67.337646484375
        },
        {
            "docid": "33353100_10",
            "document": "Vibrio anguillarum . \"Vibrio anguillarum\" have optimal growth temperatures between 30\u00a0\u00b0C and 34\u00a0\u00b0C. Growth rates are found to be increasing with temperature with a maximum growth temperature at 38.5\u00a0\u00b0C. They are Halophiles but growth is more dependent on temperature than salinity. Lethal temperatures and salinity are at values greater than 41\u00a0\u00b0C and 7 parts per hundred. The efficacy of Binary fission o\"f Vibrio anguillaru\"m cells are dependent on the pH levels of their surroundings. Binary fission is inhibited at pH greater than 9, disrupted at pH 6 or below, and most efficient at pH 7. They are more common in environments producing fertilized fish eggs with inhabitants such as larval fish or rotifers. An environment that contains divalent cations allow \"Vibrio anguillarum\" to thrive. \"V. anguillarum\" must adapt to its environment as changes in temperature and salinity as well as depleted nutrient source creates a stressful environment for the majority of its life cycle. \"Vibrio anguillarum\" must rely on sodium ions for survival in seawater during stressed starvation for long-term survival. When inside a host, \"Vibrio anguillarum\" can direct nutrients to itself through the use of a proton motive force. Environmental stress such as high temperature, osmotic stress, UV radiation, or oxidative stress may activate the rpoS gene which is responsible for genetic regulation of cellular response to environment stress.",
            "score": 66.5579833984375
        },
        {
            "docid": "343457_17",
            "document": "Maternal effect . When analyzing the types of changes that can occur to a phenotype, we can see changes that are behavioral, morphological, or physiological. A characteristic of the phenotype that arises through adaptive maternal effects, is the plasticity of this phenotype. Phenotypic plasticity allows organisms to adjust their phenotype to various environments, thereby enhancing their fitness to changing environmental conditions. Ultimately it is a key attribute to an organism\u2019s, and a population\u2019s, ability to adapt to short term environmental change.",
            "score": 66.5066909790039
        },
        {
            "docid": "41727187_5",
            "document": "Kathy Willis . Willis's research focuses on reconstructing long term responses of ecosystems to environmental change, including climate change, human impact and sea level rise. She argues that understanding long-term records of ecosystem change is essential for a proper understanding of future ecosystem responses. Many scientific studies are limited to short-term datasets that rarely span more than 40 to 50 years, although many larger organisms, including trees and large mammals, have an average generation time which exceeds this timescale. Short-term records therefore are unable to reconstruct natural variability over time, or the rates of migration as a result of environmental change. She also argues that a short-term approach gives a static view of ecosystems, and an unrealistic \"norm\" which must be maintained or restored and protected. Her research group in the Oxford Long-term Ecology laboratory therefore focuses on the reconstruction of ecosystem responses to environmental change on timescales ranging from tens to millions of years, and the applications of long-term records in biodiversity conservation. She has argued that the impacts of contemporary climate change on plant biota is uncertain and potentially not as severe as researchers envision, and challenged assumptions made in the interpretation of spatially constrained temperature records. Kew's \"State of the World's Plants\" report (2016) pinpoints land cover change as the major threat to global biodiversity, not climate change.",
            "score": 66.50322723388672
        },
        {
            "docid": "3069503_10",
            "document": "Structural health monitoring . Because data can be measured under varying conditions, the ability to normalize the data becomes very important to the damage identification process. As it applies to SHM, data normalization is the process of separating changes in sensor reading caused by damage from those caused by varying operational and environmental conditions. One of the most common procedures is to normalize the measured responses by the measured inputs. When environmental or operational variability is an issue, the need can arise to normalize the data in some temporal fashion to facilitate the comparison of data measured at similar times of an environmental or operational cycle. Sources of variability in the data acquisition process and with the system being monitored need to be identified and minimized to the extent possible. In general, not all sources of variability can be eliminated. Therefore, it is necessary to make the appropriate measurements such that these sources can be statistically quantified. Variability can arise from changing environmental and test conditions, changes in the data reduction process, and unit-to-unit inconsistencies.",
            "score": 66.42225646972656
        }
    ]
}