{
    "q": [
        {
            "docid": "8953380_9",
            "document": "Auditory scene analysis . One example of this is the phenomenon of streaming, also called \"stream segregation.\" If two sounds, A and B, are rapidly alternated in time, after a few seconds the perception may seem to \"split\" so that the listener hears two rather than one stream of sound, each stream corresponding to the repetitions of one of the two sounds, for example, A-A-A-A-, etc. accompanied by B-B-B-B-, etc. The tendency towards segregation into separate streams is favored by differences in the acoustical properties of sounds A and B. Among the differences classically shown to promote segregation are those of frequency (for pure tones), fundamental frequency (for complex tones), frequency composition, source location. But it has been suggested that about any systematic perceptual difference between two sequences can elicit streaming, provided the speed of the sequence is sufficient.",
            "score": 116.247722864151
        },
        {
            "docid": "8953842_15",
            "document": "Computational auditory scene analysis . While many models consider the audio signal as a complex combination of different frequencies, modeling the auditory system can also require consideration for the neural components. By taking a holistic process, where a stream (of feature-based sounds) correspond to neuronal activity distributed in many brain areas, the perception of the sound could be mapped and modeled. Two different solutions have been proposed to the binding of the audio perception and the area in the brain. Hierarchical coding models many cells to encode all possible combinations of features and objects in the auditory scene. Temporal or oscillatory correlation addressing the binding problem by focusing on the synchrony and desynchrony between neural oscillations to encode the state of binding among the auditory features. These two solutions are very similar to the debacle between place coding and temporal coding. While drawing from modeling neural components, another phenomenon of ASA comes into play with CASA systems: the extent of modeling neural mechanisms. The studies of CASA systems have involved modeling some known mechanisms, such as the bandpass nature of cochlear filtering and random auditory nerve firing patterns, however, these models may not lead to finding new mechanisms, but rather give an understanding of purpose to the known mechanisms.",
            "score": 90.99647402763367
        },
        {
            "docid": "42984420_7",
            "document": "Multistable auditory perception . When using a two stream tone test, specific populations of neurons activate, known as the place model. Event related potential (ERP) amplitude increases when the difference of the frequency of the two tones increase. This model hypothesizes that when this is happening, the distance between the two populations of neurons increase, so that the two populations will interact less with each other, allowing for easier tone segregation.",
            "score": 75.49589800834656
        },
        {
            "docid": "35988494_3",
            "document": "Selective auditory attention . The cocktail party problem was first brought up in 1953 by Colin Cherry. This common problem is how our minds solves the issue of knowing what in the auditory scene is important and combining those in a coherent whole, such as the problem of how we can perceive our friend talking in the midst of a crowded cocktail party. He suggested that the auditory system can filter sounds being heard. Physical characteristics of the auditory information such as speaker's voice or location can improve a person's ability to focus on certain stimuli even if there is other auditory stimuli present. Cherry also did work with shadowing which involves different information being played into both ears and only one ear's information can be processed and remembered (Eysneck, 2012, p.\u00a084). Another psychologist, Albert Bregman, came up with the auditory scene analysis model. The model has three main characteristics: segmentation, integration, and segregation. Segmentation involves the division of auditory messages into segments of importance. The process of combining parts of an auditory message to form a whole is associated with integration. Segregation is the separation of important auditory messages and the unwanted information in the brain. It is important to note that Bregman also makes a link back to the idea of perception. He states that it is essential for one to make a useful representation of the world from sensory inputs around us. Without perception, an individual will not recognize or have the knowledge of what is going on around them. While Begman's seminal work is critical to understanding selective auditory attention, his studies did not focus on the way in which an auditory message is selected, if and when it was correctly segregated from other sounds in a mixture, which is a critical stage of selective auditory attention. Inspired in part by Bregman's work, a number of researchers then set out to link directly work on auditory scene analysis to the processes governing attention, including Maria Chait, Mounya Elhilali, Shihab Shamma, and Barbara Shinn-Cunningham.",
            "score": 98.93616712093353
        },
        {
            "docid": "19208664_10",
            "document": "Neural modeling fields . Models represent signals in the following way. Suppose that signal X(\"n\") is coming from sensory neurons n activated by object m, which is characterized by parameters S. These parameters may include position, orientation, or lighting of an object m. Model M(S,n) predicts a value X(n) of a signal at neuron n. For example, during visual perception, a neuron n in the visual cortex receives a signal X(n) from retina and a priming signal M(S,n) from an object-concept-model \"m\". Neuron \"n\" is activated if both the bottom-up signal from lower-level-input and the top-down priming signal are strong. Various models compete for evidence in the bottom-up signals, while adapting their parameters for better match as described below. This is a simplified description of perception. The most benign everyday visual perception uses many levels from retina to object perception. The NMF premise is that the same laws describe the basic interaction dynamics at each level. Perception of minute features, or everyday objects, or cognition of complex abstract concepts is due to the same mechanism described below. Perception and cognition involve concept-models and learning. In perception, concept-models correspond to objects; in cognition models correspond to relationships and situations.",
            "score": 44.991666197776794
        },
        {
            "docid": "42984420_4",
            "document": "Multistable auditory perception . Different experimental paradigms have since been used to study multistable perception in the auditory modality. One is auditory stream segregation, in which two different frequencies are presented in a temporal pattern. Listeners experience alternating percepts: one percept is of a single stream fluctuating between frequencies, and the alternative percept is of two separate streams repeating single frequencies each.",
            "score": 73.80971336364746
        },
        {
            "docid": "8953380_11",
            "document": "Auditory scene analysis . Many experiments have studied the segregation of more complex patterns of sound, such as a sequence of high notes of different pitches, interleaved with low ones. In such sequences, the segregation of co-occurring sounds into distinct streams has a profound effect on the way they are heard. Perception of a melody is formed more easily if all its notes fall in the same auditory stream. We tend to hear the rhythms among notes that are in the same stream, excluding those that are in other streams. Judgments of timing are more precise between notes in the same stream than between notes in separate streams. Even perceived spatial location and perceived loudness can be affected by sequential grouping.",
            "score": 111.74986910820007
        },
        {
            "docid": "42984420_10",
            "document": "Multistable auditory perception . A theory explaining the alternation of auditory percepts is that different interpretations are neurally represented simultaneously, but all but the dominant one at the time are suppressed. This idea of competition among parallel hypotheses might provide an explanation for the temporal dynamics observed in auditory stream segregation. The initial perceptual phase is held longer than the subsequent ones, \u201cwith the duration of the first phase being stimulus-parameter dependent and an order of magnitude longer in duration than parameter-independent subsequent phases\u201d. At stimulus onset, the first percept might be that which is easiest to discover, based on featural proximity (and thus stimulus-parameter dependent), and it is held for relatively longer because time is required for other hypotheses to form. As more sensory information is received and processed, the \u201cneural associations underlying the alternative sound organizations become strong and start to vie for dominance\u201d and \u201cthe probabilities of perceiving different organizations tend to become more balanced with time\u201d",
            "score": 60.645989656448364
        },
        {
            "docid": "42984420_8",
            "document": "Multistable auditory perception . FMRIs have been used to measure the correlation between listening to alternating tones compared to single stream of tones. The posterior regions of the left auditory cortex were modulated by the alternating tones, indicating that there may be areas of the brains responsible for stream segregation.",
            "score": 65.53259754180908
        },
        {
            "docid": "8953842_13",
            "document": "Computational auditory scene analysis . Since the biological auditory system is deeply connected with the actions of neurons, CASA systems also incorporated neural models within the design. Two different models provide the basis for this area. Malsburg and Schneider proposed a neural network model with oscillators to represent features of different streams (synchronized and desynchronized). Wang also presented a model using a network of excitatory units with a global inhibitor with delay lines to represent the auditory scene within the time-frequency.",
            "score": 66.54287695884705
        },
        {
            "docid": "42984420_5",
            "document": "Multistable auditory perception . The temporal dynamics observed in auditory stream segregation are similar to those of bistable visual perception, suggesting that the mechanisms mediating multistable perception, the alternating dominance and suppression of multiple competing interpretations of ambiguous sensory input, might be shared across modalities. Pressnitzer and Hupe analyzed results of an auditory streaming experiment and demonstrated that the perceptual experience that occurred exhibited all three properties of multistable perception found in the visual modality\u2014exclusivity, randomness, and inevitability.",
            "score": 52.227723836898804
        },
        {
            "docid": "739262_12",
            "document": "Neural correlate . Using such design, Nikos Logothetis and colleagues discovered perception-reflecting neurons in the temporal lobe. They created an experimental situation in which conflicting images were presented to different eyes (\"i.e.\", binocular rivalry). Under such conditions, human subjects report bistable percepts: they perceive alternatively one or the other image. Logothetis and colleagues trained the monkeys to report with their arm movements which image they perceived. Interestingly, temporal lobe neurons in Logothetis experiments often reflected what the monkeys' perceived. Neurons with such properties were less frequently observed in the primary visual cortex that corresponds to relatively early stages of visual processing. Another set of experiments using binocular rivalry in humans showed that certain layers of the cortex can be excluded as candidates of the neural correlate of consciousness. Logothetis and colleagues switched the images between eyes during the percept of one of the images. Surprisingly the percept stayed stable. This means that the conscious percept stayed stable and at the same time the primary input to layer 4, which is the input layer, in the visual cortex changed. Therefore layer 4 can not be a part of the neural correlate of consciousness. Mikhail Lebedev and their colleagues observed a similar phenomenon in monkey prefrontal cortex. In their experiments monkeys reported the perceived direction of visual stimulus movement (which could be an illusion) by making eye movements. Some prefrontal cortex neurons represented actual and some represented perceived displacements of the stimulus. Observation of perception related neurons in prefrontal cortex is consistent with the theory of Christof Koch and Francis Crick who postulated that neural correlate of consciousness resides in prefrontal cortex. Proponents of distributed neuronal processing may likely dispute the view that consciousness has a precise localization in the brain.",
            "score": 60.67647075653076
        },
        {
            "docid": "42984420_6",
            "document": "Multistable auditory perception . Exclusivity was satisfied, as there was \u201cspontaneous alternation between mutually exclusive percepts,\u201d and very little time was spent in an \u201cindeterminate\u201d experience. Randomness also characterized the phenomenon, as the first phase of perception is longer in duration than subsequent phases, and then the \u201csteady-state of the temporal dynamics of auditory streaming is purely stochastic with no long-term trend.\u201d Lastly, the percept alternation was inevitable; even though volitional control did reduce suppression of the specified percept, it did not exclude perception of the alternative percept altogether. These similarities between perceptual bistability in the visual and auditory modalities raise the possibility of a common mechanism governing the phenomenon. In Pressnitzer and Hupe\u2019s subjects, the distributions of phase durations in the two modalities were not significantly different, and it has been speculated that the intraparietal sulcus, likely involved in crossmodal integration, could be responsible for bistability in both domains. However, the absence of subject-specific biases across the modalities contradicts the notion that a \u201csingle top-down selection mechanism were the sole determinant of the auditory and visual bistability.\u201d This observation, along with evidence of neural correlates at different stages of processing, instead suggests that competition is distributed and \u201cbased on adaptation and mutual inhibition, at multiple neural processing stages.\u201d",
            "score": 57.59689462184906
        },
        {
            "docid": "41121858_9",
            "document": "Binocular neurons . An energy model, a kind of stimulus-response model, of binocular neurons allows for investigation behind the computational function these disparity tuned cells play in the creation of depth perception. Energy models of binocular neurons involve the combination of monocular receptive fields that are either shifted in position or phase. These shifts in either position or phase allow for the simulated binocular neurons to be sensitive to disparity. The relative contributions of phase and position shifts in simple and complex cells combine together in order to create depth perception of an object in 3-dimensional space. Binocular simple cells are modeled as linear neurons. Due to the linear nature of these neurons, positive and negative values are encoded by two neurons where one neuron encodes the positive part and the other the negative part. This results in the neurons being complements of each other where the excitatory region of one binocular simple cell overlaps with the inhibitory region of another. Each neuron's response is limited such that only one may have a non-zero response for any time. This kind of limitation is called halfwave-rectifing. Binocular complex cells are modeled as energy neurons since they do not have discrete on and off regions in their receptive fields. Energy neurons sum the squared responses of two pairs of linear neurons which must be 90 degrees out of phase. Alternatively, they can also be the sum the squared responses of four halfwave-rectified linear neurons.",
            "score": 50.75751733779907
        },
        {
            "docid": "35982062_6",
            "document": "Biased Competition Theory . There are two major neural pathways that process the information in the visual field; the ventral stream and the dorsal stream. The two pathways run in parallel and are both working simultaneously. The ventral stream is important for object recognition and often referred to as the \u201cwhat\u201d system of the brain; it projects to the inferior temporal cortex. The dorsal stream is important for spatial perception and performance and is referred to as the \u201cwhere\u201d system which projects to the posterior parietal cortex. According to the biased competition theory, an individual\u2019s visual system has limited capacity to process information about multiple objects at any given time. For example, if an individual was presented with two stimuli (objects) and was asked to identify attributes of each object at the same time, the individual\u2019s performance would be worse in comparison to if the objects were presented separately. This suggests multiple objects presented simultaneously in the visual field will compete for neural representation due to limited processing resources. Single cell recording studies conducted by Kastner and Ungerleider examined the neural mechanisms behind the biased competition theory. In their experiment the size of the receptive field's (RF) of neurons within the visual cortex were examined. A single visual stimulus was presented alone in a neuron\u2019s RF, followed with another stimulus presented simultaneously within the same RF. The single \u2018effective\u2019 stimuli produced a low firing rate, whereas the two stimuli presented together produced a high firing rate. The response to the paired stimuli was reduced. This suggests that when two stimuli are presented together within a neuron\u2019s RF, the stimuli are processed in a mutually suppressive manner, rather than being processed independently. This suppression process, according to Kastner and Ungerleider, occurs when two stimuli are presented together because they compete for neural representation, due to limited cognitive processing capacity. The RF experiment suggests that as the number of objects increase, the information available for each object will decrease due to increased neural workload (suppression), and decreased cognitive capacity. In order for an object in the visual field or RF be efficiently processed, there needs to be a way to bias these neurological resources towards the object. Attention prioritizes task relevant objects, biasing this process. For example, this bias can be towards an object which is currently attended to in the visual field or RF, or towards the object that is most relevant to one\u2019s behavior. Functional magnetic resonance imaging (fMRI) has shown that biased competition theory can explain the observed attention effects at a neuronal level. Attention effects bias the internal weight (strengthens connections) of task relevant features toward the attended object. This was shown by Reddy, Kanwisher, and van Rullen who found an increase in oxygenated blood to a specific neuron following a locational cue. Further neurological support comes from neurophysiological studies which have shown that attention results from Top-down biasing, which in turn influences neuronal spiking. In sum, external inputs affect the Top-down guidance of attention, which bias specific neurons in the brain.",
            "score": 60.61562538146973
        },
        {
            "docid": "29354346_5",
            "document": "Change deafness . Another study examined the effect of selective attention on the perception of changes to auditory scenes consisting of multiple naturalistic sounds, and found that auditory perception is limited by attention. In the task, listeners heard two versions of any auditory scene, with one object missing from the second version. Participants were either instructed to attend to a specific object, and report whether that object was missing in the second version of the scene, or to attend to all objects, and report whether any object was missing in the second scene; these are called the directed- and non-directed attention conditions respectively. Results showed that in the absence of an attentional cue, change-detection in auditory scenes consisting of more than about four objects is unreliable, where changes consist of either the disappearance of an object or a change in its location. It is important to note the ambiguity concerning the mechanism that produces the effect of attention on change-deafness, and this study suggests two possibilities. The first is that segregation of the distinct streams composing an auditory scene requires directed attention, meaning that the change-deafness effects observed in the study would reflect a difficulty in perceiving separate auditory scenes in the absence of attentional cues. A second alternative is that complex auditory scenes are initially perceived as consisting of separate streams, and thus change-deafness effects are the result of limits in encoding and storing multiple sets of auditory information for comparison with a subsequent scene.",
            "score": 64.39238727092743
        },
        {
            "docid": "8953380_8",
            "document": "Auditory scene analysis . The job of ASA is to group incoming sensory information to form an accurate mental representation of the individual sounds. When sounds are grouped by the auditory system into a perceived sequence, distinct from other co-occurring sequences, each of these perceived sequences is called an \"auditory stream\". In the real world, if the ASA is successful, a stream corresponds to a distinct environmental sound source producing a pattern that persists over time, such as a person talking, a piano playing, or a dog barking. However, in the lab, by manipulating the acoustic parameters of the sounds, it is possible to induce the perception of one or more auditory streams.",
            "score": 83.86503720283508
        },
        {
            "docid": "2534964_12",
            "document": "Sensory processing . It may seem redundant that we are being provided with multiple sensory inputs about the same object, but that is not necessarily the case. This so-called \"redundant\" information is in fact verification that what we are experiencing is in fact happening. Perceptions of the world are based on models that we build of the world. Sensory information informs these models, but this information can also confuse the models. Sensory illusions occur when these models do not match up. For example, where our visual system may fool us in one case, our auditory system can bring us back to a ground reality. This prevents sensory misrepresentations, because through the combination of multiple sensory modalities, the model that we create is much more robust and gives a better assessment of the situation. Thinking about it logically, it is far easier to fool one sense than it is to simultaneously fool two or more senses.",
            "score": 73.26978063583374
        },
        {
            "docid": "941909_5",
            "document": "Receptive field . The auditory system processes the temporal and spectral (i.e. frequency) characteristics of sound waves, so the receptive fields of neurons in the auditory system are modeled as spectro-temporal patterns that cause the firing rate of the neuron to modulate with the auditory stimulus. Auditory receptive fields are often modeled as spectro-temporal receptive fields (STRFs), which are the specific pattern in the auditory domain that causes modulation of the firing rate of a neuron. Linear STRFs are created by first calculating a spectrogram of the acoustic stimulus, which determines the how the spectral density of the acoustic stimulus changes over time, often using the Short-time Fourier transform (STFT). Firing rate is modeled over time for the neuron, possibly using a peristimulus time histogram if combining over multiple repetitions of the acoustic stimulus. Then, linear regression is used to predict the firing rate of that neuron as a weighted sum of the spectrogram. The weights learned by the linear model are the STRF, and represent the specific acoustic pattern that causes modulation in the firing rate of the neuron. STRFs can also be understood as the transfer function that maps an acoustic stimulus input to a firing rate response output.",
            "score": 72.48703646659851
        },
        {
            "docid": "38041454_5",
            "document": "Richard Parncutt . The psychoacoustic model of harmony proposed by Parncutt in 1989 assumes that the auditory system treats all acoustic input similarly, whether it is a single tone or a musical chord. That is, the \"input\" to his model is a spectrum that can be a single complex tone, or any simultaneity such as a chord. The psychoacoustic effects of auditory masking are then taken into account before a \"template\" of the harmonic series is compared with the input spectrum. This comparison yields a number of properties of the sound, including tonalness, multiplicity, and salience. Tonalness is the degree to which a sound evokes the sensation of a single pitch. As such, individual tones, octave dyads, and major chords are quite high in tonalness. Multiplicity is an estimation of the number of tones in the sound. Intriguingly, for most chords the multiplicity values are less than the actual number of constituent tones \u2013 a prediction that has been validated empirically. Finally, pitch salience is the prominence of a given pitch sensation. For example, the root of a major chord in root position has greater pitch salience than other tones in that chord. Parncut also considers how we perceive successions of sounds, such as chord progressions, and his model makes testable predictions about musical phenomena such as consonance.",
            "score": 87.95458102226257
        },
        {
            "docid": "29354346_7",
            "document": "Change deafness . One study used fMRI data to distinguish neural correlates of physical changes in auditory input (independent of conscious change detection), from those of conscious perception of change (independent of an actual physical change). The study made use of a change deafness paradigm in which participants were exposed to complex auditory scenes consisting of six individual auditory streams differing in pitch, rhythm, and sound source location, and received a cue indicating which stream to attend to. Each participant listened to two consecutively presented auditory scenes after which they were prompted to indicate whether both scenes were identical or not. Functional MRI results revealed that physical change in stimulus was correlated with increased BOLD responses in the right auditory cortex, near the lateral portion of Heschl's gyrus, the first cortical structure to process incoming auditory information, but not in hierarchically higher brain regions. Conscious change detection was correlated with increased coupled responses in the ACC and the right insula, consistent with additional evidence that the anterior insula functions to mediate dynamic interactions between other brain networks involved in attention to external stimuli, forming a salience network with the ACC that identifies salient stimulus events and initiates additional processing. In absence of change detection, this salience network was not activated; however increased activity in other cortical areas suggests that undetected changes are still perceived on some level, but fail to trigger conscious change detection, thus producing the change deafness phenomenon.",
            "score": 62.94894218444824
        },
        {
            "docid": "5664_64",
            "document": "Consciousness . In neuroscience, a great deal of effort has gone into investigating how the perceived world of conscious awareness is constructed inside the brain. The process is generally thought to involve two primary mechanisms: (1) hierarchical processing of sensory inputs, and (2) memory. Signals arising from sensory organs are transmitted to the brain and then processed in a series of stages, which extract multiple types of information from the raw input. In the visual system, for example, sensory signals from the eyes are transmitted to the thalamus and then to the primary visual cortex; inside the cerebral cortex they are sent to areas that extract features such as three-dimensional structure, shape, color, and motion. Memory comes into play in at least two ways. First, it allows sensory information to be evaluated in the context of previous experience. Second, and even more importantly, working memory allows information to be integrated over time so that it can generate a stable representation of the world\u2014Gerald Edelman expressed this point vividly by titling one of his books about consciousness \"The Remembered Present\". In computational neuroscience, Bayesian approaches to brain function have been used to understand both the evaluation of sensory information in light of previous experience, and the integration of information over time. Bayesian models of the brain are probabilistic inference models, in which the brain takes advantage of prior knowledge to interpret uncertain sensory inputs in order to formulate a conscious percept; Bayesian models have successfully predicted many perceptual phenomena in vision and the nonvisual senses.",
            "score": 64.36659920215607
        },
        {
            "docid": "27169449_12",
            "document": "Auditory spatial attention . Alain et al. utilized a delayed match to sample task in which participants held an initial tone in memory, comparing it to a second tone presented 500 ms later. Although the set of stimuli tones remained the same throughout the experiment, task blocks alternated between pitch and spatial comparisons. For example, during pitch comparison blocks, participants were instructed to report whether the second stimulus was higher, lower, or equal in pitch relative to the first pitch, regardless of the two tones spatial locations. Conversely, during spatial comparison blocks, participants were instructed to report whether the second tone was leftward, rightward, or equal in space relative to the first tone, regardless of tone pitch. This task was used in two experiments, one utilizing fMRI and one ERP, to gauge the spatial and temporal properties, respectively, of 'what' and 'where' auditory processing. Comparing the pitch and spatial judgements revealed increased activation in primary auditory cortices and right inferior frontal gyrus during the pitch task, and increased activation in bilateral posterior temporal areas, and inferior and superior parietal cortices during the spatial task. The ERP results revealed divergence between the pitch and spatial tasks at 300-500 ms following the onset of the first stimulus, in the form of increased positivity in inferior frontotemporal regions with the pitch task, and increased positivity over centroparietal regions during the spatial task. This suggested that, similar to what is thought to occur in vision, elements of an auditory scene are split into separate 'what' (ventral) and 'where' (dorsal) pathways, however it was unclear if this similarity is the result of a supramodal division of feature and spatial processes.",
            "score": 49.62326669692993
        },
        {
            "docid": "6147487_45",
            "document": "Neural coding . The correlation coding model of neuronal firing claims that correlations between action potentials, or \"spikes\", within a spike train may carry additional information above and beyond the simple timing of the spikes. Early work suggested that correlation between spike trains can only reduce, and never increase, the total mutual information present in the two spike trains about a stimulus feature. However, this was later demonstrated to be incorrect. Correlation structure can increase information content if noise and signal correlations are of opposite sign. Correlations can also carry information not present in the average firing rate of two pairs of neurons. A good example of this exists in the pentobarbital-anesthetized marmoset auditory cortex, in which a pure tone causes an increase in the number of correlated spikes, but not an increase in the mean firing rate, of pairs of neurons.",
            "score": 46.61151361465454
        },
        {
            "docid": "14408479_47",
            "document": "Biological neuron model . The spiking neuron model by Nossenson & Messer produces the probability of the neuron to fire a spike as a function of either an external or pharmacological stimulus. The model consists of a cascade of a receptor layer model and a spiking neuron model, as shown in Fig 4. The connection between the external stimulus to the spiking probability is made in two steps: First, a receptor cell model translates the raw external stimulus to neurotransmitter concentration, then, a spiking neuron model connects between neurotransmitter concentration to the firing rate (spiking probability). Thus, the spiking neuron model by itself depends on neurotransmitter concentration at the input stage. An important feature of this model is the prediction for neurons firing rate pattern which captures, using a low number of free parameters, the characteristic edge emphasized response of neurons to a stimulus pulse, as shown in Fig. 5. The firing rate is identified both as a normalized probability for neural spike firing, and as a quantity proportional to the current of neurotransmitters released by the cell. The expression for the firing rate takes the following form:",
            "score": 43.27680575847626
        },
        {
            "docid": "51547415_5",
            "document": "Interindividual differences in perception . The McGurk effect is an auditory illusion in which people perceive a different syllable when incongruent audiovisual speech is presented: an auditory syllable \"ba\" is presented while the mouth movement is \"ga\". As a result, the listener perceives the syllable \"da\". However, according to Gentilucci and Cattaneo (2005), not everyone perceives this illusion; only about 26% to 98% of the population are susceptible to this illusion. One of the psychological models that explains the interindividual differences in speech perception is the fuzzy logic model of speech perception According to this model, a categorization process is carried out when processing speech sounds. When listening to a stimulus, the features of the acoustic signal are analyzed. Subsequently, this signal is compared with the features that are stored in the memory; finally the sound is classified into the category that best fits. However, this classification may have a blurred boundary respectively to the category which the sound belongs to. As a result, the final decision may depend on integration of multiple sources of information. When the McGurk effect is presented the auditory and visual components of the speech are separately evaluated before being integrated. In those who perceive the McGurk effect, the visual information has a higher influence on the perception of the ambiguous audiovisual information and thus the sound is classified as \"da\".",
            "score": 89.6883190870285
        },
        {
            "docid": "14408479_42",
            "document": "Biological neuron model . The models in this category were derived following experiments involving natural stimulation such as light, sound, touch, or odor. In these experiments, the spike pattern resulting from each stimulus presentation varies from trial to trial, but the averaged response from several trials often converges to a clear pattern. Consequently, the models in this category generate a probabilistic relationship between the input stimulus to spike occurrences.",
            "score": 44.66820740699768
        },
        {
            "docid": "3382372_2",
            "document": "Normalization model . The normalization model is an influential model of responses of neurons in primary visual cortex. David Heeger developed the model in the early 1990s, and later refined it together with Matteo Carandini and J. Anthony Movshon. The model involves a divisive stage. In the numerator is the output of the classical receptive field. In the denominator, a constant plus a measure of local stimulus contrast. Although the normalization model was initially developed to explain responses in the primary visual cortex, normalization is now thought to operate throughout the visual system, and in many other sensory modalities and brain regions, including the representation of odors, the modulatory effects of visual attention, the encoding of value, and the integration of multisensory information. Its presence in such a diversity of neural systems in multiple species, from invertebrates to mammals, suggests that normalization serves as a canonical neural computation.",
            "score": 32.96708273887634
        },
        {
            "docid": "505717_72",
            "document": "Image segmentation . Pulse-coupled neural networks (PCNNs) are neural models proposed by modeling a cat\u2019s visual cortex and developed for high-performance biomimetic image processing. In 1989, Eckhorn introduced a neural model to emulate the mechanism of a cat\u2019s visual cortex. The Eckhorn model provided a simple and effective tool for studying the visual cortex of small mammals, and was soon recognized as having significant application potential in image processing. In 1994, the Eckhorn model was adapted to be an image processing algorithm by Johnson, who termed this algorithm Pulse-Coupled Neural Network. Over the past decade, PCNNs have been utilized for a variety of image processing applications, including: image segmentation, feature generation, face extraction, motion detection, region growing, noise reduction, and so on. A PCNN is a two-dimensional neural network. Each neuron in the network corresponds to one pixel in an input image, receiving its corresponding pixel\u2019s color information (e.g. intensity) as an external stimulus. Each neuron also connects with its neighboring neurons, receiving local stimuli from them. The external and local stimuli are combined in an internal activation system, which accumulates the stimuli until it exceeds a dynamic threshold, resulting in a pulse output. Through iterative computation, PCNN neurons produce temporal series of pulse outputs. The temporal series of pulse outputs contain information of input images and can be utilized for various image processing applications, such as image segmentation and feature generation. Compared with conventional image processing means, PCNNs have several significant merits, including robustness against noise, independence of geometric variations in input patterns, capability of bridging minor intensity variations in input patterns, etc.",
            "score": 42.12713658809662
        },
        {
            "docid": "49159_9",
            "document": "Hallucination . Auditory hallucinations (also known as \"paracusia\") are the perception of sound without outside stimulus. Auditory hallucinations are the most common type of hallucination. Auditory hallucinations can be divided into two categories: elementary and complex. Elementary hallucinations are the perception of sounds such as hissing, whistling, an extended tone, and more. In many cases, tinnitus is an elementary auditory hallucination. However, some people who experience certain types of tinnitus, especially pulsatile tinnitus, are actually hearing the blood rushing through vessels near the ear. Because the auditory stimulus is present in this situation, it does not qualify it as a hallucination.",
            "score": 53.17474913597107
        },
        {
            "docid": "33818014_26",
            "document": "Nervous system network models . The action potential or the spike does not itself carry any information. It is the stream of spikes, called spike train, that carry the information in its number and pattern of spikes and timing of spikes. The postsynaptic potential can be either positive, the excitatory synapse or negative, inhibitory synapse. In modeling, the postsynaptic potentials received by the dendrites in the postsynaptic neuron are integrated and when the integrated potential exceeds the resting potential, the neuron fires an action potential along its axon. This model is the Integrate-and-Fire (IF) model that was mentioned in Section 2.3. Closely related to IF model is a model called Spike Response Model (SRM) (Gerstner, W. (1995) Pages 738-758) that is dependent on impulse function response convoluted with the input stimulus signal. This forms a base for a large number of models developed for spiking neural networks.",
            "score": 45.637282848358154
        },
        {
            "docid": "21647661_3",
            "document": "Self model . The PSM is an entity that \u201cactually exists, not only as a distinct theoretical entity but something that will be empirically discovered in the future- for instance, as a specific stage of the global neural dynamics in the human brain\u201d. Involved in the PSM are three phenomenal properties that must occur in order to explain the concept of the self. The first is mineness, \u201ca higher order property of particular forms of phenomenal content,\u201d or the idea of ownership. The second is perspectivalness, which is \u201ca global, structural property of phenomenal space as a whole\u201d. More simply, it is what is commonly referred to as the ecological self, the immovable center of perception. The third phenomenal property is selfhood, which is \u201cthe phenomenal target property\u201d or the idea of the self over time. It is the property of phenomenal selfhood that plays the most important role in creating the fictional self and the first person perspective. Metzinger defines the first person perspective as the \u201cexistence of single coherent and temporally stable model of reality which is representationally centered around or on a single coherent and temporally stable phenomenal subject\u201d. The first-person perspective can be non-conceptual and is autonomously active due to the constant reception of perceptual information by the brain. The brain, specifically the brainstem and hypothalamus, processes this information into representational content, namely linguistic reflections. The PSM then uses this representational content to attribute phenomenal states to our perceived objects and ourselves. We are thus what Metzinger calls na\u00efve realists, who believe we are perceiving reality directly when in actuality we are only perceiving representations of reality. The data structures and transport mechanisms of the data are \u201ctransparent\u201d so that we can introspect on our representations of perceptions, but cannot introspect on the data or mechanisms themselves. These systemic representational experiences are then connected by subjective experience to generate the phenomenal property of selfhood. Subjective experience is the result of the Phenomenal Model of Intentionality Relationship (PMIR). The PMIR is a \u201cconscious mental model, and its content is an ongoing, episodic subject-object relation\u201d. The model is a result of the combination of our unique set of sensory receptors that acquire input, our unique set of experiences that shape connections within the brain, and our unique positions in space that give our perception perspectivalness.",
            "score": 71.11352586746216
        }
    ],
    "r": [
        {
            "docid": "32116125_4",
            "document": "Amblyaudia . Amblyaudia is a deficit in binaural integration of environmental information entering the auditory system. It is a disorder related to brain organization and function rather than what is typically considered a \u201chearing loss\u201d (damage to the cochlea). It may be genetic or developmentally acquired or both. When animals are temporarily deprived of hearing from an early age, profound changes occur in the brain. Specifically, cell sizes in brainstem nuclei are reduced, the configuration of brainstem dendrites are altered and neurons respond in different ways to sounds presented to both the deprived and non-deprived ears (in cases of asymmetric deprivation). This last point is particularly important for listening tasks that require inputs from two ears to perform well. There are multiple auditory functions that rely on the computation of well calibrated inputs from the two ears. Chief among these is the ability to localize sound sources and separate what we want to hear from a background of noise. In the brainstem, the auditory system compares the timing and levels of sounds between the two ears to encode the location of sound sources (sounds that originate from our right as opposed to left side are louder and arrive earlier in our right ear). This ability to separate sound sources not only helps us locate the trajectories of moving objects, but also to separate different sound sources in noisy environments.",
            "score": 116.32877349853516
        },
        {
            "docid": "8953380_9",
            "document": "Auditory scene analysis . One example of this is the phenomenon of streaming, also called \"stream segregation.\" If two sounds, A and B, are rapidly alternated in time, after a few seconds the perception may seem to \"split\" so that the listener hears two rather than one stream of sound, each stream corresponding to the repetitions of one of the two sounds, for example, A-A-A-A-, etc. accompanied by B-B-B-B-, etc. The tendency towards segregation into separate streams is favored by differences in the acoustical properties of sounds A and B. Among the differences classically shown to promote segregation are those of frequency (for pure tones), fundamental frequency (for complex tones), frequency composition, source location. But it has been suggested that about any systematic perceptual difference between two sequences can elicit streaming, provided the speed of the sequence is sufficient.",
            "score": 116.24772644042969
        },
        {
            "docid": "8953380_11",
            "document": "Auditory scene analysis . Many experiments have studied the segregation of more complex patterns of sound, such as a sequence of high notes of different pitches, interleaved with low ones. In such sequences, the segregation of co-occurring sounds into distinct streams has a profound effect on the way they are heard. Perception of a melody is formed more easily if all its notes fall in the same auditory stream. We tend to hear the rhythms among notes that are in the same stream, excluding those that are in other streams. Judgments of timing are more precise between notes in the same stream than between notes in separate streams. Even perceived spatial location and perceived loudness can be affected by sequential grouping.",
            "score": 111.74987030029297
        },
        {
            "docid": "14339999_6",
            "document": "Virtual pitch . Terhardt rejected the idea of periodicity pitch, because it was not consistent with empirical data on pitch perception, e.g. measurements of the gradual shift of the virtual pitch of a complex tone with a missing fundamental when the partials were gradually shifted. Terhardt instead broke pitch perception into two steps: auditory frequency analysis in the inner ear, and harmonic pitch pattern recognition in the brain. The inner ear effectively performs a running frequency analysis of incoming sounds - otherwise we would not be able to hear out spectral pitches within a complex tone. Physiologically, each spectral pitch depends on both temporal and spectral aspects (i.e. periodicity of the waveform and position of excitation on the basilar membrane), but in Terhardt's approach the spectral pitch itself is a purely experiential parameter, not a physical parameter: it is the outcome of a psychoacoustical experiment in which the conscious listener plays an active role. Psychoacoustic measurements and models can predict which partials are \"perceptually relevant\" in a given complex tone; they are perceptually relevant if you can hear a difference in the whole sound if the frequency or amplitude of a partial is changed). The ear has evolved to separate spectral frequencies, because due to reflection and superposition in everyday environments spectral frequencies are more reliably carriers of environmental information than spectral amplitudies, which in turn are more reliable carriers of environmentally relevant information than phase relationships between partials (when perceived monoaurally). On this basis, Terhardt proposed that spectral pitches - which are what the listener experiences when hearing out partials (as opposed to the physical partials themselves) - are the only information available to the brain for the purpose of extracting virtual pitches. The \"pitch extraction\" process then involves the recognition of incomplete harmonic patterns and happens in neural networks.",
            "score": 103.28612518310547
        },
        {
            "docid": "35988494_3",
            "document": "Selective auditory attention . The cocktail party problem was first brought up in 1953 by Colin Cherry. This common problem is how our minds solves the issue of knowing what in the auditory scene is important and combining those in a coherent whole, such as the problem of how we can perceive our friend talking in the midst of a crowded cocktail party. He suggested that the auditory system can filter sounds being heard. Physical characteristics of the auditory information such as speaker's voice or location can improve a person's ability to focus on certain stimuli even if there is other auditory stimuli present. Cherry also did work with shadowing which involves different information being played into both ears and only one ear's information can be processed and remembered (Eysneck, 2012, p.\u00a084). Another psychologist, Albert Bregman, came up with the auditory scene analysis model. The model has three main characteristics: segmentation, integration, and segregation. Segmentation involves the division of auditory messages into segments of importance. The process of combining parts of an auditory message to form a whole is associated with integration. Segregation is the separation of important auditory messages and the unwanted information in the brain. It is important to note that Bregman also makes a link back to the idea of perception. He states that it is essential for one to make a useful representation of the world from sensory inputs around us. Without perception, an individual will not recognize or have the knowledge of what is going on around them. While Begman's seminal work is critical to understanding selective auditory attention, his studies did not focus on the way in which an auditory message is selected, if and when it was correctly segregated from other sounds in a mixture, which is a critical stage of selective auditory attention. Inspired in part by Bregman's work, a number of researchers then set out to link directly work on auditory scene analysis to the processes governing attention, including Maria Chait, Mounya Elhilali, Shihab Shamma, and Barbara Shinn-Cunningham.",
            "score": 98.93616485595703
        },
        {
            "docid": "861492_12",
            "document": "Intercultural communication . The Intercultural Praxis Model by Kathryn Sorrells, PH.D shows us how to navigate through the complexities of cultural differences along with power differences. This model will help you understand who you are as an individual, and how you can better communicate with others that may be different from you. In order to continue in living in globalized society one can use this Praxis model to understand cultural differences (based on race, ethnicity, gender, class, sexual orientation, religion, nationality, etc.) within the institutional and historical systems of power. Intercultural Communication Praxis Model requires for us to respond to someone who comes from a different culture than us, in the most opening way we can. The media may always want to influence what we think of other cultures and also what we should think about our own selves. However it is important, we educate ourselves, and learn how to communicate with others through Sorrells Praxis Model.",
            "score": 98.82141876220703
        },
        {
            "docid": "9736652_7",
            "document": "Auditory masking . If two sounds of two different frequencies are played at the same time, two separate sounds can often be heard rather than a combination tone. The ability to hear frequencies separately is known as \"frequency resolution\" or \"frequency selectivity\". When signals are perceived as a combination tone, they are said to reside in the same \"critical bandwidth\". This effect is thought to occur due to filtering within the cochlea, the hearing organ in the inner ear. A complex sound is split into different frequency components and these components cause a peak in the pattern of vibration at a specific place on the cilia inside the basilar membrane within the cochlea. These components are then coded independently on the auditory nerve which transmits sound information to the brain. This individual coding only occurs if the frequency components are different enough in frequency, otherwise they are in the same critical band and are coded at the same place and are perceived as one sound instead of two.",
            "score": 98.09562683105469
        },
        {
            "docid": "22751270_6",
            "document": "Illusory conjunctions . Auditory illusory conjunctions occur either when two sounds are presented in different positions in space, but a single sound is heard, or when two different sounds are presented in different positions in space, but some of them are heard in the wrong spatial location. In the octave illusion, the listener is presented via earphones with a 20-second sequence consisting of two alternating tones that are an octave apart, and are repeatedly presented in alternation. The tones are 250 ms in duration. The same sequence is presented to the two ears, but when the right ear receives the high tone the left ear receives the low tone, and vice versa. Most listeners hear this sequence as a single tone that repeatedly changes both in pitch and in location. It has been suggested that time limitations contribute to this auditory illusory conjunction but see other explanations in terms of separate 'what' and 'where' pathways. In the scale illusion, the listener is presented via headphones with a scale with alternating tones switching from ear to ear. The scale is presented in both ascending and descending form, such that when a tone form the ascending scale is in the right ear, a tone from the descending scale is in the left ear. This gives rise to illusory conjunctions of pitch and location, such that all the higher tones are heard in one ear and all the lower tones in the other ear. Similar illusory conjunctions give rise to the chromatic illusion, the glissando illusion, and the cambiata illusion.",
            "score": 94.72428894042969
        },
        {
            "docid": "44673348_4",
            "document": "Ernst Terhardt . According to Terhardt\u2019s theory of pitch perception, pitch perception can be divided into two separate stages: auditory spectral analysis and harmonic pitch pattern recognition. In the first stage, the inner ear (cochlea and basilar membrane) performs a running spectral analysis of the incoming signal. The parameters of this analysis (e.g. the effective length and shape of the analysis window) depend directly on physiology and indirectly on the co-evolution of ear and voice as our human and prehuman ancestors interacted with their social and physical environments. The output of this first stage is called a spectral pitch pattern, when it is determined by psychoacoustic experiments in which listeners make subjective judgments, matching the perceived pitch of a pure reference tone to that of a successively presented complex tone. The spectral pitches differ in perceptual salience since their sound pressure levels differ physically, they lie at different distances above the threshold of hearing, they mask each other (and therefore lie at different distances above the masked threshold), and may or may not lie in a region to which the ear is particularly sensitive (a dominance region of pitch perception). A cornerstone of Terhardt\u2019s is approach is the idea that because spectral pitches are subjective, we must not jump to conclusions about the relationship between them and their physiological (physical) foundations in the ear and brain.",
            "score": 92.90506744384766
        },
        {
            "docid": "5366050_62",
            "document": "Speech perception . The fuzzy logical theory of speech perception developed by Dominic Massaro proposes that people remember speech sounds in a probabilistic, or graded, way. It suggests that people remember descriptions of the perceptual units of language, called prototypes. Within each prototype various features may combine. However, features are not just binary (true or false), there is a fuzzy value corresponding to how likely it is that a sound belongs to a particular speech category. Thus, when perceiving a speech signal our decision about what we actually hear is based on the relative goodness of the match between the stimulus information and values of particular prototypes. The final decision is based on multiple features or sources of information, even visual information (this explains the McGurk effect). Computer models of the fuzzy logical theory have been used to demonstrate that the theory's predictions of how speech sounds are categorized correspond to the behavior of human listeners.",
            "score": 91.71774291992188
        },
        {
            "docid": "433584_4",
            "document": "McGurk effect . Vision is the primary sense for humans, but speech perception is multimodal, which means that it involves information from more than one sensory modality, in particular, audition and vision. The McGurk effect arises during phonetic processing because the integration of audio and visual information happens early in speech perception. The McGurk effect is very robust; that is, knowledge about it seems to have little effect on one's perception of it. This is different from certain optical illusions, which break down once one 'sees through' them. Some people, including those that have been researching the phenomenon for more than twenty years, experience the effect even when they are aware that it is taking place. With the exception of people who can identify most of what is being said from speech-reading alone, most people are quite limited in their ability to identify speech from visual-only signals. A more extensive phenomenon is the ability of visual speech to increase the intelligibility of heard speech in a noisy environment. Visible speech can also alter the perception of perfectly audible speech sounds when the visual speech stimuli are mismatched with the auditory speech. Normally, speech perception is thought to be an auditory process; however, our use of information is immediate, automatic, and, to a large degree, unconscious and therefore, despite what is widely accepted as true, speech is not only something we hear. Speech is perceived by all of the senses working together (seeing, touching, and listening to a face move). The brain is often unaware of the separate sensory contributions of what it perceives. Therefore, when it comes to recognizing speech the brain cannot differentiate whether it is seeing or hearing the incoming information.",
            "score": 91.07901763916016
        },
        {
            "docid": "2263473_17",
            "document": "Volley theory . A fundamental frequency is the lowest frequency of a harmonic. In some cases, sound can have all the frequencies of a harmonic but be missing the fundamental frequency, this is known as missing fundamental. When listening to a sound with a missing fundamental, the human brain still receives information for all frequencies, including the fundamental frequency which does not exist in the sound. This implies that sound is encoded by neurons firing at all frequencies of a harmonic, therefore, the neurons must be locked in some way to result in the hearing of one sound. Congenital deafness or sensorineural hearing loss is an often used model for the study of the inner ear regarding pitch perception and theories of hearing in general. Frequency analysis of these individuals\u2019 hearing has given insight on common deviations from normal tuning curves, excitation patterns, and frequency discrimination ranges. By applying pure or complex tones, information on pitch perception can be obtained. In 1983, it was shown that subjects with low frequency sensorineural hearing loss demonstrated abnormal psychophysical tuning curves. Changes in the spatial responses in these subjects showed similar pitch judgment abilities when compared to subjects with normal spatial responses. This was especially true regarding low frequency stimuli. These results suggest that the place theory of hearing does not explain pitch perception at low frequencies, but that the temporal (frequency) theory is more likely. This conclusion is due to the finding that when deprived of basilar membrane place information, these patients still demonstrated normal pitch perception. Computer models for pitch perception and loudness perception are often used during hearing studies on acoustically impaired subjects. The combination of this modeling and knowledge of natural hearing allows for better development of hearing aids.",
            "score": 90.99832916259766
        },
        {
            "docid": "8953842_15",
            "document": "Computational auditory scene analysis . While many models consider the audio signal as a complex combination of different frequencies, modeling the auditory system can also require consideration for the neural components. By taking a holistic process, where a stream (of feature-based sounds) correspond to neuronal activity distributed in many brain areas, the perception of the sound could be mapped and modeled. Two different solutions have been proposed to the binding of the audio perception and the area in the brain. Hierarchical coding models many cells to encode all possible combinations of features and objects in the auditory scene. Temporal or oscillatory correlation addressing the binding problem by focusing on the synchrony and desynchrony between neural oscillations to encode the state of binding among the auditory features. These two solutions are very similar to the debacle between place coding and temporal coding. While drawing from modeling neural components, another phenomenon of ASA comes into play with CASA systems: the extent of modeling neural mechanisms. The studies of CASA systems have involved modeling some known mechanisms, such as the bandpass nature of cochlear filtering and random auditory nerve firing patterns, however, these models may not lead to finding new mechanisms, but rather give an understanding of purpose to the known mechanisms.",
            "score": 90.99647521972656
        },
        {
            "docid": "51547415_5",
            "document": "Interindividual differences in perception . The McGurk effect is an auditory illusion in which people perceive a different syllable when incongruent audiovisual speech is presented: an auditory syllable \"ba\" is presented while the mouth movement is \"ga\". As a result, the listener perceives the syllable \"da\". However, according to Gentilucci and Cattaneo (2005), not everyone perceives this illusion; only about 26% to 98% of the population are susceptible to this illusion. One of the psychological models that explains the interindividual differences in speech perception is the fuzzy logic model of speech perception According to this model, a categorization process is carried out when processing speech sounds. When listening to a stimulus, the features of the acoustic signal are analyzed. Subsequently, this signal is compared with the features that are stored in the memory; finally the sound is classified into the category that best fits. However, this classification may have a blurred boundary respectively to the category which the sound belongs to. As a result, the final decision may depend on integration of multiple sources of information. When the McGurk effect is presented the auditory and visual components of the speech are separately evaluated before being integrated. In those who perceive the McGurk effect, the visual information has a higher influence on the perception of the ambiguous audiovisual information and thus the sound is classified as \"da\".",
            "score": 89.68832397460938
        },
        {
            "docid": "11698_62",
            "document": "Franz Boas . In short, he shifted attention to the \"perception\" of different sounds. Boas begins by raising an empirical question: when people describe one sound in different ways, is it because they cannot perceive the difference, or might there be another reason? He immediately establishes that he is not concerned with cases involving perceptual deficit\u2014the aural equivalent of color-blindness. He points out that the question of people who describe one sound in different ways is comparable to that of people who describe different sounds in one way. This is crucial for research in descriptive linguistics: when studying a new language, how are we to note the pronunciation of different words? (in this point, Boas anticipates and lays the groundwork for the distinction between phonemics and phonetics.) People may pronounce a word in a variety of ways and still recognize that they are using the same word. The issue, then, is not \"that such sensations are not recognized in their individuality\" (in other words, people recognize differences in pronunciations); rather, it is that sounds \"are classified according to their similarity\" (in other words, that people classify a variety of perceived sounds into one category). A comparable visual example would involve words for colors. The English word \"green\" can be used to refer to a variety of shades, hues, and tints. But there are some languages that have no word for \"green\". In such cases, people might classify what we would call \"green\" as either \"yellow\" or \"blue\". This is not an example of color-blindness\u2014people can perceive differences in color, but they categorize similar colors in a different way than English speakers.",
            "score": 88.52090454101562
        },
        {
            "docid": "38041454_5",
            "document": "Richard Parncutt . The psychoacoustic model of harmony proposed by Parncutt in 1989 assumes that the auditory system treats all acoustic input similarly, whether it is a single tone or a musical chord. That is, the \"input\" to his model is a spectrum that can be a single complex tone, or any simultaneity such as a chord. The psychoacoustic effects of auditory masking are then taken into account before a \"template\" of the harmonic series is compared with the input spectrum. This comparison yields a number of properties of the sound, including tonalness, multiplicity, and salience. Tonalness is the degree to which a sound evokes the sensation of a single pitch. As such, individual tones, octave dyads, and major chords are quite high in tonalness. Multiplicity is an estimation of the number of tones in the sound. Intriguingly, for most chords the multiplicity values are less than the actual number of constituent tones \u2013 a prediction that has been validated empirically. Finally, pitch salience is the prominence of a given pitch sensation. For example, the root of a major chord in root position has greater pitch salience than other tones in that chord. Parncut also considers how we perceive successions of sounds, such as chord progressions, and his model makes testable predictions about musical phenomena such as consonance.",
            "score": 87.95458221435547
        },
        {
            "docid": "1379269_3",
            "document": "Musical acoustics . Whenever two different pitches are played at the same time, their sound waves interact with each other \u2013 the highs and lows in the air pressure reinforce each other to produce a different sound wave. Any repeating sound wave that is not a sine wave can be modeled by many different sine waves of the appropriate frequencies and amplitudes (a frequency spectrum). In humans the hearing apparatus (composed of the ears and brain) can usually isolate these tones and hear them distinctly. When two or more tones are played at once, a variation of air pressure at the ear \"contains\" the pitches of each, and the ear and/or brain isolate and decode them into distinct tones.",
            "score": 85.43498229980469
        },
        {
            "docid": "25977026_9",
            "document": "P. J. Snow . (8 sections; 210,000 words; 38 original figures). The Human Psyche addresses and answers a host of highly significant questions relating to human existence and survival, at both the individuals and social level. Examples of some of these questions are as follows: What is the biological origin of consciousness, thought and intelligence? What is the relationship between emotions and instinct? How did the human psyche evolve from the brains of our animal relatives? What and where is the human mind? How is the human mind organized to produce different cognitive attributes and different personalities? Why do we need a God and where is the human soul? What conjures within us the experience of love or, alternatively, casts us into the oblivion of darkness? What are the origins of good and evil? How are these extremes of human experience represented in religion? How do we imagine things and how does our imagination differ from our dreams? Do all people think in the same way and, if not, are there greater differences between people from different societies, countries, cultures or races than between members of the same community? Where does belief and faith come from? What is the origin of racial conflict? Why is it so easy for politicians to persuade us to go to war? Why does the aggression of battle always result in rape, plunder and pillage? Are we motivated by our thoughts or our emotions? Is there any basis for the politically correct view that all humans have identical brains and that all behavioral and cultural diversity is learned? Why is it said that the longest road is that between the heart and the head?",
            "score": 84.54070281982422
        },
        {
            "docid": "1781678_2",
            "document": "Cocktail party effect . The cocktail party effect is the phenomenon of the brain's ability to focus one's auditory attention (an effect of selective attention in the brain) on a particular stimulus while filtering out a range of other stimuli, as when a partygoer can focus on a single conversation in a noisy room. Listeners have the ability to both segregate different stimuli into different streams, and subsequently decide which streams are most pertinent to them. Thus, it has been proposed that one's sensory memory subconsciously parses all stimuli, identifying discrete pieces of information and classifying them by salience. This effect is what allows most people to \"tune into\" a single voice and \"tune out\" all others. It may also describe a similar phenomenon that occurs when one may immediately detect words of importance originating from unattended stimuli, for instance hearing one's name among a wide range of auditory input.",
            "score": 83.9439468383789
        },
        {
            "docid": "8953380_8",
            "document": "Auditory scene analysis . The job of ASA is to group incoming sensory information to form an accurate mental representation of the individual sounds. When sounds are grouped by the auditory system into a perceived sequence, distinct from other co-occurring sequences, each of these perceived sequences is called an \"auditory stream\". In the real world, if the ASA is successful, a stream corresponds to a distinct environmental sound source producing a pattern that persists over time, such as a person talking, a piano playing, or a dog barking. However, in the lab, by manipulating the acoustic parameters of the sounds, it is possible to induce the perception of one or more auditory streams.",
            "score": 83.86503601074219
        },
        {
            "docid": "433584_2",
            "document": "McGurk effect . The McGurk effect is a perceptual phenomenon that demonstrates an interaction between hearing and vision in speech perception. The illusion occurs when the auditory component of one sound is paired with the visual component of another sound, leading to the perception of a third sound. The visual information a person gets from seeing a person speak changes the way they hear the sound. If a person is getting poor quality auditory information but good quality visual information, they may be more likely to experience the McGurk effect. Integration abilities for audio and visual information may also influence whether a person will experience the effect. People who are better at sensory integration have been shown to be more susceptible to the effect. Many people are affected differently by the McGurk effect based on many factors, including brain damage and other disorders. It was first described in 1976 in a paper by Harry McGurk and John MacDonald, titled \"Hearing Lips and Seeing Voices\" in \"Nature\" (23 Dec 1976). This effect was discovered by accident when McGurk and his research assistant, MacDonald, asked a technician to dub a video with a different phoneme from the one spoken while conducting a study on how infants perceive language at different developmental stages. When the video was played back, both researchers heard a third phoneme rather than the one spoken or mouthed in the video.",
            "score": 83.75605773925781
        },
        {
            "docid": "38967181_24",
            "document": "Sednaya Prison . The lack of accessibility to reports from journalists and monitoring groups has made the prison a deep black hole which no outsider knows anything about. The only sources the world has on the incidents inside the Sednaya prison derive from the memories of detainees that have made it out from what has been referred to as hell on Earth. The memories they bear on are what we have in order to understand what is going on within Sednaya. In April 2016, Amnesty International and Forensic Architecture traveled to Turkey to meet five Sednaya survivors. The researchers used architectural and acoustic modeling to reconstruct the prison and the survivors\u2019 experiences at detention. As there are no images of the prison and because the prisoners were held in darkness under brutally enforced silence, researchers had to depend entirely on their memories and acute experience of sound, footsteps, door opening and locking and water dripping in the pipes among other things. The fact that prisoners did barely see daylight, they were, consequently, forced to develop an acute relation to sound. Having to cover their eyes with their hands whenever a guard entered the room made them become attuned to the smallest sounds. In a video interview, a former Sednaya detainee says \"You try to build an image based on the sounds you hear. You know the person by the sound of his footsteps. You can tell the food times by the sound of the bowl. If you hear screaming, you know newcomers have arrived. When there is no screaming, we know they are accustomed to Sednaya.\" Sound became the instrument by which inmates navigated and measured their environment. Therefore, sound also became one of the essential tools with which the prison could be digitally reconstructed. The sound artist Lawrence Abu Hamdan used a technique of \u201cecho profiling\u201d which made it possible for him to decide the size of cells, stairwells, and corridors. He played different sound reflections and asked former inmates to match these tones of different decibel levels to the levels of specific incidents inside the prison.",
            "score": 83.30026245117188
        },
        {
            "docid": "18994087_31",
            "document": "Sound . Timbre is perceived as the quality of different sounds (e.g. the thud of a fallen rock, the whir of a drill, the tone of a musical instrument or the quality of a voice) and represents the pre-conscious allocation of a sonic identity to a sound (e.g. \u201cit\u2019s an oboe!\"). This identity is based on information gained from frequency transients, noisiness, unsteadiness, perceived pitch and the spread and intensity of overtones in the sound over an extended time frame. The way a sound changes over time (see figure 4) provides most of the information for timbre identification. Even though a small section of the wave form from each instrument looks very similar (see the expanded sections indicated by the orange arrows in figure 4), differences in changes over time between the clarinet and the piano are evident in both loudness and harmonic content. Less noticeable are the different noises heard, such as air hisses for the clarinet and hammer strikes for the piano. Sonic texture relates to the number of sound sources and the interaction between them. The word 'texture', in this context, relates to the cognitive separation of auditory objects. In music, texture is often referred to as the difference between unison, polyphony and homophony, but it can also relate (for example) to a busy cafe; a sound which might be referred to as 'cacophony'. However texture refers to more than this. The texture of an orchestral piece is very different to the texture of a brass quintet because of the different numbers of players. The texture of a market place is very different to a school hall because of the differences in the various sound sources.",
            "score": 83.14690399169922
        },
        {
            "docid": "44673348_7",
            "document": "Ernst Terhardt . Terhardt\u2019s approach to acoustic communication is based on Karl Popper\u2019s theory of three worlds according to which reality is either physical, experiential (perception, sensations, emotions) or abstract (thoughts, knowledge, information, culture). Terhardt maintains that these three aspects of acoustic communication must be carefully separated before we empirically explore the relationships between them. In the physical world, we consider the physics of sound sources such as the voice and musical instruments; auditory environments including reflectors; electroacoustic systems such as microphones and loudspeakers; and the ear and brain, considered as a purely physical system. Sound is a signal that is that is analysed by the ear; to understand this process, we need foundations of signal processing. To understand auditory perception, we perform psychoacoustic experiments, which are generally about relationships between and among Popper\u2019s three worlds.",
            "score": 82.88410949707031
        },
        {
            "docid": "8953380_4",
            "document": "Auditory scene analysis . Sound reaches the ear and the eardrum vibrates as a whole. This signal has to be analyzed (in some way). Bregman's ASA model proposes that sounds will either be heard as \"integrated\" (heard as a whole \u2013 much like harmony in music), or \"segregated\" into individual components (which leads to counterpoint). For example, a bell can be heard as a 'single' sound (integrated), or some people are able to hear the individual components \u2013 they are able to segregate the sound. This can be done with chords where it can be heard as a 'color', or as the individual notes. Natural sounds, such as the human voice, musical instruments, or cars passing in the street, are made up of many frequencies, which contribute to the perceived quality (like timbre) of the sounds. When two or more natural sounds occur at once, all the components of the simultaneously active sounds are received at the same time, or overlapped in time, by the ears of listeners. This presents their auditory systems with a problem: which parts of the sound should be grouped together and treated as parts of the same source or object? Grouping them incorrectly can cause the listener to hear non-existent sounds built from the wrong combinations of the original components.",
            "score": 82.70462036132812
        },
        {
            "docid": "53472_7",
            "document": "Illusion . An auditory illusion is an illusion of hearing, the auditory equivalent of an optical illusion: the listener hears either sounds which are not present in the stimulus, or \"impossible\" sounds. In short, audio illusions highlight areas where the human ear and brain, as organic, makeshift tools, differ from perfect audio receptors (for better or for worse). One example of an auditory illusion is a shepard tone.",
            "score": 82.50146484375
        },
        {
            "docid": "12001969_4",
            "document": "The Altar and the Door . The main ideas for \"The Altar and the Door\" were inspired roughly eighteen months before the album's release. Lead vocalist Mark Hall and his co-youth pastor were encouraged by one of their students to look at MySpace. According to Hall, \u201cIt wasn\u2019t any major surprise, but we did see a lot of kids who had two worlds going on. MySpace can be Spring Break for the brain, this place you can go and not think anyone\u2019s ever going to find out. Kids would be listed as Christians and then show their porn star name or what kind of kisser they are. They were just presenting so many contradictions on one page. The temptation was to just get upset and think that\u2019s terrible. But MySpace isn\u2019t really a big problem\u00a0\u2013 it\u2019s just revealing what the problem is\". He noted that this situation isn't unique to teenagers and that while at church \"we [Christians] want to serve [God]\" but when \"we get out there in the world\u00a0... it\u2019s just different. We want to be accepted; we want friends. The compromises start coming in small little increments until you\u2019re just kind of out there. Church becomes more of a guilt activator than a place to go to be with the Lord. It\u2019s a nasty place to live, and we all live there\". Hall says that \"When we\u2019re at the altar, everything\u2019s clear, and it all makes perfect sense, and we know how to live. We know what\u2019s right and what\u2019s wrong. The struggle is getting this life at the altar out the door\u00a0... That\u2019s the problem; we\u2019re finding ourselves somewhere in the middle\". Hall elaborated in a separate interview that \"Somewhere between the altar and the door, it all leaks out and I'm out here wondering what to do, rationalising things instead of living the life that's in me. So the struggle that we have as believers is trying to get those truths (that are) in our heads and highlighted in our Bibles out to our hands and feet. The songs are all the things that happen in the middle of that\". Although Hall says that he \"always think[s] lyrics first\", he felt that \"Once we [Casting Crowns] got into the recording I knew we were in for something different, a more progressive approach to the music. These songs sounded different in my head; they've been a big challenge for us as a band. And the music definitely sets the tone for the whole project\".",
            "score": 82.49895477294922
        },
        {
            "docid": "4220231_8",
            "document": "Evolutionary musicology . The evolutionary switch to bipedalism may have influenced the origins of music. The background is that noise of locomotion and ventilation may mask critical auditory information. Human locomotion is likely to produce more predictable sounds than those of non-human primates. Predictable locomotion sounds may have improved our capacity of entrainment to external rhythms and to feel the beat in music. A sense of rhythm could aid the brain in distinguishing among sounds arising from discrete sources and also help individuals to synchronize their movements with one another. Synchronization of group movement may improve perception by providing periods of relative silence and by facilitating auditory processing. The adaptive value of such skills to early human ancestors may have been keener detection of prey or stalkers and enhanced communication. Thus, bipedal walking may have influenced the development of entrainment in humans and thereby the evolution of rhythmic abilities. Primitive hominids lived and moved around in small groups. The noise generated by the locomotion of two or more individuals can result in a complicated mix of footsteps, breathing, movements against vegetation, echoes, etc. The ability to perceive differences in pitch, rhythm, and harmonies, i.e. \u201cmusicality,\u201d could help the brain to distinguish among sounds arising from discrete sources, and also help the individual to synchronize movements with the group. Endurance and an interest in listening might, for the same reasons, have been associated with survival advantages eventually resulting in adaptive selection for rhythmic and musical abilities and reinforcement of such abilities. Listening to music seems to stimulate release of dopamine. Rhythmic group locomotion combined with attentive listening in nature may have resulted in reinforcement through dopamine release. A primarily survival-based behavior may eventually have attained similarities to dance and music, due to such reinforcement mechanisms . Since music may facilitate social cohesion, improve group effort, reduce conflict, facilitate perceptual and motor skill development, and improve trans-generational communication, music-like behavior may at some stage have become incorporated into human culture.",
            "score": 82.16040802001953
        },
        {
            "docid": "4560168_22",
            "document": "Cherish (group) . \"Unappreciated\" features mainly hip hop and R&B tracks. During an interview about the album, when asked about its composition, the group said: \"You can expect a lot of realness, [and] we're trying to bring real R&B back to the world. Right now, R&B is not a genre anymore. It's pop, it's hip-hop, it's other things. I wanna bring back rhythm and blues.\" During a different interview, the girls talked about how different this album was from their shelved debut: \"Well it's very different coming out now because with this album we have more creative control. When we came out in 2003, we were young and we didn't have much of a say in what went down. But this time around, we were able to write every song on the album, which makes this album very personal for all of us.\" They later added on, \"From this album you can expect realness. Expect to hear all of our individual voices. A lot of crunk tracks like our first single 'Do It to It'. Also expect a lot of a cappella singing.\"",
            "score": 82.13548278808594
        },
        {
            "docid": "42984420_9",
            "document": "Multistable auditory perception . A problem of large behavioral importance is the question of how to group auditory stimuli. When a continuous stream of auditory information is received, numerous alternative interpretations are possible, but individuals are only consciously aware of one percept at a time. For this to occur, the auditory system must segregate and group incoming sounds, the goal being to \u201cconstruct, modify, and maintain dynamic representations of putative objects within its environment\u201d. It has been suggested that this process of binding sound events into groups is driven by different levels of similarities. One principle for binding is based on the perceptual similarity between individual events. Sounds that share many or all of their acoustic features are more likely to have been emitted by the same source, and thus are more likely to be linked to form a \u201cproto-object\u201d. The other principle for binding is based on the sequential predictability of sound events. If events reliably follow each other, it is also more likely that they have a common underlying cause.",
            "score": 82.1331558227539
        },
        {
            "docid": "4643899_5",
            "document": "Categorical perception . According to the (now abandoned) motor theory of speech perception, the reason people perceive an abrupt change between /ba/ and /pa/ is that the way we hear speech sounds is influenced by how people produce them when they speak. What is varying along this continuum is voice-onset-time: the \"b\" in /ba/ is voiced and the \"p\" in /pa/ is not. But unlike the synthetic \"morphing\" apparatus, people's natural vocal apparatus is not capable of producing anything in between ba and pa. So when one hears a sound from the voicing continuum, their brain perceives it by trying to match it with what it would have had to do to produce it. Since the only thing they can produce is /ba/ or /pa/, they will perceive any of the synthetic stimuli along the continuum as either /ba/ or /pa/, whichever it is closer to. A similar CP effect is found with ba/da; these too lie along a continuum acoustically, but vocally, /ba/ is formed with the two lips, /da/ with the tip of the tongue and the alveolar ridge, and our anatomy does not allow any intermediates.",
            "score": 81.54469299316406
        },
        {
            "docid": "9736652_25",
            "document": "Auditory masking . When a sinusoidal signal and a sinusoidal masker (tone) are presented simultaneously the envelope of the combined stimulus fluctuates in a regular pattern described as beats. The fluctuations occur at a rate defined by the difference between the frequencies of the two sounds. If the frequency difference is small then the sound is perceived as a periodic change in the loudness of a single tone. If the beats are fast then this can be described as a sensation of roughness. When there is a large frequency separation, the two components are heard as separate tones without roughness or beats. Beats can be a cue to the presence of a signal even when the signal itself is not audible. The influence of beats can be reduced by using a narrowband noise rather than a sinusoidal tone for either signal or masker.",
            "score": 81.32489776611328
        }
    ]
}