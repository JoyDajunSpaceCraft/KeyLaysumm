{
    "q": [
        {
            "docid": "1434685_7",
            "document": "Prognostics . Model-based prognostics attempts to incorporate physical understanding (physical models) of the system into the estimation of remaining useful life (RUL). Modeling physics can be accomplished at different levels, for example, micro and macro levels. At the micro level (also called material level), physical models are embodied by series of dynamic equations that define relationships, at a given time or load cycle, between damage (or degradation) of a system/component and environmental and operational conditions under which the system/component are operated. The micro-level models are often referred as damage propagation model. For example, Yu and Harris\u2019s fatigue life model for ball bearings, which relates the fatigue life of a bearing to the induced stress, Paris and Erdogan's crack growth model, and stochastic defect-propagation model are other examples of micro-level models. Since measurements of critical damage properties (such as stress or strain of a mechanical component) are rarely available, sensed system parameters have to be used to infer the stress/strain values. Micro-level models need to account in the uncertainty management the assumptions and simplifications, which may pose significant limitations of that approach.",
            "score": 41.857818841934204
        },
        {
            "docid": "9223719_8",
            "document": "Needs assessment . Considered the \"father of needs assessment\", Roger Kaufman first developed a model for determining needs defined as a gap in results. This particular emphasis in results focuses on the outcomes (or ends) that result from an organization's products, processes, or inputs (the means to the ends). Kaufman argues that an actual need can only be identified independent of premature selection of a solution (wherein processes are defined as means to an end, not an end unto themselves). To conduct a quality needs assessment according to Kaufman, first determine the current results, articulate the desired results, and the distance between results is the actual need. Once a need is identified, then a solution can be selected that is targeted to closing the gap. Kaufman's model in particular identifies gaps in needs at the societal level, what Kaufman calls \"Mega\" planning, along with gaps at the Macro (or organizational) and Micro level (the level of individuals and small groups). Organizational elements vary among the three different levels: they are outcomes at the Mega level, outputs at the Macro level, and products at the Micro level. A Mega level needs assessment should be conducted if the primary beneficiary of the desired results is society itself (as with the results of a clean environment or continuing profit). If the desired results are not directly societal, but are delivered to society (such as automobiles or college graduates), then a Macro level assessment should be performed. If the desired results are building blocks for larger results (such as a single sale or a passed inspection), then a Micro level needs assessment is appropriate.",
            "score": 21.80811357498169
        },
        {
            "docid": "579403_164",
            "document": "Development communication . Guyonne Kalbe(2004) identifies and distinguishes two types of simulation models: macro and micro levels. According to Kalbe, the macro-level is applied mostly for huge sectors of the industries. This macro simulation is usually applied by developed countries in order to assess and understand policy changes. On the other hand, the micro-level is used for a specific company using a sample of population when a need for more precised and focused information is its goal. In contrast to large-scale industries that use the macro-level approach, the micro-level is individualized.",
            "score": 22.66104221343994
        },
        {
            "docid": "4880907_12",
            "document": "Double counting (accounting) . The implicit assumption made in national accounts, is that the account at the macro-level must be similar to that at the micro-level. Economic relations are regarded as broadly the same at the micro-level and the macro-level. An individual business buys and uses up inputs and produces outputs for sale; it has costs and revenues. Thus, in social accounting all transactors are treated in a similar way (\"as if\" they were a business). The accounts can be criticised for being eclectic in some ways, but that is not necessarily a problem; the aim of the exercise is to identify and categorise all flows, and the user can then reaggregate them in different ways.",
            "score": 23.863861799240112
        },
        {
            "docid": "863410_4",
            "document": "Eco-efficiency . According to the WBCSD definition, eco-efficiency is achieved through the delivery of \"competitively priced goods and services that satisfy human needs and bring quality of life while progressively reducing environmental impacts of goods and resource intensity throughout the entire life-cycle to a level at least in line with the Earth's estimated carrying capacity.\"  It works by implementing 4 main types of ratios.  The ratios may be applied to any unit comprising economic activities because such activities always relate to cost and value, \u201cand having some physical substrate, always influence the environment.\u201d Furthermore, there are two different levels upon which to orchestrate the ratios: \u2018\u2019micro\u2019\u2019 and \u2018\u2019macro\u2019\u2019.  There are three different methods to determine eco-efficiency at the micro-level. First, \u2018\u2019incremental eco-efficiency\u2019\u2019, which \u201cspecifies the effects of the total value of a product system or sector and its total concomitant environmental effects.\u201d Second, an analysis method nicknamed \u2018\u2019win-win\u2019\u2019, which \u201cgives a comparison between a historical reference situation and potentially new situations based on the use of new technologies.\u201d It should be noted that the win-win micro-method is limited because it cannot give a concrete answer on the question of whether it improves overall environmental performance. And the third is \u2018\u2019difference eco-efficiency\u2019\u2019, which is similar to the win-win variant, but removes all irrelevant alternatives to heighten potential for optimal technologies while comparing two alternatives. Now the macro-level is much less defined and has shown less accurate results. However, \u201cthe ultimate aim of eco-efficiency analysis is to help move micro-level decision making into macro-level optimality.\u201d The main goal in years to come is to create headline indicators to carry out macro-level analysis at a country/world scale.",
            "score": 25.53793168067932
        },
        {
            "docid": "34058307_5",
            "document": "Mitotic exit . Because eukaryotic cell cycle involves a variety of proteins and regulatory interactions, dynamical systems approach can be taken to simplify a complex biological circuit into a general framework for better analysis. Among the four possible input/output relationships, the relationship between Sic1 level and mitotic exit seems to show the characteristics of an irreversible bistable switch, driven by feedback between APC-Cdh1, Sic1, and Clb2-Cdk1. Bistability is known to control biological functions such as cell cycle control and cellular differentiation and play a key role in many cellular regulatory networks. Bistable input/output relationship is characterized by two stable states with two bifurcation points. Multiple outputs are possible for one specific input in the region of bistability, marked by two bifurcation points. In addition, the bistable relationship displays hysteresis: the final state/output depends on the history of the input as well as the current value of input because the system has a memory. One bifurcation point has a negative control parameter value (the bifurcation point is on the other side of the axis), resulting in disconnection between the two stable states and irreversibility of the transition from one state to the other. With regard to mitotic exit, the two stable states are defined by mitosis and G1 phase. Once Sic1 level (input) accumulates beyond the threshold, irreversible transition occurs from mitosis (stable state I) to G1 phase (stable state II).  In the imperfect environment, the only bifurcation that remains intact is saddle-node bifurcation. Saddle-node bifurcation does not break down (saddle-node is the expected generic behavior), while transcritical and pitchfork bifurcations break down in the presence of imperfections. Thus, the only one-dimensional bifurcation that can exist in imperfect biological world is the saddle-node bifurcation. The bistable relation between M-G1 transition and Sic1 level can be represented as a diagram of two saddle-node bifurcations in which the system\u2019s behavior changes qualitatively with a small change in control parameter, the amount of Sic1.",
            "score": 37.7860631942749
        },
        {
            "docid": "34327569_8",
            "document": "Social network . In general, social networks are self-organizing, emergent, and complex, such that a globally coherent pattern appears from the local interaction of the elements that make up the system. These patterns become more apparent as network size increases. However, a global network analysis of, for example, all interpersonal relationships in the world is not feasible and is likely to contain so much information as to be uninformative. Practical limitations of computing power, ethics and participant recruitment and payment also limit the scope of a social network analysis. The nuances of a local system may be lost in a large network analysis, hence the quality of information may be more important than its scale for understanding network properties. Thus, social networks are analyzed at the scale relevant to the researcher's theoretical question. Although levels of analysis are not necessarily mutually exclusive, there are three general levels into which networks may fall: micro-level, meso-level, and macro-level.",
            "score": 43.1975884437561
        },
        {
            "docid": "25368725_36",
            "document": "Biochemical switches in the cell cycle . Because eukaryotic cell cycle involves a variety of proteins and regulatory interactions, dynamical systems approach can be taken to simplify a complex biological circuit into a general framework for better analysis. Among the four possible input/output relationships, the relationship between Sic1 level and mitotic exit seems to show the characteristics of an irreversible bistable switch, driven by feedback between APC-Cdh1, Sic1, and Clb2-Cdk1. Bistability is known to control biological functions such as cell cycle control and cellular differentiation and play a key role in many cellular regulatory networks. Bistable input/output relationship is characterized by two stable states with two bifurcation points. Multiple outputs are possible for one specific input in the region of bistability, marked by two bifurcation points. In addition, the bistable relationship displays hysteresis: the final state/output depends on the history of the input as well as the current value of input because the system has a memory. One bifurcation point has a negative control parameter value (the bifurcation point is on the other side of the axis), resulting in disconnection between the two stable states and irreversibility of the transition from one state to the other. With regard to mitotic exit, the two stable states are defined by mitosis and G1 phase. Once Sic1 level (input) accumulates beyond the threshold, irreversible transition occurs from mitosis (stable state I) to G1 phase (stable state II). In the imperfect environment, the only bifurcation that remains intact is saddle-node bifurcation. Saddle-node bifurcation does not break down (saddle-node is the expected generic behavior), while transcritical and pitchfork bifurcations break down in the presence of imperfections. Thus, the only one-dimensional bifurcation that can exist in imperfect biological world is the saddle-node bifurcation. The bistable relation between M-G1 transition and Sic1 level can be represented as a diagram of two saddle-node bifurcations in which the system\u2019s behavior changes qualitatively with a small change in control parameter, the amount of Sic1.",
            "score": 36.68590259552002
        },
        {
            "docid": "27453461_8",
            "document": "Integrated information theory . The properties required of a conscious physical substrate are called the \"postulates,\" since the existence of the physical substrate is itself only postulated (remember, IIT maintains that the only thing one can be sure of is the existence of one's own consciousness). In what follows, a \"physical system\" is taken to be a set of elements, each with two or more internal states, inputs that influence that state, and outputs that are influenced by that state (neurons or logic gates are the natural examples). Given this definition of \"physical system\", the postulates are: </math>]]), thus laying maximal claim to intrinsic existence. ... With respect to causation, this has the consequence that the \"winning\" cause-effect structure excludes alternative cause-effect structures specified over overlapping elements, otherwise there would be causal overdetermination. ... The exclusion postulate can be said to enforce Occam's razor (entities should not be multiplied beyond necessity): it is more parsimonious to postulate the existence of a single cause-effect structure over a system of elements\u2014the one that is maximally irreducible from the system's intrinsic perspective\u2014than a multitude of overlapping cause-effect structures whose existence would make no further difference. The exclusion postulate also applies to individual mechanisms: a subset of elements in a state specifies the cause-effect repertoire that is maximally irreducible (MICE) within the system (formula_1), called a core concept, or concept for short. Again, it cannot additionally specify a cause-effect repertoire overlapping over the same elements, because otherwise the difference a mechanism makes would be counted multiple times. ... Finally, the exclusion postulate also applies to spatio-temporal grains, implying that a conceptual structure is specified over a definite grain size in space (either quarks, atoms, neurons, neuronal groups, brain areas, and so on) and time (either microseconds, milliseconds, seconds, minutes, and so on), the one at which formula_2 reaches a maximum. ... Once more, this implies that a mechanism cannot specify a cause-effect repertoire at a particular temporal grain, and additional effects at a finer or coarser grain, otherwise the differences a mechanism makes would be counted multiple times.",
            "score": 47.458312034606934
        },
        {
            "docid": "36122619_14",
            "document": "Artificial life . Mathematical models of complex systems are of three types: black-box (phenomenological), white-box (mechanistic, based on the first principles) and grey-box (mixtures of phenomenological and mechanistic models) . In black-box models, the individual-based (mechanistic) mechanisms of a complex dynamic system remain hidden. Black-box models are completely nonmechanistic. They are phenomenological and ignore a composition and internal structure of a complex system. We cannot investigate interactions of subsystems of such a non-transparent model. A white-box model of complex dynamic system has \u2018transparent walls\u2019 and directly shows underlying mechanisms. All events at micro-, meso- and macro-levels of a dynamic system are directly visible at all stages of its white-box model evolution. In most cases mathematical modelers use the heavy black-box mathematical methods, which cannot produce mechanistic models of complex dynamic systems. Grey-box models are intermediate and combine black-box and white-box approaches. Creation of a white-box model of complex system is associated with the problem of the necessity of an a priori basic knowledge of the modeling subject. The deterministic logical cellular automata are necessary but not sufficient condition of a white-box model. The second necessary prerequisite of a white-box model is the presence of the physical ontology of the object under study. The white-box modeling represents an automatic hyper-logical inference from the first principles because it is completely based on the deterministic logic and axiomatic theory of the subject. The purpose of the white-box modeling is to derive from the basic axioms a more detailed, more concrete mechanistic knowledge about the dynamics of the object under study. The necessity to formulate an intrinsic axiomatic system of the subject before creating its white-box model distinguishes the cellular automata models of white-box type from cellular automata models based on arbitrary logical rules. If cellular automata rules have not been formulated from the first principles of the subject, then such a model may have a weak relevance to the real problem .",
            "score": 57.67788887023926
        },
        {
            "docid": "984692_9",
            "document": "Computational sociology . By the late 1960s and early 1970s, social scientists used increasingly available computing technology to perform macro-simulations of control and feedback processes in organizations, industries, cities, and global populations. These models used differential equations to predict population distributions as holistic functions of other systematic factors such as inventory control, urban traffic, migration, and disease transmission. Although simulations of social systems received substantial attention in the mid-1970s after the Club of Rome published reports predicting global environmental catastrophe based upon the predictions of global economy simulations, the inflammatory conclusions also temporarily discredited the nascent field by demonstrating the extent to which results of the models are highly sensitive to the specific quantitative assumptions (backed by little evidence, in the case of the Club of Rome) made about the model's parameters. As a result of increasing skepticism about employing computational tools to make predictions about macro-level social and economic behavior, social scientists turned their attention toward micro-simulation models to make forecasts and study policy effects by modeling aggregate changes in state of individual-level entities rather than the changes in distribution at the population level. However, these micro-simulation models did not permit individuals to interact or adapt and were not intended for basic theoretical research.",
            "score": 28.740315675735474
        },
        {
            "docid": "1727031_58",
            "document": "History of geography . In 1877, Thomas Henry Huxley published his Physiography with the philosophy of universality presented as an integrated approach in the study of the natural environment. The philosophy of universality in geography was not a new one but can be seen as evolving from the works of Alexander von Humboldt and Immanuel Kant. The publication of Huxley physiography presented a new form of geography that analysed and classified cause and effect at the micro-level and then applied these to the macro-scale (due to the view that the micro was part of the macro and thus an understanding of all the micro-scales was need to understand the macro level). This approach emphasized the empirical collection of data over the theoretical. The same approach was also used by Halford John Mackinder in 1887. However, the integration of the Geosphere, Atmosphere and Biosphere under physiography was soon over taken by Davisian geomorphology.",
            "score": 25.97151017189026
        },
        {
            "docid": "1128429_5",
            "document": "Class analysis . Sociologist Erik Olin Wright splits class analysis into a macro and micro level. The foundation of class analysis on a macro level can be identified with class structure. Examples of such class structure in a macro level can be analyzed within a firm, city, country, or the entire world. On a micro level, class analysis focuses on the effects that the class may have on an individual. Erik Olin Wright exclaims examples of this to be: \"Analyses of labor market strategies of unskilled workers, or the effects of technological change on class consciousness, or political contributions of corporate executives\". Macro and Micro level events can correlate with one another through different perspectives. Wright proclaims that Macro level events are not created and set on one large effect, but instead processed through multiple individuals in a very intricate and complex pattern. He dictates that Macro class size events are endorsed by an embodiment of multiple micro class events. He also states the opposite effect each size has on each other and how Micro level events relative to class relations can be reinforced by the context of macro level events.",
            "score": 21.754419565200806
        },
        {
            "docid": "8503698_5",
            "document": "Complexity economics . More generally, complexity economics models are often used to study how non-intuitive results at the macro-level of a system can emerge from simple interactions at the micro level. This avoids assumptions of the representative agent method, which attributes outcomes in collective systems as the simple sum of the rational actions of the individuals.",
            "score": 37.79987335205078
        },
        {
            "docid": "44537503_4",
            "document": "Naive dialecticism . Naive dialecticism provides a framework for analysis that is halfway between a macro-level and a micro-level approach. On a macro-level, individualism and collectivism define the social differences between cultures, which influences the specific common beliefs about how the world operates. Na\u00efve dialecticism shares this macro-level framework while also looking at individual and situation-specific differences between people (micro-approach).",
            "score": 21.185277223587036
        },
        {
            "docid": "1434685_8",
            "document": "Prognostics . Macro-level models are the mathematical model at system level, which defines the relationship among system input variables, system state variables, and system measures variables/outputs where the model is often a somewhat simplified representation of the system, for example a lumped parameter model. The trade-off is increased coverage with possibly reducing accuracy of a particular degradation mode. Where this trade-off is permissible, faster prototyping may be the result. However, where systems are complex (e.g., a gas turbine engine), even a macro-level model may be a rather time-consuming and labor-intensive process. As a result, macro-level models may not be available in detail for all subsystems. The resulting simplifications need to be accounted for by the uncertainty management.",
            "score": 41.72281002998352
        },
        {
            "docid": "7587198_41",
            "document": "Media system dependency theory . Three basic factors of MSD \u2013 individual characteristics, social environment, and media system activity \u2013 are derived from both micro and macro levels in a society, postulating media remained on a single level. However, the functionality of social media has been suggested as crossing those levels. Through social media, users are able to create the story (on the micro level), and the story can be either shared publicly (on the macro level) or not. Thus, social media gain the ability to move across levels.",
            "score": 23.646896362304688
        },
        {
            "docid": "332306_21",
            "document": "Anthony Giddens . To illustrate this relationship, Giddens discusses changing attitudes towards marriage in developed countries. He claims that any effort to explain this phenomenon solely in terms of micro or macro level causes will result in a circular cause and consequence. Social relationships and visible sexuality (micro-level change) are related to the decline of religion and the rise of rationality (macro-level change), but also with changes in the laws relating to marriage and sexuality (macro), change caused by different practices and changing attitudes on the level of everyday lives (micro). Practices and attitudes in turn can be affected by social movements (for example, women's liberation and egalitarianism), a macro-scale phenomena; but the movements usually grow out of everyday life grievances \u2014 a micro-scale phenomenon.",
            "score": 22.894891023635864
        },
        {
            "docid": "29100672_2",
            "document": "Macro BIM . Macro BIM (Building Information Model) is a building information model, assembled of higher level building elements, used for macro level analysis including visualization, spatial validation, cost modeling/estimating, phasing/sequencing, energy performance, and risk. Macro models are intended to be built quickly, facilitating rapid analysis of multiple concepts or ideas prior to launching into a more detailed in depth study of a preferred concept using \"Micro BIM\" applications.  Macro BIM authoring applications often utilize parametric variables and properties as well as inferencing capabilities to quickly build enough relevant data to facilitate analysis.",
            "score": 24.40330147743225
        },
        {
            "docid": "147885_23",
            "document": "Power (social and political) . Stewart Clegg proposes another three-dimensional model with his \"circuits of power\" theory. This model likens the production and organizing of power to an electric circuit board consisting of three distinct interacting circuits: episodic, dispositional, and facilitative. These circuits operate at three levels, two are macro and one is micro. The \"episodic circuit\" is the micro level and is constituted of irregular exercise of power as agents address feelings, communication, conflict, and resistance in day-to-day interrelations. The outcomes of the episodic circuit are both positive and negative. The \"dispositional circuit\" is constituted of macro level rules of practice and socially constructed meanings that inform member relations and legitimate authority. The \"facilitative circuit\" is constituted of macro level technology, environmental contingencies, job design, and networks, which empower or disempower and thus punish or reward, agency in the episodic circuit. All three independent circuits interact at \"obligatory passage points\" which are channels for empowerment or disempowerment.",
            "score": 10.502870798110962
        },
        {
            "docid": "648470_19",
            "document": "Oppression . Addressing social oppression on both a macro and micro level, feminist Patricia Hill Collins discusses her \"matrix of domination\". The matrix of domination discusses the interrelated nature of four domains of power, including the structural, disciplinary, hegemonic, and interpersonal domains. Each of these spheres works to sustain current inequalities that are faced by marginalized, excluded or oppressed groups. The structural, disciplinary and hegemonic domains all operate on a macro level, creating social oppression through macro structures such as education, or the criminal justice system, which play out in the interpersonal sphere of everyday life through micro-oppressions.",
            "score": 21.703349113464355
        },
        {
            "docid": "5943097_8",
            "document": "Community practice . It should also be noted that a third social work practice category is sometimes referenced called 'mezzo practice'. Mezzo practice can be defined by its combination of micro and macro aspects with the focus of interventions being smaller groups or systems. Whereas macro practice often focuses on policy or systematic changes, some researchers and practitioners consider mezzo practice to focus more on change at the community or neighborhood level. Because there is often an overlap between macro and mezzo, some argue mezzo practice to be a sub-category within macro social work. Although it makes up a smaller portion of social work practice, mezzo practice represents an effective way to bridge some of the perceived distances between micro and macro practice methods.",
            "score": 18.864102602005005
        },
        {
            "docid": "984629_11",
            "document": "Social complexity . Computational sociology is influenced by a number of micro-sociological areas as well as the macro-level traditions of systems science and systems thinking. The micro-level influences of symbolic interaction, exchange, and rational choice, along with the micro-level focus of computational political scientists, such as Robert Axelrod, helped to develop computational sociology's bottom-up, agent-based approach to modeling complex systems. This is what Joshua M. Epstein calls generative science. Other important areas of influence include statistics, mathematical modeling and computer simulation.",
            "score": 33.02268934249878
        },
        {
            "docid": "20560_18",
            "document": "Macro (computer science) . Before Lisp had macros, it had so-called FEXPRs, function-like operators whose inputs were not the values computed by the arguments but rather the syntactic forms of the arguments, and whose output were values to be used in the computation. In other words, FEXPRs were implemented at the same level as EVAL, and provided a window into the meta-evaluation layer. This was generally found to be a difficult model to reason about effectively.",
            "score": 23.69455099105835
        },
        {
            "docid": "3772909_22",
            "document": "Large deviations theory . The rate function is related to the entropy in statistical mechanics. This can be heuristically seen in the following way. In statistical mechanics the entropy of a particular macro-state is related to the number of micro-states which corresponds to this macro-state. In our coin tossing example the mean value formula_2 could designate a particular macro-state. And the particular sequence of heads and tails which gives rise to a particular value of formula_2 constitutes a particular micro-state. Loosely speaking a macro-state having a higher number of micro-states giving rise to it, has higher entropy. And a state with higher entropy has a higher chance of being realised in actual experiments. The macro-state with mean value of 1/2 (as many heads as tails) has the highest number of micro-states giving rise to it and it is indeed the state with the highest entropy. And in most practical situations we shall indeed obtain this macro-state for large numbers of trials. The \"rate function\" on the other hand measures the probability of appearance of a particular macro-state. The smaller the rate function the higher is the chance of a macro-state appearing. In our coin-tossing the value of the \"rate function\" for mean value equal to 1/2 is zero. In this way one can see the \"rate function\" as the negative of the \"entropy\".",
            "score": 25.70759892463684
        },
        {
            "docid": "31575541_5",
            "document": "Learning enterprises . First of all, micro strategies are ways to approach instruction on particular topics of learning goals. Typically, an instructional design and development task for either education or training involves more than one goal or topic. Therefore, we need to approach with Macro level.  Developing instruction at the macro level is frequently referred to as curriculum development or design which is concerned with making decisions about the scope, organization, and sequence of content at the macro level.  Posner and coworkers (Posner&Strike, 1976; Poser&Rudnitsky, 1994) classified curriculum sequencing structures, which is used in curriculum development, into five major categories: world related structure, learning-related structure, utilization- related structure, inquiry-related structure, and concept-related structure.  Learning enterprise is based on macro strategy, because it is beyond the single topic, or goal. Especially it is connected with learning-related structure. Learning-related structures organize information in such a way that new learning builds on relevant prior knowledge. We use this into two ways. One is prerequisite-based structure which is based on prerequisite relationship of all the information and skills in the course. The other is Knowledge structure. The knowledge structure idea is seen in learning enterprises. The unifying element in an enterprise is a declarative knowledge representation of enterprise itself. A strong emphasis is placed on the purposeful and interrelated qualities of the knowledge, and hence, various theories related to schema. The declarative knowledge of the enterprise is referred to as an enterprise schema.",
            "score": 29.488773822784424
        },
        {
            "docid": "3756817_20",
            "document": "Robert C. Tucker . Tucker distinguished between \"real\" and \"ideal\" culture and between \"macro-level\" and \"micro-level\" culture. \"Real\" cultural patterns consist of \"\"prevalent practices in a society\"\"; \"ideal\" patterns consist of \"\"accepted norms, values, and beliefs\"\". A \"macro-level\" culture is a society\u2019s \"complex totality of patterns and sub-patterns\" of traditions and orientations; \"micro-level\" cultural elements are \"individual patterns and clusters of them\". Cultural patterns are \"ingrained by \"custom\" in the \"conduct\" and \"thoughtways\" of large numbers of people\". More like an anthropologist than a political scientist, Tucker included behavior as well as values, attitudes, and beliefs in his concept of culture. Tucker affirmed that \"a strength of the concept of political culture as an analytic tool (in comparison with such macro concepts as modernization and development) is its micro/macro [and real/ideal] character\". He studied these four characteristics individually and in various juxtapositions, configurations, and interactions. And he hypothesized that different components of political culture \"can have differing fates in times of radical change\", especially in revolutionary transitions from one type of political system to another and from one stage of political development to another. Tucker corroborated this hypothesis with evidence from the Soviet Union. In 1987, he affirmed: \"The pattern of thinking one thing in private and being conformist in public will not vanish or radically change simply because \"glasnost\" has come into currency as a watchword of policy. Changing the pattern will take time and effort and, above all, some risk-taking openness in action by citizens who speak up ... [and] forsake the pattern of pretence which for so long has governed public life in their country.\" In 1993, he elaborated: \"Although communism as a belief system ... is dying out [in post-Soviet Russia], very many of the real culture patterns of the Soviet period, including that very \"\u2019bureaucratism\u2019 that made a comeback after the revolutionary break in 1917, are still tenaciously holding on.\" And, in 1995, he added: \"The banning of the CPSU, the elimination of communism as a state creed, and the breakup of the USSR as an imperial formation marked in a deep sense the ending of the Soviet era. But in part because of the abruptness with which these events came about, much of the statist Soviet system and political culture survived into the 1990s.\"  As Tucker saw it, the \"ideal\" and \"macro\" political cultures of the Communist party collapsed with the Soviet Union, but the \"real\" and \"micro\" political cultures of tsarist and Soviet Russia adapted to the emerging governmental, commercial, legal, and moral cultures of post-Soviet Russia. He underscored the impact of tsarist political culture on Soviet political culture and, in turn, their combined impact on post-Soviet political culture. Tucker was not a historical determinist, but he observed that centuries-old statism was alive and well in Russia after the breakup of the Soviet Union.",
            "score": 24.830504179000854
        },
        {
            "docid": "52833045_18",
            "document": "China's Circular Economy . The Chinese government has extensively studied the EU\u2019s and Japan\u2019s models of material flow analysis, where they have adapted it to fit their needs using the input of various actors and government agencies. The Chinese indicator system is based on the 3R principle, where two separate indicator systems are used for meso and macro level measurement. The only difference between the two indicator systems is that macro level measurement takes into account recycling at the regional level. Both indicator systems categorize quantitative data in the same four categories, being resource output, resource consumption, integrated resource utilization and reduction in waste generation. Resource output is the measure of GDP produced from consumption of land, energy and water, where higher ratios indicate resource efficiency. Resource consumption refers to the measurement of resources consumed per unit GDP level, where lower ratios indicate that less resources were used by the economic system. The implications for this measurement is that it demonstrates that there are fewer impacts on the natural environment, while having high economic returns. Integrated resource utilization measures the rates of re-using industrial water and recycling industrial waste, where high ratios indicate efficient resource recycling and regeneration of those material back into the economy. Having a high ratio on the integrated resource utilization indicator demonstrates a reduction in the use of natural resources and waste being sent to landfill sites. The reduction in waste generation indicator measures the total amount of waste disposal and emission of pollutants, where lower ratios indicate that there are low levels of waste needed to be disposed and low levels of toxic emissions.",
            "score": 36.57163643836975
        },
        {
            "docid": "27473233_4",
            "document": "Stewart Clegg . Clegg's \"circuits of power\" theory likens the production and organising of power to an electric circuit board consisting of three distinct interacting circuits: episodic, dispositional, and facilitative. These circuits operate at three levels, two are macro and one is micro. The \"episodic circuit\" is the micro level and is constituted of irregular exercise of power as agents address feelings, communication, conflict, and resistance in day-to-day interrelations. The outcomes of the episodic circuit are both positive and negative. The \"dispositional circuit\" is constituted of macro level rules of practice and socially constructed meanings that inform member relations and legitimate authority. The \"facilitative circuit\" is constituted of macro level technology, environmental contingencies, job design, and networks, which empower or disempower and thus punish or reward, agency in the episodic circuit. All three independent circuits interact at \u201cobligatory passage points\u201d which are channels for empowerment or disempowerment.",
            "score": 10.460198402404785
        },
        {
            "docid": "19822072_3",
            "document": "Macrocognition . A macro-theory is a theory which is concerned with the obvious regularities of human experience, rather than with some theoretically defined unit. To refer to another psychological school, it would correspond to a theory at the level of Gestalten. It resembles Newell\u2019s suggestion for a solution that would analyse more complex tasks, although the idea of a macro-theory does not entail an analysis of the mechanistic materialistic kind which is predominant in cognitive psychology. Thus we should have a macro-theory of remembering rather than of memory, to say nothing of short-term memory, proactive inhibition release, or memory scanning. To take another example, we should have a macro-theory of attending, rather than a mini-theory of attention, or micro-theories of limited channel capacities or logarithmic dependencies in disjunctive reaction times. This would ease the dependence on the information processing analogy, but not necessarily lead to an abandonment of the information processing terminology, the Flowchart, or the concept of control structures. The meta-technical sciences can contribute to a psychology of cognition as well as to cognitive psychology. What should be abandoned is rather the tendency to think in elementaristic terms and to increase the plethora of mini-and micro-theories. To conclude, if the psychological study of cognition shall have a future that is not a continued description of human information processing, its theories must be at what we have called the macro-level. This means that they must correspond to the natural units of experience and consider these in relation to the regularities of human experience, rather than as manifestations of hypothetical information processing mechanisms in the brain. A psychology should start at the level of natural units in human experience and try to work upwards towards the level of functions and human action, rather than downwards towards the level of elementary information processes and the structure of the IPS. The use of the term suggests that there is strong evidence in which naturalistic decision-making and the environments in which they occur are navigated in cognitively different ways than artificial or controlled environments.",
            "score": 37.715654611587524
        },
        {
            "docid": "32029481_2",
            "document": "Global environmental analysis . The analysis of the global environment of a company is called global environmental analysis. This analysis is part of a company\u2019s analysis-system, which also comprises various other analyses, like the industry analysis, the market analysis and the analyses of companies, clients and competitors. This system can be divided into a macro and micro level. Except for the global environmental analysis, all other analyses can be found on the micro level. Though, the global environmental analysis describes the macro environment of a company. Obviously, a company is influenced by its environment. Many environmental factors, especially economical or social factors, play a big role in a company\u2019s decisions, because the analysis and the monitoring of those factors reveal chances and risks for the company\u2019s business. This environmental framework also gives information about location issues. A company is thereby able to determine its location sites. Furthermore, many other strategic decisions are based on this analysis. One may also apply the BBW model. In addition, the factors are analyzed to evaluate external business developments. It is finally the task of the management to adapt the firm to its environment or to influence the environment in an adequate way. The latter is mostly the more difficult option. There are different instruments to analyze the company\u2019s environment which are going to be explained afterwards.",
            "score": 24.178863525390625
        },
        {
            "docid": "332306_19",
            "document": "Anthony Giddens . Structuration is very useful in synthesising micro and macro issues. On a micro scale, one of individuals' internal sense of self and identity, consider the example of a family: we are increasingly free to choose our own mates and how to relate with them, which creates new opportunities but also more work, as the relationship becomes a reflexive project that has to be interpreted and maintained. Yet this micro-level change cannot be explained only by looking at the individual level as people did not spontaneously change their minds about how to live; neither can we assume they were directed to do so by social institutions and the state.",
            "score": 30.99832773208618
        }
    ],
    "r": [
        {
            "docid": "39198919_2",
            "document": "Cancer systems biology . Cancer systems biology encompasses the application of systems biology approaches to cancer research, in order to study the disease as a complex adaptive system with emerging properties at multiple biological scales. Cancer systems biology represents the application of systems biology approaches to the analysis of how the intracellular networks of normal cells are perturbed during carcinogenesis to develop effective predictive models that can assist scientists and clinicians in the validations of new therapies and drugs. Tumours are characterized by genomic and epigenetic instability that alters the functions of many different molecules and networks in a single cell as well as altering the interactions with the local environment. Cancer systems biology approaches, therefore, are based on the use of computational and mathematical methods to decipher the complexity in tumorigenesis as well as cancer heterogeneity.  Cancer systems biology encompasses concrete applications of systems biology approaches to cancer research, notably (a) the need for better methods to distill insights from large-scale networks, (b) the importance of integrating multiple data types in constructing more realistic models, (b) challenges in translating insights about tumorigenic mechanisms into therapeutic interventions, and (d) the role of the tumor microenvironment, at the physical, cellular, and molecular levels. Cancer systems biology therefore adopts a holistic view of cancer aimed at integrating its many biological scales, including genetics, signaling networks, epigenetics, cellular behavior, histology, (pre)clinical manifestations and epidemiology. Ultimately, cancer properties at one scale, e.g., histology, are explained by properties at a scale below, e.g., cell behavior.",
            "score": 60.84264373779297
        },
        {
            "docid": "40435056_3",
            "document": "Equation-free modeling . In a wide range of chemical, physical and biological systems, coherent macroscopic behavior emerges from interactions between microscopic entities themselves (molecules, cells, grains, animals in a population, agents) and with their environment. Sometimes, remarkably, a coarse-scale differential equation model (such as the Navier-Stokes equations for fluid flow, or a reaction-diffusion system) can accurately describe macroscopic behavior. Such macroscale modeling makes use of general principles of conservation (atoms, particles, mass, momentum, energy), and closed into a well-posed system through phenomenological constitutive equations or equations of state. However, one increasingly encounters complex systems that only have known microscopic, fine scale, models. In such cases, although we observe the emergence of coarse-scale, macroscopic behavior, modeling it through explicit closure relations may be impossible or impractical. Non-Newtonian fluid flow, chemotaxis, porous media transport, epidemiology, brain modeling and neuronal systems are some typical examples. Equation-free modeling aims to use such microscale models to predict coarse macroscale emergent phenomena.",
            "score": 60.79201889038086
        },
        {
            "docid": "17935654_2",
            "document": "Integrative neuroscience . Integrative neuroscience sculptures a theoretical neuroscience with a mathematical neuroscience that is different from computational neuroscience. In computational neuroscience, reductionist approaches span multiple levels of neural organization. However, in integrative neuroscience, each level is seamlessly sculptured as part of a continuum of levels. The roots of integrative neuroscience originated from the Rashevsky-Rosen school of relational biology that characterizes functional organization mathematically by abstracting away the structure (i.e., physics and chemistry). It was further expanded by Chauvet who introduced hierarchical and functional integration. Hierarchical integration is structural involving spatiotemporal dynamic continuity in Euclidean space to bring about functional organization, viz. However, functional integration is relational and as such this requires a topology not restricted to Euclidean space, but rather occupying vector spaces This means that for any given functional organization the methods of functional analysis enable a relational organization to be mapped by the functional integration, viz.  Thus hierarchical and functional integration entails a \"neurobiology of cognitive semantics\" where hierarchical organization is associated with the neurobiology and relational organization is associated with the cognitive semantics. Relational organization throws away the matter; \"function dictates structure\", hence material aspects are entailed, while in reductionism the causal nexus between structure and dynamics entails function that obviates functional integration because the causal entailment in the brain of hierarchical integration is absent from the structure. If integrative neuroscience is studied from the viewpoint of functional organization of hierarchical levels then it is defined as causal entailment in the brain of hierarchical integration. If it is studied from the viewpoint of relational organization then it is defined as semantic entailment in the brain of functional integration. It aims to present studies of functional organization of particular brain systems across scale through hierarchical integration leading to species-typical behaviors under normal and pathological states. As such, integrative neuroscience aims for a unified understanding of brain function across scale. Spivey's continuity of mind thesis extends integrative neuroscience to the domain of continuity psychology.",
            "score": 60.016483306884766
        },
        {
            "docid": "1303219_5",
            "document": "Holism in science . Holistic science is naturally suited to subjects such as ecology, biology, physics and the social sciences, where complex, non-linear interactions are the norm. These are systems where emergent properties arise at the level of the whole that cannot be predicted by focusing on the parts alone, which may make mainstream, reductionist science ill-equipped to provide understanding beyond a certain level. This principle of emergence in complex systems is often captured in the phrase \u2032the whole is greater than the sum of its parts\u2032. Living organisms are an example: no knowledge of all the chemical and physical properties of matter can explain or predict the functioning of living organisms. The same happens in complex social human systems, where detailed understanding of individual behaviour cannot predict the behaviour of the group, which emerges at the level of the collective. The phenomenon of emergence may impose a theoretical limit on knowledge available through reductionist methodology, arguably making complex systems natural subjects for holistic approaches.",
            "score": 59.84577560424805
        },
        {
            "docid": "20786042_3",
            "document": "Cybernetics . Cybernetics is applicable when a system being analyzed incorporates a closed signaling loop\u2014originally referred to as a \"circular causal\" relationship\u2014that is, where action by the system generates some change in its environment and that change is reflected in the system in some manner (feedback) that triggers a system change. Cybernetics is relevant to, for example, mechanical, physical, biological, cognitive, and social systems. The essential goal of the broad field of cybernetics is to understand and define the functions and processes of systems that have goals and that participate in circular, causal chains that move from action to sensing to comparison with desired goal, and again to action. Its focus is how anything (digital, mechanical or biological) processes information, reacts to information, and changes or can be changed to better accomplish the first two tasks. Cybernetics includes the study of feedback, black boxes and derived concepts such as communication and control in living organisms, machines and organizations including self-organization.",
            "score": 59.124149322509766
        },
        {
            "docid": "7628535_11",
            "document": "The Hedgehog, the Fox, and the Magister's Pox . Gould reproves Wilson's program of reductionism by utilizing two main arguments based upon the emergence and contingency or randomness found in some complex, nonlinear or non-additive systems. He indicates that there exist new entities, properties, and interactions that \"emerge\" in some complex systems which cannot be predicted from knowledge of properties of the components, or of laws governing at the level of those components alone. Thus reductionism can only fail in attempts to model, explain, or describe such systems, and we must search for and depend upon new emergent principles embedded in higher, more complex levels. He also indicates that the historical contingency in some systems may cause effects that do not necessarily strictly follow a single path from identified causes and therefore may require narrative methods drawn from historical analysis and the humanities rather that classical deductive mathematical formulas prescribing necessarily linear consequences. He highlights evolution by natural selection as a primary example of how entities such as ourselves are not a necessary, but rather a contingent product, \"we have preferred to think of \"Homo sapiens\" not only as something special (which I surely do not deny), but also as something ordained, necessary, or at the very least, predictable from some form of general process... But if \"Homo sapiens\" represents more of a contingent and improbable fact of history than an apotheosis of a predictable tendency, then our peculiarities, even though they be universal \"within\" our species, remain more within the narrative realm of the sciences of historical contingency than within the traditional, and potentially reductionist, domain of repeated and predictable natural phenomenon generated by laws of nature.\"",
            "score": 58.17632293701172
        },
        {
            "docid": "7497569_3",
            "document": "Systems immunology . The immune system has been thoroughly analyzed as regards to its components and function by using a very successful \"reductionist\" approach, but its overall functioning principles cannot easily be predicted by studying the properties of its isolated components because they strongly rely on and arise from the interactions among these numerous constituents. Systems immunology represents a different approach for the integrated comprehension of the immune system structure and function based on complex systems theory, high-throughput techniques, as well as on mathematical and computational tools.",
            "score": 58.05230712890625
        },
        {
            "docid": "36122619_14",
            "document": "Artificial life . Mathematical models of complex systems are of three types: black-box (phenomenological), white-box (mechanistic, based on the first principles) and grey-box (mixtures of phenomenological and mechanistic models) . In black-box models, the individual-based (mechanistic) mechanisms of a complex dynamic system remain hidden. Black-box models are completely nonmechanistic. They are phenomenological and ignore a composition and internal structure of a complex system. We cannot investigate interactions of subsystems of such a non-transparent model. A white-box model of complex dynamic system has \u2018transparent walls\u2019 and directly shows underlying mechanisms. All events at micro-, meso- and macro-levels of a dynamic system are directly visible at all stages of its white-box model evolution. In most cases mathematical modelers use the heavy black-box mathematical methods, which cannot produce mechanistic models of complex dynamic systems. Grey-box models are intermediate and combine black-box and white-box approaches. Creation of a white-box model of complex system is associated with the problem of the necessity of an a priori basic knowledge of the modeling subject. The deterministic logical cellular automata are necessary but not sufficient condition of a white-box model. The second necessary prerequisite of a white-box model is the presence of the physical ontology of the object under study. The white-box modeling represents an automatic hyper-logical inference from the first principles because it is completely based on the deterministic logic and axiomatic theory of the subject. The purpose of the white-box modeling is to derive from the basic axioms a more detailed, more concrete mechanistic knowledge about the dynamics of the object under study. The necessity to formulate an intrinsic axiomatic system of the subject before creating its white-box model distinguishes the cellular automata models of white-box type from cellular automata models based on arbitrary logical rules. If cellular automata rules have not been formulated from the first principles of the subject, then such a model may have a weak relevance to the real problem .",
            "score": 57.67789077758789
        },
        {
            "docid": "768799_2",
            "document": "White-box testing . White-box testing (also known as clear box testing, glass box testing, transparent box testing, and structural testing) is a method of testing software that tests internal structures or workings of an application, as opposed to its functionality (i.e. black-box testing). In white-box testing an internal perspective of the system, as well as programming skills, are used to design test cases. The tester chooses inputs to exercise paths through the code and determine the expected outputs. This is analogous to testing nodes in a circuit, e.g. in-circuit testing (ICT). White-box testing can be applied at the unit, integration and system levels of the software testing process. Although traditional testers tended to think of white-box testing as being done at the unit level, it is used for integration and system testing more frequently today. It can test paths within a unit, paths between units during integration, and between subsystems during a system\u2013level test. Though this method of test design can uncover many errors or problems, it has the potential to miss unimplemented parts of the specification or missing requirements.",
            "score": 57.20796203613281
        },
        {
            "docid": "20590_20",
            "document": "Mathematical model . In black-box models one tries to estimate both the functional form of relations between variables and the numerical parameters in those functions. Using a priori information we could end up, for example, with a set of functions that probably could describe the system adequately. If there is no a priori information we would try to use functions as general as possible to cover all different models. An often used approach for black-box models are neural networks which usually do not make assumptions about incoming data. Alternatively the NARMAX (Nonlinear AutoRegressive Moving Average model with eXogenous inputs) algorithms which were developed as part of nonlinear system identification can be used to select the model terms, determine the model structure, and estimate the unknown parameters in the presence of correlated and nonlinear noise. The advantage of NARMAX models compared to neural networks is that NARMAX produces models that can be written down and related to the underlying process, whereas neural networks produce an approximation that is opaque.",
            "score": 57.09853744506836
        },
        {
            "docid": "37436_36",
            "document": "Emergence . An emergent behavior or emergent property can appear when a number of simple entities (agents) operate in an environment, forming more complex behaviors as a collective. If emergence happens over disparate size scales, then the reason is usually a causal relation across different scales. In other words, there is often a form of top-down feedback in systems with emergent properties. The processes causing emergent properties may occur in either the observed or observing system, and are commonly identifiable by their patterns of accumulating change, generally called 'growth'. Emergent behaviours can occur because of intricate causal relations across different scales and feedback, known as interconnectivity. The emergent property itself may be either very predictable or unpredictable and unprecedented, and represent a new level of the system's evolution. The complex behaviour or properties are not a property of any single such entity, nor can they easily be predicted or deduced from behaviour in the lower-level entities, and might in fact be irreducible to such behavior. The shape and behaviour of a flock of birds or school of fish are good examples of emergent properties.",
            "score": 56.56285858154297
        },
        {
            "docid": "42944052_3",
            "document": "Causal fermion system . Instead of introducing physical objects on a preexisting space-time manifold, the general concept is to derive space-time as well as all the objects therein as secondary objects from the structures of an underlying causal fermion system. This concept also makes it possible to generalize notions of differential geometry to the non-smooth setting. In particular, one can describe situations when space-time no longer has a manifold structure on the microscopic scale (like a space-time lattice or other discrete or continuous structures on the Planck scale). As a result, the theory of causal fermion systems is a proposal for quantum geometry and an approach to quantum gravity.",
            "score": 56.47268295288086
        },
        {
            "docid": "44783487_24",
            "document": "Data lineage . DISC system consists of several levels of operators and data, and different use cases of lineage can dictate the level at which lineage needs to be captured. Lineage can be captured at the level of the job, using files and giving lineage tuples of form {IF i, M RJob, OF i }, lineage can also be captured at the level of each task, using records and giving, for example, lineage tuples of form {(k rr, v rr ), map, (k m, v m )}. The first form of lineage is called coarse-grain lineage, while the second form is called fine-grain lineage. Integrating lineage across different granularities enables users to ask questions such as \u201cWhich file read by a MapReduce job produced this particular output record?\u201d and can be useful in debugging across different operator and data granularities within a dataflow. To capture end-to-end lineage in a DISC system, we use the Ibis model, which introduces the notion of containment hierarchies for operators and data. Specifically, Ibis proposes that an operator can be contained within another and such a relationship between two operators is called operator containment. \"Operator containment implies that the contained (or child) operator performs a part of the logical operation of the containing (or parent) operator.\" For example, a MapReduce task is contained in a job. Similar containment relationships exist for data as well, called data containment. Data containment implies that the contained data is a subset of the containing data (superset).",
            "score": 56.39908218383789
        },
        {
            "docid": "37438_9",
            "document": "Complex system . As it relates to complex systems, systems theory contributes an emphasis on the way relationships and dependencies between a system's parts can determine system-wide properties. It also contributes the interdisciplinary perspective of the study of complex systems: the notion that shared properties link systems across disciplines, justifying the pursuit of modeling approaches applicable to complex systems wherever they appear. Specific concepts important to complex systems, such as emergence, feedback loops, and adaptation, also originate in systems theory.",
            "score": 56.388038635253906
        },
        {
            "docid": "18950050_7",
            "document": "Black box . The \"black box\" is an abstraction representing a class of concrete open system which can be viewed solely in terms of its \"stimuli inputs\" and \"output reactions\": The understanding of a \"black box\" is based on the \"explanatory principle\", the hypothesis of a causal relation between the \"input\" and the \"output\", and:",
            "score": 56.230934143066406
        },
        {
            "docid": "30217390_14",
            "document": "Grouped Events . Causality appears to us to be reasonably objective to determine on the macroscopic scale. (But not on the quantum scale, where random chance prevails.) However, even large-scale physical causality is a somewhat mysterious notion; there is no general theory of causality in physics, and most events in physics are theoretically reversible in time . However, the Second Law of Thermodynamics (which states that entropy always increases with time, in any closed system) can be regarded as providing a direction for the \"arrow of time\" and thus entropy may be related to the physically mysterious notion of causality. (See Causality (physics).)",
            "score": 56.060848236083984
        },
        {
            "docid": "40354723_3",
            "document": "Structural complexity (applied mathematics) . Structural complexity emerges from all systems that display morphological organization (Nicolis & Prigogine 1989). Filamentary structures, for instance, are an example of coherent structures that emerge, interact and evolve in many physical and biological systems, such as mass distribution in the Universe, vortex filaments in turbulent flows, neural networks in our brain and genetic material (such as DNA) in a cell. In general information on the degree of morphological disorder present in the system tells us something important about fundamental physical or biological processes.",
            "score": 54.81718826293945
        },
        {
            "docid": "355240_16",
            "document": "Cognitive model . In the context of dynamical systems and embodied cognition, representations can be conceptualized as indicators or mediators. In the indicator view, internal states carry information about the existence of an object in the environment, where the state of a system during exposure to an object is the representation of that object. On the mediator view, internal states carry information about the environment which is used by the system in obtaining its goals. In this more complex account, the states of the system carries information that mediates between the information the agent takes in from the environment, and the force exerted on the environment by the agents behavior. The application of open dynamical systems have been discussed for four types of classical embodied cognition examples:  The interpretations of these examples rely on the following logic: (1) the total system captures embodiment; (2) one or more agent systems capture the intrinsic dynamics of individual agents; (3) the complete behavior of an agent can be understood as a change to the agent's intrinsic dynamics in relation to its situation in the environment; and (4) the paths of an open dynamical system can be interpreted as representational processes. These embodied cognition examples show the importance of studying the emergent dynamics of an agent-environment systems, as well as the intrinsic dynamics of agent systems. Rather than being at odds with traditional cognitive science approaches, dynamical systems are a natural extension of these methods and should be studied in parallel rather than in competition.",
            "score": 54.399658203125
        },
        {
            "docid": "29090_37",
            "document": "Software testing . Grey-box testing (American spelling: gray-box testing) involves having knowledge of internal data structures and algorithms for purposes of designing tests while executing those tests at the user, or black-box level. The tester is not required to have full access to the software's source code. Manipulating input data and formatting output do not qualify as grey-box, as the input and output are clearly outside of the \"black box\" that we are calling the system under test. This distinction is particularly important when conducting integration testing between two modules of code written by two different developers, where only the interfaces are exposed for the test.",
            "score": 54.18466567993164
        },
        {
            "docid": "53489871_21",
            "document": "Software testing tactics . Grey-box testing (American spelling: gray-box testing) involves having knowledge of internal data structures and algorithms for purposes of designing tests, while executing those tests at the user, or black-box level. The tester is not required to have full access to the software's source code. Manipulating input data and formatting output do not qualify as grey-box, because the input and output are clearly outside of the \"black box\" that we are calling the system under test. This distinction is particularly important when conducting integration testing between two modules of code written by two different developers, where only the interfaces are exposed for test.",
            "score": 54.18466567993164
        },
        {
            "docid": "37196_37",
            "document": "Causality . Nobel Prize laureate Herbert A. Simon and philosopher Nicholas Rescher claim that the asymmetry of the causal relation is unrelated to the asymmetry of any mode of implication that contraposes. Rather, a causal relation is not a relation between values of variables, but a function of one variable (the cause) on to another (the effect). So, given a system of equations, and a set of variables appearing in these equations, we can introduce an asymmetric relation among individual equations and variables that corresponds perfectly to our commonsense notion of a causal ordering. The system of equations must have certain properties, most importantly, if some values are chosen arbitrarily, the remaining values will be determined uniquely through a path of serial discovery that is perfectly causal. They postulate the inherent serialization of such a system of equations may correctly capture causation in all empirical fields, including physics and economics.",
            "score": 54.175140380859375
        },
        {
            "docid": "9630_44",
            "document": "Ecology . Ecological studies are necessarily holistic as opposed to reductionistic. Holism has three scientific meanings or uses that identify with ecology: 1) the mechanistic complexity of ecosystems, 2) the practical description of patterns in quantitative reductionist terms where correlations may be identified but nothing is understood about the causal relations without reference to the whole system, which leads to 3) a metaphysical hierarchy whereby the causal relations of larger systems are understood without reference to the smaller parts. Scientific holism differs from mysticism that has appropriated the same term. An example of metaphysical holism is identified in the trend of increased exterior thickness in shells of different species. The reason for a thickness increase can be understood through reference to principles of natural selection via predation without need to reference or understand the biomolecular properties of the exterior shells.",
            "score": 54.14760971069336
        },
        {
            "docid": "44783487_47",
            "document": "Data lineage . Tracing is essential for debugging, during which, a user can issue multiple tracing queries. Thus, it is important that tracing has fast turnaround times. Ikeda et al. can perform efficient backward tracing queries for MapReduce dataflows, but are not generic to different DISC systems and do not perform efficient forward queries. Lipstick, a lineage system for Pig, while able to perform both backward and forward tracing, is specific to Pig and SQL operators and can only perform coarse-grain tracing for black-box operators. Thus, there is a need for a lineage system that enables efficient forward and backward tracing for generic DISC systems and dataflows with black-box operators.",
            "score": 54.04735565185547
        },
        {
            "docid": "28564320_3",
            "document": "Complex systems biology . Most complex system models are often formulated in terms of concepts drawn from statistical physics, information theory and non-linear dynamics; however, such approaches are not focused on, or do not include, the conceptual part of complexity related to organization and topological attributes or algebraic topology, such as network connectivity of genomes, interactomes and biological organisms that are very important. Recently, the two complementary approaches based both on information theory, network topology/abstract graph theory concepts are being combined for example in the fields of neuroscience and human cognition. It is generally agreed that there is a hierarchy of complexity levels of organization that should be considered as distinct from that of the levels of reality in ontology. The hierarchy of complexity levels of organization in the biosphere is also recognized in modern classifications of taxonomic ranks, such as: biological domain and biosphere, biological kingdom, Phylum, biological class, order, family, genus and species. Because of their dynamic and composition variability, intrinsic \"fuzziness\", autopoietic attributes, ability to self-reproduce, and so on, organisms do not fit into the 'standard' definition of general systems, and they are therefore 'super-complex' in both their function and structure; organisms can be thus be defined in CSB only as 'meta-systems' of simpler dynamic systems Such a meta-system definition of organisms, species, 'ecosystems', and so on, is not equivalent to the definition of a \"system of systems\" as in Autopoietic Systems Theory,; it also differs from the definition proposed for example by K.D. Palmer in meta-system engineering, organisms being quite different from machines and automata with fixed input-output transition functions, or a continuous dynamical system with fixed phase space, contrary to the Cartesian philosophical thinking; thus, organisms cannot be defined merely in terms of a quintuple A of \"(states, startup state, input and output sets/alphabet, transition function)\", although 'non-deterministic automata', as well as 'fuzzy automata' have also been defined. Tessellation or cellular automata provide however an intuitive, visual/computational insight into the lower levels of complexity, and have therefore become an increasingly popular, discrete model studied in computability theory, applied mathematics, physics, computer science, theoretical biology/systems biology, cancer simulations and microstructure modeling. Evolving cellular automata using genetic algorithms is also an emerging field attempting to bridge the gap between the tessellation automata and the higher level complexity approaches in CSB.",
            "score": 53.853370666503906
        },
        {
            "docid": "6165293_10",
            "document": "Antecedent moisture . Data-based approaches based on system identification, such as the i3D antecedent moisture model, have been applied to hydrologic modeling for simulating antecedent moisture effects on wet weather events in sanitary collection systems. This modeling approach differs from traditional techniques because it is based on system identification and is guided by system observations (i.e. data) and mathematical routines are used to generate the correct model structure, rather than physically based first principles. This is in contrast to assuming that the correct model is known beforehand, as is typically the case for modeling within civil engineering. This technique allows information within the observations to guide the modeling algorithms so that only the relevant and observed dynamics are present in the model structure. The resulting models are not black box, but are grey box models that have parameters and structure that tie directly to physical understanding and interpretation.",
            "score": 53.42394256591797
        },
        {
            "docid": "23204_22",
            "document": "Physical quantity . The meaning of the term physical \"quantity\" is generally well understood (everyone understands what is meant by \"the frequency of a periodic phenomenon\", or \"the resistance of an electric wire\"). The term \"physical quantity\" does not imply a physically \"invariant quantity\". \"Length\" for example is a \"physical quantity\", yet it is variant under coordinate change in special and general relativity. The notion of physical quantities is so basic and intuitive in the realm of science, that it does not need to be explicitly \"spelled out\" or even \"mentioned\". It is universally understood that scientists will (more often than not) deal with quantitative data, as opposed to qualitative data. Explicit mention and discussion of \"physical quantities\" is not part of any standard science program, and is more suited for a \"philosophy of science\" or \"philosophy\" program. The notion of \"physical quantities\" is seldom used in physics, nor is it part of the standard physics vernacular. The idea is often misleading, as its name implies \"a quantity that can be physically measured\", yet is often incorrectly used to mean a physical invariant. Due to the rich complexity of physics, many different fields possess different physical invariants. There is no known physical invariant sacred in all possible fields of physics. Energy, space, momentum, torque, position, and length (just to name a few) are all found to be experimentally variant in some particular scale and system. Additionally, the notion that it is possible to measure \"physical quantities\" comes into question, particular in quantum field theory and normalization techniques. As infinities are produced by the theory, the actual \u201cmeasurements\u201d made are not really those of the physical universe (as we cannot measure infinities), they are those of the renormalization scheme which is expressly dependent on our measurement scheme, coordinate system and metric system.",
            "score": 53.1515998840332
        },
        {
            "docid": "20433319_2",
            "document": "Materiomics . Materiomics is defined as the holistic study of material systems. Materiomics examines links between physiochemical material properties and material characteristics and function. The focus of materiomics is system functionality and behavior, rather than a piecewise collection of properties, a paradigm similar to systems biology. While typically applied to complex biological systems and biomaterials, materiomics is equally applicable to non-biological systems. Materiomics investigates the material properties of natural and synthetic materials by examining fundamental links between processes, structures and properties at multiple scales, from nano to macro, by using systematic experimental, theoretical or computational methods.",
            "score": 52.77813720703125
        },
        {
            "docid": "10013669_11",
            "document": "Function (biology) . Causal role theories of biological function trace their origin back to a 1975 paper by Robert Cummins. Cummins defines the functional role of a component of a system to be the causal effect that the component has on the larger containing system. For example, the heart has the actual causal role of pumping blood in the circulatory system; therefore, the function of the heart is to pump blood. This account has been objected to on the grounds that it is too loose a notion of function. For example, the heart also has the causal effect of producing a sound, but we would not consider producing sound to be the function of the heart. More recent defenses of causal-role theory of function include Amudson & Lauder (1994) and Craver (2001).",
            "score": 52.68988800048828
        },
        {
            "docid": "4003614_5",
            "document": "Multiscale modeling . In physics and chemistry, multiscale modeling is aimed to calculation of material properties or system behavior on one level using information or models from different levels. On each level particular approaches are used for description of a system. The following levels are usually distinguished: level of quantum mechanical models (information about electrons is included), level of molecular dynamics models (information about individual atoms is included), coarse-grained models (information about atoms and/or groups of atoms is included), mesoscale or nano level (information about large groups of atoms and/or molecule positions is included), level of continuum models, level of device models. Each level addresses a phenomenon over a specific window of length and time. Multiscale modeling is particularly important in integrated computational materials engineering since it allows the prediction of material properties or system behavior based on knowledge of the process-structure-property relationships.",
            "score": 52.304588317871094
        },
        {
            "docid": "1617522_11",
            "document": "Extensibility . In black-box extensibility (also called data-driven frameworks) no details about a system\u2019s implementation are used for implementing deployments or extensions; only interface specifications are provided. This type of approach is more limited than the various white-box approaches. Black-box extensions are typically achieved through system configuration applications or the use of application-specific scripting languages by defining components interfaces.",
            "score": 52.12672805786133
        },
        {
            "docid": "18985062_12",
            "document": "Information . Often information can be viewed as a type of input to an organism or system. Inputs are of two kinds; some inputs are important to the function of the organism (for example, food) or system (energy) by themselves. In his book \"Sensory Ecology\" Dusenbery called these causal inputs. Other inputs (information) are important only because they are associated with causal inputs and can be used to predict the occurrence of a causal input at a later time (and perhaps another place). Some information is important because of association with other information but eventually there must be a connection to a causal input. In practice, information is usually carried by weak stimuli that must be detected by specialized sensory systems and amplified by energy inputs before they can be functional to the organism or system. For example, light is mainly (but not only, e.g. plants can grow in the direction of the lightsource) a causal input to plants but for animals it only provides information. The colored light reflected from a flower is too weak to do much photosynthetic work but the visual system of the bee detects it and the bee's nervous system uses the information to guide the bee to the flower, where the bee often finds nectar or pollen, which are causal inputs, serving a nutritional function.",
            "score": 51.97697067260742
        },
        {
            "docid": "22939_30",
            "document": "Physics . While physics aims to discover universal laws, its theories lie in explicit domains of applicability. Loosely speaking, the laws of classical physics accurately describe systems whose important length scales are greater than the atomic scale and whose motions are much slower than the speed of light. Outside of this domain, observations do not match predictions provided by classical mechanics. Albert Einstein contributed the framework of special relativity, which replaced notions of absolute time and space with spacetime and allowed an accurate description of systems whose components have speeds approaching the speed of light. Max Planck, Erwin Schr\u00f6dinger, and others introduced quantum mechanics, a probabilistic notion of particles and interactions that allowed an accurate description of atomic and subatomic scales. Later, quantum field theory unified quantum mechanics and special relativity. General relativity allowed for a dynamical, curved spacetime, with which highly massive systems and the large-scale structure of the universe can be well-described. General relativity has not yet been unified with the other fundamental descriptions; several candidate theories of quantum gravity are being developed.",
            "score": 51.699989318847656
        }
    ]
}