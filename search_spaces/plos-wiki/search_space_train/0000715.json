{
    "q": [
        {
            "docid": "4350008_8",
            "document": "Protein\u2013protein interaction prediction . This group of methods makes use of known protein complex structures to predict and structurally model interactions between query protein sequences. The prediction process generally starts by employing a sequence based method (e.g. Interolog) to search for protein complex structures that are homologous to the query sequences. These known complex structures are then used as templates to structurally model the interaction between query sequences. This method has the advantage of not only inferring protein interactions but also suggests models of how proteins interact structurally, which can provide some insights into the atomic level mechanism of that interaction. On the other hand, the ability for these methods to make a prediction is constrained by a limited number of known protein complex structures.",
            "score": 69.3768081665039
        },
        {
            "docid": "23634_47",
            "document": "Protein . Complementary to the field of structural genomics, \"protein structure prediction\" develops efficient mathematical models of proteins to computationally predict the molecular formations in theory, instead of detecting structures with laboratory observation. The most successful type of structure prediction, known as homology modeling, relies on the existence of a \"template\" structure with sequence similarity to the protein being modeled; structural genomics' goal is to provide sufficient representation in solved structures to model most of those that remain. Although producing accurate models remains a challenge when only distantly related template structures are available, it has been suggested that sequence alignment is the bottleneck in this process, as quite accurate models can be produced if a \"perfect\" sequence alignment is known. Many structure prediction methods have served to inform the emerging field of protein engineering, in which novel protein folds have already been designed. A more complex computational problem is the prediction of intermolecular interactions, such as in molecular docking and protein\u2013protein interaction prediction.",
            "score": 58.76845645904541
        },
        {
            "docid": "24574814_3",
            "document": "Models of collaborative tagging . Just like any social phenomena, behavioral patterns in social tagging systems can be characterized by either a descriptive or predictive model. While descriptive models ask the question of \"what\", predictive models go deeper to also ask the question of \"why\" by attempting to provide explanations to the aggregate behavioral patterns. While there may be no general agreement on what an acceptable explanation should be like, many believe that a good explanation should have certain level of predictive accuracy. Descriptive models of social tagging typically are not concerned with explaining the actions of single individuals but describing the patterns that emerge as individual behavior is aggregated in a large social information system. Predictive models, however, attempt to explain aggregate patterns by analyzing how individuals interact and link to each other in ways that bring about similar or different emergent patterns of social behavior. In particular, a mechanism-based predictive model assumes a certain set of rule that individuals interact with each other, and understand how these interactions could produce aggregate patterns as observed and characterized by descriptive models. Predictive models can therefore provide explanations to why different system characteristics may lead to different aggregate patterns, and can therefore potentially provide information on how systems should be designed to achieve different social purposes.",
            "score": 78.26987516880035
        },
        {
            "docid": "387746_23",
            "document": "Social simulation . Agent-based modeling (ABM) is a system in which a collection of agents independently interact on networks. Each individual agent is responsible for different behaviors that result in collective behaviors. These behaviors as a whole help to define the workings of the network. ABM focuses on human social interactions and how people work together and communicate with one another without having one, single \"group mind\". This essentially means that it tends to focus on the consequences of interactions between people (the agents) in a population. Researchers are better able to understand this type of modeling by modeling these dynamics on a smaller, more localized level. Essentially, ABM helps to better understand interactions between people (agents) who, in turn, influence one another (in response to these influences). Simple individual rules or actions can result in coherent group behavior. Changes in these individual acts can affect the collective group in any given population.",
            "score": 63.05098485946655
        },
        {
            "docid": "50890026_22",
            "document": "Direct coupling analysis . DCA can be used for detecting conserved interaction between protein families and for predicting which residue pairs form contacts in a protein complex. Such predictions can be used when generating structural models for these complexes, or when inferring protein-protein interaction networks made from more than two proteins.",
            "score": 54.09980392456055
        },
        {
            "docid": "4350008_2",
            "document": "Protein\u2013protein interaction prediction . Protein\u2013protein interaction prediction is a field combining bioinformatics and structural biology in an attempt to identify and catalog physical interactions between pairs or groups of proteins. Understanding protein\u2013protein interactions is important for the investigation of intracellular signaling pathways, modelling of protein complex structures and for gaining insights into various biochemical processes. Experimentally, physical interactions between pairs of proteins can be inferred from a variety of experimental techniques, including yeast two-hybrid systems, protein-fragment complementation assays (PCA), affinity purification/mass spectrometry, protein microarrays, fluorescence resonance energy transfer (FRET), and Microscale Thermophoresis (MST). Efforts to experimentally determine the interactome of numerous species are ongoing, and a number of computational methods for interaction prediction have been developed in recent years.",
            "score": 59.60007953643799
        },
        {
            "docid": "36080526_17",
            "document": "KcsA potassium channel . Due to the high sequence similarity between the pore of KcsA and other eukaryotic K ion channel proteins, KcsA has provided important insight into the behavior of other important voltage conducting proteins such as the drosophilla-derived \"Shaker\" and the human \"hERG\" potassium channel. KcsA has been used in mutagenesis studies to model the interactions between hERG and various drug compounds. Such tests can screen for drug-hERG channel interactions that cause acquired long QT syndrome, are essential for determining the cardiac safety of new medications. In addition, homology models based on the closed state KcsA crystal structure have been generated computationally to construct a multiple state representation of the hERG cardiac K channel. Such models reveal the flexibility of the hERG channel and can consistently predict the binding affinity of a set of diverse ion channel-interacting ligands. Analysis of the complex ligand-hERG structures can be used to guide the synthesis of drug analogs with reduced hERG liability, based on drug structure and docking potential.",
            "score": 53.80623257160187
        },
        {
            "docid": "207874_7",
            "document": "Swarm behaviour . The boids computer program, created by Craig Reynolds in 1986, simulates swarm behaviour following the above rules. Many subsequent and current models use variations on these rules, often implementing them by means of concentric \"zones\" around each animal. In the \"zone of repulsion\", very close to the animal, the focal animal will seek to distance itself from its neighbours to avoid collision. Slightly further away, in the \"zone of alignment\", the focal animal will seek to align its direction of motion with its neighbours. In the outermost \"zone of attraction\", which extends as far away from the focal animal as it is able to sense, the focal animal will seek to move towards a neighbour.",
            "score": 42.32841610908508
        },
        {
            "docid": "15672176_9",
            "document": "Igal Talmi . Talmi's main field of research is the theory of nuclear structure. The atomic nucleus is composed of a large number of protons and neutrons which move due to strong interactions between them. In spite of their complexity, nuclei exhibit some simple and regular features. Most importantly, nuclei behave as if they move independently in a common static potential well. This gives rise to the existence of shells of protons and neutrons much like the electronic shells in atoms. Nuclei whose proton and neutron shells are complete have special stability and the numbers of protons and of neutrons in them are called magic numbers. This picture of the nucleus is called the nuclear shell model to obtain the information from experimental data and use it to calculate and predict energies which have not been measured. This method has been successfully used by many nuclear physicists and has led to deeper understanding of nuclear structure. To calculate energies of nuclear states it is necessary to know the exact form of the forces which act between the nuclear constituents. These are still not sufficiently known even after many years of research. Talmi developed a method to obtain the information from experimental data and use it to calculate and predict energies which have not been measured. This method has been successfully used by many nuclear physicists and has led to deeper understanding of nuclear structure. The theory which gives a good description of these properties was developed. This description turned out to furnish the shell model basis of the elegant and successful interacting boson models. Talmi also participated in the study of explicit fermion\u2013boson mappings required to connect the interacting-boson model with its shell-model roots and in the introduction of the boson F-spin analog to nucleon isospin.",
            "score": 63.14223325252533
        },
        {
            "docid": "8865843_2",
            "document": "Loop modeling . Loop modeling is a problem in protein structure prediction requiring the prediction of the conformations of loop regions in proteins with or without the use of a structural template. Computer programs that solve these problems have been used to research a broad range of scientific topics from ADP to breast cancer. Because protein function is determined by its shape and the physiochemical properties of its exposed surface, it is important to create an accurate model for protein/ligand interaction studies. The problem arises often in homology modeling, where the tertiary structure of an amino acid sequence is predicted based on a sequence alignment to a \"template\", or a second sequence whose structure is known. Because loops have highly variable sequences even within a given structural motif or protein fold, they often correspond to unaligned regions in sequence alignments; they also tend to be located at the solvent-exposed surface of globular proteins and thus are more conformationally flexible. Consequently, they often cannot be modeled using standard homology modeling techniques. More constrained versions of loop modeling are also used in the data fitting stages of solving a protein structure by X-ray crystallography, because loops can correspond to regions of low electron density and are therefore difficult to resolve.",
            "score": 61.4766149520874
        },
        {
            "docid": "1702398_5",
            "document": "Hubbard model . For electrons in a solid, the Hubbard model can be considered as an improvement on the tight-binding model, which includes only the hopping term. For strong interactions, it can give qualitatively different behavior from the tight-binding model and correctly predicts the existence of so-called Mott insulators, which are prevented from becoming conducting by the strong repulsion between the particles.",
            "score": 54.213751792907715
        },
        {
            "docid": "1117979_26",
            "document": "VSEPR theory . Many transition metal compounds have unusual geometries, which can be ascribed to ligand bonding interaction with the d subshell and to absence of valence shell lone pairs. Gillespie suggested that this interaction can be weak or strong. Weak interaction is dealt with by the Kepert model, while strong interaction produces bonding pairs that also occupy the respective antipodal points of the sphere. This is similar to predictions based on sd hybrid orbitals using the VALBOND theory. The repulsion of these bidirectional bonding pairs leads to a different prediction of shapes.",
            "score": 46.63704800605774
        },
        {
            "docid": "40435056_3",
            "document": "Equation-free modeling . In a wide range of chemical, physical and biological systems, coherent macroscopic behavior emerges from interactions between microscopic entities themselves (molecules, cells, grains, animals in a population, agents) and with their environment. Sometimes, remarkably, a coarse-scale differential equation model (such as the Navier-Stokes equations for fluid flow, or a reaction-diffusion system) can accurately describe macroscopic behavior. Such macroscale modeling makes use of general principles of conservation (atoms, particles, mass, momentum, energy), and closed into a well-posed system through phenomenological constitutive equations or equations of state. However, one increasingly encounters complex systems that only have known microscopic, fine scale, models. In such cases, although we observe the emergence of coarse-scale, macroscopic behavior, modeling it through explicit closure relations may be impossible or impractical. Non-Newtonian fluid flow, chemotaxis, porous media transport, epidemiology, brain modeling and neuronal systems are some typical examples. Equation-free modeling aims to use such microscale models to predict coarse macroscale emergent phenomena.",
            "score": 53.39545154571533
        },
        {
            "docid": "22415983_21",
            "document": "Collective animal behavior . The simplest mathematical models of animal aggregations generally instruct the individual animals to follow three rules: An example of such a simulation is the Boids program created by Craig Reynolds in 1986. Another is the Self Propelled Particle model. Many current models use variations on these rules. For instance, many models implement these three rules through layered zones around each animal. In the zone of repulsion very close to the animal, the focal animal will seek to distance itself from its neighbors in order to avoid a collision. In the slightly further away zone of alignment, a focal animal will seek to align its direction of motion with its neighbors. In the outmost zone of attraction, which extends as far away from the focal animal as it is able to sense, the focal animal will seeks to move towards a neighbor. The shape of these zones will necessarily be affected by the sensory capabilities of the animal. For example, the visual field of a bird does not extend behind its body. Fish, on the other hand, rely on both vision and on hydrodynamic signals relayed through its lateral line. Antarctic krill rely on vision and on hydrodynamic signals relayed through its antennae.",
            "score": 43.35094118118286
        },
        {
            "docid": "29181673_2",
            "document": "MOSCED . MOSCED (short for \u201cmodified separation of cohesive energy density model) is a thermodynamic model for the estimation of limiting activity coefficients (also known as activity coefficient at infinite dilution). From a historical point of view MOSCED can be regarded as an improved modification of the Hansen method and the Hildebrand solubility model by adding higher interaction term such as polarity, induction and separation of hydrogen bonding terms.This allows the prediction of polar and associative compounds, which most solubility parameter models have been found to do poorly.In addition to making quantitative prediction,MOSCED can be used to understand fundamental molecular level interaction for intuitive solvent selection and formulation .",
            "score": 58.49637234210968
        },
        {
            "docid": "57153310_11",
            "document": "Genome architecture mapping . The SLICE Model is based on a hypothesis that the probability of non-interacting loci falls into the same NP is predictable. The probability is depended on the distance of these loci. The SLICE Model considers a pair of loci as two types: one is interacting, the other is non-interacting. As the hypothesis, the proportions of nuclear profiles state can be predicted by mathematical analysis. By deriving a function of the interaction probability, these GAM data can also be used to find prominent interactions and explore the sensitivity of GAM.",
            "score": 50.985092639923096
        },
        {
            "docid": "3135539_14",
            "document": "Threading (protein sequence) . Homology modeling treats the template in an alignment as a sequence, and only sequence homology is used for prediction. Protein threading treats the template in an alignment as a structure, and both sequence and structure information extracted from the alignment are used for prediction. When there is no significant homology found, protein threading can make a prediction based on the structure information. That also explains why protein threading may be more effective than homology modeling in many cases.",
            "score": 47.8781658411026
        },
        {
            "docid": "1037854_32",
            "document": "Free electron model . Interestingly, adding repulsive interactions between electrons does not change very much the picture presented here. Lev Landau showed that a Fermi gas under repulsive interactions, can be seen as a gas of equivalent quasiparticles that slightly modify the properties of the metal. Landau's model is now known as the Fermi liquid theory. More exotic phenomena like superconductivity, where interactions can be attractive, require a more refined theory.",
            "score": 42.6791615486145
        },
        {
            "docid": "51865496_8",
            "document": "Crowd analysis . There are countless social applications of crowd analysis ranging from uses within the film and video game industries, to uses in public planning. Being that crowd simulations are based on group dynamics and crowd psychology, the accuracy and relevance to real life situations is clear. A large aspect of public planning and its use of crowd analysis lies within the realm of situational representations for emergency evacuation. Evacuations can be planned via the modeling and study of crowd interaction and reaction. These representations are based on biological models and patterns, thus the movements predicted are quite realistic. Similar models are utilized within motion picture industries to produce realistic and life-like simulations and scenes.  A system can generate a realistic crowd simulation with given inputs and simulate how the simulated moving objects, or agents, will interact with each other and with the environment. The goal is to replicate a crowd's movement patterns given a large number of agents in a given space. Algorithms based on crowd analysis attempt to manage the movement of the crowd. The more efficient and realistic a simulation becomes, the more complex the algorithm must become. The software must be able to manipulate the trajectory of individual agents based on variables such as the agents' goals, stress forces, obstacles, and levels of arousal. There are several software utilized to develop and study crowd dynamics:",
            "score": 51.096118807792664
        },
        {
            "docid": "985619_3",
            "document": "Agent-based model . Agent-based models are a kind of microscale model that simulate the simultaneous operations and interactions of multiple agents in an attempt to re-create and predict the appearance of complex phenomena. The process is one of emergence from the lower (micro) level of systems to a higher (macro) level. As such, a key notion is that simple behavioral rules generate complex behavior. This principle, known as K.I.S.S. (\"Keep it simple, stupid\"), is extensively adopted in the modeling community. Another central tenet is that the whole is greater than the sum of the parts. Individual agents are typically characterized as boundedly rational, presumed to be acting in what they perceive as their own interests, such as reproduction, economic benefit, or social status, using heuristics or simple decision-making rules. ABM agents may experience \"learning\", adaptation, and reproduction.",
            "score": 55.101253032684326
        },
        {
            "docid": "22313802_4",
            "document": "Compaction simulation . Discrete element method (DEM) is an explicit numerical model capable of tracking the motion and interaction of individual modeled particles. DEM has enhanced rapidly our understanding of granular system by producing quantitative predictions rather than only qualitative description, increased our insight into particle assemblies by providing both microscopic and macroscopic information. DEM has been proved to be of great potential in scientific tasks and industries, including chemical and mechanical engineering, food industry, geo-sciences and agriculture.",
            "score": 54.645288705825806
        },
        {
            "docid": "10902_6",
            "document": "Force . With modern insights into quantum mechanics and technology that can accelerate particles close to the speed of light, particle physics has devised a Standard Model to describe forces between particles smaller than atoms. The Standard Model predicts that exchanged particles called gauge bosons are the fundamental means by which forces are emitted and absorbed. Only four main interactions are known: in order of decreasing strength, they are: strong, electromagnetic, weak, and gravitational. High-energy particle physics observations made during the 1970s and 1980s confirmed that the weak and electromagnetic forces are expressions of a more fundamental electroweak interaction.",
            "score": 62.191651821136475
        },
        {
            "docid": "886876_48",
            "document": "Crowd simulation . \"Modeling\" techniques of crowds vary from holistic or network approaches to understanding individualistic or behavioral aspects of each agent. For example, the Social Force Model describes a need for individuals to find a balance between social interaction and physical interaction. An approach that incorporates both aspects, and is able to adapt depending on the situation, would better describe natural human behavior, always incorporating some measure of unpredictability. With the use of multi-agent models understanding these complex behaviors has become a much more comprehensible task. With the use of this type of software, systems can now be tested under extreme conditions, and simulate conditions over long periods of time in the matter of seconds.",
            "score": 43.85918879508972
        },
        {
            "docid": "50956705_2",
            "document": "Coarse-grained modeling . Coarse-grained modeling, coarse-grained models, aim at simulating the behaviour of complex systems using their coarse-grained (simplified) representation. Coarse-grained models are widely used for molecular modeling of biomolecules at various granularity levels. A wide range of coarse-grained models have been proposed. They are usually dedicated to computational modeling of specific molecules: proteins, nucleic acids, lipid membranes, carbohydrates or water. In these models, molecules are represented by individual atoms and pseudo-atoms (that replace the group of atoms), or pseudo-atoms only. By decreasing the degrees of freedom much longer simulation times can be studied than using classical atomistic models. Coarse-grained models have found practical applications in: protein structure prediction, prediction of protein interactions and molecular dynamics simulations of protein folding.",
            "score": 56.369001626968384
        },
        {
            "docid": "11129423_7",
            "document": "Social impact theory . The social impact theory is both a generalizable and a specific theory. It uses one set of equations, which are applicable to many social situations. For example, the psychosocial law can be used to predict instances of conformity, imitation and embarrassment. Yet, it is also specific because the predictions that it makes are specific and can be applied to and observed in the world. The theory is falsifiable as well. It makes predictions through the use of equations; however, the equations may not be able to accurately predict the outcome of social situations. Social impact theory is also useful. It can be used to understand which social situations result in the greatest impact and which situations present exceptions to the rules. While Social Impact theory explores social situations and can help predict the outcomes of social situations, it also has some shortcomings and questions that are left unresolved. The rules guiding the theory depict people as recipients that passively accept social impact and do not take into account the social impact that people may actively seek out. The model is also static, and does not fully compensate for the dynamics involved in social interactions. The theory is relatively new and fails to address some pertinent issues. These issues include finding more accurate ways to measure social outcomes, understanding the \"t\" exponent in psychosocial law, taking susceptibility into account, understanding how short-term consequences can develop into chronic consequences, application to group interactions, understanding the model's nature (descriptive vs. explanatory, generalization vs. theory).",
            "score": 53.62945556640625
        },
        {
            "docid": "28748337_13",
            "document": "Dynamic tidal power . No DTP dam has ever been built, although all of the technologies required to build a DTP dam are available. Various mathematical and physical models have been conducted to model and predict the 'head' or water level differential over a dynamic tidal power dam. The interaction between tides and long dams has been observed and recorded in large engineering projects, such as the Delta Works and the Afsluitdijk in the Netherlands. The interaction of tidal currents with natural peninsulas is also well-known, and such data is used to calibrate numerical models of tides. Formulas for the calculation of added mass were applied to develop an analytical model of DTP. Observed water level differentials closely match current analytical and numerical models. Water level differential generated over a DTP dam can now be predicted with a useful degree of accuracy.",
            "score": 56.53816318511963
        },
        {
            "docid": "49134790_16",
            "document": "Frenkel\u2013Kontorova model . In this section we examine in detail the simplest form of the FK-model. A detailed version of this derivation can be found in the following paper.  The model, shown schematically in figure 1, describes a one-dimensional chain of atoms with a harmonic nearest neighbor interaction and subject to a sinusoidal potential. Transverse motion of the atoms is ignored, i.e. the atoms can only move along the chain.  The Hamiltonian for this situation is given by formula_7 where we specify the interaction potential to be",
            "score": 50.01404404640198
        },
        {
            "docid": "7623862_13",
            "document": "Rubber elasticity . When these elastic force models are combined with the complex morphology of the network, it is not possible to obtain simple analytic formulae to predict the macroscopic stress. It is only via numerical simulations on computers that it is possible to capture the complex interaction between the molecular forces and the network morphology to predict the stress and ultimate failure of a rubber sample as it is strained.",
            "score": 53.5160608291626
        },
        {
            "docid": "4881262_13",
            "document": "Exploratory search . There have been recent attempts to develop a process model of exploratory search behavior, especially in social information system (e.g., see models of collaborative tagging. Recent development in exploratory search is often concentrated in predicting users' search intents in interaction with the user. Such predictive user modeling, also referred as intent modeling, can help users to get accustomed to a body of domain knowledge and help users to make sense of the potential directions to be explored around their initial, often vague, expression of information needs. Key figures, including experts from both information seeking and human\u2013computer interaction, are:",
            "score": 55.83127808570862
        },
        {
            "docid": "20018985_12",
            "document": "Enterprise engineering . Unified Modeling Language (UML) is a broadly accepted modeling language for the development of software systems and applications. Many within the Object-oriented analysis and design community also use UML for enterprise modeling purposes. Here, emphasis is placed on the usage of enterprise objects or business objects from which complex enterprise systems are made. A collection of these objects and corresponding interactions between them can represent a complex business system or process. While Petri Nets focus on the interaction and states of objects, UML focuses more on the business objects themselves. Sometimes these are called \u201centerprise building blocks\u201d and includes resources, processes, goals, rules and metamodels. Despite the fact that UML can be used to model an integrated software system, it has been argued that the reality of business can be modeled with a software modeling language. In response, the object oriented community makes business extensions for UML and adapts the language accordingly. Extended Enterprise Modeling Language (EEML) is derived from UML and is proposed as a business modeling language. The question remains as to whether this business transformation is the correct method to use, as it was earlier said that UML in combination with other \u201cpure\u2019 business methods may be a better alternative.",
            "score": 47.4733247756958
        },
        {
            "docid": "21331886_4",
            "document": "Paola Sebastiani . Her most important contribution is a model based on a Bayesian network that integrates more than 60 single-nucleotide polymorphisms (SNPs) and other biomarkers to compute the risk for stroke in patients with sickle cell anemia. This model was shown to have high sensitivity and specificity and demonstrated, for the first time, how an accurate risk prediction model of a complex genetic trait that is modulated by several interacting genes can be built using Bayesian networks.",
            "score": 61.669921875
        },
        {
            "docid": "2843988_27",
            "document": "Motor control . Forward models are a predictive internal model of motor control that takes the available perceptual information, combined with a particular motor program, and tries to predict the outcome of the planned motor movement. Forward models structure action by determining how the forces, velocities, and positions of motor components affect changes in the environment and in the individual. It is proposed that forward models help with the Neural control of limb stiffness when individuals interact with their environment. Forward models are thought to use motor programs as input to predict the outcome of an action. An error signal is generated when the predictions made by a forward model do not match the actual outcome of the movement, prompting an update of an existing model and providing a mechanism for learning. These models explain why it is impossible to tickle yourself. A sensation is experienced as ticklish when it is unpredictable. However, forward models predict the outcome of your motor movements, meaning the motion is predictable, and therefore not ticklish.",
            "score": 61.239025354385376
        }
    ],
    "r": [
        {
            "docid": "40158142_9",
            "document": "Nonlinear system identification . Artificial neural networks try loosely to imitate the network of neurons in the brain where computation takes place through a large number of simple processing elements. A typical neural network consists of a number of simple processing units interconnected to form a complex network. Layers of such units are arranged so that data is entered at the input layer and passes through either one or several intermediate layers before reaching the output layer. In supervised learning the network is trained by operating on the difference between the actual output and the desired output of the network, the prediction error, to change the connection strengths between the nodes. By iterating the weights are modified until the output error reaches an acceptable level. This process is called machine learning because the network adjusts the weights so that the output pattern is reproduced. Neural networks have been extensively studied and there are many excellent textbooks devoted to this topic in general, and more focussed textbooks which emphasise control and systems applications. There are two main problem types that can be studied using neural networks: static problems, and dynamic problems. Static problems include pattern recognition, classification, and approximation. Dynamic problems involve lagged variables and are more appropriate for system identification and related applications. Depending on the architecture of the network the training problem can be either nonlinear-in-the-parameters which involves optimisation or linear-in-the-parameters which can be solved using classical approaches. The training algorithms can be categorised into supervised, unsupervised, or reinforcement learning. Neural networks have excellent approximation properties but these are usually based on standard function approximation results using for example the Weierstrass Theorem that applies equally well to polynomials, rational functions, and other well-known models.  Neural networks have been applied extensively to system identification problems which involve nonlinear and dynamic relationships. However, classical neural networks are purely gross static approximating machines. There is no dynamics within the network. Hence when fitting dynamic models all the dynamics arise by allocating lagged inputs and outputs to the input layer of the network. The training procedure then produces the best static approximation that relates the lagged variables assigned to the input nodes to the output. There are more complex network architectures, including recurrent networks, that produce dynamics by introducing increasing orders of lagged variables to the input nodes. But in these cases it is very easy to over specify the lags and this can lead to over fitting and poor generalisation properties.  Neural networks have several advantages; they are conceptually simple, easy to train and to use, have excellent approximation properties, the concept of local and parallel processing is important and this provides integrity and fault tolerant behaviour. The biggest criticism of the classical neural network models is that the models produced are completely opaque and usually cannot be written down or analysed. It is therefore very difficult to know what is causing what, to analyse the model, or to compute dynamic characteristics from the model. Some of these points will not be relevant to all applications but they are for dynamic modelling.",
            "score": 85.55809783935547
        },
        {
            "docid": "106418_4",
            "document": "Computational physics . In physics, different theories based on mathematical models provide very precise predictions on how systems behave. Unfortunately, it is often the case that solving the mathematical model for a particular system in order to produce a useful prediction is not feasible. This can occur, for instance, when the solution does not have a closed-form expression, or is too complicated. In such cases, numerical approximations are required. Computational physics is the subject that deals with these numerical approximations: the approximation of the solution is written as a finite (and typically large) number of simple mathematical operations (algorithm), and a computer is used to perform these operations and compute an approximated solution and respective error.",
            "score": 82.19572448730469
        },
        {
            "docid": "24574814_3",
            "document": "Models of collaborative tagging . Just like any social phenomena, behavioral patterns in social tagging systems can be characterized by either a descriptive or predictive model. While descriptive models ask the question of \"what\", predictive models go deeper to also ask the question of \"why\" by attempting to provide explanations to the aggregate behavioral patterns. While there may be no general agreement on what an acceptable explanation should be like, many believe that a good explanation should have certain level of predictive accuracy. Descriptive models of social tagging typically are not concerned with explaining the actions of single individuals but describing the patterns that emerge as individual behavior is aggregated in a large social information system. Predictive models, however, attempt to explain aggregate patterns by analyzing how individuals interact and link to each other in ways that bring about similar or different emergent patterns of social behavior. In particular, a mechanism-based predictive model assumes a certain set of rule that individuals interact with each other, and understand how these interactions could produce aggregate patterns as observed and characterized by descriptive models. Predictive models can therefore provide explanations to why different system characteristics may lead to different aggregate patterns, and can therefore potentially provide information on how systems should be designed to achieve different social purposes.",
            "score": 78.2698745727539
        },
        {
            "docid": "56775942_7",
            "document": "Mathematical models of social learning . The statistician George E. P. Box once said, \"All models are wrong; however, some of them are useful.\" Along the same lines, the DeGroot model is a fairly simple model but it can provide us with useful insights about the learning process in social networks. Indeed, the simplicity of this model makes it tractable for theoretical studies. Specifically, we can analyze different network structure to see for which structures these naive agents can successfully aggregate decentralized information. Since the DeGroot model can be considered a Markov chain, provided that a network is strongly connected (so there is a direct path from any agent to any other) and satisfies a weak aperiodicity condition, beliefs will converge to a consensus. When consensus is reached, the belief of each agent is a weighted average of agents' initial beliefs. These weights provide a measure of social influence.",
            "score": 77.08240509033203
        },
        {
            "docid": "841429_24",
            "document": "Synthetic biology . Models inform the design of engineered biological systems by better predicting system behavior prior to fabrication. Synthetic biology benefits from better models of how biological molecules bind substrates and catalyze reactions, how DNA encodes the information needed to specify the cell and how multi-component integrated systems behave. Multiscale models of gene regulatory networks focus on synthetic biology applications. Simulations can model all biomolecular interactions in transcription, translation, regulation and induction of gene regulatory networks. In a living cell, molecular motifs are embedded in a bigger network with upstream and downstream components. These components may alter the signalling capability of the modeling module. In the case of ultrasensitive modules, the sensitivity contribution of a module can differ from the sensitivity that the module sustains in isolation.",
            "score": 74.07862091064453
        },
        {
            "docid": "44046734_5",
            "document": "Matched molecular pair analysis . MMPA is quite useful in the field of quantitative structure\u2013activity relationship (QSAR) modelling studies. One of the issues of QSAR models is they are difficult to interpret in a chemically meaningful manner. While it can be pretty easy to interpret simple linear regression models, the most powerful algorithms like neural networks, support vector machine are similar to \"black boxes\", which provide predictions that can't be easily interpreted. This problem undermines the applicability of QSAR model in helping the medicinal chemist to make the decision. If the compound is predicted to be active against some microorganism, what are the driving factors of its activity? Or if it is predicted to be inactive, how its activity can be modulated? The black box nature of the QSAR model prevents it from addressing these crucial issues. The use of predicted MMPs allows to interpret models and identify which MMPs were learned by the model. The MMPs, which were not reproduced by the model, could correspond to experimental errors or deficiency of the model (inappropriate descriptors, too few data, etc.).",
            "score": 73.95367431640625
        },
        {
            "docid": "33980770_31",
            "document": "Organ-on-a-chip . Mathematical pharmacokinetic (PK) models aim to estimate concentration-time profiles within each organ on the basis of the initial drug dose. Such mathematical models can be relatively simple, treating the body as a single compartment in which the drug distribution reaches a rapid equilibrium after administration. Mathematical models can be highly accurate when all parameters involved are known. Models that combine PK or PBPK models with PD models can predict the time-dependent pharmacological effects of a drug. We can nowadays predict with PBPK models the PK of about any chemical in humans, almost from first principles. These models can be either very simple, like statistical dose-response models, or sophisticated and based on systems biology, according to the goal pursued and the data available. All we need for those models are good parameter values for the molecule of interest.",
            "score": 72.68380737304688
        },
        {
            "docid": "1726672_12",
            "document": "Neural circuit . Connectionist models serve as a test platform for different hypotheses of representation, information processing, and signal transmission. Lesioning studies in such models, e.g. artificial neural networks, where parts of the nodes are deliberately destroyed to see how the network performs, can also yield important insights in the working of several cell assemblies. Similarly, simulations of dysfunctional neurotransmitters in neurological conditions (e.g., dopamine in the basal ganglia of Parkinson's patients) can yield insights into the underlying mechanisms for patterns of cognitive deficits observed in the particular patient group. Predictions from these models can be tested in patients or via pharmacological manipulations, and these studies can in turn be used to inform the models, making the process iterative.",
            "score": 72.57646942138672
        },
        {
            "docid": "37438_36",
            "document": "Complex system . One of Friedrich Hayek's main contributions to early complexity theory is his distinction between the human capacity to predict the behaviour of simple systems and its capacity to predict the behaviour of complex systems through modeling. He believed that economics and the sciences of complex phenomena in general, which in his view included biology, psychology, and so on, could not be modeled after the sciences that deal with essentially simple phenomena like physics. Hayek would notably explain that complex phenomena, through modeling, can only allow pattern predictions, compared with the precise predictions that can be made out of non-complex phenomena.",
            "score": 70.60213470458984
        },
        {
            "docid": "34022094_4",
            "document": "Active perception . \"Active Perception (Active Vision specifically) is defined as a study of Modeling and Control strategies for perception. By modeling we mean models of sensors, processing modules and their interaction. We distinguish local models from global models by their extent of application in space and time. The local models represent procedures and parameters such as optical distortions of the lens, focal lens, spatial resolution, band-pass filter, etc. The global models on the other hand characterize the overall performance and make predictions on how the individual modules interact. The control strategies are formulated as a search of such sequence of steps that would minimize a loss function while one is seeking the most information. Examples are shown as the existence proof of the proposed theory on obtaining range from focus and sterolvergence on 2-0 segmentation of an image and 3-0 shape parametrization\".",
            "score": 70.264892578125
        },
        {
            "docid": "45329906_21",
            "document": "Solvent models . Quantitative Structure\u2013Activity Relationships (QSAR)/Quantitative Structure\u2013Property Relationships (QSPR), whilst unable to directly model the physical process occurring in a condensed solvent phase, can provide useful predictions of solvent and solvation properties and activities; such as the solubility of a solute. These methods come in a varied way from simple regression models to sophisticated machine learning methods. Generally, QSAR/QSPR methods require descriptors; these come in many different forms and are used to represent physical features and properties of a system of interest. Descriptors are generally single numerical values which hold some information about a physical property. A regression model or statistical learning model is then applied to find a correlation between the descriptor(s) and the property of interest. Once trained on some known data these model can be applied to similar unknown data to make predictions. Typically the known data comes from experimental measurement, although there is no reason why similar methods can not be used to correlate descriptor(s) with theoretical or predicted values. It is currently debated whether if more accurate experimental data was used to train these models whether the prediction from such models would be more accurate.",
            "score": 70.07235717773438
        },
        {
            "docid": "48684895_7",
            "document": "Electricity price forecasting . Multi-agent models generally focus on qualitative issues rather than quantitative results. They may provide insights as to whether or not prices will be above marginal costs, and how this might influence the players\u2019 outcomes. However, they pose problems if more quantitative conclusions have to be drawn, particularly if electricity prices have to be predicted with a high level of precision.",
            "score": 69.85694885253906
        },
        {
            "docid": "44182726_16",
            "document": "Continuum model of impression formation . In 1996, Kunda and Thagard proposed a parallel-constraint-satisfaction theory of impression formation, which focuses on social stereotypes, target traits, and behaviors that influence the impressions people form. Kunda and Thagard contrasted their theory with the continuum model, criticizing the continuum model for its \"alleged serial nature\" as well as the \"priority given to social stereotype information over individuating information.\" The alleged differences between the two models are much less significant than they appear. The Kunda\u2013Thagard model provides a mechanism where the features of a target can constrain the meaning of other features as a way of capturing the flow from categorization to recategorization in a more dynamic way, as well as gives consideration to how the response of a target may be different, despite given identical information. The continuum model argues that the influencing factors, which categorizes targets, connects the motivational and attention aspects of the model, bringing target information serially into the system. The continuum model also shows that different features of a target shapes how it is organized into social categories. Certain features, such as race or gender, are usually predominant because of their visual accessibility. Both models have strengths in different areas but a combination of the two would move beyond the debate between which is most important, serial or parallel processing, and allow both to be used together. Although both models have merit in the way they predict the processing of information, the dynamic nature of the continuum model and its integration of serial and parallel processing makes it the most comprehensive model for predicting impressions.",
            "score": 69.63424682617188
        },
        {
            "docid": "9272073_45",
            "document": "Option (finance) . Closely following the derivation of Black and Scholes, John Cox, Stephen Ross and Mark Rubinstein developed the original version of the binomial options pricing model. It models the dynamics of the option's theoretical value for discrete time intervals over the option's life. The model starts with a binomial tree of discrete future possible underlying stock prices. By constructing a riskless portfolio of an option and stock (as in the Black\u2013Scholes model) a simple formula can be used to find the option price at each node in the tree. This value can approximate the theoretical value produced by Black Scholes, to the desired degree of precision. However, the binomial model is considered more accurate than Black\u2013Scholes because it is more flexible; e.g., discrete future dividend payments can be modeled correctly at the proper forward time steps, and American options can be modeled as well as European ones. Binomial models are widely used by professional option traders. The Trinomial tree is a similar model, allowing for an up, down or stable path; although considered more accurate, particularly when fewer time-steps are modelled, it is less commonly used as its implementation is more complex. For a more general discussion, as well as for application to commodities, interest rates and hybrid instruments, see Lattice model (finance).",
            "score": 69.40396881103516
        },
        {
            "docid": "4350008_8",
            "document": "Protein\u2013protein interaction prediction . This group of methods makes use of known protein complex structures to predict and structurally model interactions between query protein sequences. The prediction process generally starts by employing a sequence based method (e.g. Interolog) to search for protein complex structures that are homologous to the query sequences. These known complex structures are then used as templates to structurally model the interaction between query sequences. This method has the advantage of not only inferring protein interactions but also suggests models of how proteins interact structurally, which can provide some insights into the atomic level mechanism of that interaction. On the other hand, the ability for these methods to make a prediction is constrained by a limited number of known protein complex structures.",
            "score": 69.3768081665039
        },
        {
            "docid": "12610_9",
            "document": "Grand Unified Theory . The fact that the electric charges of electrons and protons seem to cancel each other exactly to extreme precision is essential for the existence of the macroscopic world as we know it, but this important property of elementary particles is not explained in the Standard Model of particle physics. While the description of strong and weak interactions within the Standard Model is based on gauge symmetries governed by the simple symmetry groups and which allow only discrete charges, the remaining component, the weak hypercharge interaction is described by an abelian symmetry which in principle allows for arbitrary charge assignments. The observed charge quantization, namely the fact that all known elementary particles carry electric charges which appear to be exact multiples of \u2153 of the \"elementary\" charge, has led to the idea that hypercharge interactions and possibly the strong and weak interactions might be embedded in one Grand Unified interaction described by a single, larger simple symmetry group containing the Standard Model. This would automatically predict the quantized nature and values of all elementary particle charges. Since this also results in a prediction for the relative strengths of the fundamental interactions which we observe, in particular the weak mixing angle, Grand Unification ideally reduces the number of independent input parameters, but is also constrained by observations.",
            "score": 68.93512725830078
        },
        {
            "docid": "50399682_11",
            "document": "Predictive engineering analytics . 1D system simulation, also referred to as 1D CAE or mechatronics system simulation, allows scalable modeling of multi-domain systems. The full system is presented in a schematic way, by connecting validated analytical modeling blocks of electrical, hydraulic, pneumatic and mechanical subsystems (including control systems). It helps engineers predict the behavior of concept designs of complex mechatronics, either transient or steady-state.  Manufacturers often have validated libraries available that contain predefined components for different physical domains. Or if not, specialized software suppliers can provide them. Using those, the engineers can do concept predictions very early, even before any Computer-aided Design (CAD) geometry is available. During later stages, parameters can then be adapted. 1D system simulation calculations are very efficient. The components are analytically defined, and have input and output ports. Causality is created by connecting inputs of a components to outputs of another one (and vice versa). Models can have various degrees of complexity, and can reach very high accuracy as they evolve. Some model versions may allow real-time simulation, which is particularly useful during control systems development or as part of built-in predictive functionality.",
            "score": 68.86748504638672
        },
        {
            "docid": "706999_11",
            "document": "Atmospheric chemistry . In order to synthesise and test theoretical understanding of atmospheric chemistry, computer models (such as chemical transport models) are used. Numerical models solve the differential equations governing the concentrations of chemicals in the atmosphere. They can be very simple or very complicated. One common trade off in numerical models is between the number of chemical compounds and chemical reactions modelled versus the representation of transport and mixing in the atmosphere. For example, a box model might include hundreds or even thousands of chemical reactions but will only have a very crude representation of mixing in the atmosphere. In contrast, 3D models represent many of the physical processes of the atmosphere but due to constraints on computer resources will have far fewer chemical reactions and compounds. Models can be used to interpret observations, test understanding of chemical reactions and predict future concentrations of chemical compounds in the atmosphere. One important current trend is for atmospheric chemistry modules to become one part of earth system models in which the links between climate, atmospheric composition and the biosphere can be studied.",
            "score": 67.8472900390625
        },
        {
            "docid": "40158142_12",
            "document": "Nonlinear system identification . Structure detection forms the most fundamental part of NARMAX. For example a NARMAX model which consists of one lagged input and one lagged output term, three lagged noise terms, expanded as a cubic polynomial would consist of fifty six possible candidate terms. This number of candidate terms arises because the expansion by definition includes all possible combinations within the cubic expansion. Naively proceeding to estimate a model which includes all these terms and then pruning will cause numerical and computational problems and should always be avoided. However, only a few terms are often important in the model. Structure detection, which aims to select terms one at a time, is therefore critically important. These objectives can easily be achieved by using the Orthogonal Least Squares algorithm and its derivatives to select the NARMAX model terms one at a time. These ideas can also be adapted for pattern recognition and feature selection and provide an alternative to principal component analysis but with the advantage that the features are revealed as basis functions that are easily related back to the original problem.  NARMAX methods are designed to do far more than to just find the best approximating model. System identification can be divided into two aims. The first involves approximation where the key aim is to develop a model that approximates the data set such that good predictions can be made. There are many applications where this approach is appropriate, for example in time series prediction of the weather, stock prices, speech, target tracking, pattern classification etc. In such applications the form of the model is not that important. The objective is to find an approximation scheme which produces the minimum prediction errors. A second objective of system identification, which includes the first objective as a subset, involves much more than just finding a model to achieve the best mean squared errors. This second aim is why the NARMAX philosophy was developed and is linked to the idea of finding the simplest model structure. The aim here is to develop models that reproduce the dynamic characteristics of the underlying system, to find the simplest possible model, and if possible to relate this to components and behaviours of the system under study. The core aim of this second approach to identification is therefore to identify and reveal the rule that represents the system. These objectives are relevant to model simulation and control systems design, but increasingly to applications in medicine, neuro science, and the life sciences. Here the aim is to identify models, often nonlinear, that can be used to understand the basic mechanisms of how these systems operate and behave so that we can manipulate and utilise these. NARMAX methods have also been developed in the frequency and spatio-temporal domains.",
            "score": 67.20498657226562
        },
        {
            "docid": "21855574_5",
            "document": "Brain simulation . The connectivity of the neural circuit for touch sensitivity of the simple C. elegans nematode (roundworm) was mapped in 1985 and partly simulated in 1993. Since 2004, many software simulations of the complete neural and muscular system have been developed, including simulation of the worm's physical environment. Some of these models have been made available for download. However, there is still a lack of understanding of how the neurons and the connections between them generate the surprisingly complex range of behaviors that are observed in the relatively simple organism. This contrast between the apparent simplicity of how the mapped neurons interact with their neighbours, and exceeding complexity of the overall brain function, is an example of an emergent property. Interestingly, this kind of emergent property is paralleled within artificial neural networks, the neurons of which are exceedingly simple compared to their often complex, abstract outputs.",
            "score": 67.02349853515625
        },
        {
            "docid": "14938064_6",
            "document": "Computational epigenetics . A substantial amount of bioinformatic research has been devoted to the prediction of epigenetic information from characteristics of the genome sequence. Such predictions serve a dual purpose. First, accurate epigenome predictions can substitute for experimental data, to some degree, which is particularly relevant for newly discovered epigenetic mechanisms and for species other than human and mouse. Second, prediction algorithms build statistical models of epigenetic information from training data and can therefore act as a first step toward quantitative modeling of an epigenetic mechanism. Successful computational prediction of DNA and lysine methylation and acetylation has been achieved by combinations of various features. The important role of epigenetic defects for cancer opens up new opportunities for improved diagnosis and therapy. These active areas of research give rise to two questions that are particularly amenable to bioinformatic analysis. First, given a list of genomic regions exhibiting epigenetic differences between tumor cells and controls (or between different disease subtypes), can we detect common patterns or find evidence of a functional relationship of these regions to cancer? Second, can we use bioinformatic methods in order to improve diagnosis and therapy by detecting and classifying important disease subtypes?",
            "score": 66.77784729003906
        },
        {
            "docid": "24574814_10",
            "document": "Models of collaborative tagging . Descriptive models mentioned above were based on analyses of word-word relations as revealed by the various statistical structures in the organization of tags (e.g., how likely one tag would co-occur with other tags or how likely each tag was reused over time). These models are therefore descriptive models at the aggregate level, and have little to offer about predictions at the level of interface interactions and cognitive processes of individual.  Rather than imitating other users at the word level, one possible explanation for this kind of social cohesion could be grounded on the natural tendency for people to process tags at the semantic level, and it was at this level of processing that most imitation occurred. This explanation was supported by research in the area of reading comprehension, which showed that people tended to be influenced by meanings of words, rather than the words themselves during comprehension. Assuming that background knowledge of people in the same culture tend to have shared structures (e.g., using similar vocabularies and their corresponding meanings in order to conform and communicate with each), users of the same social tagging system may also share similar semantic representations of words and concepts, even when the use of tags may vary across individuals at the word level. In other words, we argued that part of the reason for the stability of social tagging systems can be attributed to the shared semantic representations among the users, such that users may have relatively stable and coherent interpretation of information contents and tags as they interact with the system. Based on this assumption, the semantic imitation model predicts how different semantic representations may lead to differences in individual tag choices and eventually different emergent properties at the aggregate behavioral level. The model also predicts that the folksonomies (i.e., knowledge structures) in the system reflect the shared semantic representations of the users.",
            "score": 66.59879302978516
        },
        {
            "docid": "271430_22",
            "document": "Computational neuroscience . The interactions of neurons in a small network can be often reduced to simple models such as the Ising model. The statistical mechanics of such simple systems are well-characterized theoretically. There has been some recent evidence that suggests that dynamics of arbitrary neuronal networks can be reduced to pairwise interactions. It is not known, however, whether such descriptive dynamics impart any important computational function. With the emergence of two-photon microscopy and calcium imaging, we now have powerful experimental methods with which to test the new theories regarding neuronal networks.",
            "score": 66.50154113769531
        },
        {
            "docid": "5620279_3",
            "document": "Hydrological transport model . There are dozens of different transport models that can be generally grouped by pollutants addressed, complexity of pollutant sources, whether the model is steady state or dynamic, and time period modeled. Another important designation is whether the model is distributed (i.e. capable of predicting multiple points within a river) or lumped. In a basic model, for example, only one pollutant might be addressed from a simple point discharge into the receiving waters. In the most complex of models, various line source inputs from surface runoff might be added to multiple point sources, treating a variety of chemicals plus sediment in a dynamic environment including vertical river stratification and interactions of pollutants with in-stream biota. In addition watershed groundwater may also be included. The model is termed \"physically based\" if its parameters can be measured in the field.",
            "score": 66.16606140136719
        },
        {
            "docid": "25092894_15",
            "document": "Projection pursuit regression . Note that because PPR attempts to fit projections of the data, it can be difficult to interpret the fitted model as a whole, because each input variable has been accounted for in a complex and multifaceted way. This can make the model more useful for prediction than for understanding the data, though visualizing individual ridge functions and considering which projections the model is discovering can yield some insight.",
            "score": 66.12500762939453
        },
        {
            "docid": "8054792_5",
            "document": "Background selection . Background selection can be measured by assessing the degree of departure of the levels of neutral variants from the predictions of neutral model-based estimations of mutation rates and genetic drift. However, it is not enough to study variation alone because the two main forms of linked selection, background and hitchhiking, produce a loss in diversity, and the models both predict similar results in genomic regions of high recombination. The relative influence of these two effects is not yet well understood, though methods have been developed for differentiating between the two effects. One technique is to compare levels of nucleotide diversity in regions of low recombination, where the models differ appreciably in their predictions. Thus, studying variation in genomic neighborhoods with relatively low recombination rates, rather than across the whole genome, can yield insights about the relative prevalence of background and hitchhiking selection.",
            "score": 65.951416015625
        },
        {
            "docid": "179092_7",
            "document": "Neurolinguistics . Much work in neurolinguistics involves testing and evaluating theories put forth by psycholinguists and theoretical linguists. In general, theoretical linguists propose models to explain the structure of language and how language information is organized, psycholinguists propose models and algorithms to explain how language information is processed in the mind, and neurolinguists analyze brain activity to infer how biological structures (populations and networks of neurons) carry out those psycholinguistic processing algorithms. For example, experiments in sentence processing have used the ELAN, N400, and P600 brain responses to examine how physiological brain responses reflect the different predictions of sentence processing models put forth by psycholinguists, such as Janet Fodor and Lyn Frazier's \"serial\" model, and Theo Vosse and Gerard Kempen's \"unification model\". Neurolinguists can also make new predictions about the structure and organization of language based on insights about the physiology of the brain, by \"generalizing from the knowledge of neurological structures to language structure\".",
            "score": 65.74246215820312
        },
        {
            "docid": "9299409_31",
            "document": "Nucleic acid thermodynamics . A more realistic way of modeling the behavior of nucleic acids would seem to be to have parameters that depend on the neighboring groups on both sides of a nucleotide, giving a table with entries like \"TCG/AGC\". However, this would involve around 32 groups for Watson-Crick pairing and even more for sequences containing mismatches; the number of DNA melting experiments needed to get reliable data for so many groups would be inconveniently high. However, other means exist to access thermodynamic parameters of nucleic acids: microarray technology allows hybridization monitoring of tens of thousands sequences in parallel. This data, in combination with molecular adsorption theory allows the determination of many thermodynamic parameters in a single experiment and to go beyond the nearest neighbor model. In general the predictions from the nearest neighbor method agree reasonably well with experimental results, but some unexpected outlying sequences, calling for further insights, do exist. Finally, we should also mention the increased accuracy provided by single molecule unzipping assays which provide a wealth of new insight into the thermodynamics of DNA hybridization and the validity of the nearest-neighbour model as well.",
            "score": 65.45357513427734
        },
        {
            "docid": "20590_20",
            "document": "Mathematical model . In black-box models one tries to estimate both the functional form of relations between variables and the numerical parameters in those functions. Using a priori information we could end up, for example, with a set of functions that probably could describe the system adequately. If there is no a priori information we would try to use functions as general as possible to cover all different models. An often used approach for black-box models are neural networks which usually do not make assumptions about incoming data. Alternatively the NARMAX (Nonlinear AutoRegressive Moving Average model with eXogenous inputs) algorithms which were developed as part of nonlinear system identification can be used to select the model terms, determine the model structure, and estimate the unknown parameters in the presence of correlated and nonlinear noise. The advantage of NARMAX models compared to neural networks is that NARMAX produces models that can be written down and related to the underlying process, whereas neural networks produce an approximation that is opaque.",
            "score": 65.1230239868164
        },
        {
            "docid": "2860430_20",
            "document": "Neural oscillation . Computational models adopt a variety of abstractions in order to describe complex oscillatory dynamics observed in brain activity. Many models are used in the field, each defined at a different level of abstraction and trying to model different aspects of neural systems. They range from models of the short-term behaviour of individual neurons, through models of how the dynamics of neural circuitry arise from interactions between individual neurons, to models of how behaviour can arise from abstract neural modules that represent complete subsystems.",
            "score": 65.10359954833984
        },
        {
            "docid": "47152350_48",
            "document": "Human performance modeling . ACT-R has been used to model a wide variety of phenomena. It consists of several modules, each one modeling a different aspect of the human system. Modules are associated with specific brain regions, and the ACT-R has thus successfully predicted neural activity in parts of those regions. Each model essentially represents a theory of how that piece of the overall system works - derived from research literature in the area. For example, the declarative memory system in ACT-R is based on series of equations considering frequency and recency and that incorporate Baysean notions of need probability given context, also incorporating equations for learning as well as performance, Some modules are of higher fidelity than others, however - the manual module incorporates Fitt's law and other simple operating principles, but is not as detailed as the optimal control theory model (as of yet). The notion, however, is that each of these modules require strong empirical validation. This is both a benefit and a limitation to the ACT-R, as there is still much work to be done in the integration of cognitive, perceptual, and motor components, but this process is promising (Byrne, 2007; Foyle and Hooey, 2008; Pew & Mavor, 1998).",
            "score": 65.05209350585938
        },
        {
            "docid": "24048_3",
            "document": "Particle in a box . The particle in a box model is one of the very few problems in quantum mechanics which can be solved analytically, without approximations. Due to its simplicity, the model allows insight into quantum effects without the need for complicated mathematics. It serves as a simple illustration of how energy quantization (energy levels), which are found in more complicated quantum systems such as atoms and molecules, come about. It is one of the first quantum mechanics problems taught in undergraduate physics courses, and it is commonly used as an approximation for more complicated quantum systems.",
            "score": 64.61181640625
        }
    ]
}