{
    "q": [
        {
            "docid": "42734031_3",
            "document": "Bayesian hierarchical modeling . Frequentist statistics, the more popular foundation of statistics, may yield conclusions seemingly incompatible with those offered by Bayesian statistics due to the Bayesian treatment of the parameters as random variables and its use of subjective information in establishing assumptions on these parameters.. As the approaches answer different questions the formal results aren't technically contradictory but the two approaches disagree over which answer is relevant to particular applications. Bayesians argue that relevant information regarding decision making and updating beliefs cannot be ignored and that hierarchical modeling has the potential to overrule classical methods in applications where respondents give multiple observational data. Moreover, the model has proven to be robust, with the posterior distribution less sensitive to the more flexible hierarchical priors. Hierarchical modeling is used when information is available on several different levels of observational units. The hierarchical form of analysis and organization helps in the understanding of multiparameter problems and also plays an important role in developing computational strategies.",
            "score": 94.46232914924622
        },
        {
            "docid": "38265760_10",
            "document": "Hierarchical generalized linear model . A summary of commonly used models are:  Hierarchical generalized linear models are used when observations come from different clusters. There are two types of estimators: fixed effect estimators and random effect estimators, corresponding to parameters in : formula_36  and in formula_37 , respectively. There are different ways to obtain parameter estimates for a hierarchical generalized linear model. If only fixed effect estimators are of interests, the population-averaged model can be used. If inference is focused on individuals, random effects will have to be estimated. There are different techniques to fit a hierarchical generalized linear model.",
            "score": 83.0430953502655
        },
        {
            "docid": "824552_7",
            "document": "Bayes factor . When the two models are equally probable a priori, so that formula_7, the Bayes factor is equal to the ratio of posterior probabilities of \"M\" and \"M\". If instead of the Bayes factor integral, the likelihood corresponding to the maximum likelihood estimate of the parameter for each statistical model is used, then the test becomes a classical likelihood-ratio test. Unlike a likelihood-ratio test, this Bayesian model comparison does not depend on any single set of parameters, as it integrates over all parameters in each model (with respect to the respective priors). However, an advantage of the use of Bayes factors is that it automatically, and quite naturally, includes a penalty for including too much model structure. It thus guards against overfitting. For models where an explicit version of the likelihood is not available or too costly to evaluate numerically, approximate Bayesian computation can be used for model selection in a Bayesian framework, with the caveat that approximate-Bayesian estimates of Bayes factors are often biased.",
            "score": 77.27010488510132
        },
        {
            "docid": "57448874_2",
            "document": "Bayesian model reduction . Bayesian model reduction is a method for computing the evidence and posterior over the parameters of Bayesian models that differ in their priors. A full model is fitted to data using standard approaches. Hypotheses are then tested by defining one or more 'reduced' models with alternative (and usually more restrictive) priors, which usually \u2013 in the limit \u2013 switch off certain parameters. The evidence and parameters of the reduced models can then be computed from the evidence and estimated (posterior) parameters of the full model using Bayesian model reduction. If the priors and posteriors are normally distributed, then there is an analytic solution which can be computed rapidly. This has multiple scientific and engineering applications: these include scoring the evidence for large numbers of models very quickly and facilitating the estimation of hierarchical models (Parametric Empirical Bayes).",
            "score": 94.42982411384583
        },
        {
            "docid": "11864519_25",
            "document": "Approximate Bayesian computation . Outside of parameter estimation, the ABC framework can be used to compute the posterior probabilities of different candidate models. In such applications, one possibility is to use rejection sampling in a hierarchical manner. First, a model is sampled from the prior distribution for the models. Then, parameters are sampled from the prior distribution assigned to that model. Finally, a simulation is performed as in single-model ABC. The relative acceptance frequencies for the different models now approximate the posterior distribution for these models. Again, computational improvements for ABC in the space of models have been proposed, such as constructing a particle filter in the joint space of models and parameters.",
            "score": 80.46814489364624
        },
        {
            "docid": "42734031_2",
            "document": "Bayesian hierarchical modeling . Bayesian hierarchical modelling is a statistical model written in multiple levels (hierarchical form) that estimates the parameters of the posterior distribution using the Bayesian method. The sub-models combine to form the hierarchical model, and Bayes' theorem is used to integrate them with the observed data and account for all the uncertainty that is present. The result of this integration is the posterior distribution, also known as the updated probability estimate, as additional evidence on the prior distribution is acquired.",
            "score": 75.04494953155518
        },
        {
            "docid": "44674478_19",
            "document": "Rigid motion segmentation . Motion segmentation is a field under research as there are many issues which provide scope of improvement. One of the major problems is of feature detection and finding correspondences.There are strong feature detection algorithms but they still give false positives which can lead to unexpected correspondences. Finding these pixel or feature correspondences is a difficult task. These mismatched feature points from the objects and the background often introduce outliers. The presence of image noise and outliers further affect the accuracy of structure from motion (SFM) estimation. Another issue is that of motion models or motion representations. It requires the motion to be modeled or estimated in the given model used in the algorithm. Most algorithms perform 2-D motion segmentation by assuming the motions in the scene can be modeled by 2-D affine motion models. Theoretically, this is valid because 2-D translational motion model can be represented by general affine motion model. However, such approximations in modeling can have negative consequences. The translational model has two parameters and the affine model has 6 parameters so we estimate four extra parameters. Moreover, there may not be enough data to estimate the affine motion model so the parameter estimation might be erroneous. Some of the other problems faced are:",
            "score": 70.02448344230652
        },
        {
            "docid": "140806_4",
            "document": "Maximum likelihood estimation . From the point of view of Bayesian inference, MLE is a special case of maximum a posteriori estimation (MAP) that assumes a uniform prior distribution of the parameters. In frequentist inference, MLE is one of several methods to get estimates of parameters without using prior distributions. Priors are avoided by not making probability statements about the parameters, but only about their estimates, whose properties are fully defined by the observations and the statistical model. The method of maximum likelihood is based on the likelihood function, formula_1. We are given a statistical model, i.e. a family of distributions formula_2, where formula_3 denotes the (possibly multi-dimensional) parameter for the model. The method of maximum likelihood finds the values of the model parameter, formula_3, that maximize the likelihood function, formula_1. Intuitively, this selects the parameter values that make the data most probable.",
            "score": 100.16107654571533
        },
        {
            "docid": "35639275_4",
            "document": "Multilevel modeling for repeated measures . Multilevel modeling with repeated measures employs the same statistical techniques as MLM with clustered data. In multilevel modeling for repeated measures data, the measurement occasions are nested within cases (e.g. individual or subject). Thus, level-1 units consist of the repeated measures for each subject, and the level-2 unit is the individual or subject. In addition to estimating overall parameter estimates, MLM allows regression equations at the level of the individual. Thus, as a growth curve modeling technique, it allows the estimation of inter-individual differences in intra-individual change over time by modeling the variances and covariances. In other words, it allows the testing of individual differences in patterns of responses over time (i.e. growth curves). This characteristic of multilevel modeling makes it preferable to other repeated measures statistical techniques such as repeated measures-analysis of variance (RM-ANOVA) for certain research questions.",
            "score": 58.59882473945618
        },
        {
            "docid": "3986130_30",
            "document": "Computational phylogenetics . The selection of an appropriate model is critical for the production of good phylogenetic analyses, both because underparameterized or overly restrictive models may produce aberrant behavior when their underlying assumptions are violated, and because overly complex or overparameterized models are computationally expensive and the parameters may be overfit. The most common method of model selection is the likelihood ratio test (LRT), which produces a likelihood estimate that can be interpreted as a measure of \"goodness of fit\" between the model and the input data. However, care must be taken in using these results, since a more complex model with more parameters will always have a higher likelihood than a simplified version of the same model, which can lead to the naive selection of models that are overly complex. For this reason model selection computer programs will choose the simplest model that is not significantly worse than more complex substitution models. A significant disadvantage of the LRT is the necessity of making a series of pairwise comparisons between models; it has been shown that the order in which the models are compared has a major effect on the one that is eventually selected.",
            "score": 80.02749371528625
        },
        {
            "docid": "42734031_4",
            "document": "Bayesian hierarchical modeling . Numerous statistical applications involve multiple parameters that can be regarded as related or connected in such a way that the problem implies dependence of the joint probability model for these parameters. Individual degrees of belief, expressed in the form of probabilities, come with uncertainty. Amidst this is the change of the degrees of belief over time. As was stated by Professor Jos\u00e9 M. Bernardo and Professor Adrian F. Smith, \u201cThe actuality of the learning process consists in the evolution of individual and subjective beliefs about the reality.\u201d These subjective probabilities are more directly involved in the mind rather than the physical probabilities. Hence, it is with this need of updating beliefs that Bayesians have formulated an alternative statistical model which takes into account the prior occurrence of a particular event.",
            "score": 59.4524290561676
        },
        {
            "docid": "32104707_15",
            "document": "Cellular noise . The problem of inferring the values of parameters in stochastic models (parametric inference) for biological processes, which are typically characterised by sparse and noisy experimental data, is an active field of research, with methods including Bayesian MCMC and approximate Bayesian computation proving adaptable and robust . Regarding the two-state model, a moment-based method was described for parameters inference from mRNAs distributions.",
            "score": 103.70293283462524
        },
        {
            "docid": "250001_13",
            "document": "Molecular clock . Molecular clock users have developed workaround solutions using a number of statistical approaches including maximum likelihood techniques and later Bayesian modeling. In particular, models that take into account rate variation across lineages have been proposed in order to obtain better estimates of divergence times. These models are called relaxed molecular clocks because they represent an intermediate position between the 'strict' molecular clock hypothesis and Joseph Felsenstein's many-rates model and are made possible through MCMC techniques that explore a weighted range of tree topologies and simultaneously estimate parameters of the chosen substitution model. It must be remembered that divergence dates inferred using a molecular clock are based on statistical inference and not on direct evidence.",
            "score": 74.8484754562378
        },
        {
            "docid": "62329_37",
            "document": "Meta-analysis . Specifying a Bayesian network meta-analysis model involves writing a directed acyclic graph (DAG) model for general-purpose Markov chain Monte Carlo (MCMC) software such as WinBUGS. In addition, prior distributions have to be specified for a number of the parameters, and the data have to be supplied in a specific format. Together, the DAG, priors, and data form a Bayesian hierarchical model. To complicate matters further, because of the nature of MCMC estimation, overdispersed starting values have to be chosen for a number of independent chains so that convergence can be assessed. Currently, there is no software that automatically generates such models, although there are some tools to aid in the process. The complexity of the Bayesian approach has limited usage of this methodology. Methodology for automation of this method has been suggested but requires that arm-level outcome data are available, and this is usually unavailable. Great claims are sometimes made for the inherent ability of the Bayesian framework to handle network meta-analysis and its greater flexibility. However, this choice of implementation of framework for inference, Bayesian or frequentist, may be less important than other choices regarding the modeling of effects (see discussion on models above).",
            "score": 74.57473921775818
        },
        {
            "docid": "11864519_7",
            "document": "Approximate Bayesian computation . Although Diggle and Gratton\u2019s approach had opened a new frontier, their method was not yet exactly identical to what is now known as ABC, as it aimed at approximating the likelihood rather than the posterior distribution. An article of Simon Tavar\u00e9 \"et al.\" was first to propose an ABC algorithm for posterior inference. In their seminal work, inference about the genealogy of DNA sequence data was considered, and in particular the problem of deciding the posterior distribution of the time to the most recent common ancestor of the sampled individuals. Such inference is analytically intractable for many demographic models, but the authors presented ways of simulating coalescent trees under the putative models. A sample from the posterior of model parameters was obtained by accepting/rejecting proposals based on comparing the number of segregating sites in the synthetic and real data. This work was followed by an applied study on modeling the variation in human Y chromosome by Jonathan K. Pritchard \"et al.\" using the ABC method. Finally, the term approximate Bayesian computation was established by Mark Beaumont \"et al.\", extending further the ABC methodology and discussing the suitability of the ABC-approach more specifically for problems in population genetics. Since then, ABC has spread to applications outside population genetics, such as systems biology, epidemiology, and phylogeography.",
            "score": 97.40860152244568
        },
        {
            "docid": "16959378_3",
            "document": "Invariant estimator . In statistical inference, there are several approaches to estimation theory that can be used to decide immediately what estimators should be used according to those approaches. For example, ideas from Bayesian inference would lead directly to Bayesian estimators. Similarly, the theory of classical statistical inference can sometimes lead to strong conclusions about what estimator should be used. However, the usefulness of these theories depends on having a fully prescribed statistical model and may also depend on having a relevant loss function to determine the estimator. Thus a Bayesian analysis might be undertaken, leading to a posterior distribution for relevant parameters, but the use of a specific utility or loss function may be unclear. Ideas of invariance can then be applied to the task of summarising the posterior distribution. In other cases, statistical analyses are undertaken without a fully defined statistical model or the classical theory of statistical inference cannot be readily applied because the family of models being considered are not amenable to such treatment. In addition to these cases where general theory does not prescribe an estimator, the concept of invariance of an estimator can be applied when seeking estimators of alternative forms, either for the sake of simplicity of application of the estimator or so that the estimator is robust.",
            "score": 73.25211238861084
        },
        {
            "docid": "14275245_3",
            "document": "Jeff Gill . He was a Professor of Political Science at Washington University in St. Louis and the Director of the Center for Applied Statistics. He was also President of the Society for Political Methodology, and is an inaugural fellow of the Society for Political Methodology. Major areas of research and interest include: Political Methodology, American Politics, Statistical Computing, Research Methods, and Public Administration. Current research is focused on projects on work in the development of Bayesian hierarchical models, nonparametric Bayesian models, elicited prior development from expert interviews, as well in fundamental issues in statistical inference. He has extensive expertise in statistical computing, Markov chain Monte Carlo (MCMC) tools in particular. Most sophisticated Bayesian models for the social or medical sciences require complex, compute-intensive tools such as MCMC to efficiently estimate parameters of interest. Gill is an expert in these statistical and computational techniques and uses them to contribute to empirical knowledge in the biomedical and social sciences. Current theoretical work builds logically on Gill's prior applied work and adds opportunities to develop new hybrid algorithms for statistical estimation with multilevel specifications and complex time-series and spatial relationships.",
            "score": 101.43977582454681
        },
        {
            "docid": "48403381_23",
            "document": "Dragon King Theory . Given a model and data, one can obtain a statistical model estimate. This model estimate can then be used to compute interesting quantities such as the conditional probability of the occurrence of a dragon king event in a future time interval, and the most probable occurrence time. When doing statistical modeling of extremes, and using complex or nonlinear dynamic models, there is bound to be substantial uncertainty. Thus, one should be diligent in uncertainty quantification: not only considering the randomness present in the fitted stochastic model, but also the uncertainty of its estimated parameters (e.g., with Bayesian techniques or by first simulating parameters and then simulating from the model with those parameters), and the uncertainty in model selection (e.g., by considering an ensemble of different models).",
            "score": 80.27451610565186
        },
        {
            "docid": "33246145_26",
            "document": "Neural decoding . While it is possible to take the firing rates of these modeled neurons, and transform them into the probabilistic and mathematical frameworks described above, agent-based models provide the ability to observe the behavior of the entire population of modeled neurons. Researchers can circumvent the limitations implicit with lab-based recording techniques. Because this approach does rely on modeling biological systems, error arises in the assumptions made by the researcher and in the data used in parameter estimation.",
            "score": 69.19675970077515
        },
        {
            "docid": "11864519_48",
            "document": "Approximate Bayesian computation . Interestingly, fundamentally novel approaches for model choice that incorporate quality control as an integral step in the process have recently been proposed. ABC allows, by construction, estimation of the discrepancies between the observed data and the model predictions, with respect to a comprehensive set of statistics. These statistics are not necessarily the same as those used in the acceptance criterion. The resulting discrepancy distributions have been used for selecting models that are in agreement with many aspects of the data simultaneously, and model inconsistency is detected from conflicting and co-dependent summaries. Another quality-control-based method for model selection employs ABC to approximate the effective number of model parameters and the deviance of the posterior predictive distributions of summaries and parameters. The deviance information criterion is then used as measure of model fit. It has also been shown that the models preferred based on this criterion can conflict with those supported by Bayes factors. For this reason, it is useful to combine different methods for model selection to obtain correct conclusions.",
            "score": 81.75057601928711
        },
        {
            "docid": "3190431_29",
            "document": "Spatial analysis . Spatial regression methods capture spatial dependency in regression analysis, avoiding statistical problems such as unstable parameters and unreliable significance tests, as well as providing information on spatial relationships among the variables involved. The estimated spatial relationships can be used on spatial and spatio-temporal predictions. Depending on the specific technique, spatial dependency can enter the regression model as relationships between the independent variables and the dependent, between the dependent variables and a spatial lag of itself, or in the error terms. Geographically weighted regression (GWR) is a local version of spatial regression that generates parameters disaggregated by the spatial units of analysis. This allows assessment of the spatial heterogeneity in the estimated relationships between the independent and dependent variables. The use of Bayesian hierarchical modeling in conjunction with Markov Chain Monte Carlo (MCMC) methods have recently shown to be effective in modeling complex relationships using Poisson-Gamma-CAR, Poisson-lognormal-SAR, or Overdispersed logit models.  Spatial stochastic processes, such as Gaussian processes are also increasingly being deployed in spatial regression analysis. Model-based versions of GWR, known as spatially varying coefficient models have been applied to conduct Bayesian inference. Spatial stochastic process can become computationally effective and scalable Gaussian process models, such as Gaussian Predictive Processes and Nearest Neighbor Gaussian Processes (NNGP).",
            "score": 75.28715944290161
        },
        {
            "docid": "15845763_39",
            "document": "Conway\u2013Maxwell\u2013Poisson distribution . A classical GLM formulation for a CMP regression has been developed which generalizes Poisson regression and logistic regression. This takes advantage of the exponential family properties of the CMP distribution to obtain elegant model estimation (via maximum likelihood), inference, diagnostics, and interpretation. This approach requires substantially less computational time than the Bayesian approach, at the cost of not allowing expert knowledge to be incorporated into the model. In addition it yields standard errors for the regression parameters (via the Fisher Information matrix) compared to the full posterior distributions obtainable via the Bayesian formulation. It also provides a statistical test for the level of dispersion compared to a Poisson model. Code for fitting a CMP regression, testing for dispersion, and evaluating fit is available.",
            "score": 75.03885841369629
        },
        {
            "docid": "11864519_63",
            "document": "Approximate Bayesian computation . ABC can be used to infer problems in high-dimensional parameter spaces, although one should account for the possibility of overfitting (e.g., see the model selection methods in and ). However, the probability of accepting the simulated values for the parameters under a given tolerance with the ABC rejection algorithm typically decreases exponentially with increasing dimensionality of the parameter space (due to the global acceptance criterion). Although no computational method (based on ABC or not) seems to be able to break the curse-of-dimensionality, methods have recently been developed to handle high-dimensional parameter spaces under certain assumptions (e.g., based on polynomial approximation on sparse grids, which could potentially heavily reduce the simulation times for ABC). However, the applicability of such methods is problem dependent, and the difficulty of exploring parameter spaces should in general not be underestimated. For example, the introduction of deterministic global parameter estimation led to reports that the global optima obtained in several previous studies of low-dimensional problems were incorrect. For certain problems, it might therefore be difficult to know whether the model is incorrect or, as discussed above, whether the explored region of the parameter space is inappropriate. A more pragmatic approach is to cut the scope of the problem through model reduction.",
            "score": 95.75764918327332
        },
        {
            "docid": "2889768_11",
            "document": "Image stitching . To estimate a robust model from the data, a common method used is known as RANSAC.<br> The name RANSAC is an abbreviation for \"RANdom SAmple Consensus\". It is an iterative method for robust parameter estimation to fit mathematical models from sets of observed data points which may contain outliers. The algorithm is non-deterministic in the sense that it produces a reasonable result only with a certain probability, with this probability increasing as more iterations are performed. It being a probabilistic method means that different results will be obtained for every time the algorithm is run.  The RANSAC algorithm has found many applications in computer vision, including the simultaneous solving of the correspondence problem and the estimation of the fundamental matrix related to a pair of stereo cameras. The basic assumption of the method is that the data consists of \"inliers\", i.e., data whose distribution can be explained by some mathematical model, and \"outliers\" which are data that do not fit the model. Outliers are considered points which come from noise, erroneous measurements, or simply incorrect data. For the problem of homography estimation, RANSAC works by trying to fit several models using some of the point pairs and then checking if the models were able to relate most of the points. The best model, i.e., the homography which produces the highest number of correct matches, is then chosen as the answer for the problem thus if the ratio of number of outliers to data points is very low the RANSAC outputs a decent model fitting the data.",
            "score": 73.0875164270401
        },
        {
            "docid": "48849871_3",
            "document": "Ghost population . In 2004, it was proposed that Maximum likelihood or Bayesian approaches that estimate the migration rates and population sizes using coalescent theory can use datasets which contain a population that has no data. This is referred to as a \"ghost population\". The manipulation allows exploration in the effects of missing populations on the estimation of population sizes and migration rates between two specific populations. The biases of the inferred population parameters depend on the magnitude of the migration rate from the unknown populations. The technique for deriving ghost populations attracted criticism because ghost populations were the result of statistical models, along with their limitations.",
            "score": 78.89529776573181
        },
        {
            "docid": "43387431_7",
            "document": "Inverted Dirichlet distribution . T. Bdiri et al. have developed several models that use the inverted Dirichlet distribution to represent and model non-Gaussian data. They have introduced finite and infinite mixture models of inverted Dirichlet distributions using the Newton\u2013Raphson technique to estimate the parameters and the Dirichlet process to model infinite mixtures.  T. Bdiri et al. have also used the inverted Dirichlet distribution to propose an approach to generate Support Vector Machine kernels basing on Bayesian inference and another approach to establish hierarchical clustering.",
            "score": 66.11396837234497
        },
        {
            "docid": "484872_3",
            "document": "Linear least squares (mathematics) . Mathematically, linear least squares is the problem of approximately solving an overdetermined system of linear equations, where the best approximation is defined as that which minimizes the sum of squared differences between the data values and their corresponding modeled values. The approach is called \"linear\" least squares since the assumed function is linear in the parameters to be estimated. Linear least squares problems are convex and have a closed-form solution that is unique, provided that the number of data points used for fitting equals or exceeds the number of unknown parameters, except in special degenerate situations. In contrast, non-linear least squares problems generally must be solved by an iterative procedure, and the problems can be non-convex with multiple optima for the objective function. If prior distributions are available, then even an underdetermined system can be solved using the Bayesian MMSE estimator. In statistics, linear least squares problems correspond to a particularly important type of statistical model called linear regression which arises as a particular form of regression analysis. One basic form of such a model is an ordinary least squares model. The present article concentrates on the mathematical aspects of linear least squares problems, with discussion of the formulation and interpretation of statistical regression models and statistical inferences related to these being dealt with in the articles just mentioned. See outline of regression analysis for an outline of the topic.",
            "score": 63.60470509529114
        },
        {
            "docid": "1117833_23",
            "document": "Dirichlet distribution . In a model where a Dirichlet prior distribution is placed over a set of categorical-valued observations, the marginal joint distribution of the observations (i.e. the joint distribution of the observations, with the prior parameter marginalized out) is a Dirichlet-multinomial distribution. This distribution plays an important role in hierarchical Bayesian models, because when doing inference over such models using methods such as Gibbs sampling or variational Bayes, Dirichlet prior distributions are often marginalized out. See the article on this distribution for more details.",
            "score": 82.9163465499878
        },
        {
            "docid": "20484367_6",
            "document": "Bayesian econometrics . The ideas underlying Bayesian statistics were developed by Rev. Thomas Bayes during the 18th century and later expanded by Pierre-Simon Laplace. As early as 1950, the potential of the Bayesian inference in econometrics was recognized by Jacob Marschak. The Bayesian approach was first applied to econometrics in the early 1960s by W. D. Fisher, Jacques Dr\u00e8ze, Clifford Hildreth, Thomas J. Rothenberg, George Tiao, and Arnold Zellner. The central motivation behind these early endeavors in Bayesian econometrics was the combination of the parameter estimators with available uncertain information on the model parameters that was not included in a given model formulation. From the mid-1960s to the mid-1970s, the reformulation of econometric techniques along Bayesian principles under the traditional structural approach dominated the research agenda, with Zellner's \"An Introduction to Bayesian Inference in Econometrics\" in 1971 as one of its highlights, and thus closely followed the work of frequentist econometrics. Therein, the main technical issues were the difficulty of specifying prior densities without losing either economic interpretation or mathematical tractability and the difficulty of integral calculation in the context of density functions. The result of the Bayesian reformulation program was to highlight the fragility of structural models to uncertain specification. This fragility came to motivate the work of Edward Leamer, who emphatically criticized modelers' tendency to indulge in \"post-data model construction\" and consequently developed a method of economic modelling based on the selection of regression models according to the types of prior density specification in order to identify the prior structures underlying modelers' working rules in model selection explicitly. Bayesian econometrics also became attractive to Christopher Sims' attempt to move from structural modeling to VAR modeling due to its explicit probability specification of parameter restrictions. Driven by the rapid growth of computing capacities from the mid-1980s on, the application of Markov chain Monte Carlo simulation to statistical and econometric models, first performed in the early 1990s, enabled Bayesian analysis to drastically increase its influence in economics and econometrics.",
            "score": 82.31012308597565
        },
        {
            "docid": "29668256_4",
            "document": "Separation (statistics) . This observed form of the data is important because it causes problems with estimated regression coefficients. Loosely, a parameter in the model \"wants\" to be infinite, if complete separation is observed. If quasi-complete separation is the case, the likelihood is maximized at a very large but not infinite value for that parameter. Computer programs will often output an arbitrarily large parameter estimate with a very large standard error. Methods to fit these models include exact logistic regression and Firth logistic regression, a bias-reduction method based on a penalized likelihood.",
            "score": 81.76924705505371
        },
        {
            "docid": "11864519_46",
            "document": "Approximate Bayesian computation . A number of heuristic approaches to the quality control of ABC have been proposed, such as the quantification of the fraction of parameter variance explained by the summary statistics. A common class of methods aims at assessing whether or not the inference yields valid results, regardless of the actually observed data. For instance, given a set of parameter values, which are typically drawn from the prior or the posterior distributions for a model, one can generate a large number of artificial datasets. In this way, the quality and robustness of ABC inference can be assessed in a controlled setting, by gauging how well the chosen ABC inference method recovers the true parameter values, and also models if multiple structurally different models are considered simultaneously.",
            "score": 105.4113097190857
        },
        {
            "docid": "1434444_61",
            "document": "Autoregressive model . The question of how to interpret the measured forecasting accuracy arises\u2014for example, what is a \"high\" (bad) or a \"low\" (good) value for the mean squared prediction error? There are two possible points of comparison. First, the forecasting accuracy of an alternative model, estimated under different modeling assumptions or different estimation techniques, can be used for comparison purposes. Second, the out-of-sample accuracy measure can be compared to the same measure computed for the in-sample data points (that were used for parameter estimation) for which enough prior data values are available (that is, dropping the first \"p\" data points, for which \"p\" prior data points are not available). Since the model was estimated specifically to fit the in-sample points as well as possible, it will usually be the case that the out-of-sample predictive performance will be poorer than the in-sample predictive performance. But if the predictive quality deteriorates out-of-sample by \"not very much\" (which is not precisely definable), then the forecaster may be satisfied with the performance.",
            "score": 59.894187450408936
        }
    ],
    "r": [
        {
            "docid": "605477_14",
            "document": "Behavioral neuroscience . Computational models - Using a computer to formulate real-world problems to develop solutions. Although this method is often focused in computer science, it has begun to move towards other areas of study.For example, psychology is one of these areas. Computational models allow researchers in psychology to enhance their understanding of the functions and developments in nervous systems. Examples of methods include the modelling of neurons, networks and brain systems and theoretical analysis. Computational methods have a wide variety of roles including clarifying experiments, hypothesis testing and generating new insights. These techniques play an increasing role in the advancement of biological psychology.",
            "score": 105.78752899169922
        },
        {
            "docid": "2945624_16",
            "document": "Psychonomic Society . The Psychonomic Society publishes seven journals covering the full range of experimental psychology: The journal \"Psychonomic Bulletin & Review\" provides coverage spanning a broad spectrum of topics in all areas of experimental psychology, intended for a general readership. The journal is primarily dedicated to the publication of theory and review articles and brief reports of outstanding experimental work. Areas of coverage include attention and perception, cognitive psychology, psycholinguistics, behavioral and cognitive neuroscience, memory, comparative psychology, social cognition, and cognitive development. \"Memory & Cognition\" covers human memory and learning, conceptual processes, psycholinguistics, problem solving, thinking, decision making, and skilled performance, including relevant work in the areas of computer simulation, information processing, mathematical psychology, developmental psychology, and experimental social psychology. \"Attention, Perception, & Psychophysics\" spans all areas of research in sensory processes, perception, attention, and psychophysics. Most articles published are reports of experimental work; the journal also presents theoretical, integrative, and evaluative reviews. Commentary on issues of importance to researchers appears in a special section of the journal. \"Cognitive, Affective, & Behavioral Neuroscience\" (CABN) offers theoretical, review, and primary research articles on behavior and brain processes in humans. Coverage includes normal function as well as patients with injuries or processes that influence brain function: neurological disorders, including both healthy and disordered aging; and psychiatric disorders such as schizophrenia and depression. CABN is the leading vehicle for strongly psychologically motivated studies of brain\u2013behavior relationships, through the presentation of papers that integrate psychological theory and the conduct and interpretation of the neuroscientific data. The range of topics includes perception, attention, memory, language, problem solving, reasoning, and decision-making; emotional processes, motivation, reward prediction, and affective states; and individual differences in relevant domains, including personality. \"Learning & Behavior\" presents experimental and theoretical contributions and critical reviews concerning fundamental processes of learning and behavior in nonhuman and human animals. Topics covered include sensation, perception, conditioning, learning, attention, memory, motivation, emotion, development, social behavior, and comparative investigations. \"Behavior Research Methods\" publishes articles concerned with the methods, techniques, and instrumentation of research in experimental psychology. The journal focuses particularly on the use of computer technology in psychological research. An annual special issue is devoted to this field. \"Cognitive Research: Principles & Implications\" publishes new empirical and theoretical work covering all areas of Cognition, with a special emphasis on use-inspired basic research: fundamental research that grows from hypotheses about real-world problems.",
            "score": 105.56892395019531
        },
        {
            "docid": "11864519_46",
            "document": "Approximate Bayesian computation . A number of heuristic approaches to the quality control of ABC have been proposed, such as the quantification of the fraction of parameter variance explained by the summary statistics. A common class of methods aims at assessing whether or not the inference yields valid results, regardless of the actually observed data. For instance, given a set of parameter values, which are typically drawn from the prior or the posterior distributions for a model, one can generate a large number of artificial datasets. In this way, the quality and robustness of ABC inference can be assessed in a controlled setting, by gauging how well the chosen ABC inference method recovers the true parameter values, and also models if multiple structurally different models are considered simultaneously.",
            "score": 105.41130828857422
        },
        {
            "docid": "32104707_15",
            "document": "Cellular noise . The problem of inferring the values of parameters in stochastic models (parametric inference) for biological processes, which are typically characterised by sparse and noisy experimental data, is an active field of research, with methods including Bayesian MCMC and approximate Bayesian computation proving adaptable and robust . Regarding the two-state model, a moment-based method was described for parameters inference from mRNAs distributions.",
            "score": 103.70292663574219
        },
        {
            "docid": "14275245_3",
            "document": "Jeff Gill . He was a Professor of Political Science at Washington University in St. Louis and the Director of the Center for Applied Statistics. He was also President of the Society for Political Methodology, and is an inaugural fellow of the Society for Political Methodology. Major areas of research and interest include: Political Methodology, American Politics, Statistical Computing, Research Methods, and Public Administration. Current research is focused on projects on work in the development of Bayesian hierarchical models, nonparametric Bayesian models, elicited prior development from expert interviews, as well in fundamental issues in statistical inference. He has extensive expertise in statistical computing, Markov chain Monte Carlo (MCMC) tools in particular. Most sophisticated Bayesian models for the social or medical sciences require complex, compute-intensive tools such as MCMC to efficiently estimate parameters of interest. Gill is an expert in these statistical and computational techniques and uses them to contribute to empirical knowledge in the biomedical and social sciences. Current theoretical work builds logically on Gill's prior applied work and adds opportunities to develop new hybrid algorithms for statistical estimation with multilevel specifications and complex time-series and spatial relationships.",
            "score": 101.43977355957031
        },
        {
            "docid": "140806_4",
            "document": "Maximum likelihood estimation . From the point of view of Bayesian inference, MLE is a special case of maximum a posteriori estimation (MAP) that assumes a uniform prior distribution of the parameters. In frequentist inference, MLE is one of several methods to get estimates of parameters without using prior distributions. Priors are avoided by not making probability statements about the parameters, but only about their estimates, whose properties are fully defined by the observations and the statistical model. The method of maximum likelihood is based on the likelihood function, formula_1. We are given a statistical model, i.e. a family of distributions formula_2, where formula_3 denotes the (possibly multi-dimensional) parameter for the model. The method of maximum likelihood finds the values of the model parameter, formula_3, that maximize the likelihood function, formula_1. Intuitively, this selects the parameter values that make the data most probable.",
            "score": 100.16107940673828
        },
        {
            "docid": "2208074_18",
            "document": "Neurophilosophy . Many of the philosophical controversies surrounding computational neuroscience involve the role of simulation and modeling as explanation. Carl Craver has been especially vocal about such interpretations. Jones and Love wrote an especially critical article targeted at Bayesian behavioral modeling that did not constrain the modeling parameters by psychological or neurological considerations Eric Winsberg has written about the role of computer modeling and simulation in science generally, but his characterization is applicable to computational neuroscience.",
            "score": 98.88223266601562
        },
        {
            "docid": "11864519_7",
            "document": "Approximate Bayesian computation . Although Diggle and Gratton\u2019s approach had opened a new frontier, their method was not yet exactly identical to what is now known as ABC, as it aimed at approximating the likelihood rather than the posterior distribution. An article of Simon Tavar\u00e9 \"et al.\" was first to propose an ABC algorithm for posterior inference. In their seminal work, inference about the genealogy of DNA sequence data was considered, and in particular the problem of deciding the posterior distribution of the time to the most recent common ancestor of the sampled individuals. Such inference is analytically intractable for many demographic models, but the authors presented ways of simulating coalescent trees under the putative models. A sample from the posterior of model parameters was obtained by accepting/rejecting proposals based on comparing the number of segregating sites in the synthetic and real data. This work was followed by an applied study on modeling the variation in human Y chromosome by Jonathan K. Pritchard \"et al.\" using the ABC method. Finally, the term approximate Bayesian computation was established by Mark Beaumont \"et al.\", extending further the ABC methodology and discussing the suitability of the ABC-approach more specifically for problems in population genetics. Since then, ABC has spread to applications outside population genetics, such as systems biology, epidemiology, and phylogeography.",
            "score": 97.40859985351562
        },
        {
            "docid": "38889813_34",
            "document": "Viral phylodynamics . In general, in needing to run simulations rather than compute likelihoods, it may be difficult to make fine-scale inferences on epidemiological parameters, and instead, this work usually focuses on broader questions, testing whether overall genealogical patterns are consistent with one epidemiological model or another. Additionally, simulation-based methods are often used to validate inference results, providing test data where the correct answer is known ahead of time. Because computing likelihoods for genealogical data under complex simulation models has proven difficult, an alternative statistical approach called Approximate Bayesian Computation (ABC) is becoming popular in fitting these simulation models to patterns of genetic variation, following successful application of this approach to bacterial diseases. This is because ABC makes use of easily computable summary statistics to approximate likelihoods, rather than the likelihoods themselves.",
            "score": 96.04776763916016
        },
        {
            "docid": "11864519_63",
            "document": "Approximate Bayesian computation . ABC can be used to infer problems in high-dimensional parameter spaces, although one should account for the possibility of overfitting (e.g., see the model selection methods in and ). However, the probability of accepting the simulated values for the parameters under a given tolerance with the ABC rejection algorithm typically decreases exponentially with increasing dimensionality of the parameter space (due to the global acceptance criterion). Although no computational method (based on ABC or not) seems to be able to break the curse-of-dimensionality, methods have recently been developed to handle high-dimensional parameter spaces under certain assumptions (e.g., based on polynomial approximation on sparse grids, which could potentially heavily reduce the simulation times for ABC). However, the applicability of such methods is problem dependent, and the difficulty of exploring parameter spaces should in general not be underestimated. For example, the introduction of deterministic global parameter estimation led to reports that the global optima obtained in several previous studies of low-dimensional problems were incorrect. For certain problems, it might therefore be difficult to know whether the model is incorrect or, as discussed above, whether the explored region of the parameter space is inappropriate. A more pragmatic approach is to cut the scope of the problem through model reduction.",
            "score": 95.75765228271484
        },
        {
            "docid": "30525054_7",
            "document": "Anders Dale . In a 2003 interview, Dale explained that he had \u201calways been interested in using quantitative modeling methods and simulations to answer biological questions,\u201d and that as a Harvard student he had been \u201cinterested in approaching connectionist neural networks from a more biological angle.\u201d When he went to UCSD to continue his graduate work his interest \u201cshifted to learning how to test models of how the brain works. Ideally you'd like to test your models not in anesthetized animals and brain slices, but by measuring brain activity in humans non-invasively. I wanted to study normal people doing normal tasks. That was what brought me to imaging. My goal was to see what kind of things we can measure non-invasively that can be quantitatively related to the models we want to build...I wanted to know what exactly we are measuring, how can you model it, and how can you relate the signal to what is going on in the brain physiologically...at a level that say you could measure invasively and that you could relate to parameters of quantitative models.\u201d His thesis work at UCSD, he said, \u201cwas on the EEG and MEG forward and inverse problems, and how to use anatomical information to constrain the solutions. It is clear that if you only use EEG or MEG measures, the spatial precision is not good enough to make inferences at a scale that's most useful to neuroscience. That led us into trying to use information with higher spatial resolution to constrain or bias our estimations of the signal sources in the brain.\u201d",
            "score": 94.56040954589844
        },
        {
            "docid": "42734031_3",
            "document": "Bayesian hierarchical modeling . Frequentist statistics, the more popular foundation of statistics, may yield conclusions seemingly incompatible with those offered by Bayesian statistics due to the Bayesian treatment of the parameters as random variables and its use of subjective information in establishing assumptions on these parameters.. As the approaches answer different questions the formal results aren't technically contradictory but the two approaches disagree over which answer is relevant to particular applications. Bayesians argue that relevant information regarding decision making and updating beliefs cannot be ignored and that hierarchical modeling has the potential to overrule classical methods in applications where respondents give multiple observational data. Moreover, the model has proven to be robust, with the posterior distribution less sensitive to the more flexible hierarchical priors. Hierarchical modeling is used when information is available on several different levels of observational units. The hierarchical form of analysis and organization helps in the understanding of multiparameter problems and also plays an important role in developing computational strategies.",
            "score": 94.46232604980469
        },
        {
            "docid": "57448874_2",
            "document": "Bayesian model reduction . Bayesian model reduction is a method for computing the evidence and posterior over the parameters of Bayesian models that differ in their priors. A full model is fitted to data using standard approaches. Hypotheses are then tested by defining one or more 'reduced' models with alternative (and usually more restrictive) priors, which usually \u2013 in the limit \u2013 switch off certain parameters. The evidence and parameters of the reduced models can then be computed from the evidence and estimated (posterior) parameters of the full model using Bayesian model reduction. If the priors and posteriors are normally distributed, then there is an analytic solution which can be computed rapidly. This has multiple scientific and engineering applications: these include scoring the evidence for large numbers of models very quickly and facilitating the estimation of hierarchical models (Parametric Empirical Bayes).",
            "score": 94.42982482910156
        },
        {
            "docid": "41578765_18",
            "document": "Paul Glimcher . Glimcher\u2019s research aims to describe the neural events that underlie behavioral decision-making using tools from neuroscience, psychology, and economics. His research merges psychological and economic models with computational neuroscience, including pioneering uses of fMRI (function magnetic resonance imaging) for behavioral science, to understand how value is encoded in the brain and how the brain uses those neural representations of value to guide decision-making; for example, how the brain carries out delay discounting or action-selection in the face of both risk and ambiguity. His laboratory in NYU\u2019s Center for Neural Science uses a wide range of methods including cohort studies in experimental economics, brain imaging, and single-neuron studies in non-human animals.",
            "score": 94.36012268066406
        },
        {
            "docid": "248799_5",
            "document": "Philosophy of psychology . Philosophy of psychology also closely monitors contemporary work conducted in cognitive neuroscience, evolutionary psychology, and artificial intelligence, for example questioning whether psychological phenomena can be explained using the methods of neuroscience, evolutionary theory, and computational modeling, respectively. Although these are all closely related fields, some concerns still arise about the appropriateness of importing their methods into psychology. Some such concerns are whether psychology, as the study of individuals as information processing systems (see Donald Broadbent), is autonomous from what happens in the brain (even if psychologists largely agree that the brain in some sense causes behavior (see supervenience)); whether the mind is \"hard-wired\" enough for evolutionary investigations to be fruitful; and whether computational models can do anything more than offer possible implementations of cognitive theories that tell us nothing about the mind (Fodor & Pylyshyn 1988).",
            "score": 93.89027404785156
        },
        {
            "docid": "11864519_5",
            "document": "Approximate Bayesian computation . The first ABC-related ideas date back to the 1980s. Donald Rubin, when discussing the interpretation of Bayesian statements in 1984, described a hypothetical sampling mechanism that yields a sample from the posterior distribution. This scheme was more of a conceptual thought experiment to demonstrate what type of manipulations are done when inferring the posterior distributions of parameters. The description of the sampling mechanism coincides exactly with that of the ABC-rejection scheme, and this article can be considered to be the first to describe approximate Bayesian computation. However, a two-stage quincunx was constructed by Francis Galton in the late 1800s that can be seen as a physical implementation of an ABC-rejection scheme for a single unknown (parameter) and a single observation. Another prescient point was made by Rubin when he argued that in Bayesian inference, applied statisticians should not settle for analytically tractable models only, but instead consider computational methods that allow them to estimate the posterior distribution of interest. This way, a wider range of models can be considered. These arguments are particularly relevant in the context of ABC.",
            "score": 93.04830169677734
        },
        {
            "docid": "43104837_2",
            "document": "Indirect Inference . Indirect inference is a simulation-based method for estimating the parameters of economic models. It is a computational method for determining acceptable macroeconomic model parameters in circumstances where the available data is too voluminous or unsuitable for formal modeling.",
            "score": 92.94477081298828
        },
        {
            "docid": "22921_96",
            "document": "Psychology . Computational modeling is a tool used in mathematical psychology and cognitive psychology to simulate behavior. This method has several advantages. Since modern computers process information quickly, simulations can be run in a short time, allowing for high statistical power. Modeling also allows psychologists to visualize hypotheses about the functional organization of mental events that couldn't be directly observed in a human. Computational neuroscience uses mathematical models to simulate the brain. Another method is symbolic modeling, which represents many mental objects using variables and rules. Other types of modeling include dynamic systems and stochastic modeling.",
            "score": 92.79353332519531
        },
        {
            "docid": "6937286_5",
            "document": "Zoubin Ghahramani . Ghahramani has made significant contributions in the areas of Bayesian machine learning (particularly variational methods for approximate Bayesian inference), as well as graphical models and computational neuroscience. His current research focuses on nonparametric Bayesian modelling and statistical machine learning. He has also worked on artificial intelligence, information retrieval, bioinformatics and statistics which provide the mathematical foundations for handling uncertainty, making decisions, and designing learning systems. He has published over 200 papers, receiving over 30,000 citations (an h-index of 74).",
            "score": 92.06299591064453
        },
        {
            "docid": "18166009_14",
            "document": "Lower critical solution temperature . There are three groups of methods for correlating and predicting LCSTs. The first group proposes models that are based on a solid theoretical background using liquid\u2013liquid or vapor\u2013liquid experimental data. These methods require experimental data to adjust the unknown parameters, resulting in limited predictive ability . Another approach uses empirical equations that correlate \u03b8(LCST) with physicochemical properties such as density, critical properties etc., but suffers from the disadvantage that these properties are not always available. A new approach proposed by Liu and Zhong develops linear models for the prediction of \u03b8(LCST) using molecular connectivity indices, which depends only on the solvent and polymer structures. The latter approach has proven to be a very useful technique in quantitative structure\u2013activity/property relationships (QSAR/QSPR) research for polymers and polymer solutions. QSAR/QSPR studies constitute an attempt to reduce the trial-and-error element in the design of compounds with desired activity/properties by establishing mathematical relationships between the activity/property of interest and measurable or computable parameters, such as topological, physicochemical, stereochemistry, or electronic indices. More recently QSPR models for the prediction of the \u03b8 (LCST) using molecular (electronic, physicochemical etc.) descriptors have been published. Using validated robust QSPR models, experimental time and effort can be reduced significantly as reliable estimates of \u03b8(LCST) for polymer solutions can be obtained before they are actually synthesized in the laboratory.",
            "score": 91.51313018798828
        },
        {
            "docid": "11864519_3",
            "document": "Approximate Bayesian computation . ABC methods bypass the evaluation of the likelihood function. In this way, ABC methods widen the realm of models for which statistical inference can be considered. ABC methods are mathematically well-founded, but they inevitably make assumptions and approximations whose impact needs to be carefully assessed. Furthermore, the wider application domain of ABC exacerbates the challenges of parameter estimation and model selection.",
            "score": 90.6297378540039
        },
        {
            "docid": "26459934_21",
            "document": "Seismic noise . The obtained results cannot directly give information on the physical parameters (S-wave velocity, structural stiffness...) of the ground structures or civil engineering structures. Therefore, models are needed to compute these products (dispersion curve, modal shapes...) that could be compared with the experimental data. Computing a lot of models to find which agree with the data is solving the Inverse problem. The main issue of inversion is to well explore the parameter space with a limited number of computations of the model. However, the model fitting best the data is not the most interesting because parameter compensation, uncertainties on both models and data make many models with different input parameters as good compared to the data. The sensitivity of the parameters may also be very different depending on the model used. The inversion process is generally the weak point of these ambient vibration methods.",
            "score": 90.55049133300781
        },
        {
            "docid": "521509_23",
            "document": "Bar-Ilan University . The Leslie and Susan Gonda Multidisciplinary Brain Research Center (Hebrew: \u05d4\u05de\u05e8\u05db\u05d6 \u05d4\u05e8\u05d1 \u05ea\u05d7\u05d5\u05de\u05d9 \u05dc\u05d7\u05e7\u05e8 \u05d4\u05de\u05d5\u05d7 \u05e2\u2033\u05e9 \u05dc\u05e1\u05dc\u05d9 \u05d5\u05e1\u05d5\u05d6\u05df \u05d2\u05d5\u05e0\u05d3\u05d4) focuses on a multidisciplinary approach to neuroscience. It houses over 30 laboratories investigating brain complexity at multiple levels, from single neurons, through information processing and computations in neural networks to cognition, behavior and human mind. The center's core members and affiliates combine multiple fields that are crucial for brain understanding, including molecular and systems neuroscience, cognitive neuroscience, psychology, psychiatry, linguistics, mathematics, computer sciences, engineering and physics. Numerous research approaches are employed by the center\u2019s scientists, such as brain stimulation techniques, neuroimaging, electrophysiology, molecular techniques, computational methods, mathematical modeling and behavioral and cognitive paradigms. The center was founded in 2002 thanks to the contributions of the Gonda family, the president of Bar-Ilan University Moshe Kaveh, and Moshe Abeles, a pioneer of Israel\u2019s neuroscience research, Emet Prize laureate (2004) and the founding director of the Interdisciplinary Center for Neural Computation at the Hebrew University (1992\u20131999), has led the Gonda Multidisciplinary Brain Research Center for nearly a decade. Since 2011 the center is headed by Moshe Bar, a cognitive neuroscientist and an expert in brain imaging technologies. Bar returned to Israel to head the Gonda Multidisciplinary Brain Research Center as its new director after thirteen years at Harvard University.",
            "score": 90.52946472167969
        },
        {
            "docid": "3062721_2",
            "document": "Neuroinformatics . Neuroinformatics is a research field concerned with the organization of neuroscience data by the application of computational models and analytical tools. These areas of research are important for the integration and analysis of increasingly large-volume, high-dimensional, and fine-grain experimental data. Neuroinformaticians provide computational tools, mathematical models, and create interoperable databases for clinicians and research scientists. Neuroscience is a heterogeneous field, consisting of many and various sub-disciplines (e.g., cognitive psychology, behavioral neuroscience, and behavioral genetics). In order for our understanding of the brain to continue to deepen, it is necessary that these sub-disciplines are able to share data and findings in a meaningful way; Neuroinformaticians facilitate this.",
            "score": 90.21916961669922
        },
        {
            "docid": "33699847_8",
            "document": "Society for Computers in Psychology . The Society for Computers in Psychology is a non-profit organization of researchers interested in applications of computers in psychology. Its primary purpose is to \"increase and diffuse knowledge of the use of computers in psychological research.\" Over the past several years the organization has focused on many important issues in psychology, such as computational models of cognitive processes and behavior, computational tools for data collection and analysis, human-computer interaction, knowledge representation in both humans and machines, machine learning, methods and tools for Internet-based research, and technology in the service of improving and evaluating outcomes. We have also encouraged a consideration of cognitively-inspired design of computational technologies and models.",
            "score": 89.55501556396484
        },
        {
            "docid": "515094_3",
            "document": "Neuroeconomics . It combines research from neuroscience, experimental and behavioral economics, and cognitive and social psychology. As research into decision-making behavior becomes increasingly computational, it has also incorporated new approaches from theoretical biology, computer science, and mathematics. Neuroeconomics studies decision making by using a combination of tools from these fields so as to avoid the shortcomings that arise from a single-perspective approach. In mainstream economics, expected utility (EU) and the concept of rational agents are still being used. Many economic behaviors are not fully explained by these models, such as heuristics and framing.",
            "score": 89.26493072509766
        },
        {
            "docid": "98770_28",
            "document": "Hidden Markov model . The parameter learning task in HMMs is to find, given an output sequence or a set of such sequences, the best set of state transition and emission probabilities. The task is usually to derive the maximum likelihood estimate of the parameters of the HMM given the set of output sequences. No tractable algorithm is known for solving this problem exactly, but a local maximum likelihood can be derived efficiently using the Baum\u2013Welch algorithm or the Baldi\u2013Chauvin algorithm. The Baum\u2013Welch algorithm is a special case of the expectation-maximization algorithm. If the HMMs are used for time series prediction, more sophisticated Bayesian inference methods, like Markov chain Monte Carlo (MCMC) sampling are proven to be favorable over finding a single maximum likelihood model both in terms of accuracy and stability. Since MCMC imposes significant computational burden, in cases where computational scalability is also of interest, one may alternatively resort to variational approximations to Bayesian inference, e.g. Indeed, approximate variational inference offers computational efficiency comparable to expectation-maximization, while yielding an accuracy profile only slightly inferior to exact MCMC-type Bayesian inference.",
            "score": 89.21733856201172
        },
        {
            "docid": "179092_2",
            "document": "Neurolinguistics . Neurolinguistics is the study of the neural mechanisms in the human brain that control the comprehension, production, and acquisition of language. As an interdisciplinary field, neurolinguistics draws methods and theories from fields such as neuroscience, linguistics, cognitive science, communication disorders and neuropsychology. Researchers are drawn to the field from a variety of backgrounds, bringing along a variety of experimental techniques as well as widely varying theoretical perspectives. Much work in neurolinguistics is informed by models in psycholinguistics and theoretical linguistics, and is focused on investigating how the brain can implement the processes that theoretical and psycholinguistics propose are necessary in producing and comprehending language. Neurolinguists study the physiological mechanisms by which the brain processes information related to language, and evaluate linguistic and psycholinguistic theories, using aphasiology, brain imaging, electrophysiology, and computer modeling.",
            "score": 88.74586486816406
        },
        {
            "docid": "11864519_2",
            "document": "Approximate Bayesian computation . Approximate Bayesian computation (ABC) constitutes a class of computational methods rooted in Bayesian statistics. In all model-based statistical inference, the likelihood function is of central importance, since it expresses the probability of the observed data under a particular statistical model, and thus quantifies the support data lend to particular values of parameters and to choices among different models. For simple models, an analytical formula for the likelihood function can typically be derived. However, for more complex models, an analytical formula might be elusive or the likelihood function might be computationally very costly to evaluate.",
            "score": 88.64824676513672
        },
        {
            "docid": "1988689_2",
            "document": "Computational cognition . Computational cognition (sometimes referred to as computational cognitive science or computational psychology) is the study of the computational basis of learning and inference by mathematical modeling, computer simulation, and behavioral experiments. In psychology, it is an approach which develops computational models based on experimental results. It seeks to understand the basis behind the human method of processing of information. Early on computational cognitive scientists sought to bring back and create a scientific form of Brentano\u2019s psychology",
            "score": 88.6440200805664
        },
        {
            "docid": "32107929_2",
            "document": "Mark A. Gluck . Mark A. Gluck is a professor of neuroscience at Rutgers\u2013Newark in New Jersey, director of the Rutgers Memory Disorders Project, and publisher of the public health newsletter, \"Memory Loss and the Brain\". He works at the interface between neuroscience, psychology, and computer science, studying the neural bases of learning and memory. His research spans numerous methodologies, including neurocomputational modeling, clinical studies of brain-damaged patients, functional and structural brain imaging, behavioral genetics, and comparative studies of rodent and human learning. He is the co-author of \"Gateway to Memory: An Introduction to Neural Network Models of the Hippocampus\" and an undergraduate textbook \"Learning and Memory: From Brain to Behavior\" (Worth Publishers, 2008).",
            "score": 87.6466064453125
        },
        {
            "docid": "50227566_16",
            "document": "Center for Operations Research and Econometrics . Econometrics research at CORE is aimed at the development of quantitative models as well as statistical and computational methods applied to treating economic data. Among the major CORE contributions in econometrics are Bayesian estimation of simultaneous equations systems (Bayesian inference methods are widely used in research at CORE) and the concepts of weak and strong exogeneity used in statistical inference. Other important research fields include financial econometrics with such topics as the microstructure of financial markets or volatility models and structural econometrics. The current research areas in econometrics are financial econometrics, time series econometrics and Bayesian methods.",
            "score": 87.14183044433594
        }
    ]
}