{
    "q": [
        {
            "docid": "18130567_17",
            "document": "Primary consciousness . The dynamic core hypothesis (DCH) proposes that consciousness arises from neural dynamics in the thalamocortical system, as measured by the quantity neural complexity (CN). \"CN\" is an information-theoretic measure; the \"CN\" value is high if each subset of a neural system can take on many different states, and if these states make a difference to the rest of the system. The information integration theory of consciousness (IITC) shares with the DCH the idea that conscious experiences provide informative discriminations among a vast repertoire of possible experiences. In the IITC, the quantity \"phi\" is defined as the information that is integrated across the informational \"weakest link\" of a system. Importantly, \"phi\" is a measure of the capacity of a neural system to integrate information, whereas \"CN\" is a measure of the actual dynamics of the system. A third measure, causal density \"(CD)\", measures the fraction of causal interactions among elements of a system that are statistically significant.",
            "score": 84.03783679008484
        },
        {
            "docid": "5664_54",
            "document": "Consciousness . Regarding the primary function of conscious processing, a recurring idea in recent theories is that phenomenal states somehow integrate neural activities and information-processing that would otherwise be independent. This has been called the \"integration consensus\". Another example has been proposed by Gerald Edelman called dynamic core hypothesis which puts emphasis on reentrant connections that reciprocally link areas of the brain in a massively parallel manner. Edelman also stresses the importance of the evolutionary emergence of higher-order consciousness in humans from the historically older trait of primary consciousness which humans share with non-human animals (see \"Neural correlates\" section above). These theories of integrative function present solutions to two classic problems associated with consciousness: differentiation and unity. They show how our conscious experience can discriminate between a virtually unlimited number of different possible scenes and details (differentiation) because it integrates those details from our sensory systems, while the integrative nature of consciousness in this view easily explains how our experience can seem unified as one whole despite all of these individual parts. However, it remains unspecified which kinds of information are integrated in a conscious manner and which kinds can be integrated without consciousness. Nor is it explained what specific causal role conscious integration plays, nor why the same functionality cannot be achieved without consciousness. Obviously not all kinds of information are capable of being disseminated consciously (e.g., neural activity related to vegetative functions, reflexes, unconscious motor programs, low-level perceptual analyses, etc.) and many kinds of information can be disseminated and combined with other kinds without consciousness, as in intersensory interactions such as the ventriloquism effect. Hence it remains unclear why any of it is conscious. For a review of the differences between conscious and unconscious integrations, see the article of E. Morsella.",
            "score": 92.06127369403839
        },
        {
            "docid": "221519_76",
            "document": "Differential form . On a \"general\" differentiable manifold (without additional structure), differential forms \"cannot\" be integrated over subsets of the manifold; this distinction is key to the distinction between differential forms, which are integrated over chains, and measures, which are integrated over subsets. The simplest example is attempting to integrate the -form over the interval . Assuming the usual distance (and thus measure) on the real line, this integral is either or , depending on \"orientation:\" formula_52, while formula_53. By contrast, the integral of the \"measure\" on the interval is unambiguously (formally, the integral of the constant function with respect to this measure is ). Similarly, under a change of coordinates a differential -form changes by the Jacobian determinant , while a measure changes by the \"absolute value\" of the Jacobian determinant, , which further reflects the issue of orientation. For example, under the map on the line, the differential form pulls back to ; orientation has reversed; while the Lebesgue measure, which here we denote , pulls back to ; it does not change.",
            "score": 40.28193140029907
        },
        {
            "docid": "19873_4",
            "document": "Measure (mathematics) . Measure theory was developed in successive stages during the late 19th and early 20th centuries by \u00c9mile Borel, Henri Lebesgue, Johann Radon, and Maurice Fr\u00e9chet, among others. The main applications of measures are in the foundations of the Lebesgue integral, in Andrey Kolmogorov's axiomatisation of probability theory and in ergodic theory. In integration theory, specifying a measure allows one to define integrals on spaces more general than subsets of Euclidean space; moreover, the integral with respect to the Lebesgue measure on Euclidean spaces is more general and has a richer theory than its predecessor, the Riemann integral. Probability theory considers measures that assign to the whole set the size 1, and considers measurable subsets to be events whose probability is given by the measure. Ergodic theory considers measures that are invariant under, or arise naturally from, a dynamical system.",
            "score": 47.19534873962402
        },
        {
            "docid": "96558_13",
            "document": "Maxwell's demon . Although the argument by Landauer and Bennett only answers the consistency between the second law of thermodynamics and the whole cyclic process of the entire system of a Szilard engine (a composite system of the engine and the demon), a recent approach based on the non-equilibrium thermodynamics for small fluctuating systems has provided deeper insight on each information process with each subsystem. From this viewpoint, the measurement process is regarded as a process where the correlation (mutual information) between the engine and the demon increases, and the feedback process is regarded as a process where the correlation decreases. If the correlation changes, thermodynamic relations as the second law of thermodynamics and the fluctuation theorem for each subsystem should be modified, and for the case of external control a second-law like inequality and a generalized fluctuation theorem with mutual information are satisfied. These relations suggest that we need extra thermodynamic cost to increase correlation (measurement case), and in contrast we can apparently violate the second law up to the consumption of correlation (feedback case). For more general information processes including biological information processing, both inequality and equality with mutual information hold.",
            "score": 48.17514705657959
        },
        {
            "docid": "18360358_7",
            "document": "Affect consciousness . As noted by Solbakken et al., affect consciousness scores (both overall mean of all aspect-scores across affects and scores on each integrating aspect, and discrete affects) are strongly correlated with relevant measures of psychological dysfunction. Affect integration (operationalized through Affect Consciousness constructs and measured with the ACI and ACS) at different levels are stable correlates of psychopathology and psychological dysfunction such as symptom severity, interpersonal problems, personality disorder traits, and general functioning. Furthermore, the integration of specific affects have been shown to have distinct and predictable relationships with various types of relational problems.",
            "score": 40.81511044502258
        },
        {
            "docid": "34864050_2",
            "document": "Cognitive-experiential self-theory . Cognitive-experiential self-theory (CEST) is a dual-process model of perception developed by Seymour Epstein. CEST is based around the idea that people operate using two separate systems for information processing: analytical-rational and intuitive-experiential. The analytical-rational system is deliberate, slow, and logical. The intuitive-experiential system is fast, automatic, and emotionally driven. These are independent systems that operate in parallel and interact to produce behavior and conscious thought. There have been other dual-process theories in the past. Shelly Chaiken's heuristic-systematic model, Carl Jung's distinction between thinking and feeling, and John Bargh's theory on automatic vs. non-automatic processing all have similar components to CEST. However, Epstein's cognitive-experiential self-theory is unique in that it places a dual-process model within the context of a global theory of personality, rather than considering it as an isolated construct or cognitive shortcut. Epstein argues that within the context of day-to-day life, a constant interaction occurs between the two systems. Because the experiential system is fast, guided by emotion and past experience, and requires little in terms of cognitive resources, it is especially equipped to handle the majority of information processing on a daily basis, all of which occurs outside of conscious awareness. This, in turn, allows us to focus the limited capacity of our rational system on whatever requires our conscious attention at the time. Individual difference in preference for analytical or experiential processing can be measured using the Rational Experiential Inventory (REI). The REI measures the two independent processing modes with two factors: need for cognition (rational measure) and faith in intuition (experiential measure). Several studies have confirmed that the REI is a reliable measure of individual difference in information processing, and that the two independent thinking styles measured account for a substantial amount of variance that is not addressed by other personality theories such as the five factor model.",
            "score": 64.02693390846252
        },
        {
            "docid": "39512857_13",
            "document": "Community food security . As mentioned above CFS is a fairly recent term with limited articulation as a theoretical framework (see Anderson & Cook, 1999); it is difficult to measure; and can vary greatly across different community\u2019s needs, wants and values. CFS is also complex in that it considers such a wide range of influential factors including individual and household food security, food environments, sustainability and the strength of the local food system. Understanding CFS requires seeing the food system as a whole, engaging a variety of actors across the system from producer to consumer/citizen, and, importantly, growing new \u201cnext practice solutions\u201d to address ever emerging challenges. CFS is dynamic and cannot be addressed piece by piece but instead must be approached holistically and as a dynamic process. CFS is socially complex as it involves participation from individuals of various different perspectives and interests, as well as on multiple scales (e.g. local, national or international). Also, the world we live in is constantly changing and what worked in the past may not work in the future.",
            "score": 31.208900332450867
        },
        {
            "docid": "17950725_40",
            "document": "Sustainability accounting . In light of these aspects, Geoff Lamberton provides a promising framework for the various forms of accounting. It draws together the five general major themes evident in social and environmental accounting research and practice, including the GRI Sustainability Accounting Guidelines. He depicts a comprehensive sustainability accounting framework which displays the complex interconnections between the various components and dimensions of sustainability. It balances the need for integration of the variety in information, measurements and reporting with the differentiated unitary information effects between the dimensions of sustainable development. The multiple units of measurement include narratives of social policy and procedures as well traditional accounting principles and practice.",
            "score": 40.46766710281372
        },
        {
            "docid": "39351731_4",
            "document": "J\u00fcrgen Kurths . From time series analysis and its application to solar and stellar activity phenomena, his interest was drawn in the 1980s to complex systems and nonlinearity or chaos theory. Kurths has been noted especially for his seminal contributions to new synchronization phenomena, recurrence, coherence resonance, measures of complexity and causality as well as dynamics and stability of complex networks. This is what later led him to do research on the basics of complex systems theory as well as applications to the Earth system, the human brain, the cardio-respiratory system and other systems which are characterised by a high degree of complexity and nonlinearity.",
            "score": 38.45080375671387
        },
        {
            "docid": "22982978_12",
            "document": "Resilient control systems . Establishing a metric that can capture the resilience attributes can be complex, at least if considered based upon differences between the interactions or interdependencies. Evaluating the control, cyber and cognitive disturbances, especially if considered from a disciplinary standpoint, leads to measures that already had been established. However, if the metric were instead based upon a normalizing dynamic attribute, such a performance characteristic that can be impacted by degradation, an alternative is suggested. Specifically, applications of base metrics to resilience characteristics are given as follows for type of disturbance: Such performance characteristics exist with both time and data integrity. Time, both in terms of delay of mission and communications latency, and data, in terms of corruption or modification, are normalizing factors. In general, the idea is to base the metric on \u201cwhat is expected\u201d and not necessarily the actual initiator to the degradation. Considering time as a metrics basis, resilient and un-resilient systems can be observed in Fig. 2.",
            "score": 41.45050287246704
        },
        {
            "docid": "25524586_5",
            "document": "Partnership for European Environmental Research . In recent climate change projects, PEER studied climate policy integration and compared adaptation strategies in European countries. The final reports of these projects deal with several aspects of implementing climate policy in Europe. The first report analyses the adaptation strategies of the EU member states, identifying a number of common strengths and weaknesses of the current strategies in the countries studied. The second report assesses the degree of climate policy integration in six different European countries, at national and local levels, as well as within key policy sectors such as energy and transport. It analyses measures and means to enhance climate policy integration and improve policy coherence. The reports show that communication and awareness raising is going to be important to get public support for adaptation measures, and to help stakeholders to adapt. Since adaptation is very different from mitigation, communication should be designed specifically for that purpose, including exchange of experiences on adaptation practices. Although the inclusion of climate change mitigation and adaptation in general governmental programmes and strategies has substantially increased in recent years, much more is needed in terms of integrating climate issues into specific policy measures. Annual budgets, environmental impact assessments and spatial planning procedures are three examples of existing measures which we believe have significant potential to be climate policy instruments.",
            "score": 34.466963052749634
        },
        {
            "docid": "25479078_5",
            "document": "Stochastic resonance (sensory neurobiology) . The idea of adding noise to a system in order to improve the quality of measurements is counter-intuitive. Measurement systems are usually constructed or evolved to reduce noise as much as possible and thereby provide the most precise measurement of the signal of interest. Numerous experiments have demonstrated that, in both biological and non-biological systems, the addition of noise can actually improve the probability of detecting the signal; this is stochastic resonance. The systems in which stochastic resonance occur are always nonlinear systems. The addition of noise to a linear system will always decrease the information transfer rate.",
            "score": 43.42122828960419
        },
        {
            "docid": "28564320_3",
            "document": "Complex systems biology . Most complex system models are often formulated in terms of concepts drawn from statistical physics, information theory and non-linear dynamics; however, such approaches are not focused on, or do not include, the conceptual part of complexity related to organization and topological attributes or algebraic topology, such as network connectivity of genomes, interactomes and biological organisms that are very important. Recently, the two complementary approaches based both on information theory, network topology/abstract graph theory concepts are being combined for example in the fields of neuroscience and human cognition. It is generally agreed that there is a hierarchy of complexity levels of organization that should be considered as distinct from that of the levels of reality in ontology. The hierarchy of complexity levels of organization in the biosphere is also recognized in modern classifications of taxonomic ranks, such as: biological domain and biosphere, biological kingdom, Phylum, biological class, order, family, genus and species. Because of their dynamic and composition variability, intrinsic \"fuzziness\", autopoietic attributes, ability to self-reproduce, and so on, organisms do not fit into the 'standard' definition of general systems, and they are therefore 'super-complex' in both their function and structure; organisms can be thus be defined in CSB only as 'meta-systems' of simpler dynamic systems Such a meta-system definition of organisms, species, 'ecosystems', and so on, is not equivalent to the definition of a \"system of systems\" as in Autopoietic Systems Theory,; it also differs from the definition proposed for example by K.D. Palmer in meta-system engineering, organisms being quite different from machines and automata with fixed input-output transition functions, or a continuous dynamical system with fixed phase space, contrary to the Cartesian philosophical thinking; thus, organisms cannot be defined merely in terms of a quintuple A of \"(states, startup state, input and output sets/alphabet, transition function)\", although 'non-deterministic automata', as well as 'fuzzy automata' have also been defined. Tessellation or cellular automata provide however an intuitive, visual/computational insight into the lower levels of complexity, and have therefore become an increasingly popular, discrete model studied in computability theory, applied mathematics, physics, computer science, theoretical biology/systems biology, cancer simulations and microstructure modeling. Evolving cellular automata using genetic algorithms is also an emerging field attempting to bridge the gap between the tessellation automata and the higher level complexity approaches in CSB.",
            "score": 64.39877820014954
        },
        {
            "docid": "15507968_3",
            "document": "Vector field reconstruction . A differential equation model is one that describes the value of dependent variables as they evolve in time or space by giving equations involving those variables and their derivatives with respect to some independent variables, usually time and/or space. An ordinary differential equation is one in which the system's dependent variables are functions of only one independent variable. Many physical, chemical, biological and electrical systems are well described by ordinary differential equations. Frequently we assume a system is governed by differential equations, but we do not have exact knowledge of the influence of various factors on the state of the system. For instance, we may have an electrical circuit that in theory is described by a system of ordinary differential equations, but due to the tolerance of resistors, variations of the supply voltage or interference from outside influences we do not know the exact parameters of the system. For some systems, especially those that support chaos, a small change in parameter values can cause a large change in the behavior of the system, so an accurate model is extremely important. Therefore, it may be necessary to construct more exact differential equations by building them up based on the actual system performance rather than a theoretical model. Ideally, one would measure all the dynamical variables involved over an extended period of time, using many different initial conditions, then build or fine tune a differential equation model based on these measurements.",
            "score": 43.73549818992615
        },
        {
            "docid": "30773_2",
            "document": "Theoretical ecology . Theoretical ecology is the scientific discipline devoted to the study of ecological systems using theoretical methods such as simple conceptual models, mathematical models, computational simulations, and advanced data analysis. Effective models improve understanding of the natural world by revealing how the dynamics of species populations are often based on fundamental biological conditions and processes. Further, the field aims to unify a diverse range of empirical observations by assuming that common, mechanistic processes generate observable phenomena across species and ecological environments. Based on biologically realistic assumptions, theoretical ecologists are able to uncover novel, non-intuitive insights about natural processes. Theoretical results are often verified by empirical and observational studies, revealing the power of theoretical methods in both predicting and understanding the noisy, diverse biological world.",
            "score": 34.01629853248596
        },
        {
            "docid": "38501477_11",
            "document": "Donabedian model . Donabedian\u2019s model can also be applied to a large health system to measure overall quality and align improvement work across a hospital, group practice or the large integrated health system to improve quality and outcomes for a population. In 2007, the US Institute for Healthcare Improvement proposed \u201cwhole system measures\u201d that address structure, process, and outcomes of care. These indicators supply health care leaders with data to evaluate the organization\u2019s performance in order to design strategic QI planning. The indicators are limited to 13 non-disease specific measures that provide system-level indications of quality, applicable to both inpatient and outpatient settings and across the continuum of care. In addition to informing the QI plan, these measures can be used to evaluate the quality of the system\u2019s care over time, how it performs relative to stated strategic planning goals, and how it performs compared to similar organizations.",
            "score": 56.41218686103821
        },
        {
            "docid": "2558855_75",
            "document": "Leibniz integral rule . The measure-theoretic version of differentiation under the integral sign also applies to summation (finite or infinite) by interpreting summation as counting measure. An example of an application is the fact that power series are differentiable in their radius of convergence.",
            "score": 31.17477560043335
        },
        {
            "docid": "19530493_4",
            "document": "ICU quality and management tools . Several measures of ICU performance have been proposed in the past 30 years. It is intuitive, and correct, to assume that ICU mortality may be a useful marker of quality. However, crude mortality rates does not take into consideration the singular aspects of each specific patient population that is treated in a certain geographic region, hospital or ICU. Therefore, approaches looking for standardized mortality ratios that are adjusted for disease severity, comorbidities and other clinical aspects are often sought. Severity of illness is usually evaluated by scoring systems that integrates clinical, physiologic and demographic variables. Scoring systems are interesting tools to describe ICU populations and explain their different outcomes. The most frequently used are the APACHE II, SAPS II and MPM. The APACHE II, for example, provides an estimate of ICU mortality based on a number of laboratory values and patient signs taking both acute and chronic disease into account. The data used should be from the initial 24 hours in the ICU, and the worst value (furtherest from baseline/normal) should be used. The APACHE II can also define \"chronic organ insufficiency\" - including liver, cardiovascular, respiratory and renal- as well as defining when a patient is immunocompromised. However, newer scores as APACHE IV and SAPS III have been recently introduced in clinical practice. More than only using scoring systems, one should search for a high rate of adherence to clinically effective interventions. Adherence to interventions as deep venous thrombosis prophylaxis, reduction of ICU-acquired infections, adequate sedation regimens and decreasing and reporting serious adverse events are essential and have been accepted as benchmarking of quality.  The complex task of collecting and analyzing data on performance measures are made easier when clinical information systems are available. Although several clinical information systems focus on important aspects as computerized physician order entry systems and individual patient tracking information, few have attempted to gather clinical information generating full reports that provide a panorama of the ICU performance and detailed data on several domains as mortality, length of stay, severity of illness, clinical scores, nosocomial infections, adverse events and adherence to good clinical practice. Through implementing quality initiatives, increasing the quality of care and patient safety are major and feasible goals. Such systems (for example: Epimed Monitor) are available for clinical use and may facilitate the process of care on a daily basis and provide data for an in-depth analysis of ICU performance.",
            "score": 55.98258364200592
        },
        {
            "docid": "221519_80",
            "document": "Differential form . On a Riemannian manifold, one may define a -dimensional Hausdorff measure for any (integer or real), which may be integrated over -dimensional subsets of the manifold. A function times this Hausdorff measure can then be integrated over -dimensional subsets, providing a measure-theoretic analog to integration of -forms. The -dimensional Hausdorff measure yields a density, as above.",
            "score": 40.05924201011658
        },
        {
            "docid": "656228_4",
            "document": "Counterintuitive Behavior of Social Systems . The paper summarizes the results of a previous study on the system dynamics governing economic dynamics in urban centers, which showed \"how industry, housing, and people interact with each other as a city grows and decays.\" The study's findings, presented more fully in Forrester's 1969 book Urban Dynamics, suggest that the root cause of depressed economic conditions is a significant shortage of job opportunities relative to the population level, and that the most popular solutions proposed at the time (e.g. an increase in the amount of low-income housing available, or a reduction in real estate taxes) counter-intuitively serve to make the situation worse by increasing the population but not the availability of jobs, so that the relative shortage increases. The paper further suggests that measures to reduce the shortage -- such as the conversion of land use from housing to industry, or an increase in real estate taxes to spur redevelopment of property -- would counter-intuitively create the result desired when enacting the failed policies.",
            "score": 32.92473804950714
        },
        {
            "docid": "8642567_8",
            "document": "Process integration . Most process integration techniques employ Pinch analysis or Pinch Tools to evaluate several processes as a whole system. Therefore, strictly speaking, both concepts are not the same, even if in certain contexts they are used interchangeably. The review by Nick Hallale (2001) explains that in the future, several trends are to be expected in the field. In the future, it seems probable that the boundary between targets and design will be blurred and that these will be based on more structural information regarding the process network. Second, it is likely that we will see a much wider range of applications of process integration. There is still much work to be carried out in the area of separation, not only in complex distillation systems, but also in mixed types of separation systems. This includes processes involving solids, such as flotation and crystallization. The use of process integration techniques for reactor design has seen rapid progress, but is still in its early stages. Third, a new generation of software tools is expected. The emergence of commercial software for process integration is fundamental to its wider application in process design.",
            "score": 52.99317157268524
        },
        {
            "docid": "341810_6",
            "document": "Joseph Liouville . Liouville worked in a number of different fields in mathematics, including number theory, complex analysis, differential geometry and topology, but also mathematical physics and even astronomy. He is remembered particularly for Liouville's theorem. In number theory, he was the first to prove the existence of transcendental numbers by a construction using continued fractions (Liouville numbers). In mathematical physics, Liouville made two fundamental contributions: the Sturm\u2013Liouville theory, which was joint work with Charles Fran\u00e7ois Sturm, and is now a standard procedure to solve certain types of integral equations by developing into eigenfunctions, and the fact (also known as Liouville's theorem) that time evolution is measure preserving for a Hamiltonian system. In Hamiltonian dynamics, Liouville also introduced the notion of action-angle variables as a description of completely integrable systems. The modern formulation of this is sometimes called the Liouville\u2013Arnold theorem, and the underlying concept of integrability is referred to as Liouville integrability.",
            "score": 49.607585072517395
        },
        {
            "docid": "41224221_2",
            "document": "Weighted correlation network analysis . Weighted correlation network analysis, also known as weighted gene co-expression network analysis (WGCNA), is a widely used data mining method especially for studying biological networks based on pairwise correlations between variables. While it can be applied to most high-dimensional data sets, it has been most widely used in genomic applications. It allows one to define modules (clusters), intramodular hubs, and network nodes with regard to module membership, to study the relationships between co-expression modules, and to compare the network topology of different networks (differential network analysis). WGCNA can be used as a data reduction technique (related to oblique factor analysis ), as a clustering method (fuzzy clustering), as a feature selection method (e.g. as gene screening method), as a framework for integrating complementary (genomic) data (based on weighted correlations between quantitative variables), and as a data exploratory technique. Although WGCNA incorporates traditional data exploratory techniques, its intuitive network language and analysis framework transcend any standard analysis technique. Since it uses network methodology and is well suited for integrating complementary genomic data sets, it can be interpreted as systems biologic or systems genetic data analysis method. By selecting intramodular hubs in consensus modules, WGCNA also gives rise to network based meta analysis techniques.",
            "score": 42.22063100337982
        },
        {
            "docid": "46383767_3",
            "document": "Agrometeorology . It is an interdisciplinary, holistic science forming a bridge between physical and biological sciences and beyond. It deals with a complex system involving soil, plant, atmosphere, agricultural management options, and others, which are interacting dynamically on various spatial and temporal scales. Specifically, the fully coupled soil-plant-atmosphere system has to be well understood in order to develop reasonable operational applications or recommendations for stakeholders. For these reasons, a comprehensive analysis of cause-effect relationships and principles that describe the influence of the state of the atmosphere, plants, and soil on different aspects of agricultural production, as well as the nature and importance of feedback between these elements of the system is necessary. Agrometeorological methods therefore use information and data from different key sciences such as soil physics and chemistry, hydrology, meteorology, crop and animal physiology and phenology, agronomy, and others. Observed information is often combined in more or less complex models, focused on various components of system parts such as mass balances (i.e. soil carbon, nutrients, and water), biomass production, crop growth and yield, and crop or pest phenology in order to detect sensitivities or potential responses of the soil-biosphere-atmosphere system. However, model applications still involve many uncertainties, which calls for further improvements of the description of system processes. A better quality of operational applications at various scales (monitoring, forecasting, warning, recommendations, etc.) is crucial for stakeholders. For example, new methods for spatial applications involve GIS and Remote Sensing for spatial data presentation and generation. Further, tailor-made products and information transfer are critical to allow effective management decisions in the short and long term. These should cover sustainability and enhancement strategies (including risk management, mitigation and adaptation) considering climate variability and change. Papers are invited addressing these problems in the context of agrometeorological applications in \u201catmosphere\u201d as an actual and important contribution to the state of the art.",
            "score": 54.19793891906738
        },
        {
            "docid": "15450044_3",
            "document": "Normalization process theory . Normalization process theory focuses attention on agentic contributions \u2013 the things that individuals and groups do to operationalize new or modified modes of practice as they interact with dynamic elements of their environments. It defines the implementation, embedding, and integration as a process that occurs when participants deliberately initiate and seek to sustain a sequence of events that bring it into operation. The dynamics of implementation processes are complex, but normalization process theory facilitates understanding by focusing attention on the mechanisms through which participants invest and contribute to them. It reveals \"the work that actors do as they engage with some ensemble of activities (that may include new or changed ways of thinking, acting, and organizing) and by which means it becomes routinely embedded in the matrices of already existing, socially patterned, knowledge and practices\". These have explored objects, agents, and contexts. In a paper published under a creative commons license, May and colleagues describe how, since 2006, NPT has undergone three iterations.",
            "score": 26.483753204345703
        },
        {
            "docid": "25202_56",
            "document": "Quantum mechanics . The Everett many-worlds interpretation, formulated in 1956, holds that \"all\" the possibilities described by quantum theory \"simultaneously\" occur in a multiverse composed of mostly independent parallel universes. This is not accomplished by introducing some \"new axiom\" to quantum mechanics, but on the contrary, by \"removing\" the axiom of the collapse of the wave packet. \"All\" of the possible consistent states of the measured system and the measuring apparatus (including the observer) are present in a \"real\" physical - not just formally mathematical, as in other interpretations - quantum superposition. Such a superposition of consistent state combinations of different systems is called an entangled state. While the multiverse is deterministic, we perceive non-deterministic behavior governed by probabilities, because we can only observe the universe (i.e., the consistent state contribution to the aforementioned superposition) that we, as observers, inhabit. Everett's interpretation is perfectly consistent with John Bell's experiments and makes them intuitively understandable. However, according to the theory of quantum decoherence, these \"parallel universes\" will never be accessible to us. The inaccessibility can be understood as follows: once a measurement is done, the measured system becomes entangled with \"both\" the physicist who measured it \"and\" a huge number of other particles, some of which are photons flying away at the speed of light towards the other end of the universe. In order to prove that the wave function did not collapse, one would have to bring \"all\" these particles back and measure them again, together with the system that was originally measured. Not only is this completely impractical, but even if one \"could\" theoretically do this, it would have to destroy any evidence that the original measurement took place (including the physicist's memory). In light of these Bell tests, Cramer (1986) formulated his transactional interpretation which is unique in providing a physical explanation for the Born rule..Relational quantum mechanics appeared in the late 1990s as the modern derivative of the Copenhagen Interpretation.",
            "score": 45.24336922168732
        },
        {
            "docid": "20598932_52",
            "document": "Hilbert space . An ergodic dynamical system is one for which, apart from the energy\u2014measured by the Hamiltonian\u2014there are no other functionally independent conserved quantities on the phase space. More explicitly, suppose that the energy is fixed, and let be the subset of the phase space consisting of all states of energy (an energy surface), and let denote the evolution operator on the phase space. The dynamical system is ergodic if there are no continuous non-constant functions on such that for all on and all time . Liouville's theorem implies that there exists a measure on the energy surface that is invariant under the time translation. As a result, time translation is a unitary transformation of the Hilbert space consisting of square-integrable functions on the energy surface with respect to the inner product",
            "score": 34.25986838340759
        },
        {
            "docid": "39621288_14",
            "document": "Deep Carbon Observatory . Recent advances in data generation techniques lead to increasingly complex data. At the same time, science and engineering disciplines are rapidly becoming more and more data driven with the ultimate aim of better understanding and modeling the dynamics of complex systems. However complex data requires integration of information and knowledge across multiple scales and spanning traditional disciplinary boundaries. Significant advances in methods, tools and applications for data science and informatics over the last five years can now be applied to multi- and inter-disciplinary problem areas. Given these challenges, it is clear that each DCO Research Community faces diverse data science and data management needs to fulfill both their overarching objectives and their day-to-day tasks. The Deep Carbon Observatory Data Science Team handles the data science and data management needs for each DCO program and for the DCO as a whole, using a combination of informatics methods, use case development, requirements analysis, inventories and interviews.",
            "score": 50.36900329589844
        },
        {
            "docid": "48534652_2",
            "document": "Integral length scale . The integral length scale measures the correlation distance of a process in terms of space or time. In essence, it looks at the overall memory of the process and how it is influenced by previous positions and parameters. An intuitive example would be the case in which you have very low Reynolds number flows (e.g., a \"Stokes\" flow), where the flow is fully reversible and thus fully correlated with previous particle positions. This concept may be extended to turbulence, where it may be thought of as the time during which a particle is influenced by its previous position.",
            "score": 26.406550645828247
        },
        {
            "docid": "465183_20",
            "document": "Educational assessment . A good assessment has both validity and reliability, plus the other quality attributes noted above for a specific context and purpose. In practice, an assessment is rarely totally valid or totally reliable. A ruler which is marked wrongly will always give the same (wrong) measurements. It is very reliable, but not very valid. Asking random individuals to tell the time without looking at a clock or watch is sometimes used as an example of an assessment which is valid, but not reliable. The answers will vary between individuals, but the average answer is probably close to the actual time. In many fields, such as medical research, educational testing, and psychology, there will often be a trade-off between reliability and validity. A history test written for high validity will have many essay and fill-in-the-blank questions. It will be a good measure of mastery of the subject, but difficult to score completely accurately. A history test written for high reliability will be entirely multiple choice. It isn't as good at measuring knowledge of history, but can easily be scored with great precision. We may generalize from this. The more reliable our estimate is of what we purport to measure, the less certain we are that we are actually measuring that aspect of attainment.",
            "score": 26.023529291152954
        },
        {
            "docid": "941613_25",
            "document": "Binding problem . The majority of theoretical frameworks for the unified richness of phenomenal experience adhere to the intuitive idea that experience exists as a single copy, and draw on \"functional\" descriptions of distributed networks of cells. Baars has suggested that certain signals, encoding what we experience, enter a \"Global Workspace\" within which they are \"broadcast\" to many sites in the cortex for parallel processing. Dehaene, Changeux and colleagues have developed a detailed neuro-anatomical version of such a workspace. Tononi and colleagues have suggested that the level of richness of an experience is determined by the narrowest information interface \"bottleneck\" in the largest sub-network or \"complex\" that acts as an integrated functional unit. Lamme has suggested that networks supporting reciprocal signaling rather than those merely involved in feed-forward signaling support experience. Edelman and colleagues have also emphasized the importance of re-entrant signaling. Cleeremans emphasizes meta-representation as the functional signature of signals contributing to consciousness.",
            "score": 39.84445774555206
        }
    ],
    "r": [
        {
            "docid": "5664_54",
            "document": "Consciousness . Regarding the primary function of conscious processing, a recurring idea in recent theories is that phenomenal states somehow integrate neural activities and information-processing that would otherwise be independent. This has been called the \"integration consensus\". Another example has been proposed by Gerald Edelman called dynamic core hypothesis which puts emphasis on reentrant connections that reciprocally link areas of the brain in a massively parallel manner. Edelman also stresses the importance of the evolutionary emergence of higher-order consciousness in humans from the historically older trait of primary consciousness which humans share with non-human animals (see \"Neural correlates\" section above). These theories of integrative function present solutions to two classic problems associated with consciousness: differentiation and unity. They show how our conscious experience can discriminate between a virtually unlimited number of different possible scenes and details (differentiation) because it integrates those details from our sensory systems, while the integrative nature of consciousness in this view easily explains how our experience can seem unified as one whole despite all of these individual parts. However, it remains unspecified which kinds of information are integrated in a conscious manner and which kinds can be integrated without consciousness. Nor is it explained what specific causal role conscious integration plays, nor why the same functionality cannot be achieved without consciousness. Obviously not all kinds of information are capable of being disseminated consciously (e.g., neural activity related to vegetative functions, reflexes, unconscious motor programs, low-level perceptual analyses, etc.) and many kinds of information can be disseminated and combined with other kinds without consciousness, as in intersensory interactions such as the ventriloquism effect. Hence it remains unclear why any of it is conscious. For a review of the differences between conscious and unconscious integrations, see the article of E. Morsella.",
            "score": 92.06127166748047
        },
        {
            "docid": "18130567_17",
            "document": "Primary consciousness . The dynamic core hypothesis (DCH) proposes that consciousness arises from neural dynamics in the thalamocortical system, as measured by the quantity neural complexity (CN). \"CN\" is an information-theoretic measure; the \"CN\" value is high if each subset of a neural system can take on many different states, and if these states make a difference to the rest of the system. The information integration theory of consciousness (IITC) shares with the DCH the idea that conscious experiences provide informative discriminations among a vast repertoire of possible experiences. In the IITC, the quantity \"phi\" is defined as the information that is integrated across the informational \"weakest link\" of a system. Importantly, \"phi\" is a measure of the capacity of a neural system to integrate information, whereas \"CN\" is a measure of the actual dynamics of the system. A third measure, causal density \"(CD)\", measures the fraction of causal interactions among elements of a system that are statistically significant.",
            "score": 84.03783416748047
        },
        {
            "docid": "634216_28",
            "document": "Hard problem of consciousness . Integrated information theory (IIT), developed by the neuroscientist and psychiatrist Giulio Tononi in 2004 and more recently also advocated by Koch, is one of the most discussed models of consciousness in neuroscience and elsewhere. The theory proposes an identity between consciousness and integrated information, with the latter item (denoted as \u03a6) defined mathematically and thus in principle measurable. The hard problem of consciousness, write Tononi and Koch, may indeed be intractable when working from matter to consciousness. However, because IIT inverts this relationship and works from phenomenological axioms to matter, they say it could be able to solve the hard problem. In this vein, proponents have said the theory goes beyond identifying human neural correlates and can be extrapolated to all physical systems. Tononi wrote (along with two colleagues):",
            "score": 77.14111328125
        },
        {
            "docid": "43374303_4",
            "document": "Andreas K. Engel . Andreas Engel has become known by his work on the so-called \u201ebinding problem\u201c. His research focuses on the hypothesis that temporal synchrony serves for dynamic coordination of signals in the brain. In addition to working on the experimental validation of this hypothesis, Engel pursues research on its cognitive and theoretical implications. As a postdoc with Wolf Singer at the Max Planck Institute for Brain Research at Frankfurt, Engel was involved in studies that demonstrated the relevance of neural synchrony, in particular of so-called gamma waves, for processing of perceptual information. In particular, the group provided evidence that temporal correlations can serve for the binding of features into coherent sensory representations. In addition to addressing the relevance of synchrony and neuronal oscillations in the visual system, the work of Engel's group yielded evidence for a relation between neural synchrony and visual awareness. In addition, Engel and coworkers contributed to demonstrating a functional role of neural synchrony for sensorimotor coupling. In the past 15 years, Engel's group has expanded their work to the human brain, using EEG and MEG in combination with source modeling techniques. The results of these studies demonstrate the importance of neuronal oscillations and synchrony for perceptual processing, attention, working memory, decision-making and consciousness. Recent work of the group on the interaction of visual, auditory and tactile systems suggests a role of temporal binding for multisensory integration. Furthermore, the group has developed novel methods for the electrophysiological analysis of resting state network activity. Engel's group also applies these approaches for the study of network malfunction in patients with movement disorders, multiple sclerosis and schizophrenia, in studies on pain, and altered networks after early sensory deprivation.  Engel also explores implications of these neurophysiogical results for theories of perception, cognition and action. A major focus of his work are the implications of the studies on neural synchrony for understanding the neural correlates of consciousness. Recent papers address links between neural dynamics and enactive views of cognition, investigating the grounding of cognition in sensorimotor coupling.",
            "score": 72.09013366699219
        },
        {
            "docid": "17909855_22",
            "document": "Theories of humor . The Computer Model of a Sense of Humor theory was suggested by Suslov in 1992. Investigation of the general scheme of information processing shows the possibility of a specific malfunction, conditioned by the necessity of a quick deletion from consciousness of a false version. This specific malfunction can be identified with a humorous effect on psychological grounds: it exactly corresponds to incongruity-resolution theory. However, an essentially new ingredient, the role of timing, is added to the well-known role of ambiguity. In biological systems, a sense of humor inevitably develops in the course of evolution, because its biological function consists of quickening the transmission of the processed information into consciousness and in a more effective use of brain resources. A realization of this algorithm in neural networks justifies naturally Spencer's hypothesis on the mechanism of laughter: deletion of a false version corresponds to zeroing of some part of the neural network and excessive energy of neurons is thrown out to the motor cortex, arousing muscular contractions.",
            "score": 71.61917114257812
        },
        {
            "docid": "27453461_2",
            "document": "Integrated information theory . Integrated information theory (IIT) attempts to explain what consciousness is and why it might be associated with certain physical systems. Given any such system, the theory predicts whether that system is conscious, to what degree it is conscious, and what particular experience it is having (see Central identity). According to IIT, a system's consciousness is determined by its causal properties and is therefore an intrinsic, fundamental property of any physical system.",
            "score": 71.04925537109375
        },
        {
            "docid": "5664_64",
            "document": "Consciousness . In neuroscience, a great deal of effort has gone into investigating how the perceived world of conscious awareness is constructed inside the brain. The process is generally thought to involve two primary mechanisms: (1) hierarchical processing of sensory inputs, and (2) memory. Signals arising from sensory organs are transmitted to the brain and then processed in a series of stages, which extract multiple types of information from the raw input. In the visual system, for example, sensory signals from the eyes are transmitted to the thalamus and then to the primary visual cortex; inside the cerebral cortex they are sent to areas that extract features such as three-dimensional structure, shape, color, and motion. Memory comes into play in at least two ways. First, it allows sensory information to be evaluated in the context of previous experience. Second, and even more importantly, working memory allows information to be integrated over time so that it can generate a stable representation of the world\u2014Gerald Edelman expressed this point vividly by titling one of his books about consciousness \"The Remembered Present\". In computational neuroscience, Bayesian approaches to brain function have been used to understand both the evaluation of sensory information in light of previous experience, and the integration of information over time. Bayesian models of the brain are probabilistic inference models, in which the brain takes advantage of prior knowledge to interpret uncertain sensory inputs in order to formulate a conscious percept; Bayesian models have successfully predicted many perceptual phenomena in vision and the nonvisual senses.",
            "score": 69.08171844482422
        },
        {
            "docid": "20092666_22",
            "document": "Technical data management system . Complex organizations may need large amounts of technical information, which can be distributed among several independent archives. Existing approaches span from \u201cno integration\u201d to \u201cstrong integration\u201d, that is based on a common database or product model. The so-called \u201cWeak Information Systems\u201d (WIS) lie somewhere in the middle. Their basic concept is to add to the pre-existing information a new layer of multiple partial models of products and processes, so that it is possible to reuse existing databases, to reduce the development from scratch, and to provide evolutionary paths relevant for the development of the WIS. Each partial model may include specific knowledge and it acts as a way to structure and access the information according to a specific user view. The comparison between strong and weak information systems may be summarized as follows:",
            "score": 68.89889526367188
        },
        {
            "docid": "39541470_7",
            "document": "Divided visual field paradigm . Visual information can be transferred from one cerebral hemisphere to the other in as little as 3ms, so any task differences greater than 3ms may represent asymmetries in neural dynamics that are more complex than a single hemisphere's simple dominance for a particular task.  Moreover, the divided visual field technique represents a relatively coarse and indirect method for localizing brain regions associated with cognitive function. Other neuroimaging techniques, including fMRI, PET, and EEG, will provide more spatial resolution, and more direct measures of neural activity. However, these methods are significantly more costly than the divided visual field paradigm.",
            "score": 68.5818862915039
        },
        {
            "docid": "1619306_6",
            "document": "Multisensory integration . However, considerations of how unified conscious representations are formed are not the full focus of multisensory Integration research. It is obviously important for the senses to interact in order to maximize how efficiently people interact with the environment. For perceptual experience and behavior to benefit from the simultaneous stimulation of multiple sensory modalities, integration of the information from these modalities is necessary. Some of the mechanisms mediating this phenomenon and its subsequent effects on cognitive and behavioural processes will be examined hereafter. Perception is often defined as one's conscious experience, and thereby combines inputs from all relevant senses and prior knowledge. Perception is also defined and studied in terms of feature extraction, which is several hundred milliseconds away from conscious experience. Notwithstanding the existence of Gestalt psychology schools that advocate a holistic approach to the operation of the brain, the physiological processes underlying the formation of percepts and conscious experience have been vastly understudied. Nevertheless, burgeoning neuroscience research continues to enrich our understanding of the many details of the brain, including neural structures implicated in multisensory integration such as the superior colliculus (SC) and various cortical structures such as the superior temporal gyrus (GT) and visual and auditory association areas. Although the structure and function of the SC are well known, the cortex and the relationship between its constituent parts are presently the subject of much investigation. Concurrently, the recent impetus on integration has enabled investigation into perceptual phenomena such as the ventriloquism effect, rapid localization of stimuli and the McGurk effect; culminating in a more thorough understanding of the human brain and its functions.",
            "score": 68.55561828613281
        },
        {
            "docid": "14760591_3",
            "document": "Computational humor . The first \"computer model of a sense of humor\" was suggested by Suslov as early as 1992. Investigation of the general scheme of information processing shows the possibility of a specific malfunction, conditioned by the necessity of a quick deletion from consciousness of a false version. This specific malfunction can be identified with a humorous effect on psychological grounds: it exactly corresponds to incongruity-resolution theory. However, an essentially new ingredient, the role of timing, is added to the well-known role of ambiguity. In biological systems, a sense of humor inevitably develops in the course of evolution, because its biological function consists of quickening the transmission of the processed information into consciousness and in a more effective use of brain resources. A realization of this algorithm in neural networks justifies naturally Spencer's hypothesis on the mechanism of laughter: deletion of a false version corresponds to zeroing of some part of the neural network and excessive energy of neurons is thrown out to the motor cortex, arousing muscular contractions.",
            "score": 68.45740509033203
        },
        {
            "docid": "9854390_13",
            "document": "Information filtering system . As data is entered, the system includes new rules; if we consider that this data can generalize the training data information, then we have to evaluate the system development and measure the system's ability to correctly predict the categories of new information. This step is simplified by separating the training data in a new series called \"test data\" that we will use to measure the error rate. As a general rule it is important to distinguish between types of errors (false positives and false negatives). For example, in the case on an aggregator of content for children, it doesn\u2019t have the same gravity to allow the passage of information not suitable for them, that shows violence or pornography, than the mistake to discard some appropriated information. To improve the system to lower error rates and have these systems with learning capabilities similar to humans we require development of systems that simulate human cognitive abilities, such as natural language understanding, capturing meaning Common an other forms of advanced processing to achieve the semantics of information.",
            "score": 67.88619232177734
        },
        {
            "docid": "7597528_9",
            "document": "International Neuroinformatics Coordinating Facility . A key element to successfully understanding the nervous system is the integration of neuroscience with information sciences. The field that studies the nervous system, neuroscience, has responded to the fantastic challenge of understanding how our brain works with the use of the most sophisticated technologies, from studies on the genome to those on brain imaging of behaviour in humans and other species, under different functional states, and at all intervening analytical levels. This effort has resulted in large quantities of data, which are ever increasing at higher levels of complexity. The data produced are heterogeneous, coming from different levels of study and modalities of analysis. To rise to this challenge of integration, and to ensure efficient and maximum use of these data, it is now necessary to develop and create these shared resources:  This challenge is being met through the merging of neurosciences with information science \u2013 the field of neuroinformatics.",
            "score": 67.77153015136719
        },
        {
            "docid": "18488828_8",
            "document": "Jarl-Thure Eriksson . In Eriksson\u2019s view, the understanding and modelling of complex systems required better understanding of the human mental functions. In his article \u201cImpact of information compression on intellectual activities in the brain\u201d in 1996 Eriksson presented an information theory based model for cognition. According to the model, humans perceive the real world through the representations of neural networks that are formed by cumulating experiences and learning processes. When brain functions are active, the cerebral cortex processes large volumes of information. Conscious thought is a result of a massive information compression process, which originates from external sensations, such as sight and hearing, or from internal thoughts or associations, and is governed by the context and the emotional state. The condensation of relevant information, which takes place in the subconscious, generates instructions for reactions as well as new cognitive input for the cortex. A conscious thought itself should be considered as the control feedback of what the subconscious has already determined.",
            "score": 67.60166931152344
        },
        {
            "docid": "71627_4",
            "document": "Integrated geography . The links between human and physical geography were once more apparent than they are today. As human experience of the world is increasingly mediated by technology, the relationships between humans and the environment have often become obscured. Thereby, integrated geography represents a critically important set of analytical tools for assessing the impact of human presence on the environment. This is done by measuring the result of human activity on natural landforms and cycles. Methods for which this information is gained include remote sensing, and geographic information systems. Integrated geography helps us to ponder the environment in terms of its relationship to people. With integrated geography we can analyze different social science and humanities perspectives and their use in understanding human-environment processes. Hence, it is considered the third branch of geography. The other branches being physical and human geography",
            "score": 65.9310302734375
        },
        {
            "docid": "27453461_4",
            "document": "Integrated information theory . David Chalmers has argued that any attempt to explain consciousness in purely physical terms (i.e. to start with the laws of physics as they are currently formulated and derive the necessary and inevitable existence of consciousness) eventually runs into the so-called \"hard problem\". Rather than try to start from physical principles and arrive at consciousness, IIT \"starts with consciousness\" (accepts the existence of consciousness as certain) and reasons about the properties that a postulated physical substrate would have to have in order to account for it. The ability to perform this jump from phenomenology to mechanism rests on IIT's assumption that if a conscious experience can be fully accounted for by an underlying physical system, then the properties of the physical system must be constrained by the properties of the experience.",
            "score": 65.92747497558594
        },
        {
            "docid": "843899_22",
            "document": "R. Scott Bakker . The title of Bakker's paper, \"A Blind Brain Theory for the Appearance of Consciousness\", reflects his assertion therein that the \"Blind Brain Theory\" serves as a vehicle to describe and distinguish the features of how conscious experience appears to self-reflection rather than the actual specific systems and functioning underlying consciousness. To begin, Bakker imagines an explanatory vehicle he refers to as a Recursive System, the brain architecture which has evolved to track even more the basic architecture of itself, possibly akin to the relationship between the frontal cortex to the rest of the brain. Bakker suggests two parts to a Recursive System, the system \"open,\" all the information processed by the brain, and the system \"closed,\" referring to the information accessible to consciousness. The basic assumption regarding the existence of a Recursive System implies a limit on the information available to the RS-closed, conscious awareness, which Bakker dubs Informatic Asymmetry and its the Asymptotic Limit. As specific examples Bakker refers to the cognitive psychology literature citing change blindness and inattentional bias (cognitive bias), wherein there are clear divergences between the information processed by the brain and information from the self-reports of perception.",
            "score": 65.74030303955078
        },
        {
            "docid": "1988689_11",
            "document": "Computational cognition . The field of cognition may have benefitted from the use of connectionist network but because of the completed system, setting up the neural network models can be quite a tedious task and the results may be less interpretable than the system they are trying to model. Therefore, the results can be used as evidence for broad theory of cognition without explaining the particular process happening within the cognitive function. Other disadvantages of connectionism lie in the research methods it employs or hypothesis it tests, which has been proven inaccurate or ineffective often, taking connectionist models further from an accurate representation of how the brain functions. These issues cause neural network models to be ineffective on studying higher forms of information-processing, and hinder connectionism from advancing the general understanding of human cognition.",
            "score": 65.63988494873047
        },
        {
            "docid": "6045563_4",
            "document": "TransducerML . Metadata relating to archiving, indexing and cataloguing is an integral part of TML, since a TML data stream is designed to be self-contained and self-sufficient. Any information about the system, as well as information required to later parse and process the data, is captured in the TML system description. In addition to information about the system that produced the data, precise information about the data itself is captured. Data types, data sizes, ordering and arrangement, calibration information, units of measurement, precise time-tagging of individual groups of data, information about uncertainty, coordinate reference frames (where applicable) and physical phenomena relating to the data are among the details which are captured and retained. The TML system description therefore automatically tags all fields, which can later be stored in a registry for discovery.",
            "score": 65.43578338623047
        },
        {
            "docid": "3062721_16",
            "document": "Neuroinformatics . Biology is concerned with molecular data (from genes to cell specific expression); medicine and anatomy with the structure of synapses and systems level anatomy; engineering \u2013 electrophysiology (from single channels to scalp surface EEG), brain imaging; computer science \u2013 databases, software tools, mathematical sciences \u2013 models, chemistry \u2013 neurotransmitters, etc. Neuroscience uses all aforementioned experimental and theoretical studies to learn about the brain through its various levels. Medical and biological specialists help to identify the unique cell types, and their elements and anatomical connections. Functions of complex organic molecules and structures, including a myriad of biochemical, molecular, and genetic mechanisms which regulate and control brain function, are determined by specialists in chemistry and cell biology. Brain imaging determines structural and functional information during mental and behavioral activity. Specialists in biophysics and physiology study physical processes within neural cells neuronal networks. The data from these fields of research is analyzed and arranged in databases and neural models in order to integrate various elements into a sophisticated system; this is the point where neuroinformatics meets other disciplines.",
            "score": 65.07804870605469
        },
        {
            "docid": "2920040_2",
            "document": "Neuronal tuning . Neuronal tuning refers to the hypothesized property of brain cells by which they selectively represent a particular type of sensory, association, motor, or cognitive information. Some neuronal responses have been hypothesized to be optimally tuned to specific patterns through experience. Neuronal tuning can be strong and sharp, as observed in primary visual cortex (area V1) (but see Carandini et al 2005 ), or weak and broad, as observed in neural ensembles. Single neurons are hypothesized to be simultaneously tuned to several modalities, such as visual, auditory, and olfactory. Neurons hypothesized to be tuned to different signals are often hypothesized to integrate information from the different sources. In computational models called neural networks, such integration is the major principle of operation. The best examples of neuronal tuning can be seen in the visual, auditory, olfactory, somatosensory, and memory systems, although due to the small number of stimuli tested the generality of neuronal tuning claims is still an open question.",
            "score": 64.97679901123047
        },
        {
            "docid": "28564320_3",
            "document": "Complex systems biology . Most complex system models are often formulated in terms of concepts drawn from statistical physics, information theory and non-linear dynamics; however, such approaches are not focused on, or do not include, the conceptual part of complexity related to organization and topological attributes or algebraic topology, such as network connectivity of genomes, interactomes and biological organisms that are very important. Recently, the two complementary approaches based both on information theory, network topology/abstract graph theory concepts are being combined for example in the fields of neuroscience and human cognition. It is generally agreed that there is a hierarchy of complexity levels of organization that should be considered as distinct from that of the levels of reality in ontology. The hierarchy of complexity levels of organization in the biosphere is also recognized in modern classifications of taxonomic ranks, such as: biological domain and biosphere, biological kingdom, Phylum, biological class, order, family, genus and species. Because of their dynamic and composition variability, intrinsic \"fuzziness\", autopoietic attributes, ability to self-reproduce, and so on, organisms do not fit into the 'standard' definition of general systems, and they are therefore 'super-complex' in both their function and structure; organisms can be thus be defined in CSB only as 'meta-systems' of simpler dynamic systems Such a meta-system definition of organisms, species, 'ecosystems', and so on, is not equivalent to the definition of a \"system of systems\" as in Autopoietic Systems Theory,; it also differs from the definition proposed for example by K.D. Palmer in meta-system engineering, organisms being quite different from machines and automata with fixed input-output transition functions, or a continuous dynamical system with fixed phase space, contrary to the Cartesian philosophical thinking; thus, organisms cannot be defined merely in terms of a quintuple A of \"(states, startup state, input and output sets/alphabet, transition function)\", although 'non-deterministic automata', as well as 'fuzzy automata' have also been defined. Tessellation or cellular automata provide however an intuitive, visual/computational insight into the lower levels of complexity, and have therefore become an increasingly popular, discrete model studied in computability theory, applied mathematics, physics, computer science, theoretical biology/systems biology, cancer simulations and microstructure modeling. Evolving cellular automata using genetic algorithms is also an emerging field attempting to bridge the gap between the tessellation automata and the higher level complexity approaches in CSB.",
            "score": 64.3987808227539
        },
        {
            "docid": "44391005_33",
            "document": "Information field theory . The calculation of the Gibbs free energy requires the calculation of Gaussian integrals over an information Hamiltonian, since the internal information energy isformula_114Such integrals can be calculated via a field operator formalism, in which formula_115 is the field operator. This generates the field expression formula_4 within the integral if applied to the Gaussian distribution function, formula_117and any higher power of the field if applied several times,formula_118If the information Hamiltonian is analytical, all its terms can be generated via the field operatorformula_119As the field operator does not depend on the field formula_120 itself, it can be pulled out of the path integral of the internal information energy construction,formula_121where formula_122 should be regarded as a functional that always returns the value formula_123 irrespective the value of its input formula_88. The resulting expression can be calculated by commuting the mean field annihilator formula_125 to the right of the expression, where they vanish since formula_126. The mean field annihilator formula_125 commutes with the mean field as formula_128",
            "score": 64.12483215332031
        },
        {
            "docid": "7325282_15",
            "document": "HIV/AIDS in Pakistan . Lack of the ability to measure the outcomes or impact of interventions in real time (so that this knowledge can inform program direction) was likely the most important factor in the low performance of the first Enhanced Program. Other challenges that must be overcome include establishment of a transparent financial management and a smooth logistical and procurement system. Much of the Enhanced Program services are contracted out and delays in procurement of these services meant that many of the cities went without services for months to years. More complex (and longer term) challenges will include determining how to integrate many of HIV activities within other health activities, improve planning to anticipate future direction of the epidemic and its response and to enhance efficiency and effectiveness of the interventions. For these research must become part of the interventions to guide their implementation using local context and to involve epidemiological tools such as routine analysis of available data and even mathematical modeling to guide program planning. After devolution in 2011 the Provinces mobilized their own resources that were mainly used to prevent the infection from establishing in Key Populations. The country also succeeded in getting three Global Fund grants and the present grant is more directed to strengthening HIV treatment care and support services to HIV positives and their families . However, the available information suggests a slower case detection and confirmation and any response services for the infected population. This may be resulting from the fact that HIV is mainly confined to high risk population mentioned above. However, it can not be ruled out mainly due to the strong stigma attached, main surveys focused only in the urban areas and general population not having access to free laboratory test.",
            "score": 64.08391571044922
        },
        {
            "docid": "7284218_15",
            "document": "Integrated enterprise modeling . The delimited real system is convicted with help of the IEM method in an abstract model. IEM is the construction of the two main positions \"information model\" and \"business process model\". The \"information model\" is made by the specification of the object classes to be modeled for \"product\", \"order\" and \"resource\" with the class structures as well as descriptive and relational features. By identification and description of functions, activities and its combination to processes the \"business process model\" is formed. As a general rule the construction of the \"information model\" follows first in which the modeling person can go back to available reference class structures. The reference classes which do not correspond to the real system or were not found to be relevant at the system delimitation are deleted. The missing relevant classes are inserted. After the object base is fixed, the activities and functions are joined together at the objects according to the \"generic activity model\" and with the help of combination elements to business processes. A model is made which can be analysed and changed if it's required. It often happens, that during the construction of the \"business process model\" new relevant object classes are identified so that the class trees getting completed. The construction of the two positions is therefore an iterative process.",
            "score": 64.06367492675781
        },
        {
            "docid": "34864050_2",
            "document": "Cognitive-experiential self-theory . Cognitive-experiential self-theory (CEST) is a dual-process model of perception developed by Seymour Epstein. CEST is based around the idea that people operate using two separate systems for information processing: analytical-rational and intuitive-experiential. The analytical-rational system is deliberate, slow, and logical. The intuitive-experiential system is fast, automatic, and emotionally driven. These are independent systems that operate in parallel and interact to produce behavior and conscious thought. There have been other dual-process theories in the past. Shelly Chaiken's heuristic-systematic model, Carl Jung's distinction between thinking and feeling, and John Bargh's theory on automatic vs. non-automatic processing all have similar components to CEST. However, Epstein's cognitive-experiential self-theory is unique in that it places a dual-process model within the context of a global theory of personality, rather than considering it as an isolated construct or cognitive shortcut. Epstein argues that within the context of day-to-day life, a constant interaction occurs between the two systems. Because the experiential system is fast, guided by emotion and past experience, and requires little in terms of cognitive resources, it is especially equipped to handle the majority of information processing on a daily basis, all of which occurs outside of conscious awareness. This, in turn, allows us to focus the limited capacity of our rational system on whatever requires our conscious attention at the time. Individual difference in preference for analytical or experiential processing can be measured using the Rational Experiential Inventory (REI). The REI measures the two independent processing modes with two factors: need for cognition (rational measure) and faith in intuition (experiential measure). Several studies have confirmed that the REI is a reliable measure of individual difference in information processing, and that the two independent thinking styles measured account for a substantial amount of variance that is not addressed by other personality theories such as the five factor model.",
            "score": 64.02693176269531
        },
        {
            "docid": "2035274_4",
            "document": "Clark\u2013Wilson model . The model was described in a 1987 paper (\"A Comparison of Commercial and Military Computer Security Policies\") by David D. Clark and David R. Wilson. The paper develops the model as a way to formalize the notion of information integrity, especially as compared to the requirements for multilevel security (MLS) systems described in the Orange Book. Clark and Wilson argue that the existing integrity models such as Biba (read-up/write-down) were better suited to enforcing data integrity rather than information confidentiality. The Biba models are more clearly useful in, for example, banking classification systems to prevent the untrusted modification of information and the tainting of information at higher classification levels, respectively. In contrast, Clark\u2013Wilson is more clearly applicable to business and industry processes in which the integrity of the information content is paramount at any level of classification (although the authors stress that all three models are obviously of use to both government and industry organizations).",
            "score": 63.90996551513672
        },
        {
            "docid": "33818014_8",
            "document": "Nervous system network models . On a high level representation, the neurons can be viewed as connected to other neurons to form a neural network in one of three ways. A specific network can be represented as a physiologically (or anatomically) connected network and modeled that way. There are several approaches to this (see Ascoli, G.A. (2002) Sporns, O. (2007), Connectionism, Rumelhart, J. L., McClelland, J. L., and PDP Research Group (1986), Arbib, M. A. (2007)). Or, it can form a functional network that serves a certain function and modeled accordingly (Honey, C. J., Kotter, R., Breakspear, R., & Sporns, O. (2007), Arbib, M. A. (2007)). A third way is to hypothesize a theory of the functioning of the biological components of the neural system by a mathematical model, in the form of a set of mathematical equations. The variables of the equation are some or all of the neurobiological properties of the entity being modeled, such as the dimensions of the dendrite or the stimulation rate of action potential along the axon in a neuron. The mathematical equations are solved using computational techniques and the results are validated with either simulation or experimental processes. This approach to modeling is called computational neuroscience. This methodology is used to model components from the ionic level to system level of the brain. This method is applicable for modeling integrated system of biological components that carry information signal from one neuron to another via intermediate active neurons that can pass the signal through or create new or additional signals. The computational neuroscience approach is extensively used and is based on two generic models, one of cell membrane potential Goldman (1943) and Hodgkin and Katz (1949), and the other based on Hodgkin-Huxley model of action potential (information signal).",
            "score": 63.8583984375
        },
        {
            "docid": "18345264_11",
            "document": "Neural correlates of consciousness . The potential \"richness of conscious experience\" appears to increase from deep sleep to drowsiness to full wakefulness, as might be quantified using notions from complexity theory that incorporate both the dimensionality as well as the granularity of conscious experience to give an integrated-information-theoretical account of consciousness. As behavioral arousal increases so does the range and complexity of possible behavior. Yet in REM sleep there is a characteristic atonia, low motor arousal and the person is difficult to wake up, but there is still high metabolic and electric brain activity and vivid perception.",
            "score": 63.61598587036133
        },
        {
            "docid": "27453461_17",
            "document": "Integrated information theory . The calculation of even a modestly-sized system's formula_12 is often computationally intractable, so efforts have been made to develop heuristic or proxy measures of integrated information. For example, Masafumi Oizumi has developed formula_13, a practical approximation for integrated information that solves the theoretical shortcomings of previously proposed proxy measures, such as the one proposed by Adam Barrett.",
            "score": 63.184791564941406
        },
        {
            "docid": "30138821_4",
            "document": "Quantum cognition . The brain is definitely a macroscopic physical system operating on the scales (of time, space, temperature) which differ crucially from the corresponding quantum scales. (The macroscopic quantum physical phenomena such as e.g. the Bose-Einstein condensate are also characterized by the special conditions which are definitely not fulfilled in the brain.) In particular, the brain is simply too hot to be able perform the real quantum information processing, i.e., to use the quantum carriers of information such as photons, ions, electrons. As is commonly accepted in brain science, the basic unit of information processing is a neuron. It is clear that a neuron cannot be in the superposition of two states: firing and non-firing. Hence, it cannot produce superposition playing the basic role in the quantum information processing. Superpositions of mental states are created by complex networks of neurons (and these are classical neural networks). Quantum cognition community states that the activity of such neural networks can produce effects which are formally described as interference (of probabilities) and entanglement. In principle, the community does not try to create the concrete models of quantum (-like) representation of information in the brain.",
            "score": 63.09233093261719
        },
        {
            "docid": "30138821_2",
            "document": "Quantum cognition . Quantum cognition is an emerging field which applies the mathematical formalism of quantum theory to model cognitive phenomena such as information processing by the human brain, language, decision making, human memory, concepts and conceptual reasoning, human judgment, and perception. The field clearly distinguishes itself from the quantum mind as it is not reliant on the hypothesis that there is something micro-physical quantum mechanical about the brain. Quantum cognition is based on the quantum-like paradigm or generalized quantum paradigm or quantum structure paradigm that information processing by complex systems such as the brain, taking into account contextual dependence of information and probabilistic reasoning, can be mathematically described in the framework of quantum information and quantum probability theory.",
            "score": 62.89509582519531
        }
    ]
}