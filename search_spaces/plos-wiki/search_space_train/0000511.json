{
    "q": [
        {
            "docid": "23732_14",
            "document": "Psychophysiology . Physiological computing systems all contain an element that may be termed as an adaptive controller that may be used to represent the player. This adaptive controller represents the decision-making process underlying software adaptation. In their simplest form, adaptive controllers are expressed in Boolean statements. Adaptive controllers encompass not only the decision-making rules, but also the psychophysiological inference that is implicit in the quantification of those trigger points used to activate the rules. The representation of the player using an adaptive controller can become very complex and often only one-dimensional. The loop used to describe this process is known as the biocybernetic loop. The biocybernetic loop describes the closed loop system that receives psychophysiological data from the player, transforms that data into a computerized response, which then shapes the future psychophysiological response from the player. A positive control loop tends towards instability as player-software loop strives towards a higher standard of desirable performance. The physiological computer game may wish to incorporate both positive and negative loops into the adaptive controller.",
            "score": 79.02656257152557
        },
        {
            "docid": "3581220_6",
            "document": "Single-unit recording . Single-unit recordings have provided tools to explore the brain and apply this knowledge to current technologies. Cognitive scientists have used single-unit recordings in the brains of animals and humans to study behaviors and functions. Electrodes can also be inserted into the brain of epileptic patients to determine the position of epileptic foci. More recently, single-unit recordings have been used in brain machine interfaces (BMI). BMIs record brain signals and decode an intended response, which then controls the movement of an external device (such as a computer cursor or prosthetic limb).",
            "score": 117.45839619636536
        },
        {
            "docid": "16928506_10",
            "document": "Visual servoing . Feddema et al. introduced the idea of generating task trajectory with respect to the feature velocity. This is to ensure that the sensors are not rendered ineffective (stopping the feedback) for any the robot motions. The authors assume that the objects are known a priori (e.g. CAD model) and all the features can be extracted from the object. The work by Espiau et al. discusses some of the basic questions in visual servoing. The discussions concentrate on modeling of the interaction matrix, camera, visual features (points, lines, etc..). In  an adaptive servoing system was proposed with a look-and-move servoing architecture. The method used optical flow along with SSD to provide a confidence metric and a stochastic controller with Kalman filtering for the control scheme. The system assumes (in the examples) that the plane of the camera and the plane of the features are parallel., discusses an approach of velocity control using the Jacobian relationship s\u02d9 = Jv\u02d9 . In addition the author uses Kalman filtering, assuming that the extracted position of the target have inherent errors (sensor errors). A model of the target velocity is developed and used as a feed-forward input in the control loop. Also, mentions the importance of looking into kinematic discrepancy, dynamic effects, repeatability, settling time oscillations and lag in response.",
            "score": 78.47492206096649
        },
        {
            "docid": "33246145_6",
            "document": "Neural decoding . Implicit about the decoding hypothesis is the assumption that neural spiking in the brain somehow represents stimuli in the external world. The decoding of neural data would be impossible if the neurons were firing randomly: nothing would be represented. This process of decoding neural data forms a loop with neural encoding. First, the organism must be able to perceive a set of stimuli in the world \u2013 say a picture of a hat. Seeing the stimuli must result in some internal learning: the encoding stage. After varying the range of stimuli that is presented to the observer, we expect the neurons to adapt to the statistical properties of the signals, encoding those that occur most frequently: the efficient-coding hypothesis. Now neural decoding is the process of taking these statistical consistencies, a statistical model of the world, and reproducing the stimuli. This may map to the process of thinking and acting, which in turn guide what stimuli we receive, and thus, completing the loop.",
            "score": 103.26628375053406
        },
        {
            "docid": "15502859_11",
            "document": "EICASLAB . EICASLAB includes the following tools and features to support the control algorithm design: The Automatic Algorithm Generation tool, starting from the \u201cplant simplified model\u201d and from the \"control required performance\" generates the control algorithm. On the basis of the plant design data, the applied control design methodology allows design of controllers with guaranteed performance without requiring any tuning in field in spite of the unavoidable uncertainty which always exists between any mathematical model built on the basis of plant design data and the plant actual performance (for fundamentals on control in presence of uncertainty see ). The designer can choose among three control basic schemes and for each one he has the option of selecting control algorithms at different level of complexity.  In synthesis, the automatically generated control is performed by the resultant of three actions: The plant's state observer task may be extended to estimate and predict the disturbance acting on the plant. The plant disturbance prediction and compensation is an original control feature, which allows significant reduction of control error.  Model Parameter Identification is a tool which allows the identification of the most appropriate values of the simplified model parameters from recorded experimental data or simulated trials performed by using the \u201cplant fine model\u201d. The parameter's \"true\" value does not exist: the model is an approximated description of the plant and then, the parameter's \"best\" value depends on the cost function adopted to evaluate the difference between model and plant. The identification method estimates the best values of the simplified model parameters from the point of view of the closed loop control design. Control Parameter Optimization is a tool which performs control parameter tuning in simulated environment. The optimization is performed numerically over a predefined simulated trial, that is for a given mission (host command sequence and disturbance acting on the plant and any other potential event related to the plant performance) and for a given functional cost associated to the plant control performance.",
            "score": 118.92438447475433
        },
        {
            "docid": "52020537_9",
            "document": "Ji-Feng Zhang . He investigated the capability issues of robust and adaptive control in dealing with uncertainty, and revealed that to capture the intrinsic limitations of adaptive control, it is necessary to use sup-types of transient and persistent performance, rather than limsup-types which reflect only asymptotic behavior of a system. This indicates that intimate interaction and inherent conflict between identification and control result in a certain performance lower bound which does not approach the nominal performance even when the system varies very slowly. For nonlinear hybrid stochastic systems with unknown jump-Markov parameters, he with co-authors used the Wonham nonlinear filter to estimate the unknown parameters and presented an estimation error bound, which is a basic tool and plays an important role in performance analysis of adaptive control of nonlinear hybrid stochastic systems. He also attacked a series of hard problems related on global output-feedback control of nonlinear stochastic systems with inverse dynamics, including practical output-feedback risk-sensitive control, robust adaptive stabilization, small-gain theorem of general nonlinear stochastic systems. Different from the existing literature, the systems considered in his work are so complicated that renders any control design for them is much difficult. He developed a set of predominant methods and obtained many innovative results. The work represents an accomplishment for both the field of stochastic nonlinear stabilization and the backstepping method.",
            "score": 96.59178900718689
        },
        {
            "docid": "172071_2",
            "document": "Adaptive filter . An adaptive filter is a system with a linear filter that has a transfer function controlled by variable parameters and a means to adjust those parameters according to an optimization algorithm. Because of the complexity of the optimization algorithms, almost all adaptive filters are digital filters. Adaptive filters are required for some applications because some parameters of the desired processing operation (for instance, the locations of reflective surfaces in a reverberant space) are not known in advance or are changing. The closed loop adaptive filter uses feedback in the form of an error signal to refine its transfer function.",
            "score": 89.68813395500183
        },
        {
            "docid": "1100516_2",
            "document": "Model predictive control . Model predictive control (MPC) is an advanced method of process control that is used to control a process while satisfying a set of constraints. It has been in use in the process industries in chemical plants and oil refineries since the 1980s. In recent years it has also been used in power system balancing models and in power electronics. Model predictive controllers rely on dynamic models of the process, most often linear empirical models obtained by system identification. The main advantage of MPC is the fact that it doesn't allow the current timeslot to be optimized, while keeping future timeslots in account. This is achieved by optimizing a finite time-horizon, but only implementing the current timeslot and then optimizing again, repeatedly, thus differing from LQR. Also MPC has the ability to anticipate future events and can take control actions accordingly. PID controllers do not have this predictive ability. MPC is nearly universally implemented as a digital control, although there is research into achieving faster response times with specially designed analog circuitry.",
            "score": 83.36372828483582
        },
        {
            "docid": "40158142_9",
            "document": "Nonlinear system identification . Artificial neural networks try loosely to imitate the network of neurons in the brain where computation takes place through a large number of simple processing elements. A typical neural network consists of a number of simple processing units interconnected to form a complex network. Layers of such units are arranged so that data is entered at the input layer and passes through either one or several intermediate layers before reaching the output layer. In supervised learning the network is trained by operating on the difference between the actual output and the desired output of the network, the prediction error, to change the connection strengths between the nodes. By iterating the weights are modified until the output error reaches an acceptable level. This process is called machine learning because the network adjusts the weights so that the output pattern is reproduced. Neural networks have been extensively studied and there are many excellent textbooks devoted to this topic in general, and more focussed textbooks which emphasise control and systems applications. There are two main problem types that can be studied using neural networks: static problems, and dynamic problems. Static problems include pattern recognition, classification, and approximation. Dynamic problems involve lagged variables and are more appropriate for system identification and related applications. Depending on the architecture of the network the training problem can be either nonlinear-in-the-parameters which involves optimisation or linear-in-the-parameters which can be solved using classical approaches. The training algorithms can be categorised into supervised, unsupervised, or reinforcement learning. Neural networks have excellent approximation properties but these are usually based on standard function approximation results using for example the Weierstrass Theorem that applies equally well to polynomials, rational functions, and other well-known models.  Neural networks have been applied extensively to system identification problems which involve nonlinear and dynamic relationships. However, classical neural networks are purely gross static approximating machines. There is no dynamics within the network. Hence when fitting dynamic models all the dynamics arise by allocating lagged inputs and outputs to the input layer of the network. The training procedure then produces the best static approximation that relates the lagged variables assigned to the input nodes to the output. There are more complex network architectures, including recurrent networks, that produce dynamics by introducing increasing orders of lagged variables to the input nodes. But in these cases it is very easy to over specify the lags and this can lead to over fitting and poor generalisation properties.  Neural networks have several advantages; they are conceptually simple, easy to train and to use, have excellent approximation properties, the concept of local and parallel processing is important and this provides integrity and fault tolerant behaviour. The biggest criticism of the classical neural network models is that the models produced are completely opaque and usually cannot be written down or analysed. It is therefore very difficult to know what is causing what, to analyse the model, or to compute dynamic characteristics from the model. Some of these points will not be relevant to all applications but they are for dynamic modelling.",
            "score": 75.77580273151398
        },
        {
            "docid": "33882236_6",
            "document": "Adaptive collaborative control . Adaptive collaborative control is most accurately modeled as a closed loop feedback control system. Closed loop feedback control describes the event where the outputs of a system from an input are used to influence the present or future behavior of the system. The feedback control model is governed by a set of equations that are used to predict the future state of the simuland and regulate its behavior. These equations \u2013 in conjunction with principles of control theory \u2013 are used to evolve physical operations of the simuland to include, but not limited to: dialogue, path planning, motion, monitoring, and lifting objects over time. Many times, these equations are modeled as nonlinear partial differential equations over a continuous time domain.  Due to their complexity, powerful computers are necessary to implement these models. A consequence of using computers to simulate these models is that continuous systems cannot be fully calculated. Instead, numerical solutions, such as the Runge-Kutta methods, are utilized to approximate these continuous models.  These equations are initialized from the response of one or more sources and rates of changes and outputs are calculated. These rates of changes predict the states of the simuland a short time in the future. The time increment for this prediction is called a time step. These new states are applied to the model to determine the new rates of changes and observational data. This behavior is continued until the desired number of iterations is completed. In the event a future state violates or comes within a tolerance of the violation the simuland will confer with its human counterpart seeking advice on how to proceed from that point. The outputs, or observational data, are used by the human operators to determine what they believe is the best course of action for the simuland. Their commands are fed with the input into the control system and assessed regarding its effectiveness in resolving the issues. If the human commands are determined to be valuable, the simuland will adjust its control input to what the human suggested. If the human\u2019s commands are determined to be unbeneficial, malicious, or non-existent, the model will seek its own correction approach.",
            "score": 92.22832679748535
        },
        {
            "docid": "14246162_76",
            "document": "Memristor . Memristor patents include applications in programmable logic, signal processing, neural networks, control systems, reconfigurable computing, brain-computer interfaces and RFID. Memristive devices are potentially used for stateful logic implication, allowing a replacement for CMOS-based logic computation. Several early works have been reported in this direction. In 2009, a simple electronic circuit consisting of an LC network and a memristor was used to model experiments on adaptive behavior of unicellular organisms. It was shown that subjected to a train of periodic pulses, the circuit learns and anticipates the next pulse similar to the behavior of slime molds \"Physarum polycephalum\" where the viscosity of channels in the cytoplasm responds to periodic environment changes. Applications of such circuits may include, e.g., pattern recognition. The DARPA SyNAPSE project funded HP Labs, in collaboration with the Boston University Neuromorphics Lab, has been developing neuromorphic architectures which may be based on memristive systems. In 2010, Versace and Chandler described the MoNETA (Modular Neural Exploring Traveling Agent) model. MoNETA is the first large-scale neural network model to implement whole-brain circuits to power a virtual and robotic agent using memristive hardware. Application of the memristor crossbar structure in the construction of an analog soft computing system was demonstrated by Merrikh-Bayat and Shouraki. In 2011, they showed how memristor crossbars can be combined with fuzzy logic to create an analog memristive neuro-fuzzy computing system with fuzzy input and output terminals. Learning is based on the creation of fuzzy relations inspired from Hebbian learning rule.",
            "score": 79.98763942718506
        },
        {
            "docid": "11545_53",
            "document": "Feedback . Feedback loops provide generic mechanisms for controlling the running, maintenance, and evolution of software and computing systems. Feedback-loops are important models in the engineering of adaptive software, as they define the behaviour of the interactions among the control elements over the adaptation process, to guarantee system properties at run-time. Feedback loops and foundations of control theory have been successfully applied to computing systems. In particular, they have been applied to the development of products such as IBM's Universal Database server and IBM Tivoli. From a software perspective, the autonomic (MAPE, monitor analyze plan execute) loop proposed by researchers of IBM is another valuable contribution to the application of feedback loops to the control of dynamic properties and the design and evolution of autonomic software systems.",
            "score": 64.4956464767456
        },
        {
            "docid": "3581220_22",
            "document": "Single-unit recording . Brain-machine interfaces (BMIs) have been developed within the last 20 years. By recording single unit potentials, these devices can decode signals through a computer and output this signal for control of an external device such as a computer cursor or prosthetic limb. BMIs have the potential to restore function in patients with paralysis or neurological disease. This technology has potential to reach a wide variety of patients but is not yet available clinically due to lack of reliability in recording signals over time. The primary hypothesis regarding this failure is that the chronic inflammatory response around the electrode causes neurodegeneration that reduces the number of neurons it is able to record from (Nicolelis, 2001). In 2004, the BrainGate pilot clinical trial was initiated to \"test the safety and feasibility of a neural interface system based on an intracortical 100-electrode silicon recording array\". This initiative has been successful in advancement of BCIs and in 2011, published data showing long term computer control in a patient with tetraplegia (Simeral, 2011).",
            "score": 105.17150831222534
        },
        {
            "docid": "10159567_7",
            "document": "Spiking neural network . In practice, there is a major difference between the theoretical power of spiking neural networks and what has been demonstrated. They have proved useful in neuroscience, but not (yet) in engineering. Some large scale neural network models have been designed that take advantage of the pulse coding found in spiking neural networks, these networks mostly rely on the principles of reservoir computing. However, the real world application of large scale spiking neural networks has been limited because the increased computational costs associated with simulating realistic neural models have not been justified by commensurate benefits in computational power. As a result, there has been little application of large scale spiking neural networks to solve computational tasks of the order and complexity that are commonly addressed using rate coded (second generation) neural networks. In addition it can be difficult to adapt second generation neural network models into real time, spiking neural networks (especially if these network algorithms are defined in discrete time). It is relatively easy to construct a spiking neural network model and observe its dynamics. It is much harder to develop a model with stable behavior that computes a specific function.",
            "score": 83.84962236881256
        },
        {
            "docid": "57009374_7",
            "document": "IDA Indoor Climate and Energy . IDA ICE can be used for complete energy and design studies, involving the envelope, systems, plant and control strategies. The equation-based approach enables solving more complex mathematical problems than software using imperative programming languages. The IDA ICE model library is open source, the model equations can be viewed and adapted, every variable in the whole system can be logged. The flexible architecture of the software makes it easy to develop the software continuously to adapt it to local requirements and languages, and to expand it with new capabilities. Additional features like parametric simulation runs and visual scripting support decision making in a parametric design process. The coupling with optimization engines like GenOpt is available directly in the program.",
            "score": 62.4678111076355
        },
        {
            "docid": "2567511_12",
            "document": "Neural engineering . Neuromechanics is the coupling of neurobiology, biomechanics, sensation and perception, and robotics (Edwards 2010). Researchers are using advanced techniques and models to study the mechanical properties of neural tissues and their effects on the tissues' ability to withstand and generate force and movements as well as their vulnerability to traumatic loading (Laplaca & Prado 2010). This area of research focuses on translating the transformations of information among the neuromuscular and skeletal systems to develop functions and governing rules relating to operation and organization of these systems (Nishikawa et al. 2007). Neuromechanics can be simulated by connecting computational models of neural circuits to models of animal bodies situated in virtual physical worlds (Edwards 2010). Experimental analysis of biomechanics including the kinematics and dynamics of movements, the process and patterns of motor and sensory feedback during movement processes, and the circuit and synaptic organization of the brain responsible for motor control are all currently being researched to understand the complexity of animal movement. Dr. Michelle LaPlaca's lab at Georgia Institute of Technology is involved in the study of mechanical stretch of cell cultures, shear deformation of planar cell cultures, and shear deformation of 3D cell containing matrices. Understanding of these processes is followed by development of functioning models capable of characterizing these systems under closed loop conditions with specially defined parameters. The study of neuromechanics is aimed at improving treatments for physiological health problems which includes optimization of prostheses design, restoration of movement post injury, and design and control of mobile robots. By studying structures in 3D hydrogels, researchers can identify new models of nerve cell mechanoproperties. For example, LaPlaca et al. developed a new model showing that strain may play a role in cell culture (LaPlaca et al. 2005).",
            "score": 98.41497015953064
        },
        {
            "docid": "295601_24",
            "document": "Dynamic positioning . In the beginning PID controllers were used and today are still used in the simpler DP systems. But modern controllers use a mathematical model of the ship that is based on a hydrodynamic and aerodynamic description concerning some of the ship's characteristics such as mass and drag. Of course, this model is not entirely correct. The ship's position and heading are fed into the system and compared with the prediction made by the model. This difference is used to update the model by using Kalman filtering technique. For this reason, the model also has input from the wind sensors and feedback from the thrusters. This method even allows not having input from any PRS for some time, depending on the quality of the model and the weather. This process is known as dead reckoning.",
            "score": 73.80717444419861
        },
        {
            "docid": "16708680_5",
            "document": "Adaptive Modeler . To avoid overfitting (or curve-fitting) to historical data -and unlike many other techniques used in trading software such as optimizing of trading rules by repeated backtesting, genetic algorithms and neural networks- Adaptive Modeler does not optimize trading rules on historical data. Instead, its models evolve incrementally over the available price data so that agents experience every price change only once (as in the real world). Also, there is no difference in the processing of historical and new price data. Therefore, there is no specific reason to expect that a model's back-tested historical performance is better than its future performance (unlike when trading rules have been optimized on historical data). The historical results can therefore be considered more meaningful than results demonstrated by techniques based on optimization.",
            "score": 56.808013677597046
        },
        {
            "docid": "9107843_14",
            "document": "SelTrac . SelTrac is offered in two ways: 1. A complete integrated solution in which movement authority and interlocking are integrated within wayside zone controllers; this reduces equipment and potential interfacing issues. Integrating the management of the interlocking with train location information, as communicated through the CBTC system, allows faster response times, more tightly controlled movements, and easier expandability and adaptability. Interfaces within the zone controller are more easily designed than those between subsystems. The integrated system knows the position of each train to a high degree of accuracy. It can control the behavior of the train at all times and, in response to changing conditions, can modify the behavior to ensure safety of the system while offering maximum service.It can adapt its algorithms to take advantage of individual train behavior, and change parameters to ensure optimum use of resources, such as platform availability and traction power. The Limit of Movement Authority setting logic has a high impact on the end system performance, i.e. managing the interlocking in an integrated manner. Interlocking and switch control logic is optimized using position reports of communicating trains. The integrated design includes:",
            "score": 76.81428718566895
        },
        {
            "docid": "33244792_4",
            "document": "Non-spiking neuron . There are an abundance of neurons that propagate signals via action potentials and the mechanics of this particular kind of transmission is well understood. Spiking neurons exhibit action potentials as a result of a neuron characteristic known as membrane potential. Through studying these complex spiking networks in animals, a neuron that did not exhibit characteristic spiking behavior was discovered. These neurons use a graded potential to transmit data as they lack the membrane potential that spiking neurons possess. This method of transmission has a huge effect on the fidelity, strength, and lifetime of the signal. Non-spiking neurons were identified as a special kind of interneuron and function as an intermediary point of process for sensory-motor systems. Animals have become substantial models for understanding more about non-spiking neural networks and the role they play in an animal\u2019s ability to process information and its overall function. Animal models indicate that the interneurons modulate directional and posture coordinating behaviors. Crustaceans and arthropods such as the crawfish have created many opportunities to learn about the modulatory role that these neurons have in addition to their potential to be modulated regardless of their lack of exhibiting spiking behavior. Most of the known information about nonspiking neurons is derived from animal models. Studies focus on neuromuscular junctions and modulation of abdominal motor cells. Modulatory interneurons are neurons that are physically situated next to muscle fibers and innervate the nerve fibers which allow for some orienting movement. These modulatory interneurons are usually nonspiking neurons. Advances in studying nonspiking neurons included determining new delineations among the different types of interneurons. These discoveries were due to the usage of methods such as protein receptor silencing. Studies have been done on the non-spiking neuron qualities in animals of specific non-spiking neural networks that have a corollary in humans, e.g. retina amacrine cell of the eye.",
            "score": 74.02986884117126
        },
        {
            "docid": "39314537_8",
            "document": "Biomimetic architecture . Biomimetic architecture uses nature as a model, measure and mentor to solve problems in architecture. It is not the same as biomorphic architecture, which uses natural existing elements as sources of inspiration for aesthetic components of form. Instead, biomimetic architecture looks to nature as a model to imitate or take inspiration from natural designs and processes and applies it to the man-made. It uses nature as a measure meaning biomimicry uses an ecological standard to judge the efficiency of human innovations. Nature as a mentor means that biomimicry does not try to exploit nature by extracting material goods from it, but values nature as something humans can learn from.  Architectural innovations that are responsive to architecture do not have to resemble a plant or an animal. Where form is intrinsic to an organism\u2019s function, then a building modeled on a life form\u2019s processes may end up looking like the organism too. Architecture can emulate natural forms, functions and processes. Though a contemporary concept in a technological age, biomimicry does not entail the incorporation of complex technology in architecture. In response to prior architectural movements biomimetic architecture strives to move towards radical increases in resource efficiency, work in a closed loop model rather than linear (work in a closed cycle that does not need a constant intake of resources to function), and rely on solar energy instead of fossil fuels.  The design approach can either work from design to nature or from nature to design. Design to nature means identifying a design problem and finding a parallel problem in nature for a solution. An example of this is the DaimlerChrysler bionic car that looked to the boxfish to build an aerodynamic body. The nature to design method is a solution-driven biologically inspired design. Designers start with a specific biological solution in mind and apply it to design. An example of this is Sto\u2019s Lotusan paint, which is self-cleaning, an idea presented by the lotus flower, which emerges clean from swampy waters.",
            "score": 56.72743105888367
        },
        {
            "docid": "1326926_27",
            "document": "Array processing . While the spectral-based methods presented in previous section are computationally attractive, they do not always yield sufficient accuracy. In particular, for the cases when we have highly correlated signals, the performance of spectral-based methods may be insufficient. An alternative is to more fully exploit the underlying data model, leading to so-called parametric array processing methods. The cost of using such methods to increase the efficiency is that the algorithms typically require a multidimensional search to find the estimates. The most common used model based approach in signal processing is the maximum likelihood (ML) technique. This method requires a statistical framework for the data generation process. When applying the ML technique to the array processing problem, two main methods have been considered depending on the signal data model assumption. According to the Stochastic ML, the signals are modeled as Gaussian random processes. On the other hand, in the Deterministic ML the signals are considered as unknown, deterministic quantities that need to be estimated in conjunction with the direction of arrival.",
            "score": 69.55969715118408
        },
        {
            "docid": "12781902_6",
            "document": "User modeling . Though the first method is a good way to quickly collect main data it lacks the ability to automatically adapt to shifts in users' interests. It depends on the users' readiness to give information and it is unlikely that they are going to edit their answers once the registration process is finished. Therefore, there is a high likelihood that the user models are not up to date. However, this first method allows the users to have full control over the collected data about them. It is in their decision which information they are willing to provide. This possibility is missing in the second method. Adaptive changes in a system that learns users' preferences and needs only by interpreting their behavior might appear a bit opaque to the users, because they cannot fully understand and reconstruct why the system behaves the way it does. Moreover, the system is forced to collect a certain amount of data before it is able to predict the users' needs with the required accuracy. Therefore, it takes a certain learning time before a user can benefit from adaptive changes. However, afterwards these automatically adjusted user models allow a quite accurate adaptivity of the system. The hybrid approach tries to combine the advantages of both methods. Through collecting data by directly asking its users it gathers a first stock of information which can be used for adaptive changes. By learning from the users' interactions it can adjust the user models and reach more accuracy. Yet, the designer of the system has to decide, which of these information should have which amount of influence and what to do with learned data that contradicts some of the information given by a user.",
            "score": 46.65360176563263
        },
        {
            "docid": "1800660_4",
            "document": "Event-driven process chain . Businesses use event-driven process chain diagrams to lay out business process workflows, originally in conjunction with SAP R/3 modeling, but now more widely. It is used by many companies for modeling, analyzing, and redesigning business processes. The event-driven process chain method was developed within the framework of Architecture of Integrated Information Systems (ARIS). As such it forms the core technique for modeling in ARIS, which serves to link the different views in the so-called control view. To quote from a 2006 publication on event-driven process chains:",
            "score": 67.758540391922
        },
        {
            "docid": "1674342_14",
            "document": "Functional software architecture . These methodologies/techniques and methods are all more or less suited in modeling the enterprise and its underlying processes. So, which of them are suited for the further development of information technology systems that are needed for effective and efficient (re)designed processes? More important, why using a time consuming enterprise methodology when information and software engineers can\u2019t or won\u2019t use the unclear results in the development of efficiency enabling IT systems? Before we can give the answers to these questions some short descriptions of the listed methods above are given.",
            "score": 53.56432580947876
        },
        {
            "docid": "44031786_5",
            "document": "Classical control theory . To overcome the limitations of the open-loop controller, classical control theory introduces feedback. A closed-loop controller uses feedback to control states or outputs of a dynamical system. Its name comes from the information path in the system: process inputs (e.g., voltage applied to an electric motor) have an effect on the process outputs (e.g., speed or torque of the motor), which is measured with sensors and processed by the controller; the result (the control signal) is \"fed back\" as input to the process, closing the loop.",
            "score": 73.44414520263672
        },
        {
            "docid": "7039_20",
            "document": "Control theory . To overcome the limitations of the open-loop controller, control theory introduces feedback. A closed-loop controller uses feedback to control states or outputs of a dynamical system. Its name comes from the information path in the system: process inputs (e.g., voltage applied to an electric motor) have an effect on the process outputs (e.g., speed or torque of the motor), which is measured with sensors and processed by the controller; the result (the control signal) is \"fed back\" as input to the process, closing the loop.",
            "score": 73.44414520263672
        },
        {
            "docid": "17808050_2",
            "document": "Accommodation index . The accommodation index is a metric used in the neurosciences for describing spike train data. Many methods of experimental neuroscience, such as voltage clamp recordings, give their output in the form of measured voltages of individual neurons. Generally, the only important element of these voltage traces is the occurrence of spikes in the voltage, representing action potentials. It is often useful to be able to describe the data in terms of the spike timings, for instance we wish to optimize a compartmental model towards observed behaviour, then metrics such as this would be used as error functions. Various metrics are used to do this, such as spike rate, average interspike interval and, the accommodation index.",
            "score": 67.85259485244751
        },
        {
            "docid": "984692_9",
            "document": "Computational sociology . By the late 1960s and early 1970s, social scientists used increasingly available computing technology to perform macro-simulations of control and feedback processes in organizations, industries, cities, and global populations. These models used differential equations to predict population distributions as holistic functions of other systematic factors such as inventory control, urban traffic, migration, and disease transmission. Although simulations of social systems received substantial attention in the mid-1970s after the Club of Rome published reports predicting global environmental catastrophe based upon the predictions of global economy simulations, the inflammatory conclusions also temporarily discredited the nascent field by demonstrating the extent to which results of the models are highly sensitive to the specific quantitative assumptions (backed by little evidence, in the case of the Club of Rome) made about the model's parameters. As a result of increasing skepticism about employing computational tools to make predictions about macro-level social and economic behavior, social scientists turned their attention toward micro-simulation models to make forecasts and study policy effects by modeling aggregate changes in state of individual-level entities rather than the changes in distribution at the population level. However, these micro-simulation models did not permit individuals to interact or adapt and were not intended for basic theoretical research.",
            "score": 69.09300589561462
        },
        {
            "docid": "2020708_12",
            "document": "Adaptive control . Usually these methods adapt the controllers to both the process statics and dynamics. In special cases the adaptation can be limited to the static behavior alone, leading to adaptive control based on characteristic curves for the steady-states or to extremum value control, optimizing the steady state. Hence, there are several ways to apply adaptive control algorithms.",
            "score": 55.076621532440186
        },
        {
            "docid": "623686_55",
            "document": "Brain\u2013computer interface . A further parameter is the method of feedback used and this is shown in studies of P300 signals. Patterns of P300 waves are generated involuntarily (stimulus-feedback) when people see something they recognize and may allow BCIs to decode categories of thoughts without training patients first. By contrast, the biofeedback methods described above require learning to control brainwaves so the resulting brain activity can be detected.",
            "score": 87.43618440628052
        },
        {
            "docid": "16928506_9",
            "document": "Visual servoing . Visual servo systems, also called servoing, have been around since the early 1980s  , although the term visual servo itself was only coined in 1987. Visual Servoing is, in essence, a method for robot control where the sensor used is a camera (visual sensor).  Servoing consists primarily of two techniques, one involves using information from the image to directly control the degrees of freedom (DOF) of the robot, thus referred to as Image Based Visual Servoing (IBVS). While the other involves the geometric interpretation of the information extracted from the camera, such as estimating the pose of the target and parameters of the camera (assuming some basic model of the target is known). Other servoing classifications exist based on the variations in each component of a servoing system  e.g. the location of the camera, the two kinds are eye-in-hand and hand\u2013eye configurations.  Based on the control loop, the two kinds are end-point-open-loop and end-point-closed-loop. Based on whether the control is applied to the joints (or DOF) directly or as a position command to a robot controller the two types are direct servoing and dynamic look-and-move. Being one of the earliest works  the authors proposed a hierarchical visual servo scheme applied to image-based servoing. The technique relies on the assumption that a good set of features can be extracted from the object of interest (e.g. edges, corners and centroids) and used as a partial model along with global models of the scene and robot. The control strategy is applied to a simulation of a two and three DOF robot arm.",
            "score": 92.64909505844116
        }
    ],
    "r": [
        {
            "docid": "15502859_11",
            "document": "EICASLAB . EICASLAB includes the following tools and features to support the control algorithm design: The Automatic Algorithm Generation tool, starting from the \u201cplant simplified model\u201d and from the \"control required performance\" generates the control algorithm. On the basis of the plant design data, the applied control design methodology allows design of controllers with guaranteed performance without requiring any tuning in field in spite of the unavoidable uncertainty which always exists between any mathematical model built on the basis of plant design data and the plant actual performance (for fundamentals on control in presence of uncertainty see ). The designer can choose among three control basic schemes and for each one he has the option of selecting control algorithms at different level of complexity.  In synthesis, the automatically generated control is performed by the resultant of three actions: The plant's state observer task may be extended to estimate and predict the disturbance acting on the plant. The plant disturbance prediction and compensation is an original control feature, which allows significant reduction of control error.  Model Parameter Identification is a tool which allows the identification of the most appropriate values of the simplified model parameters from recorded experimental data or simulated trials performed by using the \u201cplant fine model\u201d. The parameter's \"true\" value does not exist: the model is an approximated description of the plant and then, the parameter's \"best\" value depends on the cost function adopted to evaluate the difference between model and plant. The identification method estimates the best values of the simplified model parameters from the point of view of the closed loop control design. Control Parameter Optimization is a tool which performs control parameter tuning in simulated environment. The optimization is performed numerically over a predefined simulated trial, that is for a given mission (host command sequence and disturbance acting on the plant and any other potential event related to the plant performance) and for a given functional cost associated to the plant control performance.",
            "score": 118.92438507080078
        },
        {
            "docid": "3581220_6",
            "document": "Single-unit recording . Single-unit recordings have provided tools to explore the brain and apply this knowledge to current technologies. Cognitive scientists have used single-unit recordings in the brains of animals and humans to study behaviors and functions. Electrodes can also be inserted into the brain of epileptic patients to determine the position of epileptic foci. More recently, single-unit recordings have been used in brain machine interfaces (BMI). BMIs record brain signals and decode an intended response, which then controls the movement of an external device (such as a computer cursor or prosthetic limb).",
            "score": 117.4583969116211
        },
        {
            "docid": "2860457_12",
            "document": "Neural ensemble . After the techniques of multielectrode recordings were introduced, the task of real-time decoding of information from large neuronal ensembles became feasible. If, as Georgopoulos showed, just a few primary motor neurons could accurately predict hand motion in two planes, reconstruction of the movement of an entire limb should be possible with enough simultaneous recordings. In parallel, with the introduction of an enormous Neuroscience boost from DARPA, several lab groups used millions of dollars to make brain-machine interfaces. Of these groups, two were successful in experiments showing that animals could control external interfaces with models based on their neural activity, and that once control was shifted from the hand to the brain-model, animals could learn to control it better. These two groups are led by John Donoghue and Miguel Nicolelis, and both are involved in towards human trials with their methods.",
            "score": 106.29177856445312
        },
        {
            "docid": "33246145_7",
            "document": "Neural decoding . In order to build a model of neural spike data, one must both understand how information is originally stored in the brain and how this information is used at a later point in time. This neural coding and decoding loop is a symbiotic relationship and the crux of the brain's learning algorithm. Furthermore, the processes that underlie neural decoding and encoding are very tightly coupled and may lead to varying levels of representative ability.",
            "score": 105.73319244384766
        },
        {
            "docid": "3581220_22",
            "document": "Single-unit recording . Brain-machine interfaces (BMIs) have been developed within the last 20 years. By recording single unit potentials, these devices can decode signals through a computer and output this signal for control of an external device such as a computer cursor or prosthetic limb. BMIs have the potential to restore function in patients with paralysis or neurological disease. This technology has potential to reach a wide variety of patients but is not yet available clinically due to lack of reliability in recording signals over time. The primary hypothesis regarding this failure is that the chronic inflammatory response around the electrode causes neurodegeneration that reduces the number of neurons it is able to record from (Nicolelis, 2001). In 2004, the BrainGate pilot clinical trial was initiated to \"test the safety and feasibility of a neural interface system based on an intracortical 100-electrode silicon recording array\". This initiative has been successful in advancement of BCIs and in 2011, published data showing long term computer control in a patient with tetraplegia (Simeral, 2011).",
            "score": 105.1715087890625
        },
        {
            "docid": "33246145_6",
            "document": "Neural decoding . Implicit about the decoding hypothesis is the assumption that neural spiking in the brain somehow represents stimuli in the external world. The decoding of neural data would be impossible if the neurons were firing randomly: nothing would be represented. This process of decoding neural data forms a loop with neural encoding. First, the organism must be able to perceive a set of stimuli in the world \u2013 say a picture of a hat. Seeing the stimuli must result in some internal learning: the encoding stage. After varying the range of stimuli that is presented to the observer, we expect the neurons to adapt to the statistical properties of the signals, encoding those that occur most frequently: the efficient-coding hypothesis. Now neural decoding is the process of taking these statistical consistencies, a statistical model of the world, and reproducing the stimuli. This may map to the process of thinking and acting, which in turn guide what stimuli we receive, and thus, completing the loop.",
            "score": 103.26628875732422
        },
        {
            "docid": "2567511_12",
            "document": "Neural engineering . Neuromechanics is the coupling of neurobiology, biomechanics, sensation and perception, and robotics (Edwards 2010). Researchers are using advanced techniques and models to study the mechanical properties of neural tissues and their effects on the tissues' ability to withstand and generate force and movements as well as their vulnerability to traumatic loading (Laplaca & Prado 2010). This area of research focuses on translating the transformations of information among the neuromuscular and skeletal systems to develop functions and governing rules relating to operation and organization of these systems (Nishikawa et al. 2007). Neuromechanics can be simulated by connecting computational models of neural circuits to models of animal bodies situated in virtual physical worlds (Edwards 2010). Experimental analysis of biomechanics including the kinematics and dynamics of movements, the process and patterns of motor and sensory feedback during movement processes, and the circuit and synaptic organization of the brain responsible for motor control are all currently being researched to understand the complexity of animal movement. Dr. Michelle LaPlaca's lab at Georgia Institute of Technology is involved in the study of mechanical stretch of cell cultures, shear deformation of planar cell cultures, and shear deformation of 3D cell containing matrices. Understanding of these processes is followed by development of functioning models capable of characterizing these systems under closed loop conditions with specially defined parameters. The study of neuromechanics is aimed at improving treatments for physiological health problems which includes optimization of prostheses design, restoration of movement post injury, and design and control of mobile robots. By studying structures in 3D hydrogels, researchers can identify new models of nerve cell mechanoproperties. For example, LaPlaca et al. developed a new model showing that strain may play a role in cell culture (LaPlaca et al. 2005).",
            "score": 98.41497039794922
        },
        {
            "docid": "623686_33",
            "document": "Brain\u2013computer interface . The use of BMIs has also led to a deeper understanding of neural networks and the central nervous system. Research has shown that despite the inclination of neuroscientists to believe that neurons have the most effect when working together, single neurons can be conditioned through the use of BMIs to fire at a pattern that allows primates to control motor outputs. The use of BMIs has led to development of the single neuron insufficiency principle which states that even with a well tuned firing rate single neurons can only carry a narrow amount of information and therefore the highest level of accuracy is achieved by recording firings of the collective ensemble. Other principles discovered with the use of BMIs include the neuronal multitasking principle, the neuronal mass principle, the neural degeneracy principle, and the plasticity principle.",
            "score": 98.27900695800781
        },
        {
            "docid": "21084005_8",
            "document": "Control flow diagram . The figure presents an example of a performance seeking control flow diagram of the algorithm. The control law consists of estimation, modeling, and optimization processes. In the Kalman filter estimator, the inputs, outputs, and residuals were recorded. At the compact propulsion system modeling stage, all the estimated inlet and engine parameters were recorded.",
            "score": 98.12763214111328
        },
        {
            "docid": "2020708_3",
            "document": "Adaptive control . The foundation of adaptive control is parameter estimation, which is branch of system identification. Common methods of estimation include recursive least squares and gradient descent. Both of these methods provide update laws which are used to modify estimates in real time (i.e., as the system operates). Lyapunov stability is used to derive these update laws and show convergence criterion (typically persistent excitation, relaxation to this condition are studied in Concurrent Learning adaptive control). Projection (mathematics) and normalization are commonly used to improve the robustness of estimation algorithms.",
            "score": 97.62214660644531
        },
        {
            "docid": "33246145_26",
            "document": "Neural decoding . While it is possible to take the firing rates of these modeled neurons, and transform them into the probabilistic and mathematical frameworks described above, agent-based models provide the ability to observe the behavior of the entire population of modeled neurons. Researchers can circumvent the limitations implicit with lab-based recording techniques. Because this approach does rely on modeling biological systems, error arises in the assumptions made by the researcher and in the data used in parameter estimation.",
            "score": 97.35340881347656
        },
        {
            "docid": "2860457_16",
            "document": "Neural ensemble . Demonstrations of decoding of neuronal ensemble activity can be subdivided into two major classes: off-line decoding and on-line (real time) decoding. In the off-line decoding, investigators apply different algorithms to previously recorded data. Time considerations are usually not an issue in these studies: a sophisticated decoding algorithm can run for many hours on a computer cluster to reconstruct a 10-minute data piece. On-line algorithms decode (and, importantly, predict) behavioral parameters in real time. Moreover, the subject may receive a feedback about the results of decoding \u2014 the so-called closed-loop mode as opposed to the open-loop mode in which the subject does not receive any feedback.",
            "score": 97.25984191894531
        },
        {
            "docid": "19498707_6",
            "document": "Linear-nonlinear-Poisson cascade model . The parameters of the LNP model consist of the linear filters formula_14 and the nonlinearity formula_4. The estimation problem (also known as the problem of \"neural characterization\") is the problem of determining these parameters from data consisting of a time-varying stimulus and the set of observed spike times. Techniques for estimating the LNP model parameters include:",
            "score": 97.06421661376953
        },
        {
            "docid": "52020537_9",
            "document": "Ji-Feng Zhang . He investigated the capability issues of robust and adaptive control in dealing with uncertainty, and revealed that to capture the intrinsic limitations of adaptive control, it is necessary to use sup-types of transient and persistent performance, rather than limsup-types which reflect only asymptotic behavior of a system. This indicates that intimate interaction and inherent conflict between identification and control result in a certain performance lower bound which does not approach the nominal performance even when the system varies very slowly. For nonlinear hybrid stochastic systems with unknown jump-Markov parameters, he with co-authors used the Wonham nonlinear filter to estimate the unknown parameters and presented an estimation error bound, which is a basic tool and plays an important role in performance analysis of adaptive control of nonlinear hybrid stochastic systems. He also attacked a series of hard problems related on global output-feedback control of nonlinear stochastic systems with inverse dynamics, including practical output-feedback risk-sensitive control, robust adaptive stabilization, small-gain theorem of general nonlinear stochastic systems. Different from the existing literature, the systems considered in his work are so complicated that renders any control design for them is much difficult. He developed a set of predominant methods and obtained many innovative results. The work represents an accomplishment for both the field of stochastic nonlinear stabilization and the backstepping method.",
            "score": 96.59178924560547
        },
        {
            "docid": "33246145_12",
            "document": "Neural decoding . When decoding neural data, arrival times of each spike formula_1, and the probability of seeing a certain stimulus, formula_2 may be the extent of the available data. The prior distribution formula_2 defines an ensemble of signals, and represents the likelihood of seeing a stimulus in the world based on previous experience. The spike times may also be drawn from a distribution formula_4; however, what we want to know is the probability distribution over a set of stimuli given a series of spike trains formula_5, which is called the response-conditional ensemble. What remains is the characterization of the neural code by translating stimuli into spikes, formula_6; the traditional approach to calculating this probability distribution has been to fix the stimulus and examine the responses of the neuron. Combining everything using Bayes' Rule results in the simplified probabilistic characterization of neural decoding: formula_7. An area of active research consists of finding better ways of representing and determining formula_8. The following are some such examples.",
            "score": 95.00929260253906
        },
        {
            "docid": "38364055_3",
            "document": "Linear parameter-varying control . In designing feedback controllers for dynamical systems a variety of modern, multivariable controllers are used. In general, these controllers are often designed at various operating points using linearized models of the system dynamics and are scheduled as a function of a parameter or parameters for operation at intermediate conditions. It is an approach for the control of non-linear systems that uses a family of linear controllers, each of which provides satisfactory control for a different operating point of the system. One or more observable variables, called the scheduling variables, are used to determine the current operating region of the system and to enable the appropriate linear controller. For example, in case of aircraft control, a set of controllers are designed at different gridded locations of corresponding parameters such as AoA, Mach, dynamic pressure, CG etc. In brief, gain scheduling is a control design approach that constructs a nonlinear controller for a nonlinear plant by patching together a collection of linear controllers. These linear controllers are blended in real-time via switching or interpolation.",
            "score": 94.66127014160156
        },
        {
            "docid": "3282143_5",
            "document": "Robust control . Informally, a controller designed for a particular set of parameters is said to be robust if it also works well under a different set of assumptions. High-gain feedback is a simple example of a robust control method; with sufficiently high gain, the effect of any parameter variations will be negligible. From the closed loop transfer function perspective, high open loop gain leads to substantial disturbance rejection in the face of system parameter uncertainty. Other examples on robust control include sliding mode and terminal sliding mode control.",
            "score": 94.55211639404297
        },
        {
            "docid": "1514392_4",
            "document": "Training, test, and validation sets . The model is initially fit on a training dataset, that is a set of examples used to fit the parameters (e.g. weights of connections between neurons in artificial neural networks) of the model. The model (e.g. a neural net or a naive Bayes classifier) is trained on the training dataset using a supervised learning method (e.g. gradient descent or stochastic gradient descent). In practice, the training dataset often consist of pairs of an input vector and the corresponding \"answer\" vector or scalar, which is commonly denoted as the \"target\". The current model is run with the training dataset and produces a result, which is then compared with the \"target\", for each input vector in the training dataset. Based on the result of the comparison and the specific learning algorithm being used, the parameters of the model are adjusted. The model fitting can include both variable selection and parameter estimation.",
            "score": 93.5735855102539
        },
        {
            "docid": "13891813_9",
            "document": "Emery N. Brown . Brown later focused his statistics research on developing signal processing algorithms and statistical methods for neuronal data analysis. He developed a state-space point process (SSPP) paradigm to study how neural systems maintain dynamic representations of information. For the analysis of neural spiking activity and binary behavioral tasks represented as multivariate or univariate point processes (0-1 events that occur in continuous time), his research produced analogs of the Kalman filter, Kalman smoothing, sequential Monte Carlo algorithms, and combined state and parameter estimation algorithms commonly applied to continuous-valued time series observations.",
            "score": 93.23023223876953
        },
        {
            "docid": "16928506_9",
            "document": "Visual servoing . Visual servo systems, also called servoing, have been around since the early 1980s  , although the term visual servo itself was only coined in 1987. Visual Servoing is, in essence, a method for robot control where the sensor used is a camera (visual sensor).  Servoing consists primarily of two techniques, one involves using information from the image to directly control the degrees of freedom (DOF) of the robot, thus referred to as Image Based Visual Servoing (IBVS). While the other involves the geometric interpretation of the information extracted from the camera, such as estimating the pose of the target and parameters of the camera (assuming some basic model of the target is known). Other servoing classifications exist based on the variations in each component of a servoing system  e.g. the location of the camera, the two kinds are eye-in-hand and hand\u2013eye configurations.  Based on the control loop, the two kinds are end-point-open-loop and end-point-closed-loop. Based on whether the control is applied to the joints (or DOF) directly or as a position command to a robot controller the two types are direct servoing and dynamic look-and-move. Being one of the earliest works  the authors proposed a hierarchical visual servo scheme applied to image-based servoing. The technique relies on the assumption that a good set of features can be extracted from the object of interest (e.g. edges, corners and centroids) and used as a partial model along with global models of the scene and robot. The control strategy is applied to a simulation of a two and three DOF robot arm.",
            "score": 92.64909362792969
        },
        {
            "docid": "1513195_7",
            "document": "Fast Kalman filter . Reliable operational Kalman filtering requires continuous fusion of data in real-time. Its optimality depends essentially on the use of exact variances and covariances between all measurements and the estimated state and calibration parameters. This large error covariance matrix is obtained by matrix inversion from the respective system of Normal Equations. Its coefficient matrix is usually sparse and the exact solution of all the estimated parameters can be computed by using the HWB (and FKF) method. The optimal solution may also be obtained by Gauss elimination using other sparse-matrix techniques or some iterative methods based e.g. on Variational Calculus. However, these latter methods may solve the large matrix of all the error variances and covariances only approximately and the data fusion would not be performed in a strictly optimal fashion. Consequently, the long-term stability of Kalman filtering becomes uncertain even if Kalman's observability and controllability conditions were permanently satisfied.",
            "score": 92.49452209472656
        },
        {
            "docid": "37052063_4",
            "document": "Moving horizon estimation . The application of MHE is generally to estimate measured or unmeasured states of dynamical systems. Initial conditions and parameters within a model are adjusted by MHE to align measured and predicted values. MHE is based on a finite horizon optimization of a process model and measurements. At time the current process state is sampled and a minimizing strategy is computed (via a numerical minimization algorithm) for a relatively short time horizon in the past: formula_1. Specifically, an online or on-the-fly calculation is used to explore state trajectories that find (via the solution of Euler\u2013Lagrange equations) a objective-minimizing strategy until time formula_2. Only the last step of the estimation strategy is used, then the process state is sampled again and the calculations are repeated starting from the time-shifted states, yielding a new state path and predicted parameters. The estimation horizon keeps being shifted forward and for this reason the technique is called moving horizon estimation. Although this approach is not optimal, in practice it has given very good results when compared with the Kalman filter and other estimation strategies.",
            "score": 92.39572143554688
        },
        {
            "docid": "2020708_6",
            "document": "Adaptive control . Direct methods are ones wherein the estimated parameters are those directly used in the adaptive controller. In contrast, indirect methods are those in which the estimated parameters are used to calculate required controller parameters. Hybrid methods rely on both estimation of parameters and direct modification of the control law.",
            "score": 92.38455200195312
        },
        {
            "docid": "33882236_6",
            "document": "Adaptive collaborative control . Adaptive collaborative control is most accurately modeled as a closed loop feedback control system. Closed loop feedback control describes the event where the outputs of a system from an input are used to influence the present or future behavior of the system. The feedback control model is governed by a set of equations that are used to predict the future state of the simuland and regulate its behavior. These equations \u2013 in conjunction with principles of control theory \u2013 are used to evolve physical operations of the simuland to include, but not limited to: dialogue, path planning, motion, monitoring, and lifting objects over time. Many times, these equations are modeled as nonlinear partial differential equations over a continuous time domain.  Due to their complexity, powerful computers are necessary to implement these models. A consequence of using computers to simulate these models is that continuous systems cannot be fully calculated. Instead, numerical solutions, such as the Runge-Kutta methods, are utilized to approximate these continuous models.  These equations are initialized from the response of one or more sources and rates of changes and outputs are calculated. These rates of changes predict the states of the simuland a short time in the future. The time increment for this prediction is called a time step. These new states are applied to the model to determine the new rates of changes and observational data. This behavior is continued until the desired number of iterations is completed. In the event a future state violates or comes within a tolerance of the violation the simuland will confer with its human counterpart seeking advice on how to proceed from that point. The outputs, or observational data, are used by the human operators to determine what they believe is the best course of action for the simuland. Their commands are fed with the input into the control system and assessed regarding its effectiveness in resolving the issues. If the human commands are determined to be valuable, the simuland will adjust its control input to what the human suggested. If the human\u2019s commands are determined to be unbeneficial, malicious, or non-existent, the model will seek its own correction approach.",
            "score": 92.22833251953125
        },
        {
            "docid": "47152350_27",
            "document": "Human performance modeling . Numerical typing is an important perceptual-motor task whose performance may vary with different pacing, finger strategies and urgency of situations. Queuing network-model human processor (QN-MHP), a computational architecture, allows performance of perceptual-motor tasks to be modelled mathematically. The current study enhanced QN-MHP with a top-down control mechanism, a close-loop movement control and a finger-related motor control mechanism to account for task interference, endpoint reduction, and force deficit, respectively. The model also incorporated neuromotor noise theory to quantify endpoint variability in typing. The model predictions of typing speed and accuracy were validated with Lin and Wu\u2019s (2011) experimental results. The resultant root-meansquared errors were 3.68% with a correlation of 95.55% for response time, and 35.10% with a correlation of 96.52% for typing accuracy. The model can be applied to provide optimal speech rates for voice synthesis and keyboard designs in different numerical typing situations.",
            "score": 91.89268493652344
        },
        {
            "docid": "2843988_7",
            "document": "Motor control . Most movements that are carried out during day-to-day activity are formed using a continual process of accessing sensory information and using it to more accurately continue the motion. This type of motor control is called feedback control, as it relies on sensory feedback to control movements. Feedback control is a situated form of motor control, relying on sensory information about performance and specific sensory input from the environment in which the movement is carried out. This sensory input, while processed, does not necessarily cause conscious awareness of the action. \"Closed loop control\" is a feedback based mechanism of motor control, where any act on the environment creates some sort of change that affects future performance through feedback. Closed loop motor control is best suited to continuously controlled actions, but does not work quickly enough for ballistic actions. Ballistic actions are actions that continue to the end without thinking about it, even when they no longer are appropriate.Because feedback control relies on sensory information, it is as slow as sensory processing. These movements are subject to a speed/accuracy trade-off, because sensory processing is being used to control the movement, the faster the movement is carried out, the less accurate it becomes.",
            "score": 91.64334869384766
        },
        {
            "docid": "180855_3",
            "document": "Kalman filter . The Kalman filter has numerous applications in technology. A common application is for guidance, navigation, and control of vehicles, particularly aircraft and spacecraft. Furthermore, the Kalman filter is a widely applied concept in time series analysis used in fields such as signal processing and econometrics. Kalman filters also are one of the main topics in the field of robotic motion planning and control, and they are sometimes included in trajectory optimization. The Kalman filter also works for modeling the central nervous system's control of movement. Due to the time delay between issuing motor commands and receiving sensory feedback, use of the Kalman filter supports a realistic model for making estimates of the current state of the motor system and issuing updated commands.",
            "score": 91.34111022949219
        },
        {
            "docid": "66256_65",
            "document": "PID controller . This method was developed in 1953 and is based on a first order + time delay model. Similar to the Ziegler-Nichols method, a set of tuning parameters were developed to yield a closed-loop response with a decay ratio of 1/4. Arguably the biggest problem with these parameters is that a small change in the process parameters could potentially cause a closed-loop system to become unstable.",
            "score": 90.73878479003906
        },
        {
            "docid": "13070117_9",
            "document": "Spectral density estimation . Many other techniques for spectral estimation have been developed to mitigate the disadvantages of the basic periodogram. These techniques can generally be divided into \"non-parametric\" and \"parametric\" methods. The non-parametric approaches explicitly estimate the covariance or the spectrum of the process without assuming that the process has any particular structure. Some of the most common estimators in use for basic applications (e.g. Welch's method) are non-parametric estimators closely related to the periodogram. By contrast, the parametric approaches assume that the underlying stationary stochastic process has a certain structure that can be described using a small number of parameters (for example, using an auto-regressive or moving average model). In these approaches, the task is to estimate the parameters of the model that describes the stochastic process.",
            "score": 90.53214263916016
        },
        {
            "docid": "56275884_11",
            "document": "Data-driven control system . IFT is a model-free technique for the direct iterative optimization of the parameters of a fixed-order controller; such parameters can be successively updated using information coming from standard (closed-loop) system operation.",
            "score": 90.41923522949219
        },
        {
            "docid": "140806_4",
            "document": "Maximum likelihood estimation . From the point of view of Bayesian inference, MLE is a special case of maximum a posteriori estimation (MAP) that assumes a uniform prior distribution of the parameters. In frequentist inference, MLE is one of several methods to get estimates of parameters without using prior distributions. Priors are avoided by not making probability statements about the parameters, but only about their estimates, whose properties are fully defined by the observations and the statistical model. The method of maximum likelihood is based on the likelihood function, formula_1. We are given a statistical model, i.e. a family of distributions formula_2, where formula_3 denotes the (possibly multi-dimensional) parameter for the model. The method of maximum likelihood finds the values of the model parameter, formula_3, that maximize the likelihood function, formula_1. Intuitively, this selects the parameter values that make the data most probable.",
            "score": 90.0543441772461
        },
        {
            "docid": "172071_2",
            "document": "Adaptive filter . An adaptive filter is a system with a linear filter that has a transfer function controlled by variable parameters and a means to adjust those parameters according to an optimization algorithm. Because of the complexity of the optimization algorithms, almost all adaptive filters are digital filters. Adaptive filters are required for some applications because some parameters of the desired processing operation (for instance, the locations of reflective surfaces in a reverberant space) are not known in advance or are changing. The closed loop adaptive filter uses feedback in the form of an error signal to refine its transfer function.",
            "score": 89.6881332397461
        }
    ]
}