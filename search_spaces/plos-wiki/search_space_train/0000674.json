{
    "q": [
        {
            "docid": "1872854_31",
            "document": "Biochemical cascade . In the post-genomic age, high-throughput sequencing and gene/protein profiling techniques have transformed biological research by enabling comprehensive monitoring of a biological system, yielding a list of differentially expressed genes or proteins, which is useful in identifying genes that may have roles in a given phenomenon or phenotype. With DNA microarrays and genome-wide gene engineering, it is possible to screen global gene expression profiles to contribute a wealth of genomic data to the public domain. With RNA interference, it is possible to distill the inferences contained in the experimental literature and primary databases into knowledge bases that consist of annotated representations of biological pathways. In this case, individual genes and proteins are known to be involved in biological processes, components, or structures, as well as how and where gene products interact with each other. Pathway-oriented approaches for analyzing microarray data, by grouping long lists of individual genes, proteins, and/or other biological molecules according to the pathways they are involved in into smaller sets of related genes or proteins, which reduces the complexity, have proven useful for connecting genomic data to specific biological processes and systems. Identifying active pathways that differ between two conditions can have more explanatory power than a simple list of different genes or proteins. In addition, a large number of pathway analytic methods exploit pathway knowledge in public repositories such as Gene Ontology (GO) or Kyoto Encyclopedia of Genes and Genomes (KEGG), rather than inferring pathways from molecular measurements. Furthermore, different research focuses have given the word \"pathway\" different meanings. For example, 'pathway' can denote a metabolic pathway involving a sequence of enzyme-catalyzed reactions of small molecules, or a signaling pathway involving a set of protein phosphorylation reactions and gene regulation events. Therefore, the term \"pathway analysis\" has a very broad application. For instance, it can refer to the analysis physical interaction networks (e.g., protein\u2013protein interactions), kinetic simulation of pathways, and steady-state pathway analysis (e.g., flux-balance analysis), as well as its usage in the inference of pathways from expression and sequence data. Several functional enrichment analysis tools and algorithms have been developed to enhance data interpretation. The existing knowledge base\u2013driven pathway analysis methods in each generation have been summarized in recent literature.",
            "score": 58.196805238723755
        },
        {
            "docid": "29467449_4",
            "document": "Protein function prediction . While techniques such as microarray analysis, RNA interference, and the yeast two-hybrid system can be used to experimentally demonstrate the function of a protein, advances in sequencing technologies have made the rate at which proteins can be experimentally characterized much slower than the rate at which new sequences become available. Thus, the annotation of new sequences is mostly by \"prediction\" through computational methods, as these types of annotation can often be done quickly and for many genes or proteins at once. The first such methods inferred function based on homologous proteins with known functions (homology-based function prediction). The development of context-based and structure based methods have expanded what information can be predicted, and a combination of methods can now be used to get a picture of complete cellular pathways based on sequence data. The importance and prevalence of computational prediction of gene function is underlined by an analysis of 'evidence codes' used by the GO database: as of 2010, 98% of annotations were listed under the code IEA (inferred from electronic annotation) while only 0.6% were based on experimental evidence.",
            "score": 38.39998936653137
        },
        {
            "docid": "1872854_27",
            "document": "Biochemical cascade . For either DDO or KDO pathway construction, the first step is to mine pertinent information from relevant information sources about the entities and interactions. The information retrieved is assembled using appropriate formats, information standards, and pathway building tools to obtain a pathway prototype. The pathway is further refined to include context-specific annotations such as species, cell/tissue type, or disease type. The pathway can then be verified by the domain experts and updated by the curators based on appropriate feedback. Recent attempts to improve knowledge integration have led to refined classifications of cellular entities, such as GO, and to the assembly of structured knowledge repositories. Data repositories, which contain information regarding sequence data, metabolism, signaling, reactions, and interactions are a major source of information for pathway building. A few useful databases are described in the following table. Legend: Y \u2013 Yes, N \u2013 No; BIND \u2013 Biomolecular Interaction Network Database, DIP \u2013 Database of Interacting Proteins, GNPV \u2013 Genome Network Platform Viewer, HPRD = Human Protein Reference Database, MINT \u2013 Molecular Interaction database, MIPS \u2013 Munich Information center for Protein Sequences, UNIHI \u2013 Unified Human Interactome, OPHID \u2013 Online Predicted Human Interaction Database, EcoCyc \u2013 Encyclopaedia of E. Coli Genes and Metabolism, MetaCyc \u2013 aMetabolic Pathway database, KEGG \u2013 Kyoto Encyclopedia of Genes and Genomes, PANTHER \u2013 Protein Analysis Through Evolutionary Relationship database, STKE \u2013 Signal Transduction Knowledge Environment, PID \u2013 The Pathway Interaction Database, BioPP \u2013 Biological Pathway Publisher. A comprehensive list of resources can be found at http://www.pathguide.org.",
            "score": 43.37778878211975
        },
        {
            "docid": "16784415_2",
            "document": "Protein structure database . In biology, a protein structure database is a database that is modeled around the various experimentally determined protein structures. The aim of most protein structure databases is to organize and annotate the protein structures, providing the biological community access to the experimental data in a useful way. Data included in protein structure databases often includes three-dimensional coordinates as well as experimental information, such as unit cell dimensions and angles for x-ray crystallography determined structures. Though most instances, in this case either proteins or a specific structure determinations of a protein, also contain sequence information and some databases even provide means for performing sequence based queries, the primary attribute of a structure database is structural information, whereas sequence databases focus on sequence information, and contain no structural information for the majority of entries. Protein structure databases are critical for many efforts in computational biology such as structure based drug design, both in developing the computational methods used and in providing a large experimental dataset used by some methods to provide insights about the function of a protein.",
            "score": 46.17632699012756
        },
        {
            "docid": "969126_27",
            "document": "Protein structure . Protein structure database is a database that is modeled around the various experimentally determined protein structures. The aim of most protein structure databases is to organize and annotate the protein structures, providing the biological community access to the experimental data in a useful way. Data included in protein structure databases often includes 3D coordinates as well as experimental information, such as unit cell dimensions and angles for x-ray crystallography determined structures. Though most instances, in this case either proteins or a specific structure determinations of a protein, also contain sequence information and some databases even provide means for performing sequence based queries, the primary attribute of a structure database is structural information, whereas sequence databases focus on sequence information, and contain no structural information for the majority of entries. Protein structure databases are critical for many efforts in computational biology such as structure based drug design, both in developing the computational methods used and in providing a large experimental dataset used by some methods to provide insights about the function of a protein.",
            "score": 44.9044406414032
        },
        {
            "docid": "4214_4",
            "document": "Bioinformatics . Bioinformatics has become an important part of many areas of biology. In experimental molecular biology, bioinformatics techniques such as image and signal processing allow extraction of useful results from large amounts of raw data. In the field of genetics and genomics, it aids in sequencing and annotating genomes and their observed mutations. It plays a role in the text mining of biological literature and the development of biological and gene ontologies to organize and query biological data. It also plays a role in the analysis of gene and protein expression and regulation. Bioinformatics tools aid in the comparison of genetic and genomic data and more generally in the understanding of evolutionary aspects of molecular biology. At a more integrative level, it helps analyze and catalogue the biological pathways and networks that are an important part of systems biology. In structural biology, it aids in the simulation and modeling of DNA, RNA, proteins as well as biomolecular interactions.",
            "score": 52.75089740753174
        },
        {
            "docid": "7766542_9",
            "document": "Microarray analysis techniques . Commercial systems for gene network analysis such as Ingenuity and Pathway studio create visual representations of differentially expressed genes based on current scientific literature. Non-commercial tools such as FunRich, GenMAPP and Moksiskaan also aid in organizing and visualizing gene network data procured from one or several microarray experiments. A wide variety of microarray analysis tools are available through Bioconductor written in the R programming language. The frequently cited SAM module and other microarray tools are available through Stanford University. Another set is available from Harvard and MIT. Specialized software tools for statistical analysis to determine the extent of over- or under-expression of a gene in a microarray experiment relative to a reference state have also been developed to aid in identifying genes or gene sets associated with particular phenotypes. One such method of analysis, known as Gene Set Enrichment Analysis (GSEA), uses a Kolmogorov-Smirnov-style statistic to identify groups of genes that are regulated together. This third-party statistics package offers the user information on the genes or gene sets of interest, including links to entries in databases such as NCBI's GenBank and curated databases such as Biocarta and Gene Ontology. Protein complex enrichment analysis tool (COMPLEAT) provides similar enrichment analysis at the level of protein complexes. The tool can identify the dynamic protein complex regulation under different condition or time points. Related system, PAINT and SCOPE performs a statistical analysis on gene promoter regions, identifying over and under representation of previously identified transcription factor response elements. Another statistical analysis tool is Rank Sum Statistics for Gene Set Collections (RssGsc), which uses rank sum probability distribution functions to find gene sets that explain experimental data. A further approach is contextual meta-analysis, i.e. finding out how a gene cluster responds to a variety of experimental contexts. Genevestigator is a public tool to perform contextual meta-analysis across contexts such as anatomical parts, stages of development, and response to diseases, chemicals, stresses, and neoplasms.",
            "score": 51.93752408027649
        },
        {
            "docid": "44248347_6",
            "document": "Gene Disease Database . At different stages of any gene disease project, molecular biologists need to choose, even after careful statistical data analysis, which genes or proteins to investigate further experimentally and which to leave out because of limited resources. Computational methods that integrate complex, heterogeneous data sets, such as expression data, sequence information, functional annotation and the biomedical literature, allow prioritizing genes for future study in a more informed way. Such methods can substantially increase the yield of downstream studies and are becoming invaluable to researchers. So one of the main concerns in biological and biomedical research is to recognise the underlying mechanisms behind this intricate genetic phenotypes. Great effort has been spent on finding the genes related to diseases",
            "score": 72.22569131851196
        },
        {
            "docid": "3878_53",
            "document": "Biostatistics . The development of biological databases enables storage and management of biological data with the possibility of ensuring access for users around the world. They are useful for researchers depositing data, retrieve information and files (raw or processed) originated from other experiments or indexing scientific articles, as PubMed. Another possibility is search for the desired term (a gene, a protein, a disease, an organism, and so on) and check all results related to this search. There are databases dedicated to SNPs (dbSNP), the knowledge on genes characterization and their pathways (KEGG) and the description of gene function classifying it by cellular component, molecular function and biological process (Gene Ontology). In addition to databases that contain specific molecular information, there are others that are ample in the sense that they store information about an organism or group of organisms. As an example of a database directed towards just one organism, but that contains lots of data about it, is the \"Arabidopsis thaliana\" genetic and molecular database - TAIR. Phytozome, in turn, stores the assemblies and annotation files of dozen of plant genomes, also containing visualization and analysis tools. Moreover, there is an interconnection between some databases in the information exchange/sharing and a major initiative was the International Nucleotide Sequence Database Collaboration (INSDC) which relates data from DDBJ, EMBL-EBI, and NCBI.",
            "score": 62.801570773124695
        },
        {
            "docid": "20400528_3",
            "document": "Ontology engineering . Automated processing of information not interpretable by software agents can be improved by adding rich semantics to the corresponding resources, such as video files. One of the approaches for the formal conceptualization of represented knowledge domains is the use of machine-interpretable ontologies, which provide structured data in, or based on, RDF, RDFS, and OWL. Ontology engineering is the design and creation of such ontologies, which can contain more than just the list of terms (controlled vocabulary); they contain terminological, assertional, and relational axioms to define concepts (classes), individuals, and roles (properties) (TBox, ABox, and RBox, respectively). Ontology engineering is a relatively new field of study concerning the ontology development process, the ontology life cycle, the methods and methodologies for building ontologies, and the tool suites and languages that support them. A common way to provide the logical underpinning of ontologies is to formalize the axioms with description logics, which can then be translated to any serialization of RDF, such as RDF/XML or Turtle. Beyond the description logic axioms, ontologies might also contain SWRL rules. The concept definitions can be mapped to any kind of resource or resource segment in RDF, such as images, videos, and regions of interest, to annotate objects, persons, etc., and interlink them with related resources across knowledge bases, ontologies, and LOD datasets. This information, based on human experience and knowledge, is valuable for reasoners for the automated interpretation of sophisticated and ambiguous contents, such as the visual content of multimedia resources. Application areas of ontology-based reasoning include, but are not limited to, information retrieval, automated scene interpretation, and knowledge discovery.",
            "score": 35.00007963180542
        },
        {
            "docid": "34567821_4",
            "document": "Critical Assessment of Function Annotation . The genome of an organism may consist of hundreds to tens of thousands of genes, which encode for hundreds of thousands of different protein sequences. Due to the relatively low cost of genome sequencing, determining gene and protein sequences is fast and inexpensive. Thousands of species have been sequenced so far, yet many of the proteins are not well characterized. The process of experimentally determining the role of a protein in the cell, is an expensive and time consuming task. Further, even when functional assays are performed they are unlikely to provide complete insight into protein function. Therefore it has become important to use computational tools in order to functionally annotate proteins. There are several computational methods of protein function prediction that can infer protein function using a variety of biological and evolutionary data, but there is significant room for improvement. Accurate prediction of protein function can have longstanding implications on biomedical and pharmaceutical research.",
            "score": 51.38419818878174
        },
        {
            "docid": "10795520_22",
            "document": "National Centre for Text Mining . Big mechanisms are large, explanatory models of complicated systems in which interactions have important causal effects. Whilst the collection of big data is increasingly automated, the creation of big mechanisms remains a largely human effort, which is becoming made increasingly challenging, according to the fragmentation and distribution of knowledge. The ability to automate the construction of big mechanisms could have a major impact on scientific research. As one of a number of different projects that make up the big mechanism programme, funded by DARPA, the aim is to assemble an overarching big mechanism from the literature and prior experiments and to utilise this for the probabilistic interpretation of new patient panomics data. We will integrate machine reading of the cancer literature with probabilistic reasoning across cancer claims using specially-designed ontologies, computational modeling of cancer mechanisms (pathways), automated hypothesis generation to extend knowledge of the mechanisms and a 'Robot Scientist' that performs experiments to test the hypotheses. A repetitive cycle of text mining, modelling, experimental testing, and worldview updating is intended to lead to increased knowledge about cancer mechanisms.",
            "score": 41.872520446777344
        },
        {
            "docid": "23634_44",
            "document": "Protein . The development of such tools has been driven by the large amount of genomic and proteomic data available for a variety of organisms, including the human genome. It is simply impossible to study all proteins experimentally, hence only a few are subjected to laboratory experiments while computational tools are used to extrapolate to similar proteins. Such homologous proteins can be efficiently identified in distantly related organisms by sequence alignment. Genome and gene sequences can be searched by a variety of tools for certain properties. Sequence profiling tools can find restriction enzyme sites, open reading frames in nucleotide sequences, and predict secondary structures. Phylogenetic trees can be constructed and evolutionary hypotheses developed using special software like ClustalW regarding the ancestry of modern organisms and the genes they express. The field of bioinformatics is now indispensable for the analysis of genes and proteins.",
            "score": 46.31638503074646
        },
        {
            "docid": "31002435_2",
            "document": "Knowledge extraction . Knowledge extraction is the creation of knowledge from structured (relational databases, XML) and unstructured (text, documents, images) sources. The resulting knowledge needs to be in a machine-readable and machine-interpretable format and must represent knowledge in a manner that facilitates inferencing. Although it is methodically similar to information extraction (NLP) and ETL (data warehouse), the main criteria is that the extraction result goes beyond the creation of structured information or the transformation into a relational schema. It requires either the reuse of existing formal knowledge (reusing identifiers or ontologies) or the generation of a schema based on the source data.",
            "score": 22.365496158599854
        },
        {
            "docid": "393024_3",
            "document": "Biological database . Biological databases can be broadly classified into sequence, structure and functional databases. Nucleic acid and protein sequences are stored in sequence databases and structure databases store solved structures of RNA and proteins. Functional databases provide information on the physiological role of gene products, for example enzyme activities, mutant phenotypes, or biological pathways. Model Organism Databases are functional databases that provide species-specific data. Databases are important tools in assisting scientists to analyze and explain a host of biological phenomena from the structure of biomolecules and their interaction, to the whole metabolism of organisms and to understanding the evolution of species. This knowledge helps facilitate the fight against diseases, assists in the development of medications, predicting certain genetic diseases and in discovering basic relationships among species in the history of life.",
            "score": 38.698578119277954
        },
        {
            "docid": "8758178_2",
            "document": "FlyBase . FlyBase is an online bioinformatics database and the primary repository of genetic and molecular data for the insect family \"Drosophilidae\". For the most extensively studied species and model organism, \"Drosophila melanogaster\", a wide range of data are presented in different formats. Information in FlyBase originates from a variety of sources ranging from large-scale genome projects to the primary research literature. These data types include mutant phenotypes, molecular characterization of mutant alleles and other deviations, cytological maps, wild-type expression patterns, anatomical images, transgenic constructs and insertions, sequence-level gene models and molecular classification of gene product functions. Query tools allow navigation of FlyBase through DNA or protein sequence, by gene or mutant name, or through terms from the several ontologies used to capture functional, phenotypic, and anatomical data. The database offers several different query tools in order to provide efficient access to the data available and facilitate the discovery of significant relationships within the database. Links between FlyBase and external databases, such as BDGP or modENCODE, provide opportunity for further exploration into other model organism databases and other resources of biological and molecular information. The FlyBase project is carried out by a consortium of \"Drosophila\" researchers and computer scientists at Harvard University and Indiana University in the United States, and University of Cambridge in the United Kingdom.",
            "score": 42.445550203323364
        },
        {
            "docid": "44248347_28",
            "document": "Gene Disease Database . The response of bioinformatics to new experimental techniques brings a new perspective into the analysis of the experimental data, as demonstrated by the advances in the analysis of information from gene disease databases and other technologies. It is expected that this trend will continue with novel approaches to respond to new techniques, such as next-generation sequencing technologies. For instance, the availability of large numbers of individual human genomes will promote the development of computational analyses of rare variants, including the statistical mining of their relations to lifestyles, drug interactions and other factors. Biomedical research will also be driven by our ability to efficiently mine the large body of existing and continuously generated biomedical data. Text-mining techniques, in particular, when combined with other molecular data, can provide information about gene mutations and interactions and will become crucial to stay ahead of the exponential growth of data generated in biomedical research. Another field that is benefiting from the advances in mining and integration of molecular, clinical and drug analysis is pharmacogenomics. \"In silico\" studies of the relationships between human variations and their effect on diseases will be key to the development of personalized medicine. In summary, Gene Disease Databases have already transformed the search for disease genes and has the potential to become a crucial component of other areas of medical research.",
            "score": 71.9622175693512
        },
        {
            "docid": "31182986_3",
            "document": "ArrayTrack . ArrayTrack is composed of three major components: Study Database, Tools, and Libraries, which primarily handle data management, analysis, and interpretation, respectively. Each of these components can be directly accessed from the other two, e.g., analysis Tools can be used directly on experimental data stored in the Study Database, and significant genes discovered from the results can be queried in the Libraries to view additional annotations and associated proteins, pathways, Gene Ontology terms, etc.",
            "score": 37.04860854148865
        },
        {
            "docid": "24258072_20",
            "document": "Short linear motif . More recently computational methods have been developed that can identify new Short Linear Motifs de novo. Interactome-based tools rely on identifying a set of proteins that are likely to share a common function, such as binding the same protein or being cleaved by the same peptidase. Two examples of such software are DILIMOT and SLiMFinder. Anchor and \u03b1-MoRF-Pred use physicochemical properties to search for motif-like peptides in disordered regions (termed MoRFs, among others). ANCHOR identifies stretches of intrinsically disordered regions that cannot form favorable intrachain interactions to fold without additional stabilising energy contributed by a globular interaction partner. \u03b1-MoRF-Pred uses the inherent propensity of many SLiM to undergo a disorder to order transition upon binding to discover \u03b1-helical forming stretches within disordered regions.  MoRFPred and MoRFchibi SYSTEM are SVM based predictors which utilize multiple features including local sequence physicochemical properties, long stretches of disordered regions and conservation in their predictions. SLiMPred is neural network\u2013based method for the de novo discovery of SLiMs from the protein sequence. Information about the structural context of the motif (predicted secondary structure, structural motifs, solvent accessibility, and disorder) are used during the predictive process. Importantly, no previous knowledge about the protein (i.e., no evolutionary or experimental information) is required.",
            "score": 25.30708885192871
        },
        {
            "docid": "41489324_8",
            "document": "Intelligent maintenance system . Watchdog Agent is the IMS Center's collection of tools and techniques for Prognostics and Health Management (PHM). For developing intelligent maintenance systems, a major step is to properly select such tools for data analysis and facilitate a decision support system. This toolbox can be customized and reconfigured for nearly any application \u2013 from products and assets, to complex systems, processes or manufacturing lines. The Watchdog Agent\u00ae includes four categories of analytical tools that can be used to assess and predict the performance or degradation of machines and processes by extracting the performance-related features from inputs such as measured sensor data, controller signals, and also expert knowledge, etc. Prediction results are then used for maintenance decision-making infrastructure operations IMS Brochure. The Watchdog Agent\u00ae conducts its performance assessment based on the readings from multiple sensors that measure the critical properties of a process, machine or component. Since the degradation process alters the sensor measurements, the Watchdog Agent\u00ae is capable of quantitatively describing such changes in the sensor readings by using analytical tools and extracting those changes by means of appropriate tools and methods.",
            "score": 23.901638388633728
        },
        {
            "docid": "37783228_41",
            "document": "William A. Haseltine . At the time, the idea that newly isolated human genes of unknown function could prove useful for drug development was widely criticized. Haseltine's experience with HIV taught him that knowledge of the genome without prior knowledge of function was useful and had led to the discovery of new and useful drug targets and new and effective drugs. Haseltine argued that if one new human gene were discovered, the techniques of modern biology would allow its natural function and potential medical use identified. If that were true for one gene, then why not all the human genes? New tools had been developed that allowed what had been tedious hard work of gene isolation and characterization to be replaced by highly automated instruments and the data regarding the structure, tissue and cell location and the results of functional tests to be stored and easily accessed using advanced computer technologies. He summarized these views with the statement \"Genomics is not necessarily Genetics\". It was not until the Human Genome Science approach was validated by its own work and the work of its partners that it was ultimately adopted by the scientific community. Today, the approach pioneered by Human Genome Sciences is one of the principal tools used today for the discovery and characterization of novel human genes and as well as the genes of other species.",
            "score": 45.23595201969147
        },
        {
            "docid": "8721272_5",
            "document": "PHI-base . Each entry in PHI-base is curated by domain experts and supported by strong experimental evidence (gene disruption experiments) as well as literature references in which the experiments are described. Each gene in PHI-base is presented with its nucleotide and deduced amino acid sequence as well as a detailed structured description of the predicted protein's function during the host infection process. To facilitate data interoperability, genes are annotated using controlled vocabularies (Gene Ontology terms, EC Numbers, etc.), and links to other external data sources such as UniProt, EMBL and the NCBI taxonomy services.",
            "score": 33.41987442970276
        },
        {
            "docid": "31312165_3",
            "document": "LocDB . Proteins are the fundamental functional components of cells. They are responsible for transforming genetic information into physical reality. These macromolecules mediate gene regulation, enzymatic catalysis, cellular metabolism, DNA replication, and transport of nutrients, recognition, and transmission of signals. The interpretation of this wealth of data to elucidate protein function in post-genomic era is a fundamental challenge. To date, even for the most well-studied organisms such as yeast, about one-fourth of the proteins remain uncharacterized. A major obstacle in experimentally determining protein function is that the studies require enormous resources. Hence, the gap between the amount of sequences deposited in databases and the experimental characterization of the corresponding proteins is ever-growing. Bioinformatics plays a central role in bridging this sequence-function gap through the development of tools for faster and more effective prediction of protein function. This repository effectively fills the gap between experimental annotations and predictions and provides a bigger and more reliable dataset for the testing of new prediction methods.",
            "score": 46.98537850379944
        },
        {
            "docid": "1872854_26",
            "document": "Biochemical cascade . Pathway building has been performed by individual groups studying a network of interest (e.g., immune signaling pathway) as well as by large bioinformatics consortia (e.g., the Reactome Project) and commercial entities (e.g., Ingenuity Systems). Pathway building is the process of identifying and integrating the entities, interactions, and associated annotations, and populating the knowledge base. Pathway construction can have either a data-driven objective (DDO) or a knowledge-driven objective (KDO). Data-driven pathway construction is used to generate relationship information of genes or proteins identified in a specific experiment such as a microarray study. Knowledge-driven pathway construction entails development of a detailed pathway knowledge\u3000base for particular domains of interest, such as a cell type, disease, or system. The curation process of a biological pathway entails identifying and structuring content, mining information manually and/or computationally, and assembling a knowledgebase using appropriate software tools. A schematic illustrating the major steps involved in the data-driven and knowledge-driven construction processes.",
            "score": 32.4871621131897
        },
        {
            "docid": "53970843_16",
            "document": "Machine learning in bioinformatics . This technique has been applied to the search for novel drug targets, as this task requires the examination of information stored in biological databases and journals. Annotations of proteins in protein databases often do not reflect the complete known set of knowledge of each protein, so additional information must be extracted from biomedical literature. Machine learning has been applied to automatic annotation of the function of genes and proteins, determination of the subcellular localization of a protein, analysis of DNA-expression arrays, large-scale protein interaction analysis, and molecule interaction analysis.",
            "score": 51.34495234489441
        },
        {
            "docid": "30352058_5",
            "document": "Autophagy database . Human autophagy database is a product of the Luxembourg Institute of Health (LIH). LIH has several branches throughout Luxembourg available for Biomonitoring, Infection and Immunity, Health administration, Oncology, Sports Medicine, and Biobank. Each of these departments aims to support the LIH mission statement, which is \"to generate and translate research knowledge into clinical applications with an impact on the future challenges of health care and personalised medicine.\" It accomplishes these aims by working toward the forefront of biomedical research, by maintaining excellence, integrity, and passion in their work and offered tools. The Laboratory of Experimental Cancer Research of LIH helped to establish one of these tools, that tool being the database known as Human autophagy database. Human autophagy database (HADb) is another available autophagy resource. Unlike Autophagy database, Human autophagy database only compares those proteins found in humans. HADb is the first human-only autophagy database, where researchers may find an updated listing of directly and indirectly related autophagic proteins, given no consistent database previously available to compensate for a huge expansion in autophagy research. HADb does not only provide information on the gene of interest, but also aims to evolve into a database which can be used to analyze the gene of interest. For this purpose, HADb was made as complete as possible in terms of autophagy-related proteins, though newly discovered proteins and genes may be submitted by different users to the \"Submission\" section. The information provided by Human autophagy database can be used further in bioinformatics applications.",
            "score": 49.98057663440704
        },
        {
            "docid": "33890874_12",
            "document": "De novo transcriptome assembly . Functional annotation of the assembled transcripts allows for insight into the particular molecular functions, cellular components, and biological processes in which the putative proteins are involved. Blast2GO (B2G) enables Gene Ontology based data mining to annotate sequence data for which no GO annotation is available yet. It is a research tool often employed in functional genomics research on non-model species. It works by blasting assembled contigs against a non-redundant protein database (at NCBI), then annotating them based on sequence similarity. GOanna is another GO annotation program specific for animal and agricultural plant gene products that works in a similar fashion. It is part of the AgBase database of curated, publicly accessible suite of computational tools for GO annotation and analysis. Following annotation, KEGG (Kyoto Encyclopedia of Genes and Genomes) enables visualization of metabolic pathways and molecular interaction networks captured in the transcriptome.",
            "score": 36.90097093582153
        },
        {
            "docid": "2871644_3",
            "document": "Information Hyperlinked over Proteins . The concept underlying iHOP is that by using genes and proteins as hyperlinks between sentences and abstracts, the information in PubMed can be converted into one navigable resource. Navigating across interrelated sentences within this network rather than the use of conventional keyword searches allows for stepwise and controlled acquisition of information. Moreover, this literature network can be superimposed upon experimental interaction data to facilitate the simultaneous analysis of novel and existing knowledge. As of September 2014, the network presented in iHOP contains 28.4 million sentences and 110,000 genes from over 2,700 organisms, including the model organisms \"Homo sapiens\", \"Mus musculus\", \"Drosophila melanogaster\", \"Caenorhabditis elegans\", \"Danio rerio\", \"Arabidopsis thaliana\", \"Saccharomyces cerevisiae\" and \"Escherichia coli\".",
            "score": 33.129128217697144
        },
        {
            "docid": "1872854_32",
            "document": "Biochemical cascade . A program package MatchMiner was used to scan HUGO names for cloned genes of interest are scanned, then are input into GoMiner, which leveraged the GO to identify the biological processes, functions and components represented in the gene profile. Also, Database for Annotation, Visualization, and Integrated Discovery (DAVID) and KEGG database can be used for the analysis of microarray expression data and the analysis of each GO biological process (P), cellular component (C), and molecular function (F) ontology. In addition, DAVID tools can be used to analyze the roles of genes in metabolic pathways and show the biological relationships between genes or gene-products and may represent metabolic pathways. These two databases also provide bioinformatics tools online to combine specific biochemical information on a certain organism and facilitate the interpretation of biological meanings for experimental data. By using a combined approach of Microarray-Bioinformatic technologies, a potential metabolic mechanism contributing to colorectal cancer (CRC) has been demonstrated Several environmental factors may be involved in a series of points along the genetic pathway to CRC. These include genes associated with bile acid metabolism, glycolysis metabolism and fatty acid metabolism pathways, supporting a hypothesis that some metabolic alternations observed in colon carcinoma may occur in the development of CRC.",
            "score": 58.72581708431244
        },
        {
            "docid": "42253_3",
            "document": "Data mining . An interdisciplinary subfield of computer science, it is an essential process \u2014 wherein intelligent methods are applied to extract data patterns \u2014 the overall goal of which is to extract information from a data set, and transform it into an understandable structure for further use. Aside from the raw analysis step, it involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating. Data mining is the analysis step of the \"knowledge discovery in databases\" process, or KDD.",
            "score": 20.444655060768127
        },
        {
            "docid": "14246962_2",
            "document": "Database of Interacting Proteins . The Database of Interacting Proteins (DIP) is a biological database which catalogs experimentally determined interactions between proteins. It combines information from a variety of sources to create a single, consistent set of protein\u2013protein interactions. The data stored within DIP have been curated, both manually, by expert curators, and automatically, using computational approaches that utilize the knowledge about the protein\u2013protein interaction networks extracted from the most reliable, core subset of the DIP data. The database was initially released in 2002. As of 2014, DIP is curated by the research group of David Eisenberg at UCLA.",
            "score": 31.535869121551514
        },
        {
            "docid": "4635854_2",
            "document": "Hypothetical protein . In biochemistry, a hypothetical protein is a protein whose existence has been predicted, but for which there is a lack of experimental evidence that it is expressed in vivo. Sequencing of several genomes has resulted in numerous predicted open reading frames to which functions cannot be readily assigned. These proteins, either orphan or conserved hypothetical proteins, make up ~ 20% to 40% of proteins encoded in each newly sequenced genome. Even when there is enough evidence that the product of the gene is expressed, by techniques such as microarray and mass-spectrometry, it is difficult to assign a function to it given its lack of identity to protein sequences with annotated biochemical function. Nowadays, most protein sequences are inferred from computational analysis of genomic DNA sequence. Hypothetical proteins are created by gene prediction software during genome analysis. When the bioinformatic tool used for the gene identification finds a large open reading frame without a characterised homologue in the protein database, it returns \"hypothetical protein\" as an annotation remark.",
            "score": 44.0491259098053
        }
    ],
    "r": [
        {
            "docid": "44248347_6",
            "document": "Gene Disease Database . At different stages of any gene disease project, molecular biologists need to choose, even after careful statistical data analysis, which genes or proteins to investigate further experimentally and which to leave out because of limited resources. Computational methods that integrate complex, heterogeneous data sets, such as expression data, sequence information, functional annotation and the biomedical literature, allow prioritizing genes for future study in a more informed way. Such methods can substantially increase the yield of downstream studies and are becoming invaluable to researchers. So one of the main concerns in biological and biomedical research is to recognise the underlying mechanisms behind this intricate genetic phenotypes. Great effort has been spent on finding the genes related to diseases",
            "score": 72.22569274902344
        },
        {
            "docid": "44248347_28",
            "document": "Gene Disease Database . The response of bioinformatics to new experimental techniques brings a new perspective into the analysis of the experimental data, as demonstrated by the advances in the analysis of information from gene disease databases and other technologies. It is expected that this trend will continue with novel approaches to respond to new techniques, such as next-generation sequencing technologies. For instance, the availability of large numbers of individual human genomes will promote the development of computational analyses of rare variants, including the statistical mining of their relations to lifestyles, drug interactions and other factors. Biomedical research will also be driven by our ability to efficiently mine the large body of existing and continuously generated biomedical data. Text-mining techniques, in particular, when combined with other molecular data, can provide information about gene mutations and interactions and will become crucial to stay ahead of the exponential growth of data generated in biomedical research. Another field that is benefiting from the advances in mining and integration of molecular, clinical and drug analysis is pharmacogenomics. \"In silico\" studies of the relationships between human variations and their effect on diseases will be key to the development of personalized medicine. In summary, Gene Disease Databases have already transformed the search for disease genes and has the potential to become a crucial component of other areas of medical research.",
            "score": 71.96221923828125
        },
        {
            "docid": "3878_53",
            "document": "Biostatistics . The development of biological databases enables storage and management of biological data with the possibility of ensuring access for users around the world. They are useful for researchers depositing data, retrieve information and files (raw or processed) originated from other experiments or indexing scientific articles, as PubMed. Another possibility is search for the desired term (a gene, a protein, a disease, an organism, and so on) and check all results related to this search. There are databases dedicated to SNPs (dbSNP), the knowledge on genes characterization and their pathways (KEGG) and the description of gene function classifying it by cellular component, molecular function and biological process (Gene Ontology). In addition to databases that contain specific molecular information, there are others that are ample in the sense that they store information about an organism or group of organisms. As an example of a database directed towards just one organism, but that contains lots of data about it, is the \"Arabidopsis thaliana\" genetic and molecular database - TAIR. Phytozome, in turn, stores the assemblies and annotation files of dozen of plant genomes, also containing visualization and analysis tools. Moreover, there is an interconnection between some databases in the information exchange/sharing and a major initiative was the International Nucleotide Sequence Database Collaboration (INSDC) which relates data from DDBJ, EMBL-EBI, and NCBI.",
            "score": 62.801570892333984
        },
        {
            "docid": "50968824_2",
            "document": "Model organism databases . Model organism databases (MODs) are biological databases, or knowledgebases, dedicated to the provision of in-depth biological data for intensively studied model organisms. MODs allow researchers to easily find background information on large sets of genes, plan experiments efficiently, combine their data with existing knowledge, and construct novel hypotheses. They allow users to analyse results and interpret datasets, and the data they generate are increasingly used to describe less well studied species. Where possible, MODs share common approaches to collect and represent biological information. For example, all MODs use the Gene Ontology to describe functions, processes and cellular locations of specific gene products. Projects also exist to enable software sharing for curation, visualization and querying between different MODs. Organismal diversity and varying user requirements however mean that MODs are often required to customize capture, display, and provision of data.",
            "score": 61.491111755371094
        },
        {
            "docid": "4007073_29",
            "document": "Gene expression profiling . Expression profiling provides new information about what genes do under various conditions. Overall, microarray technology produces reliable expression profiles. From this information one can generate new hypotheses about biology or test existing ones. However, the size and complexity of these experiments often results in a wide variety of possible interpretations. In many cases, analyzing expression profiling results takes far more effort than performing the initial experiments.",
            "score": 61.22713851928711
        },
        {
            "docid": "24947798_2",
            "document": "GeneCards . GeneCards is a database of human genes that provides genomic, proteomic, transcriptomic, genetic and functional information on all known and predicted human genes. It is being developed and maintained by the Crown Human Genome Center at the Weizmann Institute of Science. This database aims at providing a quick overview of the current available biomedical information about the searched gene, including the human genes, the encoded proteins, and the relevant diseases. The GeneCards database provides access to free Web resources about more than 7000 all known human genes that integrated from >90 data resources, such as HGNC, Ensembl, and NCBI. The core gene list is based on approved gene symbols published by the HUGO Gene Nomenclature Committee (HGNC). The information are carefully gathered and selected from these databases by the powerful and user-friendly engine. If the search does not return any results, this database will give several suggestions to help users accomplish their searching depended on the type of query, and offer direct links to other databases\u2019 search engine. Over time, the GeneCards database has developed a suite of tools (GeneDecks, GeneLoc, GeneALaCart) that has more specialised capability. Since 1998, the GeneCards database has been widely used by bioinformatics, genomics and medical communities for more than 15 years.",
            "score": 59.905460357666016
        },
        {
            "docid": "1872854_32",
            "document": "Biochemical cascade . A program package MatchMiner was used to scan HUGO names for cloned genes of interest are scanned, then are input into GoMiner, which leveraged the GO to identify the biological processes, functions and components represented in the gene profile. Also, Database for Annotation, Visualization, and Integrated Discovery (DAVID) and KEGG database can be used for the analysis of microarray expression data and the analysis of each GO biological process (P), cellular component (C), and molecular function (F) ontology. In addition, DAVID tools can be used to analyze the roles of genes in metabolic pathways and show the biological relationships between genes or gene-products and may represent metabolic pathways. These two databases also provide bioinformatics tools online to combine specific biochemical information on a certain organism and facilitate the interpretation of biological meanings for experimental data. By using a combined approach of Microarray-Bioinformatic technologies, a potential metabolic mechanism contributing to colorectal cancer (CRC) has been demonstrated Several environmental factors may be involved in a series of points along the genetic pathway to CRC. These include genes associated with bile acid metabolism, glycolysis metabolism and fatty acid metabolism pathways, supporting a hypothesis that some metabolic alternations observed in colon carcinoma may occur in the development of CRC.",
            "score": 58.7258186340332
        },
        {
            "docid": "1872854_31",
            "document": "Biochemical cascade . In the post-genomic age, high-throughput sequencing and gene/protein profiling techniques have transformed biological research by enabling comprehensive monitoring of a biological system, yielding a list of differentially expressed genes or proteins, which is useful in identifying genes that may have roles in a given phenomenon or phenotype. With DNA microarrays and genome-wide gene engineering, it is possible to screen global gene expression profiles to contribute a wealth of genomic data to the public domain. With RNA interference, it is possible to distill the inferences contained in the experimental literature and primary databases into knowledge bases that consist of annotated representations of biological pathways. In this case, individual genes and proteins are known to be involved in biological processes, components, or structures, as well as how and where gene products interact with each other. Pathway-oriented approaches for analyzing microarray data, by grouping long lists of individual genes, proteins, and/or other biological molecules according to the pathways they are involved in into smaller sets of related genes or proteins, which reduces the complexity, have proven useful for connecting genomic data to specific biological processes and systems. Identifying active pathways that differ between two conditions can have more explanatory power than a simple list of different genes or proteins. In addition, a large number of pathway analytic methods exploit pathway knowledge in public repositories such as Gene Ontology (GO) or Kyoto Encyclopedia of Genes and Genomes (KEGG), rather than inferring pathways from molecular measurements. Furthermore, different research focuses have given the word \"pathway\" different meanings. For example, 'pathway' can denote a metabolic pathway involving a sequence of enzyme-catalyzed reactions of small molecules, or a signaling pathway involving a set of protein phosphorylation reactions and gene regulation events. Therefore, the term \"pathway analysis\" has a very broad application. For instance, it can refer to the analysis physical interaction networks (e.g., protein\u2013protein interactions), kinetic simulation of pathways, and steady-state pathway analysis (e.g., flux-balance analysis), as well as its usage in the inference of pathways from expression and sequence data. Several functional enrichment analysis tools and algorithms have been developed to enhance data interpretation. The existing knowledge base\u2013driven pathway analysis methods in each generation have been summarized in recent literature.",
            "score": 58.19680404663086
        },
        {
            "docid": "5219699_34",
            "document": "Human Genome Project . There are also many tangible benefits for biologists. For example, a researcher investigating a certain form of cancer may have narrowed down their search to a particular gene. By visiting the human genome database on the World Wide Web, this researcher can examine what other scientists have written about this gene, including (potentially) the three-dimensional structure of its product, its function(s), its evolutionary relationships to other human genes, or to genes in mice or yeast or fruit flies, possible detrimental mutations, interactions with other genes, body tissues in which this gene is activated, and diseases associated with this gene or other datatypes. Further, deeper understanding of the disease processes at the level of molecular biology may determine new therapeutic procedures. Given the established importance of DNA in molecular biology and its central role in determining the fundamental operation of cellular processes, it is likely that expanded knowledge in this area will facilitate medical advances in numerous areas of clinical interest that may not have been possible without them.",
            "score": 53.79546356201172
        },
        {
            "docid": "4214_4",
            "document": "Bioinformatics . Bioinformatics has become an important part of many areas of biology. In experimental molecular biology, bioinformatics techniques such as image and signal processing allow extraction of useful results from large amounts of raw data. In the field of genetics and genomics, it aids in sequencing and annotating genomes and their observed mutations. It plays a role in the text mining of biological literature and the development of biological and gene ontologies to organize and query biological data. It also plays a role in the analysis of gene and protein expression and regulation. Bioinformatics tools aid in the comparison of genetic and genomic data and more generally in the understanding of evolutionary aspects of molecular biology. At a more integrative level, it helps analyze and catalogue the biological pathways and networks that are an important part of systems biology. In structural biology, it aids in the simulation and modeling of DNA, RNA, proteins as well as biomolecular interactions.",
            "score": 52.75089645385742
        },
        {
            "docid": "7766542_9",
            "document": "Microarray analysis techniques . Commercial systems for gene network analysis such as Ingenuity and Pathway studio create visual representations of differentially expressed genes based on current scientific literature. Non-commercial tools such as FunRich, GenMAPP and Moksiskaan also aid in organizing and visualizing gene network data procured from one or several microarray experiments. A wide variety of microarray analysis tools are available through Bioconductor written in the R programming language. The frequently cited SAM module and other microarray tools are available through Stanford University. Another set is available from Harvard and MIT. Specialized software tools for statistical analysis to determine the extent of over- or under-expression of a gene in a microarray experiment relative to a reference state have also been developed to aid in identifying genes or gene sets associated with particular phenotypes. One such method of analysis, known as Gene Set Enrichment Analysis (GSEA), uses a Kolmogorov-Smirnov-style statistic to identify groups of genes that are regulated together. This third-party statistics package offers the user information on the genes or gene sets of interest, including links to entries in databases such as NCBI's GenBank and curated databases such as Biocarta and Gene Ontology. Protein complex enrichment analysis tool (COMPLEAT) provides similar enrichment analysis at the level of protein complexes. The tool can identify the dynamic protein complex regulation under different condition or time points. Related system, PAINT and SCOPE performs a statistical analysis on gene promoter regions, identifying over and under representation of previously identified transcription factor response elements. Another statistical analysis tool is Rank Sum Statistics for Gene Set Collections (RssGsc), which uses rank sum probability distribution functions to find gene sets that explain experimental data. A further approach is contextual meta-analysis, i.e. finding out how a gene cluster responds to a variety of experimental contexts. Genevestigator is a public tool to perform contextual meta-analysis across contexts such as anatomical parts, stages of development, and response to diseases, chemicals, stresses, and neoplasms.",
            "score": 51.937522888183594
        },
        {
            "docid": "34567821_4",
            "document": "Critical Assessment of Function Annotation . The genome of an organism may consist of hundreds to tens of thousands of genes, which encode for hundreds of thousands of different protein sequences. Due to the relatively low cost of genome sequencing, determining gene and protein sequences is fast and inexpensive. Thousands of species have been sequenced so far, yet many of the proteins are not well characterized. The process of experimentally determining the role of a protein in the cell, is an expensive and time consuming task. Further, even when functional assays are performed they are unlikely to provide complete insight into protein function. Therefore it has become important to use computational tools in order to functionally annotate proteins. There are several computational methods of protein function prediction that can infer protein function using a variety of biological and evolutionary data, but there is significant room for improvement. Accurate prediction of protein function can have longstanding implications on biomedical and pharmaceutical research.",
            "score": 51.38419723510742
        },
        {
            "docid": "53970843_16",
            "document": "Machine learning in bioinformatics . This technique has been applied to the search for novel drug targets, as this task requires the examination of information stored in biological databases and journals. Annotations of proteins in protein databases often do not reflect the complete known set of knowledge of each protein, so additional information must be extracted from biomedical literature. Machine learning has been applied to automatic annotation of the function of genes and proteins, determination of the subcellular localization of a protein, analysis of DNA-expression arrays, large-scale protein interaction analysis, and molecule interaction analysis.",
            "score": 51.34495162963867
        },
        {
            "docid": "7819348_3",
            "document": "Minimum Information Standards . The individual minimum information standards are brought by the communities of cross-disciplinary specialists focused on the problematic of the specific method used in experimental biology. The standards then provide specifications what information about the experiments (metadata) is crucial and important to be reported together with the resultant data to make it comprehensive. The need for this standardization is largely driven by the development of high-throughput experimental methods that provide tremendous amounts of data. The development of minimum information standards of different methods is since 2008 being harmonized by \"Minimum Information about a Biomedical or Biological Investigation\" (MIBBI) project.",
            "score": 51.207237243652344
        },
        {
            "docid": "6051479_9",
            "document": "Niall Shanks . Evolution and Medicine: Shanks made contributions to debates on topics in what may be broadly characterized as evolutionary medicine. It is now generally recognized that the implications evolutionary biology for medicine must be taken into account if modern medical inquiry is to be placed on a sound theoretical foundation. The great population geneticist Theodosius Dobzhansky once remarked that \"nothing in biology makes sense except in the light of evolution.\" This is especially true of modern medicine. Shanks has argued that evolutionary biology is highly important when considering the so-called \"prediction problem\" in biomedical research. It has become common scientific practice in research in toxicology, drug development, pathology and therapeutics to explore human biomedical phenomena by performing experiments on nonhuman animals and then extrapolating the results to humans (animals play many roles in biomedical research, but one important role is as predictors of human biomedical phenomena). Shanks claims that a prediction problem exists due to the fact that humans and their nonhuman models have taken divergent evolutionary trajectories, and are thus not qualitatively identical systems (once compensation has been made for purely quantitative differences in body weight, for example). Put bluntly, the prediction problem boils down to this: if mice are not men writ small, under what conditions do we expect experimental results on mice to have informational value for human biomedical phenomena? Two of Shanks' essays have generated some interest in this puzzle (see \"Evolution and Medicine: The Long Reach of Dr. Darwin\", http://www.peh-med.com/content/2/1/4, written with Rebecca Pyles, Ph.D., and \"Are Animal Models Predictive for Humans\", http://www.peh-med.com/content/4/1/2, written with Ray Greek, MD., and Jean Greek, DVM).",
            "score": 50.886260986328125
        },
        {
            "docid": "44248347_24",
            "document": "Gene Disease Database . DisGeNET is a comprehensive gene-disease association database that integrates associations from several sources that covers different biomedical aspects of diseases. In particular, it is focused on the current knowledge of human genetic diseases including Mendelian, complex and environmental diseases. To assess the concept of modularity of human diseases, this database performs a systematic study of the emergent properties of human gene-disease networks by means of network topology and functional annotation analysis. The results indicate a highly shared genetic origin of human diseases and show that for most diseases, including Mendelian, complex and environmental diseases, functional modules exist. Moreover, a core set of biological pathways is found to be associated with most human diseases. Obtaining similar results when studying clusters of diseases, the findings in this database suggest that related diseases might arise due to dysfunction of common biological processes in the cell. The network analysis of this integrated database points out that data integration is needed to obtain a comprehensive view of the genetic landscape of human diseases and that the genetic origin of complex diseases is much more common than expected.",
            "score": 50.66269302368164
        },
        {
            "docid": "22921_61",
            "document": "Psychology . All researched psychological traits are influenced by both genes and environment, to varying degrees. These two sources of influence are often confounded in observational research of individuals or families. An example is the transmission of depression from a depressed mother to her offspring. Theory may hold that the offspring, by virtue of having a depressed mother in his or her (the offspring's) environment, is at risk for developing depression. However, risk for depression is also influenced to some extent by genes. The mother may both carry genes that contribute to her depression but will also have passed those genes on to her offspring thus increasing the offspring's risk for depression. Genes and environment in this simple transmission model are completely confounded. Experimental and quasi-experimental behavioral genetic research uses genetic methodologies to disentangle this confound and understand the nature and origins of individual differences in behavior. Traditionally this research has been conducted using twin studies and adoption studies, two designs where genetic and environmental influences can be partially un-confounded. More recently, the availability of microarray molecular genetic or genome sequencing technologies allows researchers to measure participant DNA variation directly, and test whether individual genetic variants within genes are associated with psychological traits and psychopathology through methods including genome-wide association studies. One goal of such research is similar to that in positional cloning and its success in Huntington's: once a causal gene is discovered biological research can be conducted to understand how that gene influences the phenotype. One major result of genetic association studies is the general finding that psychological traits and psychopathology, as well as complex medical diseases, are highly polygenic, where a large number (on the order of hundreds to thousands) of genetic variants, each of small effect, contribute to individual differences in the behavioral trait or propensity to the disorder. Active research continues to understand the genetic and environmental bases of behavior and their interaction.",
            "score": 50.24943923950195
        },
        {
            "docid": "20971660_2",
            "document": "One gene\u2013one enzyme hypothesis . The one gene\u2013one enzyme hypothesis is the idea that genes act through the production of enzymes, with each gene responsible for producing a single enzyme that in turn affects a single step in a metabolic pathway. The concept was proposed by George Beadle and Edward Tatum in an influential 1941 paper on genetic mutations in the mold \"Neurospora crassa\", and subsequently was dubbed the \"one gene\u2013one enzyme hypothesis\" by their collaborator Norman Horowitz. In 2004 Norman Horowitz reminisced that \"these experiments founded the science of what Beadle and Tatum called 'biochemical genetics.' In actuality they proved to be the opening gun in what became molecular genetics and all the developments that have followed from that.\" The development of the one gene\u2013one enzyme hypothesis is often considered the first significant result in what came to be called molecular biology. Although it has been extremely influential, the hypothesis was recognized soon after its proposal to be an oversimplification. Even the subsequent reformulation of the \"one gene\u2013one polypeptide\" hypothesis is now considered too simple to describe the relationship between genes and proteins.",
            "score": 50.00830841064453
        },
        {
            "docid": "30352058_5",
            "document": "Autophagy database . Human autophagy database is a product of the Luxembourg Institute of Health (LIH). LIH has several branches throughout Luxembourg available for Biomonitoring, Infection and Immunity, Health administration, Oncology, Sports Medicine, and Biobank. Each of these departments aims to support the LIH mission statement, which is \"to generate and translate research knowledge into clinical applications with an impact on the future challenges of health care and personalised medicine.\" It accomplishes these aims by working toward the forefront of biomedical research, by maintaining excellence, integrity, and passion in their work and offered tools. The Laboratory of Experimental Cancer Research of LIH helped to establish one of these tools, that tool being the database known as Human autophagy database. Human autophagy database (HADb) is another available autophagy resource. Unlike Autophagy database, Human autophagy database only compares those proteins found in humans. HADb is the first human-only autophagy database, where researchers may find an updated listing of directly and indirectly related autophagic proteins, given no consistent database previously available to compensate for a huge expansion in autophagy research. HADb does not only provide information on the gene of interest, but also aims to evolve into a database which can be used to analyze the gene of interest. For this purpose, HADb was made as complete as possible in terms of autophagy-related proteins, though newly discovered proteins and genes may be submitted by different users to the \"Submission\" section. The information provided by Human autophagy database can be used further in bioinformatics applications.",
            "score": 49.9805793762207
        },
        {
            "docid": "579390_3",
            "document": "Gene prediction . In its earliest days, \"gene finding\" was based on painstaking experimentation on living cells and organisms. Statistical analysis of the rates of homologous recombination of several different genes could determine their order on a certain chromosome, and information from many such experiments could be combined to create a genetic map specifying the rough location of known genes relative to each other. Today, with comprehensive genome sequence and powerful computational resources at the disposal of the research community, gene finding has been redefined as a largely computational problem.",
            "score": 49.8130989074707
        },
        {
            "docid": "22228777_2",
            "document": "Interferome . Interferome is an online bioinformatics database of interferon-regulated genes (IRGs). These Interferon Regulated Genes are also known as Interferon Stimulated Genes (ISGs). The database contains information on type I (IFN alpha, beta), type II (IFN gamma) and type III (IFN lambda) regulated genes and is regularly updated. It is used by the interferon and cytokine research community both as an analysis tool and an information resource. Interferons were identified as antiviral proteins more than 50 years ago. However, their involvement in immunomodulation, cell proliferation, inflammation and other homeostatic processes has been since identified. These cytokines are used as therapeutics in many diseases such as chronic viral infections, cancer and multiple sclerosis. These interferons regulate the transcription of approximately 2000 genes in an interferon subtype, dose, cell type and stimulus dependent manner. This database of interferon regulated genes is an attempt at integrating information from high-throughput experiments and molecular biology databases to gain a detailed understanding of interferon biology.",
            "score": 49.669010162353516
        },
        {
            "docid": "41656489_6",
            "document": "Oikopleura dioica . \"Oikopleura dioica\" is used as a model organism, a role for which it has several features to recommend it. It has the typical chordate body plan, it is simple to keep and breed in the laboratory, it produces large numbers of eggs and the generation time is only four days at . The body is also transparent, making it easier to study, and at hatching only consist of 550 cells. In the Sars International Centre for Marine Molecular Biology, inbred lines have been developed using repeated matings of closely related individuals. The molecular base of a number of aspects of vertebrate development is identical in these simple chordates to those in higher vertebrates. As an example, the brachyury gene and the homolog of the PAX2 gene both play a similar role in the development of tunicates as they do in vertebrates. Complex aspects of vertebral development such as the differentiation of the central nervous system can thus be studied in the laboratory. The genome has been sequenced and contains about 15,000 genes, approximately half the number occurring in vertebrates. All central Hox genes have been lost. Comparison of the genome with that of other chordates will help identify the genes which appeared early in the vertebrate lineage.",
            "score": 49.6046257019043
        },
        {
            "docid": "393024_2",
            "document": "Biological database . Biological databases are libraries of life sciences information, collected from scientific experiments, published literature, high-throughput experiment technology, and computational analysis. They contain information from research areas including genomics, proteomics, metabolomics, microarray gene expression, and phylogenetics. Information contained in biological databases includes gene function, structure, localization (both cellular and chromosomal), clinical effects of mutations as well as similarities of biological sequences and structures.",
            "score": 49.59071731567383
        },
        {
            "docid": "4007073_24",
            "document": "Gene expression profiling . One might further hypothesize that the experimental treatment regulates cholesterol, because the treatment seems to selectively regulate genes associated with cholesterol. While this may be true, there are a number of reasons why making this a firm conclusion based on enrichment alone represents an unwarranted leap of faith. One previously mentioned issue has to do with the observation that gene regulation may have no direct impact on protein regulation: even if the proteins coded for by these genes do nothing other than make cholesterol, showing that their mRNA is altered does not directly tell us what is happening at the protein level. It is quite possible that the amount of these cholesterol-related proteins remains constant under the experimental conditions. Second, even if protein levels do change, perhaps there is always enough of them around to make cholesterol as fast as it can be possibly made, that is, another protein, not on our list, is the rate determining step in the process of making cholesterol. Finally, proteins typically play many roles, so these genes may be regulated not because of their shared association with making cholesterol but because of a shared role in a completely independent process.",
            "score": 49.42327117919922
        },
        {
            "docid": "13512486_6",
            "document": "Biobank . By the late 1990s scientists realized that although many diseases are caused at least in part by a genetic component, few diseases originate from a single defective gene; most genetic diseases are caused by multiple genetic factors on multiple genes. Because the strategy of looking only at single genes was ineffective for finding the genetic components of many diseases, and because new technology made the cost of examining a single gene versus doing a genome-wide scan about the same, scientists began collecting much larger amounts of genetic information when any was to be collected at all. At the same time technological advances also made it possible for wide sharing of information, so when data was collected, many scientists doing genetics work found that access to data from genome-wide scans collected for any one reason would actually be useful in many other types of genetic research. Whereas before data usually stayed in one laboratory, now scientists began to store large amounts of genetic data in single places for community use and sharing.",
            "score": 49.24067687988281
        },
        {
            "docid": "2571276_2",
            "document": "Computational genomics . Computational genomics (often referred to as Computational Genetics) refers to the use of computational and statistical analysis to decipher biology from genome sequences and related data, including both DNA and RNA sequence as well as other \"post-genomic\" data (i.e., experimental data obtained with technologies that require the genome sequence, such as genomic DNA microarrays). These, in combination with computational and statistical approaches to understanding the function of the genes and statistical association analysis, this field is also often referred to as Computational and Statistical Genetics/genomics. As such, computational genomics may be regarded as a subset of bioinformatics and computational biology, but with a focus on using whole genomes (rather than individual genes) to understand the principles of how the DNA of a species controls its biology at the molecular level and beyond. With the current abundance of massive biological datasets, computational studies have become one of the most important means to biological discovery.",
            "score": 49.13059997558594
        },
        {
            "docid": "7281961_3",
            "document": "Saccharomyces Genome Database . The Saccharomyces Genome Database (SGD) provides Internet access to the complete \"Saccharomyces cerevisiae\" genomic DNA sequence, its genes and their products, the phenotypes of its mutants, and the literature supporting these data. In the peer-reviewed literature report, experiment result on function and interaction of yeast genes are extracted by high-quality manual curation and integrated within a well-developed database. The data are combined with quality high-throughput results and post on Locus Summary pages which is a powerful query engine and rich genome browser. Based on the complexity of information collection, multiple bioinformatic tools are used to integrate information and allow productive discovery of new biological details. The gold standard for functional description of budding yeast is provided by SGD resource. The SGD resource also provide a platform from which to investigate related genes and pathways in higher organisms.The amount of information and the number of features provided by SGD have increased greatly following the release of the \"S. cerevisiae\" genomic sequence. SGD aids researchers by providing not only basic information, but also tools such as sequence similarity searching that lead to detailed information about features of the genome and relationships between genes. SGD presents information using a variety of user-friendly, dynamically created graphical displays illustrating physical, genetic and sequence feature maps. All of the data in SGD are freely accessible to researchers and educators worldwide via web pages designed for optimal ease of use.",
            "score": 49.01092529296875
        },
        {
            "docid": "21177423_10",
            "document": "GEN2PHEN . No system yet exists that even begins to approximate to a \u2018biomedical knowledge environment\u2019 properly able to support G2P data gathering and analysis. There are instead a limited number of unconnected G2P databases that are mostly at rather early stages in their development, with no agreed structured way of effectively modelling phenotype data or G2P relationships, and no convenient mode for passing data from discovery laboratories into the database world. A few recent initiatives are building large databases to host individual-specific genotypes and phenotypes to support some high-throughput disease association studies, but these do not have a global remit, have not engaged with the extensive existing knowledge from Medelian disorders, and are not focused on all the research and clinical communities around G2P. Most progress has arguably been made with locus-specific databases (LSDBs) that target specific diseases or genes, but the vast majority of the several hundred LSDBs that do exist are rudimentary in design and implementation, and operationally isolated from one another. This all contrasts with the situation for databases concerned with purely genetic data (without phenotype association), of which there are many, including several large data warehouses and genome browsers that act as central repositories and search centres for all the human and model organism genome sequences, variants, and feature annotations yet produced.",
            "score": 48.92359161376953
        },
        {
            "docid": "31192209_2",
            "document": "Yeastract . YEASTRACT (Yeast Search for Transcriptional Regulators And Consensus Tracking) is a curated repository of more than 48000 regulatory associations between transcription factors (TF) and target genes in \"Saccharomyces cerevisiae\", based on more than 1200 bibliographic references. It also includes the description of about 300 specific DNA binding sites for more than a hundred characterized TFs. Further information about each Yeast gene has been extracted from the Saccharomyces Genome Database (SGD). For each gene the associated Gene Ontology (GO) terms and their hierarchy in GO was obtained from the GO consortium. Currently, YEASTRACT maintains more than 7100 terms from GO. The nucleotide sequences of the promoter and coding regions for Yeast genes were obtained from Regulatory Sequence Analysis Tools (RSAT). All the information in YEASTRACT is updated regularly to match the latest data from SGD, GO consortium, RSA Tools and recent literature on yeast regulatory networks.",
            "score": 48.789432525634766
        },
        {
            "docid": "47878_26",
            "document": "Huntington's disease . Testing before the onset of symptoms is a life-changing event and a very personal decision. The main reason given for choosing testing for HD is to aid in career and family decisions. Before 1993 there was not an available test for individuals to learn if they carried the Huntington's gene. At that time surveys indicated that 50\u201370% of at-risk individuals would have been interested in receiving testing, but since predictive testing has been offered far fewer choose to be tested. Over 95% of individuals at risk of inheriting HD do not proceed with testing, mostly because there is no treatment. A key issue is the anxiety an individual experiences about not knowing whether they will eventually develop HD, compared to the impact of a positive result. Irrespective of the result, stress levels have been found to be lower two years after being tested, but the risk of suicide is increased after a positive test result. Individuals found to have not inherited the disorder may experience survivor guilt with regard to family members who are affected. Other factors taken into account when considering testing include the possibility of discrimination and the implications of a positive result, which usually means a parent has an affected gene and that the individual's siblings will be at risk of inheriting it. In one study genetic discrimination was found in 46% of individuals at risk for Huntington's disease. It occurred at higher rates within personal relationships than health insurance or employment relations. Genetic counseling in HD can provide information, advice and support for initial decision-making, and then, if chosen, throughout all stages of the testing process. Because of the implications of this test, patients who wish to undergo testing must complete three counseling sessions which provide information about Huntington's.",
            "score": 48.684814453125
        },
        {
            "docid": "4007073_10",
            "document": "Gene expression profiling . In general, expression profiling studies report those genes that showed statistically significant differences under changed experimental conditions. This is typically a small fraction of the genome for several reasons. First, different cells and tissues express a subset of genes as a direct consequence of cellular differentiation so many genes are turned off. Second, many of the genes code for proteins that are required for survival in very specific amounts so many genes do not change. Third, cells use many other mechanisms to regulate proteins in addition to altering the amount of mRNA, so these genes may stay consistently expressed even when protein concentrations are rising and falling. Fourth, financial constraints limit expression profiling experiments to a small number of observations of the same gene under identical conditions, reducing the statistical power of the experiment, making it impossible for the experiment to identify important but subtle changes. Finally, it takes a great amount of effort to discuss the biological significance of each regulated gene, so scientists often limit their discussion to a subset. Newer microarray analysis techniques automate certain aspects of attaching biological significance to expression profiling results, but this remains a very difficult problem.",
            "score": 48.28575897216797
        },
        {
            "docid": "38245286_3",
            "document": "Phenotype microarray . High-throughput phenotypic testing is increasingly important for exploring the biology of bacteria, fungi, yeasts, and animal cell lines such as human cancer cells. Just as DNA microarrays and proteomic technologies have made it possible to assay the level of thousands of genes or proteins all a once, phenotype microarrays (PMs) make it possible to quantitatively measure thousands of cellular phenotypes all at once. The approach also offers potential for testing gene function and improving genome annotation. In contrast to the hitherto available molecular high-throughput technologies, phenotypic testing is processed with living cells, thus providing comprehensive information about the performance of entire cells. The major applications of the PM technology are in the fields of systems biology, microbial cell physiology and taxonomy, and mammalian cell physiology including clinical research such as on autism. Advantages of PMs over standard growth curves are that cellular respiration can be measured in environmental conditions where cellular replication (growth) may not be possible, and that respiration reactions are usually detected much earlier than cellular growth.",
            "score": 48.23519515991211
        }
    ]
}