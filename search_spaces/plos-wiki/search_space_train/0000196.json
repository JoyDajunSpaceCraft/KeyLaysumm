{
    "q": [
        {
            "docid": "40841348_11",
            "document": "Computational and Statistical Genetics . In this era of large amount of genetic and genomic data, accurate representation and identification of statistical interactions in biological/genetic/genomic data constitutes a vital basis for designing interventions and curative solutions for many complex diseases. Variations in human genome have been long known to make us susceptible to many diseases. We are hurtling towards the era of personal genomics and personalized medicine that require accurate predictions of disease risk posed by predisposing genetic factors. Computational and statistical methods for identifying these genetic variations, and building these into intelligent models for diseaseassociation and interaction analysis studies genome-wide are a dire necessity across many disease areas. The principal challenges are: (1) most complex diseases involve small or weak contributions from multiple genetic factors that explain only a minuscule fraction of the population variation attributed to genetic factors. (2) Biological data is inherently extremely noisy, so the underlying complexities of biological systems (such as linkage disequilibrium and genetic heterogeneity) need to be incorporated into the statistical models for disease association studies. The chances of developing many common diseases such as cancer, autoimmune diseases and cardiovascular diseases involves complex interactions between multiple genes and several endogenous and exogenous environmental agents or covariates. Many previous disease association studies could not produce significant results because of the lack of incorporation of statistical interactions in their mathematical models explaining the disease outcome. Consequently much of the genetic risks underlying several diseases and disorders remain unknown. Computational methods such as to model and identify the genetic/genomic variations underlying disease risks has a great potential to improve prediction of disease outcomes, understand the interactions and design better therapeutic methods based on them.",
            "score": 101.86613595485687
        },
        {
            "docid": "43778895_12",
            "document": "Predictive genomics . In 2009, a study was conducted on the WTCCC (GWA study involving 7 cohorts with 7 diseases: including bipolar disorder, Crohn\u2019s disease, hypertension, rheumatoid arthritis, Type I Diabetes (T1D) and Type II Diabetes (T2D)). With particular attention to T2D, Evans et al. were able to discern a marginal increase in AUC (+0.04) based on genome-wide information with respect to known susceptible variants. However, non-genetic based tests such as the Cambridge and Framingham offspring risk scores have been purported to perform better than genetic-risk models with 20 loci. Moreover, the addition of genetic risk with these phenotypical models did not produce statistically significant AUC results.",
            "score": 92.48854041099548
        },
        {
            "docid": "40841348_9",
            "document": "Computational and Statistical Genetics . Over the past few years, genome-wide association studies (GWAS) have become a powerful tool for investigating the genetic basis of common diseases and has improved our understanding of the genetic basis of many complex traits. Traditional single SNP (single-nucleotide polymorphism) GWAS is the most commonly used method to find trait associated DNA sequence variants - associations between variants and one or more phenotypes of interest are investigated by studying individuals with different phenotypes and examining their genotypes at the position of each SNP individually. The SNPs for which one variant is statistically more common in individuals belonging to one phenotypic group are then reported as being associated with the phenotype. However, most complex common diseases involve small population-level contributions from multiple genomic loci. To detect such small effects as genome-wide significant, traditional GWAS rely on increased sample size e.g. to detect an effect which accounts for 0.1% of total variance, traditional GWAS needs to sample almost 30,000 individuals. Although the development of high throughput SNP genotyping technologies has lowered the cost and improved the efficiency of genotyping. Performing such a large scale study still costs considerable money and time. Recently, association analysis methods utilizing gene-based tests have been proposed that are based on the fact that variations in protein-coding and adjacent regulatory regions are more likely to have functional relevance. These methods have the advantage that they can account for multiple independent functional variants within a gene, with the potential to greatly increase the power to identify disease/trait associated genes. Also, imputation of ungenotyped markers using known reference panels(e.g. HapMap and the 1000 Genomes Project) predicts genotypes at the missing or untyped markers thereby allowing one to accurately evaluate the evidence for association at genetic markers that are not directly genotyped (in addition to the typed markers) and has been shown to improve the power of GWAS to detect disease associated loci.",
            "score": 89.50691735744476
        },
        {
            "docid": "7972254_2",
            "document": "Predictive medicine . Predictive medicine is a field of medicine that entails predicting the probability of disease and instituting preventive measures in order to either prevent the disease altogether or significantly decrease its impact upon the patient (such as by preventing mortality or limiting morbidity). While different prediction methodologies exist, such as genomics, proteomics, and cytomics, the most fundamental way to predict future disease is based on genetics. Although proteomics and cytomics allow for the early detection of disease, much of the time those detect biological markers that exist because a disease process has \"already\" started. However, comprehensive genetic testing (such as through the use of DNA arrays or full genome sequencing) allows for the estimation of disease risk years to decades before any disease even exists, or even whether a healthy fetus is at higher risk for developing a disease in adolescence or adulthood. Individuals who are more susceptible to disease in the future can be offered lifestyle advice or medication with the aim of preventing the predicted illness. Current genetic testing guidelines supported by the health care professionals discourage purely predictive genetic testing of minors until they are competent to understand the relevancy of genetic screening so as to allow them to participate in the decision about whether or not it is appropriate for them. Genetic screening of newborns and children in the field of predictive medicine is deemed appropriate if there is a compelling clinical reason to do so, such as the availability of prevention or treatment as a child that would prevent future disease.",
            "score": 74.3432559967041
        },
        {
            "docid": "43778895_5",
            "document": "Predictive genomics . Whilst the single-gene, single-disease hypothesis holds for Mendelian disorders such as Huntington's disease and Cystic Fibrosis, complex diseases and traits are affected by a number of gene loci and genetic variants with varying risk. A precursor to the development of preventative, prognostic and diagnostic tools in these diseases requires mapping genetic loci in disease etiology and discovering causal mutations. Creating a \u2018genomic profile\u2019 of individuals with the number of variants at the genome-wide level facilitates not only the prediction of disease prior to onset, but also serves as a primer to increasing the knowledge of causal variants.",
            "score": 64.42895984649658
        },
        {
            "docid": "52142704_3",
            "document": "Polygenic score . Polygenic scores are widely employed in animal, plant, and behavioral genetics for predicting and understanding genetic architectures. In a genome-wide association study (GWAS), polygenic scores having substantially higher predictive performance than the genome-wide statistically-significant hits indicates that the trait in question is affected by a larger number of variants than just the hits and larger sample sizes will yield more hits; a conjunction of low variance explained and high heritability as measured by GCTA, twin studies or other methods, indicates that a trait may be massively polygenic and affected by thousands of variants. Once a polygenic score has been created, which explains at least a few percent of a phenotype's variance and can therefore be assumed to effectively incorporate a significant fraction of the genetic variants affecting that phenotype, it can be used in several different ways: as a lower bound to test whether heritability estimates may be biased; as a measure of genetic overlap of traits (genetic correlation), which might indicate e.g. shared genetic bases for groups of mental disorders; as a means to assess group differences in a trait such as height, or to examine changes in a trait over time due to natural selection indicative of a soft selective sweep (as e.g. for intelligence where the changes in frequency would be too small to detect on each individual hit but not on the overall polygenic score); in Mendelian randomization (assuming no pleiotropy with relevant traits); to detect & control for the presence of genetic confounds in outcomes (e.g. the correlation of schizophrenia with poverty); or to investigate gene\u2013environment interactions.",
            "score": 86.61688482761383
        },
        {
            "docid": "43778895_9",
            "document": "Predictive genomics . In the table below is a performance comparison of diseases selected on disease frequency and known heritability estimates, with use of single-nucleotide polymorphism (SNP) based models reflecting known genetic factors for a European population (subject to change as more associations are discovered). formula_1 denotes lifetime morbid risk, formula_2 denotes heritability of liability, formula_3 denotes area under the ROC curve. In the applications of predictive genomics below, these complex diseases either lack or are lacking reliable diagnostics for disease. Given the medical consequences of these diseases, the economic impact is also significant. However, none of the use cases below has been translated into the clinic.",
            "score": 93.6747350692749
        },
        {
            "docid": "25569540_10",
            "document": "Genomic counseling . There has been very limited study of patients receiving potentially actionable genomic based results or the utilization of genetic counselors in the online result delivery process. A randomized controlled trial on 199 patients with chronic disease each receiving eight personalized and actionable complex disease reports online. Primary study aims were to assess the impact of in-person genomic counseling on 1) causal attribution of disease risk, 2) personal awareness of disease risk, and 3) perceived risk of developing a particular disease. Of 98 intervention arm participants (mean age = 57.8; 39% female) randomized for in-person genomic counseling, 76 (78%) were seen. In contrast, control arm participants (n = 101; mean age = 58.5; 54% female) were initially not offered genomic counseling as part of the study protocol but were able to access in-person genomic counseling, if they requested it, 3-months post viewing of at least one test report and post-completion of the study-specific follow-up survey. A total of 64 intervention arm and 59 control arm participants completed follow-up survey measures. We found that participants receiving in-person genomic counseling had enhanced objective understanding of the genetic variant risk contribution for multiple complex diseases. Genomic counseling was associated with lowered participant causal beliefs in genetic influence across all eight diseases, compared to control participants. Our findings also illustrate that for the majority of diseases under study, intervention arm participants believed they knew their genetic risk status better than control arm subjects. Disease risk was modified for the majority during genomic counseling, due to the assessment of more comprehensive family history. In conclusion, for patients receiving personalized and actionable genomic results through a web portal, genomic counseling enhanced their objective understanding of the genetic variant risk contribution to multiple common diseases. These results support the development of additional genomic counseling interventions to ensure a high level of patient comprehension and improve patient-centered health outcomes.",
            "score": 76.03707349300385
        },
        {
            "docid": "43778895_11",
            "document": "Predictive genomics . Type 2 diabetes (T2D), an extremely common metabolic disorder, has demonstrated interplay between many environmental and genetic risk factors leading to disease onset. A number of risk assessment models incorporating a number of demographic, environmental and clinical risk factors are already shown to elicit reasonable discrimination in case-control studies; it has been proposed that identifying genetic variants that contribute to T2D as for standalone prediction or in conjunction with current risk models can improve prediction of T2D risk, if current models lack sufficient coverage of the full effect of an individual's genotype. Approximately 20 associated SNPs have been replicated in T2D; however, their effect sizes do not seem to be substantial: OR 1.37 for SNPs in the \"TCF7L2\" gene purported to give highest genetic risk.",
            "score": 92.47641038894653
        },
        {
            "docid": "43778895_10",
            "document": "Predictive genomics . Age-related macular degeneration (AMD) is one of the flagship complex diseases from the genomic revolution with over 19 associated genetic loci replicated in GWA studies. In particular, the first significant genetic risk variant was identified in the complement factor H(\"CFH\") gene in 2005 motivating the search for more genetic variants in the disease. Over the past decade, a number of models have been proposed to assess individual risk to AMD. The genetic predisposition of AMD risk varies from 45% to 71% where highly effectual odds ratios (OR) have been reported (greater than 2.0 per allele in some cases). In 2013, a comprehensive case-control GWA study with approximately 77,000 observations involving 18 international research groups from the International AMD Genetics Consortium implicated 19 gene loci and 9 biological pathways including the regulation of complement, lipid metabolism and angiogenic activity. The predictive performance of the full model including all 19 loci exhibited 0.74 AUC - according to Jakobsdottir et al., 0.75 AUC is sufficient to distinguish between extreme cases and controls. In particular, of the 19 associated gene loci, there were 7 newly discovered loci, which the authors point to as additional entry points into AMD etiology and drug targets.",
            "score": 91.20851063728333
        },
        {
            "docid": "14402695_7",
            "document": "Personal genomics . Disease risk may be calculated based on genetic markers and genome-wide association studies for common medical conditions, which are multifactorial and include environmental components in the assessment. Diseases which are individually rare (less than 200,000 people affected in the USA) are nevertheless collectively common (affecting roughly 8-10% of the US population). Over 2500 of these diseases (including a few more common ones) have predictive genetics of sufficiently high clinical impact that they are recommended as medical genetic tests available for single genes (and in whole genome sequencing) and growing at about 200 new genetic diseases per year.",
            "score": 80.69273209571838
        },
        {
            "docid": "6003871_9",
            "document": "Tag SNP . Genome-wide association studies (GWAS) use single-nucleotide polymorphisms (SNPs) to identify genetic associations with clinical conditions and phenotypic traits. They are hypothesis free and use a whole-genome approach to investigate traits by comparing large group of individuals that express a phenotype with a large group of people that don't. The ultimate goal of GWAS is to determine genetic risk factors that can be used to make predictions about who is at risk for a disease, what are the biological underpinnings of disease susceptibility and creating new prevention and treatment strategies. The National Human Genome Research Institute and the European Bioinformatics Institute publishes a Catalog of published genome-wide association studies that highlights statistically significant associations between hundreds of SNPs with a broad range of phenotypes.",
            "score": 61.076287269592285
        },
        {
            "docid": "11808249_6",
            "document": "Genome-wide association study . Any two human genomes differ in millions of different ways. There are small variations in the individual nucleotides of the genomes (SNPs) as well as many larger variations, such as deletions, insertions and copy number variations. Any of these may cause alterations in an individual's traits, or phenotype, which can be anything from disease risk to physical properties such as height. Around the year 2000, prior to the introduction of GWA studies, the primary method of investigation was through inheritance studies of genetic linkage in families. This approach had proven highly useful towards single gene disorders. However, for common and complex diseases the results of genetic linkage studies proved hard to reproduce. A suggested alternative to linkage studies was the genetic association study. This study type asks if the allele of a genetic variant is found more often than expected in individuals with the phenotype of interest (e.g. with the disease being studied). Early calculations on statistical power indicated that this approach could be better than linkage studies at detecting weak genetic effects.",
            "score": 66.6533111333847
        },
        {
            "docid": "30487688_23",
            "document": "Diagnosis of schizophrenia . Estimates of the heritability of schizophrenia is around 80%, which implies that 80% of the individual differences in risk to schizophrenia is explained by individual differences in genetics. Although many genetic variants associated with schizophrenia have been identified, their effects are usually very small, so they are combined onto a polygenic risk score. These scores, despite accounting for hundreds of variants, only explain up to 6% in symptom variation and 7% of the risk for developing the disease. An example of a well-studied genetic biomarker in schizophrenia is the single nucleotide polymorphism in the HLA-DQB1 gene, which is part of the human leukocyte antigen (HLA) complex. A G to C replacement on position 6672 predicts risk of agranulocytosis, a side effect of clozapine that can be fatal.",
            "score": 79.02908635139465
        },
        {
            "docid": "50613151_3",
            "document": "Genome-wide complex trait analysis . GCTA heritability estimates are useful because they can lower bound the genetic contributions to traits such as intelligence without relying on the assumptions used in twin studies and other family studies and pedigree analyses, thereby corroborating them, and enabling the design of well-powered Genome-wide association study (GWAS) designs to find the specific genetic variants. For example, a GCTA estimate of 30% SNP heritability is consistent with a larger total genetic heritability of 70%. However, if the GCTA estimate was ~0%, then that would imply one of three things: a) there is no genetic contribution, b) the genetic contribution is entirely in the form of genetic variants not included, or c) the genetic contribution is entirely in the form of non-additive effects such as epistasis/dominance. The ability to run GCTA on subsets of chromosomes and regress against chromosome length can reveal whether the responsible genetic variants cluster or are distributed evenly across the genome or are sex-linked. Examining genetic correlations can reveal to what extent observed correlations, such as between intelligence and socioeconomic status, are due to the same genetic traits, and in the case of diseases, can indicate shared causal pathways such as the overlap of schizophrenia with other mental diseases and intelligence-reducing variants.",
            "score": 61.706305503845215
        },
        {
            "docid": "12229900_5",
            "document": "Genetic exceptionalism . There is ongoing debate over whether or when certain genetic information should be considered exceptional. In some cases, the predictive power of genetic information (such as a risk for a disease like Huntington's Disease, which is highly penetrant) may justify special considerations for genetic exceptionalism, in that individuals with a high risk for developing this condition may face a certain amount of discrimination. However, for most common human health conditions, a specific genetic variant only plays a partial role, interacting with other genetic variants and environmental and lifestyle influences to contribute to disease development. In these cases, genetic information is often considered similarly to other medical and lifestyle data, such as smoking status, age, or biomarkers.",
            "score": 80.41286897659302
        },
        {
            "docid": "7972254_7",
            "document": "Predictive medicine . The future of medicine's focus may potentially shift from treating existing diseases, typically late in their progression, to preventing disease before it sets in. Predictive health and predictive medicine is based on probabilities: while it evaluates susceptibility to diseases, it is not able to predict with 100% certainty that a specific disease will occur. Unlike many preventive interventions that are directed at groups (e.g., immunization programs), predictive medicine is conducted on an individualized basis. For example, glaucoma is a monogenic disease whose early detection can allow to prevent permanent loss of vision. Predictive medicine is expected to be most effective when applied to polygenic multifactorial disease that are prevalent in industrialized countries, such as diabetes mellitus, hypertension, and myocardial infarction. With careful usage, predictive medicine methods such as genetic screens can help diagnose inherited genetic disease caused by problems with a single gene (such as cystic fibrosis) and help early treatment. Some forms of cancer and heart disease are inherited as single-gene diseases and some people in these high-risk families may also benefit from access to genetic tests. As more and more genes associated with increased susceptibility to certain diseases are reported, predictive medicine becomes more useful.",
            "score": 66.85115814208984
        },
        {
            "docid": "53653356_33",
            "document": "Elective genetic and genomic testing . When considering elective genetic testing, it is important to take into account the type and goals of testing. Providers and patients should be familiar with differing testing methodologies the potential results from each test. For many individuals, factors such as test cost, scope, and deliverables, in combination with their specific clinical questions, play into the decision to undergo elective testing. It is also important to recognize that potential results from elective genetic testing are constrained by the current limits of medical knowledge concerning the association between genetics and human disease. As knowledge of rare genetic factors that confer high risk, as well as common factors that confer lower risks, increases, we will have the ability to learn more about an individual's current and future health.",
            "score": 77.64023327827454
        },
        {
            "docid": "22921_61",
            "document": "Psychology . All researched psychological traits are influenced by both genes and environment, to varying degrees. These two sources of influence are often confounded in observational research of individuals or families. An example is the transmission of depression from a depressed mother to her offspring. Theory may hold that the offspring, by virtue of having a depressed mother in his or her (the offspring's) environment, is at risk for developing depression. However, risk for depression is also influenced to some extent by genes. The mother may both carry genes that contribute to her depression but will also have passed those genes on to her offspring thus increasing the offspring's risk for depression. Genes and environment in this simple transmission model are completely confounded. Experimental and quasi-experimental behavioral genetic research uses genetic methodologies to disentangle this confound and understand the nature and origins of individual differences in behavior. Traditionally this research has been conducted using twin studies and adoption studies, two designs where genetic and environmental influences can be partially un-confounded. More recently, the availability of microarray molecular genetic or genome sequencing technologies allows researchers to measure participant DNA variation directly, and test whether individual genetic variants within genes are associated with psychological traits and psychopathology through methods including genome-wide association studies. One goal of such research is similar to that in positional cloning and its success in Huntington's: once a causal gene is discovered biological research can be conducted to understand how that gene influences the phenotype. One major result of genetic association studies is the general finding that psychological traits and psychopathology, as well as complex medical diseases, are highly polygenic, where a large number (on the order of hundreds to thousands) of genetic variants, each of small effect, contribute to individual differences in the behavioral trait or propensity to the disorder. Active research continues to understand the genetic and environmental bases of behavior and their interaction.",
            "score": 70.2318788766861
        },
        {
            "docid": "26418006_23",
            "document": "Exome sequencing . Current association studies have focused on common variation across the genome, as these are the easiest to identify with our current assays. However, disease-causing variants of large effect have been found to lie within exomes in candidate gene studies, and because of negative selection, are found in much lower allele frequencies and may remain untyped in current standard genotyping assays. Whole genome sequencing is a potential method to assay novel variant across the genome. However, in complex disorders (such as autism), a large number of genes are thought to be associated with disease risk. This heterogeneity of underlying risk means that very large sample sizes are required for gene discovery, and thus whole genome sequencing is not particularly cost-effective. This sample size issue is alleviated by the development of novel advanced analytic methods, which effectively map disease genes despite the genetic mutations are rare at variant level. In addition, variants in coding regions have been much more extensively studied and their functional implications are much easier to derive, making the practical applications of variants within the targeted exome region more immediately accessible.",
            "score": 62.163007855415344
        },
        {
            "docid": "50613151_2",
            "document": "Genome-wide complex trait analysis . Genome-wide complex trait analysis (GCTA) GREML is a statistical method for variance component estimation in genetics which quantifies the total narrow-sense (additive) contribution to a trait's heritability of a particular subset of genetic variants (typically limited to SNPs with MAF >1%, hence terms such as \"chip heritability\"/\"SNP heritability\"). This is done by directly quantifying the chance genetic similarity of unrelated strangers and comparing it to their measured similarity on a trait; if two strangers are relatively similar genetically and also have similar trait measurements, then this indicates that the measured genetics causally influence that trait, and how much. This can be seen as plotting prediction error against relatedness. The GCTA framework extends to bivariate genetic correlations between traits; it can also be done on a per-chromosome basis comparing against chromosome length; and it can also examine changes in heritability over aging and development.",
            "score": 63.36630058288574
        },
        {
            "docid": "11808249_3",
            "document": "Genome-wide association study . When applied to human data, GWA studies compare the DNA of participants having varying phenotypes for a particular trait or disease. These participants may be people with a disease (cases) and similar people without the disease (controls), or they may be people with different phenotypes for a particular trait, for example blood pressure. This approach is known as phenotype-first, in which the participants are classified first by their clinical manifestation(s), as opposed to genotype-first. Each person gives a sample of DNA, from which millions of genetic variants are read using SNP arrays. If one type of the variant (one allele) is more frequent in people with the disease, the variant is said to be \"associated\" with the disease. The associated SNPs are then considered to mark a region of the human genome that may influence the risk of disease.",
            "score": 57.9412682056427
        },
        {
            "docid": "43778895_2",
            "document": "Predictive genomics . Predictive genomics is at the intersection of multiple disciplines: predictive medicine, personal genomics and translational bioinformatics. Specifically, predictive genomics deals with the future phenotypic outcomes via prediction in areas such as complex multifactorial diseases in humans. To date, the success of predictive genomics has been dependent on the genetic framework underlying these applications, typically explored in genome-wide association (GWA) studies. The identification of associated single-nucleotide polymorphisms (variation of a DNA sequence in a population) underpin GWA studies in complex diseases that have ranged from Type 2 Diabetes (T2D), Age-related macular degeneration (AMD) and Crohn\u2019s Disease.",
            "score": 53.11983561515808
        },
        {
            "docid": "11304934_9",
            "document": "Causes of schizophrenia . Although twin studies and family studies have indicated a large degree of heritability for schizophrenia, the exact genetic causes remain unclear. Recently however, quite some large-scale studies have now begun to unravel the genetic underpinnings for the disease. Important segregation should be made between lower risk, common variants (identified by candidate studies or genome-wide association studies(GWAS)) and high risk, rare variants (which could be caused by de novo mutations) and copy-number variations (CNVs).",
            "score": 69.8886170387268
        },
        {
            "docid": "24235330_24",
            "document": "Behavioural genetics . Finally, there are classical behavioural disorders that are genetically simple in their etiology, such as Huntington's disease. Huntington's is caused by a single autosomal dominant variant in the \"HTT\" gene, which is the only variant that accounts for any differences among individuals in their risk for developing the disease, assuming they live long enough. In the case of genetically simple and rare diseases such as Huntington's, the variant formula_33 and the formula_36 are simultaneously large. Behavioural genetic research and findings have at times been controversial. Some of this controversy has arisen because behavioural genetic findings can challenge societal beliefs about the nature of human behaviour and abilities. Major areas of controversy have included genetic research on topics such as racial differences, intelligence, violence, and human sexuality. Other controversies have arisen due to misunderstandings of behavioural genetic research, whether by the lay public or the researchers themselves. The notion of heritability is easily misunderstood to imply causality. When behavioral genetics researchers say that a behavior is X% heritable, that does not mean that genetics causes up to X% of the behavior. Instead, heritability is a statement about population level correlations.",
            "score": 70.13660454750061
        },
        {
            "docid": "57196924_3",
            "document": "Complex traits . When Mendel\u2019s work on inheritance was rediscovered in 1900, scientists debated whether Mendel\u2019s laws could account for the continuous variation observed for many traits. One group known as the biometricians argued that continuous traits such as height were largely heritable, but could not be explained by the inheritance of single Mendelian genetic factors. Work by Ronald Fisher in 1918 mostly resolved debate by demonstrating that the variation in continuous traits could be accounted for if multiple such factors contributed additively to each trait. However, the number of genes involved in such traits remained undetermined; until recently, genetic loci were expected to have moderate effect sizes and each explain several percent of heritability. After the conclusion of the Human Genome Project in 2001, it seemed that the sequencing and mapping of many individuals would soon allow for a complete understanding of traits\u2019 genetic architectures. However, variants discovered through genome-wide association studies (GWASs) accounted for only a small percentage of predicted heritability; for example, while height is estimated to be 80-90% heritable, early studies only identified variants accounting for 5% of this heritability. Later research showed that most missing heritability could be accounted for by common variants missed by GWASs because their effect sizes fell below significance thresholds; a smaller percentage is accounted for by rare variants with larger effect sizes, although in certain traits such as autism, rare variants play a more dominant role. While many genetic factors involved in complex traits have been identified, determining their specific contributions to phenotypes\u2014specifically, the molecular mechanisms through which they act\u2014remains a major challenge.",
            "score": 60.07027447223663
        },
        {
            "docid": "21647820_40",
            "document": "Whole genome sequencing . Currently available newborn screening for childhood diseases allows detection of rare disorders that can be prevented or better treated by early detection and intervention. Specific genetic tests are also available to determine an etiology when a child's symptoms appear to have a genetic basis. Full genome sequencing, in addition has the potential to reveal a large amount of information (such as carrier status for autosomal recessive disorders, genetic risk factors for complex adult-onset diseases, and other predictive medical and non-medical information) that is currently not completely understood, may not be clinically useful to the child during childhood, and may not necessarily be wanted by the individual upon reaching adulthood.",
            "score": 61.35932946205139
        },
        {
            "docid": "44248347_18",
            "document": "Gene Disease Database . This one of the largest resources available for all genomic and genetic studies, it provides a centralized resource for geneticists, molecular biologists and other researchers studying the genomes of our own species and other vertebrates and model disease organisms. Ensembl is one of several well-known genome browsers for the retrieval of genomic-disease information. Ensembl imports variation data from a variety of different sources, Ensembl predicts the effects of variants. For each variation that is mapped to the reference genome, each Ensembl transcript is identified that overlap the variation. Then it uses a rule-based approach to predict the effects that each allele of the variation may have on the transcript. The set of consequence terms, defined by the Sequence Ontology (SO) can be currently assigned to each combination of an allele and a transcript. Each allele of each variation may have a different effect in different transcripts. A variety of different tools are used to predict human mutations in the Ensembl database, one of the most widely used is SIFT, that predicts whether an amino acid substitution is likely to affect protein function based on sequence homology and the physic-chemical similarity between the alternate amino acids. The data provided for each amino acid substitution is a score and a qualitative prediction (either 'tolerated' or 'deleterious'). The score is the normalized probability that the amino acid change is tolerated so scores near 0 are more likely to be deleterious. The qualitative prediction is derived from this score such that substitutions with a score < 0.05 are called 'deleterious' and all others are called 'tolerated'.SIFT can be applied to naturally occurring nonsynonymous polymorphisms and laboratory-induced missense mutations, that will lead to build relationships in phenotype characteristics, proteomics and genomics",
            "score": 66.98061442375183
        },
        {
            "docid": "7952689_14",
            "document": "Threshold model . Early genetics models were developed to deal with very rare genetic diseases by treating them as Mendelian diseases caused by 1 or 2 genes: the presence or absence of the gene corresponds to the presence or absence of the disease, and the occurrence of the disease will follow predictable patterns within families. Continuous traits like height or intelligence could be modeled as normal distributions, influenced by a large number of genes, and the heritability and effects of selection easily analyzed. Some diseases, like alcoholism, epilepsy, or schizophrenia, cannot be Mendelian diseases because they are common; do not appear in Mendelian ratios; respond slowly to selection against them; often occur in families with no prior history of that disease; however, relatives and adoptees of someone with that disease are far more likely (but not certain) to develop it, indicating a strong genetic component. The liability threshold model was developed to deal with these non-Mendelian binary cases; the model proposes that there is a continuous normally-distributed trait expressing risk polygenically influenced by many genes, which all individuals above a certain value develop the disease and all below it do not.",
            "score": 103.92710208892822
        },
        {
            "docid": "610184_10",
            "document": "Eric Lander . Sequence data is just that: a list of bases found in a given stretch of DNA. Its value lies in the discoveries and new technologies it allows. In Lander's case, one of these applications is the study of disease. He is the founder and director of the Broad Institute, a collaboration between MIT, Harvard, the Whitehead institute and affiliated hospitals. Its goal is \"to create tools for genome medicine and make them broadly available to the scientific community; to apply these tools to propel the understanding and treatment of disease\". To this end they are studying the variation in the human genome and have led an international effort which has assembled a library of 2.1 million single-nucleotide polymorphisms (SNP). These act as markers or signposts in the genome allowing the identification of disease susceptibility genes. They hope to construct a map of the human genome using blocks of these SNP called Linkage disequilibrium or LD. This map will be of significant help in medical genetics. It will allow researchers to link a given condition to a given gene or set of genes using the LD as a marker. This will allow for improved diagnostic procedures. Lander and his colleagues are hoping the LD map will allow them to test the Common Disease-Common Variant hypothesis which states that many common diseases may be caused by a small number of common alleles, for example 50% of the variance in susceptibility to Alzheimer's disease is explained by the common allele ApoE4. Lander's group have recently discovered an important association that accounts for a large proportion of population risk for adult onset diabetes.",
            "score": 79.60560941696167
        },
        {
            "docid": "7972254_4",
            "document": "Predictive medicine . A number of association studies have been published in scientific literature that show associations between specific genetic variants in a person's genetic code and a specific disease. Association and correlation studies have found that a female individual with a mutation in the BRCA1 gene has a 65% cumulative risk of breast cancer. Additionally, new tests from Genetic Technologies LTD and Phenogen Sciences Inc. comparing non-coding DNA to a woman's lifetime exposure to estrogen can now determine a woman's probability of developing estrogen positive breast cancer also known as sporadic breast cancer (the most prevalent form of breast cancer). Genetic variants in the Factor V gene is associated with an increased tendency to form blood clots, such as deep vein thrombosis (DVTs). Genetics tests are expected to reach the market more quickly than new medicines. Myriad Genetics is already generating revenue from genetic tests for BRCA1 and BRCA2.",
            "score": 63.923603773117065
        },
        {
            "docid": "43778895_13",
            "document": "Predictive genomics . Celiac disease (CD) is a complex immune disorder that has been found to have strong genetic links in disease. In particular, human leukocyte antigen (HLA) genes are strongly implicated in CD development and HLA testing is undertaken in clinical practice. However, although there are serological and histological tests available for CD, these clinical screenings have been found to generate false positives. In 2014, Abraham et al. used a genomic risk score (GRS) generated over 6 cohorts with an AUC of 0.86 to 0.90.",
            "score": 71.24761772155762
        }
    ],
    "r": [
        {
            "docid": "7952689_14",
            "document": "Threshold model . Early genetics models were developed to deal with very rare genetic diseases by treating them as Mendelian diseases caused by 1 or 2 genes: the presence or absence of the gene corresponds to the presence or absence of the disease, and the occurrence of the disease will follow predictable patterns within families. Continuous traits like height or intelligence could be modeled as normal distributions, influenced by a large number of genes, and the heritability and effects of selection easily analyzed. Some diseases, like alcoholism, epilepsy, or schizophrenia, cannot be Mendelian diseases because they are common; do not appear in Mendelian ratios; respond slowly to selection against them; often occur in families with no prior history of that disease; however, relatives and adoptees of someone with that disease are far more likely (but not certain) to develop it, indicating a strong genetic component. The liability threshold model was developed to deal with these non-Mendelian binary cases; the model proposes that there is a continuous normally-distributed trait expressing risk polygenically influenced by many genes, which all individuals above a certain value develop the disease and all below it do not.",
            "score": 103.9271011352539
        },
        {
            "docid": "40841348_11",
            "document": "Computational and Statistical Genetics . In this era of large amount of genetic and genomic data, accurate representation and identification of statistical interactions in biological/genetic/genomic data constitutes a vital basis for designing interventions and curative solutions for many complex diseases. Variations in human genome have been long known to make us susceptible to many diseases. We are hurtling towards the era of personal genomics and personalized medicine that require accurate predictions of disease risk posed by predisposing genetic factors. Computational and statistical methods for identifying these genetic variations, and building these into intelligent models for diseaseassociation and interaction analysis studies genome-wide are a dire necessity across many disease areas. The principal challenges are: (1) most complex diseases involve small or weak contributions from multiple genetic factors that explain only a minuscule fraction of the population variation attributed to genetic factors. (2) Biological data is inherently extremely noisy, so the underlying complexities of biological systems (such as linkage disequilibrium and genetic heterogeneity) need to be incorporated into the statistical models for disease association studies. The chances of developing many common diseases such as cancer, autoimmune diseases and cardiovascular diseases involves complex interactions between multiple genes and several endogenous and exogenous environmental agents or covariates. Many previous disease association studies could not produce significant results because of the lack of incorporation of statistical interactions in their mathematical models explaining the disease outcome. Consequently much of the genetic risks underlying several diseases and disorders remain unknown. Computational methods such as to model and identify the genetic/genomic variations underlying disease risks has a great potential to improve prediction of disease outcomes, understand the interactions and design better therapeutic methods based on them.",
            "score": 101.86613464355469
        },
        {
            "docid": "2520461_7",
            "document": "Genetic association . Case control studies are a classical epidemiological tool. Case-control studies use subjects who already have a disease, trait or other condition and determine if there are characteristics of these patients that differ from those who do not have the disease or trait. In genetic case-control studies, the frequency of alleles or genotypes is compared between the cases and controls. The cases will have been diagnosed with the disease under study, or have the trait under test; the controls, who are either known to be unaffected, or who have been randomly selected from the population. A difference in the frequency of an allele or genotype of the polymorphism under test between the two groups indicates that the genetic marker may increase risk of the disease or likelihood of the trait, or be in linkage disequilibrium with a polymorphism which does. Haplotypes can also show association with a disease or trait. One of the earliest successes in this field was finding a single base mutation in the non-coding region of the APOC3 gene (apolipoprotein C3 gene) that associated with higher risks of hypertriglyceridemia and atherosclerosis using a case-control design.",
            "score": 95.35499572753906
        },
        {
            "docid": "43778895_9",
            "document": "Predictive genomics . In the table below is a performance comparison of diseases selected on disease frequency and known heritability estimates, with use of single-nucleotide polymorphism (SNP) based models reflecting known genetic factors for a European population (subject to change as more associations are discovered). formula_1 denotes lifetime morbid risk, formula_2 denotes heritability of liability, formula_3 denotes area under the ROC curve. In the applications of predictive genomics below, these complex diseases either lack or are lacking reliable diagnostics for disease. Given the medical consequences of these diseases, the economic impact is also significant. However, none of the use cases below has been translated into the clinic.",
            "score": 93.67473602294922
        },
        {
            "docid": "43778895_12",
            "document": "Predictive genomics . In 2009, a study was conducted on the WTCCC (GWA study involving 7 cohorts with 7 diseases: including bipolar disorder, Crohn\u2019s disease, hypertension, rheumatoid arthritis, Type I Diabetes (T1D) and Type II Diabetes (T2D)). With particular attention to T2D, Evans et al. were able to discern a marginal increase in AUC (+0.04) based on genome-wide information with respect to known susceptible variants. However, non-genetic based tests such as the Cambridge and Framingham offspring risk scores have been purported to perform better than genetic-risk models with 20 loci. Moreover, the addition of genetic risk with these phenotypical models did not produce statistically significant AUC results.",
            "score": 92.48854064941406
        },
        {
            "docid": "43778895_11",
            "document": "Predictive genomics . Type 2 diabetes (T2D), an extremely common metabolic disorder, has demonstrated interplay between many environmental and genetic risk factors leading to disease onset. A number of risk assessment models incorporating a number of demographic, environmental and clinical risk factors are already shown to elicit reasonable discrimination in case-control studies; it has been proposed that identifying genetic variants that contribute to T2D as for standalone prediction or in conjunction with current risk models can improve prediction of T2D risk, if current models lack sufficient coverage of the full effect of an individual's genotype. Approximately 20 associated SNPs have been replicated in T2D; however, their effect sizes do not seem to be substantial: OR 1.37 for SNPs in the \"TCF7L2\" gene purported to give highest genetic risk.",
            "score": 92.47640991210938
        },
        {
            "docid": "43778895_10",
            "document": "Predictive genomics . Age-related macular degeneration (AMD) is one of the flagship complex diseases from the genomic revolution with over 19 associated genetic loci replicated in GWA studies. In particular, the first significant genetic risk variant was identified in the complement factor H(\"CFH\") gene in 2005 motivating the search for more genetic variants in the disease. Over the past decade, a number of models have been proposed to assess individual risk to AMD. The genetic predisposition of AMD risk varies from 45% to 71% where highly effectual odds ratios (OR) have been reported (greater than 2.0 per allele in some cases). In 2013, a comprehensive case-control GWA study with approximately 77,000 observations involving 18 international research groups from the International AMD Genetics Consortium implicated 19 gene loci and 9 biological pathways including the regulation of complement, lipid metabolism and angiogenic activity. The predictive performance of the full model including all 19 loci exhibited 0.74 AUC - according to Jakobsdottir et al., 0.75 AUC is sufficient to distinguish between extreme cases and controls. In particular, of the 19 associated gene loci, there were 7 newly discovered loci, which the authors point to as additional entry points into AMD etiology and drug targets.",
            "score": 91.20851135253906
        },
        {
            "docid": "9732182_4",
            "document": "Base rate . A large number of psychological studies have examined a phenomenon called base-rate neglect\" or \"base rate fallacy in which category base rates are not integrated with featural evidence in the normative manner. Mathematician Keith Devlin provides an illustration of the risks of this: He asks us to imagine that there is a type of cancer that afflicts 1% of all people. A doctor then says there is a test for that cancer which is about 80% reliable. He also says that the test provides a positive result for 100% of people who have the cancer, but it also results in a 'false positive' for 20% of people - who do not have the cancer. Now, if we test positive, we may be tempted to think it is 80% likely that we have the cancer. Devlin explains that, in fact, our odds are less than 5%. What is missing from the jumble of statistics is the most relevant base rate information. We should ask the doctor, \"\"Out of the number of people who test positive (this is the base rate group that we care about), how many have the cancer?\"\" In assessing the probability that a given individual is a member of a particular class, we must account for other information besides the base rate. In particular, we must account for featural evidence. For example, when we see a person wearing a white doctor's coat and stethoscope, and prescribing medication, we have evidence which may allow us to conclude that the probability of this \"particular\" individual being a \"medical professional\" is considerably greater than the category base rate of 1%.",
            "score": 89.84325408935547
        },
        {
            "docid": "40841348_9",
            "document": "Computational and Statistical Genetics . Over the past few years, genome-wide association studies (GWAS) have become a powerful tool for investigating the genetic basis of common diseases and has improved our understanding of the genetic basis of many complex traits. Traditional single SNP (single-nucleotide polymorphism) GWAS is the most commonly used method to find trait associated DNA sequence variants - associations between variants and one or more phenotypes of interest are investigated by studying individuals with different phenotypes and examining their genotypes at the position of each SNP individually. The SNPs for which one variant is statistically more common in individuals belonging to one phenotypic group are then reported as being associated with the phenotype. However, most complex common diseases involve small population-level contributions from multiple genomic loci. To detect such small effects as genome-wide significant, traditional GWAS rely on increased sample size e.g. to detect an effect which accounts for 0.1% of total variance, traditional GWAS needs to sample almost 30,000 individuals. Although the development of high throughput SNP genotyping technologies has lowered the cost and improved the efficiency of genotyping. Performing such a large scale study still costs considerable money and time. Recently, association analysis methods utilizing gene-based tests have been proposed that are based on the fact that variations in protein-coding and adjacent regulatory regions are more likely to have functional relevance. These methods have the advantage that they can account for multiple independent functional variants within a gene, with the potential to greatly increase the power to identify disease/trait associated genes. Also, imputation of ungenotyped markers using known reference panels(e.g. HapMap and the 1000 Genomes Project) predicts genotypes at the missing or untyped markers thereby allowing one to accurately evaluate the evidence for association at genetic markers that are not directly genotyped (in addition to the typed markers) and has been shown to improve the power of GWAS to detect disease associated loci.",
            "score": 89.50691986083984
        },
        {
            "docid": "13967547_5",
            "document": "Dry lab . As a means of surpassing the limitations of these techniques, projects such as Folding@home and Rosetta@home are aimed at resolving this problem using computational analysis, this means of resolving protein structure is referred to as protein structure prediction. Although many labs have a slightly different approach, the main concept is to find, from a myriad of protein conformations, which conformation has the lowest energy or, in the case of Folding@Home, to find relatively low energies of proteins that could cause the protein to misfold and aggregate other proteins to itself\u2014like in the case of sickle cell anemia. The general scheme in these projects is that a small number of computations are parsed to, or sent to be calculated on, a computer, generally a home computer, and then that computer analyzes the likelihood that a specific protein will take a certain shape or conformation based on the amount of energy required for that protein to stay in that shape, this way of processing data is what is generally referred to as distributed computing. This analysis is done on an extraordinarily large number of different conformations, owing to the support of hundreds of thousands of home-based computers, in hopes to find the conformation of lowest possible energy or set of conformations of lowest possible energy relative to any conformations that are just slightly different. Although doing so is quite difficult, one can, by observing the energy distribution of a large number of conformations, despite the almost infinite number of different protein conformations possible for any given protein (see Levinthal Paradox), with a reasonably large number of protein energy samplings, predict relatively closely what conformation, within a range of conformations, has the expected lowest energy using methods in statistical inference. There are other factors such as salt concentration, pH, ambient temperature or chaperonins, which are proteins that assist in the folding process of other proteins, that can greatly affect how a protein folds. However, if the given protein is shown to fold on its own, especially in vitro, these findings can be further supported. Once we can see how a protein folds then we can see how it works as a catalyst, or in intracellular communication, e.g. neuroreceptor-neurotransmitter interaction. How certain compounds may be used to enhance or prevent the function of these proteins and how an elucidated protein overall plays a role in diseases such as Alzheimer's Disease or Huntington's Disease can also be much better understood.",
            "score": 88.5201416015625
        },
        {
            "docid": "33470882_5",
            "document": "Risk adjusted mortality rate . In medical science, RAMR could be a predictor of mortality that takes into account the predicted risk for a group of patients. For example, for a group of patients first we need to find the observed mortality rates for all the hospitals of interest. Then we can build/construct a model or use an existing model to predict mortality rates for each of the hospitals. It is expected that the number of patients in each hospital will be different and hence we need an overall (weighted) mortality rate for all these hospitals. Once we have the above three rates, then we can utilize the above formula to find the risk adjusted mortality rate which will reflect the actual mortality rate of a particular hospital without being biased from the observed mortality.",
            "score": 87.7486801147461
        },
        {
            "docid": "52142704_3",
            "document": "Polygenic score . Polygenic scores are widely employed in animal, plant, and behavioral genetics for predicting and understanding genetic architectures. In a genome-wide association study (GWAS), polygenic scores having substantially higher predictive performance than the genome-wide statistically-significant hits indicates that the trait in question is affected by a larger number of variants than just the hits and larger sample sizes will yield more hits; a conjunction of low variance explained and high heritability as measured by GCTA, twin studies or other methods, indicates that a trait may be massively polygenic and affected by thousands of variants. Once a polygenic score has been created, which explains at least a few percent of a phenotype's variance and can therefore be assumed to effectively incorporate a significant fraction of the genetic variants affecting that phenotype, it can be used in several different ways: as a lower bound to test whether heritability estimates may be biased; as a measure of genetic overlap of traits (genetic correlation), which might indicate e.g. shared genetic bases for groups of mental disorders; as a means to assess group differences in a trait such as height, or to examine changes in a trait over time due to natural selection indicative of a soft selective sweep (as e.g. for intelligence where the changes in frequency would be too small to detect on each individual hit but not on the overall polygenic score); in Mendelian randomization (assuming no pleiotropy with relevant traits); to detect & control for the presence of genetic confounds in outcomes (e.g. the correlation of schizophrenia with poverty); or to investigate gene\u2013environment interactions.",
            "score": 86.61688232421875
        },
        {
            "docid": "7952689_13",
            "document": "Threshold model . In a genetic context, the variables are all the genes and different environmental conditions, which protect against or increase the risk of a disease, and the threshold \"z\" is the biological limit past which disease develops. The threshold can be estimated from population prevalence of the disease (which is usually low). Because the threshold is defined relative to the population & environment, the liability score is generally considered as a N(0, 1) normally distributed random variable.",
            "score": 86.6156234741211
        },
        {
            "docid": "6021781_31",
            "document": "HIV/AIDS in China . These estimates assumed substantial spread of the virus from high-risk groups to the general population. Yet, trends from sentinel surveillance of pregnant women in high-risk areas might indicate that such spread may not have occurred. Another study showed, however, that 43% of the tested infected people were from low-risk groups. More recently, China AIDS Info reported that \"HIV infection has caused a 75% increase in the worldwide mortality rate for newborns\" and quoted a case in China. It is discussed, whether these predictions may have been made on unfounded assumptions. Some have argued that the effect of the high predictions have drawn attention and resources away from areas of greater need. For example, China's burden of disease from tobacco use is enormous. Others argue that due to the large number of cases of undiagnosed infections HIV testing must be introduced via \"anonymous surveillance and voluntary counselling and testing in order to reduce transmission\".",
            "score": 85.54641723632812
        },
        {
            "docid": "406880_13",
            "document": "Odds ratio . We may already note that if the disease is rare, then OR\u00a0\u2248\u00a0RR. Indeed, for a rare disease, we will have formula_18 and so formula_19 but then formula_20 in other words, for the exposed population, the risk of developing the disease is approximately equal to the odds. Analogous reasoning shows that the risk is approximately equal to the odds for the non-exposed population as well; but then the \"ratio\" of the risks, which is RR, is approximately equal to the ratio of the odds, which is OR. Or, we could just notice that the rare disease assumption says that formula_21 and formula_22 from which it follows that formula_23 in other words that the denominators in the final expressions for the RR and the OR are approximately the same. The numerators are exactly the same, and so, again, we conclude that\u00a0OR\u00a0\u2248\u00a0RR. Returning to our hypothetical study, the problem we often face is that we may not have the data to estimate these four numbers. For example, we may not have the population-wide data on who did or did not have the childhood injury.",
            "score": 85.4324951171875
        },
        {
            "docid": "637199_17",
            "document": "Automatic summarization . Beginning with the work of Turney, many researchers have approached keyphrase extraction as a supervised machine learning problem. Given a document, we construct an example for each unigram, bigram, and trigram found in the text (though other text units are also possible, as discussed below). We then compute various features describing each example (e.g., does the phrase begin with an upper-case letter?). We assume there are known keyphrases available for a set of training documents. Using the known keyphrases, we can assign positive or negative labels to the examples. Then we learn a classifier that can discriminate between positive and negative examples as a function of the features. Some classifiers make a binary classification for a test example, while others assign a probability of being a keyphrase. For instance, in the above text, we might learn a rule that says phrases with initial capital letters are likely to be keyphrases. After training a learner, we can select keyphrases for test documents in the following manner. We apply the same example-generation strategy to the test documents, then run each example through the learner. We can determine the keyphrases by looking at binary classification decisions or probabilities returned from our learned model. If probabilities are given, a threshold is used to select the keyphrases. Keyphrase extractors are generally evaluated using precision and recall. Precision measures how many of the proposed keyphrases are actually correct. Recall measures how many of the true keyphrases your system proposed. The two measures can be combined in an F-score, which is the harmonic mean of the two (\"F\"\u00a0=\u00a02\"PR\"/(\"P\"\u00a0+\u00a0\"R\") ). Matches between the proposed keyphrases and the known keyphrases can be checked after stemming or applying some other text normalization.",
            "score": 84.31774139404297
        },
        {
            "docid": "24235330_23",
            "document": "Behavioural genetics . On the other hand, when assessing effects according to the formula_35 metric, there are a large number of genetic variants that have very large effects on complex behavioural phenotypes. The risk alleles within such variants are exceedingly rare, such that their large behavioural effects impact only a small number of individuals. Thus, when assessed at a population level using the formula_33 metric, they account for only a small amount of the differences in risk between individuals in the population. Examples include variants within \"APP\" that result in familial forms of severe early onset Alzheimer's disease but affect only relatively few individuals. Compare this to risk alleles within \"APOE\", which pose much smaller risk compared to \"APP\", but are far more common and therefore affect a much greater proportion of the population.",
            "score": 83.89397430419922
        },
        {
            "docid": "42327240_3",
            "document": "Dignity of risk . The concept was first articulated in a 1972 article \"The dignity of risk and the mentally retarded\" by Robert Perske: Overprotection may appear on the surface to be kind, but it can be really evil. An oversupply can smother people emotionally, squeeze the life out of their hopes and expectations, and strip them of their dignity. Overprotection can keep people from becoming all they could become. Many of our best achievements came the hard way: We took risks, fell flat, suffered, picked ourselves up, and tried again. Sometimes we made it and sometimes we did not. Even so, we were given the chance to try. Persons with special needs need these chances, too. Of course, we are talking about prudent risks. People should not be expected to blindly face challenges that, without a doubt, will explode in their faces. Knowing which chances are prudent and which are not \u2013 this is a new skill that needs to be acquired. On the other hand, a risk is really only when it is not known beforehand whether a person can succeed. The real world is not always safe, secure, and predictable, it does not always say \u201cplease,\u201d \u201cexcuse me\u201d, or \u201cI\u2019m sorry\u201d. Every day we face the possibility of being thrown into situations where we will have to risk everything\u200a\u2026\u2002In the past, we found clever ways to build avoidance of risk into the lives of persons living with disabilities. Now we must work equally hard to help find the proper amount of risk these people have the right to take. We have learned that there can be healthy development in risk taking and there can be crippling indignity in safety!",
            "score": 83.63646697998047
        },
        {
            "docid": "833690_3",
            "document": "Tolerance interval . A tolerance interval can be seen as a statistical version of a probability interval. \"In the parameters-known case, a 95% tolerance interval and a 95% prediction interval are the same.\" If we knew a population's exact parameters, we would be able to compute a range within which a certain proportion of the population falls. For example, if we know a population is normally distributed with mean formula_1 and standard deviation formula_2, then the interval formula_3 includes 95% of the population (1.96 is the z-score for 95% coverage of a normally distributed population).",
            "score": 83.38592529296875
        },
        {
            "docid": "42630847_3",
            "document": "Receiver Operating Characteristic Curve Explorer and Tester . Biomarkers are commonly defined as measured characteristics that may be used as indicators of some biological state or condition. They may be genes, chemicals, proteins, physiological parameters, imaging data or histological measurements. Biomarkers can consist of single components (i.e. blood glucose) or multiplc components (a biomarker panel such as acylcarnitines). Medical biomarkers fall into 5 major categories: 1) diagnostic (used to identify if you have a disease or condition); 2) prognostic (used to determine how well you will do with the disease or condition); 3) predictive (used to determine if you may get the disease); 4) efficacy or monitoring (used to determine how well a drug or treatment is doing in fighting the disease) and 5) exposure (used to determine if you have been exposed to a drug, food, toxin or other kind of substance). Good biomarkers should exhibit good sensitivity (the fraction of correctly identified true positives) and good specificity (the fraction of correctly identified true negatives). A perfect biomarker or biomarker panel would be 100% sensitive (predict all people in the sick group as being sick) and 100% specific (not predicting anyone from the healthy group as being sick). However, since few things in life are perfect, there is often a trade-off between sensitivity and specificity. In medical biomarker studies it is becoming increasingly common to report this tradeoff in sensitivity and specificity using a Receiver Operating Characteristic (ROC) curve. ROC curves plot the sensitivity of a biomarker on the y axis, against the false discovery rate (1- specificity) on the x axis. An image of different ROC curves is shown in Figure 1. ROC curves provide a simple visual method for one to determine the boundary limit (or the separation threshold) of a biomarker or a combination of biomarkers for the optimal combination of sensitivity and specificity. The AUC (area under the curve) of the ROC curve reflects the overall accuracy and the separation performance of the biomarker (or biomarkers), and can be readily used to compare different biomarker combinations or models. As a rule of thumb, the fewer the biomarkers that one uses to maximize the AUC of the ROC curve, the better.",
            "score": 82.50675201416016
        },
        {
            "docid": "4141563_35",
            "document": "Predictive analytics . Once the model has been estimated we would be interested to know if the predictor variables belong in the model\u2014i.e. is the estimate of each variable's contribution reliable? To do this we can check the statistical significance of the model's coefficients which can be measured using the t-statistic. This amounts to testing whether the coefficient is significantly different from zero. How well the model predicts the dependent variable based on the value of the independent variables can be assessed by using the R\u00b2 statistic. It measures predictive power of the model i.e. the proportion of the total variation in the dependent variable that is \"explained\" (accounted for) by variation in the independent variables.",
            "score": 81.66829681396484
        },
        {
            "docid": "7952689_12",
            "document": "Threshold model . The liability-threshold model is a threshold model of categorical (usually binary) outcomes in which a large number of variables are summed to yield an overall 'liability' score; the observed outcome is determined by whether the latent score is smaller or larger than the threshold. The liability-threshold model is frequently employed in medicine and genetics to model risk factors contributing to disease.",
            "score": 81.29192352294922
        },
        {
            "docid": "27864034_25",
            "document": "Fuzzy-trace theory . Like other people, clinicians apply cognitive heuristics and fall into systematic errors which affect decisions in everyday life. Research has shown that patients and their physicians have difficulty understanding a host of numerical concepts, especially risks and probabilities, and this often implies some problems with numeracy, or mathematical proficiency. For example, physicians and patients both demonstrate great difficulty understanding the probabilities of certain genetic risks and were prone to the same errors, despite vast differences in medical knowledge. Though traditional dual process theory generally predicts that decisions made by computation are superior to those made by intuition, FTT assumes the opposite: that intuitive processing is more sophisticated and is capable of making better decisions, and that increases in expertise are accompanied by reliance on intuitive, gist-based reasoning rather than on literal, verbatim reasoning. FTT predicts that simply educating people with statistics regarding risk factors can hinder prevention efforts. Due to low prevalence of HIV or cancer, for example, people tend to overestimate their risks, and consequently interventions stressing the actual numbers may move people toward complacency as opposed to risk reduction. When women learn that their actual risks for breast cancer are lower than they thought, they return for screening at a lower rate. Also, some interventions to discourage adolescent drug use by presenting the risks have been shown to be ineffective or can even backfire.",
            "score": 81.19796752929688
        },
        {
            "docid": "14402695_7",
            "document": "Personal genomics . Disease risk may be calculated based on genetic markers and genome-wide association studies for common medical conditions, which are multifactorial and include environmental components in the assessment. Diseases which are individually rare (less than 200,000 people affected in the USA) are nevertheless collectively common (affecting roughly 8-10% of the US population). Over 2500 of these diseases (including a few more common ones) have predictive genetics of sufficiently high clinical impact that they are recommended as medical genetic tests available for single genes (and in whole genome sequencing) and growing at about 200 new genetic diseases per year.",
            "score": 80.69273376464844
        },
        {
            "docid": "12229900_5",
            "document": "Genetic exceptionalism . There is ongoing debate over whether or when certain genetic information should be considered exceptional. In some cases, the predictive power of genetic information (such as a risk for a disease like Huntington's Disease, which is highly penetrant) may justify special considerations for genetic exceptionalism, in that individuals with a high risk for developing this condition may face a certain amount of discrimination. However, for most common human health conditions, a specific genetic variant only plays a partial role, interacting with other genetic variants and environmental and lifestyle influences to contribute to disease development. In these cases, genetic information is often considered similarly to other medical and lifestyle data, such as smoking status, age, or biomarkers.",
            "score": 80.4128646850586
        },
        {
            "docid": "736803_29",
            "document": "Expected utility hypothesis . Often people refer to \"risk\" in the sense of a potentially quantifiable entity. In the context of mean-variance analysis, variance is used as a risk measure for portfolio return; however, this is only valid if returns are normally distributed or otherwise jointly elliptically distributed, or in the unlikely case in which the utility function has a quadratic form. However, David E. Bell proposed a measure of risk which follows naturally from a certain class of von Neumann-Morgenstern utility functions. Let utility of wealth be given by formula_27 for individual-specific positive parameters \"a\" and \"b\". Then expected utility is given by Thus the risk measure is formula_29, which differs between two individuals if they have different values of the parameter formula_30, allowing different people to disagree about the degree of risk associated with any given portfolio. See also Entropic risk measure.",
            "score": 79.81792449951172
        },
        {
            "docid": "610184_10",
            "document": "Eric Lander . Sequence data is just that: a list of bases found in a given stretch of DNA. Its value lies in the discoveries and new technologies it allows. In Lander's case, one of these applications is the study of disease. He is the founder and director of the Broad Institute, a collaboration between MIT, Harvard, the Whitehead institute and affiliated hospitals. Its goal is \"to create tools for genome medicine and make them broadly available to the scientific community; to apply these tools to propel the understanding and treatment of disease\". To this end they are studying the variation in the human genome and have led an international effort which has assembled a library of 2.1 million single-nucleotide polymorphisms (SNP). These act as markers or signposts in the genome allowing the identification of disease susceptibility genes. They hope to construct a map of the human genome using blocks of these SNP called Linkage disequilibrium or LD. This map will be of significant help in medical genetics. It will allow researchers to link a given condition to a given gene or set of genes using the LD as a marker. This will allow for improved diagnostic procedures. Lander and his colleagues are hoping the LD map will allow them to test the Common Disease-Common Variant hypothesis which states that many common diseases may be caused by a small number of common alleles, for example 50% of the variance in susceptibility to Alzheimer's disease is explained by the common allele ApoE4. Lander's group have recently discovered an important association that accounts for a large proportion of population risk for adult onset diabetes.",
            "score": 79.6056137084961
        },
        {
            "docid": "7013774_68",
            "document": "Omnibus test . First we define the test statistic as the deviate which indicates testing the ratio: While the saturated model is a model with a theoretically perfect fit. Given that deviance is a measure of the difference between a given model and the saturated model, smaller values indicate better fit as the fitted model deviates less from the saturated model. When assessed upon a chi-square distribution, non-significant chi-square values indicate very little unexplained variance and thus, good model fit. Conversely, a significant chi-square value indicates that a significant amount of the variance is unexplained.  Two measures of deviance D are particularly important in logistic regression: null deviance and model deviance. The null deviance represents the difference between a model with only the intercept and no predictors and the saturated model. And, the model deviance represents the difference between a model with at least one predictor and the saturated model.[3] In this respect, the null model provides a baseline upon which to compare predictor models. Therefore, to assess the contribution of a predictor or set of predictors, one can subtract the model deviance from the null deviance and assess the difference on a chi-square distribution with one degree of freedom. If the model deviance is significantly smaller than the null deviance then one can conclude that the predictor or set of predictors significantly improved model fit. This is analogous to the F-test used in linear regression analysis to assess the significance of prediction.  In most cases, the exact distribution of the likelihood ratio corresponding to specific hypotheses is very difficult to determine. A convenient result, attributed to Samuel S. Wilks, says that as the sample size n approaches the test statistic has asymptotically distribution with degrees of freedom equal to the difference in dimensionality of and parameters the \u03b2 coefficients as mentioned before on the omnibus test. e.g., if n is large enough and if the fitted model assuming the null hypothesis consist of 3 predictors and the saturated ( full ) model consist of 5 predictors, the Wilks' statistic is approximately distributed ( with 2 degrees of freedom). This means that we can retrieve the critical value C from the chi squared with 2 degrees of freedom under a specific significance level.",
            "score": 79.33301544189453
        },
        {
            "docid": "339553_32",
            "document": "Diagnosis of HIV/AIDS . Of course, the actual numbers vary depending on the testing population. This is because interpreting of the results of any medical test (assuming no test is 100% accurate) depends upon the initial degree of belief, or the prior probability that an individual has, or does not have a disease. Generally the prior probability is estimated using the prevalence of a disease within a population or at a given testing location. The positive predictive value and negative predictive value of all tests, including HIV tests, take into account the prior probability of having a disease along with the accuracy of the testing method to determine a new degree of belief that an individual has or does not have a disease (also known as the posterior probability). The chance that a positive test accurately indicates an HIV infection increases as the prevalence or rate of HIV infection increases in the population. Conversely, the negative predictive value will decrease as the HIV prevalence rises. Thus a positive test in a high-risk population, such as people who frequently engage in unprotected anal intercourse with unknown partners, is more likely to correctly represent HIV infection than a positive test in a very low-risk population, such as unpaid blood donors.",
            "score": 79.19861602783203
        },
        {
            "docid": "23913383_8",
            "document": "Indiana Harbor Belt Railroad Co. v. American Cyanamid Co. . Circuit Judge Posner begins by stating that this case presents a case of first impression so there is no precedent directly governing the disposition of the case, and leaving the court to decide the case on the basic principles underlying Illinois tort law. To address the question of whether strict liability is appropriate in this case, Posner turns to several foundational 19th century cases, including \"Rylands v. Fletcher\" (1868) and \"Guille v. Swan\" (1822), and then consults the Restatement (Second) of Torts. Section 520 of the Restatement sets out six factors for determining when strict liability is appropriate: \"Guille is a paradigmatic case for strict liability. (a) The risk (probability) of harm was great, and (b) the harm that would ensue if the risk materialized could be .. great ... The confluence of these two factors established the urgency of seeking to prevent such accidents. (c) Yet such accidents could not be prevented by the exercise of due care ... (d) The activity [is] not a matter of common usage ... (e) The activity [is] inappropriate to the place in which it took place ... [and], (f) Reinforcing(d), [is] the value to the community of the activity great enough to offset its unavoidable risks[?]\" Posner explains the relationship between negligence and strict liability as follows: The baseline common law regime of tort liability is negligence. When it is a workable regime, because the hazards of an activity can be avoided by being careful (which is to say, nonnegligent), there is no need to switch to strict liability. Sometimes, however, a particular type of accident cannot be prevented by taking care but can be avoided, or its consequences minimized, by shifting the activity in which the accident occurs to another locale, where the risk or harm of an accident will be less..., or by reducing the scale of the activity in order to minimize the number of accidents caused by it... By making the actor strictly liable\u2014by denying him in other words an excuse based on his inability to avoid accidents by being more careful\u2014we give him an incentive, missing in a negligence regime, to experiment with methods of preventing accidents that involve not greater exertions of care, assumed to be futile, but instead relocating, changing, or reducing (perhaps to the vanishing point) the activity giving rise to the accident... The greater the risk of an accident...and the costs of an accident if one occurs..., the more we want the actor to consider the possibility of making accident-reducing activity changes; the stronger, therefore, is the case for strict liability.",
            "score": 79.18492889404297
        },
        {
            "docid": "167008_11",
            "document": "Incidence (epidemiology) . Incidence should not be confused with prevalence, which is the proportion of cases in the population at a given time rather than rate of occurrence of new cases. Thus, incidence conveys information about the risk of contracting the disease, whereas prevalence indicates how widespread the disease is. Prevalence is the proportion of the total number of cases to the total population and is more a measure of the burden of the disease on society with no regard to time at risk or when subjects may have been exposed to a possible risk factor. Prevalence can also be measured with respect to a specific subgroup of a population (see: denominator data). Incidence is usually more useful than prevalence in understanding the disease etiology: for example, if the incidence rate of a disease in a population increases, then there is a risk factor that promotes the incidence.",
            "score": 79.15242004394531
        },
        {
            "docid": "30487688_23",
            "document": "Diagnosis of schizophrenia . Estimates of the heritability of schizophrenia is around 80%, which implies that 80% of the individual differences in risk to schizophrenia is explained by individual differences in genetics. Although many genetic variants associated with schizophrenia have been identified, their effects are usually very small, so they are combined onto a polygenic risk score. These scores, despite accounting for hundreds of variants, only explain up to 6% in symptom variation and 7% of the risk for developing the disease. An example of a well-studied genetic biomarker in schizophrenia is the single nucleotide polymorphism in the HLA-DQB1 gene, which is part of the human leukocyte antigen (HLA) complex. A G to C replacement on position 6672 predicts risk of agranulocytosis, a side effect of clozapine that can be fatal.",
            "score": 79.02909088134766
        }
    ]
}