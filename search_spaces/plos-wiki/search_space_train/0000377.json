{
    "q": [
        {
            "docid": "58894_21",
            "document": "Phosphorylation . Elucidating complex signaling pathway phosphorylation events can be difficult. In cellular signaling pathways, protein A phosphorylates protein B, and B phosphorylates C. However, in another signaling pathway, protein D phosphorylates A, or phosphorylates protein C. Global approaches such as phosphoproteomics, the study of phosphorylated proteins, which is a sub-branch of proteomics, combined with mass spectrometry-based proteomics, have been utilised to identify and quantify dynamic changes in phosphorylated proteins over time. These techniques are becoming increasingly important for the systematic analysis of complex phosphorylation networks. They have been successfully used to identify dynamic changes in the phosphorylation status of more than 6000 sites after stimulation with epidermal growth factor. Another approach for understanding Phosphorylation Network, is by measuring the genetic interactions between multiple phosphorylating proteins and their targets. This reveals interesting recurring patterns of interactions \u2013 network motifs. Computational methods have been developed to model phosphorylation networks and predict their responses under different perturbations.",
            "score": 21.816598415374756
        },
        {
            "docid": "1181008_10",
            "document": "Computational science . Exciting new developments in biotechnology are now revolutionizing biology and biomedical research. Examples of these techniques are high-throughput sequencing, high-throughput quantitative PCR, intra-cellular imaging, in-situ hybridization of gene expression, three-dimensional imaging techniques like Light Sheet Fluorescence Microscopy and Optical Projection, (micro)-Computer Tomography. Given the massive amounts of complicated data that is generated by these techniques, their meaningful interpretation, and even their storage, form major challenges calling for new approaches. Going beyond current bioinformatics approaches, computational biology needs to develop new methods to discover meaningful patterns in these large data sets. Model-based reconstruction of gene networks can be used to organize the gene expression data in systematic way and to guide future data collection. A major challenge here is to understand how gene regulation is controlling fundamental biological processes like biomineralisation and embryogenesis. The sub-processes like gene regulation, organic molecules interacting with the mineral deposition process, cellular processes, physiology and other processes at the tissue and environmental levels are linked. Rather than being directed by a central control mechanism, biomineralisation and embryogenesis can be viewed as an emergent behavior resulting from a complex system in which several sub-processes on very different temporal and spatial scales (ranging from nanometer and nanoseconds to meters and years) are connected into a multi-scale system. One of the few available options to understand such systems is by developing a multi-scale model of the system.",
            "score": 24.24480676651001
        },
        {
            "docid": "39198919_8",
            "document": "Cancer systems biology . List of High-Throughput Technologies and the Data they generated, with representative databases and publications The computational approaches used in cancer systems biology include new mathematical and computational algorithms that reflect the dynamic interplay between experimental biology and the quantitative sciences. A cancer systems biology approach can be applied at different levels, from an individual cell to a tissue, a patient with a primary tumour and possible metastases, or to any combination of these situations. This approach can integrate the molecular characteristics of tumours at different levels (DNA, RNA, protein, epigenetic, imaging) and different intervals (seconds versus days) with multidisciplinary analysis. One of the major challenges to its success, besides the challenge posed by the heterogeneity of cancer per se, resides in acquiring high-quality data that describe clinical characteristics, pathology, treatment, and outcomes and integrating the data into robust predictive models",
            "score": 25.827605485916138
        },
        {
            "docid": "29330_2",
            "document": "Social dynamics . Social dynamics can refer to the behavior of groups that results from the interactions of individual group members as well to the study of the relationship between individual interactions and group level behaviors. The field of social dynamics brings together ideas from Economics, Sociology, Social Psychology, and other disciplines, and is a sub-field of complex adaptive systems or complexity science. The fundamental assumption of the field is that individuals are influenced by one another's behavior. The field is closely related to system dynamics. Like system dynamics, social dynamics is concerned with changes over time and emphasizes the role of feedbacks. However, in social dynamics individual choices and interactions are typically viewed as the source of aggregate level behavior, while system dynamics posits that the structure of feedbacks and accumulations are responsible for system level dynamics. Research in the field typically takes a behavioral approach, assuming that individuals are boundedly rational and act on local information. Mathematical and computational modeling are important tools for studying social dynamics. Because social dynamics focuses on individual level behavior, and recognizes the importance of heterogeneity across individuals, strict analytic results are often impossible. Instead, approximation techniques, such as mean field approximations from statistical physics, or computer simulations are used to understand the behaviors of the system. In contrast to more traditional approaches in economics, scholars of social dynamics are often interested in non-equilibrium, or dynamic, behavior. That is, behavior that changes over time.",
            "score": 24.151408910751343
        },
        {
            "docid": "46581687_2",
            "document": "Pathway analysis . In bioinformatics research, pathway analysis software is used to identify related proteins within a pathway or building pathway de novo from the proteins of interest. This is helpful when studying differential expression of a gene in a disease or analyzing any omics dataset with a large number of proteins. By examining the changes in gene expression in a pathway, its biological causes can be explored. Pathway is the term from molecular biology which depicts an artificial simplified model of a process within a cell or tissue. Typical pathway model starts with extracellular signaling molecule that activates a specific protein. Thus triggers a chain of protein-protein or protein-small molecule interactions. Pathway analysis helps to understand or interpret omics data from the point of view of canonical prior knowledge structured in the form of pathways diagrams. It allows finding distinct cell processes (), diseases or signaling pathways that are statistically associated with selection of differentially expressed genes between two samples. Often but erroneously pathway analysis is used as synonym for network analysis (functional enrichment analysis and gene set analysis).",
            "score": 13.942440509796143
        },
        {
            "docid": "32663259_7",
            "document": "Gregorio Weber . Gregorio Weber\u2019s original and lifelong motivation was to use fluorescence methods to probe the nature of proteins and in addition to his contributions to the fluorescence field, he was one of the true pioneers of protein dynamics. A study of his papers from the 1960s demonstrates that even then he regarded proteins as highly dynamic molecules. He rejected the view, common at that time after the appearance of the first x-ray structures, that proteins had a unique and rigid conformation. In an important innovation, he introduced the use of molecular oxygen to quench fluorescence in aqueous solutions, which led to the detection, for the first time and to the surprise of many, of the existence of fast fluctuations in protein structures on the nanosecond time scale. The impact of this work was shown by the increasing interest in experimental and theoretical work in protein dynamics, which followed. Weber\u2019s early description of proteins in solution as \u201ckicking and screaming stochastic molecules\u201d has, in recent years, been fully verified both from theoretical and experimental studies. These contributions were recognized by the American Chemical Society in 1986, which named Weber as the first recipient of Repligen Award for the Chemistry of Biological Processes. In the 1970s, initially in collaboration with H.G. Drickamer, Weber combined fluorescence and hydrostatic pressure methods to the study of molecular complexes and proteins. The initial system he thought to study was the complex formed by isoalloxazine and adenine, one of his original research interests. These observations confirmed the applicability of fluorescence and high-pressure techniques to problems of structure, and particularly dynamics, at the molecular level. Weber and collaborators demonstrated that most proteins made up of subunits can be dissociated by the application of hydrostatic pressure, and opened, in this way, a new method to study protein-protein interactions. In these studies, quite unexpected properties of protein aggregates were revealed and a new approach to problems in biology and medicine was opened by these observations. For example, Weber and his collaborators demonstrated the possibility of destroying the infectivity of viruses, without affecting their immunogenic capacity, by subjecting them to hydrostatic pressure, and thus opened the possibility of developing viral vaccines that contain, without covalent modification, all the antigens present in the original virus.",
            "score": 28.55924344062805
        },
        {
            "docid": "413102_6",
            "document": "Folding@home . Due to the complexity of proteins' conformation or configuration space (the set of possible shapes a protein can take), and limits in computing power, all-atom molecular dynamics simulations have been severely limited in the timescales which they can study. While most proteins typically fold in the order of milliseconds, before 2010 simulations could only reach nanosecond to microsecond timescales. General-purpose supercomputers have been used to simulate protein folding, but such systems are intrinsically costly and typically shared among many research groups. Further, because the computations in kinetic models occur serially, strong scaling of traditional molecular simulations to these architectures is exceptionally difficult. Moreover, as protein folding is a stochastic process and can statistically vary over time, it is challenging computationally to use long simulations for comprehensive views of the folding process. Protein folding does not occur in one step. Instead, proteins spend most of their folding time, nearly 96% in some cases, \"waiting\" in various intermediate conformational states, each a local thermodynamic free energy minimum in the protein's energy landscape. Through a process known as adaptive sampling, these conformations are used by Folding@home as starting points for a set of simulation trajectories. As the simulations discover more conformations, the trajectories are restarted from them, and a Markov state model (MSM) is gradually created from this cyclic process. MSMs are discrete-time master equation models which describe a biomolecule's conformational and energy landscape as a set of distinct structures and the short transitions between them. The adaptive sampling Markov state model method significantly increases the efficiency of simulation as it avoids computation inside the local energy minimum itself, and is amenable to distributed computing (including on GPUGRID) as it allows for the statistical aggregation of short, independent simulation trajectories. The amount of time it takes to construct a Markov state model is inversely proportional to the number of parallel simulations run, i.e., the number of processors available. In other words, it achieves linear parallelization, leading to an approximately four orders of magnitude reduction in overall serial calculation time. A completed MSM may contain tens of thousands of sample states from the protein's phase space (all the conformations a protein can take on) and the transitions between them. The model illustrates folding events and pathways (i.e., routes) and researchers can later use kinetic clustering to view a coarse-grained representation of the otherwise highly detailed model. They can use these MSMs to reveal how proteins misfold and to quantitatively compare simulations with experiments.",
            "score": 13.154994249343872
        },
        {
            "docid": "1795200_13",
            "document": "Candidate gene . Many studies have similarly used candidate genes as part of a multi-disciplinary approach to examining a trait or phenotype. One example of manipulating candidate genes can be seen in a study completed by Martin E. Feder on heat-shock proteins and their function in \"Drosophila melanogaster\". Feder designed a holistic approach to study Hsp70, a candidate gene that was hypothesized to play a role in how an organism adapted to stress. \"Drosophila\" is a highly useful model organism for studying this trait due to the way it can support a diverse number of genetic approaches for studying a candidate gene. The different approaches this study took included both genetically modifying the candidate gene (using site-specific homologous recombination and the expression of various proteins), as well as examining the natural variation of Hsp70. He concluded that the results of these studies gave a multi-faceted view of Hsp70. By engineering and modifying these candidate genes, they were able to confirm the ways in which this gene was linked to a change phenotype. Understanding the natural and historical context in which these phenotypes operate by examining the natural genome structure complemented this.",
            "score": 11.520840167999268
        },
        {
            "docid": "20155525_5",
            "document": "Cliodynamics . Many historical processes are dynamic (a dynamic process is one that changes with time). Populations increase and decline, economies expand and contract, states grow and collapse. A very common approach, which has proved its worth in innumerable applications (particularly, but not exclusively, in the natural sciences), consists of taking a holistic phenomenon and splitting it up into separate parts that are assumed to interact with each other. This is the dynamical systems approach, because the whole phenomenon is represented as a system consisting of several elements (or subsystems) that interact and change dynamically; that is, over time. In the dynamical systems approach, one sets out explicitly with mathematical formulae how different subsystems interact with each other. This mathematical description is the model of the system, and one can use a variety of methods to study the dynamics predicted by the model, as well as attempt to test the model by comparing its predictions with observed empirical, dynamic evidence. Cliodynamics is the application of this same approach to the social sciences in general and to the study of historical dynamics in particular.",
            "score": 20.124751567840576
        },
        {
            "docid": "1686272_39",
            "document": "Chemical biology . Posttranslational modification of proteins with phosphate groups has proven to be a key regulatory step throughout all biological systems. Phosphorylation events, either phosphorylation by protein kinases or dephosphorylation by phosphatases, result in protein activation or deactivation. These events have an immense impact on the regulation of physiological pathways, which makes the ability to dissect and study these pathways integral to understanding the details of cellular processes. There exist a number of challenges\u2014namely the sheer size of the phosphoproteome, the fleeting nature of phosphorylation events and related physical limitations of classical biological and biochemical techniques\u2014that have limited the advancement of knowledge in this area. A recent review provides a detailed examination of the impact of newly developed chemical approaches to dissecting and studying biological systems both in vitro and in vivo.",
            "score": 16.06188702583313
        },
        {
            "docid": "2592262_6",
            "document": "Protein subcellular localization prediction . Through the development of new approaches in computer science, coupled with an increased dataset of proteins of known localization, computational tools can now provide fast and accurate localization predictions for many organisms. This has resulted in subcellular localization prediction becoming one of the challenges being successfully aided by bioinformatics, and machine learning.",
            "score": 3.7848587036132812
        },
        {
            "docid": "10747136_3",
            "document": "Max Planck Institute of Molecular Plant Physiology . The MPIP's mission is to study the dynamics of plant metabolism in the context of the plant system as a whole. Since this system is more than a collection of genes and gene products, they focus our efforts on understanding how the individual components dynamically interact over time and under different conditions. By combining traditional biological approaches with techniques relevant to functional genomics, the institute is forming a holistic view of the structure, function, dynamics, and regulation of entire plant genomes, proteomes, and metabolomes.",
            "score": 28.069823265075684
        },
        {
            "docid": "25181073_8",
            "document": "Neuroscience of multilingualism . Insights into language storage in the brain have come from studying multilingual individuals afflicted with a form of aphasia. The symptoms and severity of aphasia in multilingual individuals depend on the number of languages the individual knows, what order they learned them, and thus have them stored in the brain, the age at which they learned them, how frequently each language is used, and how proficient the individual is in using those languages. Two primary theoretical approaches to studying and viewing multilingual aphasics exist\u2014the localizationalist approach and the dynamic approach. The localizationalist approach views different languages as stored in different regions of the brain, explaining why multilingual aphasics may lose one language they know, but not the other(s). The dynamical theory (or shared representation) approach suggests that the language system is supervised by a dynamic equilibrium between the existing language capabilities and the constant alteration and adaptation to the communicative requirements of the environment. The dynamic approach views the representation and control aspects of the language system as compromised as a result of brain damage to the brain's language regions. The dynamic approach offers a satisfactory explanation for the various recovery times of each of the languages the aphasic has had impaired or lost because of the brain damage. Recovery of languages varies across aphasic patients. Some may recover all lost or impaired languages simultaneously. For some, one language is recovered before the others. In others, an involuntary mix of languages occurs in the recovery process; they intermix words from the various languages they know when speaking. Research affirms with the two approaches combined into the amalgamated hypothesis, it states that while languages do share some parts of the brain, they can also be allotted to some separate areas that are neutral.",
            "score": 22.522806525230408
        },
        {
            "docid": "27051151_69",
            "document": "Big data . Much in the same line, it has been pointed out that the decisions based on the analysis of big data are inevitably \"informed by the world as it was in the past, or, at best, as it currently is\". Fed by a large number of data on past experiences, algorithms can predict future development if the future is similar to the past. If the systems dynamics of the future change (if it is not a stationary process), the past can say little about the future. In order to make predictions in changing environments, it would be necessary to have a thorough understanding of the systems dynamic, which requires theory. As a response to this critique Alemany Oliver and Vayre suggested to use \"abductive reasoning as a first step in the research process in order to bring context to consumers\u2019 digital traces and make new theories emerge\". Additionally, it has been suggested to combine big data approaches with computer simulations, such as agent-based models and Complex Systems. Agent-based models are increasingly getting better in predicting the outcome of social complexities of even unknown future scenarios through computer simulations that are based on a collection of mutually interdependent algorithms. Finally, use of multivariate methods that probe for the latent structure of the data, such as factor analysis and cluster analysis, have proven useful as analytic approaches that go well beyond the bi-variate approaches (cross-tabs) typically employed with smaller data sets.",
            "score": 26.49762797355652
        },
        {
            "docid": "10571004_5",
            "document": "Biological network inference . Genes are the nodes and the edges are directed. A gene serves as the source of a direct regulatory edge to a target gene by producing an RNA or protein molecule that functions as a transcriptional activator or inhibitor of the target gene. If the gene is an activator, then it is the source of a positive regulatory connection; if an inhibitor, then it is the source of a negative regulatory connection. Computational algorithms take as primary input data measurements of mRNA expression levels of the genes under consideration for inclusion in the network, returning an estimate of the network topology. Such algorithms are typically based on linearity, independence or normality assumptions, which must be verified on a case-by-case basis. Clustering or some form of statistical classification is typically employed to perform an initial organization of the high-throughput mRNA expression values derived from microarray experiments, in particular to select sets of genes as candidates for network nodes. The question then arises: how can the clustering or classification results be connected to the underlying biology? Such results can be useful for pattern classification \u2013 for example, to classify subtypes of cancer, or to predict differential responses to a drug (pharmacogenomics). But to understand the relationships between the genes, that is, to more precisely define the influence of each gene on the others, the scientist typically attempts to reconstruct the transcriptional regulatory network. This can be done by data integration in dynamic models supported by background literature, or information in public databases, combined with the clustering results. The modelling can be done by a Boolean network, by Ordinary differential equations or Linear regression models, e.g. Least-angle regression, by Bayesian network or based on Information theory approaches. For instance it can be done by the application of a correlation-based inference algorithm, as will be discussed below, an approach which is having increased success as the size of the available microarray sets keeps increasing",
            "score": 9.664315462112427
        },
        {
            "docid": "143533_20",
            "document": "Green fluorescent protein . The availability of GFP and its derivatives has thoroughly redefined fluorescence microscopy and the way it is used in cell biology and other biological disciplines. While most small fluorescent molecules such as FITC (fluorescein isothiocyanate) are strongly phototoxic when used in live cells, fluorescent proteins such as GFP are usually much less harmful when illuminated in living cells. This has triggered the development of highly automated live-cell fluorescence microscopy systems, which can be used to observe cells over time expressing one or more proteins tagged with fluorescent proteins. For example, GFP had been widely used in labelling the spermatozoa of various organisms for identification purposes as in \"Drosophila melanogaster\", where expression of GFP can be used as a marker for a particular characteristic. GFP can also be expressed in different structures enabling morphological distinction. In such cases, the gene for the production of GFP is incorporated into the genome of the organism in the region of the DNA that codes for the target proteins and that is controlled by the same regulatory sequence; that is, the gene's regulatory sequence now controls the production of GFP, in addition to the tagged protein(s). In cells where the gene is expressed, and the tagged proteins are produced, GFP is produced at the same time. Thus, only those cells in which the tagged gene is expressed, or the target proteins are produced, will fluoresce when observed under fluorescence microscopy. Analysis of such time lapse movies has redefined the understanding of many biological processes including protein folding, protein transport, and RNA dynamics, which in the past had been studied using fixed (i.e., dead) material. Obtained data are also used to calibrate mathematical models of intracellular systems and to estimate rates of gene expression.",
            "score": 15.735411167144775
        },
        {
            "docid": "1053858_2",
            "document": "Functional genomics . Functional genomics is a field of molecular biology that attempts to make use of the vast wealth of data given by genomic and transcriptomic projects (such as genome sequencing projects and RNA sequencing) to describe gene (and protein) functions and interactions. Unlike structural genomics, functional genomics focuses on the dynamic aspects such as gene transcription, translation, regulation of gene expression and protein\u2013protein interactions, as opposed to the static aspects of the genomic information such as DNA sequence or structures. Functional genomics attempts to answer questions about the function of DNA at the levels of genes, RNA transcripts, and protein products. A key characteristic of functional genomics studies is their genome-wide approach to these questions, generally involving high-throughput methods rather than a more traditional \u201cgene-by-gene\u201d approach. The goal of functional genomics is to understand the function of larger numbers of genes or proteins, eventually all components of a genome. A more long-term goal is to understand the relationship between an organism's genome and its phenotype. The term functional genomics is often used broadly to refer to the many technical approaches to study an organism's genes and proteins, including the \"biochemical, cellular, and/or physiological properties of each and every gene product\" while some authors include the study of nongenic elements in his definition. Functional genomics may also include studies of natural genetic variation over time (such as an organism's development) or space (such as its body regions), as well as functional disruptions such as mutations.",
            "score": 16.74337661266327
        },
        {
            "docid": "10571004_4",
            "document": "Biological network inference . There is great interest in network medicine for the modelling biological systems. This article focuses on a necessary prerequisite to dynamic modeling of a network: inference of the topology, that is, prediction of the \"wiring diagram\" of the network. More specifically, we focus here on inference of biological network structure using the growing sets of high-throughput expression data for genes, proteins, and metabolites. Briefly, methods using high-throughput data for inference of regulatory networks rely on searching for patterns of partial correlation or conditional probabilities that indicate causal influence. Such patterns of partial correlations found in the high-throughput data, possibly combined with other supplemental data on the genes or proteins in the proposed networks, or combined with other information on the organism, form the basis upon which such algorithms work. Such algorithms can be of use in inferring the topology of any network where the change in state of one node can affect the state of other nodes.",
            "score": 15.097669839859009
        },
        {
            "docid": "19892153_2",
            "document": "Online machine learning . In computer science, online machine learning is a method of machine learning in which data becomes available in a sequential order and is used to update our best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. Online learning is a common technique used in areas of machine learning where it is computationally infeasible to train over the entire dataset, requiring the need of out-of-core algorithms. It is also used in situations where it is necessary for the algorithm to dynamically adapt to new patterns in the data, or when the data itself is generated as a function of time, e.g. stock price prediction. Online learning algorithms may be prone to catastrophic interference. This problem is tackled by incremental learning approaches.",
            "score": 16.126885890960693
        },
        {
            "docid": "55222888_64",
            "document": "Numerical modeling (geology) . The Fast Lagrangian Analysis of Continua (FLAC) is one of the most popular approaches in modeling crustal dynamics. The approach is \"fast\" as it solves the equations of momentum and continuity without using a matrix, hence it is fast but time steps must be small enough. The approach has been used in 2D, 2.5D, and 3D studies of crustal dynamics, in which the 2.5D results were generated by combining multiple slices of two-dimensional results.",
            "score": 16.817071199417114
        },
        {
            "docid": "10622545_9",
            "document": "James Collins (bioengineer) . Collins has pioneered the development and use of nonlinear dynamical approaches to study, mimic and improve biological function, and helped to transform biology into an engineering science. His current research interests include: synthetic biology - modeling, designing and constructing synthetic gene networks, and systems biology - reverse engineering naturally occurring gene regulatory networks.",
            "score": 19.78015422821045
        },
        {
            "docid": "152611_11",
            "document": "Cellular differentiation . Each specialized cell type in an organism expresses a subset of all the genes that constitute the genome of that species. Each cell type is defined by its particular pattern of regulated gene expression. Cell differentiation is thus a transition of a cell from one cell type to another and it involves a switch from one pattern of gene expression to another. Cellular differentiation during development can be understood as the result of a gene regulatory network. A regulatory gene and its cis-regulatory modules are nodes in a gene regulatory network; they receive input and create output elsewhere in the network. The systems biology approach to developmental biology emphasizes the importance of investigating how developmental mechanisms interact to produce predictable patterns (morphogenesis). (However, an alternative view has been proposed recently. Based on stochastic gene expression, cellular differentiation is the result of a Darwinian selective process occurring among cells. In this frame, protein and gene networks are the result of cellular processes and not their cause. See: Cellular Darwinism) A few evolutionarily conserved types of molecular processes are often involved in the cellular mechanisms that control these switches. The major types of molecular processes that control cellular differentiation involve cell signaling. Many of the signal molecules that convey information from cell to cell during the control of cellular differentiation are called growth factors. Although the details of specific signal transduction pathways vary, these pathways often share the following general steps. A ligand produced by one cell binds to a receptor in the extracellular region of another cell, inducing a conformational change in the receptor. The shape of the cytoplasmic domain of the receptor changes, and the receptor acquires enzymatic activity. The receptor then catalyzes reactions that phosphorylate other proteins, activating them. A cascade of phosphorylation reactions eventually activates a dormant transcription factor or cytoskeletal protein, thus contributing to the differentiation process in the target cell. Cells and tissues can vary in competence, their ability to respond to external signals.",
            "score": 12.946545600891113
        },
        {
            "docid": "4214_8",
            "document": "Bioinformatics . The primary goal of bioinformatics is to increase the understanding of biological processes. What sets it apart from other approaches, however, is its focus on developing and applying computationally intensive techniques to achieve this goal. Examples include: pattern recognition, data mining, machine learning algorithms, and visualization. Major research efforts in the field include sequence alignment, gene finding, genome assembly, drug design, drug discovery, protein structure alignment, protein structure prediction, prediction of gene expression and protein\u2013protein interactions, genome-wide association studies, the modeling of evolution and cell division/mitosis.",
            "score": 11.457364320755005
        },
        {
            "docid": "40158142_9",
            "document": "Nonlinear system identification . Artificial neural networks try loosely to imitate the network of neurons in the brain where computation takes place through a large number of simple processing elements. A typical neural network consists of a number of simple processing units interconnected to form a complex network. Layers of such units are arranged so that data is entered at the input layer and passes through either one or several intermediate layers before reaching the output layer. In supervised learning the network is trained by operating on the difference between the actual output and the desired output of the network, the prediction error, to change the connection strengths between the nodes. By iterating the weights are modified until the output error reaches an acceptable level. This process is called machine learning because the network adjusts the weights so that the output pattern is reproduced. Neural networks have been extensively studied and there are many excellent textbooks devoted to this topic in general, and more focussed textbooks which emphasise control and systems applications. There are two main problem types that can be studied using neural networks: static problems, and dynamic problems. Static problems include pattern recognition, classification, and approximation. Dynamic problems involve lagged variables and are more appropriate for system identification and related applications. Depending on the architecture of the network the training problem can be either nonlinear-in-the-parameters which involves optimisation or linear-in-the-parameters which can be solved using classical approaches. The training algorithms can be categorised into supervised, unsupervised, or reinforcement learning. Neural networks have excellent approximation properties but these are usually based on standard function approximation results using for example the Weierstrass Theorem that applies equally well to polynomials, rational functions, and other well-known models.  Neural networks have been applied extensively to system identification problems which involve nonlinear and dynamic relationships. However, classical neural networks are purely gross static approximating machines. There is no dynamics within the network. Hence when fitting dynamic models all the dynamics arise by allocating lagged inputs and outputs to the input layer of the network. The training procedure then produces the best static approximation that relates the lagged variables assigned to the input nodes to the output. There are more complex network architectures, including recurrent networks, that produce dynamics by introducing increasing orders of lagged variables to the input nodes. But in these cases it is very easy to over specify the lags and this can lead to over fitting and poor generalisation properties.  Neural networks have several advantages; they are conceptually simple, easy to train and to use, have excellent approximation properties, the concept of local and parallel processing is important and this provides integrity and fault tolerant behaviour. The biggest criticism of the classical neural network models is that the models produced are completely opaque and usually cannot be written down or analysed. It is therefore very difficult to know what is causing what, to analyse the model, or to compute dynamic characteristics from the model. Some of these points will not be relevant to all applications but they are for dynamic modelling.",
            "score": 27.29051411151886
        },
        {
            "docid": "219736_49",
            "document": "Indoor air quality . It is common to assume that buildings are simply inanimate physical entities, relatively stable over time. This implies that there is little interaction between the triad of the building, what is in it (occupants and contents), and what is around it (the larger environment). We commonly see the overwhelming majority of the mass of material in a building as relatively unchanged physical material over time. In fact, the true nature of buildings can be viewed as the result of a complex set of dynamic interactions among their physical, chemical, and biological dimensions. Buildings can be described and understood as complex systems. Research applying the approaches ecologists use to the understanding of ecosystems can help increase our understanding. \u201cBuilding ecology \u201c is proposed here as the application of those approaches to the built environment considering the dynamic system of buildings, their occupants, and the larger environment.",
            "score": 20.19185495376587
        },
        {
            "docid": "55170_23",
            "document": "Genomics . Functional genomics is a field of molecular biology that attempts to make use of the vast wealth of data produced by genomic projects (such as genome sequencing projects) to describe gene (and protein) functions and interactions. Functional genomics focuses on the dynamic aspects such as gene transcription, translation, and protein\u2013protein interactions, as opposed to the static aspects of the genomic information such as DNA sequence or structures. Functional genomics attempts to answer questions about the function of DNA at the levels of genes, RNA transcripts, and protein products. A key characteristic of functional genomics studies is their genome-wide approach to these questions, generally involving high-throughput methods rather than a more traditional \u201cgene-by-gene\u201d approach.",
            "score": 12.399847745895386
        },
        {
            "docid": "49313006_5",
            "document": "Brian J. Enquist . (1) \"Scaling and Functional Biology\" \u2013 Understanding the origin and diversity of organismal form, function, and diversity by developing general models for the origin of biological scaling laws. This research shows how general scaling laws and allometry, underlie organismal form, function, and diversity; physiological ecology and can be used to 'scale up' biological processes from genes to cells to ecosystems. (2) \"Macroecology\" \u2013 assessing the large scale biogeographic and evolutionary drivers of biological diversity and developing novel theoretical and informatics approaches that build from scaling principles and functional biology;  (3) \"Forecasting and Visualizing the Fate of Biological Diversity and Ecosystem Functioning.\" This work is building novel approaches to complex ecological problems \u2013 utilizing integrative computation, big data, statistical, and visualisation tools to visualize and analyze biological data and to assess how climate change will influence the distribution of diversity and functioning of forests and ecosystems.  His lab's research utilizes differing approaches including: developing theory and informatics infrastructure, field work, big datasets, scaling, empirically measuring numerous attributes of organismal form and function, utilizing physiological and trait-based techniques, and assessing macroecological and large-scale patterns. His collaborative group often works in contrasting environments including tropical forests, on elevation gradients, and in high alpine ecosystems.",
            "score": 15.134519577026367
        },
        {
            "docid": "51165385_11",
            "document": "TCP-seq . TCP-seq was designed to specifically target these blind spots. It can essentially provide the same level of details for elongation phase as ribosome (translation) profiling, but also includes recording of initiation, termination and recycling intermediates (and basically any other possible translation complexes as long as the ribosome or its subunits are contacting and protecting the mRNA) of protein synthesis that previously remained out of the reach. Therefore, TCP-seq provides a single approach for a complete insight into the translation process of a biological sample. This particular aspect of the method can be expected to be developed further as the dynamics of ribosomal scanning on mRNA during translation initiation is generally unknown for the most of life. Current dataset containing TCP-seq data for translation initiation is available for yeast \"Saccharomyces cerevisiae\", and likely to be extended for other organisms in the future.",
            "score": 11.203463792800903
        },
        {
            "docid": "42067251_16",
            "document": "Tumour heterogeneity . Mechanochemical heterogeneity is a hallmark of living eukaryotic cells. It has an impact on epigenetic gene regulation. The heterogeneous dynamic mechanochemical processes regulate interrelationships within the group of cellular surfaces through adhesion. Tumour development and spreading is accompanied by change in heterogeneous chaotic dynamics of mechanochemical interaction process in the group cells, including cells within tumour, and is hierarchical for the host of cancer patients. It is suggested that the heterogeneity of hypoxia in solid tumors is due to the mechanochemical reactions with oxygen nanobubbles. The biological phenomena of mechanochemical heterogeneity maybe used for differential gastric cancer diagnostics against patients with inflammation of gastric mucosa and for increasing antimetastatic activity of dendritic cells based on vaccines when mechanically heterogenized microparticles of tumor cells are used for their loading. There is also a possible methodical approach based on the simultaneous ultrasound imaging diagnostic techniques and therapy, regarding the mechanochemical effect on nanobubles conglomerates with drugs in the tumour.",
            "score": 15.473510026931763
        },
        {
            "docid": "15065584_17",
            "document": "JADE1 . The biological role of JADE1 has not been elucidated. Limited number of publications addresses this question using mice models. The most comprehensive study which was published in 2003, identified mice orthologue of human JADE1, Jade1, and investigated Jade1 expression during mice embryogenesis. Searching for developmentally regulated genes the authors used gene trap screen analysis and identified mouse Jade1 as gene strongly regulated during embryogenesis. Insertion of the vector into the third intron of the Jade1 gene lead to the production of a 47-amino-acid truncated protein. The gene trap insertional mutation resulted in Jade1-beta-galactosidase reporter fusion product and Jade1 null allele. While the homozygotes for the gene trap integration did not produce strong developmental phenotype, the fusion product revealed Jade1 gene spatial-temporal expression in mouse embryonic cells and tissues of developing embryo up to 15.5-d.p.c. In addition the study reports experimental and in silico comparative analysis of Jade1 mRNA transcripts, Jade1 gene structure and analysis of Jade1 protein orthologues from mouse human and zebra fish. Jade1 expression was detected in extraembryonic ectoderm and trophoblast, which are placental components important for vasculogenesis, as well as in sites enriched with multipotent or tissue-specific progenitors, including neural progenitors(2). The dynamics of Jade1 reporter expression in these areas indicates the involvement in the determination and elongation of anterior posterior axis, an important point of the study). The potential role for human JADE1 in the renewal of embryonic stem cell and embryonal carcinoma cell cultures was suggested in another screening study which showed that, in cultured stem cells activation of stem cell transcription factor OCT4 pathway upregulated JADE1 gene expression along with stem cell factors NANOG, PHC1, USP44 and SOX2. Role of JADE1 in epithelial cell proliferation was addressed in a murine model of acute kidney injury and regeneration. Expression patterns and dynamics of HBO1-JADE1S/L were examined in regenerating tubular epithelial cells. Ischemia and reperfusion injury resulted in an initial decrease in JADE1S, JADE1L, and HBO1 protein levels, which returned to the baseline during renal recovery. Expression levels of HBO1 and JADE1S recovered as cell proliferation rate reached maximum, whereas JADE1L recovered after bulk proliferation had diminished. The temporal expression of JADE1 correlated with the acetylation of histone H4 (H4K5 and H4K12) but not that of histone H3 (H4K14), suggesting that the JADE1-HBO1 complex specifically marks H4 during epithelial cell proliferation. The results of the study implicate JADE1-HBO1 complex in acute kidney injury and suggest distinct roles for JADE1 isoforms during epithelial cell recovery.",
            "score": 18.685099601745605
        },
        {
            "docid": "56784404_7",
            "document": "Active fluid . The mechanism behind the formation of various structures in active fluids is an area of active research. It is well understood that the structure formation in active fluids is intimately related to defects or disclinations in the order parameter field (the orientational order of the constituent agents). An important part of research on active fluids involve modelling of dynamics of these defects to study its role in pattern formation and turbulent dynamics in active fluids. Modified versions of Vicsek model are among earliest and continually used approach to model active fluids. Such models have been shown to capture the various dynamical states exhibited by active fluids. More refined approaches include derivation of continuum limit hydrodynamic equations for active fluids and adaptation of liquid crystal theory by including the activity terms.",
            "score": 16.175537824630737
        },
        {
            "docid": "55172_55",
            "document": "Proteomics . Advances in quantitative proteomics would clearly enable more in-depth analysis of cellular systems. Biological systems are subject to a variety of perturbations (cell cycle, cellular differentiation, carcinogenesis, environment (biophysical), etc.). Transcriptional and translational responses to these perturbations results in functional changes to the proteome implicated in response to the stimulus. Therefore, describing and quantifying proteome-wide changes in protein abundance is crucial towards understanding biological phenomenon more holistically, on the level of the entire system. In this way, proteomics can be seen as complementary to genomics, transcriptomics, epigenomics, metabolomics, and other -omics approaches in integrative analyses attempting to define biological phenotypes more comprehensively. As an example, \"The Cancer Proteome Atlas\" provides quantitative protein expression data for ~200 proteins in over 4,000 tumor samples with matched transcriptomic and genomic data from The Cancer Genome Atlas. Similar datasets in other cell types, tissue types, and species, particularly using deep shotgun mass spectrometry, will be an immensely important resource for research in fields like cancer biology, developmental and stem cell biology, medicine, and evolutionary biology.",
            "score": 15.165743827819824
        }
    ],
    "r": [
        {
            "docid": "1145683_25",
            "document": "Raymond Cattell . Rather than pursue a \"univariate\" research approach to psychology, studying the effect that a single variable (such as \"dominance\") might have on another variable (such as \"decision-making\"), Cattell pioneered the use of multivariate experimental psychology (the analysis of several variables simultaneously). He believed that behavioral dimensions were too complex and interactive to fully understand variables in isolation. The classical univariate approach required bringing the individual into an artificial laboratory situation and measuring the effect of one particular variable on another - also known as the \"bivariate\" approach, while the multivariate approach allowed psychologists to study the whole person and their unique combination of traits within a natural environmental context. Multivariate experimental research designs and multivariate statistical analyses allowed for the study of \"real-life\" situations (e.g., depression, divorce, loss) that could not be manipulated in an artificial laboratory environment.",
            "score": 40.895450592041016
        },
        {
            "docid": "41771352_5",
            "document": "Ignacio J. P\u00e9rez Arriaga . His contributions in the field of electrical engineering began with the completion of his PhD at MIT under the supervision of Prof. Fred Schweppe, in which he formulated a new technique for the dynamic analysis of small perturbations of electric power systems known as Selective Modal Analysis. This analytical framework allows approaching large and complex linear time-invariant dynamic system problems (such as dynamic stability analysis, or the determination of coherent generator groups and dynamic equivalents in transient stability studies) through a reduction technique that extracts the relevant quantitative and qualitative information. This framework was later applied intensively to solve different power system stability and control problems, such as the analysis of the oscillatory stability and control, subsynchronous resonance or multi-area analysis of small signal stability.",
            "score": 36.0029182434082
        },
        {
            "docid": "314204_7",
            "document": "Chemometrics . Multivariate analysis was a critical facet even in the earliest applications of chemometrics. The data resulting from infrared and UV/visible spectroscopy are often easily numbering in the thousands of measurements per sample. Mass spectrometry, nuclear magnetic resonance, atomic emission/absorption and chromatography experiments are also all by nature highly multivariate. The structure of these data was found to be conducive to using techniques such as principal components analysis (PCA), and partial least-squares (PLS). This is primarily because, while the datasets may be highly multivariate there is strong and often linear low-rank structure present. PCA and PLS have been shown over time very effective at empirically modeling the more chemically interesting low-rank structure, exploiting the interrelationships or \u2018latent variables\u2019 in the data, and providing alternative compact coordinate systems for further numerical analysis such as regression, clustering, and pattern recognition. Partial least squares in particular was heavily used in chemometric applications for many years before it began to find regular use in other fields.",
            "score": 35.5340690612793
        },
        {
            "docid": "19384_3",
            "document": "Multivariate statistics . Multivariate statistics concerns understanding the different aims and background of each of the different forms of multivariate analysis, and how they relate to each other. The practical application of multivariate statistics to a particular problem may involve several types of univariate and multivariate analyses in order to understand the relationships between variables and their relevance to the problem being studied.",
            "score": 34.55324172973633
        },
        {
            "docid": "41918927_2",
            "document": "Shearlet . In applied mathematical analysis, shearlets are a multiscale framework which allows efficient encoding anisotropic features in multivariate problem classes. Originally, shearlets were introduced in 2006 for the analysis as well as sparse approximation of functions formula_1. They are a natural extension of wavelets, to accommodate the fact that multivariate functions are typically governed by anisotropic features such as edges in images, since wavelets, as isotropic objects, are not capable of capturing such phenomena.",
            "score": 33.769493103027344
        },
        {
            "docid": "252582_25",
            "document": "Landscape ecology . Developments in landscape ecology illustrate the important relationships between spatial patterns and ecological processes. These developments incorporate quantitative methods that link spatial patterns and ecological processes at broad spatial and temporal scales. This linkage of time, space, and environmental change can assist managers in applying plans to solve environmental problems. The increased attention in recent years on spatial dynamics has highlighted the need for new quantitative methods that can analyze patterns, determine the importance of spatially explicit processes, and develop reliable models. Multivariate analysis techniques are frequently used to examine landscape level vegetation patterns. Studies use statistical techniques, such as cluster analysis, canonical correspondence analysis (CCA), or detrended correspondence analysis (DCA), for classifying vegetation. Gradient analysis is another way to determine the vegetation structure across a landscape or to help delineate critical wetland habitat for conservation or mitigation purposes (Choesin and Boerner 2002).",
            "score": 33.291534423828125
        },
        {
            "docid": "18831_43",
            "document": "Mathematics . Understanding and describing change is a common theme in the natural sciences, and calculus was developed as a powerful tool to investigate it. Functions arise here, as a central concept describing a changing quantity. The rigorous study of real numbers and functions of a real variable is known as real analysis, with complex analysis the equivalent field for the complex numbers. Functional analysis focuses attention on (typically infinite-dimensional) spaces of functions. One of many applications of functional analysis is quantum mechanics. Many problems lead naturally to relationships between a quantity and its rate of change, and these are studied as differential equations. Many phenomena in nature can be described by dynamical systems; chaos theory makes precise the ways in which many of these systems exhibit unpredictable yet still deterministic behavior.",
            "score": 32.19174575805664
        },
        {
            "docid": "3347734_2",
            "document": "Multivariate analysis . Multivariate analysis (MVA) is based on the statistical principle of multivariate statistics, which involves observation and analysis of more than one statistical outcome variable at a time. In design and analysis, the technique is used to perform trade studies across multiple dimensions while taking into account the effects of all variables on the responses of interest.",
            "score": 32.050987243652344
        },
        {
            "docid": "51865496_8",
            "document": "Crowd analysis . There are countless social applications of crowd analysis ranging from uses within the film and video game industries, to uses in public planning. Being that crowd simulations are based on group dynamics and crowd psychology, the accuracy and relevance to real life situations is clear. A large aspect of public planning and its use of crowd analysis lies within the realm of situational representations for emergency evacuation. Evacuations can be planned via the modeling and study of crowd interaction and reaction. These representations are based on biological models and patterns, thus the movements predicted are quite realistic. Similar models are utilized within motion picture industries to produce realistic and life-like simulations and scenes.  A system can generate a realistic crowd simulation with given inputs and simulate how the simulated moving objects, or agents, will interact with each other and with the environment. The goal is to replicate a crowd's movement patterns given a large number of agents in a given space. Algorithms based on crowd analysis attempt to manage the movement of the crowd. The more efficient and realistic a simulation becomes, the more complex the algorithm must become. The software must be able to manipulate the trajectory of individual agents based on variables such as the agents' goals, stress forces, obstacles, and levels of arousal. There are several software utilized to develop and study crowd dynamics:",
            "score": 32.00619888305664
        },
        {
            "docid": "23637389_5",
            "document": "Russell Lande . Lande is best known for his early work extending quantitative genetics theory to the context of evolutionary biology in natural populations. In particular, he developed a stochastic theory for the evolution of quantitative traits by genetic drift and natural selection. He also proposed a multivariate framework to describe the effect of selection on multiple correlated characters, thus helping clarify the much-debated notion of genetic constraints in phenotypic evolution. He later applied and extended these results to study a wide variety of topics in evolutionary biology, including: sexual selection, speciation, the evolution of phenotypic plasticity, of self-fertilization, of life history, of a species range in space and time.",
            "score": 31.86031723022461
        },
        {
            "docid": "23623750_11",
            "document": "ARC Centre for Complex Systems . This program applied complex systems science to the problem of improving the efficiency of air travel without compromising safety. To do this, researchers developed and used air traffic simulators to study new concepts and tools for air traffic management, and developed new approaches to assuarance of system-level properties including safety and efficiency In this program, complex systems and network theory was applied to economics and business to understand how evolutionary change occurs. Multi-agent modelling and associated simulation and calibration techniques were core components of the methodology used. We investigated new ways of testing for complex patterns in high frequency data, by studying trade-by trade data in stock markets and in electricity markets and seeking 'pattern matches' in artificially generated agent-based modelling data. We investigated new ways of dealing with spatial complexity in several contexts. Also visualisation techniques, rarely used in economics, were applied in a range of data-rich contexts to better understand the architecture and complex dynamics of systems.",
            "score": 31.229354858398438
        },
        {
            "docid": "1145683_7",
            "document": "Raymond Cattell . In regard to statistical methodology, in 1960 Cattell founded the Society of Multivariate Experimental Psychology (SMEP), and its journal \"Multivariate Behavioral Research\", in order to bring together, encourage, and support scientists interested in multi-variate research. He was an early and frequent user of factor analysis (a statistical procedure for finding underlying factors in data). Cattell also developed new factor analytic techniques, for example, by inventing the Scree Test, which uses the curve of latent roots to judge the optimal number of factors to extract. He also developed a new factor analysis rotation procedure\u2014the \"Procrustes\" or non-orthogonal rotation, designed to let the data itself determine the best location of factors, rather than requiring orthogonal factors. Additional contributions include the Coefficient of Profile Similarity (taking account of shape, scatter, and level of two score profiles); P-technique factor analysis based on repeated measurements of a single individual (sampling of variables, rather than sampling of persons); dR-technique factor analysis for elucidating change dimensions (including transitory emotional states, and longer-lasting mood states); the Taxonome program for ascertaining the number and contents of clusters in a data set; the Rotoplot program for attaining maximum simple structure factor pattern solutions. As well, he put forward the Dynamic Calculus for assessing interests and motivation, the Basic Data Relations Box (assessing dimensions of experimental designs), the group syntality construct (\"personality\" of a group), the triadic theory of cognitive abilities, the Ability Dimension Analysis Chart (ADAC), and Multiple Abstract Variance Analysis (MAVA), with \"specification equations\" to embody genetic and environmental variables and their interactions.",
            "score": 30.971435546875
        },
        {
            "docid": "615222_16",
            "document": "Multivariable calculus . Multivariable calculus can be applied to analyze deterministic systems that have multiple degrees of freedom. Functions with independent variables corresponding to each of the degrees of freedom are often used to model these systems, and multivariable calculus provides tools for characterizing the system dynamics.",
            "score": 30.91750717163086
        },
        {
            "docid": "8514693_6",
            "document": "Multivariate optical element . While each chemical problem presents its own unique challenges and opportunities, the design of a system for a specific analysis is complex and requires the assembly of several pieces of a spectroscopic puzzle. The data necessary for a successful design are spectral characteristics of light sources, detectors and a variety of optics to be used in the final assemblage, dispersion characteristics of the materials used in the wavelength range of interest, and a set of calibrated sample spectra for pattern-recognition-based analysis. With these pieces assembled, suitable application specific multivariate optical computer designs can be generated and the performance accurately modeled and predicted.",
            "score": 30.794418334960938
        },
        {
            "docid": "68686_22",
            "document": "Raman spectroscopy . Raman spectroscopy has a wide variety of applications in biology and medicine. It has helped confirm the existence of low-frequency phonons in proteins and DNA, promoting studies of low-frequency collective motion in proteins and DNA and their biological functions. Raman reporter molecules with olefin or alkyne moieties are being developed for tissue imaging with SERS-labeled antibodies. Raman spectroscopy has also been used as a noninvasive technique for real-time, in situ biochemical characterization of wounds. Multivariate analysis of Raman spectra has enabled development of a quantitative measure for wound healing progress. Spatially offset Raman spectroscopy (SORS), which is less sensitive to surface layers than conventional Raman, can be used to discover counterfeit drugs without opening their packaging, and to non-invasively study biological tissue. A huge reason why Raman spectroscopy is so useful in biological applications is because its results often do not face interference from water molecules, due to the fact that they have permanent dipole moments, and as a result, the Raman scattering cannot be picked up on. This is a large advantage, specifically in biological applications. Raman spectroscopy also has a wide usage for studying biominerals. Lastly, Raman gas analyzers have many practical applications, including real-time monitoring of anesthetic and respiratory gas mixtures during surgery.",
            "score": 30.726518630981445
        },
        {
            "docid": "371273_14",
            "document": "Speckle imaging . Speckle imaging in biology refers to the underlabeling of periodic cellular components (such as filaments and fibers) so that instead of appearing as a continuous and uniform structure, it appears as a discrete set of speckles. This is due to statistical distribution of the labeled component within unlabeled components. The technique, also known as dynamic speckle enables real-time monitoring of dynamical systems and video image analysis to understand biological processes.",
            "score": 30.603721618652344
        },
        {
            "docid": "20651606_7",
            "document": "Divided consciousness . New trends in psychology and cognitive neuroscience suggest that applications of nonlinear dynamics, chaos and self-organization seem to be particularly important for research of some fundamental problems regarding mind-brain relationship. Relevant problems among others are formations of memories during alterations of mental states and nature of a barrier that divides mental states, and leads to the process called dissociation. This process is related to a formation of groups of neurons which often synchronize their firing patterns in a unique spatial manner. The central theme of this study is the relationship between level of moving and oscillating mental processes and their neurophysiological substrate. This presents a question about principles of organization of the conscious experience and how the experiences happen in the brain. Chaotic self-organization provides a unique theoretical and experimental tool for deeper understanding of dissociative phenomena and enables to study how dissociative phenomena can be linked to epileptiform discharges which are related to various forms of psychological and somatic manifestations. Organizing principles that constitute human consciousness and other mental phenomena from this point of view may be described by analysis and reconstruction of underlying dynamics of psychophysiological measures.",
            "score": 30.294343948364258
        },
        {
            "docid": "2885691_75",
            "document": "Robust statistics . However, using these types of models to predict missing values or outliers in a long time series is difficult and often unreliable, particularly if the number of values to be in-filled is relatively high in comparison with total record length. The accuracy of the estimate depends on how good and representative the model is and how long the period of missing values extends. The in a case of a dynamic process, so any variable is dependent, not just on the historical time series of the same variable but also on several other variables or parameters of the process. In other words, the problem is an exercise in multivariate analysis rather than the univariate approach of most of the traditional methods of estimating missing values and outliers; a multivariate model will therefore be more representative than a univariate one for predicting missing values. The Kohonen self organising map (KSOM) offers a simple and robust multivariate model for data analysis, thus providing good possibilities to estimate missing values, taking into account its relationship or correlation with other pertinent variables in the data record.",
            "score": 30.281944274902344
        },
        {
            "docid": "52039299_4",
            "document": "Patrice Brun (archaeologist) . Patrice Brun\u2019s research covers a broad range of aspects present in all Europe, namely trade and exchanges, the settlement patterns, and dynamics of identity. This multivariate and multidisciplinary approach allows shedding the light on Patrice Brun\u2019s main focus: dynamics of social changes that led to the rise of State. His main contributions were on the \"Urnfield Culture\", the \"Hallstatt Princely Phenomenon\" in the Celtic territory in the North of the Alps, the origin of the Celts, the social meaning of funerary and non-funerary remains, along with the task specialization and the growing complexity of the european societies.",
            "score": 30.25691032409668
        },
        {
            "docid": "20786042_41",
            "document": "Cybernetics . Geocybernetics aims to study and control the complex co-evolution of ecosphere and anthroposphere, for example, for dealing with planetary problems such as anthropogenic global warming. Geocybernetics applies a dynamical systems perspective to Earth system analysis. It provides a theoretical framework for studying the implications of following different sustainability paradigms on co-evolutionary trajectories of the planetary socio-ecological system to reveal attractors in this system, their stability, resilience and reachability. Concepts such as tipping points in the climate system, planetary boundaries, the safe operating space for humanity and proposals for manipulating Earth system dynamics on a global scale such as geoengineering have been framed in the language of geocybernetic Earth system analysis.",
            "score": 30.133604049682617
        },
        {
            "docid": "2153191_2",
            "document": "Tone mapping . Tone mapping is a technique used in image processing and computer graphics to map one set of colors to another to approximate the appearance of high-dynamic-range images in a medium that has a more limited dynamic range. Print-outs, CRT or LCD monitors, and projectors all have a limited dynamic range that is inadequate to reproduce the full range of light intensities present in natural scenes. Tone mapping addresses the problem of strong contrast reduction from the scene radiance to the displayable range while preserving the image details and color appearance important to appreciate the original scene content.",
            "score": 30.072723388671875
        },
        {
            "docid": "3347734_4",
            "document": "Multivariate analysis . Multivariate analysis can be complicated by the desire to include physics-based analysis to calculate the effects of variables for a hierarchical \"system-of-systems\". Often, studies that wish to use multivariate analysis are stalled by the dimensionality of the problem. These concerns are often eased through the use of surrogate models, highly accurate approximations of the physics-based code. Since surrogate models take the form of an equation, they can be evaluated very quickly. This becomes an enabler for large-scale MVA studies: while a Monte Carlo simulation across the design space is difficult with physics-based codes, it becomes trivial when evaluating surrogate models, which often take the form of response-surface equations.",
            "score": 30.0383243560791
        },
        {
            "docid": "10435898_5",
            "document": "Multivariate testing in marketing . Testing can be carried out on a dynamically generated website by setting up the server to display the different variations of content in equal proportions to incoming visitors. Statistics on how each visitor went on to behave after seeing the content under test must then be gathered and presented. Outsourced services can also be used to provide multivariate testing on websites with minor changes to page coding. These services insert their content to predefined areas of a site and monitor user behavior.",
            "score": 29.98965072631836
        },
        {
            "docid": "22008131_6",
            "document": "Math 55 . Through 2006, the instructor had broad latitude in choosing the content of the course. Though Math 55 bore the official title \"Honors Advanced Calculus and Linear Algebra\", advanced topics in complex analysis, point set topology, group theory, and/or differential geometry could be covered in depth at the discretion of the instructor, in addition to single and multivariable real analysis and abstract linear algebra. In 1970, for example, students studied the differential geometry of Banach manifolds in the second semester of Math 55. In contrast, Math 25, entitled \"Honors Multivariable Calculus and Linear Algebra\", tended to be more narrowly focused, usually covering real analysis, together with the relevant theory of metric spaces and (multi)linear maps. These topics typically culminated in the proof of the generalized Stokes' theorem, though, time permitting, other relevant topics (e.g., category theory, de Rham cohomology) might also be covered. Although both courses presented calculus from a rigorous point of view and emphasized theory and proof writing, Math 55 was generally faster paced, more abstract, and demanded a higher level of mathematical sophistication.",
            "score": 29.935394287109375
        },
        {
            "docid": "8221717_2",
            "document": "Information-based complexity . Information-based complexity (IBC) studies optimal algorithms and computational complexity for the continuous problems which arise in physical science, economics, engineering, and mathematical finance. IBC has studied such continuous problems as path integration, partial differential equations, systems of ordinary differential equations, nonlinear equations, integral equations, fixed points, and very-high-dimensional integration. All these problems involve functions (typically multivariate) of a real or complex variable. Since one can never obtain a closed-form solution of the problems of interest one has to settle for a numerical solution. Since a function of a real or complex variable cannot be entered into a digital computer, the solution of continuous problems involves \"partial\" information. To give a simple illustration, in the numerical approximation of an integral, only samples of the integrand at a finite number of points are available. In the numerical solution of partial differential equations the functions specifying the boundary conditions and the coefficients of the differential operator can only be sampled. Furthermore, this partial information can be expensive to obtain. Finally the information is often \"contaminated\" by noise.",
            "score": 29.811113357543945
        },
        {
            "docid": "39783039_2",
            "document": "Function of several real variables . In mathematical analysis, and applications in geometry, applied mathematics, engineering, natural sciences, and economics, a function of several real variables or real multivariate function is a function with more than one argument, with all arguments being real variables. This concept extends the idea of a function of a real variable to several variables. The \"input\" variables take real values, while the \"output\", also called the \"value of the function\", may be real or complex. However, the study of the complex valued functions may be easily reduced to the study of the real valued functions, by considering the real and imaginary parts of the complex function; therefore, unless explicitly specified, only real valued functions will be considered in this article.",
            "score": 29.788759231567383
        },
        {
            "docid": "26921073_7",
            "document": "Dynamic speckle . Biological tissue is one of the most complex that can be found in nature. Besides it is worsened by the intrinsic variability present between one sample and another. These facts make even more difficult the comparison of results between different samples even in presence of the same stimulus. In this context, speckle patterns have been applied to study bacteria, parasites, seeds and plants. In biologic materials between others.",
            "score": 29.750947952270508
        },
        {
            "docid": "51061242_2",
            "document": "Dynamic Creative Optimization . Dynamic creative optimization (DCO), is a form of programmatic advertising that allows advertisers to optimize the performance of their creative using real-time technology. While the actual optimization approaches may vary, they almost always involve the use of multivariate testing. The DCO process consists of creative development, identification of test variables, definition of the optimization objective, and method of optimization. Creative development is done using creative studio tools like Adobe Photoshop. It may include video, animation, native components, and interactive elements. Test variables represent the parts of the ad creative that are varied in the multivariate testing framework. These commonly include graphical elements, ad copy, colors, and click-through actions. The optimization objective can be initial engagement, a user action (such as click or install), or a post-install metric (such as purchase, registration, or lifetime value). Optimization of this objective is carried out using some form of discrete or combinatorial optimization.",
            "score": 29.662466049194336
        },
        {
            "docid": "13891813_9",
            "document": "Emery N. Brown . Brown later focused his statistics research on developing signal processing algorithms and statistical methods for neuronal data analysis. He developed a state-space point process (SSPP) paradigm to study how neural systems maintain dynamic representations of information. For the analysis of neural spiking activity and binary behavioral tasks represented as multivariate or univariate point processes (0-1 events that occur in continuous time), his research produced analogs of the Kalman filter, Kalman smoothing, sequential Monte Carlo algorithms, and combined state and parameter estimation algorithms commonly applied to continuous-valued time series observations.",
            "score": 29.605852127075195
        },
        {
            "docid": "38266275_14",
            "document": "Wassim Michael Haddad . Haddad\u2019s treatise on \"Nonnegative and Compartmental Dynamical Systems\", Princeton, NJ: Princeton University Press, 2010, presents a complete analysis and design framework for modeling and feedback control of nonnegative and compartmental dynamical systems. This work is rigorously theoretical in nature yet vitally practical in impact. The concepts are illustrated by examples from biology, chemistry, ecology, economics, genetics, medicine, sociology, and engineering. This book develops a unified stability and dissipativity analysis and control design framework for nonnegative and compartmental dynamical systems in order to foster the understanding of these systems as well as advancing the state-of-the-art in active control of nonnegative and compartmental systems. It has had fundamental ramifications in many areas of intense interest in today\u2019s contracting world, where medicine, economics, and sociology in closely interacting populations are becoming more important, where epidemiology and genetics are essential in understanding disease propagation in more and more closely interacting groups, and where real-time control system technology impacts modern medicine through robotic surgery, electrophysiological systems (pacemakers and automatic implantable defibrillators), life support (ventilators, artificial hearts), and image-guided therapy and surgery.",
            "score": 29.604286193847656
        },
        {
            "docid": "40726052_2",
            "document": "Community genetics . Community genetics is a recently emerged field in biology that fuses elements of community ecology, evolutionary biology, and molecular and quantitative genetics. Antonovics first articulated the vision for such a field, and Whitham et al. formalized its definition as \u201cThe study of the genetic interactions that occur between species and their abiotic environment in complex communities.\u201d The field aims to bridge the gaps in the study of evolution and ecology, within the multivariate community context that ecological and evolutionary phenomena are embedded within. The documentary movie \"A Thousand Invisible Cords\" provides an introduction to the field and its implications. To date, the primary focus of most community genetics studies has been on the influences of genetic variation in plants on foliar arthropod communities. In a wide variety of ecosystems, different plant genotypes often support different compositions of associated foliar arthropod communities. Such community phenotypes have been observed in natural hybrid complexes, among genotypes and sibling families within a single species and among different plant populations. To understand the broader impacts of differences among plant genotypes on biodiversity as a whole, researchers have begun to examine the response of other organisms, such as foliar endophytes, mycorrhizal fungi, soil microbes, litter dwelling arthropods, herbaceous plants and epiphytes. These effects are frequently examined with foundation species in temperate ecosystems, who structure ecosystems by modulating and stabilizing resources and ecosystem processes. The emphasis on foundation species allows researchers to focus on the likely most important players in a system without becoming overwhelmed by the complexity of all the genetically variable interactions occurring at the same time. However, unique effects of plant genotypes have also been found with non-foundation species, and can occur in tropical, boreal and alpine systems.",
            "score": 29.53471565246582
        },
        {
            "docid": "62329_39",
            "document": "Meta-analysis . An approach that has been tried since the late 1990s is the implementation of the multiple three-treatment closed-loop analysis. This has not been popular because the process rapidly becomes overwhelming as network complexity increases. Development in this area was then abandoned in favor of the Bayesian and multivariate frequentist methods which emerged as alternatives. Very recently, automation of the three-treatment closed loop method has been developed for complex networks by some researchers as a way to make this methodology available to the mainstream research community. This proposal does restrict each trial to two interventions, but also introduces a workaround for multiple arm trials: a different fixed control node can be selected in different runs. It also utilizes robust meta-analysis methods so that many of the problems highlighted above are avoided. Further research around this framework is required to determine if this is indeed superior to the Bayesian or multivariate frequentist frameworks. Researchers willing to try this out have access to this framework through a free software.",
            "score": 29.50076675415039
        }
    ]
}