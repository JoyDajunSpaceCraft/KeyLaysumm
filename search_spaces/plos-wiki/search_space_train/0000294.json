{
    "q": [
        {
            "docid": "26565579_50",
            "document": "Neuroscience of free will . Multivariate pattern analysis using EEG has suggested that an evidence based perceptual decision model may be applicable to free will decisions. It was found that decisions could be predicted by neural activity immediately after stimulus perception. Furthermore, when the participant was unable to determine the nature of the stimulus the recent decision history predicted the neural activity (decision). The starting point of evidence accumulation was in effect shifted towards a previous choice (suggesting a priming bias). Another study has found that subliminally priming a participant for a particular decision outcome (showing a cue for 13ms) could be used to influence free decision outcomes. Likewise, it has been found that decision history alone can be used to predict future decisions. The prediction capacities of the Soon et al. (2008) experiment were successfully replicated using a linear SVM model based on participant decision history alone (without any brain activity data). Despite this, a recent study has sought to confirm the applicability of a perceptual decision model to free will decisions. When shown a masked and therefore invisible stimulus, participants were asked to either guess between a category or make a free decision for a particular category. Multivariate pattern analysis using fMRI could be trained on \"free decision\" data to successfully predict \"guess decisions\", and trained on \"guess data\" in order to predict \"free decisions\" (in the precuneus and cuneus region).",
            "score": 79.65735793113708
        },
        {
            "docid": "7214278_2",
            "document": "Decision field theory . Decision field theory (DFT) is a dynamic-cognitive approach to human decision making. It is a cognitive model that describes how people actually make decisions rather than a rational or normative theory that prescribes what people should or ought to do. It is also a dynamic model of decision making rather than a static model, because it describes how a person's preferences evolve across time until a decision is reached rather than assuming a fixed state of preference. The preference evolution process is mathematically represented as a stochastic process called a diffusion process. It is used to predict how humans make decisions under uncertainty, how decisions change under time pressure, and how choice context changes preferences. This model can be used to predict not only the choices that are made but also decision or response times.",
            "score": 78.45790576934814
        },
        {
            "docid": "39475789_6",
            "document": "Optimal computing budget allocation . Experts in the field explain that in some problems it is important to not only know the best alternative among a sample, but the top 5, 10, or even 50, because the decision maker may have other concerns that may affect the decision which are not modeled in the simulation. According to Szechtman and Y\u00fccesan (2008), OCBA is also helpful in feasibility determination problems. This is where the decisions makers are only interested in differentiating feasible alternatives from the infeasible ones. Further, choosing an alternative that is simpler, yet similar in performance is crucial for other decision makers. In this case, the best choice is among top-r simplest alternatives, whose performance rank above desired levels. In addition, Trailovic and Pao (2004) demonstrate an OCBA approach, where we find alternatives with minimum variance, instead of with best mean. Here, we assume unknown variances, voiding the OCBA rule (assuming that the variances are known). During 2010 research was done on an OCBA algorithm that is based on a t distribution. The results show no significant differences between those from t-distribution and normal distribution. The above presented extensions of OCBA is not a complete list and is yet to be fully explored and compiled.",
            "score": 68.177579164505
        },
        {
            "docid": "2999259_3",
            "document": "Choice-supportive bias . What is remembered about a decision can be as important as the decision itself, especially in determining how much regret or satisfaction one experiences. Research indicates that the process of making and remembering choices yields memories that tend to be distorted in predictable ways. In cognitive science, one predictable way that memories of choice options are distorted is that positive aspects tend to be remembered as part of the chosen option, whether or not they originally were part of that option, and negative aspects tend to be remembered as part of rejected options. Once an action has been taken, the ways in which we evaluate the effectiveness of what we did may be biased. It is believed this may influence our future decision-making. These biases may be stored as memories, which are attributions that we make about our mental experiences based on their subjective qualities, our prior knowledge and beliefs, our motives and goals, and the social context. True and false memories arise by the same mechanism because when the brain processes and stores information, it cannot tell the difference from where they came from.",
            "score": 77.57232141494751
        },
        {
            "docid": "7214278_7",
            "document": "Decision field theory . The threshold is an important parameter for controlling speed\u2013accuracy tradeoffs. If the threshold is set to a lower value (about .30) in Figure 1, then prospect C would be chosen instead of prospect A (and done so earlier). Thus decisions can reverse under time pressure. High thresholds require a strong preference state to be reached, which allows more information about the prospects to be sampled, prolonging the deliberation process, and increasing accuracy. Low thresholds allow a weak preference state to determine the decision, which cuts off sampling information about the prospects, shortening the deliberation process, and decreasing accuracy. Under high time pressure, decision makers must choose a low threshold; but under low time pressure, a higher threshold can be used to increase accuracy. Very careful and deliberative decision makers tend to use a high threshold, and impulsive and careless decision makers use a low threshold. To provide a bit more formal description of the theory, assume that the decision maker has a choice among three actions, and also suppose for simplicity that there are only four possible final outcomes. Thus each action is defined by a probability distribution across these four outcomes. The affective values produced by each payoff are represented by the values m. At any moment in time, the decision maker anticipates the payoff of each action, which produces a momentary evaluation, U(t), for action i. This momentary evaluation is an attention-weighted average of the affective evaluation of each payoff: U(t) = \u03a3 W(t)m. The attention weight at time t, W(t), for payoff j offered by action i, is assumed to fluctuate according to a stationary stochastic process. This reflects the idea that attention is shifting from moment to moment, causing changes in the anticipated payoff of each action across time. The momentary evaluation of each action is compared with other actions to form a valence for each action at each moment, v(t) = U(t) \u2013 U.(t), where U.(t) equals the average across all the momentary actions. The valence represents the momentary advantage or disadvantage of each action. The total valence balances out to zero so that all the options cannot become attractive simultaneously. Finally, the valences are the inputs to a dynamic system that integrates the valences over time to generate the output preference states. The output preference state for action i at time t is symbolized as P(t). The dynamic system is described by the following linear stochastic difference equation for a small time step h in the deliberation process: P(t+h) = \u03a3 sP(t)+v(t+h).The positive self feedback coefficient, s = s > 0, controls the memory for past input valences for a preference state. Values of s < 1 suggest decay in the memory or impact of previous valences over time, whereas values of s > 1 suggest growth in impact over time (primacy effects). The negative lateral feedback coefficients, s = s < 0 for i not equal to j, produce competition among actions so that the strong inhibit the weak. In other words, as preference for one action grows stronger, then this moderates the preference for other actions. The magnitudes of the lateral inhibitory coefficients are assumed to be an increasing function of the similarity between choice options. These lateral inhibitory coefficients are important for explaining context effects on preference described later. Formally, this is a Markov process; matrix formulas have been mathematically derived for computing the choice probabilities and distribution of choice response times.",
            "score": 57.95522749423981
        },
        {
            "docid": "25988629_16",
            "document": "Robust decision-making . RDM analyses often employ a process called \"scenario discovery\" to facilitate the identification of vulnerabilities of proposed strategies. The process begins by specifying some performance metric, such as the total cost of a policy or its deviation from optimality (regret), which can be used to distinguish those cases in the results database where the strategy is judged successful from those where it is judged unsuccessful. Statistical or data-mining algorithms are applied to the database to generate simple descriptions of regions in the space of uncertain input parameters to the model that best describe the cases where the strategy is unsuccessful. That is, the algorithm for describing these cases is tuned to optimize both the predictability and interpretability by decision-makers. The resulting clusters have many characteristics of scenarios and can be used to help decision makers understand the vulnerabilities of the proposed policies and potential response options. A review conducted by the European Environment Agency of the rather sparse literature evaluating how scenarios actually perform in practice when used by organizations to inform decisions identified several key weaknesses of traditional scenario approaches. Scenario-discovery methods are designed to address these weaknesses. In addition, scenario discovery supports analysis for multiple stressors because it characterizes vulnerabilities as combinations of very different types of uncertain parameters (e.g. climate, economic, organizational capabilities, etc.).",
            "score": 55.09063494205475
        },
        {
            "docid": "35597124_16",
            "document": "Bayesian inference in marketing . The three principle strengths of Bayes' theorem that have been identified by scholars are that it is prescriptive, complete and coherent. Prescriptive in that it is the theorem that is the simple prescription to the conclusions reached on the basis of evidence and reasoning for the consistent decision maker. It is complete because (for a given choice of model and prior distribution) the solution is often clear and unambiguous. It allows for the incorporation of prior information when available to increase the robustness of the solutions, as well as taking into consideration the costs and risks that are associated with choosing alternate decisions. Lastly Bayes theorem is coherent. It is considered the most appropriate way to update beliefs by welcoming the incorporation of new information, as is seen through the probability distributions (see Savage and De Finetti). This is further complemented by the fact that Bayes inference satisfies the likelihood principle, which states that models or inferences for datasets leading to the same likelihood function should generate the same statistical information. Bayes methods are more cost effective than the traditional frequentist take on marketing research and subsequent decision making. The probability can be assessed from a degree of belief before and after accounting for evidence, instead of calculating the probabilities of a certain decision by carrying out a large number of trials with each one producing an outcome from a set of possible outcomes. The planning and implementation of trials to see how a decision impacts in the \u2018field\u2019 e.g. observing consumers reaction to a relabeling of a product, is time consuming and costly, a method many firms cannot afford. In place of taking the frequentist route in aiming for a universally acceptable conclusion through iteration, it is sometimes more effective to take advantage of all the information available to the firm to work out the \u2018best\u2019 decision at the time, and then subsequently when new knowledge is obtained, revise the posterior distribution to be then used as the prior, thus the inferences continue to logically contribute to one another based on Bayes theorem.",
            "score": 73.64465916156769
        },
        {
            "docid": "2663218_8",
            "document": "Neuromarketing . A greater understanding of human cognition and behaviour has led to the integration of biological and social sciences: Neuromarketing, a recent method utilized to understand consumers. The concept of neuromarketing combines marketing, psychology and neuroscience. Research is conducted around the implicit motivations to understand a consumer decisions by non-invasive psychoanalysis methods of measuring brain activity. These include electroencephalography (EEG), magnetoencephalography (MEG) and functional magnetic resonance imaging (fMRI), eye tracking, electrodermal response measures and other neuro-technologies. Researchers investigate and learn how consumers respond and feel when presented with products and/or related stimuli. Observations can then be correlated with a participants surmised emotions and social interactions. Market researchers use this information to determine if products or advertisements stimulate responses in the brain linked with positive emotions. The concept of neuromarketing was therefore introduced to study relevant human emotions and behavioral patterns associated with products, ads and decision-making. Neuromarketing provides models of consumer behavior and can also be used to re-interpret extant research. It provides theorization of emotional aspects of consumer behavior.",
            "score": 71.34794509410858
        },
        {
            "docid": "32421587_7",
            "document": "Collaborative decision-making software . In the 1960s, scientists deliberately started examining the utilization of automated quantitative models to help with basic decision making and planning. Automated decision support systems have become more of real time scenarios with the advancement of minicomputers, timeshare working frameworks and distributed computing. The historical backdrop of the execution of such frameworks starts in the mid-1960s. In a technology field as assorted as DSS, chronicling history is neither slick nor direct. Diverse individuals see the field of decision Support Systems from different vantage focuses and report distinctive records of what happened and what was important. As technology emerged new automated decision support applications were created and worked upon. Scientists utilized multiple frameworks to create and comprehend these applications. Today one can arrange the historical backdrop of DSS into the five expansive DSS classes, including: communications-driven, data-driven, document driven, knowledge-driven and model-driven decision support systems. Model-driven spatial decision support system (SDSS) was developed in the late 1980s and by 1995 the SDSS idea had turned out to be recognized in the literature. Data driven spatial DSS are also quite regular. All in all, a data-driven DSS stresses access to and control of a time-series of internal organization information and sometimes external and current data. Executive Information Systems are cases of data driven DSS.The very first cases of these frameworks were called data-oriented DSS, analysis Information Systems and recovery. Communications-driven DSS utilize networks and communications technologies to facilitate decision-relevant collaboration and communication. In these frameworks, communications technologies are the overwhelming design segment. Devices utilized incorporate groupware, video conferencing and computer-based bulletin boards.",
            "score": 52.11972951889038
        },
        {
            "docid": "22782655_54",
            "document": "Shoaling and schooling . Quorum sensing can function as a collective decision-making process in any decentralised system. A quorum response has been defined as \"a steep increase in the probability of group members performing a given behaviour once a threshold minimum number of their group mates already performing that behaviour is exceeded\". A recent investigation showed that small groups of fish used consensus decision-making when deciding which fish model to follow. The fish did this by a simple quorum rule such that individuals watched the decisions of others before making their own decisions. This technique generally resulted in the 'correct' decision but occasionally cascaded into the 'incorrect' decision. In addition, as the group size increased, the fish made more accurate decisions in following the more attractive fish model. Consensus decision-making, a form of collective intelligence, thus effectively uses information from multiple sources to generally reach the correct conclusion. Such behaviour has also been demonstrated in the shoaling behaviour of threespine sticklebacks.",
            "score": 73.0639271736145
        },
        {
            "docid": "26565579_40",
            "document": "Neuroscience of free will . Kuhn and Brass wanted to test participant self-knowledge. The first step was that after every decide trial, participants were next asked whether they had actually had time to decide. Specifically, the volunteers were asked to label each decide trial as either failed-to-decide (the action was the result of acting impulsively on the initial go-signal) or successful decide (the result of a deliberated decision). See the diagram on the right for this decide trial split: failed-to-decide and successful decide; the next split in this diagram (participant correct or incorrect) will be explained at the end of this experiment. Note also that the researchers sorted the participants\u2019 successful decide trials into \"decide go\" and \"decide nogo\", but were not concerned with the nogo trials since they did not yield any RT data (and are not featured anywhere in the diagram on the right). Note that successful stop trials did not yield RT data either. Kuhn and Brass now knew what to expect: primary response trials, any failed stop trials, and the \"failed-to-decide\" trials were all instances where the participant obviously acted impulsively \u2013 they would show the same quick RT. In contrast, the \"successful \"decide\"\" trials (where the decision was a \"go\" and the subject moved) should show a slower RT. Presumably, if deciding whether to veto is a conscious process, volunteers should have no trouble distinguishing impulsivity from instances of true deliberate continuation of a movement. Again, this is important since decide trials require that participants rely on self-knowledge. Note that stop trials cannot test self-knowledge because if the subject \"does\" act, it is obvious to them that they reacted impulsively.",
            "score": 59.585240483284
        },
        {
            "docid": "12516446_2",
            "document": "Expected value of sample information . In decision theory, the expected value of sample information (EVSI) is the expected increase in utility that a decision-maker could obtain from gaining access to a sample of additional observations before making a decision. The additional information obtained from the sample may allow them to make a more informed, and thus better, decision, thus resulting in an increase in expected utility. EVSI attempts to estimate what this improvement would be before seeing actual sample data; hence, EVSI is a form of what is known as \"preposterior analysis\".",
            "score": 66.84921884536743
        },
        {
            "docid": "15855253_2",
            "document": "Quantification of margins and uncertainties . Quantification of Margins and Uncertainty (QMU) is a decision-support methodology for complex technical decisions. QMU focuses on the identification, characterization, and analysis of performance thresholds and their associated margins for engineering systems that are evaluated under conditions of uncertainty, particularly when portions of those results are generated using computational modeling and simulation. QMU has traditionally been applied to complex systems where comprehensive experimental test data is not readily available and cannot be easily generated for either end-to-end system execution or for specific subsystems of interest. Examples of systems where QMU has been applied include nuclear weapons performance, qualification, and stockpile assessment. QMU focuses on characterizing in detail the various sources of uncertainty that exist in a model, thus allowing the uncertainty in the system response output variables to be well quantified. These sources are frequently described in terms of probability distributions to account for the stochastic nature of complex engineering systems. The characterization of uncertainty supports comparisons of design margins for key system performance metrics to the uncertainty associated with their calculation by the model. QMU supports risk-informed decision-making processes where computational simulation results provide one of several inputs to the decision-making authority. There is currently no standardized methodology across the simulation community for conducting QMU; the term is applied to a variety of different modeling and simulation techniques that focus on rigorously quantifying model uncertainty in order to support comparison to design margins.",
            "score": 65.72262644767761
        },
        {
            "docid": "30444145_4",
            "document": "Forest informatics . As in management science, Forest Informatics uses  decision support systems, mathematical modeling, statistics, and algorithms from engineering, operations research, computer science, and artificial intelligence to support decision-making activities. Common forestry problems include harvest scheduling, model fitting, optimal sampling, remote sensing, crew assignment, image classification, treatment timing, and log bucking problems, many of which can be formulated as optimization problems (e.g. generalized assignment problem,  traveling salesman problem, knapsack problem, job shop scheduling, and vehicle routing problems). The practice includes information processing and the engineering of information systems,  decision support systems, geographic information systems, and  global positioning systems. The research field includes studies the structure, algorithms, behavior, and interactions of natural and artificial systems that store, process, access and communicate information about forested ecosystems.",
            "score": 58.07003462314606
        },
        {
            "docid": "13772374_13",
            "document": "Renaissance Computing Institute . Many of RENCI\u2019s projects in the Environmental Sciences focus on hydrology, coastal storm surges, and advanced modeling to assist in disaster preparedness. ADCIRC is an open source software model that applies advanced analytics to multiple data sources and types (e.g., hydrology data sets, atmospheric data sets, tropical storm forecasting data, Geographic Information System data, etc.) to enable real-time, high-resolution prediction of the impact of coastal storm surges and flooding after hurricanes and related events. In collaboration with researchers at the UNC Coastal Resilience Center and the National Hurricane Center, ADCIRC is being developed as a coastal forecasting system to assist with state and federal disaster planning and decision support. EarthCube is an NSF-funded initiative that aims \u201cto develop a framework over the next decade to assist researchers in understanding and predicting the Earth system from the Sun to the center of the Earth.\u201d EarthCube is being designed as an open dynamic cyberinfrastructure to enable community-governed data sharing across the geosciences, including ocean science, polar studies, atmospheric science, geospace, computer science, and other fields. HydroShare is supported by the NSF-funded CUAHSI (Consortium of Universities for the Advancement of Hydrologic Science Inc.) and is under development as an open collaboration cyberinfrastructure for hydrology. HydroShare allows water scientists to identify and retrieve water-related data sets and associated algorithms and models and then analyze and compute on the data using a distributed computing environment that includes grid-based cloud and high-performance computing and storage capabilities",
            "score": 32.89436101913452
        },
        {
            "docid": "1125883_42",
            "document": "Markov decision process . If the state space and action space are finite, we could use linear programming to find the optimal policy, which was one of the earliest approaches applied. Here we only consider the ergodic model, which means our continuous-time MDP becomes an ergodic continuous-time Markov chain under a stationary policy. Under this assumption, although the decision maker can make a decision at any time at the current state, he could not benefit more by taking more than one action. It is better for him to take an action only at the time when system is transitioning from the current state to another state. Under some conditions,(for detail check Corollary 3.14 of \"Continuous-Time Markov Decision Processes\"), if our optimal value function formula_98 is independent of state formula_57, we will have the following inequality: If there exists a function formula_101, then formula_102 will be the smallest formula_103 satisfying the above equation. In order to find formula_102, we could use the following linear programming model: formula_107 is a feasible solution to the D-LP if formula_107 is nonnative and satisfied the constraints in the D-LP problem. A feasible solution formula_109 to the D-LP is said to be an optimal solution if for all feasible solution formula_107 to the D-LP. Once we have found the optimal solution formula_109, we can use it to establish the optimal policies.",
            "score": 72.07114219665527
        },
        {
            "docid": "492271_25",
            "document": "Clinical psychology . Clinical assessment can be characterized as a prediction problem where the purpose of assessment is to make inferences (predictions) about past, present, or future behavior. For example, many therapy decisions are made on the basis of what a clinician expects will help a patient make therapeutic gains. Once observations have been collected (e.g., psychological test results, diagnostic impressions, clinical history, X-ray, etc.), there are two mutually exclusive ways to combine those sources of information to arrive at a decision, diagnosis, or prediction. One way is to combine the data in an algorithmic, or \"mechanical\" fashion. Mechanical prediction methods are simply a mode of combination of data to arrive at a decision/prediction of behavior (e.g., treatment response). Mechanical prediction does not preclude any type of data from being combined; it can incorporate clinical judgments, properly coded, in the algorithm. The defining characteristic is that, once the data to be combined is given, the mechanical approach will make a prediction that is 100% reliable. That is, it will make exactly the same prediction for exactly the same data every time. Clinical prediction, on the other hand, does not guarantee this, as it depends on the decision-making processes of the clinician making the judgment, their current state of mind, and knowledge base.",
            "score": 55.06266951560974
        },
        {
            "docid": "33802950_14",
            "document": "Thurstonian model . The above paragraph contains a common misunderstanding of the Thurstonian resolution of Gridgeman's paradox. Although it is true that different decision rules (cognitive strategies) are used in making a choice among three alternatives, the mere fact of knowing an attribute in advance does not explain the paradox, nor are subjects required to rely on a more general, multidimensional measure of sensory difference. In the triangular method, for instance, the subject is instructed to choose the most different of three items, two of which are putatively identical. The items may differ on a unidimensional scale and the subject may be made aware of the nature of the scale in advance. Gridgeman's paradox will still be observed. This occurs because of the sampling process combined with a distance-based decision rule as opposed to a magnitude-based decision rule assumed to model the results of the 3-alternative forced choice task.",
            "score": 75.20457327365875
        },
        {
            "docid": "28961424_10",
            "document": "Network Science CTA . The modern military increasingly needs to rely on bottom up network processes, as compared to top down hierarchic processes. How does the pattern of interactions within a military unit affect performance of tasks? What kinds of ties external to the Army are necessary to success? How can we use massive streams of data to detect adversarial networks? How can a social and cognitive network quickly extract the most meaningful information for the soldier and decision maker that is useful in all aspects of their operations from supporting humanitarian operations to force protection and full combat operations? These are but a sample of network-related questions with which the 21st century Army must wrestle. The long-term objective of the center is to advance the scientific understanding of how the social networks form, operate and evolve and how they affect the functioning of large, complex organizations such as the Army; how adversary networks hidden in large social networks can be detected, monitored or dissolved; and how human cognition directs and is impacted by the network-centric interactions. SCNARC will undertake research to gain a fundamental understanding of the underlying theory, as well as create scientific foundations for modeling, simulation, measurements, analysis, prediction, and influence of social/cognitive networks and their impact on the U.S. Army.",
            "score": 56.59624171257019
        },
        {
            "docid": "1272412_6",
            "document": "Accounting information system . A big advantage of computer-based accounting information systems is that they automate and streamline reporting, develop advanced modelling and support data mining. Reporting is major tool for organizations to accurately see summarized, timely information used for decision-making and financial reporting. The accounting information system pulls data from the centralized database, processes and transforms it and ultimately generates a summary of that data as information that can now be easily consumed and analyzed by business analysts, managers or other decision makers. These systems must ensure that the reports are timely so that decision-makers are not acting on old, irrelevant information and, rather, able to act quickly and effectively based on report results. Consolidation is one of the hallmarks of reporting as people do not have to look through an enormous number of transactions. For instance, at the end of the month, a financial accountant consolidates all the paid vouchers by running a report on the system. The system\u2019s application layer provides a report with the total amount paid to its vendors for that particular month. With large corporations that generate large volumes of transactional data, running reports with even an AIS can take days or even weeks.",
            "score": 51.350054025650024
        },
        {
            "docid": "20467381_6",
            "document": "Overchoice . Decision-makers in large choice situations enjoy the decision process more than those with smaller choice sets, but feel more responsible for their decisions. Despite this, more choices result with more dissatisfaction and regret in decisions. The feeling of responsibility causes cognitive dissonance when presented with large array situations. In this situation, cognitive dissonance results when there is a mental difference between the choice made and the choice that should have been made. More choices lead to more cognitive dissonance because it increases the chance that the decision-maker made the wrong decision. These large array situations cause the chooser to feel both enjoyment as well as feel overwhelmed with their choices. These opposing emotions contribute to cognitive dissonance, and causes the chooser to feel less motivated to make a decision. This also disables them from using psychological processes to enhance the attractiveness of their own choices. Choosers in large array situations do enjoy their decision-making process more than those in small array situations. The amount of time allotted to make a decision also has an effect on an individual's perception of their choice. Larger choice sets with a small amount of time results in more regret with the decision. When more time is provided, the process of choosing is more enjoyable in large array situations and results in less regret after the decision has been made.",
            "score": 75.69407629966736
        },
        {
            "docid": "8732281_4",
            "document": "GOR method . The mathematics and algorithm of the GOR method were based on an earlier series of studies by Robson and colleagues reported mainly in the \"Journal of Molecular Biology\" (e.g.) and The Biochemical Journal (e.g.). The latter describes the information theoretic expansions in terms of conditional information measures. The use of the word \"simple\" in the title of the GOR paper reflected the fact that the above earlier methods provided proofs and techniques somewhat daunting by being rather unfamiliar in protein science in the early 1970s; even Bayes methods were then unfamiliar and controversial. An important feature of these early studies, which survived in the GOR method, was the treatment of the sparse protein sequence data of the early 1970s by expected information measures. That is, expectations on a Bayesian basis considering the distribution of plausible information measure values given the actual frequencies (numbers of observations). The expectation measures resulting from integration over this and similar distributions may now be seen as composed of \"incomplete\" or extended zeta functions, e.g. z(s,observed frequency) \u2212 z(s,expected frequency) with incomplete zeta function z(s, n) = 1 + (1/2) + (1/3)+ (1/4) + \u2026. +(1/n). The GOR method used s=1. Also, in the GOR method and the earlier methods, the measure for the contrary state to e.g. helix H, i.e. ~H, was subtracted from that for H, and similarly for beta sheet, turns, and coil or loop. Thus the method can be seen as employing a zeta function estimate of log predictive odds. An adjustable decision constant could also be applied, which thus also implies a decision theory approach; the GOR method allowed the option to use decision constants to optimize predictions for different classes of protein. The expected information measure used as a basis for the information expansion was less important by the time of publication of the GOR method because protein sequence data became more plentiful, at least for the terms considered at that time. Then, for s=1, the expression z(s,observed frequency) \u2212 z(s,expected frequency) approaches the natural logarithm of (observed frequency / expected frequency) as frequencies increase. However, this measure (including use of other values of s) remains important in later more general applications with high-dimensional data, where data for more complex terms in the information expansion are inevitably sparse (e.g.).",
            "score": 54.01775085926056
        },
        {
            "docid": "48415691_10",
            "document": "Industrial big data . Cyber-physical systems is the core technology of industrial big data. Cyber-physical systems are systems that require seamless integration between computational models and physical components. Differing from the traditional operation technology, \"Industrial Big Data\u201d requires that the decision to be informed from a way wider scope, a central part of which is equipment status. The \"5C\u201d (Connection, Conversion, Cyber, Cognition, Configuration) architecture has indicated that cyber-physical systems is focused on transferring raw data to actionable information, understanding process insights, and eventually improve the process by well-informed decision making. Improved processes will further increase productivity and reduce costs. This aligns with the mission of \"Industrial Big Data\u201d, which is to reveal insights from the large amount of raw data and turn that information into values. This combines the power of information technology and operation technology to create an information-transparent environment to support decisions for users of different levels.",
            "score": 59.959798097610474
        },
        {
            "docid": "32329761_11",
            "document": "Analytica (software) . Incorporating uncertainty into model outputs helps to provide more realistic and informative projections. Uncertain quantities in Analytica can be specified using a distribution function. When evaluated, distributions are sampled using either Latin hypercube or Monte Carlo sampling, and the samples are propagated through the computations to the results. The sampled result distribution and summary statistics can then be viewed directly (mean, fractile bands, probability density function (PDF), cumulative distribution function (CDF)), Analytica supports collaborative decision analysis and probability management through the use of the SIPMath(tm) standard.",
            "score": 64.99928092956543
        },
        {
            "docid": "38454300_15",
            "document": "Assisted colonization . Despite the uncertainty inherent in predictions of future suitable habitat, some studies have demonstrated that predictions can be quite accurate. A study of Hesperia comma butterflies in Britain identified unoccupied habitat sites that were likely to support the species under a warmer climate based on their similarity to occupied sites. As the climate warmed, the butterfly colonized many of the sites; most of the sites it did not colonize were located far from existing populations, suggesting they were uncolonized because the butterfly could not reach them on its own. The data suggested that the suitable, uncolonized sites could be good targets for assisted colonization. The results suggested that if investigators can demonstrate their model makes reliable predictions with real-world data, models might be trusted for informing assisted colonization decisions.",
            "score": 51.35982346534729
        },
        {
            "docid": "23497542_22",
            "document": "Process tracing . The total time fixation as well as the total time of a sequence of fixations can reveal important aspects of the decision process. The mean fixation duration is an indicator of the processing depth or effort, and thus, reveal an important aspect of the decision process too. There is evidence ( Pieters and Warlop, 1999) that fixation duration on a chosen brand is longer than on a nonchosen alternative. There are mostly pair comparison and fixation duration is indicating that there are different checking processes (Russo & Leclerec, 1994). Open question: Duration of saccade may improve the accuracy of the total duration. Sequences of fixation There are within-alternative and between attributes transitions. The definition of by-alternative processing is a fixation transition within an attribute between alternatives. A single transition within an attribute but between alternatives is defined as by-attribute processing. There is evidence that an increase in involvement (heightened by giving participants whichever brand they chose) not only led to longer fixation durations but also to more by-brand and fewer by-attribute transitions. ( Van Raaij, 1977) There are research that focus on how to distinguish between these two processes, such as Russo & Dosher (1983). They defined by-alternative processing as sequence that all three attributes had to be fixated without interruption. A by-attribute comparisons required contiguous by-attribute evaluations of all three attribute. There is evidence that by-attribute processing is the preferred process for decision making (Arieli et al., 2009), even if it is not possible to decide accurately e.g. between two gambles without by-alternative processing. Eye fixations as complementary data Different kinds of verbal protocol and eye fixation as a complementary data were compared (Gog et al., 2005). Such analysis reveals more information than only a single method. Fixations as a monitor of attention Some experiments investigate the peripheral process. In such studies, it is essential to ensure that an individual gaze at a fixation point and to identify if the eye has moved. With the gaze-contingent stimulus alteration (McConkie & Rayner, 1975) it can be imposed whererver an individual is currently looking. With this method the impact of emotional words, when presented parafoveally (Calvo & Castillo, 2009) and the left hemisphere advantage for peripherally presented words (Jordan et al., 2009). Critics: - Eye fixation show where people are looking, but not what they are thinking. A clear interpretation is hardly doable as a fixation e.g. can occur to an elimination or to another consideration of other alternatives, or it can be learning or eliminating. - There are only a few JDM theories that specify a decision process at the level of fixations on individual alternatives or even smaller units such as individual attributes. (e.g., a theory that propose a more or less continuously developing decision process or another theory that addresses a model of the differentiation of value over time).",
            "score": 61.93932545185089
        },
        {
            "docid": "4698768_8",
            "document": "Naturalistic decision-making . Some of the first funding into NDM research came from the U.S. Army and Navy in the mid-1980s. The U.S. Navy became interested in naturalistic decisions following the 1988 USS \"Vincennes\" shoot-down incident, in which a U.S. Navy Aegis cruiser destroyed an Iranian commercial airliner, mistaking it for a hostile attacker. Both the Army and the Navy wanted to help people make high-stakes decisions under extreme time pressure and under dynamic and uncertain conditions. The NDM researchers studied people in field settings, such as Navy commanders and army small unit leaders. From this perspective, making a decision means committing oneself to a course of action where plausible alternatives exist, even if the person does not identify or compare these alternatives. The NDM movement shifted our conception of human decision making from a domain independent general approach to a knowledge based approach exemplified by decision makers who had substantial experience. The decision making process was expanded to include a prior stage of perception and recognition of situations, as well as generation of appropriate responses, not just choice from among given options. This perspective took advantage of advances in cognitive psychology such as knowledge representation concepts of scripts, schemas, and mental models, to contrast expert versus novice behavior. NDM has even affected Army doctrine. The current edition of the Army Field Manual on Command and Control (FM 101-5) includes for the first time a section on intuitive decision making, largely influenced by research on the RPD model. The field of NDM has also provided guidance for training decision making and related cognitive skills. Cannon-Bowers and Salas (1998) have described the range of lessons learned from the TADMUS (Tactical Decision Making Under Stress) project initiated by the Navy following the USS \"Vincennes\" shoot-down decision. These include methods for providing stress inoculation along with approaches for individual and team decision training.",
            "score": 71.48336613178253
        },
        {
            "docid": "3026543_15",
            "document": "Situation awareness . Accurate mental models are one of the prerequisites for achieving SA. A mental model can be described as a set of well-defined, highly organized yet dynamic knowledge structures developed over time from experience. The volume of available data inherent in complex operational environments can overwhelm the capability of novice decision makers to attend, process, and integrate this information efficiently, resulting in information overload and negatively impacting their SA. In contrast, experienced decision makers assess and interpret the current situation (Level 1 and 2 SA) and select an appropriate action based on conceptual patterns stored in their long-term memory as \"mental models\". Cues in the environment activate these mental models, which in turn guide their decision making process.",
            "score": 59.35008883476257
        },
        {
            "docid": "23453327_11",
            "document": "Shared decision-making in medicine . With funding bodies emphasizing knowledge translation, i.e. making sure that scientific research results in changes in practice, researchers in shared decision-making have focussed on \"implementing\" SDM, or making it happen. Based on studies of barriers to shared decision-making as perceived by health professionals and patients, many researchers are developing sound, theory-based training programs and decision aids, and evaluating their results. Canada has established a research chair that focusses on practical methods for promoting and implementing shared decision-making across the healthcare continuum. Although patients who are involved in decision-making about their health have better outcomes, healthcare professionals rarely involve them in these decisions. A recently updated Cochrane review has synthesized the body of evidence about different interventions that can be used to help healthcare professionals adopt practices to better involve their patients in the process of making decisions about their health. In this review of studies testing interventions to help healthcare professionals adopt practices to better involve their patients in the process of making decisions, five studies were identified. This review found that educational meetings, giving healthcare professionals feedback, giving healthcare professionals learning materials, and using patient decision aids are some techniques that have been tried and might be helpful. However, the review could not determine from the available studies which of these were best. The review makes some suggestions for how research studies could better evaluate healthcare professionals involving patients in the process of making decisions about their health so that we can understand this better in the future. There is also a need for greater conceptual clarity. Involving patients in decisions is by definition a process that could occur over time and in many encounters. Much of the literature seems to assume that achieving shared decision-making is a matter of giving healthcare professionals enough information.",
            "score": 45.47542750835419
        },
        {
            "docid": "35597124_17",
            "document": "Bayesian inference in marketing . In marketing situations, it is important that the prior probability is (1) chosen correctly, and (2) is understood. A disadvantage to using Bayesian analysis is that there is no \u2018correct\u2019 way to choose a prior, therefore the inferences require a thorough analysis to translate the subjective prior beliefs into a mathematically formulated prior to ensure that the results will not be misleading and consequently lead to the disproportionate analysis of preposteriors. The subjective definition of probability and the selection and use of the priors have led to statisticians critiquing this subjective definition of probability that underlies the Bayesian approach. Bayesian probability is often found to be difficult when analysing and assessing probabilities due to its initial counter intuitive nature. Often when deciding between strategies based on a decision, they are interpreted as: where there is evidence X that shows condition A might hold true, is misread by judging A\u2019s likelihood by how well the evidence X matches A, but crucially without considering the prior frequency of A. In alignment with Falsification, which aims to question and falsify instead of prove hypotheses, where there is very strong evidence X, it does not necessarily mean there is a very high probability that A leads to B, but in fact should be interpreted as a very low probability of A not leading to B. In the field of marketing, behavioural experiments which have dealt with managerial decision- making, and risk perception, in consumer decisions have utilised the Bayesian model, or similar models, but found that it may not be relevant quantitatively in predicting human information processing behaviour. Instead the model has been proven as useful as a qualitative means of describing how individuals combine new evidence with their predetermined judgements. Therefore, \u201cthe model may have some value as a first approximation to the development of descriptive choice theory\u201d in consumer and managerial instances.",
            "score": 67.00065696239471
        },
        {
            "docid": "7214278_5",
            "document": "Decision field theory . The basic ideas underlying the decision process for sequential sampling models is illustrated in Figure 1 below. Suppose the decision maker is initially presented with a choice between three risky prospects, A, B, C, at time t = 0. The horizontal axis on the figure represents deliberation time (in seconds), and the vertical axis represents preference strength. Each trajectory in the figure represents the preference state for one of the risky prospects at each moment in time.",
            "score": 59.05361080169678
        },
        {
            "docid": "42035187_2",
            "document": "Intelligent Decision System . Intelligent Decision System, or IDS, is a software package for multiple criteria decision analysis. It can handle hybrid types of uncertainty, including probability uncertainty, missing data, subjective judgements, interval data, and any combination of those types of uncertainty. It uses belief function for problem modelling and the Evidential Reasoning Approach for attribute aggregation. The outcomes of the analysis include not only ranking of alternative courses of action based on average scores, but also aggregated performance distribution of each alternative for supporting informed and transparent decision making.",
            "score": 62.307557582855225
        }
    ],
    "r": [
        {
            "docid": "16695601_2",
            "document": "Regret (decision theory) . In decision theory, on making decisions under uncertainty\u2014should information about the best course of action arrive \"after\" taking a fixed decision\u2014the human emotional response of regret is often experienced. The theory of regret aversion or anticipated regret proposes that when facing a decision, individuals might \"anticipate\" regret and thus incorporate in their choice their desire to eliminate or reduce this possibility. Regret is a negative emotion with a powerful social and reputational component, and is central to how humans learn from experience and to the human psychology of risk aversion. Conscious anticipation of regret creates a feedback loop that elevates regret from the emotional realm\u2014often modeled as mere human behavior\u2014into the realm of the rational choice behavior that is modeled in decision theory.",
            "score": 83.91661834716797
        },
        {
            "docid": "11015023_27",
            "document": "Selective exposure theory . This new model combines the motivational and cognitive processes of selective exposure. In the past, selective exposure had been studied from a motivational standpoint. For instance, the reason behind the existence of selective exposure was that people felt motivated to decrease the level of dissonance they felt while encountering inconsistent information. They also felt motivated to defend their decisions and positions, so they achieved this goal by exposing themselves to consistent information only. However, the new cognitive economy model not only takes into account the motivational aspects, but it also focuses on the cognitive processes of each individual. For instance, this model proposes that people cannot evaluate the quality of inconsistent information objectively and fairly because they tend to store more of the consistent information and use this as their reference point. Thus, inconsistent information is often observed with a more critical eye in comparison to consistent information. According to this model, the levels of selective exposure experienced during the decision-making process are also dependent on how much cognitive energy people are willing to invest. Just as people tend to be careful with their finances, cognitive energy or how much time they are willing to spend evaluating all the evidence for their decisions works the same way. People are hesitant to use this energy; they tend to be careful so they don't waste it. Thus, this model suggests that selective exposure does not happen in separate stages. Rather, it is a combined process of the individuals' certain acts of motivations and their management of the cognitive energy.",
            "score": 83.60598754882812
        },
        {
            "docid": "54286931_6",
            "document": "Truncated normal hurdle model . However, these two implicit assumptions are too strong and inconsistent with many contexts in economics. For instance, when we need to decide whether to invest and build a factory, the construction cost might be more influential than the product price; but once we have already built the factory, the product price is definitely more influential to the revenue. Hence, the implicit assumption (2) doesn\u2019t match this context. The essence of this issue is that the standard Tobit implicitly models a very strong link between the participation decision formula_13 or formula_14 and the amount decision (the magnitude of formula_15 when formula_16). If a corner solution model is represented in a general form: formula_17 , where formula_18 is the participate decision and formula_19 is the amount decision, standard Tobit model assumes:",
            "score": 83.51171875
        },
        {
            "docid": "381538_5",
            "document": "OODA loop . Boyd's diagram shows that all decisions are based on observations of the evolving situation tempered with implicit filtering of the problem being addressed. The observations are the raw information on which decisions and actions are based. The observed information must be processed to orient it for decision making. In notes from his talk \"Organic Design for Command and Control\", Boyd said, The second O, orientation\u2014as the repository of our genetic heritage, cultural tradition, and previous experiences\u2014is the most important part of the O-O-D-A loop since it shapes the way we observe, the way we decide, the way we act.",
            "score": 80.63653564453125
        },
        {
            "docid": "20268918_18",
            "document": "Self-justification . One major claim of social psychology is that we experience cognitive dissonance every time we make a decision; in an attempt to alleviate this, we then submit to a largely unconscious reduction of dissonance by creating new motives of our decision making that more positively reflect on our self-concept. This process of reducing cognitive dissonance regarding decision-making relates back to the problem of individual becoming stagnant in a course of action. Furthermore, once an individual makes a decision dissonance has begun. To alleviate this dissonance, they rationalize their actions by either by changing them\u2014or in this case, continuing in their course of action, perpetuating their qualifying beliefs. In this case, the question concerns the source of the breakdown of rationale that causes the continuation of such disadvantageous behavior.",
            "score": 80.07026672363281
        },
        {
            "docid": "26565579_50",
            "document": "Neuroscience of free will . Multivariate pattern analysis using EEG has suggested that an evidence based perceptual decision model may be applicable to free will decisions. It was found that decisions could be predicted by neural activity immediately after stimulus perception. Furthermore, when the participant was unable to determine the nature of the stimulus the recent decision history predicted the neural activity (decision). The starting point of evidence accumulation was in effect shifted towards a previous choice (suggesting a priming bias). Another study has found that subliminally priming a participant for a particular decision outcome (showing a cue for 13ms) could be used to influence free decision outcomes. Likewise, it has been found that decision history alone can be used to predict future decisions. The prediction capacities of the Soon et al. (2008) experiment were successfully replicated using a linear SVM model based on participant decision history alone (without any brain activity data). Despite this, a recent study has sought to confirm the applicability of a perceptual decision model to free will decisions. When shown a masked and therefore invisible stimulus, participants were asked to either guess between a category or make a free decision for a particular category. Multivariate pattern analysis using fMRI could be trained on \"free decision\" data to successfully predict \"guess decisions\", and trained on \"guess data\" in order to predict \"free decisions\" (in the precuneus and cuneus region).",
            "score": 79.65735626220703
        },
        {
            "docid": "41419956_7",
            "document": "Description-experience gap . Contrary to the results obtained by prospect theory, people tended to underweight the probabilities of rare outcomes when they made decisions from experience. That is, they in general tended to choose the more probable outcome much more often than the rare outcomes; they behaved as if the rare outcomes were more unlikely than they really were. The effect has been observed in studies involving repeated and small samples of choices. However, people tended to choose the riskier choice when deciding from experience for tasks that are framed in terms of gains, and this, too, is in contrast with decisions made from description.",
            "score": 79.4070816040039
        },
        {
            "docid": "15963644_7",
            "document": "Peter Carruthers (philosopher) . Most people (philosophers and non-philosophers alike) assume that they have direct introspective access to their own propositional attitude events of judging, deciding, and so forth. We think of ourselves as knowing our own thought processes immediately, without having to interpret ourselves (in the way that we \"do\" need to interpret the behavior and circumstances of other people if we are to know what \"they\" are thinking). In a series of recent papers Peter Carruthers has argued that this introspective intuition is illusory. While allowing that we do have introspective access to our own experiences, including imagistic experiences of the sort that occur during \"inner speech\", he draws on evidence from across the cognitive sciences to argue that our knowledge of our own judgments and decisions results from us turning our interpretative skills upon ourselves. He also argues that while inner speech plays important roles in human cognition, it never plays the right \"sort\" of role to constitute a judgment, or a decision. The latter processes always occur below the surface of consciousness, Carruthers claims.",
            "score": 78.93785858154297
        },
        {
            "docid": "7214278_2",
            "document": "Decision field theory . Decision field theory (DFT) is a dynamic-cognitive approach to human decision making. It is a cognitive model that describes how people actually make decisions rather than a rational or normative theory that prescribes what people should or ought to do. It is also a dynamic model of decision making rather than a static model, because it describes how a person's preferences evolve across time until a decision is reached rather than assuming a fixed state of preference. The preference evolution process is mathematically represented as a stochastic process called a diffusion process. It is used to predict how humans make decisions under uncertainty, how decisions change under time pressure, and how choice context changes preferences. This model can be used to predict not only the choices that are made but also decision or response times.",
            "score": 78.4579086303711
        },
        {
            "docid": "2999259_3",
            "document": "Choice-supportive bias . What is remembered about a decision can be as important as the decision itself, especially in determining how much regret or satisfaction one experiences. Research indicates that the process of making and remembering choices yields memories that tend to be distorted in predictable ways. In cognitive science, one predictable way that memories of choice options are distorted is that positive aspects tend to be remembered as part of the chosen option, whether or not they originally were part of that option, and negative aspects tend to be remembered as part of rejected options. Once an action has been taken, the ways in which we evaluate the effectiveness of what we did may be biased. It is believed this may influence our future decision-making. These biases may be stored as memories, which are attributions that we make about our mental experiences based on their subjective qualities, our prior knowledge and beliefs, our motives and goals, and the social context. True and false memories arise by the same mechanism because when the brain processes and stores information, it cannot tell the difference from where they came from.",
            "score": 77.57231903076172
        },
        {
            "docid": "20467381_6",
            "document": "Overchoice . Decision-makers in large choice situations enjoy the decision process more than those with smaller choice sets, but feel more responsible for their decisions. Despite this, more choices result with more dissatisfaction and regret in decisions. The feeling of responsibility causes cognitive dissonance when presented with large array situations. In this situation, cognitive dissonance results when there is a mental difference between the choice made and the choice that should have been made. More choices lead to more cognitive dissonance because it increases the chance that the decision-maker made the wrong decision. These large array situations cause the chooser to feel both enjoyment as well as feel overwhelmed with their choices. These opposing emotions contribute to cognitive dissonance, and causes the chooser to feel less motivated to make a decision. This also disables them from using psychological processes to enhance the attractiveness of their own choices. Choosers in large array situations do enjoy their decision-making process more than those in small array situations. The amount of time allotted to make a decision also has an effect on an individual's perception of their choice. Larger choice sets with a small amount of time results in more regret with the decision. When more time is provided, the process of choosing is more enjoyable in large array situations and results in less regret after the decision has been made.",
            "score": 75.69407653808594
        },
        {
            "docid": "177698_7",
            "document": "Behavioral economics . In the 1960s cognitive psychology began to shed more light on the brain as an information processing device (in contrast to behaviorist models). Psychologists in this field, such as Ward Edwards, Amos Tversky and Daniel Kahneman began to compare their cognitive models of decision-making under risk and uncertainty to economic models of rational behavior. Mathematical psychology reflects a longstanding interest in preference transitivity and the measurement of utility.",
            "score": 75.62705993652344
        },
        {
            "docid": "3917598_10",
            "document": "Carol Tavris . According to Tavris and Aronson, cognitive dissonance allows us to justify our mistakes and harms, keeping us from conscious awareness that we even made any, and thereby allows us to live with ourselves. This is how even \"charlatans, scammers, and tyrants sleep at night.\" Given a choice between accepting information that we don't want to hear and justifying outdated beliefs or hurtful acts, most people choose self-justification. Indeed, Tavris says, \"the more we pride ourselves on our intelligence and our competence, the stronger our commitment to an ideology or philosophy of life...the harder it is to accept evidence that we might be wrong.\" \"Mistakes Were Made\" explains how cognitive dissonance applies in all domains of life, from decisions made during the Bush administration, prosecutors who cannot accept that they put innocent people in prison, to quarreling couples who cannot understand the other person's point of view.",
            "score": 75.40127563476562
        },
        {
            "docid": "33802950_14",
            "document": "Thurstonian model . The above paragraph contains a common misunderstanding of the Thurstonian resolution of Gridgeman's paradox. Although it is true that different decision rules (cognitive strategies) are used in making a choice among three alternatives, the mere fact of knowing an attribute in advance does not explain the paradox, nor are subjects required to rely on a more general, multidimensional measure of sensory difference. In the triangular method, for instance, the subject is instructed to choose the most different of three items, two of which are putatively identical. The items may differ on a unidimensional scale and the subject may be made aware of the nature of the scale in advance. Gridgeman's paradox will still be observed. This occurs because of the sampling process combined with a distance-based decision rule as opposed to a magnitude-based decision rule assumed to model the results of the 3-alternative forced choice task.",
            "score": 75.20457458496094
        },
        {
            "docid": "51411749_17",
            "document": "Cognitive assets . Organizations use different tools and systems to help their decision-making processes. These tools and systems are broadly defined as any activity that, based on explicit (but not necessarily formalized) models, helps decision-making agents to obtain solutions to their problems given their preferences and the uncertainty of the environment. Together with individuals\u2019 cognitive capacities, they define the organization\u2019s capacity to process information and make decisions.",
            "score": 74.59539794921875
        },
        {
            "docid": "30138821_9",
            "document": "Quantum cognition . The above deviations from classical rational expectations in agents\u2019 decisions under uncertainty produce well known paradoxes in behavioral economics, that is, the Allais, Ellsberg and Machina paradoxes. These deviations can be explained if one assumes that the overall conceptual landscape influences the subject\u2019s choice in a neither predictable nor controllable way. A decision process is thus an intrinsically contextual process, hence it cannot be modeled in a single Kolmogorovian probability space, which justifies the employment of quantum probability models in decision theory. More explicitly, the paradoxical situations above can be represented in a unified Hilbert space formalism where human behavior under uncertainty is explained in terms of genuine quantum aspects, namely, superposition, interference, contextuality and incompatibility.",
            "score": 74.49837493896484
        },
        {
            "docid": "1156527_8",
            "document": "Detection theory . Signal detection theory (SDT) is used when psychologists want to measure the way we make decisions under conditions of uncertainty, such as how we would perceive distances in foggy conditions. SDT assumes that the decision maker is not a passive receiver of information, but an active decision-maker who makes difficult perceptual judgments under conditions of uncertainty. In foggy circumstances, we are forced to decide how far away from us an object is, based solely upon visual stimulus which is impaired by the fog. Since the brightness of the object, such as a traffic light, is used by the brain to discriminate the distance of an object, and the fog reduces the brightness of objects, we perceive the object to be much farther away than it actually is (see also decision theory).",
            "score": 73.79654693603516
        },
        {
            "docid": "35597124_16",
            "document": "Bayesian inference in marketing . The three principle strengths of Bayes' theorem that have been identified by scholars are that it is prescriptive, complete and coherent. Prescriptive in that it is the theorem that is the simple prescription to the conclusions reached on the basis of evidence and reasoning for the consistent decision maker. It is complete because (for a given choice of model and prior distribution) the solution is often clear and unambiguous. It allows for the incorporation of prior information when available to increase the robustness of the solutions, as well as taking into consideration the costs and risks that are associated with choosing alternate decisions. Lastly Bayes theorem is coherent. It is considered the most appropriate way to update beliefs by welcoming the incorporation of new information, as is seen through the probability distributions (see Savage and De Finetti). This is further complemented by the fact that Bayes inference satisfies the likelihood principle, which states that models or inferences for datasets leading to the same likelihood function should generate the same statistical information. Bayes methods are more cost effective than the traditional frequentist take on marketing research and subsequent decision making. The probability can be assessed from a degree of belief before and after accounting for evidence, instead of calculating the probabilities of a certain decision by carrying out a large number of trials with each one producing an outcome from a set of possible outcomes. The planning and implementation of trials to see how a decision impacts in the \u2018field\u2019 e.g. observing consumers reaction to a relabeling of a product, is time consuming and costly, a method many firms cannot afford. In place of taking the frequentist route in aiming for a universally acceptable conclusion through iteration, it is sometimes more effective to take advantage of all the information available to the firm to work out the \u2018best\u2019 decision at the time, and then subsequently when new knowledge is obtained, revise the posterior distribution to be then used as the prior, thus the inferences continue to logically contribute to one another based on Bayes theorem.",
            "score": 73.64466094970703
        },
        {
            "docid": "10992052_7",
            "document": "Methods of neuro-linguistic programming . In business or therapy, the meta-model might be used to help a client elaborate the details of problems, proposals and objectives by asking about the important information that has been left out. For example, a person states that \"we need to make a decision\", a response could be to ask who will actually be doing the deciding and how exactly the process of deciding (from decision) would take place. The word 'we' does not specify who is doing the action. Also, the word 'decision' is a process which had been turned into an abstract noun. In that statement there was also an implied necessity (from need) which could also be challenged to find out if it really is a necessity.",
            "score": 73.14830017089844
        },
        {
            "docid": "22782655_54",
            "document": "Shoaling and schooling . Quorum sensing can function as a collective decision-making process in any decentralised system. A quorum response has been defined as \"a steep increase in the probability of group members performing a given behaviour once a threshold minimum number of their group mates already performing that behaviour is exceeded\". A recent investigation showed that small groups of fish used consensus decision-making when deciding which fish model to follow. The fish did this by a simple quorum rule such that individuals watched the decisions of others before making their own decisions. This technique generally resulted in the 'correct' decision but occasionally cascaded into the 'incorrect' decision. In addition, as the group size increased, the fish made more accurate decisions in following the more attractive fish model. Consensus decision-making, a form of collective intelligence, thus effectively uses information from multiple sources to generally reach the correct conclusion. Such behaviour has also been demonstrated in the shoaling behaviour of threespine sticklebacks.",
            "score": 73.06392669677734
        },
        {
            "docid": "35597124_14",
            "document": "Bayesian inference in marketing . Bayesian decision analysis can also be applied to the channel selection process. In order to help provide further information the method can be used that produces results in a profit or loss aspect. Prior information can include costs, expected profit, training expenses and any other costs relevant to the decision as well as managerial experience which can be displayed in a normal distribution. Bayesian decision making under uncertainty lets a marketing manager assess his/her options for channel logistics by computing the most profitable method choice. A number of different costs can be entered into the model that helps to assess the ramifications of change in distribution method. Identifying and quantifying all of the relevant information for this process can be very time consuming and costly if the analysis delays possible future earnings.",
            "score": 72.70346069335938
        },
        {
            "docid": "10069680_14",
            "document": "Hot and cold cognition . Research has demonstrated emotional manipulations on decision making processes. Participants who are induced with enthusiasm, anger or distress (different specific emotions) responded in different ways to the risky-choice problems, demonstrating that hot cognition, as an automatic process, affects decision making differently. Another example of hot cognition is a better predictor of negative emotional arousal as compared to cold cognition when they have a personal investment, such as wanting your team to win. In addition, hot cognition changes the way people use decision-making strategies, depending on the type of mood they are in, positive or negative. When people are in a positive mood, they tend to use compensatory, holistic strategies. This leads to a shallow and broad processing of information. In a negative mood people employ non-compensatory, narrow strategies which leads to a more detail-oriented and thorough processing of information. In the study participants were shown movie clips in order to induce a mood of happiness, anger or sadness and asked to complete a decision-making task. Researchers found that participants in the negative mood condition used more non-compensatory, specific decision-making techniques by focusing on the details of the situation. Participants in the positive mood condition used more compensatory, broad decision making techniques by focusing on the bigger picture of the situation. Also, hot cognition has been implicated in automatic processing and autobiographical memory. Furthermore, hot cognition extends outside the laboratory as exhibited in political process and criminal judgments. When police officers were induced with sadness they were more likely to think the suspect was guilty. However, if police officers were induced with anger there was no difference in judgments. There are also clinical implications for understanding certain disorders. Patients diagnosed with anorexia nervosa went through intervention training, which included hot cognition as a part of emotional processing development, did not show any improvement after this training. In another clinical population, those diagnosed with bipolar disorder exaggerated their perception of negative feedback and were less likely to adjust their decision making process in the face of risky-choices (gambling tasks).",
            "score": 72.4169692993164
        },
        {
            "docid": "13793648_6",
            "document": "Alexey Ivakhnenko . GMDH is the original method for solving problems for structural-parametric identification of models for experimental data under uncertainty. Such a problem occurs in the construction of a mathematical model that approximates the unknown pattern of investigated object or process. It uses information about it that is implicitly contained in data. GMDH differs from other methods of modelling by the active application of the following principles: automatic models generation, inconclusive decisions, and consistent selection by external criteria for finding models of optimal complexity. It had an original multilayered procedure for automatic models structure generation, which imitates the process of biological selection with consideration of pairwise successive features. Such procedure is currently used in Deep learning networks. To compare and choose optimal models, two or more subsets of a data sample are used. This makes it possible to avoid preliminary assumptions, because sample division implicitly acknowleges different types of uncertainty during the automatic construction of the optimal model.",
            "score": 72.19261932373047
        },
        {
            "docid": "19337310_45",
            "document": "Rodent . Because laboratory mice (house mice) and rats (brown rats) are widely used as scientific models to further our understanding of biology, a great deal has come to be known about their cognitive capacities. Brown rats exhibit cognitive bias, where information processing is biased by whether they are in a positive or negative affective state. For example, laboratory rats trained to respond to a specific tone by pressing a lever to receive a reward, and to press another lever in response to a different tone so as to avoid receiving an electric shock, are more likely to respond to an intermediate tone by choosing the reward lever if they have just been tickled (something they enjoy), indicating \"a link between the directly measured positive affective state and decision making under uncertainty in an animal model.\"",
            "score": 72.11819458007812
        },
        {
            "docid": "1125883_42",
            "document": "Markov decision process . If the state space and action space are finite, we could use linear programming to find the optimal policy, which was one of the earliest approaches applied. Here we only consider the ergodic model, which means our continuous-time MDP becomes an ergodic continuous-time Markov chain under a stationary policy. Under this assumption, although the decision maker can make a decision at any time at the current state, he could not benefit more by taking more than one action. It is better for him to take an action only at the time when system is transitioning from the current state to another state. Under some conditions,(for detail check Corollary 3.14 of \"Continuous-Time Markov Decision Processes\"), if our optimal value function formula_98 is independent of state formula_57, we will have the following inequality: If there exists a function formula_101, then formula_102 will be the smallest formula_103 satisfying the above equation. In order to find formula_102, we could use the following linear programming model: formula_107 is a feasible solution to the D-LP if formula_107 is nonnative and satisfied the constraints in the D-LP problem. A feasible solution formula_109 to the D-LP is said to be an optimal solution if for all feasible solution formula_107 to the D-LP. Once we have found the optimal solution formula_109, we can use it to establish the optimal policies.",
            "score": 72.0711441040039
        },
        {
            "docid": "22415983_24",
            "document": "Collective animal behavior . A recent investigation showed that small groups of fish used consensus decision-making when deciding which fish model to follow. The fish did this by a simple quorum rule such that individuals watched the decisions of others before making their own decisions. This technique generally resulted in the 'correct' decision but occasionally cascaded into the 'incorrect' decision. In addition, as the group size increased, the fish made more accurate decisions in following the more attractive fish model. Consensus decision-making, a form of collective intelligence, thus effectively uses information from multiple sources to generally reach the correct conclusion.",
            "score": 71.71784973144531
        },
        {
            "docid": "4698768_8",
            "document": "Naturalistic decision-making . Some of the first funding into NDM research came from the U.S. Army and Navy in the mid-1980s. The U.S. Navy became interested in naturalistic decisions following the 1988 USS \"Vincennes\" shoot-down incident, in which a U.S. Navy Aegis cruiser destroyed an Iranian commercial airliner, mistaking it for a hostile attacker. Both the Army and the Navy wanted to help people make high-stakes decisions under extreme time pressure and under dynamic and uncertain conditions. The NDM researchers studied people in field settings, such as Navy commanders and army small unit leaders. From this perspective, making a decision means committing oneself to a course of action where plausible alternatives exist, even if the person does not identify or compare these alternatives. The NDM movement shifted our conception of human decision making from a domain independent general approach to a knowledge based approach exemplified by decision makers who had substantial experience. The decision making process was expanded to include a prior stage of perception and recognition of situations, as well as generation of appropriate responses, not just choice from among given options. This perspective took advantage of advances in cognitive psychology such as knowledge representation concepts of scripts, schemas, and mental models, to contrast expert versus novice behavior. NDM has even affected Army doctrine. The current edition of the Army Field Manual on Command and Control (FM 101-5) includes for the first time a section on intuitive decision making, largely influenced by research on the RPD model. The field of NDM has also provided guidance for training decision making and related cognitive skills. Cannon-Bowers and Salas (1998) have described the range of lessons learned from the TADMUS (Tactical Decision Making Under Stress) project initiated by the Navy following the USS \"Vincennes\" shoot-down decision. These include methods for providing stress inoculation along with approaches for individual and team decision training.",
            "score": 71.48336791992188
        },
        {
            "docid": "7214278_17",
            "document": "Decision field theory . Mathematically, the spike activation pattern, as well as the choice and response time distributions, can be well described by what are known as diffusion models - especially in Two-alternative forced choice tasks(see Smith & Ratcliff for a summary). Diffusion models, such as the decision field theory, can be viewed as stochastic recurrent neural network models, except that the dynamics are approximated by linear systems. The linear approximation is important for maintaining a mathematically tractable analysis of systems perturbed by noisy inputs. In addition to these neuroscience applications, diffusion models (or their discrete time, random walk, analogues) have been used by cognitive scientists to model performance in a variety of tasks ranging from sensory detection, and perceptual discrimination, to memory recognition, and categorization. Thus, diffusion models provide the potential to form a theoretical bridge between neural models of sensory-motor tasks and behavioral models of complex-cognitive tasks.",
            "score": 71.41493225097656
        },
        {
            "docid": "2663218_8",
            "document": "Neuromarketing . A greater understanding of human cognition and behaviour has led to the integration of biological and social sciences: Neuromarketing, a recent method utilized to understand consumers. The concept of neuromarketing combines marketing, psychology and neuroscience. Research is conducted around the implicit motivations to understand a consumer decisions by non-invasive psychoanalysis methods of measuring brain activity. These include electroencephalography (EEG), magnetoencephalography (MEG) and functional magnetic resonance imaging (fMRI), eye tracking, electrodermal response measures and other neuro-technologies. Researchers investigate and learn how consumers respond and feel when presented with products and/or related stimuli. Observations can then be correlated with a participants surmised emotions and social interactions. Market researchers use this information to determine if products or advertisements stimulate responses in the brain linked with positive emotions. The concept of neuromarketing was therefore introduced to study relevant human emotions and behavioral patterns associated with products, ads and decision-making. Neuromarketing provides models of consumer behavior and can also be used to re-interpret extant research. It provides theorization of emotional aspects of consumer behavior.",
            "score": 71.34794616699219
        },
        {
            "docid": "18421485_13",
            "document": "Predictably Irrational . In chapter 6, Ariely collaborated with close friend George Loewenstein, a professor of economics and psychology at Carnegie Mellon University, to test the influence of arousal on decision making in high-emotion situations. Ariely and Loewenstein chose to test the effects of sexual arousal on decision-making in college-aged men at University of California, Berkeley. By using computers to stimulate sexual arousal, they determined that in a stimulated state, the young men were more likely to undergo an action that they would not normally consider. Using the data, Ariely argues that other high-emotion situations such as anger, frustration, and hunger have the potential to trigger similar effects on decision-making. In such situations our behavior is fully controlled by emotions. We are not the people we thought we were. No matter how much experience we have we make irrational decisions every time we are under the influence of arousal. Furthermore, he presents ideas to improve our decision-making abilities in other emotion-provoking situations such as safe sex, safe driving, and making other life decisions. For example, Ariely proposes an OnStar system that could potentially lower the number of car accidents in teenagers by performing tasks such as changing the car's temperature or dialing the teenager's mother when the car exceeds a set speed.",
            "score": 71.27259063720703
        },
        {
            "docid": "2364800_83",
            "document": "Environmental impact assessment . Thissen and Agusdinata have argued that little attention is given to the systematic identification and assessment of uncertainties in environmental studies which is critical in situations where uncertainty cannot be easily reduced by doing more research. In line with this, Maier et al. have concluded on the need to consider uncertainty at all stages of the decision-making process. In such a way decisions can be made with confidence or known uncertainty. These proposals are justified on data that shows that environmental assessments fail to predict accurately the impacts observed. Tenney et al. and Wood et al. have reported evidence of the intrinsic uncertainty attached to EIAs predictions from a number of case studies worldwide. The gathered evidence consisted of comparisons between predictions in EIAs and the impacts measured during, or following project implementation. In explaining this trend, Tenney et al. have highlighted major causes such as project changes, modelling errors, errors in data and assumptions taken and bias introduced by people in the projects analyzed.",
            "score": 71.25982666015625
        },
        {
            "docid": "11015023_17",
            "document": "Selective exposure theory . The theory of cognitive dissonance was developed in the mid-1950s to explain why people of strong convictions are so resistant in changing their beliefs even in the face of undeniable contradictory evidence. It occurs when people feel an attachment to and responsibility for a decision, position or behavior. It increases the motivation to justify their positions through selective exposure to confirmatory information (Fischer, 2011). Fischer suggested that people have an inner need to ensure that their beliefs and behaviors are consistent. In an experiment that employed commitment manipulations, it impacts perceived decision certainty. Participants were free to choose attitude-consistent and inconsistent information to write an essay. Those who wrote an attitude-consistent essay showed higher levels of confirmatory information search (Fischer, 2011). The levels and magnitude of dissonance also play a role. Selective exposure to consistent information is likely under certain levels of dissonance. At high levels, a person is expected to seek out information that increases dissonance because the best strategy to reduce dissonance would be to alter one's attitude or decision (Smith et al., 2008).",
            "score": 71.02672576904297
        }
    ]
}