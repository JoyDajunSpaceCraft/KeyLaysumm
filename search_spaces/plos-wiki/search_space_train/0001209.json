{
    "q": [
        {
            "docid": "19498707_2",
            "document": "Linear-nonlinear-Poisson cascade model . The linear-nonlinear-Poisson (LNP) cascade model is a simplified functional model of neural spike responses. It has been successfully used to describe the response characteristics of neurons in early sensory pathways, especially the visual system. The LNP model is generally implicit when using reverse correlation or the spike-triggered average to characterize neural responses with white-noise stimuli. There are three stages of the LNP cascade model. The first stage consists of a linear filter, or linear receptive field, which describes how the neuron integrates stimulus intensity over space and time. The output of this filter then passes through a nonlinear function, which gives the neuron's instantaneous spike rate as its output. Finally, the spike rate is used to generate spikes according to an inhomogeneous Poisson process.",
            "score": 149.8426113128662
        },
        {
            "docid": "3474296_4",
            "document": "Neuronal noise . Single neurons demonstrate different responses to specific neuronal input signals. This is commonly referred to as neural response variability. If a specific input signal is initiated in the dendrites of a neuron, then a hypervariability exists in the number of vesicles released from the axon terminal fiber into the synapse. This characteristic is true for fibers without neural input signals, such as pacemaker neurons, as mentioned previously, and cortical pyramidal neurons that have highly-irregular firing pattern. Noise generally hinders neural performance, but recent studies show, in dynamical non-linear neural networks, this statement does not always hold true. Non-linear neural networks are a network of complex neurons that have many connections with one another such as the neuronal systems found within our brains. Comparatively, linear networks are an experimental view of analyzing a neural system by placing neurons in series with each other.",
            "score": 117.2163417339325
        },
        {
            "docid": "941909_5",
            "document": "Receptive field . The auditory system processes the temporal and spectral (i.e. frequency) characteristics of sound waves, so the receptive fields of neurons in the auditory system are modeled as spectro-temporal patterns that cause the firing rate of the neuron to modulate with the auditory stimulus. Auditory receptive fields are often modeled as spectro-temporal receptive fields (STRFs), which are the specific pattern in the auditory domain that causes modulation of the firing rate of a neuron. Linear STRFs are created by first calculating a spectrogram of the acoustic stimulus, which determines the how the spectral density of the acoustic stimulus changes over time, often using the Short-time Fourier transform (STFT). Firing rate is modeled over time for the neuron, possibly using a peristimulus time histogram if combining over multiple repetitions of the acoustic stimulus. Then, linear regression is used to predict the firing rate of that neuron as a weighted sum of the spectrogram. The weights learned by the linear model are the STRF, and represent the specific acoustic pattern that causes modulation in the firing rate of the neuron. STRFs can also be understood as the transfer function that maps an acoustic stimulus input to a firing rate response output.",
            "score": 148.88663172721863
        },
        {
            "docid": "33993614_15",
            "document": "Neurocomputational speech processing . A neural mapping connects two cortical neural maps. Neural mappings (in contrast to neural pathways) store training information by adjusting their neural link weights (see artificial neuron, artificial neural networks). Neural mappings are capable of generating or activating a distributed representation (see above) of a sensory or motor state within a sensory or motor map from a punctual or local activation within the other map (see for example the synaptic projection from speech sound map to motor map, to auditory target region map, or to somatosensory target region map in the DIVA model, explained below; or see for example the neural mapping from phonetic map to auditory state map and motor plan state map in the ACT model, explained below and Fig. 3).",
            "score": 94.83012056350708
        },
        {
            "docid": "2920040_2",
            "document": "Neuronal tuning . Neuronal tuning refers to the hypothesized property of brain cells by which they selectively represent a particular type of sensory, association, motor, or cognitive information. Some neuronal responses have been hypothesized to be optimally tuned to specific patterns through experience. Neuronal tuning can be strong and sharp, as observed in primary visual cortex (area V1) (but see Carandini et al 2005 ), or weak and broad, as observed in neural ensembles. Single neurons are hypothesized to be simultaneously tuned to several modalities, such as visual, auditory, and olfactory. Neurons hypothesized to be tuned to different signals are often hypothesized to integrate information from the different sources. In computational models called neural networks, such integration is the major principle of operation. The best examples of neuronal tuning can be seen in the visual, auditory, olfactory, somatosensory, and memory systems, although due to the small number of stimuli tested the generality of neuronal tuning claims is still an open question.",
            "score": 116.37910151481628
        },
        {
            "docid": "5198024_22",
            "document": "Efficient coding hypothesis . Observed redundancy: A comparison of the number of retinal ganglion cells to the number of neurons in the primary visual cortex shows an increase in the number of sensory neurons in the cortex as compared to the retina. Simoncelli notes that one major argument of critics in that higher up in the sensory pathway there are greater numbers of neurons that handle the processing of sensory information so this should seem to produce redundancy. However, this observation may not be fully relevant because neurons have different neural coding. In his review, Simoncelli notes \"cortical neurons tend to have lower firing rates and may use a different form of code as compared to retinal neurons\". Cortical Neurons may also have the ability to encode information over longer periods of time than their retinal counterparts. Experiments done in the auditory system have confirmed that redundancy is decreased.",
            "score": 103.3009819984436
        },
        {
            "docid": "3766002_15",
            "document": "Orbitofrontal cortex . Neurons in the OFC respond both to primary reinforcers, as well as cues that predict rewards across multiple sensory domains. The evidence for responses to visual, gustatory, somatosensory, and olfactory stimuli is robust, but evidence or auditory responses are weaker. In a subset of OFC neurons, neural responses to rewards or reward cues are modulated by individual preference and by internal motivational states such as hunger. A fraction of neurons that respond to sensory cues predicting a reward are selective for reward, and exhibit reversal behavior when cue outcome relationships are swapped. Neurons in the OFC also exhibit responses to the absence of an expected reward, and punishment. Another population of neurons exhibits responses to novel stimuli and can \u201cremember\u201d familiar stimuli for up to a day.",
            "score": 91.64788508415222
        },
        {
            "docid": "941909_26",
            "document": "Receptive field . The term receptive field is also used in the context of artificial neural networks, most often in relation to convolutional neural networks (CNNs). When used in this sense, the term adopts a meaning reminiscent of receptive fields in actual biological nervous systems. CNNs have a distinct architecture, designed to mimic the way in which real animal brains are understood to function; instead of having every neuron in each layer connect to all neurons in the next layer (Multilayer perceptron), the neurons are arranged in a 3-dimensional structure in such a way as to take into account the spatial relationships between different neurons with respect to the original data. Since CNNs are used primarily in the field of computer vision, the data that the neurons represent is typically an image; each input neuron represents one pixel from the original image. The first layer of neurons is composed of all the input neurons; neurons in the next layer will receive connections from some of the input neurons (pixels), but not all, as would be the case in a MLP and in other traditional neural networks. Hence, instead of having each neuron receive connections from all neurons in the previous layer, CNNs use a receptive field-like layout in which each neuron receives connections only from a subset of neurons in the previous (lower) layer. The receptive field of a neuron in one of the lower layers encompasses only a small area of the image, while the receptive field of a neuron in subsequent (higher) layers involves a combination of receptive fields from several (but not all) neurons in the layer before (i. e. a neuron in a higher layer \"looks\" at a larger portion of the image than does a neuron in a lower layer). In this way, each successive layer is capable of learning increasingly abstract features of the original image. The use of receptive fields in this fashion is thought to give CNNs an advantage in recognizing visual patterns when compared to other types of neural networks.",
            "score": 127.56269299983978
        },
        {
            "docid": "33246145_2",
            "document": "Neural decoding . Neural decoding is a neuroscience field concerned with the hypothetical reconstruction of sensory and other stimuli from information that has already been encoded and represented in the brain by networks of neurons. Reconstruction refers to the ability of the researcher to predict what sensory stimuli the subject is receiving based purely on neuron action potentials. Therefore, the main goal of neural decoding is to characterize how the electrical activity of neurons elicit activity and responses in the brain.",
            "score": 89.15901064872742
        },
        {
            "docid": "56439577_8",
            "document": "Temporal envelope and fine structure . Responses to the temporal-envelope cues of speech or other complex sounds persist up the auditory pathway, eventually to the various fields of the auditory cortex in many animals. In the Primary Auditory Cortex, responses can encode AM rates by phase-locking up to about 20\u201330\u00a0Hz, while faster rates induce sustained and often tuned responses. A topographical representation of AM rate has been demonstrated in the primary auditory cortex of awake macaques. This representation is approximately perpendicular to the axis of the tonotopic gradient, consistent with an orthogonal organization of spectral and temporal features in the auditory cortex. Combining these temporal responses with the spectral selectivity of A1 neurons gives rise to the spectro-temporal receptive fields that often capture well cortical responses to complex modulated sounds. In secondary auditory cortical fields, responses become temporally more sluggish and spectrally broader, but are still able to phase-lock to the salient features of speech and musical sounds. Tuning to AM rates below about 64\u00a0Hz is also found in the human auditory cortex as revealed by brain-imaging techniques (fMRI) and cortical recordings in epileptic patients (electrocorticography). This is consistent with neuropsychological studies of brain-damaged patients and with the notion that the central auditory system performs some form of spectral decomposition of the ENVp of incoming sounds. Interestingly, the ranges over which cortical responses encode well the temporal-envelope cues of speech have been shown to be predictive of the human ability to understand speech. In the human superior temporal gyrus (STG), an anterior-posterior spatial organization of spectro-temporal modulation tuning has been found in response to speech sounds, the posterior STG being tuned for temporally fast varying speech sounds with low spectral modulations and the anterior STG being tuned for temporally slow varying speech sounds with high spectral modulations.",
            "score": 105.31187498569489
        },
        {
            "docid": "34004373_6",
            "document": "Sensory maps and brain development . Sensory maps are formed largely by experience. Basic wiring of the brain is established in vivo by a variety of molecular guidance cues, and the wiring is then refined by patterns of neural activity based in sensory experience. For synchronization of multiple maps, replay of sensory input in circuits allows neurons to be organized into vertical topographic functional units before horizontal integration. Neurons become specialized: in the big brown bat, delay-tuned neurons encode a target range and act as probability encoders, and this comes from experience. In the owl, auditory units responded to specific locations in space, and units were arranged systematically according to the relative locations of their receptive fields, thereby creating a physiological map of auditory space. The receptive fields of the neurons found in the midbrain auditory nucleus had receptive fields independent of nature and intensity of the sound.",
            "score": 113.46982502937317
        },
        {
            "docid": "8402086_18",
            "document": "Computational neurogenetic modeling . Because the amount of data on the interplay of genes and neurons and their effects is not enough to construct a rigorous model,  evolutionary computation is used to optimize artificial neural networks and gene regulatory networks, a common technique being the genetic algorithm. A genetic algorithm is a process that can be used to refine models by mimicking the process of natural selection observed in biological ecosystems. The primary advantages are that, due to not requiring derivative information, it can be applied to black box problems and multimodal optimization. The typical process for using genetic algorithms to refine a gene  regulatory network is: first, create a population; next, to create offspring via a crossover operation and  evaluate their fitness; then, on a group chosen for high fitness, simulate mutation via a mutation operator;  finally, taking the now mutated group, repeat this process until a desired level of fitness is demonstrated. Methods by which artificial neural networks may alter their structure without simulated mutation and fitness selection have been developed. A dynamically evolving neural network is one approach, as the creation of new connections and new neurons can  be modeled as the system adapts to new data. This enables the network to evolve in modeling accuracy without simulated natural selection. One method by which dynamically evolving networks may be optimized, called evolving layer neuron aggregation, combines neurons with sufficiently similar input weights into one neuron. This can take place during the training of the network, referred to as online aggregation, or between periods of training, referred to as offline aggregation. Experiments have suggested that offline aggregation is more efficient.",
            "score": 117.49201834201813
        },
        {
            "docid": "40409788_4",
            "document": "Convolutional neural network . Convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.",
            "score": 113.90681028366089
        },
        {
            "docid": "41121206_5",
            "document": "Phase resetting in neurons . Shifts in phase (or behavior of neurons) caused due to a perturbation (an external stimulus) can be quantified within a Phase Response Curve (PRC) to predict synchrony in coupled and oscillating neurons. These effects can be computed, in the case of advances or delays to responses, to observe the changes in the oscillatory behavior of neurons, pending on when a stimulus was applied in the phase cycle of an oscillating neuron. The key to understanding this is in the behavioral patterns of neurons and the routes neural information travels. Neural circuits are able to communicate efficiently and effectively within milliseconds of experiencing a stimulus and lead to the spread of information throughout the neural network. The study of neuron synchrony could provide information on the differences that occur in neural states such as normal and diseased states. Neurons that are involved significantly in diseases such as Alzheimers or Parkinsons diseases are shown to undergo phase resetting before launching into phase locking where clusters of neurons are able to begin firing rapidly to communicate information quickly. A phase response curve can be calculated by noting changes to its period over time depending on where in the cycle the input is applied. The perturbation left by the stimulus moves the stable cycle within the oscillation followed by a return to the stable cycle limit. The curve tracks the amount of advancement or delay due to the input in the oscillating neuron. The PRC assumes certain patterns of behavior in firing pattern as well as the network of oscillating neurons to model the oscillations. Currently, only a few circuits exist which can be modeled using an assumed firing pattern.",
            "score": 118.17566812038422
        },
        {
            "docid": "5473337_11",
            "document": "Neuroconstructivism . While neurons are embedded within networks, these networks are further embedded within the brain as a whole. Neural networks do not work in isolation, such as in the modularity of mind perspective. Instead, different regions interact through feedback processes and top-down interactions, constraining and specifying the development of each region. For example, the primary visual cortex in blind individuals has been shown to process tactile information. The function of cortical areas emerges as a result of this sensory input and competition for cortical space. \"This interactive specialization view implies that cortical regions might initially be non-specific in their responses but gradually narrow their responses as their functional specialization restricts them to a narrower set of circumstances.\"",
            "score": 91.84212231636047
        },
        {
            "docid": "35982062_6",
            "document": "Biased Competition Theory . There are two major neural pathways that process the information in the visual field; the ventral stream and the dorsal stream. The two pathways run in parallel and are both working simultaneously. The ventral stream is important for object recognition and often referred to as the \u201cwhat\u201d system of the brain; it projects to the inferior temporal cortex. The dorsal stream is important for spatial perception and performance and is referred to as the \u201cwhere\u201d system which projects to the posterior parietal cortex. According to the biased competition theory, an individual\u2019s visual system has limited capacity to process information about multiple objects at any given time. For example, if an individual was presented with two stimuli (objects) and was asked to identify attributes of each object at the same time, the individual\u2019s performance would be worse in comparison to if the objects were presented separately. This suggests multiple objects presented simultaneously in the visual field will compete for neural representation due to limited processing resources. Single cell recording studies conducted by Kastner and Ungerleider examined the neural mechanisms behind the biased competition theory. In their experiment the size of the receptive field's (RF) of neurons within the visual cortex were examined. A single visual stimulus was presented alone in a neuron\u2019s RF, followed with another stimulus presented simultaneously within the same RF. The single \u2018effective\u2019 stimuli produced a low firing rate, whereas the two stimuli presented together produced a high firing rate. The response to the paired stimuli was reduced. This suggests that when two stimuli are presented together within a neuron\u2019s RF, the stimuli are processed in a mutually suppressive manner, rather than being processed independently. This suppression process, according to Kastner and Ungerleider, occurs when two stimuli are presented together because they compete for neural representation, due to limited cognitive processing capacity. The RF experiment suggests that as the number of objects increase, the information available for each object will decrease due to increased neural workload (suppression), and decreased cognitive capacity. In order for an object in the visual field or RF be efficiently processed, there needs to be a way to bias these neurological resources towards the object. Attention prioritizes task relevant objects, biasing this process. For example, this bias can be towards an object which is currently attended to in the visual field or RF, or towards the object that is most relevant to one\u2019s behavior. Functional magnetic resonance imaging (fMRI) has shown that biased competition theory can explain the observed attention effects at a neuronal level. Attention effects bias the internal weight (strengthens connections) of task relevant features toward the attended object. This was shown by Reddy, Kanwisher, and van Rullen who found an increase in oxygenated blood to a specific neuron following a locational cue. Further neurological support comes from neurophysiological studies which have shown that attention results from Top-down biasing, which in turn influences neuronal spiking. In sum, external inputs affect the Top-down guidance of attention, which bias specific neurons in the brain.",
            "score": 94.91995644569397
        },
        {
            "docid": "41848173_2",
            "document": "Surround suppression . Surround suppression is a descriptive term referring to observations that the relative firing rate of a neuron may under certain conditions decrease when a particular stimulus is enlarged. It is has been observed in electrophysiology studies of the brain and has been noted in many sensory neurons, most notably in the early visual system. Surround suppression is defined as a reduction in the activity of a neuron in response to a stimulus outside its classical receptive field. (The classical receptive field refers to a concept of neural behavior that was understood to be invalid virtally from the start. As Spillman et al (2015)) note, quoting Kuffler (1953), \"not only the areas from which responses can actually be set up by retinal illumination may be included in a definition of the receptive field but also all areas which show a functional connection, by an inhibitory or excitatory effect on a ganglion cell.\" The necessary functional connections with other neurons influenced by stimulation outside a particular area and by dynamic processes in general, and the absence of a theoretical description of a system state to be treated as a baseline, deprive the term \"classical receptive field\" of functional meaning. The descriptor \"surround suppression\" suffers from a similar problem, as the activities of neurons in the \"surround\" of the \"classical receptive field are similarly determined by connectivities and processes involving neurons beyond it.) This nonlinear effect is one of many that reveals the complexity of biological sensory systems, and the connections of properties of neurons that may cause this effect (or its opposite) are still being studied. The characteristics, mechanisms, and perceptual consequences of this phenomenon are of interest to many communities, including neurobiology, computational neuroscience, psychology, and computer vision.",
            "score": 120.62586665153503
        },
        {
            "docid": "35982062_8",
            "document": "Biased Competition Theory . Bottom-up processes are characterized by an absence of higher level direction in sensory processing. It primarily relies on sensory information and incoming sensory information is the starting point for all Bottom-up processing. Bottom-up refers to when a feature stands out in a visual search. This is commonly called the \u201cpop-out\u201d effect. Salient features like bright colors, movement and big objects make the object \u201cpop-out\u201d of the visual search. \u201cPop-out\u201d features can often attract attention without conscious processing. Objects that stand out are often given priority (bias) in processing. Bottom-up processing is data driven, and according to this stimuli are perceived on the basis of the data which is being experienced through the senses. Evidence suggests that simultaneously presented stimuli do in fact compete in order to be represented in the visual cortex, with stimuli mutually suppressing each other to gain this representation. This was examined by Reynolds and colleagues, who looked at the size of neurons\u2019 receptive field\u2019s within the visual cortex. It was found that the presentation of a single stimulus resulted in a low firing rate while two stimuli presented together resulted in a higher firing rate. Reynolds and colleagues also found that when comparing the neural response of an individually presented visual stimulus to responses gathered from simultaneously presented stimuli, the responses of the concurrent presented stimuli were less than the sum of the responses gathered when each stimuli was presented alone. This suggests that two stimuli presented together increase neural work load required for attention. This increased neural load creates suppressive processes and causes the stimuli to compete for neural representation in the brain. Proulx and Egeth predicted that brighter objects would bias attention in favor of that object. Another prediction is that larger objects would bias the attention in favor of that object. The experiment was a computer-based visual search task, where participants searched for a target among distractions. The results of the study suggested that when irrelevant stimuli were large or bright, attention was biased towards the irrelevant objects, prioritizing them for cognitive processing. This research shows the effects of Bottom-up (stimulus-driven) processing on biased competition theory.",
            "score": 75.25745832920074
        },
        {
            "docid": "3717_69",
            "document": "Brain . Theorists have worked to understand these response patterns by constructing mathematical models of neurons and neural networks, which can be simulated using computers. Some useful models are abstract, focusing on the conceptual structure of neural algorithms rather than the details of how they are implemented in the brain; other models attempt to incorporate data about the biophysical properties of real neurons. No model on any level is yet considered to be a fully valid description of brain function, though. The essential difficulty is that sophisticated computation by neural networks requires distributed processing in which hundreds or thousands of neurons work cooperatively\u2014current methods of brain activity recording are only capable of isolating action potentials from a few dozen neurons at a time.",
            "score": 125.27488946914673
        },
        {
            "docid": "41121858_9",
            "document": "Binocular neurons . An energy model, a kind of stimulus-response model, of binocular neurons allows for investigation behind the computational function these disparity tuned cells play in the creation of depth perception. Energy models of binocular neurons involve the combination of monocular receptive fields that are either shifted in position or phase. These shifts in either position or phase allow for the simulated binocular neurons to be sensitive to disparity. The relative contributions of phase and position shifts in simple and complex cells combine together in order to create depth perception of an object in 3-dimensional space. Binocular simple cells are modeled as linear neurons. Due to the linear nature of these neurons, positive and negative values are encoded by two neurons where one neuron encodes the positive part and the other the negative part. This results in the neurons being complements of each other where the excitatory region of one binocular simple cell overlaps with the inhibitory region of another. Each neuron's response is limited such that only one may have a non-zero response for any time. This kind of limitation is called halfwave-rectifing. Binocular complex cells are modeled as energy neurons since they do not have discrete on and off regions in their receptive fields. Energy neurons sum the squared responses of two pairs of linear neurons which must be 90 degrees out of phase. Alternatively, they can also be the sum the squared responses of four halfwave-rectified linear neurons.",
            "score": 140.60520124435425
        },
        {
            "docid": "6147487_2",
            "document": "Neural coding . Neural coding is a neuroscience field concerned with characterising the hypothetical relationship between the stimulus and the individual or ensemble neuronal responses and the relationship among the electrical activity of the neurons in the ensemble. Based on the theory that sensory and other information is represented in the brain by networks of neurons, it is thought that neurons can encode both digital and analog information.",
            "score": 91.08756732940674
        },
        {
            "docid": "33767830_3",
            "document": "Conspecific song preference . The mechanism responsible for the ability to distinguish song types has not yet been fully characterized by researchers in the field of neuroethology, but it has been demonstrated that at least five different structures within the auditory pathway contain neurons that preferentially respond to conspecific song. The structure of neural networks, the morphology of neurons, and the receptor and ion channel complement of pre-synaptic connections cause some neurons to respond maximally to a particular stimulus frequency, phase, amplitude or temporal pattern, and this is known as spectral-temporal tuning. Tight spectral-temporal tuning in the auditory pathway provides the central nervous system of songbirds with the ability to discriminate between conspecific and heterospecific songs. Tuning characteristics of auditory neurons have been best characterized in zebra finch (\"Taeniopygia guttata\"), canary (\"Serinus canaria\"), European starling (\"Sturnus vulgaris\") and barn owl (\"Tyto alba\").",
            "score": 106.43940901756287
        },
        {
            "docid": "25345530_9",
            "document": "Models of neural computation . Linear systems are easier to analyze mathematically and are a persuasive assumption in many models including the McCulloch and Pitts neuron, population coding models, and the simple neurons often used in Artificial neural networks. Linearity may occur in the basic elements of a neural circuit such as the response of a postsynaptic neuron, or as an emergent property of a combination of nonlinear subcircuits. Though linearity is often seen as incorrect, there has been recent work suggesting it may, in fact, be biophysically plausible in some cases.",
            "score": 125.37144088745117
        },
        {
            "docid": "33767830_6",
            "document": "Conspecific song preference . Studies on zebra finch have shown that nuclei in the auditory thalamus, two steps up from the cochlea, do not passively relay input from peripheral sensory structures into higher forebrain structures. Thalamic nuclei show different patterns of gene expression in response to different stimuli, implicating them in the process of acoustic discrimination. Neurons in the nucleus ovoidalis (Ov) have receptive fields that are tuned to respond to the specific combination of spectral and temporal features present in syllables of conspecific song. Stimulus-selective tuning is determined by the receptor proteins and ion channels characterizing synapses of these neurons. Neurons can selectively respond to time-based differences between songs (e.g. syllable length or syllable-interval length) if they are post-synaptic to either fast-release (ionotropic) or slow-release (metabotropic) glutamate receptors.",
            "score": 102.62006211280823
        },
        {
            "docid": "941909_8",
            "document": "Receptive field . Tactile-sense-related cortical neurons have receptive fields on the skin that can be modified by experience or by injury to sensory nerves resulting in changes in the field's size and position. In general these neurons have relatively large receptive fields (much larger than those of dorsal root ganglion cells). However, the neurons are able to discriminate fine detail due to patterns of excitation and inhibition relative to the field which leads to spatial resolution.",
            "score": 99.41940307617188
        },
        {
            "docid": "56439577_9",
            "document": "Temporal envelope and fine structure . One unexpected aspect of phase locking in the auditory cortex has been observed in the responses elicited by complex acoustic stimuli with spectrograms that exhibit relatively slow envelopes (< 20\u00a0Hz), but that are carried by fast modulations that are as high as hundreds of Hertz. Speech and music, as well as various modulated noise stimuli have such temporal structure. For these stimuli, cortical responses phase-lock to \"both\" the envelope and fine-structure induced by interactions between unresolved harmonics of the sound, thus reflecting the pitch of the sound, and exceeding the typical lower limits of cortical phase-locking to the envelopes of a few 10\u2019s of Hertz. This paradoxical relation between the slow and fast cortical phase-locking to the carrier \u201cfine structure\u201d has been demonstrated both in the auditory and visual cortices. It has also been shown to be amply manifested in measurements of the spectro-temporal receptive fields of the primary auditory cortex giving them unexpectedly fine temporal accuracy and selectivity bordering on a 5-10 ms resolution. The underlying causes of this phenomenon have been attributed to several possible origins, including nonlinear synaptic depression and facilitation, and/or a cortical network of thalamic excitation and cortical inhibition. There are many functionally significant and perceptually relevant reasons for the coexistence of these two complementary dynamic response modes. They include the ability to accurately encode onsets and other rapid \u2018events\u2019 in the ENVp of complex acoustic and other sensory signals, features that are critical for the perception of consonants (speech) and percussive sounds (music), as well as the texture of complex sounds.",
            "score": 102.51891040802002
        },
        {
            "docid": "3975854_6",
            "document": "Sensory neuroscience . One major goal of sensory neuroscience is to try to estimate the neuron's receptive field; that is, to try to determine which stimuli cause the neuron to fire in what ways. One common way to find the receptive field is to use linear regression to find which stimulus characteristics typically caused neurons to become excited or depressed. Since the receptive field of a sensory neuron can vary in time (i.e. latency between the stimulus and the effect it has on the neuron) and in some spatial dimension (literally space for vision and somatosensory cells, but other \"spatial\" dimensions such as the frequency of a sound for auditory neurons), the term spatio temporal receptive field or STRF is often used to describe these receptive fields.",
            "score": 116.47372150421143
        },
        {
            "docid": "321869_55",
            "document": "Coding theory . Neural coding is a neuroscience-related field concerned with how sensory and other information is represented in the brain by networks of neurons. The main goal of studying neural coding is to characterize the relationship between the stimulus and the individual or ensemble neuronal responses and the relationship among electrical activity of the neurons in the ensemble. It is thought that neurons can encode both digital and analog information, and that neurons follow the principles of information theory and compress information, and detect and correct errors in the signals that are sent throughout the brain and wider nervous system.",
            "score": 93.13067317008972
        },
        {
            "docid": "38442646_6",
            "document": "Gain-field encoding . One of the key components of gain-field encoding is the variability in the response amplitude of the action potentials from neurons. This variability, when independent of change in response selectivity, is called gain modulation. Gain Modulation takes place in many cortical areas and is believed to be a common mechanism of neuronal computation. It allows for the combination of different sensory and cognitive information. For example, neurons implicated in processing a part of the visual field see again in the response amplitude due to shifting focus to that part of the field of vision. Therefore, neurons that are gain modulated can represent multiple types of information. The multi-modal nature of these neurons makes them ideal for specific types of computations, mainly coordinate transformations. This creates the ability to think spatially, the main contributor to physical coordination.",
            "score": 99.00174498558044
        },
        {
            "docid": "5473337_10",
            "document": "Neuroconstructivism . Cells do not develop in isolation. Even from a young age, neurons are influenced by the surrounding environment (e.g. other neurons). Over time, neurons interact either spontaneously or in response to some sensory stimulation to form neural networks. Competition between neurons plays a key role in establishing the exact pattern of connections. As a result, specific neural activation patterns may arise due to the underlying morphology and connection patterns within the specified neural structures. These may subsequently be modified by morphological change imposed by the current representations. Progressively more complex patterns may arise through manipulation of current neuronal structures by an organism's experience.",
            "score": 88.22039318084717
        },
        {
            "docid": "3996062_2",
            "document": "Spectro-temporal receptive field . The spectro-temporal receptive field or spatio-temporal receptive field (STRF) of a neuron represents which types of stimuli excite or inhibit that neuron. \"Spectro-temporal\" refers most commonly to audition, where the neuron's response depends on frequency versus time, while \"spatio-temporal\" refers to vision, where the neuron's response depends on spatial location versus time. Thus they are not exactly the same concept, but both referred to as STRF and serving a similar role in the analysis of neural responses.",
            "score": 92.85980153083801
        },
        {
            "docid": "11273721_33",
            "document": "Hierarchical temporal memory . Like any system that models details of the neocortex, HTM can be viewed as an artificial neural network. The tree-shaped hierarchy commonly used in HTMs resembles the usual topology of traditional neural networks. HTMs attempt to model cortical columns (80 to 100 neurons) and their interactions with fewer HTM \"neurons\". The goal of current HTMs is to capture as much of the functions of neurons and the network (as they are currently understood) within the capability of typical computers and in areas that can be made readily useful such as image processing. For example, feedback from higher levels and motor control are not attempted because it is not yet understood how to incorporate them and binary instead of variable synapses are used because they were determined to be sufficient in the current HTM capabilities.",
            "score": 115.90940189361572
        }
    ],
    "r": [
        {
            "docid": "19498707_2",
            "document": "Linear-nonlinear-Poisson cascade model . The linear-nonlinear-Poisson (LNP) cascade model is a simplified functional model of neural spike responses. It has been successfully used to describe the response characteristics of neurons in early sensory pathways, especially the visual system. The LNP model is generally implicit when using reverse correlation or the spike-triggered average to characterize neural responses with white-noise stimuli. There are three stages of the LNP cascade model. The first stage consists of a linear filter, or linear receptive field, which describes how the neuron integrates stimulus intensity over space and time. The output of this filter then passes through a nonlinear function, which gives the neuron's instantaneous spike rate as its output. Finally, the spike rate is used to generate spikes according to an inhomogeneous Poisson process.",
            "score": 149.8426055908203
        },
        {
            "docid": "941909_5",
            "document": "Receptive field . The auditory system processes the temporal and spectral (i.e. frequency) characteristics of sound waves, so the receptive fields of neurons in the auditory system are modeled as spectro-temporal patterns that cause the firing rate of the neuron to modulate with the auditory stimulus. Auditory receptive fields are often modeled as spectro-temporal receptive fields (STRFs), which are the specific pattern in the auditory domain that causes modulation of the firing rate of a neuron. Linear STRFs are created by first calculating a spectrogram of the acoustic stimulus, which determines the how the spectral density of the acoustic stimulus changes over time, often using the Short-time Fourier transform (STFT). Firing rate is modeled over time for the neuron, possibly using a peristimulus time histogram if combining over multiple repetitions of the acoustic stimulus. Then, linear regression is used to predict the firing rate of that neuron as a weighted sum of the spectrogram. The weights learned by the linear model are the STRF, and represent the specific acoustic pattern that causes modulation in the firing rate of the neuron. STRFs can also be understood as the transfer function that maps an acoustic stimulus input to a firing rate response output.",
            "score": 148.88662719726562
        },
        {
            "docid": "41121858_9",
            "document": "Binocular neurons . An energy model, a kind of stimulus-response model, of binocular neurons allows for investigation behind the computational function these disparity tuned cells play in the creation of depth perception. Energy models of binocular neurons involve the combination of monocular receptive fields that are either shifted in position or phase. These shifts in either position or phase allow for the simulated binocular neurons to be sensitive to disparity. The relative contributions of phase and position shifts in simple and complex cells combine together in order to create depth perception of an object in 3-dimensional space. Binocular simple cells are modeled as linear neurons. Due to the linear nature of these neurons, positive and negative values are encoded by two neurons where one neuron encodes the positive part and the other the negative part. This results in the neurons being complements of each other where the excitatory region of one binocular simple cell overlaps with the inhibitory region of another. Each neuron's response is limited such that only one may have a non-zero response for any time. This kind of limitation is called halfwave-rectifing. Binocular complex cells are modeled as energy neurons since they do not have discrete on and off regions in their receptive fields. Energy neurons sum the squared responses of two pairs of linear neurons which must be 90 degrees out of phase. Alternatively, they can also be the sum the squared responses of four halfwave-rectified linear neurons.",
            "score": 140.60519409179688
        },
        {
            "docid": "32528_17",
            "document": "Visual cortex . The tuning properties of V1 neurons (what the neurons respond to) differ greatly over time. Early in time (40 ms and further) individual V1 neurons have strong tuning to a small set of stimuli. That is, the neuronal responses can discriminate small changes in visual orientations, spatial frequencies and colors. Furthermore, individual V1 neurons in humans and animals with binocular vision have ocular dominance, namely tuning to one of the two eyes. In V1, and primary sensory cortex in general, neurons with similar tuning properties tend to cluster together as cortical columns. David Hubel and Torsten Wiesel proposed the classic ice-cube organization model of cortical columns for two tuning properties: ocular dominance and orientation. However, this model cannot accommodate the color, spatial frequency and many other features to which neurons are tuned . The exact organization of all these cortical columns within V1 remains a hot topic of current research. The mathematical modeling of this function has been compared to Gabor transforms.",
            "score": 133.6713104248047
        },
        {
            "docid": "941909_26",
            "document": "Receptive field . The term receptive field is also used in the context of artificial neural networks, most often in relation to convolutional neural networks (CNNs). When used in this sense, the term adopts a meaning reminiscent of receptive fields in actual biological nervous systems. CNNs have a distinct architecture, designed to mimic the way in which real animal brains are understood to function; instead of having every neuron in each layer connect to all neurons in the next layer (Multilayer perceptron), the neurons are arranged in a 3-dimensional structure in such a way as to take into account the spatial relationships between different neurons with respect to the original data. Since CNNs are used primarily in the field of computer vision, the data that the neurons represent is typically an image; each input neuron represents one pixel from the original image. The first layer of neurons is composed of all the input neurons; neurons in the next layer will receive connections from some of the input neurons (pixels), but not all, as would be the case in a MLP and in other traditional neural networks. Hence, instead of having each neuron receive connections from all neurons in the previous layer, CNNs use a receptive field-like layout in which each neuron receives connections only from a subset of neurons in the previous (lower) layer. The receptive field of a neuron in one of the lower layers encompasses only a small area of the image, while the receptive field of a neuron in subsequent (higher) layers involves a combination of receptive fields from several (but not all) neurons in the layer before (i. e. a neuron in a higher layer \"looks\" at a larger portion of the image than does a neuron in a lower layer). In this way, each successive layer is capable of learning increasingly abstract features of the original image. The use of receptive fields in this fashion is thought to give CNNs an advantage in recognizing visual patterns when compared to other types of neural networks.",
            "score": 127.56269073486328
        },
        {
            "docid": "25345530_9",
            "document": "Models of neural computation . Linear systems are easier to analyze mathematically and are a persuasive assumption in many models including the McCulloch and Pitts neuron, population coding models, and the simple neurons often used in Artificial neural networks. Linearity may occur in the basic elements of a neural circuit such as the response of a postsynaptic neuron, or as an emergent property of a combination of nonlinear subcircuits. Though linearity is often seen as incorrect, there has been recent work suggesting it may, in fact, be biophysically plausible in some cases.",
            "score": 125.3714370727539
        },
        {
            "docid": "3717_69",
            "document": "Brain . Theorists have worked to understand these response patterns by constructing mathematical models of neurons and neural networks, which can be simulated using computers. Some useful models are abstract, focusing on the conceptual structure of neural algorithms rather than the details of how they are implemented in the brain; other models attempt to incorporate data about the biophysical properties of real neurons. No model on any level is yet considered to be a fully valid description of brain function, though. The essential difficulty is that sophisticated computation by neural networks requires distributed processing in which hundreds or thousands of neurons work cooperatively\u2014current methods of brain activity recording are only capable of isolating action potentials from a few dozen neurons at a time.",
            "score": 125.27488708496094
        },
        {
            "docid": "29826376_7",
            "document": "Hippocampal prosthesis . First, we must take into account that, like most of biological processes, the behaviors of neurons are highly nonlinear and depend on many factors: input frequency patterns, etc. Also, a good model must take into account the fact that the expression of a single nerve cell is negligible, since the processes are carried by groups of neurons interacting in network. Once installed, the device must assume all (or at least most) of the function of the damaged hippocampus for a prolonged period of time. First, the artificial neurons must be able to work together in network just like real neurons. Then, they must be able, working and effective synaptics connections with the existing neurons of the brain; therefore a model for silicon/neurons interface will be required.",
            "score": 122.28700256347656
        },
        {
            "docid": "41848173_2",
            "document": "Surround suppression . Surround suppression is a descriptive term referring to observations that the relative firing rate of a neuron may under certain conditions decrease when a particular stimulus is enlarged. It is has been observed in electrophysiology studies of the brain and has been noted in many sensory neurons, most notably in the early visual system. Surround suppression is defined as a reduction in the activity of a neuron in response to a stimulus outside its classical receptive field. (The classical receptive field refers to a concept of neural behavior that was understood to be invalid virtally from the start. As Spillman et al (2015)) note, quoting Kuffler (1953), \"not only the areas from which responses can actually be set up by retinal illumination may be included in a definition of the receptive field but also all areas which show a functional connection, by an inhibitory or excitatory effect on a ganglion cell.\" The necessary functional connections with other neurons influenced by stimulation outside a particular area and by dynamic processes in general, and the absence of a theoretical description of a system state to be treated as a baseline, deprive the term \"classical receptive field\" of functional meaning. The descriptor \"surround suppression\" suffers from a similar problem, as the activities of neurons in the \"surround\" of the \"classical receptive field are similarly determined by connectivities and processes involving neurons beyond it.) This nonlinear effect is one of many that reveals the complexity of biological sensory systems, and the connections of properties of neurons that may cause this effect (or its opposite) are still being studied. The characteristics, mechanisms, and perceptual consequences of this phenomenon are of interest to many communities, including neurobiology, computational neuroscience, psychology, and computer vision.",
            "score": 120.62586212158203
        },
        {
            "docid": "271430_23",
            "document": "Computational neuroscience . In some cases the complex interactions between \"inhibitory\" and \"excitatory\" neurons can be simplified using mean field theory, which gives rise to the population model of neural networks. While many neurotheorists prefer such models with reduced complexity, others argue that uncovering structural functional relations depends on including as much neuronal and network structure as possible. Models of this type are typically built in large simulation platforms like GENESIS or NEURON. There have been some attempts to provide unified methods that bridge and integrate these levels of complexity.",
            "score": 119.74154663085938
        },
        {
            "docid": "41121206_5",
            "document": "Phase resetting in neurons . Shifts in phase (or behavior of neurons) caused due to a perturbation (an external stimulus) can be quantified within a Phase Response Curve (PRC) to predict synchrony in coupled and oscillating neurons. These effects can be computed, in the case of advances or delays to responses, to observe the changes in the oscillatory behavior of neurons, pending on when a stimulus was applied in the phase cycle of an oscillating neuron. The key to understanding this is in the behavioral patterns of neurons and the routes neural information travels. Neural circuits are able to communicate efficiently and effectively within milliseconds of experiencing a stimulus and lead to the spread of information throughout the neural network. The study of neuron synchrony could provide information on the differences that occur in neural states such as normal and diseased states. Neurons that are involved significantly in diseases such as Alzheimers or Parkinsons diseases are shown to undergo phase resetting before launching into phase locking where clusters of neurons are able to begin firing rapidly to communicate information quickly. A phase response curve can be calculated by noting changes to its period over time depending on where in the cycle the input is applied. The perturbation left by the stimulus moves the stable cycle within the oscillation followed by a return to the stable cycle limit. The curve tracks the amount of advancement or delay due to the input in the oscillating neuron. The PRC assumes certain patterns of behavior in firing pattern as well as the network of oscillating neurons to model the oscillations. Currently, only a few circuits exist which can be modeled using an assumed firing pattern.",
            "score": 118.17566680908203
        },
        {
            "docid": "2860430_15",
            "document": "Neural oscillation . Scientists have identified some intrinsic neuronal properties that play an important role in generating membrane potential oscillations. In particular, voltage-gated ion channels are critical in the generation of action potentials. The dynamics of these ion channels have been captured in the well-established Hodgkin\u2013Huxley model that describes how action potentials are initiated and propagated by means of a set of differential equations. Using bifurcation analysis, different oscillatory varieties of these neuronal models can be determined, allowing for the classification of types of neuronal responses. The oscillatory dynamics of neuronal spiking as identified in the Hodgkin\u2013Huxley model closely agree with empirical findings. In addition to periodic spiking, subthreshold membrane potential oscillations, i.e. resonance behavior that does not result in action potentials, may also contribute to oscillatory activity by facilitating synchronous activity of neighboring neurons. Like pacemaker neurons in central pattern generators, subtypes of cortical cells fire bursts of spikes (brief clusters of spikes) rhythmically at preferred frequencies. Bursting neurons have the potential to serve as pacemakers for synchronous network oscillations, and bursts of spikes may underlie or enhance neuronal resonance.",
            "score": 118.13045501708984
        },
        {
            "docid": "41121858_11",
            "document": "Binocular neurons . The stereo model is then made from a multitude of complex cell models that have differing disparities covering a testable range of disparities. Any individual stimulus is then distinguishable through finding the complex cell in the population with the strongest response to the stimuli. The stereo model accounts for most non-temporal physiological observations of binocular neurons as well as the correspondence problem. An important aspect of the stereo model is it accounts for disparity attraction and repulsion. An example of disparity attraction and repulsion is that at a close distance two objects appear closer in depth than in actuality, and at further distances from each other they appear further in depth than in actuality. Disparity attraction and repulsion is believed to be directly related to the physiological properties of binocular neurons in the visual cortex. Use of the stereo model has allowed for interpretation of the source of differing peak locations found in disparity tuning curves of some cells in visual cortex. These differing peak locations of the disparity tuning curves are called characteristic disparity. Due to the lack of defined disparity tuning curves for simple cells, they cannot have characteristic disparities., but the characteristic disparities can be attributed to complex cells instead. Two limitations of the stereo model is that it does not account for the response of binocular neurons in time, and that it does not give much insight into connectivity of binocular neurons.",
            "score": 117.84835815429688
        },
        {
            "docid": "38442646_13",
            "document": "Gain-field encoding . This multiplicative property is an effect of recurrent neural circuitry. A target neuron that takes only two types of direct input can only combine them additively. However mathematical models show that when also receiving recursive input from neighboring neurons, the resulting transformation to the target neurons firing rate is multiplicative. In this model, neurons with overlapping receptive fields excite each other, multiplying the strength. Likewise, neurons with non-overlapping receptive fields are inhibitory. The result is a response curve that is a scaled representation of the simple additive model. Observation of human developmental patterns also lend evidence toward this theory of gain-field encoding and gain modulation. Since arm movements are based on both intrinsic and extrinsic models, in order to build these connections one has to learn by self-generating movements and watching. By moving the arms to different parts of space and following with the eyes, the neurons form connections based on mechanical body movements as well as their positioning in an external space. Ideally this is done from every possible gaze angle and position available. This provides your brain with the proper translations by aligning the retinal (extrinsic) and body-centered (intrinsic) representations of space. It is not surprising that before babies develop motor control of their limbs, they tend to flail and watch their own limbs move.",
            "score": 117.66387176513672
        },
        {
            "docid": "8402086_18",
            "document": "Computational neurogenetic modeling . Because the amount of data on the interplay of genes and neurons and their effects is not enough to construct a rigorous model,  evolutionary computation is used to optimize artificial neural networks and gene regulatory networks, a common technique being the genetic algorithm. A genetic algorithm is a process that can be used to refine models by mimicking the process of natural selection observed in biological ecosystems. The primary advantages are that, due to not requiring derivative information, it can be applied to black box problems and multimodal optimization. The typical process for using genetic algorithms to refine a gene  regulatory network is: first, create a population; next, to create offspring via a crossover operation and  evaluate their fitness; then, on a group chosen for high fitness, simulate mutation via a mutation operator;  finally, taking the now mutated group, repeat this process until a desired level of fitness is demonstrated. Methods by which artificial neural networks may alter their structure without simulated mutation and fitness selection have been developed. A dynamically evolving neural network is one approach, as the creation of new connections and new neurons can  be modeled as the system adapts to new data. This enables the network to evolve in modeling accuracy without simulated natural selection. One method by which dynamically evolving networks may be optimized, called evolving layer neuron aggregation, combines neurons with sufficiently similar input weights into one neuron. This can take place during the training of the network, referred to as online aggregation, or between periods of training, referred to as offline aggregation. Experiments have suggested that offline aggregation is more efficient.",
            "score": 117.49201965332031
        },
        {
            "docid": "25260196_25",
            "document": "Neuronal encoding of sound . Primary auditory neurons carry action potentials from the cochlea into the transmission pathway shown in the adjacent image. Multiple relay stations act as integration and processing centers. The signals reach the first level of cortical processing at the primary auditory cortex (A1), in the superior temporal gyrus of the temporal lobe. Most areas up to and including A1 are tonotopically mapped (that is, frequencies are kept in an ordered arrangement). However, A1 participates in coding more complex and abstract aspects of auditory stimuli without coding well the frequency content, including the presence of a distinct sound or its echoes.  Like lower regions, this region of the brain has combination-sensitive neurons that have nonlinear responses to stimuli.",
            "score": 117.2945327758789
        },
        {
            "docid": "3474296_4",
            "document": "Neuronal noise . Single neurons demonstrate different responses to specific neuronal input signals. This is commonly referred to as neural response variability. If a specific input signal is initiated in the dendrites of a neuron, then a hypervariability exists in the number of vesicles released from the axon terminal fiber into the synapse. This characteristic is true for fibers without neural input signals, such as pacemaker neurons, as mentioned previously, and cortical pyramidal neurons that have highly-irregular firing pattern. Noise generally hinders neural performance, but recent studies show, in dynamical non-linear neural networks, this statement does not always hold true. Non-linear neural networks are a network of complex neurons that have many connections with one another such as the neuronal systems found within our brains. Comparatively, linear networks are an experimental view of analyzing a neural system by placing neurons in series with each other.",
            "score": 117.21633911132812
        },
        {
            "docid": "14408479_47",
            "document": "Biological neuron model . The spiking neuron model by Nossenson & Messer produces the probability of the neuron to fire a spike as a function of either an external or pharmacological stimulus. The model consists of a cascade of a receptor layer model and a spiking neuron model, as shown in Fig 4. The connection between the external stimulus to the spiking probability is made in two steps: First, a receptor cell model translates the raw external stimulus to neurotransmitter concentration, then, a spiking neuron model connects between neurotransmitter concentration to the firing rate (spiking probability). Thus, the spiking neuron model by itself depends on neurotransmitter concentration at the input stage. An important feature of this model is the prediction for neurons firing rate pattern which captures, using a low number of free parameters, the characteristic edge emphasized response of neurons to a stimulus pulse, as shown in Fig. 5. The firing rate is identified both as a normalized probability for neural spike firing, and as a quantity proportional to the current of neurotransmitters released by the cell. The expression for the firing rate takes the following form:",
            "score": 116.95869445800781
        },
        {
            "docid": "19498707_5",
            "document": "Linear-nonlinear-Poisson cascade model . For neurons sensitive to multiple dimensions of the stimulus space, the linear stage of the LNP model can be generalized to a bank of linear filters, and the nonlinearity becomes a function of multiple inputs. Let formula_9 denote the set of linear filters that capture a neuron's stimulus dependence. Then the multi-filter LNP model is described by or where formula_12 is a matrix whose columns are the filters formula_13.",
            "score": 116.91055297851562
        },
        {
            "docid": "3975854_6",
            "document": "Sensory neuroscience . One major goal of sensory neuroscience is to try to estimate the neuron's receptive field; that is, to try to determine which stimuli cause the neuron to fire in what ways. One common way to find the receptive field is to use linear regression to find which stimulus characteristics typically caused neurons to become excited or depressed. Since the receptive field of a sensory neuron can vary in time (i.e. latency between the stimulus and the effect it has on the neuron) and in some spatial dimension (literally space for vision and somatosensory cells, but other \"spatial\" dimensions such as the frequency of a sound for auditory neurons), the term spatio temporal receptive field or STRF is often used to describe these receptive fields.",
            "score": 116.47372436523438
        },
        {
            "docid": "14116393_2",
            "document": "Wilson\u2013Cowan model . In computational neuroscience, the Wilson\u2013Cowan model describes the dynamics of interactions between populations of very simple excitatory and inhibitory model neurons. It was developed by Hugh R. Wilson and Jack D. Cowan and extensions of the model have been widely used in modeling neuronal populations. The model is important historically because it uses phase plane methods and numerical solutions to describe the responses of neuronal populations to stimuli. Because the model neurons are simple, only elementary limit cycle behavior, i.e. neural oscillations, and stimulus-dependent evoked responses are predicted. The key findings include the existence of multiple stable states, and hysteresis, in the population response.",
            "score": 116.45960998535156
        },
        {
            "docid": "2920040_2",
            "document": "Neuronal tuning . Neuronal tuning refers to the hypothesized property of brain cells by which they selectively represent a particular type of sensory, association, motor, or cognitive information. Some neuronal responses have been hypothesized to be optimally tuned to specific patterns through experience. Neuronal tuning can be strong and sharp, as observed in primary visual cortex (area V1) (but see Carandini et al 2005 ), or weak and broad, as observed in neural ensembles. Single neurons are hypothesized to be simultaneously tuned to several modalities, such as visual, auditory, and olfactory. Neurons hypothesized to be tuned to different signals are often hypothesized to integrate information from the different sources. In computational models called neural networks, such integration is the major principle of operation. The best examples of neuronal tuning can be seen in the visual, auditory, olfactory, somatosensory, and memory systems, although due to the small number of stimuli tested the generality of neuronal tuning claims is still an open question.",
            "score": 116.37910461425781
        },
        {
            "docid": "2860430_24",
            "document": "Neural oscillation . Neural field models are another important tool in studying neural oscillations and are a mathematical framework describing evolution of variables such as mean firing rate in space and time. In modeling the activity of large numbers of neurons, the central idea is to take the density of neurons to the continuum limit, resulting in spatially continuous neural networks. Instead of modelling individual neurons, this approach approximates a group of neurons by its average properties and interactions. It is based on the mean field approach, an area of statistical physics that deals with large-scale systems. Models based on these principles have been used to provide mathematical descriptions of neural oscillations and EEG rhythms. They have for instance been used to investigate visual hallucinations.",
            "score": 116.24209594726562
        },
        {
            "docid": "11273721_33",
            "document": "Hierarchical temporal memory . Like any system that models details of the neocortex, HTM can be viewed as an artificial neural network. The tree-shaped hierarchy commonly used in HTMs resembles the usual topology of traditional neural networks. HTMs attempt to model cortical columns (80 to 100 neurons) and their interactions with fewer HTM \"neurons\". The goal of current HTMs is to capture as much of the functions of neurons and the network (as they are currently understood) within the capability of typical computers and in areas that can be made readily useful such as image processing. For example, feedback from higher levels and motor control are not attempted because it is not yet understood how to incorporate them and binary instead of variable synapses are used because they were determined to be sufficient in the current HTM capabilities.",
            "score": 115.9094009399414
        },
        {
            "docid": "33244792_15",
            "document": "Non-spiking neuron . By studying the nonspiking neuron, the field of neuroscience has benefited by having workable models that indicate how information is propagated through a neural network. This allows for the discussion of the factors that influence how networks work, and how they may be manipulated. Non-spiking neurons seem to be more sensitive to interference given that they exhibit graded potentials. So for non-spiking neurons, any stimulus will elicit a response, whereas spiking neurons exhibit action potentials which function as an \"all or none\" entity.",
            "score": 114.35043334960938
        },
        {
            "docid": "40409788_4",
            "document": "Convolutional neural network . Convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.",
            "score": 113.90680694580078
        },
        {
            "docid": "20512936_21",
            "document": "Dendritic spike . Computational modeling of neurons, artificial neural networking, has become a very popular tool in investigating the properties of neuronal signaling. These models are based on biological neural networks. Computational modeling can be used to study single neurons, groups of neurons, or even networks of neurons. This field has generated much interest and serves as a tool for all branches of neuroscience research including dendritic spike initiation.",
            "score": 113.77393341064453
        },
        {
            "docid": "34004373_6",
            "document": "Sensory maps and brain development . Sensory maps are formed largely by experience. Basic wiring of the brain is established in vivo by a variety of molecular guidance cues, and the wiring is then refined by patterns of neural activity based in sensory experience. For synchronization of multiple maps, replay of sensory input in circuits allows neurons to be organized into vertical topographic functional units before horizontal integration. Neurons become specialized: in the big brown bat, delay-tuned neurons encode a target range and act as probability encoders, and this comes from experience. In the owl, auditory units responded to specific locations in space, and units were arranged systematically according to the relative locations of their receptive fields, thereby creating a physiological map of auditory space. The receptive fields of the neurons found in the midbrain auditory nucleus had receptive fields independent of nature and intensity of the sound.",
            "score": 113.4698257446289
        },
        {
            "docid": "41848173_3",
            "document": "Surround suppression . The classical model of early vision presumes that each neuron responds independently to a specific stimulus in a localized area of the visual field. (According to Carandini et al (2005), this computational model, which may be fit to various datasets, \"degrade[s] quickly if we change almost any aspect of the test stimulus.\") The stimulus and corresponding location in the visual field are collectively called the classical receptive field. However, not all effects can be explained by via ad hoc independent filters. Surround suppression is one of an infinite number of possible effects in which neurons do not behave according to the classical model. These effects are collectively called non-classical receptive field effects, and have recently become a substantial research area in vision and other sensory systems.",
            "score": 113.17513275146484
        },
        {
            "docid": "162435_24",
            "document": "Mind uploading . A sufficiently complex and accurate model of the neurons is required. A traditional artificial neural network model, for example multi-layer perceptron network model, is not considered as sufficient. A dynamic spiking neural network model is required, which reflects that the neuron fires only when a membrane potential reaches a certain level. It is likely that the model must include delays, non-linear functions and differential equations describing the relation between electrophysical parameters such as electrical currents, voltages, membrane states (ion channel states) and neuromodulators.",
            "score": 112.95603942871094
        },
        {
            "docid": "271430_8",
            "document": "Computational neuroscience . About 40 years later, Hodgkin & Huxley developed the voltage clamp and created the first biophysical model of the action potential. Hubel & Wiesel discovered that neurons in the primary visual cortex, the first cortical area to process information coming from the retina, have oriented receptive fields and are organized in columns. David Marr's work focused on the interactions between neurons, suggesting computational approaches to the study of how functional groups of neurons within the hippocampus and neocortex interact, store, process, and transmit information. Computational modeling of biophysically realistic neurons and dendrites began with the work of Wilfrid Rall, with the first multicompartmental model using cable theory.",
            "score": 112.93861389160156
        },
        {
            "docid": "1164_53",
            "document": "Artificial intelligence . Neural networks, or neural nets, were inspired by the architecture of neurons in the human brain. A simple \"neuron\" \"N\" accepts input from multiple other neurons, each of which, when activated (or \"fired\"), cast a weighted \"vote\" for or against whether neuron \"N\" should itself activate. Learning requires an algorithm to adjust these weights based on the training data; one simple algorithm (dubbed \"fire together, wire together\") is to increase the weight between two connected neurons when the activation of one triggers the successful activation of another. The net forms \"concepts\" that are distributed among a subnetwork of shared neurons that tend to fire together; a concept meaning \"leg\" might be coupled with a subnetwork meaning \"foot\" that includes the sound for \"foot\". Neurons have a continuous spectrum of activation; in addition, neurons can process inputs in a nonlinear way rather than weighing straightforward votes. Modern neural nets can learn both continuous functions and, surprisingly, digital logical operations. Neural networks' early successes included predicting the stock market and (in 1995) a mostly self-driving car. In the 2010s, advances in neural networks using deep learning thrust AI into widespread public consciousness and contributed to an enormous upshift in corporate AI spending; for example, AI-related M&A in 2017 was over 25 times as large as in 2015.",
            "score": 112.93096160888672
        }
    ]
}