{
    "q": [
        {
            "docid": "33822344_13",
            "document": "Synaptic scaling . Hebbian plasticity and homeostatic plasticity have a hand-in-glove relationship. Neurons use Hebbian plasticity mechanisms to modify their synaptic connections within the neural circuit based on the correlated input they receive from other neurons. Long-term potentiation (LTP) mechanisms are driven by related pre-synaptic and post-synaptic neuron firings; with the help of homeostatic plasticity, LTPs and LTDs create and maintain the precise synaptic weights in the neural network. Persisting correlated neural activity\u2014without a homeostatic feedback loop\u2014causes LTP mechanisms to continually up regulate synaptic connection strengths. Unspecified strengthening of synaptic weights causes neural activity to become unstable to the point that insignificant stimulatory perturbations can trigger chaotic, synchronous network-wide firing known as bursts. This renders the neural network incapable of computing. Since homeostatic plasticity normalizes the synaptic strengths of all neurons in a network, the overall neural network activity stabilizes.",
            "score": 66.25738978385925
        },
        {
            "docid": "39182600_13",
            "document": "Heterosynaptic plasticity . A neural network that undergoes plastic changes between synapses must initiate normalization mechanisms in order to combat unrestrained potentiation or depression. One mechanism assures that the average firing rate of these neurons is kept at a reasonable rate through synaptic scaling. In this process, input levels are changed in cells to maintain average firing rate. For example, inhibitory synapses are strengthened or excitatory synapses are weakened to normalize the neural network and allow single neurons to regulate their firing rate. Another mechanism is the cell-wide redistribution of synaptic weight. This mechanism conserves the total synaptic weight across the cell by introducing competition between synapses. Thus, normalizing a single neuron after plasticity. During development, cells can be refined when some synapses are preserved and others are discarded to normalize total synaptic weight. In this way, homeostasis is conserved in cells that are undergoing plasticity and normal operation of learning networks is also preserved, allowing new information to be learned.",
            "score": 59.365219593048096
        },
        {
            "docid": "233497_5",
            "document": "Unsupervised learning . The classical example of unsupervised learning in the study of both natural and artificial neural networks is subsumed by Donald Hebb's principle, that is, neurons that fire together wire together. In Hebbian learning, the connection is reinforced irrespective of an error, but is exclusively a function of the coincidence between action potentials between the two neurons. A similar version that modifies synaptic weights takes into account the time between the action potentials (spike-timing-dependent plasticity or STDP). Hebbian Learning has been hypothesized to underlie a range of cognitive functions, such as pattern recognition and experiential learning.",
            "score": 108.15966653823853
        },
        {
            "docid": "25253854_5",
            "document": "Developmental plasticity . In order to maintain balance, homeostatic controls exist to regulate the overall activity of neural circuits specifically by regulating the destabilizing effects of developmental and learning processes that result in changes of synaptic strength. Homeostatic plasticity also helps regulate prolonged excitatory responses, which lead to a reduction in all of a neuron\u2019s synaptic responses. While the exact mechanisms by which homeostatic plasticity acts remains unclear, recent studies raise the idea that homeostatic plasticity is modulated according to the period of development or challenges in existing neural circuits.",
            "score": 60.38019895553589
        },
        {
            "docid": "613539_3",
            "document": "Spike-timing-dependent plasticity . Under the STDP process, if an input spike to a neuron tends, on average, to occur immediately \"before\" that neuron's output spike, then that particular input is made somewhat stronger. If an input spike tends, on average, to occur immediately \"after\" an output spike, then that particular input is made somewhat weaker hence: \"spike-timing-dependent plasticity\". Thus, inputs that might be the cause of the post-synaptic neuron's excitation are made even more likely to contribute in the future, whereas inputs that are not the cause of the post-synaptic spike are made less likely to contribute in the future. The process continues until a subset of the initial set of connections remain, while the influence of all others is reduced to 0. Since a neuron produces an output spike when many of its inputs occur within a brief period, the subset of inputs that remain are those that tended to be correlated in time. In addition, since the inputs that occur before the output are strengthened, the inputs that provide the earliest indication of correlation will eventually become the final input to the neuron.",
            "score": 72.31953454017639
        },
        {
            "docid": "39182600_2",
            "document": "Heterosynaptic plasticity . Synaptic plasticity refers to a chemical synapse's ability to undergo changes in strength. Synaptic plasticity is typically input-specific (i.e. homosynaptic plasticity), meaning that the activity in a particular neuron alters the efficacy of a synaptic connection between that neuron and its target. However, in the case of heterosynaptic plasticity, the activity of a particular neuron leads to input unspecific changes in the strength of synaptic connections from other unactivated neurons. A number of distinct forms of heterosynaptic plasticity have been found in a variety of brain regions and organisms. These different forms of heterosynaptic plasticity contribute to a variety of neural processes including associative learning, the development of neural circuits, and homeostasis of synaptic input.",
            "score": 61.322415351867676
        },
        {
            "docid": "45627335_9",
            "document": "Tempotron . Next, a binary classification of the input patterns is needed(formula_12 refers to a pattern which should elicit at least one post synaptic action potential and formula_13 refers to a pattern which should have no response accordingly). In the beginning, the neuron does not know which pattern belongs to which classification and has to learn it iteratively, similar to the perceptron . The tempotron learns its tasks by adapting the synaptic efficacy formula_4. If a formula_15 pattern is presented and the postsynaptic neuron did not spike, all synaptic efficacies are increased by formula_16 whereas a formula_17 pattern followed by a postsynaptic response leads to a decrease of the synaptic efficacies by formula_18 with",
            "score": 54.77764940261841
        },
        {
            "docid": "613539_5",
            "document": "Spike-timing-dependent plasticity . Early experiments on associative plasticity were carried out by W. B. Levy and O. Steward in 1983 and examined the effect of relative timing of pre and postsynaptic action potentials at millisecond level on plasticity. Bruce McNaughton contributed much to this area, too. In studies on neuromuscular synapses carried out by Y. Dan and Mu-ming Poo in 1992, and on the hippocampus by D. Debanne, B. G\u00e4hwiler, and S. Thompson in 1994, showed that asynchronous pairing of postsynaptic and synaptic activity induced long-term synaptic depression. However, STDP was more definitively demonstrated by Henry Markram in his postdoc period till 1993 in Bert Sakmann's lab (SFN and Phys Soc abstracts in 1994\u20131995) which was only published in 1997. C. Bell and co-workers also found a form of STDP in the cerebellum. Henry Markram used dual patch clamping techniques to repetitively activate pre-synaptic neurons 10 milliseconds before activating the post-synaptic target neurons, and found the strength of the synapse increased. When the activation order was reversed so that the pre-synaptic neuron was activated 10 milliseconds after its post-synaptic target neuron, the strength of the pre-to-post synaptic connection decreased. Further work, by Guoqiang Bi, Li Zhang, and Huizhong Tao in Mu-Ming Poo's lab in 1998, continued the mapping of the entire time course relating pre- and post-synaptic activity and synaptic change, to show that in their preparation synapses that are activated within 5-20 ms before a postsynaptic spike are strengthened, and those that are activated within a similar time window after the spike are weakened. This phenomenon has been observed in various other preparations, with some variation in the time-window relevant for plasticity. Several reasons for timing-dependent plasticity have been suggested. For example, STDP might provide a substrate for Hebbian learning during development, or, as suggested by Taylor in 1973, the associated Hebbian and anti-Hebbian learning rules might create informationally efficient coding in bundles of related neurons. Works from Y. Dan's lab advanced to study STDP in \"in vivo\" systems.",
            "score": 76.48222267627716
        },
        {
            "docid": "21445461_21",
            "document": "Nonsynaptic plasticity . One mechanism for preserving the dynamic range of a neuron is synaptic scaling, the homeostatic form of plasticity that restores neuronal activity to its normal 'baseline' levels by changing the postsynaptic response of all the synapses of a neuron as a function of activity. This means that the same scaling is done to each synapse, to either strengthen or weaken all of a neuron\u2019s connections. Scaling can be multiplicative (multiplying or dividing the strength of each synapse by a constant number) or additive (adding or subtracting the same value from the synaptic weight). Homeostatic mechanisms go beyond the synapse. Modulation of the intrinsic excitability of a neuron is a way to maintain stability despite changing numbers and strengths of synapses. Cultured cortical pyramidal neurons maintain stability through the regulation of ionic conductances. The regulation of ionic conductances is achieved through the controlled release of brain-derived neurotrophic factor (BDNF). BDNF has also been found to influence synaptic scaling, suggesting that this neurotrophic factor may be responsible for the coordination of synaptic and nonsynaptic mechanisms in homeostatic plasticity.",
            "score": 55.27358531951904
        },
        {
            "docid": "12237203_6",
            "document": "Henry Markram . Some of his work altered the relative timing of single pre- and post-synaptic action potentials to reveal a learning mechanism operating between neurons where the relative timing in the millisecond range affects the coupling strength between neurons. The importance of such timing has been reproduced in many brain regions and is known as spike timing-dependent synaptic plasticity (STDP).",
            "score": 69.26733136177063
        },
        {
            "docid": "38870173_8",
            "document": "Feature learning . Neural networks are a family of learning algorithms that use a \"network\" consisting of multiple layers of inter-connected nodes. It is inspired by the animal nervous system, where the nodes are viewed as neurons and edges are viewed as synapses. Each edge has an associated weight, and the network defines computational rules for passing input data from the network's input layer to the output layer. A network function associated with a neural network characterizes the relationship between input and output layers, which is parameterized by the weights. With appropriately defined network functions, various learning tasks can be performed by minimizing a cost function over the network function (weights).",
            "score": 53.00130844116211
        },
        {
            "docid": "22000_7",
            "document": "Neural Darwinism . Once the basic variegated anatomical structure of the brain is laid down during early development, it is more or less fixed. But given the numerous and diverse collection of available circuitry, there are bound to be functionally equivalent albeit anatomically non-isomorphic neuronal groups capable of responding to certain sensory input. This creates a competitive environment where circuit groups proficient in their responses to certain inputs are \"chosen\" through the enhancement of the synaptic efficacies of the selected network. This leads to an increased probability that the same network will respond to similar or identical signals at a future time. This occurs through the strengthening of neuron-to-neuron synapses. And these adjustments allow for neural plasticity along a fairly quick timetable.",
            "score": 58.94944953918457
        },
        {
            "docid": "21445461_2",
            "document": "Nonsynaptic plasticity . Nonsynaptic plasticity is a form of neuroplasticity that involves modification of ion channel function in the axon, dendrites, and cell body that results in specific changes in the integration of excitatory postsynaptic potentials (EPSPs) and inhibitory postsynaptic potentials (IPSPs). Nonsynaptic plasticity is a modification of the intrinsic excitability of the neuron. It interacts with synaptic plasticity, but it is considered a separate entity from synaptic plasticity. Intrinsic modification of the electrical properties of neurons plays a role in many aspects of plasticity from homeostatic plasticity to learning and memory itself. Nonsynaptic plasticity affects synaptic integration, subthreshold propagation, spike generation, and other fundamental mechanisms of neurons at the cellular level. These individual neuronal alterations can result in changes in higher brain function, especially learning and memory. However, as an emerging field in neuroscience, much of the knowledge about nonsynaptic plasticity is uncertain and still requires further investigation to better define its role in brain function and behavior.",
            "score": 86.12577176094055
        },
        {
            "docid": "41027689_2",
            "document": "Synaptic transistor . A synaptic transistor is an electrical device that can learn in ways similar to a neural synapse. It optimizes its own properties for the functions it has carried out in the past. The device mimics the behavior of the property of neurons called spike-timing-dependent plasticity, or STDP.",
            "score": 78.39071297645569
        },
        {
            "docid": "21523_4",
            "document": "Artificial neural network . In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called 'edges'. Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times.",
            "score": 59.16484522819519
        },
        {
            "docid": "3474296_6",
            "document": "Neuronal noise . Another theory suggests that stochastic noise in a non-linear network shows a positive relationship between the interconnectivity and noise-like activity. Thus based on this theory, Patrick Wilken and colleagues suggest that neuronal noise is the principal factor that limits the capacity of visual short-term memory. Investigators of neural ensembles and those who especially support the theory of distributed processing, propose that large neuronal populations effectively decrease noise by averaging out the noise in individual neurons. Some investigators have shown in experiments and in models that neuronal noise is a possible mechanism to facilitate neuronal processing. The presence of neuronal noise (or more specifically synaptic noise) confers to neurons more sensitivity to a broader range of inputs, it can equalize the efficacy of synaptic inputs located at different positions on the neuron, and it can also enable finer temporal discrimination. There are many theories of why noise is apparent in the neuronal networks, but many neurologists are unclear of why they exist.",
            "score": 55.2567880153656
        },
        {
            "docid": "35502244_2",
            "document": "Neuronal memory allocation . Memory allocation is a process that determines which specific synapses and neurons in a neural network will store a given memory. Although multiple neurons can receive a stimulus, only a subset of the neurons will induce the necessary plasticity for memory encoding. The selection of this subset of neurons is termed neuronal allocation. Similarly, multiple synapses can be activated by a given set of inputs, but specific mechanisms determine which synapses actually go on to encode the memory, and this process is referred to as synaptic allocation.. Memory allocation was first discovered in the lateral amygdala by Sheena Josselyn and colleagues in Alcino J Silva's laboratory.",
            "score": 58.05298829078674
        },
        {
            "docid": "33244792_4",
            "document": "Non-spiking neuron . There are an abundance of neurons that propagate signals via action potentials and the mechanics of this particular kind of transmission is well understood. Spiking neurons exhibit action potentials as a result of a neuron characteristic known as membrane potential. Through studying these complex spiking networks in animals, a neuron that did not exhibit characteristic spiking behavior was discovered. These neurons use a graded potential to transmit data as they lack the membrane potential that spiking neurons possess. This method of transmission has a huge effect on the fidelity, strength, and lifetime of the signal. Non-spiking neurons were identified as a special kind of interneuron and function as an intermediary point of process for sensory-motor systems. Animals have become substantial models for understanding more about non-spiking neural networks and the role they play in an animal\u2019s ability to process information and its overall function. Animal models indicate that the interneurons modulate directional and posture coordinating behaviors. Crustaceans and arthropods such as the crawfish have created many opportunities to learn about the modulatory role that these neurons have in addition to their potential to be modulated regardless of their lack of exhibiting spiking behavior. Most of the known information about nonspiking neurons is derived from animal models. Studies focus on neuromuscular junctions and modulation of abdominal motor cells. Modulatory interneurons are neurons that are physically situated next to muscle fibers and innervate the nerve fibers which allow for some orienting movement. These modulatory interneurons are usually nonspiking neurons. Advances in studying nonspiking neurons included determining new delineations among the different types of interneurons. These discoveries were due to the usage of methods such as protein receptor silencing. Studies have been done on the non-spiking neuron qualities in animals of specific non-spiking neural networks that have a corollary in humans, e.g. retina amacrine cell of the eye.",
            "score": 102.89958822727203
        },
        {
            "docid": "9916386_15",
            "document": "Synaptic gating . In studies with rodents, the prefrontal cortex, specifically the medial prefrontal cortex (mPFC) has been implicated in the processing of information lasting from milliseconds to several seconds, while the hippocampus has been implicated in the processing of information for longer time scales \u2013 such as minutes to hours. Damage to both these areas in people with ADHD seems to illustrate why they exhibit inattentiveness and impulsiveness. Nucleus accumbens neurons are bistable and thus can be selectively gated to either an \"up\" \u2013 depolarized state or a \"down\" \u2013 hyperpolarized state. Nucleus accumbens neurons are gated by hippocampal and amygdala input and this creates a depolarized accumbens neuron that is more receptive to innervation from input from the prefrontal cortex. Thus, in patients with ADHD not only is the input from the prefrontal cortex to the nucleus accumbens reduced but in addition the gating input from the hippocampus to the nucleus accumbens is also reduced leading to a reduction in activation of the nucleus accumbens neurons. Individuals that take medication such as methylphenidate (Ritalin) will increase their dopamine (DA) output along many of these synapses helping to compensate in the loss of synaptic activity generated from the pathophysiology of ADHD. Taking methylphenidate can increase DA projections to the nucleus accumbens, which can not only act to increase synaptic activity between the prefrontal cortex and hippocampus (improving memory) but also act as a reward system as the nucleus accumbens is part of the mesolimbic pathway. Moreover, it is possibly why individuals on Ritalin have a \u201cneed\u201d and \u201cdesire\u201d to learn as it acts as a positive reinforcer in the brain. In addition, this reward circuitry activation is most likely a reason why methylphenidate is highly addictive and carries great dependence. In conclusion, synaptic gating illustrates a plausible mechanism by which ADHD medication like Ritalin modulates synaptic activity and memory.",
            "score": 57.58293008804321
        },
        {
            "docid": "10839226_23",
            "document": "Cultured neuronal network . In order to establish learning in a cultured network, researchers have attempted to re-embody the dissociated neuronal networks in either simulated or real environments (see MEART and animat). Through this method the networks are able to interact with their environment and, therefore, have the opportunity to learn in a more realistic setting. Other studies have attempted to imprint signal patterns onto the networks via artificial stimulation. This can be done by inducing network bursts or by inputing specific patterns to the neurons, from which the network is expected to derive some meaning (as in experiments with animats, where an arbitrary signal to the network indicates that the simulated animal has run into a wall or is moving in a direction, etc.). The latter technique attempts to take advantage of the inherent ability of neuronal networks to make sense of patterns. However, experiments have had limited success in demonstrating a definition of learning that is widely agreed upon. Nevertheless, plasticity in neuronal networks is a phenomenon that is well-established in the neuroscience community, and one that is thought to play a very large role in learning.",
            "score": 89.93974077701569
        },
        {
            "docid": "33822344_2",
            "document": "Synaptic scaling . In neuroscience, synaptic scaling (or homeostatic scaling) is a form of homeostatic plasticity, in which the brain responds to chronically elevated activity in a neural circuit with negative feedback, allowing individual neurons to reduce their overall action potential firing rate. Where Hebbian plasticity mechanisms modify neural synaptic connections selectively, synaptic scaling normalizes all neural synaptic connections by decreasing the strength of each synapse by the same factor (multiplicative change), so that the relative synaptic weighting of each synapse is preserved.",
            "score": 44.10488939285278
        },
        {
            "docid": "36086848_7",
            "document": "Fear processing in the brain . Synaptic input can be strengthened when activity in the presynaptic neuron co-occurs with depolarization in the postsynaptic neuron. This is known as Hebbian synaptic plasticity. This hypothesis is especially appealing as an explanation for how simple associative learning, such as that taking place in fear conditioning, might occur. In this model of fear conditioning, strong depolarization of the lateral amygdala elicited by the stimulus leads to the strengthening of temporally and spatially relative conditioned stimulus inputs (that are coactive) onto the same neurons. Experimental data has been shown to support the idea that the plasticity and fear memory formation in the lateral amygdala are triggered by unconditioned stimulus-induced activation of the region's neurons. Thus, unconditioned stimulus-evoked depolarization is necessary for the enhancement of conditioned stimulus-elicited neural responses in this region after conditioned-unconditioned pairing and pairing a conditioned stimulus with direct depolarization of the lateral amygdala's pyramidal neurons as an unconditioned stimulus supports fear conditioning. It is also clear that synaptic plasticity at conditioned stimulus input pathways to the lateral amygdala does occur with fear conditioning.",
            "score": 61.1835823059082
        },
        {
            "docid": "14901871_4",
            "document": "Didactic organisation . Didactic organisation is primarily a consequence of spike-timing-dependent plasticity, because when the neurons within an interconnected network undergo action potentials (or \u2018spikes\u2019) at approximately the same time (within the order of tens of milliseconds) the efferent synaptic connections of neurons that spike early will have their efficacy increased (long-term potentiation), while neurons that spike late will have the efficacy of their efferent synaptic connections decreased (long-term depression).",
            "score": 56.618489503860474
        },
        {
            "docid": "941909_26",
            "document": "Receptive field . The term receptive field is also used in the context of artificial neural networks, most often in relation to convolutional neural networks (CNNs). When used in this sense, the term adopts a meaning reminiscent of receptive fields in actual biological nervous systems. CNNs have a distinct architecture, designed to mimic the way in which real animal brains are understood to function; instead of having every neuron in each layer connect to all neurons in the next layer (Multilayer perceptron), the neurons are arranged in a 3-dimensional structure in such a way as to take into account the spatial relationships between different neurons with respect to the original data. Since CNNs are used primarily in the field of computer vision, the data that the neurons represent is typically an image; each input neuron represents one pixel from the original image. The first layer of neurons is composed of all the input neurons; neurons in the next layer will receive connections from some of the input neurons (pixels), but not all, as would be the case in a MLP and in other traditional neural networks. Hence, instead of having each neuron receive connections from all neurons in the previous layer, CNNs use a receptive field-like layout in which each neuron receives connections only from a subset of neurons in the previous (lower) layer. The receptive field of a neuron in one of the lower layers encompasses only a small area of the image, while the receptive field of a neuron in subsequent (higher) layers involves a combination of receptive fields from several (but not all) neurons in the layer before (i. e. a neuron in a higher layer \"looks\" at a larger portion of the image than does a neuron in a lower layer). In this way, each successive layer is capable of learning increasingly abstract features of the original image. The use of receptive fields in this fashion is thought to give CNNs an advantage in recognizing visual patterns when compared to other types of neural networks.",
            "score": 70.48430693149567
        },
        {
            "docid": "39182600_3",
            "document": "Heterosynaptic plasticity . Heterosynaptic plasticity may play an important homeostatic role in neural plasticity by normalizing or limiting the total change of synaptic input during ongoing Hebbian plasticity.  Hebbian plasticity, an ubiquitous form of homosynaptic, associative plasticity, is believed to underlie learning and memory. Moreover, Hebbian plasticity is induced by and amplifies correlations in neural circuits which creates a positive feedback loop and renders neural circuits unstable. To avoid this instability Hebbian plasticity needs to be constrained, for instance by the conservation of the total amount of synaptic input. This role is believed to be fulfilled by a diversity of homeostatic mechanisms.",
            "score": 40.1302729845047
        },
        {
            "docid": "423771_11",
            "document": "Synaptic plasticity . If the strength of a synapse is only reinforced by stimulation or weakened by its lack, a positive feedback loop will develop, causing some cells never to fire and some to fire too much. But two regulatory forms of plasticity, called scaling and metaplasticity, also exist to provide negative feedback. Synaptic scaling is a primary mechanism by which a neuron is able to stabilize firing rates up or down. Synaptic scaling serves to maintain the strengths of synapses relative to each other, lowering amplitudes of small excitatory postsynaptic potentials in response to continual excitation and raising them after prolonged blockage or inhibition. This effect occurs gradually over hours or days, by changing the numbers of NMDA receptors at the synapse (P\u00e9rez-Ota\u00f1o and Ehlers, 2005). Metaplasticity varies the threshold level at which plasticity occurs, allowing integrated responses to synaptic activity spaced over time and preventing saturated states of LTP and LTD. Since LTP and LTD (long-term depression) rely on the influx of Ca through NMDA channels, metaplasticity may be due to changes in NMDA receptors, altered calcium buffering, altered states of kinases or phosphatases and a priming of protein synthesis machinery. Synaptic scaling is a primary mechanism by which a neuron to be selective to its varying inputs. The neuronal circuitry affected by LTP/LTD and modified by scaling and metaplasticity leads to reverberatory neural circuit development and regulation in a Hebbian manner which is manifested as memory, whereas the changes in neural circuitry, which begin at the level of the synapse, are an integral part in the ability of an organism to learn.",
            "score": 51.556392192840576
        },
        {
            "docid": "14901871_6",
            "document": "Didactic organisation . A third important feature for didactic organisation in vivo concerns the spatial scale of spike propagation within a network. While it is expected that didactic organisation will always be present among neurons that exhibit spike timing-dependent plasticity and causal activity (see above), the spatial scale over which didactic organisation can occur between neurons within a network should be limited by the spatial scale of spike propagation. Evidence suggests that the scale of spike propagation can be actively controlled by adjusting the balance of excitation and inhibition within a network (a balance that can be modulated by synaptic scaling, for example), thus providing a means by which a network can actively control when and to what extent didactic organisation can occur. For this reason, and the very specific connectivity patterns that can be achieved via didactic organisation, it has been speculated that didactic organisation may play an important role in brain development.",
            "score": 62.20967650413513
        },
        {
            "docid": "10159567_2",
            "document": "Spiking neural network . Spiking neural networks (SNNs) fall into the third generation of artificial neural network models, increasing the level of realism in a neural simulation. In addition to neuronal and synaptic state, SNNs also incorporate the concept of time into their operating model. The idea is that neurons in the SNN do not fire at each propagation cycle (as it happens with typical multi-layer perceptron networks), but rather fire only when a membrane potential\u00a0\u2013 an intrinsic quality of the neuron related to its membrane electrical charge\u00a0\u2013 reaches a specific value. When a neuron fires, it generates a signal which travels to other neurons which, in turn, increase or decrease their potentials in accordance with this signal.",
            "score": 70.74658060073853
        },
        {
            "docid": "8887731_2",
            "document": "Echo state network . The echo state network (ESN), is a recurrent neural network with a sparsely connected hidden layer (with typically 1% connectivity). The connectivity and weights of hidden neurons are fixed and randomly assigned. The weights of output neurons can be learned so that the network can (re)produce specific temporal patterns. The main interest of this network is that although its behaviour is non-linear, the only weights that are modified during training are for the synapses that connect the hidden neurons to output neurons. Thus, the error function is quadratic with respect to the parameter vector and can be differentiated easily to a linear system.",
            "score": 48.5106737613678
        },
        {
            "docid": "21523_82",
            "document": "Artificial neural network . A LAMSTAR neural network may serve as a dynamic neural network in spatial or time domains or both. Its speed is provided by Hebbian link-weights that integrate the various and usually different filters (preprocessing functions) into its many layers and to dynamically rank the significance of the various layers and functions relative to a given learning task. This grossly imitates biological learning which integrates various preprocessors (cochlea, retina, \"etc.\") and cortexes (auditory, visual, \"etc.\") and their various regions. Its deep learning capability is further enhanced by using inhibition, correlation and its ability to cope with incomplete data, or \"lost\" neurons or layers even amidst a task. It is fully transparent due to its link weights. The link-weights allow dynamic determination of innovation and redundancy, and facilitate the ranking of layers, of filters or of individual neurons relative to a task.",
            "score": 71.97161734104156
        },
        {
            "docid": "33826069_3",
            "document": "Viral neuronal tracing . Most neuroanatomists would agree that understanding how the brain is connected to itself and the body is of paramount importance. As such, it is of equal importance to have a way to visualize and study the connections among neurons. Neuronal tracing methods offer an unprecedented view into the morphology and connectivity of neural networks. Depending on the tracer used, this can be limited to a single neuron or can progress trans-synaptically to adjacent neurons. After the tracer has spread sufficiently, the extent may be measured either by fluorescence (for dyes) or by immunohistochemistry (for biological tracers). An important innovation in this field is the use of neurotropic viruses as tracers. These not only spread throughout the initial site of infection, but can jump across synapses. The use of a virus provides a self-replicating tracer. This can allow for the elucidation of neural microcircuitry to an extent that was previously unobtainable.  This has significant implications for the real world. If we can better understand what parts of the brain are intimately connected, we can predict the effect of localized brain injury. For example, if a patient has a stroke in the amygdala, primarily responsible for emotion, the patient might also have trouble learning to perform certain tasks because the amygdala is highly interconnected with the orbitofrontal cortex, responsible for reward learning. As always, the first step to solving a problem is fully understanding it, so if we are to have any hope of fixing brain injury, we must first understand its extent and complexity.",
            "score": 73.30500841140747
        },
        {
            "docid": "21445461_10",
            "document": "Nonsynaptic plasticity . Nonsynaptic plasticity has an excitatory effect on the generation of spikes. The increase in spike generation has been correlated with a decrease in the spike threshold, a response from nonsynaptic plasticity. This response can result from the modulation of certain presynaptic K (potassium ion) currents (I,I, and I), which work to increase the excitability of the sensory neurons, broaden the action potential, and enhance neurotransmitter release. These modulations of K conductances serve as common mechanisms for regulating excitability and synaptic strength.",
            "score": 54.316081047058105
        }
    ],
    "r": [
        {
            "docid": "233497_5",
            "document": "Unsupervised learning . The classical example of unsupervised learning in the study of both natural and artificial neural networks is subsumed by Donald Hebb's principle, that is, neurons that fire together wire together. In Hebbian learning, the connection is reinforced irrespective of an error, but is exclusively a function of the coincidence between action potentials between the two neurons. A similar version that modifies synaptic weights takes into account the time between the action potentials (spike-timing-dependent plasticity or STDP). Hebbian Learning has been hypothesized to underlie a range of cognitive functions, such as pattern recognition and experiential learning.",
            "score": 108.15966796875
        },
        {
            "docid": "33244792_4",
            "document": "Non-spiking neuron . There are an abundance of neurons that propagate signals via action potentials and the mechanics of this particular kind of transmission is well understood. Spiking neurons exhibit action potentials as a result of a neuron characteristic known as membrane potential. Through studying these complex spiking networks in animals, a neuron that did not exhibit characteristic spiking behavior was discovered. These neurons use a graded potential to transmit data as they lack the membrane potential that spiking neurons possess. This method of transmission has a huge effect on the fidelity, strength, and lifetime of the signal. Non-spiking neurons were identified as a special kind of interneuron and function as an intermediary point of process for sensory-motor systems. Animals have become substantial models for understanding more about non-spiking neural networks and the role they play in an animal\u2019s ability to process information and its overall function. Animal models indicate that the interneurons modulate directional and posture coordinating behaviors. Crustaceans and arthropods such as the crawfish have created many opportunities to learn about the modulatory role that these neurons have in addition to their potential to be modulated regardless of their lack of exhibiting spiking behavior. Most of the known information about nonspiking neurons is derived from animal models. Studies focus on neuromuscular junctions and modulation of abdominal motor cells. Modulatory interneurons are neurons that are physically situated next to muscle fibers and innervate the nerve fibers which allow for some orienting movement. These modulatory interneurons are usually nonspiking neurons. Advances in studying nonspiking neurons included determining new delineations among the different types of interneurons. These discoveries were due to the usage of methods such as protein receptor silencing. Studies have been done on the non-spiking neuron qualities in animals of specific non-spiking neural networks that have a corollary in humans, e.g. retina amacrine cell of the eye.",
            "score": 102.89958953857422
        },
        {
            "docid": "6972634_4",
            "document": "Mriganka Sur . Sur is a pioneer in the study of brain plasticity and its mechanisms. Using experimental and theoretical approaches, his laboratory studies developmental plasticity and the dynamic changes in mature cortical networks during information processing, learning and memory. His laboratory has discovered fundamental principles by which neurons of the cerebral cortex are wired during development and change dynamically in adulthood. In landmark experiments, he \"rewired\" the brain to explore how the environment influences the development of cortical circuits. The retina, which normally projects to the visual cortex, was induced to project to structures that normally process hearing. Visual input altered the development of neuronal connections in the auditory cortex, thus enabling animals to use their \"hearing\" cortex to \"see.\"",
            "score": 95.8508529663086
        },
        {
            "docid": "40158142_9",
            "document": "Nonlinear system identification . Artificial neural networks try loosely to imitate the network of neurons in the brain where computation takes place through a large number of simple processing elements. A typical neural network consists of a number of simple processing units interconnected to form a complex network. Layers of such units are arranged so that data is entered at the input layer and passes through either one or several intermediate layers before reaching the output layer. In supervised learning the network is trained by operating on the difference between the actual output and the desired output of the network, the prediction error, to change the connection strengths between the nodes. By iterating the weights are modified until the output error reaches an acceptable level. This process is called machine learning because the network adjusts the weights so that the output pattern is reproduced. Neural networks have been extensively studied and there are many excellent textbooks devoted to this topic in general, and more focussed textbooks which emphasise control and systems applications. There are two main problem types that can be studied using neural networks: static problems, and dynamic problems. Static problems include pattern recognition, classification, and approximation. Dynamic problems involve lagged variables and are more appropriate for system identification and related applications. Depending on the architecture of the network the training problem can be either nonlinear-in-the-parameters which involves optimisation or linear-in-the-parameters which can be solved using classical approaches. The training algorithms can be categorised into supervised, unsupervised, or reinforcement learning. Neural networks have excellent approximation properties but these are usually based on standard function approximation results using for example the Weierstrass Theorem that applies equally well to polynomials, rational functions, and other well-known models.  Neural networks have been applied extensively to system identification problems which involve nonlinear and dynamic relationships. However, classical neural networks are purely gross static approximating machines. There is no dynamics within the network. Hence when fitting dynamic models all the dynamics arise by allocating lagged inputs and outputs to the input layer of the network. The training procedure then produces the best static approximation that relates the lagged variables assigned to the input nodes to the output. There are more complex network architectures, including recurrent networks, that produce dynamics by introducing increasing orders of lagged variables to the input nodes. But in these cases it is very easy to over specify the lags and this can lead to over fitting and poor generalisation properties.  Neural networks have several advantages; they are conceptually simple, easy to train and to use, have excellent approximation properties, the concept of local and parallel processing is important and this provides integrity and fault tolerant behaviour. The biggest criticism of the classical neural network models is that the models produced are completely opaque and usually cannot be written down or analysed. It is therefore very difficult to know what is causing what, to analyse the model, or to compute dynamic characteristics from the model. Some of these points will not be relevant to all applications but they are for dynamic modelling.",
            "score": 93.62273406982422
        },
        {
            "docid": "26225095_2",
            "document": "NOMFET . NOMFET is a nanoparticle organic memory field-effect transistor. The transistor is designed to mimic the feature of the human synapse known as plasticity, or the variation of the speed and strength of the signal going from neuron to neuron. The device uses gold nano-particles of about 5\u201420\u00a0nm set with pentacene to emulate the change in voltages and speed within the signal. This device uses charge trapping/detrapping in an array of gold nanoparticules (NPs) at the SiO2/pentacene interface to design a SYNAPSTOR (synapse transistor) mimicking the dynamic plasticity of a biological synapse. This device (memristor-like) mimics short-term plasticity (STP) and temporal correlation plasticity (STDP, spike-timing dependent plasticity), two \"functions\" at the basis of learning processes. A compact model was developed, and these organic synapstors were used to demonstrate an associative memory, which can be trained to present a pavlovian response. A recent report showed that these organic synapse-transistors (synapstor) are working at 1 volt and with a plasticity typical response time in the range 100-200 ms. The device also works in contact with an electrolyte (EGOS : electrolyte gated organic synapstor) and can be interfaced with biologic neurons.",
            "score": 93.1570816040039
        },
        {
            "docid": "21523_125",
            "document": "Artificial neural network . Many types of models are used, defined at different levels of abstraction and modeling different aspects of neural systems. They range from models of the short-term behavior of individual neurons, models of how the dynamics of neural circuitry arise from interactions between individual neurons and finally to models of how behavior can arise from abstract neural modules that represent complete subsystems. These include models of the long-term, and short-term plasticity, of neural systems and their relations to learning and memory from the individual neuron to the system level.",
            "score": 92.75436401367188
        },
        {
            "docid": "586357_18",
            "document": "Artificial general intelligence . The artificial neuron model assumed by Kurzweil and used in many current artificial neural network implementations is simple compared with biological neurons. A brain simulation would likely have to capture the detailed cellular behaviour of biological neurons, presently only understood in the broadest of outlines. The overhead introduced by full modeling of the biological, chemical, and physical details of neural behaviour (especially on a molecular scale) would require computational powers several orders of magnitude larger than Kurzweil's estimate. In addition the estimates do not account for glial cells, which are at least as numerous as neurons, and which may outnumber neurons by as much as 10:1, and are now known to play a role in cognitive processes.",
            "score": 90.51274108886719
        },
        {
            "docid": "8402086_18",
            "document": "Computational neurogenetic modeling . Because the amount of data on the interplay of genes and neurons and their effects is not enough to construct a rigorous model,  evolutionary computation is used to optimize artificial neural networks and gene regulatory networks, a common technique being the genetic algorithm. A genetic algorithm is a process that can be used to refine models by mimicking the process of natural selection observed in biological ecosystems. The primary advantages are that, due to not requiring derivative information, it can be applied to black box problems and multimodal optimization. The typical process for using genetic algorithms to refine a gene  regulatory network is: first, create a population; next, to create offspring via a crossover operation and  evaluate their fitness; then, on a group chosen for high fitness, simulate mutation via a mutation operator;  finally, taking the now mutated group, repeat this process until a desired level of fitness is demonstrated. Methods by which artificial neural networks may alter their structure without simulated mutation and fitness selection have been developed. A dynamically evolving neural network is one approach, as the creation of new connections and new neurons can  be modeled as the system adapts to new data. This enables the network to evolve in modeling accuracy without simulated natural selection. One method by which dynamically evolving networks may be optimized, called evolving layer neuron aggregation, combines neurons with sufficiently similar input weights into one neuron. This can take place during the training of the network, referred to as online aggregation, or between periods of training, referred to as offline aggregation. Experiments have suggested that offline aggregation is more efficient.",
            "score": 90.0242691040039
        },
        {
            "docid": "10839226_23",
            "document": "Cultured neuronal network . In order to establish learning in a cultured network, researchers have attempted to re-embody the dissociated neuronal networks in either simulated or real environments (see MEART and animat). Through this method the networks are able to interact with their environment and, therefore, have the opportunity to learn in a more realistic setting. Other studies have attempted to imprint signal patterns onto the networks via artificial stimulation. This can be done by inducing network bursts or by inputing specific patterns to the neurons, from which the network is expected to derive some meaning (as in experiments with animats, where an arbitrary signal to the network indicates that the simulated animal has run into a wall or is moving in a direction, etc.). The latter technique attempts to take advantage of the inherent ability of neuronal networks to make sense of patterns. However, experiments have had limited success in demonstrating a definition of learning that is widely agreed upon. Nevertheless, plasticity in neuronal networks is a phenomenon that is well-established in the neuroscience community, and one that is thought to play a very large role in learning.",
            "score": 89.93974304199219
        },
        {
            "docid": "18345642_14",
            "document": "Behavioral addiction . One of the most important discoveries of addictions has been the drug based reinforcement and, even more important, reward based learning processes. Several structures of the brain are important in the conditioning process of behavioral addiction; these subcortical structures form the brain regions known as the reward system. One of the major areas of study is the amygdala, a brain structure which involves emotional significance and associated learning. Research shows that dopaminergic projections from the ventral tegmental area facilitate a motivational or learned association to a specific behavior.  Dopamine neurons take a role in the learning and sustaining of many acquired behaviors. Research specific to Parkinson\u2019s disease has led to identifying the intracellular signaling pathways that underlie the immediate actions of dopamine. The most common mechanism of dopamine is to create addictive properties along with certain behaviors. There are three stages to the dopamine reward system: bursts of dopamine, triggering of behavior, and further impact to the behavior. Once electronically signaled, possibly through the behavior, dopamine neurons let out a \u2018burst-fire\u2019 of elements to stimulate areas along fast transmitting pathways. The behavior response then perpetuates the striated neurons to further send stimuli. The fast firing of dopamine neurons can be monitored over time by evaluating the amount of extracellular concentrations of dopamine through micro dialysis and brain imaging. This monitoring can lead to a model in which one can see the multiplicity of triggering over a period of time. Once the behavior is triggered, it is hard to work away from the dopamine reward system.",
            "score": 89.71304321289062
        },
        {
            "docid": "2567511_17",
            "document": "Neural engineering . Scientists can use experimental observations of neuronal systems and theoretical and computational models of these systems to create Neural networks with the hopes of modeling neural systems in as realistic a manner as possible. Neural networks can be used for analyses to help design further neurotechnological devices. Specifically, researchers handle analytical or finite element modeling to determine nervous system control of movements and apply these techniques to help patients with brain injuries or disorders. Artificial neural networks can be built from theoretical and computational models and implemented on computers from theoretically devices equations or experimental results of observed behavior of neuronal systems. Models might represent ion concentration dynamics, channel kinetics, synaptic transmission, single neuron computation, oxygen metabolism, or application of dynamic system theory (LaPlaca et al. 2005). Liquid-based template assembly was used to engineer 3D neural networks from neuron-seeded microcarrier beads.",
            "score": 89.70304107666016
        },
        {
            "docid": "613539_2",
            "document": "Spike-timing-dependent plasticity . Spike-timing-dependent plasticity (STDP) is a biological process that adjusts the strength of connections between neurons in the brain. The process adjusts the connection strengths based on the relative timing of a particular neuron's output and input action potentials (or spikes). The STDP process partially explains the activity-dependent development of nervous systems, especially with regards to long-term potentiation and long-term depression.",
            "score": 89.2274398803711
        },
        {
            "docid": "41121206_5",
            "document": "Phase resetting in neurons . Shifts in phase (or behavior of neurons) caused due to a perturbation (an external stimulus) can be quantified within a Phase Response Curve (PRC) to predict synchrony in coupled and oscillating neurons. These effects can be computed, in the case of advances or delays to responses, to observe the changes in the oscillatory behavior of neurons, pending on when a stimulus was applied in the phase cycle of an oscillating neuron. The key to understanding this is in the behavioral patterns of neurons and the routes neural information travels. Neural circuits are able to communicate efficiently and effectively within milliseconds of experiencing a stimulus and lead to the spread of information throughout the neural network. The study of neuron synchrony could provide information on the differences that occur in neural states such as normal and diseased states. Neurons that are involved significantly in diseases such as Alzheimers or Parkinsons diseases are shown to undergo phase resetting before launching into phase locking where clusters of neurons are able to begin firing rapidly to communicate information quickly. A phase response curve can be calculated by noting changes to its period over time depending on where in the cycle the input is applied. The perturbation left by the stimulus moves the stable cycle within the oscillation followed by a return to the stable cycle limit. The curve tracks the amount of advancement or delay due to the input in the oscillating neuron. The PRC assumes certain patterns of behavior in firing pattern as well as the network of oscillating neurons to model the oscillations. Currently, only a few circuits exist which can be modeled using an assumed firing pattern.",
            "score": 87.52742004394531
        },
        {
            "docid": "10159567_6",
            "document": "Spiking neural network . This kind of neural network can in principle be used for information processing applications the same way as traditional artificial neural networks. In addition, spiking neural networks can model the central nervous system of a virtual insect for seeking food without the prior knowledge of the environment. However, due to their more realistic properties, they can also be used to study the operation of biological neural circuits. Starting with a hypothesis about the topology of a biological neuronal circuit and its function, the electrophysiological recordings of this circuit can be compared to the output of the corresponding spiking artificial neural network simulated on computer, determining the plausibility of the starting hypothesis.",
            "score": 87.29354858398438
        },
        {
            "docid": "10839226_2",
            "document": "Cultured neuronal network . A cultured neuronal network is a cell culture of neurons that is used as a model to study the central nervous system, especially the brain. Often, cultured neuronal networks are connected to an input/output device such as a multi-electrode array (MEA), thus allowing two-way communication between the researcher and the network. This model has proved to be an invaluable tool to scientists studying the underlying principles behind neuronal learning, memory, plasticity, connectivity, and information processing.",
            "score": 87.13357543945312
        },
        {
            "docid": "39619438_3",
            "document": "AnimatLab . Neuromechanical simulation enables investigators to explore the dynamical relationships between the brain, the body, and the world in ways that are difficult or impossible through experiment alone. This is done by producing biologically realistic models of the neural networks that control behavior, while also simulating the physics that controls the environment in which an animal is situated. Interactions with the simulated world can then be fed back to the virtual nervous system using models of sensory systems. This provides feedback similar to what the real animal would encounter, and makes it possible to close the sensory-motor feedback loop to study the dynamic relationship between nervous function and behavior. This relationship is crucial to understanding how nervous systems work.",
            "score": 86.33419799804688
        },
        {
            "docid": "21445461_2",
            "document": "Nonsynaptic plasticity . Nonsynaptic plasticity is a form of neuroplasticity that involves modification of ion channel function in the axon, dendrites, and cell body that results in specific changes in the integration of excitatory postsynaptic potentials (EPSPs) and inhibitory postsynaptic potentials (IPSPs). Nonsynaptic plasticity is a modification of the intrinsic excitability of the neuron. It interacts with synaptic plasticity, but it is considered a separate entity from synaptic plasticity. Intrinsic modification of the electrical properties of neurons plays a role in many aspects of plasticity from homeostatic plasticity to learning and memory itself. Nonsynaptic plasticity affects synaptic integration, subthreshold propagation, spike generation, and other fundamental mechanisms of neurons at the cellular level. These individual neuronal alterations can result in changes in higher brain function, especially learning and memory. However, as an emerging field in neuroscience, much of the knowledge about nonsynaptic plasticity is uncertain and still requires further investigation to better define its role in brain function and behavior.",
            "score": 86.12577056884766
        },
        {
            "docid": "14408479_2",
            "document": "Biological neuron model . A biological neuron model, also known as a spiking neuron model, is a mathematical description of the properties of certain cells in the nervous system that generate sharp electrical potentials across their cell membrane, roughly one millisecond in duration, as shown in Fig. 1. Spiking neurons are known to be a major signaling unit of the nervous system, and for this reason characterizing their operation is of great importance. It is worth noting that not all the cells of the nervous system produce the type of spike that define the scope of the spiking neuron models. For example, cochlear hair cells, retinal receptor cells, and retinal bipolar cells do not spike. Furthermore, many cells in the nervous system are not classified as neurons but instead are classified as glia. Ultimately, biological neuron models aim to explain the mechanisms underlying the operation of the nervous system for the purpose of restoring lost control capabilities such as perception (e.g. deafness or blindness), motor movement decision making, and continuous limb control. In that sense, biological neuron models differ from artificial neuron models that do not presume to predict the outcomes of experiments involving the biological neural tissue (although artificial neuron models are also concerned with execution of perception and estimation tasks). Accordingly, an important aspect of biological neuron models is experimental validation, and the use of physical units to describe the experimental procedure associated with the model predictions.",
            "score": 85.2933578491211
        },
        {
            "docid": "14408479_47",
            "document": "Biological neuron model . The spiking neuron model by Nossenson & Messer produces the probability of the neuron to fire a spike as a function of either an external or pharmacological stimulus. The model consists of a cascade of a receptor layer model and a spiking neuron model, as shown in Fig 4. The connection between the external stimulus to the spiking probability is made in two steps: First, a receptor cell model translates the raw external stimulus to neurotransmitter concentration, then, a spiking neuron model connects between neurotransmitter concentration to the firing rate (spiking probability). Thus, the spiking neuron model by itself depends on neurotransmitter concentration at the input stage. An important feature of this model is the prediction for neurons firing rate pattern which captures, using a low number of free parameters, the characteristic edge emphasized response of neurons to a stimulus pulse, as shown in Fig. 5. The firing rate is identified both as a normalized probability for neural spike firing, and as a quantity proportional to the current of neurotransmitters released by the cell. The expression for the firing rate takes the following form:",
            "score": 85.157470703125
        },
        {
            "docid": "1706303_41",
            "document": "Recurrent neural network . A multiple timescales recurrent neural network (MTRNN) is a neural-based computational model that can simulate the functional hierarchy of the brain through self-organization that depends on spatial connection between neurons and on distinct types of neuron activities, each with distinct time properties. With such varied neuronal activities, continuous sequences of any set of behaviors are segmented into reusable primitives, which in turn are flexibly integrated into diverse sequential behaviors. The biological approval of such a type of hierarchy was discussed in the memory-prediction theory of brain function by Hawkins in his book \"On Intelligence\".",
            "score": 84.0519027709961
        },
        {
            "docid": "38946837_4",
            "document": "Autapse . In 2000, they were first modeled as supporting persistence in recurrent neural networks. In 2004, they were modeled as demonstrating oscillatory behavior, which was absent in the same model neuron without autapse. More specifically, the neuron oscillated between high firing rates and firing suppression, reflecting the spike bursting behavior typically found in cerebral neurons. In 2009, autapses were, for the first time, associated with sustained activation. This proposed a possible function for excitatory autapses within a neural circuit. In 2014, electrical autapses were shown to generate stable target and spiral waves in a neural model network. This indicated that they played a significant role in stimulating and regulating the collective behavior of neurons in the network. In 2016, a model of resonance was offered.",
            "score": 84.04571533203125
        },
        {
            "docid": "57441533_5",
            "document": "D. James Surmeier . By mid-90s, despite widespread consensus regarding the clinical relevance of striatal dopaminergic signaling, the distribution and seggregation of different classes of dopamine receptors, into either the same or distinct neuronal populations was unclear and remained widely debated. In pioneering experiments, using patch clamp recordings in conjunction with single cell gene profiling through RT-PCR, Surmeier reconciled the seemingly confounding results from anatomical and functional studies by showing that the direct (striatonigral) and indirect pathway (striatopallidal) striatal projection neurons predominantly expressed either the D1 or D2 dopamine receptors. Following this discovery, using pharmacology and spike-timing-dependent plasticity (STDP) protocols in genetically identified D1 or D2 receptor expressing neurons, Surmeier elucidated the distinct roles played by both the receptors in the induction of long-term potentiation and depression at the cortico-striatal synapses. Simultaneously, he also showed that in projection neurons that do not express the D2 receptors, synaptic depression dependent on D2 receptor activation is mediated by D2 receptors in cholinergic neurons, M1 muscarinic receptor activation resulting in reduced calcium channel, CaV1.3 opening in projection neurons and endocannabinoid signaling. Understanding the opposing effects of D1 and D2 receptor signaling and the consequent insights into the dopaminergic modulation of bi-directional synaptic plasticity in the direct and indirect spiny neurons was a conceptual advance that has proven fundamental to understanding striatal function in both behavioral adaptation as well as Parkinson's disease pathology, and continues to provide a foundation for current models of how dopamine controls striatal circuitry.",
            "score": 83.9344253540039
        },
        {
            "docid": "33818014_15",
            "document": "Nervous system network models . The concept of artificial neural network (ANN) was introduced by McColloch, W. S. & Pitts, W. (1943) for models based on behavior of biological neurons. Norbert Wiener (1961) gave this new field the popular name of cybernetics, whose principle is the interdisciplinary relationship among engineering, biology, control systems, brain functions, and computer science. With the computer science field advancing, the von Neumann-type computer was introduced early in the neuroscience study. But it was not suitable for symbolic processing, nondeterministic computations, dynamic executions, parallel distributed processing, and management of extensive knowledge bases, which are needed for biological neural network applications; and the direction of mind-like machine development changed to a learning machine. Computing technology has since advanced extensively and computational neuroscience is now able to handle mathematical models developed for biological neural network. Research and development are progressing in both artificial and biological neural networks including efforts to merge the two.",
            "score": 83.7900390625
        },
        {
            "docid": "40435056_3",
            "document": "Equation-free modeling . In a wide range of chemical, physical and biological systems, coherent macroscopic behavior emerges from interactions between microscopic entities themselves (molecules, cells, grains, animals in a population, agents) and with their environment. Sometimes, remarkably, a coarse-scale differential equation model (such as the Navier-Stokes equations for fluid flow, or a reaction-diffusion system) can accurately describe macroscopic behavior. Such macroscale modeling makes use of general principles of conservation (atoms, particles, mass, momentum, energy), and closed into a well-posed system through phenomenological constitutive equations or equations of state. However, one increasingly encounters complex systems that only have known microscopic, fine scale, models. In such cases, although we observe the emergence of coarse-scale, macroscopic behavior, modeling it through explicit closure relations may be impossible or impractical. Non-Newtonian fluid flow, chemotaxis, porous media transport, epidemiology, brain modeling and neuronal systems are some typical examples. Equation-free modeling aims to use such microscale models to predict coarse macroscale emergent phenomena.",
            "score": 83.67387390136719
        },
        {
            "docid": "22072718_15",
            "document": "Biological network . Network analysis provides the ability to quantify associations between individuals, which makes it possible to infer details about the network as a whole at the species and/or population level. Researchers interested in animal behavior across a multitude of taxa, from insects to primates, are starting to incorporate network analysis into their research. Researchers interested in social insects (e.g., ants and bees) have used network analyses to better understand division of labor, task allocation, and foraging optimization within colonies; Other researchers are interested in how certain network properties at the group and/or population level can explain individual level behaviors. For instance, a study on wire-tailed manakins (a small passerine bird) found that a male\u2019s degree in the network largely predicted the ability of the male to rise in the social hierarchy (i.e. eventually obtain a territory and matings). In bottlenose dolphin groups, an individual\u2019s degree and betweenness centrality values may predict whether or not that individual will exhibit certain behaviors, like the use of side flopping and upside-down lobtailing to lead group traveling efforts; individuals with high betweenness values are more connected and can obtain more information, and thus are better suited to lead group travel and therefore tend to exhibit these signaling behaviors more than other group members. Network analysis can also be used to describe the social organization within a species more generally, which frequently reveals important proximate mechanisms promoting the use of certain behavioral strategies. These descriptions are frequently linked to ecological properties (e.g., resource distribution). For example, network analyses revealed subtle differences in the group dynamics of two related equid fission-fusion species, Grevy\u2019s zebra and onagers, living in variable environments; Grevy\u2019s zebras show distinct preferences in their association choices when they fission into smaller groups, whereas onagers do not. Similarly, researchers interested in primates have also utilized network analyses to compare social organizations across the diverse primate order, suggesting that using network measures (such as centrality, assortativity, modularity, and betweenness) may be useful in terms of explaining the types of social behaviors we see within certain groups and not others. Finally, social network analysis can also reveal important fluctuations in animal behaviors across changing environments. For example, network analyses in female chacma baboons (\"Papio hamadryas ursinus\") revealed important dynamic changes across seasons which were previously unknown; instead of creating stable, long-lasting social bonds with friends, baboons were found to exhibit more variable relationships which were dependent on short-term contingencies related to group level dynamics as well as environmental variability. This is a very small set of broad examples of how researchers can use network analysis to study animal behavior. Research in this area is currently expanding very rapidly. Social network analysis is a valuable tool for studying animal behavior across all animal species, and has the potential to uncover new information about animal behavior and social ecology that was previously poorly understood.",
            "score": 83.2668685913086
        },
        {
            "docid": "10839226_22",
            "document": "Cultured neuronal network . There is much controversy in the field of neuroscience surrounding whether or not a cultured neuronal network can learn. A crucial step in finding the answer to this problem lies in establishing the difference between learning and plasticity. One definition suggests that learning is \"the acquisition of novel behavior through experience\". Corollary to this argument is the necessity for interaction with the environment around it, something that cultured neurons are virtually incapable of without sensory systems. Plasticity, on the other hand, is simply the reshaping of an existing network by changing connections between neurons: formation and elimination of synapses or extension and retraction of neurites and dendritic spines. But these two definitions are not mutually exclusive; in order for learning to take place, plasticity must also take place.",
            "score": 83.21830749511719
        },
        {
            "docid": "29826376_7",
            "document": "Hippocampal prosthesis . First, we must take into account that, like most of biological processes, the behaviors of neurons are highly nonlinear and depend on many factors: input frequency patterns, etc. Also, a good model must take into account the fact that the expression of a single nerve cell is negligible, since the processes are carried by groups of neurons interacting in network. Once installed, the device must assume all (or at least most) of the function of the damaged hippocampus for a prolonged period of time. First, the artificial neurons must be able to work together in network just like real neurons. Then, they must be able, working and effective synaptics connections with the existing neurons of the brain; therefore a model for silicon/neurons interface will be required.",
            "score": 82.88082122802734
        },
        {
            "docid": "49528276_7",
            "document": "Behavioral plasticity . A major difference between developmental and contextual plasticity is the inherent trade-off between the time of interpreting a stimuli and exhibiting a behavior. Contextual plasticity is a near immediate response to the environment. The underlying hormonal networks/neuronal pathways are already present, so it is only a matter of activating them. In contrast, developmental plasticity requires internal changes in hormonal networks and neuronal pathways. As a result, developmental plasticity is often, although not always, a slower process than contextual plasticity. For instance, habituation is a type of learning (developmental plasticity) that can occur within a short period of time. One of the advantages of developmental behavioral plasticity that occurs over extended periods of time is that such changes can occur in concert with changes in morphological and physiological traits. In such cases, the same set of external or internal stimuli can lead to coordinated changes in suites of behavioral, morphological and physiological traits.",
            "score": 82.87902069091797
        },
        {
            "docid": "142910_69",
            "document": "Creativity . J\u00fcrgen Schmidhuber's formal theory of creativity postulates that creativity, curiosity, and interestingness are by-products of a simple computational principle for measuring and optimizing learning progress. Consider an agent able to manipulate its environment and thus its own sensory inputs. The agent can use a black box optimization method such as reinforcement learning to learn (through informed trial and error) sequences of actions that maximize the expected sum of its future reward signals. There are extrinsic reward signals for achieving externally given goals, such as finding food when hungry. But Schmidhuber's objective function to be maximized also includes an additional, intrinsic term to model \"wow-effects.\" This non-standard term motivates purely creative behavior of the agent even when there are no external goals. A wow-effect is formally defined as follows. As the agent is creating and predicting and encoding the continually growing history of actions and sensory inputs, it keeps improving the predictor or encoder, which can be implemented as an artificial neural network or some other machine learning device that can exploit regularities in the data to improve its performance over time. The improvements can be measured precisely, by computing the difference in computational costs (storage size, number of required synapses, errors, time) needed to encode new observations before and after learning. This difference depends on the encoder's present subjective knowledge, which changes over time, but the theory formally takes this into account. The cost difference measures the strength of the present \"wow-effect\" due to sudden improvements in data compression or computational speed. It becomes an intrinsic reward signal for the action selector. The objective function thus motivates the action optimizer to create action sequences causing more wow-effects. Irregular, random data (or noise) do not permit any wow-effects or learning progress, and thus are \"boring\" by nature (providing no reward). Already known and predictable regularities also are boring. Temporarily interesting are only the initially unknown, novel, regular patterns in both actions and observations. This motivates the agent to perform continual, open-ended, active, creative exploration.",
            "score": 82.78327941894531
        },
        {
            "docid": "22000_10",
            "document": "Neural Darwinism . Criticism of Neural \"Darwinism\" was made by Francis Crick on the basis that neuronal groups are instructed by the environment rather than undergoing blind variation. A recent review by Fernando, Szathmary and Husbands explains why Edelman's Neural Darwinism is not Darwinian because it does not contain units of evolution as defined by John Maynard Smith. It is selectionist in that it satisfies the Price equation, but there is no mechanism in Edelman's theory that explains how information can be transferred between neuronal groups. A recent theory called Evolutionary Neurodynamics being developed by Eors Szathmary and Chrisantha Fernando has proposed several means by which true replication may take place in the brain. These neuronal models have been extended by Fernando in a later paper . In the most recent model, three plasticity mechanisms i) multiplicative STDP, ii) LTD, and iii) Heterosynaptic competition, are responsible for copying of connectivity patterns from one part of the brain to another. Exactly the same plasticity rules can explain experimental data for how infants do causal learning in the experiments conducted by Alison Gopnik. It has also been shown that by adding Hebbian learning to neuronal replicators the power of neuronal evolutionary computation may actually be greater than natural selection in organisms.",
            "score": 82.69334411621094
        },
        {
            "docid": "2860430_15",
            "document": "Neural oscillation . Scientists have identified some intrinsic neuronal properties that play an important role in generating membrane potential oscillations. In particular, voltage-gated ion channels are critical in the generation of action potentials. The dynamics of these ion channels have been captured in the well-established Hodgkin\u2013Huxley model that describes how action potentials are initiated and propagated by means of a set of differential equations. Using bifurcation analysis, different oscillatory varieties of these neuronal models can be determined, allowing for the classification of types of neuronal responses. The oscillatory dynamics of neuronal spiking as identified in the Hodgkin\u2013Huxley model closely agree with empirical findings. In addition to periodic spiking, subthreshold membrane potential oscillations, i.e. resonance behavior that does not result in action potentials, may also contribute to oscillatory activity by facilitating synchronous activity of neighboring neurons. Like pacemaker neurons in central pattern generators, subtypes of cortical cells fire bursts of spikes (brief clusters of spikes) rhythmically at preferred frequencies. Bursting neurons have the potential to serve as pacemakers for synchronous network oscillations, and bursts of spikes may underlie or enhance neuronal resonance.",
            "score": 82.46775817871094
        },
        {
            "docid": "53686950_18",
            "document": "Bi-directional hypothesis of language and action . It has been proposed that the control of movement is organized hierarchically, where movement is not controlled by individually controlling single neurons, but that movements are represented at a gross, more functional level. A similar concept has been applied to the control of cognition, resulting in the theory of cognitive circuits. This theory proposes that there are functional units of neurons in the brain that are strongly connected, and act coherently as a functional unit during cognitive tasks. These functional units of neurons, or \"thought circuits,\" have been referred to as the \"building blocks of cognition\". Thought circuits are believed to have been originally formed from basic anatomical connections, that were strengthened with correlated activity through Hebbian learning and plasticity. Formation of these neural networks has been demonstrated with computational models using known anatomical connections and Hebbian learning principles. For example, sensory stimulation through interaction with an object activates a distributed network of neurons in the cortex. Repeated activation of these neurons, through Hebbian plasticity, may strengthen their connections and form a circuit. This sensory circuit may then be activated during the perception of known objects.",
            "score": 82.27336883544922
        }
    ]
}