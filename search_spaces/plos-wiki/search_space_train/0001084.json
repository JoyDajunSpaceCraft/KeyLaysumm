{
    "q": [
        {
            "docid": "51714214_8",
            "document": "Discovery and development of direct Xa inhibitors . Blood coagulation is a complex process by which the blood forms clots. It is an essential part of hemostasis and works by stopping blood loss from damaged blood vessels. At the site of injury, where there is an exposure of blood under the endothelium, the platelets gather and immediately form a plug. That process is called primary hemostasis. Simultaneously, a secondary hemostasis occurs. It is defined as the formation of insoluble fibrin by activated coagulation factors, specifically thrombin. These factors activate each other in a blood coagulation cascade that occurs through two separate pathways that interact, the intrinsic and extrinsic pathway. After activating various proenzymes, thrombin is formed in the last steps of the cascade, it then converts fibrinogen to fibrin which leads to clot formation. Factor Xa is an activated serine protease that occupies a key role in the blood coagulation pathway by converting prothrombin to thrombin. Inhibition of factor Xa leads to antithrombotic effects by decreasing the amount of thrombin. Directly targeting factor Xa is suggested to be an effective approach to anticoagulation.",
            "score": 53.5860915184021
        },
        {
            "docid": "37120076_4",
            "document": "Discovery and development of direct thrombin inhibitors . When a blood vessel ruptures or gets injured, factor VII comes into contact with tissue factors which starts a process called the blood coagulation cascade. Its purpose is to stop bleeding and repair tissue damage. When this process is too active due to various problems the risk of blood clots or embolisms increases. As the name indicates the cascade is a multi-step procedure where the main product thrombin is made by activating various proenzymes (mainly serine proteases) in each step of the cascade. Thrombin has multiple purposes, but mainly it converts soluble fibrinogen to an insoluble fibrin complex. Furthermore, it activates factors V, VIII and XI, all by cleaving the sequences GlyGlyGlyValArg-GlyPro and PhePheSerAlaArg-GlyHis, selectively between Arginine (Arg) and Glycine (Gly). These factors generate more thrombin. Thrombin also activates factor XIII that stabilizes the fibrin complex and therefore the clot and it stimulates platelets, which help with the coagulation. Given this broad action of thrombin it stands as a good drug target for anticoagulant drugs such as heparin, warfarin and DTIs and antiplatelet drugs like aspirin.",
            "score": 50.85456156730652
        },
        {
            "docid": "953471_7",
            "document": "Carboxyglutamic acid . Gamma-carboxyglutamic acid residues play an important role in coagulation. The high-affinity calcium binding sites in the GLA domain of factor IX, which is a serine protease of the coagulation system, were found to partially mediate the binding of factor IXa to platelets and in factor-X activation. In addition, upon mechanical injury to the blood vessel wall, a cell-associated tissue factor becomes exposed and initiates a series of enzymatic reactions localized on a membrane surface generally provided by cells and accumulating platelets. Gla residues partly govern the activation and binding of circulating blood-clotting enzymes and zymogens to this exposed cell membrane surface. Specifically, gla residues are needed in calcium binding and in exposing hydrophobic membrane binding regions to the cell bilayer. Lack of these gla residues results in impaired coagulation or even anticoagulation, which may lead to bleeding diathesis or thrombosis.",
            "score": 46.94137954711914
        },
        {
            "docid": "1555308_17",
            "document": "Factor X . A new model, the cell-based model of anticoagulation appears to explain more fully the steps in coagulation. This model has three stages: 1) initiation of coagulation on TF-bearing cells, 2) amplification of the procoagulant signal by thrombin generated on the TF-bearing cell and 3) propagation of thrombin generation on the platelet surface. Factor Xa plays a key role in all three of these stages.",
            "score": 52.72117352485657
        },
        {
            "docid": "4053538_6",
            "document": "Phosphatidylserine . Phosphatidylserine plays a role in blood coagulation (also known as clotting). When circulating platelets encounter the site of an injury, collagen and thrombin -mediated activation causes externalization of phosphatidylserine (PS) from the inner membrane layer, where it serves as a pro-coagulant surface. This surface acts to orient coagulation proteases, specifically tissue factor (TF) and factor VII, facilitating further proteolysis, activation of factor X, and ultimately generating thrombin.",
            "score": 34.7851722240448
        },
        {
            "docid": "37120076_2",
            "document": "Discovery and development of direct thrombin inhibitors . Direct thrombin inhibitors (DTIs) are a class of anticoagulant drugs that can be used to prevent and treat embolisms and blood clots caused by various diseases. They inhibit thrombin, a serine protease which affects the coagulation cascade in many ways. DTIs have undergone rapid development since the 90's. With technological advances in genetic engineering the production of recombinant hirudin was made possible which opened the door to this new group of drugs. Before the use of DTIs the therapy and prophylaxis for anticoagulation had stayed the same for over 50 years with the use of heparin derivatives and warfarin which have some well known disadvantages. DTIs are still under development, but the research focus has shifted towards factor Xa inhibitors, or even dual thrombin and fXa inhibitors that have a broader mechanism of action by both inhibiting factor IIa (thrombin) and Xa. A recent review of patents and literature on thrombin inhibitors has demonstrated that the development of allosteric and multi-mechanism inhibitors might lead the way to a safer anticoagulant.",
            "score": 58.01022815704346
        },
        {
            "docid": "196121_22",
            "document": "Platelet . Platelet activation causes its membrane surface to become negatively charged. One of the signaling pathways turns on scramblase, which moves negatively charged phospholipids from the inner to the outer platelet membrane surface. These phospholipids then bind the tenase and prothrombinase complexes, two of the sites of interplay between platelets and the coagulation cascade. Calcium ions are essential for the binding of these coagulation factors. In addition to interacting with vWF and fibrin, platelets interact with thrombin, Factors X, Va, VIIa, XI, IX, and prothrombin to complete clot formation via the coagulation cascade. Six studies suggested platelets express tissue factor: the definitive study shows they do not. The platelets from rats were conclusively shown to express tissue factor protein and also it was proved that the rat platelets carry both the tissue factor pre-mRNA and mature mRNA.",
            "score": 39.55237591266632
        },
        {
            "docid": "1184163_3",
            "document": "Factor VII . The main role of factor VII (FVII) is to initiate the process of coagulation in conjunction with tissue factor (TF/factor VII). Tissue factor is found on the outside of blood vessels - normally not exposed to the bloodstream. Upon vessel injury, tissue factor is exposed to the blood and circulating factor VII. Once bound to TF, FVII is activated to FVIIa by different proteases, among which are thrombin (factor IIa), factor Xa, IXa, XIIa, and the FVIIa-TF complex itself. The complex of factor VIIa with TF catalyzes the conversion of factor IX and factor X into the active proteases, factor IXa and factor Xa, respectively.",
            "score": 20.661881923675537
        },
        {
            "docid": "3488934_11",
            "document": "Rivaroxaban . Rivaroxaban inhibits both free Factor Xa and Factor Xa bound in the prothrombinase complex. It is a highly selective direct Factor Xa inhibitor with oral bioavailability and rapid onset of action. Inhibition of Factor Xa interrupts the intrinsic and extrinsic pathway of the blood coagulation cascade, inhibiting both thrombin formation and development of thrombi. Rivaroxaban does not inhibit thrombin (activated Factor II), and no effects on platelets have been demonstrated. It allows predictable anticoagulation and dose adjustments and routine coagulation monitoring as well as dietary restrictions are not needed.",
            "score": 35.70212912559509
        },
        {
            "docid": "35019978_3",
            "document": "Darexaban . Factor Xa (FXa) is an essential blood coagulation factor that is responsible for the initiation of the coagulation cascade. FXa cleaves prothrombin to its active form thrombin, which then acts to convert soluble fibrinogen to insoluble fibrin and to activate platelets. Stabilization of the platelet aggregation by fibrin mesh ultimately leads to clot formation.",
            "score": 37.06000304222107
        },
        {
            "docid": "28925462_9",
            "document": "Edoxaban . Factor Xa (FXa) is an essential blood coagulation factor that is responsible for the initiation of the coagulation cascade. FXa cleaves prothrombin to its active form thrombin, which then acts to convert soluble fibrinogen to insoluble fibrin and to activate platelets. Stabilization of the platelet aggregation by fibrin mesh ultimately leads to clot formation.",
            "score": 37.06000304222107
        },
        {
            "docid": "30883500_3",
            "document": "Magnetomyography . At the early 18th century, the electric signals from living tissues have been investigated. These researchers have promoted many innovations in healthcare especially in medical diagnostic. Some example is based on electrical signals produced by human tissues, including Electrocardiogram (ECG), Electroencephalography (EEG) and Electromyogram (EMG). Besides, with the development of technologies, the biomagnetic measurement from the human body, consisting of Magnetocardiogram (MCG), Magnetoencephalography (MEG) and Magnetomyogram (MMG), provided clear evidence that the existence of the magnetic fields from ionic action currents in electrically active tissues can be utilized to record activities. For the first attempt, Cohen et al. used a point-contact superconducting quantum interference device (SQUID) magnetometer in a shielded room to measure the MCG. They reported that the sensitivity of the recorded MCG was orders of magnitude higher than the previously recorded MCG. The same researcher continued this MEG measurement by using a more sensitive SQUID magnetometer without noise averaging. He compared the EEG and alpha rhythm MEG recorded by both normal and abnormal subjects. It is shown that the MEG has produced some new and different information provided by the EEG. Because the heart can produce a relatively large magnetic field compared to the brain and other organs, the early biomagnetic field research originated from the mathematical modelling of MCG. Early experimental studies also focused on the MCG. In addition, these experimental studies suffer from unavoidable low spatial resolution and low sensitivity due to the lack of sophisticated detection methods. With advances in technology, research has expanded into brain function, and preliminary studies of evoked MEGs began in the 1980s. These studies provided some details about which neuronal populations were contributing to the magnetic signals generated from the brain. However, the signals from single neurons were too weak to be detected. A group of over 10,000 dendrites is required as a group to generate a detectable MEG signal. At the time, the abundance of physical, technical, and mathematical limitations prevented quantitative comparisons of theories and experiments involving human electrocardiograms and other biomagnetic records. Due to the lack of an accurate micro source model, it is more difficult to determine which specific physiological factors influence the strength of MEG and other biomagnetic signals and which factors dominate the achievable spatial resolution. In the past three decades, a great deal of research has been conducted to measure and analyze the magnetic field generated by the flow of ex vivo currents in isolated axons and muscle fibers. These measurements have been supported by some complex theoretical studies and the development of ultra-sensitive room temperature amplifiers and neuromagnetic current probes. Nowadays, cell-level magnetic recording technology has become a quantitative measurement technique for operating currents.",
            "score": 48.122162222862244
        },
        {
            "docid": "3150340_2",
            "document": "Prothrombinase . The prothrombinase complex consists of the serine protein, Factor Xa, and the protein cofactor, Factor Va. The complex assembles on negatively charged phospholipid membranes in the presence of calcium ions. The prothrombinase complex catalyzes the conversion of prothrombin (Factor II), an inactive zymogen, to thrombin (Factor IIa), an active serine protease. The activation of thrombin is a critical reaction in the coagulation cascade, which functions to regulate hemostasis in the body. To produce thrombin, the prothrombinase complex cleaves two peptide bonds in prothrombin, one after Arg and the other after Arg. Although it has been shown that Factor Xa can activate prothrombin when unassociated with the prothrombinase complex, the rate of thrombin formation is severely decreased under such circumstances. The prothrombinase complex can catalyze the activation of prothrombin at a rate 3 x 10-fold faster than can Factor Xa alone. Thus, the prothrombinase complex is required for the efficient production of activated thrombin and also for adequate hemostasis.",
            "score": 45.93803024291992
        },
        {
            "docid": "196121_18",
            "document": "Platelet . Tissue factor also binds to factor VII in the blood, which initiates the extrinsic coagulation cascade to increase thrombin production. Thrombin is a potent platelet activator, acting through Gq and G12. These are G protein coupled receptors and they turn on calcium mediated signaling pathways within the platelet, overcoming the baseline calcium efflux. Families of three G proteins (Gq, Gi, G12) operate together for full activation. Thrombin also promotes secondary fibrin-reinforcement of the platelet plug. Platelet activation in turn degranulates and releases factor V and fibrinogen, potentiating the coagulation cascade. So in reality the process of platelet plugging and coagulation are occurring simultaneously rather than sequentially, with each inducing the other to form the final clot.",
            "score": 39.32072424888611
        },
        {
            "docid": "2932719_3",
            "document": "Dilute Russell's viper venom time . This \"in vitro\" diagnostic test is based on the ability of the venom of the Russell's viper to induce thrombosis. The venom contains some substances (RVV-V and RVV-X) which directly activate factor V and factor X, which turns prothrombin into thrombin in the presence of phospholipid and calcium. In the dRVVT assay, low, rate-limiting concentrations of both Russell's viper venom and phospholipid are used to give a standard clotting time of 23 to 27 seconds. This makes the test sensitive to the presence of lupus anticoagulants, because these antibodies interfere with the clot-promoting role of phospholipid \"in vitro\", and their presence results in a prolonged clotting time. A mixing study is then performed, which consists of adding an equal volume of the patient's plasma to normal plasma; in this study, one would expect the clotting time to return to the normal range if there was only a deficiency of coagulation factors alone. A prolonged clotting time of 30 seconds or greater that does not correct despite the mixing study suggests the presence of a lupus anticoagulant. An abnormal result for the initial dRVVT assay should be followed by a dRVVT confirmatory test. In this test, the inhibitory effect of lupus anticoagulants on phospholipids in the dRVVT can be overcome by adding an excess of phospholipid to the assay. The clotting times of both the initial dRVVT assay and confirmatory test are normalized and then used to determine a ratio of time without phospholipid excess to time with phospholipid excess. In general, a ratio of greater than 1.3 is considered a positive result and implies that the patient may have antiphospholipid antibodies. The dRVVT test has a higher specificity than the aPTT test for the detection of lupus anticoagulant, because it is not influenced by deficiencies or inhibitors of clotting factors VIII, IX or XI as the venom only activates factors V and X.",
            "score": 38.836934208869934
        },
        {
            "docid": "212193_18",
            "document": "Coagulation . Antithrombin is a serine protease inhibitor (serpin) that degrades the serine proteases: thrombin, FIXa, FXa, FXIa, and FXIIa. It is constantly active, but its adhesion to these factors is increased by the presence of heparan sulfate (a glycosaminoglycan) or the administration of heparins (different heparinoids increase affinity to FXa, thrombin, or both). Quantitative or qualitative deficiency of antithrombin (inborn or acquired, e.g., in proteinuria) leads to thrombophilia.",
            "score": 19.44306755065918
        },
        {
            "docid": "51714214_13",
            "document": "Discovery and development of direct Xa inhibitors . Factors IIa, Xa, VIIa, IXa and XIa are all proteolytic enzymes that have a specific role in the coagulation cascade. Factor Xa (FXa) is the most promising one due to its position at the intersection of the intrinsic and extrinsic pathway as well as generating around 1000 thrombin molecules for each Xa molecule which results in a potent anticoagulant effect. FXa is generated from FX by cleavage of a 52 amino acid activation peptide, as the \"a\" in factor Xa means activated. FXa consists of 254 amino acid catalytic domain and is also linked to a 142 amino acid light chain. The chain contains both GLA domain and two epidermal growth factor domains (EGF like domains).",
            "score": 28.36931538581848
        },
        {
            "docid": "1473339_42",
            "document": "Russell's viper . Because this venom is so effective at inducing thrombosis, it has been incorporated into an \"in vitro\" diagnostic test for blood clotting that is widely used in hospital laboratories. This test is often referred to as dilute Russell's viper venom time (dRVVT). The coagulant in the venom directly activates factor X, which turns prothrombin into thrombin in the presence of factor V and phospholipid. The venom is diluted to give a clotting time of 23 to 27 seconds and the phospholipid is reduced to make the test extremely sensitive to phospholipid. The dRVVT test is more sensitive than the aPTT test for the detection of lupus anticoagulant (an autoimmune disorder), because it is not influenced by deficiencies in clotting factors VIII, IX or XI.",
            "score": 41.57472205162048
        },
        {
            "docid": "2404687_29",
            "document": "Artificial heart valve . In the tissue factor exposure path, initiation begins when cells are ruptured and expose tissue factor (TF). Plasma Factor (f) VII binds to TF and sets off a chain reaction which activates fXa and fVa which bind to each other to produce thrombin which in turn activates platelets and fVIII. The platelets activate by binding to the damaged tissue in the initiation phase, and fibrin stabilizes the clot during the propagation phase.",
            "score": 14.447606086730957
        },
        {
            "docid": "238124_7",
            "document": "Disseminated intravascular coagulation . In DIC, the processes of coagulation and fibrinolysis are dysregulated, and the result is widespread clotting with resultant bleeding. Regardless of the triggering event of DIC, once initiated, the pathophysiology of DIC is similar in all conditions. One critical mediator of DIC is the release of a transmembrane glycoprotein called tissue factor (TF). TF is present on the surface of many cell types (including endothelial cells, macrophages, and monocytes) and is not normally in contact with the general circulation, but is exposed to the circulation after vascular damage. For example, TF is released in response to exposure to cytokines (particularly interleukin 1), tumor necrosis factor, and endotoxin. This plays a major role in the development of DIC in septic conditions. TF is also abundant in tissues of the lungs, brain, and placenta. This helps to explain why DIC readily develops in patients with extensive trauma. Upon exposure to blood and platelets, TF binds with activated factor VIIa (normally present in trace amounts in the blood), forming the extrinsic tenase complex. This complex further activates factor IX and X to IXa and Xa, respectively, leading to the common coagulation pathway and the subsequent formation of thrombin and fibrin.",
            "score": 44.23117387294769
        },
        {
            "docid": "1557752_8",
            "document": "Factor V . Factor V is able to bind to activated platelets and is activated by thrombin. On activation, factor V is spliced in two chains (heavy and light chain with molecular masses of 110000 and 73000, respectively) which are noncovalently bound to each other by calcium. The thereby activated factor V (now called FVa) is a cofactor of the prothrombinase complex: The activated factor X (FXa) enzyme requires calcium and activated factor V to convert prothrombin to thrombin on the cell surface membrane.",
            "score": 20.094051837921143
        },
        {
            "docid": "3773877_5",
            "document": "P110\u03b1 . The involvement of p110\u03b1 in human cancer has been hypothesized since 1995. Support for this hypothesis came from genetic and functional studies, including the discovery of common activating PIK3CA missense mutations in common human tumors. It has been found to be oncogenic and is implicated in cervical cancers. \"PIK3CA\" mutations are present in over one-third of breast cancers, with enrichment in the luminal and in human epidermal growth factor receptor 2-positive subtypes (HER2 +). The three hotspot mutation positions (GLU542, GLU545, and HIS1047) have been widely reported till date. While substantial preclinical data show an association with robust activation of the pathway and resistance to common therapies, clinical data do not indicate that such mutations are associated with high levels of pathway activation or with a poor prognosis. It is unknown whether the mutation predicts increased sensitivity to agents targeting the P3K pathway.",
            "score": 42.934592843055725
        },
        {
            "docid": "3498809_3",
            "document": "Disintegrin . Disintegrins work by countering the blood clotting steps, inhibiting the clumping of platelets. They interact with the beta-1 and -3 families of integrins receptors. Integrins are cell receptors involved in cell\u2013cell and cell\u2013extracellular matrix interactions, serving as the final common pathway leading to aggregation via formation of platelet\u2013platelet bridges, which are essential in thrombosis and haemostasis. Disintegrins contain an RGD (Arg-Gly-Asp) or KGD (Lys-Gly-Asp) sequence motif that binds specifically to integrin IIb-IIIa receptors on the platelet surface, thereby blocking the binding of fibrinogen to the receptor\u2013glycoprotein complex of activated platelets. Disintegrins act as receptor antagonists, inhibiting aggregation induced by ADP, thrombin, platelet-activating factor and collagen. The role of disintegrin in preventing blood coagulation renders it of medical interest, particularly with regard to its use as an anti-coagulant.",
            "score": 42.01586389541626
        },
        {
            "docid": "18943424_6",
            "document": "Direct Xa inhibitor . Specific antidotes to reverse the anticoagulant activity of direct Xa inhibitors and other direct oral anticoagulants (DOACs) in the event of major bleeding are not available. Evidence supporting non-specific prohemostatic therapies (prothrombin complex concentrate [PCC], activated prothrombin complex concentrate [aPCC], recombinant factor VIIa) in this setting is limited to healthy human volunteers, animal models, and in vitro studies. Clinical outcome data are lacking. Administration of PCC or aPCC may be considered in addition to supportive measures for patients with severe or life-threatening bleeding. Recent studies are using recombinant proteins (r-Antidote) that are catalytically inactive and lack the membrane-binding \u03b3-carboxyglutamic acid domain of native fXa but retain the ability of native fXa to bind direct fXa inhibitors as well as low molecular weight heparin-activated antithrombin III (ATIII). r-Antidote dose-dependently reverses the inhibition of fXa by direct fXa inhibitors and corrects the prolongation of \"ex vivo\" clotting times by such inhibitors. The r-antidote has the potential to be used as a universal antidote for a broad range of fXa inhibitors.. In certain clinical situations, OCT angiography has the potential for evaluating the effects of intensified antithrombotic therapy.",
            "score": 50.42871689796448
        },
        {
            "docid": "214663_12",
            "document": "Polyphosphate . In humans polyphosphates are shown to play a key role in blood coagulation. Produced and released by platelets they activate Factor XII which is essential for blood clot formation. Furthermore, platelets-derived polyphosphates activate blood coagulation factor XII (Hageman factor) that initiates fibrin formation and the generation of a proinflammatory mediator, bradykinin that contributes to leakage from the blood vessels and thrombosis. Inorganic polyphosphates play a crucial role in tolerance of yeast cells to toxic heavy metal cations.",
            "score": 50.373820066452026
        },
        {
            "docid": "3952929_3",
            "document": "Glycoprotein IIb/IIIa . Once platelets are activated, granules secrete clotting mediators, including both ADP and TXA2. These then bind their respective receptors on platelet surfaces, in both an autocrine and paracrine fashion (binds both itself and other platelets). The binding of these receptors result in a cascade of events resulting in an increase in intracellular calcium (e.g. via G receptor activation leading to Ca release from platelet endoplasmic reticulum Ca stores, which may activate PKC). Hence, this calcium increase triggers the calcium-dependent association of gpIIb and gpIIIa to form the activated membrane receptor complex gpIIb/IIIa, which is capable of binding fibrinogen (factor I), resulting in many platelets \"sticking together\" as they may connect to the same strands of fibrinogen, resulting in a clot. The coagulation cascade then follows to stabilize the clot, as thrombin (factor IIa) converts the soluble fibrinogen into insoluble fibrin strands. These strands are then cross-linked by factor XIII to form a stabilized blood clot.",
            "score": 37.14474654197693
        },
        {
            "docid": "39198919_9",
            "document": "Cancer systems biology . Mathematical modeling can provide useful context for the rational design, validation and prioritization of novel cancer drug targets and their combinations. Network-based modeling and multi-scale modeling have begun to show promise in facilitating the process of effective cancer drug discovery. Using a systems network modeling approach, Schoerberl et al. identified a previously unknown, complementary and potentially superior mechanism of inhibiting the ErbB receptor signaling network. ErbB3 was found to be the most sensitive node, leading to Akt activation; Akt regulates many biological processes, such as proliferation, apoptosis and growth, which are all relevant to tumor progression. This target driven modelling has paved way for first of its kind clinical trials. Bekkal et al. presented a nonlinear model of the dynamics of a cell population divided into proliferative and quiescent compartments. The proliferative phase represents the complete cell cycle (G (1)-S-G (2)-M) of a population committed to divide at its end. The asymptotic behavior of solutions of the nonlinear model is analysed in two cases, exhibiting tissue homeostasis or tumor exponential growth. The model is simulated and its analytic predictions are confirmed numerically.  Furthermore, advances in hardware and software have enabled the realization of clinically feasible, quantitative multimodality imaging of tissue pathophysiology. Earlier efforts relating to multimodality imaging of cancer have focused on the integration of anatomical and functional characteristics, such as PET-CT and single-photon emission CT (SPECT-CT), whereas more-recent advances and applications have involved the integration of multiple quantitative, functional measurements (for example, multiple PET tracers, varied MRI contrast mechanisms, and PET-MRI), thereby providing a more-comprehensive characterization of the tumour phenotype. The enormous amount of complementary quantitative data generated by such studies is beginning to offer unique insights into opportunities to optimize care for individual patients. Although important technical optimization and improved biological interpretation of multimodality imaging findings are needed, this approach can already be applied informatively in clinical trials of cancer therapeutics using existing tools.",
            "score": 76.18162107467651
        },
        {
            "docid": "620083_24",
            "document": "Sensitivity analysis . Emulators (also known as metamodels, surrogate models or response surfaces) are data-modeling/machine learning approaches that involve building a relatively simple mathematical function, known as an \"emulator\", that approximates the input/output behaviour of the model itself. In other words, it is the concept of \"modelling a model\" (hence the name \"metamodel\"). The idea is that, although computer models may be a very complex series of equations that can take a long time to solve, they can always be regarded as a function of their inputs \"Y\"=\"f\"(X). By running the model at a number of points in the input space, it may be possible to fit a much simpler emulator \"\u03b7\"(X), such that \"\u03b7\"(X)\u2248\"f\"(X) to within an acceptable margin of error. Then, sensitivity measures can be calculated from the emulator (either with Monte Carlo or analytically), which will have a negligible additional computational cost. Importantly, the number of model runs required to fit the emulator can be orders of magnitude less than the number of runs required to directly estimate the sensitivity measures from the model.",
            "score": 45.14356577396393
        },
        {
            "docid": "18166009_14",
            "document": "Lower critical solution temperature . There are three groups of methods for correlating and predicting LCSTs. The first group proposes models that are based on a solid theoretical background using liquid\u2013liquid or vapor\u2013liquid experimental data. These methods require experimental data to adjust the unknown parameters, resulting in limited predictive ability . Another approach uses empirical equations that correlate \u03b8(LCST) with physicochemical properties such as density, critical properties etc., but suffers from the disadvantage that these properties are not always available. A new approach proposed by Liu and Zhong develops linear models for the prediction of \u03b8(LCST) using molecular connectivity indices, which depends only on the solvent and polymer structures. The latter approach has proven to be a very useful technique in quantitative structure\u2013activity/property relationships (QSAR/QSPR) research for polymers and polymer solutions. QSAR/QSPR studies constitute an attempt to reduce the trial-and-error element in the design of compounds with desired activity/properties by establishing mathematical relationships between the activity/property of interest and measurable or computable parameters, such as topological, physicochemical, stereochemistry, or electronic indices. More recently QSPR models for the prediction of the \u03b8 (LCST) using molecular (electronic, physicochemical etc.) descriptors have been published. Using validated robust QSPR models, experimental time and effort can be reduced significantly as reliable estimates of \u03b8(LCST) for polymer solutions can be obtained before they are actually synthesized in the laboratory.",
            "score": 54.944923996925354
        },
        {
            "docid": "423938_12",
            "document": "Michael Persinger . One of Persinger's lifelong endeavors has been to establish a mechanism underlying geophysical-behavioral correlates using experimental simulations. The Tectonic Strain Theory (TST) developed by Persinger and John S. Derr predicted that luminous phenomena and associated physical effects were produced by manifestations of tectonic strain that often precedes by weeks to months seismic events within the region. Persinger argues that the labeling of these manifestations such as unidentified flying objects (UFOs) has changed over the centuries and reflects the characteristics of the culture despite a common mechanism. The support for the theory was primarily correlational. The temporal contiguity of reports of unidentified luminous phenomena preceding local seismicity due to injections of fluids was considered a quasi-experimental support for the hypothesis. Alternative models, developed by Persinger and David Vares, were quantified for interaction between quantum values and specific magnitude earthquakes, global climate variations, interactions with population densities, discrete energies as mediators of disease, and processes by which human cognition could be covertly affected by Schumann Resonances and geomagnetic activity. The hypothesis was recently criticized by a prominent blogger. Persinger has estimated that the total biomass of the planet was equivalent to the accumulated solar energy (solar constant) on the Earth\u2019s surface. The shared photon origin was considered one source for \u201centanglement\u201d between living systems.",
            "score": 63.575279116630554
        },
        {
            "docid": "2214122_4",
            "document": "Hirudin . A key event in the final stages of blood coagulation is the conversion of fibrinogen into fibrin by the serine protease enzyme thrombin. Thrombin is produced from prothrombin, by the action of an enzyme, prothrombinase (Factor Xa along with Factor Va as a cofactor), in the final states of coagulation. Fibrin is then cross linked by factor XIII (Fibrin Stabilizing Factor) to form a blood clot. The principal inhibitor of thrombin in normal blood circulation is antithrombin. Similar to antithrombin, the anticoagulant activity of hirudin is based on its ability to inhibit the procoagulant activity of thrombin.",
            "score": 34.05716872215271
        },
        {
            "docid": "212193_11",
            "document": "Coagulation . The division of coagulation in two pathways is mainly artificial, it originates from laboratory tests in which clotting times were measured after the clotting was initiated by glass (intrinsic pathway) or by thromboplastin (a mix of tissue factor and phospholipids). In fact thrombin is present from the very beginning, already when platelets are making the plug. \"Thrombin\" has a large array of functions, not only the conversion of fibrinogen to fibrin, the building block of a hemostatic plug. In addition, it is the most important platelet activator and on top of that it activates Factors VIII and V and their inhibitor protein C (in the presence of thrombomodulin), and it activates Factor XIII, which forms covalent bonds that crosslink the fibrin polymers that form from activated monomers.",
            "score": 30.973870038986206
        }
    ],
    "r": [
        {
            "docid": "4384325_11",
            "document": "Robert Gilbert (chemist) . Thirty years ago there was neither real predictability nor qualitative understanding of the dominant mechanisms in emulsion polymerisation. Mechanisms had been \u2018proved\u2019 by comparing model predictions with experimental data. The data field was limited and the models had many adjustable parameters, or else fitting parameters had values that were subject to wide uncertainty: it was possible to choose values that could suit any model. It was not uncommon to find two papers claiming that quite different mechanisms were dominant in the same system, a result of not being able to isolate the individual steps. As a result of Gilbert\u2019s work, all individual processes in emulsion polymerisation, one of the commonest ways of making everyday products, are now qualitatively and quantitatively understood. It is now possible to polymerise simple systems and to predict the molecular architecture that will be formed under chosen conditions, while for more complex conditions, trends can be semiquantitatively predicted and understood. The international scientific and technical community in this field now uses the mechanistic knowledge that he obtained as the key to understanding current processes and creating new processes and products. His work has put this industrially important field on a rigorous scientific footing.",
            "score": 96.08316040039062
        },
        {
            "docid": "23386350_4",
            "document": "BioSim . Diabetes Efforts concentrate on the role of mutations that effect the ion channels of the insulin-producing beta-cells, on the genetic basis for the development of neonatal diabetes, on the study of human (as opposed to mice) pancreatic cells, on the mechanisms underlying the development of insulin resistance, and on the possible role of prenatal nutrition for the development of type-2 diabetes. Models are also developed to analyse the balance between fat and glucose metabolism and to describe the rate of absorption of different insulin variants. Cancer In this area the network uses computer models of the cell cycle and of its coupling to the 24 h day-and-night rhythm to improve the treatment of patients with cancer. The use of chronotherapy implies that the administration of anti-cancer drugs is adjusted in accordance with the circadian rhythm of the patient. For certain forms of cancer this has been found to increase the efficiency of the drug by a factor of five. Efforts are also devoted to the development of new anti-cancer drugs. Hypertension and cardiovascular diseases Activities area focus on the development of 3D heart models that can be used to test how a new drug affects the regularity of the heart rhythm. Work is performed to develop detailed models of the mechanisms by which the individual nephron of the kidney regulates the incoming blood flow and how neighboring nephrons interact. Mental disorders and neuronal systems Work includes application of mathematical models to develop less invasive and demand-controlled electrical stimulation techniques for the treatment of Parkinson's disease. Modelling studies are performed to examine the effect of sleep deprivation in the treatment of depression, and bioinformatic approaches are applied to try to identify forms of depression on the basis of the information available from blood samples. Methodological issues The area encompasses description of complex networks of oscillating biological units, studies of the mechanisms of temperature stabilization in biological feedback regulations, application of new methods of data analysis, and development of modeling software and biomedical search machines. The area includes application of new experimental techniques such as interference microscopy and surface enhanced Raman spectroscopy to study cellular processes. Regulatory issues and dialogue with the public Testing in animal and human subjects is a necessary part of the development of new drugs. Such experiments clearly raises a number of complicated ethical issues that the use of simulation models may reduce. This requires that the regulatory authorities can evaluate computer models and accept them as part of the required documentation.  During the last five years the BioSim Network has published nine books and 800 scientific publications. The network has organized or co-organized 30 conferences and workshops, edited four issues of international journals, and trained about 130 PhD students. New National Centres in Systems Biology have been established in relation to the BioSim partners in Manchester, Warwick, and Edinburgh.",
            "score": 91.84990692138672
        },
        {
            "docid": "36122619_14",
            "document": "Artificial life . Mathematical models of complex systems are of three types: black-box (phenomenological), white-box (mechanistic, based on the first principles) and grey-box (mixtures of phenomenological and mechanistic models) . In black-box models, the individual-based (mechanistic) mechanisms of a complex dynamic system remain hidden. Black-box models are completely nonmechanistic. They are phenomenological and ignore a composition and internal structure of a complex system. We cannot investigate interactions of subsystems of such a non-transparent model. A white-box model of complex dynamic system has \u2018transparent walls\u2019 and directly shows underlying mechanisms. All events at micro-, meso- and macro-levels of a dynamic system are directly visible at all stages of its white-box model evolution. In most cases mathematical modelers use the heavy black-box mathematical methods, which cannot produce mechanistic models of complex dynamic systems. Grey-box models are intermediate and combine black-box and white-box approaches. Creation of a white-box model of complex system is associated with the problem of the necessity of an a priori basic knowledge of the modeling subject. The deterministic logical cellular automata are necessary but not sufficient condition of a white-box model. The second necessary prerequisite of a white-box model is the presence of the physical ontology of the object under study. The white-box modeling represents an automatic hyper-logical inference from the first principles because it is completely based on the deterministic logic and axiomatic theory of the subject. The purpose of the white-box modeling is to derive from the basic axioms a more detailed, more concrete mechanistic knowledge about the dynamics of the object under study. The necessity to formulate an intrinsic axiomatic system of the subject before creating its white-box model distinguishes the cellular automata models of white-box type from cellular automata models based on arbitrary logical rules. If cellular automata rules have not been formulated from the first principles of the subject, then such a model may have a weak relevance to the real problem .",
            "score": 85.10231018066406
        },
        {
            "docid": "62329_18",
            "document": "Meta-analysis . Other weaknesses are that it has not been determined if the statistically most accurate method for combining results is the fixed, IVhet, random or quality effect models, though the criticism against the random effects model is mounting because of the perception that the new random effects (used in meta-analysis) are essentially formal devices to facilitate smoothing or shrinkage and prediction may be impossible or ill-advised. The main problem with the random effects approach is that it uses the classic statistical thought of generating a \"compromise estimator\" that makes the weights close to the naturally weighted estimator if heterogeneity across studies is large but close to the inverse variance weighted estimator if the between study heterogeneity is small. However, what has been ignored is the distinction between the model \"we choose\" to analyze a given dataset, and the \"mechanism by which the data came into being\". A random effect can be present in either of these roles, but the two roles are quite distinct. There's no reason to think the analysis model and data-generation mechanism (model) are similar in form, but many sub-fields of statistics have developed the habit of assuming, for theory and simulations, that the data-generation mechanism (model) is identical to the analysis model we choose (or would like others to choose). As a hypothesized mechanisms for producing the data, the random effect model for meta-analysis is silly and it is more appropriate to think of this model as a superficial description and something we choose as an analytical tool \u2013 but this choice for meta-analysis may not work because the study effects are a fixed feature of the respective meta-analysis and the probability distribution is only a descriptive tool.",
            "score": 85.05461120605469
        },
        {
            "docid": "36597555_16",
            "document": "Acute inhalation injury . Although current treatments can be administered in a controlled hospital setting, many hospitals are ill-suited for a situation involving mass casualties among civilians. Inexpensive positive-pressure devices that can be used easily in a mass casualty situation, and drugs to prevent inflammation and pulmonary edema are needed. Several drugs that have been approved by the FDA for other indications hold promise for treating chemically induced pulmonary edema. These include \u03b22-agonists, dopamine, insulin, allopurinol, and non-steroidal anti-inflammatory drugs (NSAIDs), such as ibuprofen. Ibuprofen is particularly appealing because it has an established safety record and can be easily administered as an initial intervention. Inhaled and systemic forms of \u03b22-agonists used in the treatment of asthma and other commonly used medications, such as insulin, dopamine, and allopurinol have also been effective in reducing pulmonary edema in animal models but require further study. A recent study documented in the \"AANA Journal\" discussed the use of volatile anesthetic agents, such as sevoflurane, to be used as a bronchodilator that lowered peak airway pressures and improved oxygenation. Other promising drugs in earlier stages of development act at various steps in the complex molecular pathways underlying pulmonary edema. Some of these potential drugs target the inflammatory response or the specific site(s) of injury. Others modulate the activity of ion channels that control fluid transport across lung membranes or target surfactant, a substance that lines the air sacs in the lungs and prevents them from collapsing. Mechanistic information based on toxicology, biochemistry, and physiology may be instrumental in determining new targets for therapy. Mechanistic studies may also aid in the development of new diagnostic approaches. Some chemicals generate metabolic byproducts that could be used for diagnosis, but detection of these byproducts may not be possible until many hours after initial exposure. Additional research must be directed at developing sensitive and specific tests to identify individuals quickly after they have been exposed to varying levels of chemicals toxic to the respiratory tract.",
            "score": 77.30088806152344
        },
        {
            "docid": "1566437_3",
            "document": "Physiologically based pharmacokinetic modelling . PBPK models strive to be mechanistic by mathematically transcribing anatomical, physiological, physical, and chemical descriptions of the phenomena involved in the complex ADME processes. A large degree of residual simplification and empiricism is still present in those models, but they have an extended domain of applicability compared to that of classical, empirical function based, pharmacokinetic models. PBPK models may have purely predictive uses, but other uses, such as statistical inference, have been made possible by the development of Bayesian statistical tools able to deal with complex models. That is true for both toxicity risk assessment and therapeutic drug development.",
            "score": 76.76808166503906
        },
        {
            "docid": "39198919_9",
            "document": "Cancer systems biology . Mathematical modeling can provide useful context for the rational design, validation and prioritization of novel cancer drug targets and their combinations. Network-based modeling and multi-scale modeling have begun to show promise in facilitating the process of effective cancer drug discovery. Using a systems network modeling approach, Schoerberl et al. identified a previously unknown, complementary and potentially superior mechanism of inhibiting the ErbB receptor signaling network. ErbB3 was found to be the most sensitive node, leading to Akt activation; Akt regulates many biological processes, such as proliferation, apoptosis and growth, which are all relevant to tumor progression. This target driven modelling has paved way for first of its kind clinical trials. Bekkal et al. presented a nonlinear model of the dynamics of a cell population divided into proliferative and quiescent compartments. The proliferative phase represents the complete cell cycle (G (1)-S-G (2)-M) of a population committed to divide at its end. The asymptotic behavior of solutions of the nonlinear model is analysed in two cases, exhibiting tissue homeostasis or tumor exponential growth. The model is simulated and its analytic predictions are confirmed numerically.  Furthermore, advances in hardware and software have enabled the realization of clinically feasible, quantitative multimodality imaging of tissue pathophysiology. Earlier efforts relating to multimodality imaging of cancer have focused on the integration of anatomical and functional characteristics, such as PET-CT and single-photon emission CT (SPECT-CT), whereas more-recent advances and applications have involved the integration of multiple quantitative, functional measurements (for example, multiple PET tracers, varied MRI contrast mechanisms, and PET-MRI), thereby providing a more-comprehensive characterization of the tumour phenotype. The enormous amount of complementary quantitative data generated by such studies is beginning to offer unique insights into opportunities to optimize care for individual patients. Although important technical optimization and improved biological interpretation of multimodality imaging findings are needed, this approach can already be applied informatively in clinical trials of cancer therapeutics using existing tools.",
            "score": 76.1816177368164
        },
        {
            "docid": "45567825_7",
            "document": "Jamshid Gharajedaghi . To think about anything requires an image or concept of it, a model. To think about a thing as complex as a social system most people use a model of something similar, simpler and more familiar. Traditionally, two types of models have been used in efforts to acquire information, knowledge and understanding of social systems: mechanistic and organismic. But, in a world of accelerating change, increasing uncertainty and growing complexity, it is becoming apparent that these are inadequate as guides to decision and action. The growing number of social crises and dilemmas that we face should be clear evidence that something is fundamentally wrong with the way we think about social systems.",
            "score": 75.40399932861328
        },
        {
            "docid": "509129_23",
            "document": "Aaron T. Beck . However, some mental health professionals have opposed Beck's cognitive models and resulting therapies as too mechanistic or too limited in which parts of mental activity they will consider. From within the CBT community itself, one line of research using component analyses (dismantling studies) has found that the addition of cognitive strategies often fails to show superior efficacy over behavioral strategies alone, and that attempts to challenge thoughts can sometimes have a rebound effect. Moreover, although Beck's work was presented as a far more scientific and experimentally-based development than psychoanalysis (while being less reductive than behaviourism), Beck's key principles were not necessarily based on the general findings and models of cognitive psychology or neuroscience developing at that time but were derived from personal clinical observations and interpretations in his therapy office. And although there have been many cognitive models developed for different mental disorders and hundreds of outcome studies on the effectiveness of CBT\u2014relatively easy because of the narrow, time-limited and manual-based nature of the treatment\u2014there has been much less focus on experimentally proving the supposedly active mechanisms; in some cases the predicted causal relationships have not been found, such as between dysfunctional attitudes and outcomes.",
            "score": 75.25031280517578
        },
        {
            "docid": "620083_5",
            "document": "Sensitivity analysis . Quite often, some or all of the model inputs are subject to sources of uncertainty, including errors of measurement, absence of information and poor or partial understanding of the driving forces and mechanisms. This uncertainty imposes a limit on our confidence in the response or output of the model. Further, models may have to cope with the natural intrinsic variability of the system (aleatory), such as the occurrence of stochastic events.",
            "score": 75.2354736328125
        },
        {
            "docid": "44457082_22",
            "document": "Robustness of complex networks . An important aspect of failures in many networks is that a single failure in one node might induce failures in neighboring nodes. When a small number of failures induces more failures, resulting in a large number of failures relative to the network size, a cascading failure has occurred. There are many models for cascading failures. These models differ in many details, and model different physical propagation phenomenon from power failures to information flow over Twitter, but have some shared principals. Each model focuses on some sort of propagation or cascade, there is some threshold determining when a node will fail or activate and contribute towards propagation, and there is some mechanism defined by which propagation will be directed when nodes fail or activate. All of these models interestingly predict some critical state, in which the distribution of the size of potential cascades matches a power law, and the exponent is uniquely determined by the degree exponent of the underlying network. Because of the differences in the models and the consensus of this result, we are led to believe the underlying phenomenon is universal and model-independent.",
            "score": 73.77754974365234
        },
        {
            "docid": "15855253_13",
            "document": "Quantification of margins and uncertainties . QMU has the potential to support improved decision-making for programs that must rely heavily on modeling and simulation. Modeling and simulation results are being used more often during the acquisition, development, design, and testing of complex engineering systems. One of the major challenges of developing simulations is to know how much fidelity should be built into each element of the model. The pursuit of higher fidelity can significantly increase development time and total cost of the simulation development effort. QMU provides a formal method for describing the required fidelity relative to the design threshold margins for key performance variables. This information can also be used to prioritize areas of future investment for the simulation. Analysis of the various M/U ratios for the key performance variables can help identify model components that are in need of fidelity upgrades to order to increase simulation effectiveness.",
            "score": 73.49427795410156
        },
        {
            "docid": "15855253_2",
            "document": "Quantification of margins and uncertainties . Quantification of Margins and Uncertainty (QMU) is a decision-support methodology for complex technical decisions. QMU focuses on the identification, characterization, and analysis of performance thresholds and their associated margins for engineering systems that are evaluated under conditions of uncertainty, particularly when portions of those results are generated using computational modeling and simulation. QMU has traditionally been applied to complex systems where comprehensive experimental test data is not readily available and cannot be easily generated for either end-to-end system execution or for specific subsystems of interest. Examples of systems where QMU has been applied include nuclear weapons performance, qualification, and stockpile assessment. QMU focuses on characterizing in detail the various sources of uncertainty that exist in a model, thus allowing the uncertainty in the system response output variables to be well quantified. These sources are frequently described in terms of probability distributions to account for the stochastic nature of complex engineering systems. The characterization of uncertainty supports comparisons of design margins for key system performance metrics to the uncertainty associated with their calculation by the model. QMU supports risk-informed decision-making processes where computational simulation results provide one of several inputs to the decision-making authority. There is currently no standardized methodology across the simulation community for conducting QMU; the term is applied to a variety of different modeling and simulation techniques that focus on rigorously quantifying model uncertainty in order to support comparison to design margins.",
            "score": 73.46994018554688
        },
        {
            "docid": "19374_36",
            "document": "Model organism . Ethical concerns, as well as the cost, maintenance and relative inefficiency of animal research has encouraged development of alternative methods for the study of disease. Cell culture, or \"in vitro\" studies, provide an alternative that preserves the physiology of the living cell, but does not require the sacrifice of an animal for mechanistic studies. Human, inducible pluripotent stem cells can also elucidate new mechanisms for understanding cancer and cell regeneration. Imaging studies (such as MRI or PET scans) enable non-invasive study of human subjects. Recent advances in genetics and genomics can identify disease-associated genes, which can be targeted for therapies.  Ultimately, however, there is no substitute for a living organism when studying complex interactions in disease pathology or treatments.",
            "score": 73.3018798828125
        },
        {
            "docid": "14938064_6",
            "document": "Computational epigenetics . A substantial amount of bioinformatic research has been devoted to the prediction of epigenetic information from characteristics of the genome sequence. Such predictions serve a dual purpose. First, accurate epigenome predictions can substitute for experimental data, to some degree, which is particularly relevant for newly discovered epigenetic mechanisms and for species other than human and mouse. Second, prediction algorithms build statistical models of epigenetic information from training data and can therefore act as a first step toward quantitative modeling of an epigenetic mechanism. Successful computational prediction of DNA and lysine methylation and acetylation has been achieved by combinations of various features. The important role of epigenetic defects for cancer opens up new opportunities for improved diagnosis and therapy. These active areas of research give rise to two questions that are particularly amenable to bioinformatic analysis. First, given a list of genomic regions exhibiting epigenetic differences between tumor cells and controls (or between different disease subtypes), can we detect common patterns or find evidence of a functional relationship of these regions to cancer? Second, can we use bioinformatic methods in order to improve diagnosis and therapy by detecting and classifying important disease subtypes?",
            "score": 72.74436950683594
        },
        {
            "docid": "10795520_22",
            "document": "National Centre for Text Mining . Big mechanisms are large, explanatory models of complicated systems in which interactions have important causal effects. Whilst the collection of big data is increasingly automated, the creation of big mechanisms remains a largely human effort, which is becoming made increasingly challenging, according to the fragmentation and distribution of knowledge. The ability to automate the construction of big mechanisms could have a major impact on scientific research. As one of a number of different projects that make up the big mechanism programme, funded by DARPA, the aim is to assemble an overarching big mechanism from the literature and prior experiments and to utilise this for the probabilistic interpretation of new patient panomics data. We will integrate machine reading of the cancer literature with probabilistic reasoning across cancer claims using specially-designed ontologies, computational modeling of cancer mechanisms (pathways), automated hypothesis generation to extend knowledge of the mechanisms and a 'Robot Scientist' that performs experiments to test the hypotheses. A repetitive cycle of text mining, modelling, experimental testing, and worldview updating is intended to lead to increased knowledge about cancer mechanisms.",
            "score": 72.64697265625
        },
        {
            "docid": "467899_19",
            "document": "Systems biology . The systems biology approach often involves the development of mechanistic models, such as the reconstruction of dynamic systems from the quantitative properties of their elementary building blocks. For instance, a cellular network can be modelled mathematically using methods coming from chemical kinetics and control theory. Due to the large number of parameters, variables and constraints in cellular networks, numerical and computational techniques are often used (e.g., flux balance analysis).",
            "score": 72.19435119628906
        },
        {
            "docid": "5824073_12",
            "document": "High-content screening . This technology allows a (very) large number of experiments to be performed, allowing explorative screening. Cell-based systems are mainly used in chemical genetics where large, diverse small molecule collections are systematically tested for their effect on cellular model systems. Novel drugs can be found using screens of tens of thousands of molecules, and these have promise for the future of drug development.  Beyond drug discovery, chemical genetics is aimed at functionalizing the genome by identifying small molecules that acts on most of the 21,000 gene products in a cell. High-content technology will be part of this effort which could provide useful tools for learning where and when proteins act by knocking them out chemically. This would be most useful for gene where knock out mice (missing one or several genes) can not be made because the protein is required for development, growth or otherwise lethal when it is not there. Chemical knock out could address how and where these genes work. Further the technology is used in combination with RNAi to identify sets of genes involved in specific mechanisms, for example cell division. Here, libraries of RNAis, covering a whole set of predicted genes inside the target organism's genome can be used to identify relevant subsets, facilitating the annotation of genes for which no clear role has been established beforehand. The large datasets produced by automated cell biology contain spatially resolved, quantitative data which can be used for building for systems level models and simulations of how cells and organisms function. Systems biology models of cell function would permit prediction of why, where and how the cell responds to external changes, growth and disease.",
            "score": 72.10832977294922
        },
        {
            "docid": "40841348_11",
            "document": "Computational and Statistical Genetics . In this era of large amount of genetic and genomic data, accurate representation and identification of statistical interactions in biological/genetic/genomic data constitutes a vital basis for designing interventions and curative solutions for many complex diseases. Variations in human genome have been long known to make us susceptible to many diseases. We are hurtling towards the era of personal genomics and personalized medicine that require accurate predictions of disease risk posed by predisposing genetic factors. Computational and statistical methods for identifying these genetic variations, and building these into intelligent models for diseaseassociation and interaction analysis studies genome-wide are a dire necessity across many disease areas. The principal challenges are: (1) most complex diseases involve small or weak contributions from multiple genetic factors that explain only a minuscule fraction of the population variation attributed to genetic factors. (2) Biological data is inherently extremely noisy, so the underlying complexities of biological systems (such as linkage disequilibrium and genetic heterogeneity) need to be incorporated into the statistical models for disease association studies. The chances of developing many common diseases such as cancer, autoimmune diseases and cardiovascular diseases involves complex interactions between multiple genes and several endogenous and exogenous environmental agents or covariates. Many previous disease association studies could not produce significant results because of the lack of incorporation of statistical interactions in their mathematical models explaining the disease outcome. Consequently much of the genetic risks underlying several diseases and disorders remain unknown. Computational methods such as to model and identify the genetic/genomic variations underlying disease risks has a great potential to improve prediction of disease outcomes, understand the interactions and design better therapeutic methods based on them.",
            "score": 71.75874328613281
        },
        {
            "docid": "47878_59",
            "document": "Huntington's disease . Research into the mechanism of HD has focused on identifying the functioning of HTT, how mHTT differs or interferes with it, and the brain pathology that the disease produces. Research is conducted using \"in vitro\" methods, animal models and human volunteers. Animal models are critical for understanding the fundamental mechanisms causing the disease and for supporting the early stages of drug development. Animals with chemically induced brain injury exhibit HD-like symptoms and were initially used, but they did not mimic the progressive features of the disease. The identification of the causative gene has enabled the development of many transgenic animal models including nematode worms, \"Drosophila\" fruit flies, mice, rats, sheep, pigs and monkeys that express mutant huntingtin and develop progressive neurodegeneration and HD-like symptoms.",
            "score": 71.43285369873047
        },
        {
            "docid": "47922_42",
            "document": "Determinism . Quantum physics works differently in many ways from Newtonian physics. Physicist Aaron D. O'Connell explains that understanding our universe, at such small scales as atoms, requires a different logic than day-to-day life does. O'Connell does not deny that it is all interconnected: the scale of human existence ultimately does emerge from the quantum scale. O'Connell argues that we must simply use different models and constructs when dealing with the quantum world. Quantum mechanics is the product of a careful application of the scientific method, logic and empiricism. The Heisenberg uncertainty principle is frequently confused with the observer effect. The uncertainty principle actually describes how precisely we may measure the position and momentum of a particle at the same time \u2014 if we increase the accuracy in measuring one quantity, we are forced to lose accuracy in measuring the other. \"These uncertainty relations give us that measure of freedom from the limitations of classical concepts which is necessary for a consistent description of atomic processes.\" This is where statistical mechanics come into play, and where physicists begin to require rather unintuitive mental models: A particle's path simply cannot be exactly specified in its full quantum description. \"Path\" is a classical, practical attribute in our every day life, but one which quantum particles do not meaningfully possess. The probabilities discovered in quantum mechanics do nevertheless arise from measurement (of the perceived path of the particle). As Stephen Hawking explains, the result is not traditional determinism, but rather determined probabilities. In some cases, a quantum particle may indeed trace an exact path, and the probability of finding the particles in that path is one (certain to be true). In fact, as far as prediction goes, the quantum development is at least as predictable as the classical motion, but the key is that it describes wave functions that cannot be easily expressed in ordinary language. As far as the thesis of determinism is concerned, these probabilities, at least, are quite determined. These findings from quantum mechanics have found many applications, and allow us to build transistors and lasers. Put another way: personal computers, Blu-ray players and the internet all work because humankind discovered the determined probabilities of the quantum world. None of that should be taken to imply that other aspects of quantum mechanics are not still up for debate.",
            "score": 71.42339324951172
        },
        {
            "docid": "413102_19",
            "document": "Folding@home . Folding@home is assisting in research towards preventing some viruses, such as influenza and HIV, from recognizing and entering biological cells. In 2011, Folding@home began simulations of the dynamics of the enzyme RNase H, a key component of HIV, to try to design drugs to deactivate it. Folding@home has also been used to study membrane fusion, an essential event for viral infection and a wide range of biological functions. This fusion involves conformational changes of viral fusion proteins and protein docking, but the exact molecular mechanisms behind fusion remain largely unknown. Fusion events may consist of over a half million atoms interacting for hundreds of microseconds. This complexity limits typical computer simulations to about ten thousand atoms over tens of nanoseconds: a difference of several orders of magnitude. The development of models to predict the mechanisms of membrane fusion will assist in the scientific understanding of how to target the process with antiviral drugs. In 2006, scientists applied Markov state models and the Folding@home network to discover two pathways for fusion and gain other mechanistic insights.",
            "score": 71.355712890625
        },
        {
            "docid": "718855_10",
            "document": "Self-organized criticality . Despite the considerable interest and research output generated from the SOC hypothesis there remains no general agreement with regards to its mathematical mechanisms. Bak Tang and Wiesenfeld based their hypothesis on the behavior of their sandpile model. However, this model was subsequently shown to actually generate 1/f noise rather than 1/f noise. Other simulation models were proposed later that could produce true 1/f noise, and experimental sandpile models were observed to yield 1/f noise. In addition to the nonconservative theoretical model mentioned above, other theoretical models for SOC have been based upon information theory,  mean field theory, and cluster formation.",
            "score": 70.95433044433594
        },
        {
            "docid": "15855253_14",
            "document": "Quantification of margins and uncertainties . A variety of potential issues related to the use of QMU have also been identified. QMU can lead to longer development schedules and increased development costs relative to traditional simulation projects due to the additional rigor being applied. Proponents of QMU state that the level of uncertainty quantification required is driven by certification requirements for the intended application of the simulation. Simulations used for capability planning or system trade analyses must generally model the overall performance trends of the systems and components being analyzed. However, for safety-critical systems where experimental test data is lacking, simulation results provide a critical input to the decision-making process. Another potential risk related to the use of QMU is a false sense of confidence regarding protection from unknown risks. The use of quantified results for key simulation parameters can lead decision makers to believe all possible risks have been fully accounted for, which is particularly challenging for complex systems. Proponents of QMU advocate for a risk-informed decision-making process to counter this risk; in this paradigm, M/U results as well as SME judgment and other external factors are always factored into the final decision.",
            "score": 70.77399444580078
        },
        {
            "docid": "15731985_32",
            "document": "Post-traumatic epilepsy . How epilepsy develops after an insult to the brain is not fully understood, and gaining such understanding may help researchers find ways to prevent it, or make it less severe or easier to treat. Researchers hope to identify biomarkers, biological indications that epileptogenesis is occurring, as a means to find drugs that can target pathways by which epilepsy develops. For example, drugs could be developed to interfere with secondary brain injury (injury that does not occur at the moment of trauma but results from processes initiated by it), by blocking pathways such as free radical damage to brain tissue. An increase in understanding of age differences in epilepsy development after trauma may also help researchers find biomarkers of epileptogenesis. There is also interest in finding more antiepileptic drugs, with the potential to interfere with epileptogenesis. Some new antiepileptic drugs such as topiramate, gabapentin, and lamotrigine have already been developed and have shown promise in treatment of PTE. No animal model has all the characteristics of epileptogenesis in humans, so research efforts aim to identify one. Such a model may help researchers find new treatments and identify the processes involved in epileptogenesis. However, the most common mechanical models of traumatic brain injury such as fluid percussion injury, controlled cortical impact, and weight-drop injury models exhibit epileptogenesis at chronic time points with documented remote electroencephalographic and behavioral seizures, and increased seizure susceptibility.",
            "score": 70.64960479736328
        },
        {
            "docid": "20590_24",
            "document": "Mathematical model . For example, when modeling the flight of an aircraft, we could embed each mechanical part of the aircraft into our model and would thus acquire an almost white-box model of the system. However, the computational cost of adding such a huge amount of detail would effectively inhibit the usage of such a model. Additionally, the uncertainty would increase due to an overly complex system, because each separate part induces some amount of variance into the model. It is therefore usually appropriate to make some approximations to reduce the model to a sensible size. Engineers often can accept some approximations in order to get a more robust and simple model. For example, Newton's classical mechanics is an approximated model of the real world. Still, Newton's model is quite sufficient for most ordinary-life situations, that is, as long as particle speeds are well below the speed of light, and we study macro-particles only.",
            "score": 70.57991790771484
        },
        {
            "docid": "4537268_11",
            "document": "Neuroepithelial cell . Researchers have been able to create neural chimeras by combining neurons that developed from embryonic stem cells with glial cells that were also derived from embryonic stem cells. These neural chimeras give researchers a comprehensive way of studying the molecular mechanisms behind cell repair and regeneration via neuroepithelial precursor cells and will hopefully shed light on possible nervous system repair in a clinical setting. In an attempt to identify the key features that differentiate neuroepithelial cells from their progenitor cells, researchers identified an intermediate filament that was expressed by 98% of the neuroepithelial cells of the neural tube, but none of their progenitor cells. After this discovery it became clear that all three cell types in the nervous system resulted from a homogenous population of stem cells. In order make clinical neural repair possible researchers needed to further characterize regional determination of stem cells during brain development by determining what factors commit a precursor to becoming one or the other. While the exact factors that lead to differentiation are unknown, researchers have taken advantage of human-rat neural chimeras to explore the development of human neurons and glial cells in an animal model. These neural chimeras have permitted researchers to look at neurological diseases in an animal model where traumatic and reactive changes can be controlled. Eventually researchers hope to be able to use the information taken from these neural chimera experiments to repair regions of the brain affected by central nervous system disorders. The problem of delivery, however, has still not been resolved as neural chimeras have been shown to circulate throughout the ventricles and incorporate into all parts of the CNS. By finding environmental cues of differentiation, neuroepithelial precursor transplantation could be used in the treatment of many diseases including multiple sclerosis, Huntington\u2019s disease, and Parkinson\u2019s disease. Further exploration of neural chimera cells and chimeric brains will provide evidence for manipulating the correct genes and increasing the efficacy of neural transplant repair.",
            "score": 70.4943618774414
        },
        {
            "docid": "50825030_2",
            "document": "Sensitivity analysis of an EnergyPlus model . Sensitivity analysis of an EnergyPlus model identifies how uncertainty in an output can be allocated to uncertainty in the input parameters of a process model. Sensitivity analysis is useful for identifying which parameters need more attention during model design and which input parameters influence simulation results the most. Influence of the construction materials and number of people, so-called occupancy, on the room temperature and incoming air ventilation temperature of a house can be obtained by sensitivity analysis. Performing a crude sensitivity analysis that shows the impact of the uncertainty with respect to changes of individual values for these parameters identifies the most significant individual contributors to variability in results.",
            "score": 69.94345092773438
        },
        {
            "docid": "42610544_12",
            "document": "Patient derived xenograft . Colorectal PDX models are relatively easy to establish and the models maintain genetic similarity of primary patient tumor for about 14 generations. In 2012, a study established 27 colorectal PDX models that did not diverge from their respective human tumors in histology, gene expression, or KRAS/BRAF mutation status. Due to their stability, the 27 colorectal PDX models may be able to serve as pre-clinical models in future drug studies. Drug resistance studies have been conducted using colorectal PDX models. In one study, researchers found that the models predicted patient responsiveness to cetuximab with 90% accuracy. Another study identified the amplification of ERBB2 as another mechanism of resistance, and a putative new actionable target in treatments.",
            "score": 69.8233871459961
        },
        {
            "docid": "3736715_24",
            "document": "Apoptosome . The Apaf1/caspase-9 apoptosome formation is a crucial event in the apoptotic cascade. The identification of new potential drugs that prevent or stabilize the formation of active apoptosome complex is the ideal strategy for the treatment of disease characterized by excessive or insufficient apoptosis. Recently taurine has been found to prevent ischemia-induced apoptosis in cardiomyocytes through its ability to inhibit Apaf1/caspase-9 apoptosome formation without preventing mitochondrial dysfunction. The possible mechanism by which taurine inhibits the apoptosome formation was identified as being capable of reducing the expression of caspase-9, a fundamental component of apoptosome. However, there are studies that show Aparf1 and caspase-9 have independent roles other than the apoptosome so altering their levels could alter cell function as well. So despite encouraging experimental data several problems remain unsolved and limit the use of experimental drugs in clinical practice.",
            "score": 69.70982360839844
        },
        {
            "docid": "37619510_5",
            "document": "Fiona Powrie . Powrie performed post-doctoral studies with Robert L. Coffman at DNAX in Palo Alto, California. Here, she extended her earlier work in rats to mice and developed the \u201cT cell transfer\u201d model, one of the most prominent models of intestinal inflammation where transfer of CD4+CD45RBhi T cells to Rag deficient or SCID mice led to the development of severe intestinal inflammation and wasting disease. This could be prevented by transfer of CD4+CD45RBlo T cells. Using this model Powrie further identified the pathogenic role played by IFN-\u03b3 and TNF-\u03b1 in intestinal inflammation and the therapeutic potential of IL-10 and highlighted the requirement for TGF-\u03b2 in the prevention of colitis by the CD4+CD45RBlo regulatory T cell subset  Upon returning to the University of Oxford in 1996, first to the Nuffield Department of Surgery and later, the Sir William Dunn School of Pathology, Powrie used the T cell transfer model to identify the suppressive mechanisms used by regulatory T cells to prevent intestinal inflammation including the requirements for CTLA-4 and the capacity of Treg to prevent colitis driven by innate immune cells as well as CD4+ T cells. Focusing on pathogenic mechanisms the Powrie lab. identified the critical role played by the cytokine IL-23 in driving pathology in the intestine",
            "score": 69.64643859863281
        },
        {
            "docid": "25463587_3",
            "document": "Structural family therapy . SFT utilizes, not only a special systems terminology, but also a means of depicting key family parameters diagrammatically. Its focus is on the structure of the family, including its various substructures. In this regard, Minuchin is a follower of systems and communication theory, since his structures are defined by transactions among interrelated systems within the family. He subscribes to the systems notions of wholeness and equifinality, both of which are critical to his notion of change. An essential trait of SFT is that the therapist actually enters, or \"joins\", with the family system as a catalyst for positive change. Joining with a family is a goal of the therapist early on in his or her therapeutic relationship with the family. Structural and Strategic therapy are important therapeutic models to identify as many therapists use these models as the bases for treatment. Each model has its own approach using different ways in conceptualizing a problem and developing treatment plans that support the goals stated for therapy. In addition, theory-based treatment plans are the source for goal development and treatment options by identifying the presenting problem and social influences. Both these models use similar approaches and define goals with various therapeutic processes that begin with the building of therapist and client relationship. In addition, diversity and theory are identified as a major component in choosing a theory that addresses diversity issues.",
            "score": 69.51895904541016
        }
    ]
}