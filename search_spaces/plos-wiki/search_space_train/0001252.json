{
    "q": [
        {
            "docid": "40841348_9",
            "document": "Computational and Statistical Genetics . Over the past few years, genome-wide association studies (GWAS) have become a powerful tool for investigating the genetic basis of common diseases and has improved our understanding of the genetic basis of many complex traits. Traditional single SNP (single-nucleotide polymorphism) GWAS is the most commonly used method to find trait associated DNA sequence variants - associations between variants and one or more phenotypes of interest are investigated by studying individuals with different phenotypes and examining their genotypes at the position of each SNP individually. The SNPs for which one variant is statistically more common in individuals belonging to one phenotypic group are then reported as being associated with the phenotype. However, most complex common diseases involve small population-level contributions from multiple genomic loci. To detect such small effects as genome-wide significant, traditional GWAS rely on increased sample size e.g. to detect an effect which accounts for 0.1% of total variance, traditional GWAS needs to sample almost 30,000 individuals. Although the development of high throughput SNP genotyping technologies has lowered the cost and improved the efficiency of genotyping. Performing such a large scale study still costs considerable money and time. Recently, association analysis methods utilizing gene-based tests have been proposed that are based on the fact that variations in protein-coding and adjacent regulatory regions are more likely to have functional relevance. These methods have the advantage that they can account for multiple independent functional variants within a gene, with the potential to greatly increase the power to identify disease/trait associated genes. Also, imputation of ungenotyped markers using known reference panels(e.g. HapMap and the 1000 Genomes Project) predicts genotypes at the missing or untyped markers thereby allowing one to accurately evaluate the evidence for association at genetic markers that are not directly genotyped (in addition to the typed markers) and has been shown to improve the power of GWAS to detect disease associated loci.",
            "score": 73.70474219322205
        },
        {
            "docid": "50613151_5",
            "document": "Genome-wide complex trait analysis . As genome sequencing costs dropped steeply over the 2000s, acquiring enough markers on enough subjects for reliable estimates using very distantly related individuals became possible. An early application of the method to humans came with Visscher et al. 2006/2007, which used SNP markers to estimate the actual relatedness of siblings and estimate heritability from the direct genetics. In humans, unlike the original animal/plant applications, relatedness is usually known with high confidence in the 'wild population', and the benefit of GCTA is connected more to avoiding assumptions of classic behavioral genetics designs and verifying their results, and partitioning heritability by SNP class and chromosomes. The first use of GCTA proper in humans was published in 2010, finding 45% of variance in human height can be explained by the included SNPs. (Large GWASes on height have since confirmed the estimate.) The GCTA algorithm was then described and a software implementation published in 2011. It has since been used to study a wide variety of biological, medical, psychiatric, and psychological traits in humans, and inspired many variant approaches.",
            "score": 73.79818308353424
        },
        {
            "docid": "11808249_6",
            "document": "Genome-wide association study . Any two human genomes differ in millions of different ways. There are small variations in the individual nucleotides of the genomes (SNPs) as well as many larger variations, such as deletions, insertions and copy number variations. Any of these may cause alterations in an individual's traits, or phenotype, which can be anything from disease risk to physical properties such as height. Around the year 2000, prior to the introduction of GWA studies, the primary method of investigation was through inheritance studies of genetic linkage in families. This approach had proven highly useful towards single gene disorders. However, for common and complex diseases the results of genetic linkage studies proved hard to reproduce. A suggested alternative to linkage studies was the genetic association study. This study type asks if the allele of a genetic variant is found more often than expected in individuals with the phenotype of interest (e.g. with the disease being studied). Early calculations on statistical power indicated that this approach could be better than linkage studies at detecting weak genetic effects.",
            "score": 79.86063194274902
        },
        {
            "docid": "24235330_17",
            "document": "Behavioural genetics . The conclusion that genetic influences are pervasive has also been observed in research designs that do not depend on the assumptions of the twin method. Adoption studies show that adoptees are routinely more similar to their biological relatives than their adoptive relatives for a wide variety of traits and disorders. In the Minnesota Study of Twins Reared Apart, monozygotic twins separated shortly after birth were reunited in adulthood. These adopted, reared-apart twins were as similar to one another as were twins reared together on a wide range of measures including general cognitive ability, personality, religious attitudes, and vocational interests, among others. Approaches using genome-wide genotyping have allowed researchers to measure genetic relatedness between individuals and estimate heritability based on millions of genetic variants. Methods exist to test whether the extent of genetic similarity (aka, relatedness) between nominally unrelated individuals (individuals who are not close or even distant relatives) is associated with phenotypic similarity. Such methods do not rely on the same assumptions as twin or adoption studies, and routinely find evidence for heritability of behavioural traits and disorders.",
            "score": 57.82292902469635
        },
        {
            "docid": "155624_58",
            "document": "Heritability . When genome-wide genotype data and phenotypes from large population samples are available, one can estimate the relationships between individuals based on their genotypes and use a linear mixed model to estimate the variance explained by the genetic markers. This gives a genomic heritability estimate based on the variance captured by common genetic variants. There are multiple methods that make different adjustments for allele frequency and linkage disequilibrium.",
            "score": 52.76544785499573
        },
        {
            "docid": "21073228_43",
            "document": "Psychometric software . SimuMCAT is a free Java application program that simulates a multidimensional computer adaptive test (MCAT). The user can select from five different MCAT item selection procedures (Volume, Kullback-Leibler information, Minimize the error variance of the linear combination, Minimum Angle, and Minimize the error variance of the composite score with the optimized weight). Two exposure control approaches are possible: the traditional Sympson-Hetter approach and a maximum exposure control approach. It is also possible to implement content constraints using the Priority Index method. Different stopping rules are implemented with fixed-length test and varying-length test. The user specifies true examinee ability, item pools, and item selection procedures, and the program outputs selected items with item responses and ability estimates. Bayesian and non-Bayesian methods can be specified by the user. The examinees\u2019 ability and item pools can also be created from the program by the user specified distributions.",
            "score": 66.79677224159241
        },
        {
            "docid": "155624_53",
            "document": "Heritability . A wide variety approaches using linear mixed models have been reported in literature. Via these methods, phenomic variance may be used in tandem with either environmental or genetic variance to estimate the other, unknown parameter, and estimate heritability through variance parameters obtained from these models. Environmental variance can be explicitly modeled by studying individuals across a broad range of environments, although inference of genetic variance from phenomic and environmental variance may lead to underestimation of heritability due to the challenge of capturing the full range of environmental influence affecting a trait. Other methods for calculating heritability utilize data from genome-wide association studies to estimate the influence on a trait by genetic factors, which is reflected by the rate and influence of putatively associated genetic loci (usually single-nucleotide polymorphisms) on the trait. This can lead to underestimation of heritability, however. This discrepancy is referred to as \"missing heritability\" and reflects the challenge of accurately modeling both genetic and environmental variance in heritability models.",
            "score": 68.92987632751465
        },
        {
            "docid": "826997_56",
            "document": "Regression analysis . All major statistical software packages perform least squares regression analysis and inference. Simple linear regression and multiple regression using least squares can be done in some spreadsheet applications and on some calculators. While many statistical software packages can perform various types of nonparametric and robust regression, these methods are less standardized; different software packages implement different methods, and a method with a given name may be implemented differently in different packages. Specialized regression software has been developed for use in fields such as survey analysis and neuroimaging.",
            "score": 76.45863461494446
        },
        {
            "docid": "45497515_2",
            "document": "Genotype-first approach . The genotype-first approach is a type of strategy used in genetic epidemiological studies to associate specific genotypes to apparent clinical phenotypes of a complex disease or trait.  As opposed to \u201cphenotype-first\u201d, the traditional strategy that have been guiding genome-wide association studies (GWAS) so far, this approach characterizes individuals first by a statistically common genotype based on molecular tests prior to clinical phenotypic classification. This method of grouping leads to patient evaluations based on a shared genetic etiology for the observed phenotypes, regardless of their suspected diagnosis. Thus, this approach can prevent initial phenotypic bias and allow for identification of genes that pose a significant contribution to the disease etiology.",
            "score": 62.147040128707886
        },
        {
            "docid": "44260712_2",
            "document": "SNP annotation . Single nucleotide polymorphism plays an important role in genome wide association studies because they act as primary biomarker. SNPs are currently the marker of choice due to their large numbers in virtually all populations of individuals. The location of these biomarkers can be tremendously important in terms of predicting functional significance, genetic mapping and population genetics. Each SNP represents a nucleotide change between two individuals at a defined location. SNPs are the most common genetic variant found in all individual with one SNP every 100\u2013300 bp in some species. Since there is a massive number of SNPs on the genome, there is a clear need to prioritize SNPs according to their potential effect in order to expedite genotyping and analysis. Annotating large numbers of SNPs is a difficult and complex process, which need computational methods to handle such a large dataset. Many tools available have been developed for SNP annotation in different organism, some of them are optimized for use with organisms densely sampled for SNPs (such as humans), but there are currently few tools available that are species non-specific or support non-model organism data. The majority of SNPs annotation tools provide computationally predicted putative deleterious effects of SNPs. These tools examine whether a SNP resides in functional genomic regions such as exons, splice sites, or transcription regulatory sites, and predict the potential corresponding functional effects that the SNP may have using a variety of machine-learning approaches. But the tools and systems that prioritize functionally significant SNPs, suffer from few limitations: First, they examine the putative deleterious effects of SNPs with respect to a single biological function that provide only partial information about the functional significance of SNPs. Second, current systems classify SNPs into deleterious or neutral group.",
            "score": 61.30249834060669
        },
        {
            "docid": "40977477_17",
            "document": "Cross-species transmission . Two methods of measuring genetic variation, variable number tandem repeats (VNTRs) and single nucleotide polymorphisms (SNPs), have been very beneficial to the study of bacterial transmission. VNTRs, due to the low cost and high mutation rates, make them particularly useful to detect genetic differences in recent outbreaks, and while SNPs have a lower mutation rate per locus than VNTRs, they deliver more stable and reliable genetic relationships between isolates. Both methods are used to construct phylogenies for genetic analysis, however, SNPs are more suitable for studies on phylogenies contraction. However, it can be difficult for these methods accurately simulate CSTs everts. Estimates of CST based on phylogenys made using VNTR marker can be biased towards detecting CST events across a wide range of the parameters. SNPs tend to be less biased and variable in estimates of CST when estimations of CST rates are low and low number of SNPs is used. In general, CST rate estimates using these methods are most reliable in systems with more mutations, more markers, and high genetic differences between introduced strains. CST is very complex and models need to account for a lot of parameters to accurately represent the phenomena. Models that oversimplify reality can result in biased data. Multiple parameters such as number of mutations accumulated since introduction, stochasticity, the genetic difference of strains introduced, and the sampling effort can make unbiased estimates of CST difficult even with whole-genome sequences, especially if sampling is limited, mutation rates are low, or if pathogens were recently introduced. More information on the factors that influence CST rates is needed for the contraction of more appropriate models to study these events.",
            "score": 56.39371657371521
        },
        {
            "docid": "50518079_12",
            "document": "Human Genome Structural Variation . New methods have been developed to analyze human genetic structural variation at high resolutions. The methods used to test the genome are in either a specific targeted way or in a genome wide manner. For Genome wide tests, array-based comparative genome hybridization approaches bring the best genome wide scans to find new copy number variants. These techniques use DNA fragments that are labeled from a genome of interest and are hybridized, with another genome labeled differently, to arrays spotted with cloned DNA fragments. This reveals copy number differences between two genomes.",
            "score": 75.6986231803894
        },
        {
            "docid": "22921_61",
            "document": "Psychology . All researched psychological traits are influenced by both genes and environment, to varying degrees. These two sources of influence are often confounded in observational research of individuals or families. An example is the transmission of depression from a depressed mother to her offspring. Theory may hold that the offspring, by virtue of having a depressed mother in his or her (the offspring's) environment, is at risk for developing depression. However, risk for depression is also influenced to some extent by genes. The mother may both carry genes that contribute to her depression but will also have passed those genes on to her offspring thus increasing the offspring's risk for depression. Genes and environment in this simple transmission model are completely confounded. Experimental and quasi-experimental behavioral genetic research uses genetic methodologies to disentangle this confound and understand the nature and origins of individual differences in behavior. Traditionally this research has been conducted using twin studies and adoption studies, two designs where genetic and environmental influences can be partially un-confounded. More recently, the availability of microarray molecular genetic or genome sequencing technologies allows researchers to measure participant DNA variation directly, and test whether individual genetic variants within genes are associated with psychological traits and psychopathology through methods including genome-wide association studies. One goal of such research is similar to that in positional cloning and its success in Huntington's: once a causal gene is discovered biological research can be conducted to understand how that gene influences the phenotype. One major result of genetic association studies is the general finding that psychological traits and psychopathology, as well as complex medical diseases, are highly polygenic, where a large number (on the order of hundreds to thousands) of genetic variants, each of small effect, contribute to individual differences in the behavioral trait or propensity to the disorder. Active research continues to understand the genetic and environmental bases of behavior and their interaction.",
            "score": 57.29679000377655
        },
        {
            "docid": "4318654_8",
            "document": "Function point . There is currently no ISO recognized FSM Method that includes algorithmic complexity in the sizing result. Recently there have been different approaches proposed to deal with this perceived weakness, implemented in several commercial software products. The variations of the Albrecht-based IFPUG method designed to make up for this (and other weaknesses) include:",
            "score": 71.2022111415863
        },
        {
            "docid": "34132734_14",
            "document": "Family-based QTL mapping . Linkage and association analysis are primary tools for gene discovery, localization and functional analysis. While conceptual underpinning of these approaches have been long known, advances in recent decades in molecular genetics, development in efficient algorithms, and computing power have enabled the large scale application of these methods. While linkage studies seek to identify loci cosegregate with the trait within families, association studies seek to identify particular variants that are associated with the phenotype at the population level. These are complementary methods that, together, provide means to probe the genome and describe etiology of complex traits. In linkage studies, we seek to identify the loci that cosegregate with a specific genomic region, tagged by polymorphic markers, within families. In contrast, in association studies, we seek a correlation between a specific genetic variation and trait variation in sample of individuals, implicating a causal role of the variant.",
            "score": 79.44364166259766
        },
        {
            "docid": "40841348_7",
            "document": "Computational and Statistical Genetics . A number of different methods exist for genotype imputation. The three most widely used imputation methods are - Mach, Impute and Beagle. All three methods utilize hidden markov models as the underlying basis for estimating the distribution of the haplotype frequencies. Mach and Impute2 are more computationally intensive compared with Beagle. Both Impute and Mach are based on different implementations of the product of the conditionals or PAC model. Beagle groups the reference panel haplotypes into clusters at each SNP to form localized haplotype-cluster model that allows it to dynamically vary the number of clusters at each SNP making it computationally faster than Mach and Impute2.",
            "score": 49.26781940460205
        },
        {
            "docid": "50591385_2",
            "document": "Genotyping by sequencing . In the field of genetic sequencing, genotyping by sequencing, also called GBS, is a method to discover single nucleotide polymorphisms (SNP) in order to perform genotyping studies, such as genome-wide association studies (GWAS). GBS uses restriction enzymes to reduce genome complexity and genotype multiple DNA samples. After digestion, PCR is performed to increase fragments pool and then GBS libraries are sequenced using next generation sequencing technologies, usually resulting in about 100bp single-end reads. It is relatively inexpensive and has been used in plant breeding. Although GBS presents an approach similar to restriction-site-associated DNA sequencing (RAD-seq) method, they differ in some substantial ways.",
            "score": 67.64578247070312
        },
        {
            "docid": "34142465_10",
            "document": "Linkage based QTL mapping . Linkage and association analysis are primary tool for gene discovery, localization and functional analysis. While conceptual underpinning of these approaches have been long known, advances in recent decades in molecular genetics, development in efficient algorithms, and computing power have enabled the large scale application of these methods. While linkage studies seek to identify loci that cosegregate with the trait within families, association studies seek to identify particular variants that are associated with the phenotype at the population level. These are complementary methods that, together, provide means to probe the genome and describe etiology of complex human traits. In linkage studies, we seek to identify the loci that cosegregate with a specific genomic region, tagged by polymorphic markers, within families. In contrast, in association studies, we seek a correlation between a specific genetic variation and trait variation in sample of individuals, implicating a causal role of the variant. Linkage tests are powerful and specific for gene discovery, the localization of locus can be achieved only to a certain level of precision \u2013 on order of megabases \u2013 that potentially represents a region that potentially include hundreds of genes.",
            "score": 77.40977597236633
        },
        {
            "docid": "43808044_7",
            "document": "Action model learning . Recent action learning methods take various approaches and employ a wide variety of tools from different areas of artificial intelligence and computational logic. As an example of a method based on propositional logic, we can mention SLAF (Simultaneous Learning and Filtering) algorithm, which uses agent's observations to construct a long propositional formula over time and subsequently interprets it using a satisfiability (SAT) solver. Another technique, in which learning is converted into a satisfiability problem (weighted MAX-SAT in this case) and SAT solvers are used, is implemented in ARMS (Action-Relation Modeling System). Two mutually similar, fully declarative approaches to action learning were based on logic programming paradigm Answer Set Programming (ASP) and its extension, Reactive ASP. In another example, bottom-up inductive logic programming approach was employed. Several different solutions are not directly logic-based. For example, the action model learning using a perceptron algorithm or the multi level greedy search over the space of possible action models. In the older paper from 1992, the action model learning was studied as an extension of reinforcement learning.",
            "score": 69.6435432434082
        },
        {
            "docid": "11808249_10",
            "document": "Genome-wide association study . A key step in the majority of GWA studies is the imputation of genotypes at SNPs not on the genotype chip used in the study. This process greatly increases the number of SNPs that can be tested for association, increases the power of the study, and facilitates meta-analysis of GWAS across distinct cohorts. Genotype imputation is carried out by statistical methods that combine the GWAS data together with a reference panel of haplotypes. These methods take advantage of sharing of haplotypes between individuals over short stretches of sequence to impute alleles. Existing software packages for genotype imputation include IMPUTE2 and MaCH.",
            "score": 55.02833127975464
        },
        {
            "docid": "35201519_29",
            "document": "Medical image computing . A number of sophisticated mathematical methods have entered medical imaging, and have already been implemented in various software packages. These include approaches based on partial differential equations (PDEs) and curvature driven flows for enhancement, segmentation, and registration. Since they employ PDEs, the methods are amenable to parallelization and implementation on GPGPUs. A number of these techniques have been inspired from ideas in optimal control. Accordingly, very recently ideas from control have recently made their way into interactive methods, especially segmentation. Moreover, because of noise and the need for statistical estimation techniques for more dynamically changing imagery, the Kalman filter and particle filter have come into use. A survey of these methods with an extensive list of references may be found in.",
            "score": 59.66442394256592
        },
        {
            "docid": "54190305_12",
            "document": "FORMIND . Forest factory approach: Forest gap models like FORMIND can be used to study the relationship between forest productivity and species diversity. Recent studies have shown that forest productivity often increases with increasing tree species diversity. However, several studies show unchanged or even inverse relationships between productivity and diversity. We studied a broader range of diversity-productivity relationships. Instead of long-term simulations, thousands of different forest stands have been generated, combining different species mixtures with various forest structures (e.g., different basal area values or heterogenic tree heights). This approach is called forest factory and introduce a fundamental new concept for using vegetation models. For each of these virtual forest stands, forest productivity can be calculated using forest gap models. The obtained diversity \u2013 productivity relationships can then be compared to field studies. This new way of using a forest gap model as an analysis tool of a large number of forest stands enables a much faster analysis of numerous forests compared with the classical method of simulating forest successions (forest factory approach).",
            "score": 75.91703128814697
        },
        {
            "docid": "239993_4",
            "document": "Rapid application development . Rapid application development was a response to plan-driven waterfall processes, developed in the 1970s and 1980s, such as the Structured Systems Analysis and Design Method (SSADM). One of the problems with these methods is that they were based on a traditional engineering model used to design and build things like bridges and buildings. Software is an inherently different kind of artifact. Software can radically change the entire process used to solve a problem. As a result, knowledge gained from the development process itself can feed back to the requirements and design of the solution. Plan-driven approaches attempt to rigidly define the requirements, the solution, and the plan to implement it, and have a process that discourage changes. RAD approaches, on the other hand, recognize that software development is a knowledge intensive process and provide flexible processes that help take advantage of knowledge gained during the project to improve or adapt the solution.",
            "score": 73.29125666618347
        },
        {
            "docid": "26277853_6",
            "document": "Subfields of psychology . Behavioural genetics uses genetically informative designs to understand the nature and origins of individual differences in behavior. In focusing on the causes of individual differences, behavioral genetics is distinct from evolutionary psychology, which tends to focus on human universals. Behavioral genetics has thus been associated strongly with the diathesis stress model of psychopathology as well as the nature/nurture debate. Behavioral genetics research was pioneered by Francis Galton and his seminal work in family studies and twin studies after falling out of favor during the eugenics movement during the first part of the 20th century up until World War II. A resurgence of behavioral genetics research began in the 1960s and rose into prominence in the 1980s and beyond. During this time twin study and adoption studies were conducted on a wide array of behavioral traits, including personality, cognitive ability, psychiatric illness, medical illness, and many others. The general conclusion of this large body of work is that every behavioral and medical trait, and indeed measures of the environment is heritable to some moderate degree. Taking advantage of the Human Genome Project more recent work in behavioral genetics uses recent technologies in array-based genotyping, genome sequencing, and other omics to measure genetic variants directly. These genetic variants can then be tested for association with behavioral traits and disorders, for example through genome-wide association studies. This approach to understanding the genetic influences on behavior have seen recent successes in, for example, schizophrenia. Psychiatric genetics is a subfield of behavioral genetics.",
            "score": 59.51288282871246
        },
        {
            "docid": "11808249_8",
            "document": "Genome-wide association study . The most common approach of GWA studies is the case-control setup, which compares two large groups of individuals, one healthy control group and one case group affected by a disease. All individuals in each group are genotyped for the majority of common known SNPs. The exact number of SNPs depends on the genotyping technology, but are typically one million or more. For each of these SNPs it is then investigated if the allele frequency is significantly altered between the case and the control group. In such setups, the fundamental unit for reporting effect sizes is the odds ratio. The odds ratio is the ratio of two odds, which in the context of GWA studies are the odds of disease for individuals having a specific allele and the odds of disease for individuals who do not have that same allele. When the allele frequency in the case group is much higher than in the control group, the odds ratio is higher than 1, and vice versa for lower allele frequency. Additionally, a P-value for the significance of the odds ratio is typically calculated using a simple chi-squared test. Finding odds ratios that are significantly different from 1 is the objective of the GWA study because this shows that a SNP is associated with disease.",
            "score": 53.94469738006592
        },
        {
            "docid": "43780204_24",
            "document": "Gene set enrichment analysis . Single-nucleotide polymorphisms, or SNPs, are single base mutations that may be associated with diseases. One base change has the potential to affect the protein that results from that gene being expressed; however, it also has the potential to have no effect at all. Genome-wide association studies are comparisons between healthy and disease genotypes to try to find SNPs that are overrepresented in the disease genomes, and might be associationed with that condition. Before GSEA, the accuracy of genome-wide SNP association studies was severely limited by a high number of false positives. The theory that the SNPs contributing to a disease tend to be grouped in a set of genes that are all involved in the same biological pathway, is what the GSEA-SNP method is based on. This application of GSEA does not only aid in the discovery of disease-associated SNPs, but helps illuminate the corresponding pathways and mechanisms of the diseases.",
            "score": 60.3400616645813
        },
        {
            "docid": "50613151_4",
            "document": "Genome-wide complex trait analysis . Estimation in biology/animal breeding using standard ANOVA/REML methods of variance components such as heritability, shared-environment, maternal effects etc. typically requires individuals of known relatedness such as parent/child; this is often unavailable or the pedigree data unreliable, leading to inability to apply the methods or requiring strict laboratory control of all breeding (which threatens the external validity of all estimates), and several authors have noted that relatedness could be measured directly from genetic markers (and if individuals were reasonably related, economically few markers would have to be obtained for statistical power), leading Kermit Ritland to propose in 1996 that directly measured pairwise relatedness could be compared to pairwise phenotype measurements (Ritland 1996, \"A Marker-based Method for Inferences About Quantitative Inheritance in Natural Populations\") to combine estimated genetic relatedness with phenotypic measurements to estimate variance components such as heritability or genetic correlations. and subsequently applied to plants/animals",
            "score": 51.30490863323212
        },
        {
            "docid": "2520461_3",
            "document": "Genetic association . Studies of genetic association aim to test whether single-locus alleles or genotype frequencies (or more generally, multilocus haplotype frequencies) differ between two groups of individuals (usually diseased subjects and healthy controls). Genetic association studies today are based on the principle that genotypes can be compared \"directly\", i.e. with the sequences of the actual genomes or exomes via whole genome sequencing or whole exome sequencing. Before 2010, DNA sequencing methods were used.",
            "score": 53.66499304771423
        },
        {
            "docid": "304604_10",
            "document": "Mechatronics . Mechanical modeling calls for modeling and simulating physical complex phenomena in the scope of a multi-scale and multi-physical approach. This implies to implement and to manage modeling and optimization methods and tools, which are integrated in a systemic approach. The specialty is aimed at students in mechanics who want to open their mind to systems engineering, and able to integrate different physics or technologies, as well as students in mechatronics who want to increase their knowledge in optimization and multidisciplinary simulation techniques. The speciality educates students in robust and/or optimized conception methods for structures or many technological systems, and to the main modeling and simulation tools used in R&D. Special courses are also proposed for original applications (multi-materials composites, innovating transducers and actuators, integrated systems, \u2026) to prepare the students to the coming breakthrough in the domains covering the materials and the systems. For some mechatronic systems, the main issue is no longer how to implement a control system, but how to implement actuators. Within the mechatronic field, mainly two technologies are used to produce movement/motion.",
            "score": 59.6459835767746
        },
        {
            "docid": "4148868_8",
            "document": "Crash simulation . Large number of crash simulations use a method of analysis called the Finite Element Method. The complex problems are solved by dividing a surface into a large but still finite number of elements and determining the motion of these elements over very small periods of time. Another approach to crash simulations is performed by application of Macro Element Method. The difference between two mentioned above methodologies is that the structure in case of Macro Element Method consists of smaller number of elements. The calculation algorithm of structure deformation is based on experimental data rather than calculated from partial differential equations. Pam-Crash started crash simulation and together with LS-DYNA is a software package which is widely used for application of Finite Element Method. This method allows detailed modeling of a structure, but the disadvantage lies in high processing unit requirements and calculation time.  The Visual Crash Studio uses Macro Element Methodology. In comparison with FEM it has some modeling and boundary condition limitations but its application does not require advanced computers and the calculation time is incomparably smaller. Two presented methods complement each other. Macro Element Method is useful at early stage of the structure design process while Finite Element Method performs well at its final stages.",
            "score": 65.6423112154007
        },
        {
            "docid": "11808249_3",
            "document": "Genome-wide association study . When applied to human data, GWA studies compare the DNA of participants having varying phenotypes for a particular trait or disease. These participants may be people with a disease (cases) and similar people without the disease (controls), or they may be people with different phenotypes for a particular trait, for example blood pressure. This approach is known as phenotype-first, in which the participants are classified first by their clinical manifestation(s), as opposed to genotype-first. Each person gives a sample of DNA, from which millions of genetic variants are read using SNP arrays. If one type of the variant (one allele) is more frequent in people with the disease, the variant is said to be \"associated\" with the disease. The associated SNPs are then considered to mark a region of the human genome that may influence the risk of disease.",
            "score": 55.964144229888916
        },
        {
            "docid": "3608284_24",
            "document": "Generalized additive model . Backfit GAMs were originally provided by the `gam' function in S, now ported to the R language as the `gam' package. SAS proc GAM also provides backfit GAMs. The R recommended package for GAMs is `mgcv' (mixed gam computational vehicle) which is based on the reduced rank approach with automatic smoothing parameter selection. SAS proc GAMPL is an alternative implementation. There are many alternative packages. Examples include R package `mboost' implements a boosting approach, `gss' provides the full spline smoothing methods, `VGAM' provides vector GAMs, while package `gamlss' provides Generalized additive model for location, scale and shape. `BayesX' and its R interface provides GAMs and extensions via MCMC and penalized likelihood methods. The `INLA' software implements a fully Bayesian approach based on Markov random field representations exploiting sparse matrix methods.",
            "score": 64.28315699100494
        }
    ],
    "r": [
        {
            "docid": "34198573_4",
            "document": "Lightweight Multirole Missile . The MoD contract includes the design, development and qualification of a laser beam rider version of LMM, together with production of an initial quantity of 1,000 missiles. These will be operated from the new Wildcat and Thales graphics have shown helicopters carrying twin 7 round launchers. These are due to enter service in 2015. The contract was funded by a deal to \"re-role previously contracted budgets to facilitate the full-scale development, series production and introduction of the LMM.\" In other words, other contract(s) were cut and the funds switched to paying for LMM. The most likely contract affected is for the Starstreak, which is approaching the end of its term.",
            "score": 101.18270874023438
        },
        {
            "docid": "34198573_3",
            "document": "Lightweight Multirole Missile . LMM was initially conceived as Thales' response to the MoD's Future Air-to-Surface Guided Weapon (Light) FASGW(L) requirement. LMM has been designed to be launched from a variety of naval, air and land platforms against a wide range of targets. High precision reduces collateral damage and makes the missile suitable for asymmetric littoral operations. Development began in 2008 and the LMM uses technology from an earlier Thales missile, the Starstreak. Qualification testing and initial production commenced in late 2011, following an initial contract by the UK Ministry of Defence in April 2011. Thales has conducted successful guidance control firings, including a semi-active laser (SAL) version.",
            "score": 94.60823059082031
        },
        {
            "docid": "8870800_12",
            "document": "Multivariate probit model . Except for formula_29 typically there is no closed form solution to the integrals in the log-likelihood equation. Instead simulation methods can be used to simulated the choice probabilities. Methods using importance sampling include the GHK algorithm (Geweke, Hajivassilou, McFadden and Keane), AR (accept-reject), Stern's method. There are also MCMC approaches to this problem including CRB (Chib's method with Rao-Blackwellization), CRT (Chib, Ritter, Tanner), ARK (accept-reject kernel), and ASK (adaptive sampling kernel).. A variational approach scaling to large datasets is proposed in Probit-LMM (Mandt, Wenzel, Nakajima et al.).",
            "score": 90.80484771728516
        },
        {
            "docid": "25291810_16",
            "document": "London Music Masters . The LMM Awards support exceptional violinists in entering the professional classical music industry. LMM provides financial support, career guidance and performance opportunities including a Wigmore Hall recital and a concerto with the London Philharmonic Orchestra. LMM also regularly commissions new works of music to be premi\u00e8red by the Award Holders. The Awards are overseen by renowned violinist Itzhak Rashkovsky and are granted every three years to three violinists aged 16\u201325. Violinists Benjamin Beilman (USA), Hyeyoon Park (South Korea) and Alexandra Soumm (Russia/France) are the current LMM Award Holders (2012-2015). The first LMM Awards were granted from 2009 to 2012 to Jennifer Pike (UK), Agata Szymczewska (Poland) and Elena Urioste (USA).",
            "score": 90.72264099121094
        },
        {
            "docid": "6667807_2",
            "document": "LMMS . LMMS (formerly Linux MultiMedia Studio) is a digital audio workstation application program. When LMMS is executed on a computer with appropriate hardware, it allows music to be produced by arranging samples, synthesizing sounds, playing on a MIDI keyboard and combining the features of trackers and sequencers. It supports the Linux Audio Developer's Simple Plugin API (LADSPA), and Virtual Studio Technology (VST) plug-ins. It is free software released under the GNU General Public License, version 2 (GPLv2).",
            "score": 90.10430145263672
        },
        {
            "docid": "25291810_6",
            "document": "London Music Masters . History London Music Masters (LMM) was founded in 2007 by Victoria Sharp (now Robey OBE) and Professor Itzhak Rashkovsky with the dual aim of introducing classical music to primary school children from financially disadvantaged and diverse backgrounds and supporting some of the best of the next generation of young professional musicians. It has two main focus areas: LMM Learning (incorporating LMM Learning, teacher training and community performance)and LMM Artists (Awards programme and Ambassador Scheme. The organisation has worked with over 670 children in inner city schools and helped develop the careers of six of the finest young musicians of their generation. London Music Masters aims to invest in future generations in order to help them reach their full potential as both musicians and contributors to society.",
            "score": 87.35519409179688
        },
        {
            "docid": "34198573_8",
            "document": "Lightweight Multirole Missile . In July 2014, Thales unveiled a modification of the LMM that turns it into a glide bomb, called the FreeFall LMM (FFLMM). Thales partnered with Textron to market it as the Fury for the U.S. market, who provides a height-of-burst sensor and electronic safe and arm device. The weapon had been in development for 18 months and undergone initial test drops in August 2013. In comparison to the LMM, the FFLMM removes the rocket motor and associated components while keeping the body and control actuators, as well as adding INS and GPS navigation, semi-active laser guidance in place of the beam-riding system, and four enlarged fins for increased lift.",
            "score": 86.9259262084961
        },
        {
            "docid": "25291810_5",
            "document": "London Music Masters . Internationally renowned musicians such as percussionist Colin Currie, pianist Benjamin Grosvenor, British lyric soprano Nadine Benjamin and violinists Anthony Marwood and Tai Murray support this charity\u2019s work as LMM Ambassadors. During the last year of his life American composer Elliott Carter was also an LMM Ambassador. Other recognised musicians, such as violinists Midori Got\u014d and Nicola Benedetti have collaborated with LMM in specific projects.",
            "score": 85.30912017822266
        },
        {
            "docid": "21159249_18",
            "document": "Loan modification in the United States . Following the lead of the Middle District, the Southern District of Florida Bankruptcy Court has initiated its own loss mitigation mediation (\u201cLMM\u201d) program. The LMM Program kicked off on April 1, 2013 and unlike the Middle District, the Southern District\u2019s program has more requirements for all parties and includes debtors in all chapters, not just Chapter 13. Chapter 7 debtors may use LMM to request a surrender of the property (a real surrender that provides for a transfer of title). LMM may be used by Chapter 13 debtors to request and apply for modification through mediation or surrender of any property they no longer want to own.",
            "score": 85.07798767089844
        },
        {
            "docid": "49954010_2",
            "document": "Lahore Music Meet . Lahore Music Meet (LMM) is an annual two-day event. LMM is primarily dedicated to celebration and critique of Music in Pakistan. LMM is intended to gather music enthusiasts, artists, people from music industry and academicians involved in music. Lahore Music Meet was founded by Natasha Noorani, Zahra Paracha, Noor Habib, Ayesha Haroon and Hasan Abbas.",
            "score": 83.46282958984375
        },
        {
            "docid": "6667807_1",
            "document": "LMMS . LMMS",
            "score": 82.72532653808594
        },
        {
            "docid": "904367_13",
            "document": "Duramax V8 engine . The LMM (engine code \"6\") debuted part way through 2007 and ended production with the start of the 2011 calendar year and is mated to the 6-speed Allison transmission. The LMM was the only Duramax offered for model years 2008\u20132010. A version was used in the Trident Iceni.",
            "score": 82.01459503173828
        },
        {
            "docid": "36835_5",
            "document": "Auger electron spectroscopy . The types of state-to-state transitions available to electrons during an Auger event are dependent on several factors, ranging from initial excitation energy to relative interaction rates, yet are often dominated by a few characteristic transitions. Because of the interaction between an electron's spin and orbital angular momentum (spin-orbit coupling) and the concomitant energy level splitting for various shells in an atom, there are a variety of transition pathways for filling a core hole. Energy levels are labeled using a number of different schemes such as the j-j coupling method for heavy elements (\"Z\" \u2265 75), the Russell-Saunders L-S method for lighter elements (\"Z\" < 20), and a combination of both for intermediate elements. The j-j coupling method, which is historically linked to X-ray notation, is almost always used to denote Auger transitions. Thus for a formula_5 transition, \"K\" represents the core level hole, formula_6 the relaxing electron's initial state, and formula_7 the emitted electron's initial energy state. Figure 1(b) illustrates this transition with the corresponding spectroscopic notation. The energy level of the core hole will often determine which transition types will be favored. For single energy levels, i.e. \"K\", transitions can occur from the L levels, giving rise to strong KLL type peaks in an Auger spectrum. Higher level transitions can also occur, but are less probable. For multi-level shells, transitions are available from higher energy orbitals (different \"n, \u2113\" quantum numbers) or energy levels within the same shell (same \"n\", different \"\u2113\" number). The result are transitions of the type LMM and KLL along with faster Coster\u2013Kronig transitions such as LLM. While Coster\u2013Kronig transitions are faster, they are also less energetic and thus harder to locate on an Auger spectrum. As the atomic number Z increases, so too does the number of potential Auger transitions. Fortunately, the strongest electron-electron interactions are between levels that are close together, giving rise to characteristic peaks in an Auger spectrum. KLL and LMM peaks are some of the most commonly identified transitions during surface analysis. Finally, valence band electrons can also fill core holes or be emitted during KVV-type transitions.",
            "score": 81.8203353881836
        },
        {
            "docid": "49954010_6",
            "document": "Lahore Music Meet . LMM 2016 held on 2nd and 3 April 2016. LMM 2016 mainstage performances were by Attaullah Khan Esakhelvi, Mai Dhai, Sounds Of Kolachi and Red Blood Cat.",
            "score": 80.37100219726562
        },
        {
            "docid": "11808249_6",
            "document": "Genome-wide association study . Any two human genomes differ in millions of different ways. There are small variations in the individual nucleotides of the genomes (SNPs) as well as many larger variations, such as deletions, insertions and copy number variations. Any of these may cause alterations in an individual's traits, or phenotype, which can be anything from disease risk to physical properties such as height. Around the year 2000, prior to the introduction of GWA studies, the primary method of investigation was through inheritance studies of genetic linkage in families. This approach had proven highly useful towards single gene disorders. However, for common and complex diseases the results of genetic linkage studies proved hard to reproduce. A suggested alternative to linkage studies was the genetic association study. This study type asks if the allele of a genetic variant is found more often than expected in individuals with the phenotype of interest (e.g. with the disease being studied). Early calculations on statistical power indicated that this approach could be better than linkage studies at detecting weak genetic effects.",
            "score": 79.86063385009766
        },
        {
            "docid": "34132734_14",
            "document": "Family-based QTL mapping . Linkage and association analysis are primary tools for gene discovery, localization and functional analysis. While conceptual underpinning of these approaches have been long known, advances in recent decades in molecular genetics, development in efficient algorithms, and computing power have enabled the large scale application of these methods. While linkage studies seek to identify loci cosegregate with the trait within families, association studies seek to identify particular variants that are associated with the phenotype at the population level. These are complementary methods that, together, provide means to probe the genome and describe etiology of complex traits. In linkage studies, we seek to identify the loci that cosegregate with a specific genomic region, tagged by polymorphic markers, within families. In contrast, in association studies, we seek a correlation between a specific genetic variation and trait variation in sample of individuals, implicating a causal role of the variant.",
            "score": 79.44364166259766
        },
        {
            "docid": "23004551_22",
            "document": "Paired difference test . Another application of paired difference testing arises when comparing two groups in a set of observational data, with the goal being to isolate the effect of one factor of interest from the effects of other factors that may play a role. For example, suppose teachers adopt one of two different approaches, denoted \"A\" and \"B\", to teaching a particular mathematical topic. We may be interested in whether the performances of the students on a standardized mathematics test differ according to the teaching approach. If the teachers are free to adopt approach A or approach B, it is possible that teachers whose students are already performing well in mathematics will preferentially choose method A (or vice versa). In this situation, a simple comparison between the mean performances of students taught with approach A and approach B will likely show a difference, but this difference is partially or entirely due to the pre-existing differences between the two groups of students. In this situation, the baseline abilities of the students serve as a confounding variable, in that they are related to both the outcome (performance on the standardized test), and to the treatment assignment to approach A or approach B.",
            "score": 78.68152618408203
        },
        {
            "docid": "6667807_7",
            "document": "LMMS . As of 1 June 2018, the 6th release candidate of LMMS version 1.2.0 is in beta test and is considered to be 96% ready for production release.",
            "score": 78.32816314697266
        },
        {
            "docid": "42285920_4",
            "document": "Laser microprobe mass spectrometer . Unlike other methods of microprobe analysis which involve ions or electrons, the LMMS microproble fires an ultraviolet pulse in order to create ions. As a result, this method is much better at detecting qualitatively rather than quantitatively.",
            "score": 78.30432891845703
        },
        {
            "docid": "34198573_6",
            "document": "Lightweight Multirole Missile . LMM is intended to provide a single family of weapons that can be used in different modes, including:",
            "score": 78.1650619506836
        },
        {
            "docid": "6667807_4",
            "document": "LMMS . LMMS accepts soundfonts and GUS patches. It can import Musical Instrument Digital Interface (MIDI) and hydrogen files. FL Studio project (FLP) files had been able to be imported, but FLP support has since been removed. It can write and read customized presets and themes. Audio can be exported in the OGG and WAV file formats, and the projects are saved in the plain XML codice_1 or compressed codice_2 file format. It can use VST plug-ins, though currently the macOS port doesn't support them.",
            "score": 78.12804412841797
        },
        {
            "docid": "34142465_10",
            "document": "Linkage based QTL mapping . Linkage and association analysis are primary tool for gene discovery, localization and functional analysis. While conceptual underpinning of these approaches have been long known, advances in recent decades in molecular genetics, development in efficient algorithms, and computing power have enabled the large scale application of these methods. While linkage studies seek to identify loci that cosegregate with the trait within families, association studies seek to identify particular variants that are associated with the phenotype at the population level. These are complementary methods that, together, provide means to probe the genome and describe etiology of complex human traits. In linkage studies, we seek to identify the loci that cosegregate with a specific genomic region, tagged by polymorphic markers, within families. In contrast, in association studies, we seek a correlation between a specific genetic variation and trait variation in sample of individuals, implicating a causal role of the variant. Linkage tests are powerful and specific for gene discovery, the localization of locus can be achieved only to a certain level of precision \u2013 on order of megabases \u2013 that potentially represents a region that potentially include hundreds of genes.",
            "score": 77.40977478027344
        },
        {
            "docid": "6667807_3",
            "document": "LMMS . LMMS is available for multiple operating systems, including Linux, OpenBSD, macOS and Windows. It requires a 1\u00a0GHz CPU, 512 MB of RAM and a two-channel sound card.",
            "score": 76.96450805664062
        },
        {
            "docid": "49954010_3",
            "document": "Lahore Music Meet . LMM 2015 started on Saturday 4 April as a two-day event. LMM 2015 held at the Al Hamra Arts Council, Lahore. The two-day event included a panel discussions and storytelling sessions on various types and aspects of music with prominent names in the music business invited to engage with the audience, workshops through which aspiring musicians and interested individuals can learn from music educationists and mentors.",
            "score": 76.7353286743164
        },
        {
            "docid": "826997_56",
            "document": "Regression analysis . All major statistical software packages perform least squares regression analysis and inference. Simple linear regression and multiple regression using least squares can be done in some spreadsheet applications and on some calculators. While many statistical software packages can perform various types of nonparametric and robust regression, these methods are less standardized; different software packages implement different methods, and a method with a given name may be implemented differently in different packages. Specialized regression software has been developed for use in fields such as survey analysis and neuroimaging.",
            "score": 76.45863342285156
        },
        {
            "docid": "3259720_7",
            "document": "Multifactor dimensionality reduction . As illustrated above, the basic constructive induction algorithm in MDR is very simple. However, its implementation for mining patterns from real data can be computationally complex. As with any machine learning algorithm there is always concern about overfitting. That is, machine learning algorithms are good at finding patterns in completely random data. It is often difficult to determine whether a reported pattern is an important signal or just chance. One approach is to estimate the generalizability of a model to independent datasets using methods such as cross-validation. Models that describe random data typically don't generalize. Another approach is to generate many random permutations of the data to see what the data mining algorithm finds when given the chance to overfit. Permutation testing makes it possible to generate an empirical p-value for the result. Replication in independent data may also provide evidence for an MDR model but can be sensitive to difference in the data sets. These approaches have all been shown to be useful for choosing and evaluating MDR models. An important step in an machine learning exercise is interpretation. Several approaches have been used with MDR including entropy analysis and pathway analysis. Tips and approaches for using MDR to model gene-gene interactions have been reviewed.",
            "score": 76.36868286132812
        },
        {
            "docid": "21159249_20",
            "document": "Loan modification in the United States . Election to participate in the LMM program will suspend any pending motions for relief from stay (\u201cMFR\u201d). However, while an LMM is pending, debtors will be required to pay 31% of their gross monthly income through the Chapter 13 plan as an \u201cadequate protection\u201d payment. The fees a debtor will have to pay to participate in the program will typically include an $1,800 fee to their bankruptcy attorney for handling the modification through their Chapter 13 plan and an approximately $300 fee to the mediator.",
            "score": 76.24600219726562
        },
        {
            "docid": "13609405_20",
            "document": "Lopez Museum . Beginning in 2001, LMM instilled a semi-annual changing exhibit program in an effort to practice prudent resource use as well as infuse its exhibitions with a less-static character. This tactical shift sought to balance attention to interpreting segments of the permanent collection aongside exploring intersections in contemporary as well as pop-inflected artistic expression through curatorial intervention and the instigation of artists\u2019 projects. LMM\u2019s exhibitions have included:",
            "score": 76.06954193115234
        },
        {
            "docid": "54190305_12",
            "document": "FORMIND . Forest factory approach: Forest gap models like FORMIND can be used to study the relationship between forest productivity and species diversity. Recent studies have shown that forest productivity often increases with increasing tree species diversity. However, several studies show unchanged or even inverse relationships between productivity and diversity. We studied a broader range of diversity-productivity relationships. Instead of long-term simulations, thousands of different forest stands have been generated, combining different species mixtures with various forest structures (e.g., different basal area values or heterogenic tree heights). This approach is called forest factory and introduce a fundamental new concept for using vegetation models. For each of these virtual forest stands, forest productivity can be calculated using forest gap models. The obtained diversity \u2013 productivity relationships can then be compared to field studies. This new way of using a forest gap model as an analysis tool of a large number of forest stands enables a much faster analysis of numerous forests compared with the classical method of simulating forest successions (forest factory approach).",
            "score": 75.91703033447266
        },
        {
            "docid": "50518079_12",
            "document": "Human Genome Structural Variation . New methods have been developed to analyze human genetic structural variation at high resolutions. The methods used to test the genome are in either a specific targeted way or in a genome wide manner. For Genome wide tests, array-based comparative genome hybridization approaches bring the best genome wide scans to find new copy number variants. These techniques use DNA fragments that are labeled from a genome of interest and are hybridized, with another genome labeled differently, to arrays spotted with cloned DNA fragments. This reveals copy number differences between two genomes.",
            "score": 75.69862365722656
        },
        {
            "docid": "13609405_24",
            "document": "Lopez Museum . Also as an explicit demonstration of its commitment to scholarship are the slew of publications that have seen print under LMM\u2019s initiative. These include: \"The Philippine Insurrection Against the US\" edited by Renato Constantino, \"Juan Luna: The Filipino Painter\" by Santiago A. Pilar, \"Orchidiana Philippiniana\" by Helen Valmayor, \"Philippine Rariora\" by Mauro Garcia, \"Manansala Nudes and Fernando Zobel\" by Rod Paras-Perez. Its most recent releases are Locus: Interventions in Art Practice (co-published through the National Commission for Culture and the Arts and Pananaw ng Sining Bayan, Inc.) and LMM\u2019s contribution to the Philippine Centennial, Hidalgo and the Generation of 1872 by Alfredo Roces.",
            "score": 74.79830169677734
        },
        {
            "docid": "50613151_5",
            "document": "Genome-wide complex trait analysis . As genome sequencing costs dropped steeply over the 2000s, acquiring enough markers on enough subjects for reliable estimates using very distantly related individuals became possible. An early application of the method to humans came with Visscher et al. 2006/2007, which used SNP markers to estimate the actual relatedness of siblings and estimate heritability from the direct genetics. In humans, unlike the original animal/plant applications, relatedness is usually known with high confidence in the 'wild population', and the benefit of GCTA is connected more to avoiding assumptions of classic behavioral genetics designs and verifying their results, and partitioning heritability by SNP class and chromosomes. The first use of GCTA proper in humans was published in 2010, finding 45% of variance in human height can be explained by the included SNPs. (Large GWASes on height have since confirmed the estimate.) The GCTA algorithm was then described and a software implementation published in 2011. It has since been used to study a wide variety of biological, medical, psychiatric, and psychological traits in humans, and inspired many variant approaches.",
            "score": 73.79817962646484
        }
    ]
}