{
    "q": [
        {
            "docid": "18213407_2",
            "document": "Kinetic capillary electrophoresis . Kinetic capillary electrophoresis or KCE is capillary electrophoresis of molecules that interact during electrophoresis. KCE was introduced and developed by Professor Sergey Krylov and his research group at York University, Toronto, Canada. It serves as a conceptual platform for development of homogeneous chemical affinity methods for studies of molecular interactions (measurements of binding and rate constants) and affinity purification (purification of known molecules and search for unknown molecules). Different KCE methods are designed by varying initial and boundary conditions \u2013 the way interacting molecules enter and exit the capillary. Several KCE methods were described: non-equilibrium capillary electrophoresis of the equilibrium mixtures (NECEEM), sweeping capillary electrophoresis (SweepCE), plug-plug KCE (ppKCE). More detailed description and several applications of KCE methods (measuring equilibrium and rate constants of molecular interactions, quantitative affinity analysis of proteins, thermochemistry of protein\u2013ligand interactions, selection of aptamers, determination of temperature inside a capillary) can be found in a PDF presentation: KCE ia a conceptual platform for kinetic homogeneous affinity methods.",
            "score": 33.9526801109314
        },
        {
            "docid": "5497456_4",
            "document": "Kinetic resolution . Kinetic resolution is a possible method for irreversibly differentiating a pair of enantiomers due to (potentially) different activation energies. While both enantiomers are at the same Gibbs free energy level by definition, and the products of the reaction with both enantiomers are also at equal levels, the formula_1, or transition state energy, can differ. In the image below, the R enantiomer has a lower formula_1 and would thus react faster than the S enantiomer. The ideal kinetic resolution is that in which only one enantiomer reacts, i.e. k\u00bbk. The selectivity (s) of a kinetic resolution is related to the rate constants of the reaction of the R and S enantiomers, k and k respectively, by s=k/k, for k>k. This selectivity can also be referred to as the relative rates of reaction. This can be written in terms of the free energy difference between the high- and low-energy transitions states, formula_3. The selectivity can also be expressed in terms of ee of the recovered starting material and conversion (c), if first-order kinetics (in substrate) are assumed. If it is assumed that the S enantiomer of the starting material racemate will be recovered in excess, it is possible to express the concentrations (mole fractions) of the S and R enantiomers as where ee is the ee of the starting material. Note that for c=0, which signifies the beginning of the reaction, formula_7, where these signify the initial concentrations of the enantiomers. Then, for stoichiometric chiral resolving agent B*, Note that, if the resolving agent is stoichiometric and achiral, with a chiral catalyst, the [B*] term does not appear. Regardless, with a similar expression for R, we can express s as If we wish to express this in terms of the enantiomeric excess of the product, ee\", we must make use of the fact that, for products R' and S' from R and S, respectively From here, we see that which gives us which, when we plug into our expression for s derived above, yield",
            "score": 48.962642431259155
        },
        {
            "docid": "4384325_7",
            "document": "Robert Gilbert (chemist) . As with unimolecular reactions, the keys to the qualitative and quantitative understanding of the many processes in emulsion polymerisation are the rate coefficients of the individual steps. These steps are initiation (how quickly a growing chain starts), propagation (how quickly individual monomer units are added), radical loss processes (the termination and transfer of radical activity), and particle formation (nucleation). With Prof D Napper, Gilbert applied equations that he had solved in gas-phase chemistry to the area of emulsion polymerisation. This opened the way for him to develop\u2014initially in collaboration with Napper\u2014new theoretical and experimental methods for extracting the rate coefficients of elementary processes. He produced targeted data using these methods, particularly the time evolution of reaction rates and molecular-weight and particle-size distributions. This included novel types of systems, such as \u03b3-radiolysis relaxation, in which events such as radical loss can be separated from radical propagation and growth.",
            "score": 76.6280642747879
        },
        {
            "docid": "359135_2",
            "document": "Chemical kinetics . Chemical kinetics, also known as reaction kinetics, is the study of rates of chemical processes. Chemical kinetics includes investigations of how different experimental conditions can influence the speed of a chemical reaction and yield information about the reaction's mechanism and transition states, as well as the construction of mathematical models that can describe the characteristics of a chemical reaction.",
            "score": 31.101758003234863
        },
        {
            "docid": "413102_6",
            "document": "Folding@home . Due to the complexity of proteins' conformation or configuration space (the set of possible shapes a protein can take), and limits in computing power, all-atom molecular dynamics simulations have been severely limited in the timescales which they can study. While most proteins typically fold in the order of milliseconds, before 2010 simulations could only reach nanosecond to microsecond timescales. General-purpose supercomputers have been used to simulate protein folding, but such systems are intrinsically costly and typically shared among many research groups. Further, because the computations in kinetic models occur serially, strong scaling of traditional molecular simulations to these architectures is exceptionally difficult. Moreover, as protein folding is a stochastic process and can statistically vary over time, it is challenging computationally to use long simulations for comprehensive views of the folding process. Protein folding does not occur in one step. Instead, proteins spend most of their folding time, nearly 96% in some cases, \"waiting\" in various intermediate conformational states, each a local thermodynamic free energy minimum in the protein's energy landscape. Through a process known as adaptive sampling, these conformations are used by Folding@home as starting points for a set of simulation trajectories. As the simulations discover more conformations, the trajectories are restarted from them, and a Markov state model (MSM) is gradually created from this cyclic process. MSMs are discrete-time master equation models which describe a biomolecule's conformational and energy landscape as a set of distinct structures and the short transitions between them. The adaptive sampling Markov state model method significantly increases the efficiency of simulation as it avoids computation inside the local energy minimum itself, and is amenable to distributed computing (including on GPUGRID) as it allows for the statistical aggregation of short, independent simulation trajectories. The amount of time it takes to construct a Markov state model is inversely proportional to the number of parallel simulations run, i.e., the number of processors available. In other words, it achieves linear parallelization, leading to an approximately four orders of magnitude reduction in overall serial calculation time. A completed MSM may contain tens of thousands of sample states from the protein's phase space (all the conformations a protein can take on) and the transitions between them. The model illustrates folding events and pathways (i.e., routes) and researchers can later use kinetic clustering to view a coarse-grained representation of the otherwise highly detailed model. They can use these MSMs to reveal how proteins misfold and to quantitatively compare simulations with experiments.",
            "score": 79.78574728965759
        },
        {
            "docid": "33482057_39",
            "document": "Reaction progress kinetic analysis . Kineticists have historically relied on linearization of rate data to extrapolate rate constants, perhaps best demonstrated by the widespread use of the standard Lineweaver\u2013Burke linearization of the Michaelis\u2013Menten equation. Linearization techniques were of particular importance before the advent of computing techniques capable of fitting complex curves, and they remain a staple in kinetics due to their intuitively simple presentation. It is important to note that linearization techniques should \"NOT\" be used to extract numerical rate constants as they introduce a large degree of error relative to alternative numerical techniques. Graphical rate laws do, however, maintain that intuitive presentation of linearized data, such that visual inspection of the plot can provide mechanistic insight regarding the reaction at hand. The basis for a graphical rate law rests on the rate (\"v\") vs. substrate concentration ([S]) plots discussed above. For example, in the simple cycle discussed with regard to different-excess experiments a plot of vs. [B] and its twin vs. [A] can provide intuitive insight about the order of each of the reagents. If plots of vs. [B] overlay for multiple experiments with different-excess, the data are consistent with a first-order dependence on [A]. The same could be said for a plot of vs. [A]; overlay is consistent with a first-order dependence on [B]. Non-overlaying results of these graphical rate laws are, of course, possible and are indicative of higher order dependence on the substrates probed. Blackmond has proposed a presenting the results of different-excess experiments with a series of graphical rate equations (that she presents in a flow-chart adapted here), but it is important to note that her proposed method is only one of many possible methods to display the kinetic relationship. Furthermore, while the presentation of graphical rate laws may at times be considered a visually simplified way to present complex kinetic data, fitting the raw kinetic data for analysis by differential or other rigorous numerical methods is necessary to extract accurate and quantitative rate constants and reaction orders.",
            "score": 39.03760623931885
        },
        {
            "docid": "8124077_5",
            "document": "Transition state analog . Kinetic isotope effect (KIE) is a measurement of the reaction rate of isotope-labeled reactants against the more common natural substrate. Kinetic isotope effect values are a ratio of the turnover number and include all steps of the reaction. Intrinsic kinetic isotope values stem from the difference in the bond vibrational environment of an atom in the reactants at ground state to the environment of the atom's transition state. Through the kinetic isotope effect much insight can be gained as to what the transition state looks like of an enzyme-catalyzed reaction and guide the development of transition state analogs.",
            "score": 44.40746021270752
        },
        {
            "docid": "1106771_33",
            "document": "Kinetic isotope effect . Kinetic isotope effect measurement at natural abundance is a simple general method for measuring kinetic isotope effects (KIE) for chemical reactions performed with materials of natural abundance. This technique for measuring KIEs overcomes many limitations of previous KIE measurement methods. KIE measurements from isotopically labeled materials require a new synthesis for each isotopically labeled material (a process often prohibitively difficult), a competition reaction, and an analysis. The KIE measurement at natural abundance avoids these issues by taking advantage of high precision quantitative techniques (nuclear magnetic resonance spectroscopy, isotope-ratio mass spectrometry) to site selectively measure kinetic fractionation of isotopes, in either product or starting material for a given chemical reaction.",
            "score": 31.236196041107178
        },
        {
            "docid": "33599550_3",
            "document": "Nanoreactor . Researchers in the Netherlands have succeeded in building nanoreactors that can perform one-pot multistep reactions - the next step towards artificial cell-like devices in addition for applications involving the screening and diagnosis of a disease or illness. A biochemical nanoreactor is created simply by unwrapping a biological virus through scientific methods, eliminating its harmful contents, and re-assembling its protein coat around a single molecule of enzyme. The kinetic isotope effect is trapped in a single molecule within a membrane-based nanoreactor. This is a phenomenon that has been found by researchers in the United Kingdom during experiments done on September 2010. The kinetic isotope effect, where the rate of a reaction is influenced by the presence of an isotopic atom in solution, is an important principle for elucidating reaction mechanisms. This recent finding could open up new methods to study chemical reactions. They may even aid in the process of creating new (and even more powerful) nanoreactors.",
            "score": 48.58492636680603
        },
        {
            "docid": "1505381_29",
            "document": "Numerical weather prediction . On a molecular scale, there are two main competing reaction processes involved in the degradation of cellulose, or wood fuels, in wildfires. When there is a low amount of moisture in a cellulose fiber, volatilization of the fuel occurs; this process will generate intermediate gaseous products that will ultimately be the source of combustion. When moisture is present\u2014or when enough heat is being carried away from the fiber, charring occurs. The chemical kinetics of both reactions indicate that there is a point at which the level of moisture is low enough\u2014and/or heating rates high enough\u2014for combustion processes become self-sufficient. Consequently, changes in wind speed, direction, moisture, temperature, or lapse rate at different levels of the atmosphere can have a significant impact on the behavior and growth of a wildfire. Since the wildfire acts as a heat source to the atmospheric flow, the wildfire can modify local advection patterns, introducing a feedback loop between the fire and the atmosphere.",
            "score": 43.944353461265564
        },
        {
            "docid": "4384325_10",
            "document": "Robert Gilbert (chemist) . These developments led to a deep understanding of basic processes in free-radical polymerisation\u2014the commonest industrial process. For the propagation reaction, Gilbert led an international team that produced a methodology that overcame the long-standing problem of obtaining reliable rate coefficients for this process. He showed that the Arrhenius parameters for different types of monomer take different classes of values, and developed qualitative and quantitative understanding of these classes from fundamental transition-state theory and quantum mechanics. These new methods were based on those that he had developed in his work on unimolecular gas-phase processes. For the termination reaction, his data and models led to the qualitative and quantitative understanding of this process as diffusion-controlled.",
            "score": 37.81101632118225
        },
        {
            "docid": "1872854_31",
            "document": "Biochemical cascade . In the post-genomic age, high-throughput sequencing and gene/protein profiling techniques have transformed biological research by enabling comprehensive monitoring of a biological system, yielding a list of differentially expressed genes or proteins, which is useful in identifying genes that may have roles in a given phenomenon or phenotype. With DNA microarrays and genome-wide gene engineering, it is possible to screen global gene expression profiles to contribute a wealth of genomic data to the public domain. With RNA interference, it is possible to distill the inferences contained in the experimental literature and primary databases into knowledge bases that consist of annotated representations of biological pathways. In this case, individual genes and proteins are known to be involved in biological processes, components, or structures, as well as how and where gene products interact with each other. Pathway-oriented approaches for analyzing microarray data, by grouping long lists of individual genes, proteins, and/or other biological molecules according to the pathways they are involved in into smaller sets of related genes or proteins, which reduces the complexity, have proven useful for connecting genomic data to specific biological processes and systems. Identifying active pathways that differ between two conditions can have more explanatory power than a simple list of different genes or proteins. In addition, a large number of pathway analytic methods exploit pathway knowledge in public repositories such as Gene Ontology (GO) or Kyoto Encyclopedia of Genes and Genomes (KEGG), rather than inferring pathways from molecular measurements. Furthermore, different research focuses have given the word \"pathway\" different meanings. For example, 'pathway' can denote a metabolic pathway involving a sequence of enzyme-catalyzed reactions of small molecules, or a signaling pathway involving a set of protein phosphorylation reactions and gene regulation events. Therefore, the term \"pathway analysis\" has a very broad application. For instance, it can refer to the analysis physical interaction networks (e.g., protein\u2013protein interactions), kinetic simulation of pathways, and steady-state pathway analysis (e.g., flux-balance analysis), as well as its usage in the inference of pathways from expression and sequence data. Several functional enrichment analysis tools and algorithms have been developed to enhance data interpretation. The existing knowledge base\u2013driven pathway analysis methods in each generation have been summarized in recent literature.",
            "score": 59.438616156578064
        },
        {
            "docid": "33980770_29",
            "document": "Organ-on-a-chip . Recently, physiologically based perfusion in vitro systems have been developed to provide cell culture environment close to in vivo cell environment. A new testing platforms based on multi-compartmental perfused systems have gained a remarkable interest in pharmacology and toxicology. It aims to provide a cell culture environment close to the in vivo situation to reproduce more reliably \"in vivo\" mechanisms or ADME processes that involve its absorption, distribution, metabolism, and elimination. Perfused in vitro systems combined with kinetic modelling are promising tools for studying in vitro the different processes involved in the toxicokinetics of xenobiotics.",
            "score": 53.401835680007935
        },
        {
            "docid": "21789230_4",
            "document": "Associative substitution . Examples of associative mechanisms are commonly found in the chemistry of 16e square planar metal complexes, e.g. Vaska's complex and tetrachloroplatinate. These compounds (MX) bind the incoming (substituting) ligand Y to form pentacoordinate intermediates MXY that in a subsequent step dissociates one of their ligands. Dissociation of Y results in no reaction, but dissociation of X results in net substitution, giving the 16e complex MXY. The first step is typically rate determining. Thus, the entropy of activation is negative, which indicates an increase in order in the system. These reactions follow second order kinetics: the rate of the appearance of product depends on the concentration of MX and Y. The rate law is governed by the Eigen\u2013Wilkins Mechanism. In many substitution reactions, well-defined intermediates are not observed, when the rate of such processes are influenced by the nature of the entering ligand, the pathway is called associative interchange, abbreviated \"I\". Representative is the interchange of bulk and coordinated water in [V(HO)]. In contrast, the slightly more compact ion [Ni(HO)] exchanges water via the \"I\".",
            "score": 47.85158860683441
        },
        {
            "docid": "5229194_2",
            "document": "Kinetic Monte Carlo . The kinetic Monte Carlo (KMC) method is a Monte Carlo method computer simulation intended to simulate the time evolution of some processes occurring in nature. Typically these are processes that occur with known transition rates among states. It is important to understand that these rates are inputs to the KMC algorithm, the method itself cannot predict them.",
            "score": 41.16443967819214
        },
        {
            "docid": "23634_34",
            "document": "Protein . The activities and structures of proteins may be examined \"in vitro,\" \"in vivo, and in silico\". In vitro studies of purified proteins in controlled environments are useful for learning how a protein carries out its function: for example, enzyme kinetics studies explore the chemical mechanism of an enzyme's catalytic activity and its relative affinity for various possible substrate molecules. By contrast, in vivo experiments can provide information about the physiological role of a protein in the context of a cell or even a whole organism. In silico studies use computational methods to study proteins.",
            "score": 53.901036977767944
        },
        {
            "docid": "3408308_21",
            "document": "Metabolic network modelling . In order to perform a dynamic simulation with such a network it is necessary to construct an ordinary differential equation system that describes the rates of change in each metabolite's concentration or amount. To this end, a rate law, i.e., a kinetic equation that determines the rate of reaction based on the concentrations of all reactants is required for each reaction. Software packages that include numerical integrators, such as COPASI or SBMLsimulator, are then able to simulate the system dynamics given an initial condition. Often these rate laws contain kinetic parameters with uncertain values. In many cases it is desired to estimate these parameter values with respect to given time-series data of metabolite concentrations. The system is then supposed to reproduce the given data. For this purpose the distance between the given data set and the result of the simulation, i.e., the numerically or in few cases analytically obtained solution of the differential equation system is computed. The values of the parameters are then estimated to minimize this distance. One step further, it may be desired to estimate the mathematical structure of the differential equation system because the real rate laws are not known for the reactions within the system under study. To this end, the program SBMLsqueezer allows automatic creation of appropriate rate laws for all reactions with the network. Synthetic accessibility is a simple approach to network simulation whose goal is to predict which metabolic gene knockouts are lethal. The synthetic accessibility approach uses the topology of the metabolic network to calculate the sum of the minimum number of steps needed to traverse the metabolic network graph from the inputs, those metabolites available to the organism from the environment, to the outputs, metabolites needed by the organism to survive. To simulate a gene knockout, the reactions enabled by the gene are removed from the network and the synthetic accessibility metric is recalculated. An increase in the total number of steps is predicted to cause lethality. Wunderlich and Mirny showed this simple, parameter-free approach predicted knockout lethality in \"E. coli\" and \"S. cerevisiae\" as well as elementary mode analysis and flux balance analysis in a variety of media.",
            "score": 65.0731748342514
        },
        {
            "docid": "253272_26",
            "document": "Michaelis\u2013Menten kinetics . The first step in the derivation applies the law of mass action, which is reliant on free diffusion. However, in the environment of a living cell where there is a high concentration of proteins, the cytoplasm often behaves more like a gel than a liquid, limiting molecular movements and altering reaction rates. Although the law of mass action can be valid in heterogeneous environments, it is more appropriate to model the cytoplasm as a fractal, in order to capture its limited-mobility kinetics.",
            "score": 60.03551769256592
        },
        {
            "docid": "152611_11",
            "document": "Cellular differentiation . Each specialized cell type in an organism expresses a subset of all the genes that constitute the genome of that species. Each cell type is defined by its particular pattern of regulated gene expression. Cell differentiation is thus a transition of a cell from one cell type to another and it involves a switch from one pattern of gene expression to another. Cellular differentiation during development can be understood as the result of a gene regulatory network. A regulatory gene and its cis-regulatory modules are nodes in a gene regulatory network; they receive input and create output elsewhere in the network. The systems biology approach to developmental biology emphasizes the importance of investigating how developmental mechanisms interact to produce predictable patterns (morphogenesis). (However, an alternative view has been proposed recently. Based on stochastic gene expression, cellular differentiation is the result of a Darwinian selective process occurring among cells. In this frame, protein and gene networks are the result of cellular processes and not their cause. See: Cellular Darwinism) A few evolutionarily conserved types of molecular processes are often involved in the cellular mechanisms that control these switches. The major types of molecular processes that control cellular differentiation involve cell signaling. Many of the signal molecules that convey information from cell to cell during the control of cellular differentiation are called growth factors. Although the details of specific signal transduction pathways vary, these pathways often share the following general steps. A ligand produced by one cell binds to a receptor in the extracellular region of another cell, inducing a conformational change in the receptor. The shape of the cytoplasmic domain of the receptor changes, and the receptor acquires enzymatic activity. The receptor then catalyzes reactions that phosphorylate other proteins, activating them. A cascade of phosphorylation reactions eventually activates a dormant transcription factor or cytoskeletal protein, thus contributing to the differentiation process in the target cell. Cells and tissues can vary in competence, their ability to respond to external signals.",
            "score": 82.62000298500061
        },
        {
            "docid": "7808390_5",
            "document": "Watt W. Webb . Professor Webb pioneered the techniques of Fluorescence Correlation Spectroscopy (FCS) in 1972 and Multiphoton microscopy (MPM) in 1990. FCS enables single-molecule detection in solutions at nanomolar concentrations and provides temporal resolution of the dynamic processes of individual molecules signaled by their fluorescence. FCS reveals molecular mobility, conformational fluctuations and chemical reactions in solutions and allows the detection of extremely sparse molecules and particles. In situ measurements of the dynamics of fluorescence flicker by FCS, photobleaching, phototoxicity, and induced fluorescence are being used to discern dynamics of biological processes and molecular mechanisms of disease. Multiphoton excitation in laser scanning fluorescence microscopy provides for high resolution, high signal-to-noise imaging in living cells and deep in turbid tissues in vivo and significantly reduces photodamage and minimizes image degradation due to scattering and autofluorescence. His laboratory at Cornell University continues to extend the frontiers of these technologies, now for example extending MPM and FCS to imaging molecular processes within the cellular nucleus for gene expression in vivo. Recently initiated is the development of technology for introduction of MPM into Medical Endoscopy for in vivo, in situ real time diagnostics.",
            "score": 79.61905491352081
        },
        {
            "docid": "4350008_5",
            "document": "Protein\u2013protein interaction prediction . It was observed that the phylogenetic trees of ligands and receptors were often more similar than due to random chance. This is likely because they faced similar selection pressures and co-evolved. This method uses the phylogenetic trees of protein pairs to determine if interactions exist. To do this, homologs of the proteins of interest are found (using a sequence search tool such as BLAST) and multiple-sequence alignments are done (with alignment tools such as Clustal) to build distance matrices for each of the proteins of interest. The distance matrices should then be used to build phylogenetic trees. However, comparisons between phylogenetic trees are difficult, and current methods circumvent this by simply comparing distance matrices. The distance matrices of the proteins are used to calculate a correlation coefficient, in which a larger value corresponds to co-evolution. The benefit of comparing distance matrices instead of phylogenetic trees is that the results do not depend on the method of tree building that was used. The downside is that difference matrices are not perfect representations of phylogenetic trees, and inaccuracies may result from using such a shortcut. Another factor worthy of note is that there are background similarities between the phylogenetic trees of any protein, even ones that do not interact. If left unaccounted for, this could lead to a high false-positive rate. For this reason, certain methods construct a background tree using 16S rRNA sequences which they use as the canonical tree of life. The distance matrix constructed from this tree of life is then subtracted from the distance matrices of the proteins of interest. However, because RNA distance matrices and DNA distance matrices have different scale, presumably because RNA and DNA have different mutation rates, the RNA matrix needs to be rescaled before it can be subtracted from the DNA matrices. By using molecular clock proteins, the scaling coefficient for protein distance/RNA distance can be calculated. This coefficient is used to rescale the RNA matrix.",
            "score": 47.00112807750702
        },
        {
            "docid": "44305878_11",
            "document": "Directed differentiation . For basic science, notably developmental biology and cell biology, PSC-derived cells allow to study at the molecular and cellular levels fundamental questions in vitro, that would have been otherwise extremely difficult or impossible to study for technical and ethical reasons in vivo such as embryonic development of human. In particular, differentiating cells are amenable for quantitative and qualitative studies. More complex processes can also be studied in vitro and formation of organoids, including cerebroids, optic cup and kidney have been described.",
            "score": 73.84165549278259
        },
        {
            "docid": "14760404_3",
            "document": "60S ribosomal protein L10 . Ribosomes, the organelles that catalyze protein synthesis, consist of a small 40S subunit and a large 60S subunit. Together these subunits are composed of 4 RNA species and approximately 80 structurally distinct proteins. This gene encodes a ribosomal protein that is a component of the 60S subunit. The protein belongs to the L10E family of ribosomal proteins. It is located in the cytoplasm. In vitro studies have shown that the chicken protein can bind to c-Jun and can repress c-Jun-mediated transcriptional activation, but these activities have not been demonstrated in vivo. This gene was initially identified as a candidate for a Wilms tumor suppressor gene, but later studies determined that this gene is not involved in the suppression of Wilms tumor. This gene has been referred to as 'laminin receptor homolog' because a chimeric transcript consisting of sequence from this gene and sequence from the laminin receptor gene was isolated; however, it is not believed that this gene encodes a laminin receptor. Transcript variants utilizing alternative polyA signals exist. The variant with the longest 3' UTR overlaps the deoxyribonuclease I-like 1 gene on the opposite strand. This gene is co-transcribed with the small nucleolar RNA gene U70, which is located in its fifth intron. As is typical for genes encoding ribosomal proteins, there are multiple processed pseudogenes of this gene dispersed through the genome.",
            "score": 51.68031108379364
        },
        {
            "docid": "35041210_4",
            "document": "Donald Truhlar . Truhlar is known for his contributions to theoretical chemical dynamics of chemical reactions; quantum mechanical scattering theory of chemical reactions and molecular energy transfer; electron scattering; theoretical kinetics and chemical dynamics; potential energy surfaces and molecular interactions; path integrals; variational transition state theory; the use of electronic structure theory for calculations of chemical structure, reaction rates, electronically nonadiabatic processes, and solvation effects; photochemistry; combustion chemistry; heterogeneous, homogeneous, and enzyme catalysis; atmospheric and environmental chemistry; drug design; nanoparticle structure and energetics; and density functional theory, including the Minnesota Functionals. He has been the author of more than 1100 papers published in journals of international repute. One of his papers, in which the M06 density functional methods were described, has received more than 10000 citations.",
            "score": 42.39059340953827
        },
        {
            "docid": "57011275_6",
            "document": "Yuri Berlin . Berlin has undertaken research on a broad range of areas in both physical and theoretical chemistry, involving stochastic dynamics of complex systems, chemical kinetics and transport of active species in condensed phase and in biological molecules, physical chemistry of liquids and solids, theoretical biophysics and physical aspects of prebiotic evolution, physical methods for the initiation of chemical reactions, in particular cryochemistry, radiation chemistry, photo, and high pressure chemistry. His research covers a vast range of fields, such as the theory of excess electrons in non-polar liquids and liquid noble gases, charge transfer under extreme conditions, chemical processes coupled to structural rearrangements of molecular environment, dispersive kinetics, the effects of correlated fluctuations in chemical and biological properties, the role of static and dynamic disorder in the mechanism of chemical processes in condensed media. Later works are focused on mechanism and kinetics of charge transfer and transport in DNA, culminated in a series of studies of various DNA constructs as building blocks of molecular circuitry. Berlin has published over 170 papers in scientific journals.",
            "score": 39.027122259140015
        },
        {
            "docid": "45329906_21",
            "document": "Solvent models . Quantitative Structure\u2013Activity Relationships (QSAR)/Quantitative Structure\u2013Property Relationships (QSPR), whilst unable to directly model the physical process occurring in a condensed solvent phase, can provide useful predictions of solvent and solvation properties and activities; such as the solubility of a solute. These methods come in a varied way from simple regression models to sophisticated machine learning methods. Generally, QSAR/QSPR methods require descriptors; these come in many different forms and are used to represent physical features and properties of a system of interest. Descriptors are generally single numerical values which hold some information about a physical property. A regression model or statistical learning model is then applied to find a correlation between the descriptor(s) and the property of interest. Once trained on some known data these model can be applied to similar unknown data to make predictions. Typically the known data comes from experimental measurement, although there is no reason why similar methods can not be used to correlate descriptor(s) with theoretical or predicted values. It is currently debated whether if more accurate experimental data was used to train these models whether the prediction from such models would be more accurate.",
            "score": 41.412845611572266
        },
        {
            "docid": "3766560_3",
            "document": "Entropy of activation . The value of \u0394S provides clues about the molecularity of the rate determining step in a reaction, i.e. whether the reactants are bonded to each other, or not. Positive values suggest that entropy increases upon achieving the transition state, which often indicates a dissociative mechanism in which the activated complex is loosely bound and about to dissociate. Negative values for \u0394S indicate that entropy decreases on forming the transition state, which often indicates an associative mechanism in which two reaction partners form a single activated complex.",
            "score": 53.65415287017822
        },
        {
            "docid": "7792469_11",
            "document": "MRNA display . Although there are many other molecular display technologies, such as phage display, bacterial display, yeast display, and ribosome display, mRNA display technology has many advantages over the others. The first three biological display libraries listed have polypeptides or proteins expressed on the respective microorganism\u2019s surface and the accompanying coding information for each polypeptide or protein is retrievable from the microorganism\u2019s genome. However, the library size for these three \"in vivo\" display systems is limited by the transformation efficiency of each organism. For example, the library size for phage and bacterial display is limited to 1-10 \u00d7 10^9 different members. The library size for yeast display is even smaller. Moreover, these cell-based display system only allow the screening and enrichment of peptides/proteins containing natural amino acids. In contrast, mRNA display and ribosome display are \"in vitro\" selection methods. They allow a library size as large as 10^15 different members. The large library size increases the probability to select very rare sequences, and also improves the diversity of the selected sequences. In addition, \"in vitro\" selection methods remove unwanted selection pressure, such as poor protein expression, and rapid protein degradation, which may reduce the diversity of the selected sequences. Finally, \"in vitro\" selection methods allow the application of \"in vitro\" mutagenesis and recombination techniques throughout the selection process.",
            "score": 69.11251044273376
        },
        {
            "docid": "39510164_7",
            "document": "Mutagenesis (molecular biology technique) . Many researchers seek to introduce selected changes to DNA in a site-specific manner. Analogs of nucleotides and other chemicals were first used to generate localized point mutations. Such chemicals include aminopurine, which induces AT to GC transition, while nitrosoguanidine, bisulfite, and N-hydroxycytidine may induce GC to AT transition. These techniques allow specific mutations to be engineered into a protein; however, they are not flexible with respect to the kinds of mutants generated, nor are they as specific as later methods of site-directed mutagenesis and therefore have some degree of randomness. Current techniques for site-specific mutation commonly involve using mutagenic oligonucleotides in a primer extension reaction with DNA polymerase. This methods allows for point mutation, or deletion or insertion of small stretches of DNA to be introduced at specific sites. Advances in methodology have made such mutagenesis now a relatively simple and efficient process.",
            "score": 35.47403132915497
        },
        {
            "docid": "2421084_4",
            "document": "Isotopomers . In reaction kinetics, a rate effect is sometimes observed between different isotopomers of the same chemical. This kinetic isotope effect can be used to study reaction mechanisms by analyzing how the differently massed atom is involved in the process.",
            "score": 23.594905853271484
        },
        {
            "docid": "14149235_4",
            "document": "Energy profile (chemistry) . In simplest terms, a potential energy surface or PES is a mathematical or graphical representation of the relation between energy of a molecule and its geometry. The methods for describing the potential energy are broken down into a classical mechanics interpretation (molecular mechanics) and a quantum mechanical interpretation. In the quantum mechanical interpretation an exact expression for energy can be obtained for any molecule derived from quantum principles (although an infinite basis set may be required) but ab initio calculations/methods will often use approximations to reduce computational cost. Molecular mechanics is empirically based and potential energy is described as a function of component terms that correspond to individual potential functions such as torsion, stretches,bends, Van der Waals energies,electrostatics and cross terms. Each component potential function is fit to experimental data or properties predicted by ab initio calculations. Molecular mechanics is useful in predicting equilibrium geometries and transition states as well as relative conformational stability. As a reaction occurs the atoms of the molecules involved will generally undergo some change in spatial orientation through internal motion as well as its electronic environment. Distortions in the geometric parameters result in a deviation from the equilibrium geometry (local energy minima). These changes in geometry of a molecule or interactions between molecules are dynamic processes which call for understanding all the forces operating within the system. Since these forces can be mathematically derived as first derivative of potential energy with respect to a displacement, it makes sense to map the potential energy E of the system as a function of geometric parameters q, q, q and so on. The potential energy at given values of the geometric parameters (q, q,\u2026, q) is represented as a hyper-surface (when n >2 or a surface when n \u2264 2). Mathematically, it can be written as-",
            "score": 54.63582456111908
        },
        {
            "docid": "4384325_11",
            "document": "Robert Gilbert (chemist) . Thirty years ago there was neither real predictability nor qualitative understanding of the dominant mechanisms in emulsion polymerisation. Mechanisms had been \u2018proved\u2019 by comparing model predictions with experimental data. The data field was limited and the models had many adjustable parameters, or else fitting parameters had values that were subject to wide uncertainty: it was possible to choose values that could suit any model. It was not uncommon to find two papers claiming that quite different mechanisms were dominant in the same system, a result of not being able to isolate the individual steps. As a result of Gilbert\u2019s work, all individual processes in emulsion polymerisation, one of the commonest ways of making everyday products, are now qualitatively and quantitatively understood. It is now possible to polymerise simple systems and to predict the molecular architecture that will be formed under chosen conditions, while for more complex conditions, trends can be semiquantitatively predicted and understood. The international scientific and technical community in this field now uses the mechanistic knowledge that he obtained as the key to understanding current processes and creating new processes and products. His work has put this industrially important field on a rigorous scientific footing.",
            "score": 55.62959837913513
        }
    ],
    "r": [
        {
            "docid": "5120183_10",
            "document": "Split-step method . where formula_20 describes the wave function at position formula_21 and time formula_3. Note that  The formal solution to this equation is a complex exponential, so we have that Since formula_29 and formula_30 are operators, they do not in general commute. However, the Baker-Hausdorff formula can be applied to show that the error from treating them as if they do will be of order formula_31 if we are taking a small but finite time step formula_32. We therefore can write The part of this equation involving formula_34 can be computed directly using the wave function at time formula_35, but to compute the exponential involving formula_36 we use the fact that in frequency space, the partial derivative operator can be converted into a number by substituting formula_37 for formula_38, where formula_39 is the frequency (or more properly, wave number, as we are dealing with a spatial variable and thus transforming to a space of spatial frequencies\u2014i.e. wave numbers) associated with the Fourier transform of whatever is being operated on. Thus, we take the Fourier transform of  recover the associated wave number, compute the quantity and use it to find the product of the complex exponentials involving formula_42 and formula_36 in frequency space as below: where formula_45 denotes a Fourier transform. We then inverse Fourier transform this expression to find the final result in physical space, yielding the final expression A variation on this method is the symmetrized split-step Fourier method, which takes half a time step using one operator, then takes a full-time step with only the other, and then takes a second half time step again with only the first. This method is an improvement upon the generic split-step Fourier method because its error is of order formula_47 for a time step formula_32. The Fourier transforms of this algorithm can be computed relatively fast using the \"fast Fourier transform (FFT)\". The split-step Fourier method can therefore be much faster than typical finite difference methods.",
            "score": 86.72802734375
        },
        {
            "docid": "356382_8",
            "document": "Gene regulatory network . Genes can be viewed as nodes in the network, with input being proteins such as transcription factors, and outputs being the level of gene expression. The value of the node depends of a function which depends in the value of its regulators in previous time steps (in the Boolean network described below these are Boolean functions, typically AND, OR, and NOT). These functions have been interpreted as performing a kind of information processing within the cell, which determines cellular behavior. The basic drivers within cells are concentrations of some proteins, which determine both spatial (location within the cell or tissue) and temporal (cell cycle or developmental stage) coordinates of the cell, as a kind of \"cellular memory\". The gene networks are only beginning to be understood, and it is a next step for biology to attempt to deduce the functions for each gene \"node\", to help understand the behavior of the system in increasing levels of complexity, from gene to signaling pathway, cell or tissue level.",
            "score": 83.37043762207031
        },
        {
            "docid": "152611_11",
            "document": "Cellular differentiation . Each specialized cell type in an organism expresses a subset of all the genes that constitute the genome of that species. Each cell type is defined by its particular pattern of regulated gene expression. Cell differentiation is thus a transition of a cell from one cell type to another and it involves a switch from one pattern of gene expression to another. Cellular differentiation during development can be understood as the result of a gene regulatory network. A regulatory gene and its cis-regulatory modules are nodes in a gene regulatory network; they receive input and create output elsewhere in the network. The systems biology approach to developmental biology emphasizes the importance of investigating how developmental mechanisms interact to produce predictable patterns (morphogenesis). (However, an alternative view has been proposed recently. Based on stochastic gene expression, cellular differentiation is the result of a Darwinian selective process occurring among cells. In this frame, protein and gene networks are the result of cellular processes and not their cause. See: Cellular Darwinism) A few evolutionarily conserved types of molecular processes are often involved in the cellular mechanisms that control these switches. The major types of molecular processes that control cellular differentiation involve cell signaling. Many of the signal molecules that convey information from cell to cell during the control of cellular differentiation are called growth factors. Although the details of specific signal transduction pathways vary, these pathways often share the following general steps. A ligand produced by one cell binds to a receptor in the extracellular region of another cell, inducing a conformational change in the receptor. The shape of the cytoplasmic domain of the receptor changes, and the receptor acquires enzymatic activity. The receptor then catalyzes reactions that phosphorylate other proteins, activating them. A cascade of phosphorylation reactions eventually activates a dormant transcription factor or cytoskeletal protein, thus contributing to the differentiation process in the target cell. Cells and tissues can vary in competence, their ability to respond to external signals.",
            "score": 82.62000274658203
        },
        {
            "docid": "12385834_10",
            "document": "Prokaryotic large ribosomal subunit . After initiation, elongation, and termination, there is a fourth step of the disassembly of the post-termination complex of ribosome, mRNA, and tRNA, which is a prerequisite for the next round of protein synthesis. The large ribosomal subunit has a role in protein folding both \"in vitro\" and \"in vivo\". The large ribosomal subunit provides a hydrophobic surface for the hydrophobic collapse step of protein folding. The newly synthesized protein needs full access to the large subunit to fold; this process may take a period of time (5 minutes for beta-galactosidase).",
            "score": 81.72947692871094
        },
        {
            "docid": "43684020_5",
            "document": "Mark Maroncelli . Maroncelli\u2019s research seeks to develop a fundamental understanding of the molecular nature of solvation and how it affects chemical reactions taking place in solution. Solvation involves the interactions between dissolved molecules (solutes) and molecules of the solvent. Favorable arrangements of solvent molecules around the solute lower its energy, which leads to dissolution. The interactions involved are typically very rapid, taking place in as short a time as 1 ps (10^-12 s). Because the key steps in most chemical reactions also occur on these fast time scales, time-dependent aspects of solvation partly determine how a solvent influences the rate and course of chemical reactions. Maroncelli uses ultrafast spectroscopic techniques in combination with modern computational-chemistry methods to observe, analyze, and predict the solvation process and its impact on the chemical steps that occur during the particular reaction being investigated.",
            "score": 80.02375030517578
        },
        {
            "docid": "413102_6",
            "document": "Folding@home . Due to the complexity of proteins' conformation or configuration space (the set of possible shapes a protein can take), and limits in computing power, all-atom molecular dynamics simulations have been severely limited in the timescales which they can study. While most proteins typically fold in the order of milliseconds, before 2010 simulations could only reach nanosecond to microsecond timescales. General-purpose supercomputers have been used to simulate protein folding, but such systems are intrinsically costly and typically shared among many research groups. Further, because the computations in kinetic models occur serially, strong scaling of traditional molecular simulations to these architectures is exceptionally difficult. Moreover, as protein folding is a stochastic process and can statistically vary over time, it is challenging computationally to use long simulations for comprehensive views of the folding process. Protein folding does not occur in one step. Instead, proteins spend most of their folding time, nearly 96% in some cases, \"waiting\" in various intermediate conformational states, each a local thermodynamic free energy minimum in the protein's energy landscape. Through a process known as adaptive sampling, these conformations are used by Folding@home as starting points for a set of simulation trajectories. As the simulations discover more conformations, the trajectories are restarted from them, and a Markov state model (MSM) is gradually created from this cyclic process. MSMs are discrete-time master equation models which describe a biomolecule's conformational and energy landscape as a set of distinct structures and the short transitions between them. The adaptive sampling Markov state model method significantly increases the efficiency of simulation as it avoids computation inside the local energy minimum itself, and is amenable to distributed computing (including on GPUGRID) as it allows for the statistical aggregation of short, independent simulation trajectories. The amount of time it takes to construct a Markov state model is inversely proportional to the number of parallel simulations run, i.e., the number of processors available. In other words, it achieves linear parallelization, leading to an approximately four orders of magnitude reduction in overall serial calculation time. A completed MSM may contain tens of thousands of sample states from the protein's phase space (all the conformations a protein can take on) and the transitions between them. The model illustrates folding events and pathways (i.e., routes) and researchers can later use kinetic clustering to view a coarse-grained representation of the otherwise highly detailed model. They can use these MSMs to reveal how proteins misfold and to quantitatively compare simulations with experiments.",
            "score": 79.7857437133789
        },
        {
            "docid": "7808390_5",
            "document": "Watt W. Webb . Professor Webb pioneered the techniques of Fluorescence Correlation Spectroscopy (FCS) in 1972 and Multiphoton microscopy (MPM) in 1990. FCS enables single-molecule detection in solutions at nanomolar concentrations and provides temporal resolution of the dynamic processes of individual molecules signaled by their fluorescence. FCS reveals molecular mobility, conformational fluctuations and chemical reactions in solutions and allows the detection of extremely sparse molecules and particles. In situ measurements of the dynamics of fluorescence flicker by FCS, photobleaching, phototoxicity, and induced fluorescence are being used to discern dynamics of biological processes and molecular mechanisms of disease. Multiphoton excitation in laser scanning fluorescence microscopy provides for high resolution, high signal-to-noise imaging in living cells and deep in turbid tissues in vivo and significantly reduces photodamage and minimizes image degradation due to scattering and autofluorescence. His laboratory at Cornell University continues to extend the frontiers of these technologies, now for example extending MPM and FCS to imaging molecular processes within the cellular nucleus for gene expression in vivo. Recently initiated is the development of technology for introduction of MPM into Medical Endoscopy for in vivo, in situ real time diagnostics.",
            "score": 79.61905670166016
        },
        {
            "docid": "6885770_8",
            "document": "Bootstrapping (statistics) . As an example, assume we are interested in the average (or mean) height of people worldwide. We cannot measure all the people in the global population, so instead we sample only a tiny part of it, and measure that. Assume the sample is of size N; that is, we measure the heights of N individuals. From that single sample, only one estimate of the mean can be obtained. In order to reason about the population, we need some sense of the variability of the mean that we have computed. The simplest bootstrap method involves taking the original data set of N heights, and, using a computer, sampling from it to form a new sample (called a 'resample' or bootstrap sample) that is also of size N. The bootstrap sample is taken from the original by using sampling with replacement (e.g. we might 'resample' 5 times from [1,2,3,4,5] and get [2,5,4,4,1]), so, assuming N is sufficiently large, for all practical purposes there is virtually zero probability that it will be identical to the original \"real\" sample. This process is repeated a large number of times (typically 1,000 or 10,000 times), and for each of these bootstrap samples we compute its mean (each of these are called bootstrap estimates). We now can create a histogram of bootstrap means. This histogram provides an estimate of the shape of the distribution of the sample mean from which we can answer questions about how much the mean varies across samples. (The method here, described for the mean, can be applied to almost any other statistic or estimator.)",
            "score": 77.15596008300781
        },
        {
            "docid": "30876867_20",
            "document": "Molecular cloning . The DNA mixture, previously manipulated in vitro, is moved back into a living cell, referred to as the host organism. The methods used to get DNA into cells are varied, and the name applied to this step in the molecular cloning process will often depend upon the experimental method that is chosen (e.g. transformation, transduction, transfection, electroporation).",
            "score": 76.97193145751953
        },
        {
            "docid": "4384325_7",
            "document": "Robert Gilbert (chemist) . As with unimolecular reactions, the keys to the qualitative and quantitative understanding of the many processes in emulsion polymerisation are the rate coefficients of the individual steps. These steps are initiation (how quickly a growing chain starts), propagation (how quickly individual monomer units are added), radical loss processes (the termination and transfer of radical activity), and particle formation (nucleation). With Prof D Napper, Gilbert applied equations that he had solved in gas-phase chemistry to the area of emulsion polymerisation. This opened the way for him to develop\u2014initially in collaboration with Napper\u2014new theoretical and experimental methods for extracting the rate coefficients of elementary processes. He produced targeted data using these methods, particularly the time evolution of reaction rates and molecular-weight and particle-size distributions. This included novel types of systems, such as \u03b3-radiolysis relaxation, in which events such as radical loss can be separated from radical propagation and growth.",
            "score": 76.62805938720703
        },
        {
            "docid": "4682359_8",
            "document": "Directed evolution . During \"in vivo\" evolution, each cell (usually bacteria or yeast) is transformed with a plasmid containing a different member of the variant library. In this way, only the gene of interest differs between the cells, with all other genes being kept the same. The cells express the protein either in their cytoplasm or surface where its function can be tested. This format has the advantage of selecting for properties in a cellular environment, which is useful when the evolved protein or RNA is to be used in living organisms. When performed without cells, DE involves using \"in vitro\" transcription translation to produce proteins or RNA free in solution or compartmentalised in artificial microdroplets. This method has the benefits of being more versatile in the selection conditions (e.g. temperature, solvent), and can express proteins that would be toxic to cells. Furthermore, \"in vitro\" evolution experiments can generate far larger libraries (up to 10) because the library DNA need not be inserted into cells (often a limiting step).",
            "score": 76.23242950439453
        },
        {
            "docid": "2230_5",
            "document": "Analysis of algorithms . Exact (not asymptotic) measures of efficiency can sometimes be computed but they usually require certain assumptions concerning the particular implementation of the algorithm, called model of computation. A model of computation may be defined in terms of an abstract computer, e.g., Turing machine, and/or by postulating that certain operations are executed in unit time. For example, if the sorted list to which we apply binary search has \"n\" elements, and we can guarantee that each lookup of an element in the list can be done in unit time, then at most log \"n\" + 1 time units are needed to return an answer. Time efficiency estimates depend on what we define to be a step. For the analysis to correspond usefully to the actual execution time, the time required to perform a step must be guaranteed to be bounded above by a constant. One must be careful here; for instance, some analyses count an addition of two numbers as one step. This assumption may not be warranted in certain contexts. For example, if the numbers involved in a computation may be arbitrarily large, the time required by a single addition can no longer be assumed to be constant.",
            "score": 75.94196319580078
        },
        {
            "docid": "44372439_12",
            "document": "Computational methods for free surface flow . This gives the kinematic boundary condition a new form: This equation can be integrated and the fluid velocity at free surface can be obtained either by extrapolation from the interior or by using dynamic boundary condition. For the calculation of flow, FV method is widely used. The steps for a fully conservative FV method of this type are: The main problem with the algorithm in this procedure is that there is only one equation for one cell but large number of grid nodes moving. To avoid instability and wave reflection, the method is modified as follows: From the previous steps, we can calculate the volume of fluid to be flowed in or out of the CV to have mass conservation. To obtain the coordinates of CV vertices at free surface, we have more unknowns and less equations due to single volumetric flow rate for each cell.",
            "score": 75.7872085571289
        },
        {
            "docid": "52085_12",
            "document": "Protein folding . Molecular chaperones are a class of proteins that aid in the correct folding of other proteins \"in vivo\". Chaperones exist in all cellular compartments and interact with the polypeptide chain in order to allow the native three-dimensional conformation of the protein to form; however, chaperones themselves are not included in the final structure of the protein they are assisting in. Chaperones may assist in folding even when the nascent polypeptide is being synthesized by the ribosome. Molecular chaperones operate by binding to stabilize an otherwise unstable structure of a protein in its folding pathway, but chaperones do not contain the necessary information to know the correct native structure of the protein they are aiding; rather, chaperones work by preventing incorrect folding conformations. In this way, chaperones do not actually increase the rate of individual steps involved in the folding pathway toward the native structure; instead, they work by reducing possible unwanted aggregations of the polypeptide chain that might otherwise slow down the search for the proper intermediate and they provide a more efficient pathway for the polypeptide chain to assume the correct conformations. Chaperones are not to be confused with folding catalysts, which actually do catalyze the otherwise slow steps in the folding pathway. Examples of folding catalysts are protein disulfide isomerases and peptidyl-prolyl isomerases that may be involved in formation of disulfide bonds or interconversion between cis and trans stereoisomers, respectively. Chaperones are shown to be critical in the process of protein folding \"in vivo\" because they provide the protein with the aid needed to assume its proper alignments and conformations efficiently enough to become \"biologically relevant\". This means that the polypeptide chain could theoretically fold into its native structure without the aid of chaperones, as demonstrated by protein folding experiments conducted \"in vitro\"; however, this process proves to be too inefficient or too slow to exist in biological systems; therefore, chaperones are necessary for protein folding \"in vivo.\" Along with its role in aiding native structure formation, chaperones are shown to be involved in various roles such as protein transport, degradation, and even allow denatured proteins exposed to certain external denaturant factors an opportunity to refold into their correct native structures.",
            "score": 75.57025909423828
        },
        {
            "docid": "7210212_27",
            "document": "Stochastic simulation . Since the SSA method keeps track of each transition, it would be impractical to implement for certain applications due to high time complexity. Gillespie proposed an approximation procedure, the tau-leaping method which decreases computational time with minimal loss of accuracy. Instead of taking incremental steps in time, keeping track of X(t) at each time step as in the SSA method, the tau-leaping method leaps from one subinterval to the next, approximating how many transitions take place during a given subinterval. It is assumed that the value of the leap, \u03c4, is small enough that there is no significant change in the value of the transition rates along the subinterval [t, t + \u03c4]. This condition is known as the leap condition. The tau-leaping method thus has the advantage of simulating many transitions in one leap while not losing significant accuracy, resulting in a speed up in computational time.",
            "score": 74.98404693603516
        },
        {
            "docid": "27083516_5",
            "document": "Spinal locomotion . The spinal cord processes and interprets proprioception in a manner similar to how our visual system processes information. When we view a painting, the brain interprets the total visual field, as opposed to processing each individual pixel of information independently, and then derives an image. At any instant the spinal cord receives an ensemble of information from all receptors throughout the body that signals a proprioceptive \u201cimage\u201d that represents time and space, and it computes which neurons to excite next based on the most recently perceived \u201cimages.\u201d The importance of the CPG is not simply its ability to generate repetitive cycles, but also to receive, interpret, and predict the appropriate sequences of actions during any part of the step cycle, i.e., state dependence. The peripheral input then provides important information from which the probabilities of a given set of neurons being active at any given time can be finely tuned to a given situation during a specific phase of a step cycle. An excellent example of this is when a mechanical stimulus is applied to the dorsum of the paw of a cat. When the stimulus is applied during the swing phase, the flexor muscles of that limb are excited, and the result is enhanced flexion in order to step over the obstacle that created the stimulus. However, when the same stimulus is applied during stance, the extensors are excited. Thus, the functional connectivity between mechanoreceptors and specific interneuronal populations within the spinal cord varies according to the physiological state. Even the efficacy of the monosynaptic input from muscle spindles to the motor neuron changes readily from one part of the step cycle to another, according to whether a subject is running or walking.",
            "score": 74.5009765625
        },
        {
            "docid": "2399485_3",
            "document": "Second-order cellular automaton . In general, the evolution rule for a second-order automaton may be described as a function that maps the neighborhood of a cell to a permutation on the states of the automaton. In each time step , for each cell of the automaton, this function is applied to the neighborhood of to give a permutation . Then, this permutation is applied to the state of cell at time , and the result is the state of the cell at time  . In this way, the configuration of the automaton at each time step is computed from two previous time steps: the immediately previous step determines the permutations that are applied to the cells, and the time step before that one gives the states on which these permutations operate.",
            "score": 73.9708251953125
        },
        {
            "docid": "44305878_11",
            "document": "Directed differentiation . For basic science, notably developmental biology and cell biology, PSC-derived cells allow to study at the molecular and cellular levels fundamental questions in vitro, that would have been otherwise extremely difficult or impossible to study for technical and ethical reasons in vivo such as embryonic development of human. In particular, differentiating cells are amenable for quantitative and qualitative studies. More complex processes can also be studied in vitro and formation of organoids, including cerebroids, optic cup and kidney have been described.",
            "score": 73.84165954589844
        },
        {
            "docid": "3570572_13",
            "document": "CLIP . All CLIP library generation protocols require moderate quantities of cells or tissue (50\u2013100\u00a0mg), require numerous enzymatic steps, and, for HITS-CLIP, extensive informatic analysis (as recently reviewed). Certain steps are difficult to optimize and frequently have low efficiencies. For example, overdigestion with RNase can decrease the number of identified binding sites. Cross-linking also presents a concern. The optimal cross-linking protocol varies between proteins, and the efficiency is typically between 1-5%. Cross-linking bias has been reported in the literature, but the impact of biases present in CLIP methods remains debatable. Computationally predicted miRNA targets derived from TargetScan are comparable to CLIP in identifying miRNA targets, raising questions as to its utility relative to existing predictions. Because CLIP methods rely on immunoprecipitation, antibody-epitope interactions are a potential obstacle. For instance, cross-linking at the epitope could impede antibody binding. Finally, significant differences have been observed between cross-linking sites \"in vivo\" in living cells and \"in vitro\". Therefore, CLIP results may not necessarily reflect RNA-protein binding site interactions within the cell.",
            "score": 73.82062530517578
        },
        {
            "docid": "73231_19",
            "document": "Weather forecasting . Models are \"initialized\" using this observed data. The irregularly spaced observations are processed by data assimilation and objective analysis methods, which perform quality control and obtain values at locations usable by the model's mathematical algorithms (usually an evenly spaced grid). The data are then used in the model as the starting point for a forecast. Commonly, the set of equations used to predict the known as the physics and dynamics of the atmosphere are called primitive equations. These equations are initialized from the analysis data and rates of change are determined. The rates of change predict the state of the atmosphere a short time into the future. The equations are then applied to this new atmospheric state to find new rates of change, and these new rates of change predict the atmosphere at a yet further time into the future. This \"time stepping\" procedure is continually repeated until the solution reaches the desired forecast time. The length of the time step is related to the distance between the points on the computational grid.",
            "score": 73.45365905761719
        },
        {
            "docid": "11712_6",
            "document": "Facilitated diffusion . In living organisms, the main physical and biochemical processes that are required for survival are regulated by diffusion. Facilitated diffusion is one form of diffusion and it is important in several metabolic processes of living cells. One vital role of facilitated diffusion is that it is the main mechanism behind the binding of Transcription Factors (TFs) to designated target sites on the DNA molecule. The in vitro model, which is a very well known method of facilitated diffusion, that takes place outside of a living cell, explains the 3-dimensional pattern of diffusion in the cytosol and the 1-dimensional diffusion along the DNA contour. After carrying out extensive research on processes occurring out of the cell, this mechanism was generally accepted but there was a need to verify that this mechanism could take place in vivo or inside of living cells. Bauer & Metzler (2013) therefore carried out an experiment using a bacterial genome in which they investigated the average time for TF \u2013 DNA binding to occur. After analyzing the process for the time it takes for TF's to diffuse across the contour and cytoplasm of the bacteria's DNA, it was concluded that in vitro and in vivo are similar in that the association and dissociation rates of TF\u2019s to and from the DNA are similar in both. Also, on the DNA contour, the motion is slower and target sites are easy to localize while in the cytoplasm, the motion is faster but the TF's are not sensitive to their targets and so binding is restricted.",
            "score": 72.64128875732422
        },
        {
            "docid": "1091104_16",
            "document": "Cytochalasin B . \"In vitro\" studies showed that a concentration of 30 \u03bcM of cytochalasin B significantly reduces the relative vicosity of a 20 \u03bcM normal actin filament solution as well as it has decreased in a 20 \u03bcMm gluthathionyl-actin filament solution. \"In vivo\" the effective concentration is even lower. It seemed that a 2 \u03bcM concentration is sufficient in living cells to accomplish a measurable influence on the actin polymerization. The nucleation phase took 2-4 times as long as in the control groups. On elongation, the effects were minimal; on annealing negligible. This might be due to an actual difference in molecular interactions of cytochalasin B during those three steps or simply due to the fact that the lag phase is the rate-determining step in the overall polymerization.",
            "score": 72.44498443603516
        },
        {
            "docid": "640746_14",
            "document": "Secant method . If we compare Newton's method with the secant method, we see that Newton's method converges faster (order 2 against \u03c6 \u2248 1.6). However, Newton's method requires the evaluation of both formula_10 and its derivative formula_27 at every step, while the secant method only requires the evaluation of formula_10. Therefore, the secant method may occasionally be faster in practice. For instance, if we assume that evaluating formula_10 takes as much time as evaluating its derivative and we neglect all other costs, we can do two steps of the secant method (decreasing the logarithm of the error by a factor \u03c6\u00b2 \u2248 2.6) for the same cost as one step of Newton's method (decreasing the logarithm of the error by a factor 2), so the secant method is faster. If, however, we consider parallel processing for the evaluation of the derivative, Newton's method proves its worth, being faster in time, though still spending more steps.",
            "score": 72.38525390625
        },
        {
            "docid": "24044102_6",
            "document": "Cellular model . The eukaryotic cell cycle is very complex and is one of the most studied topics, since its misregulation leads to cancers. It is possibly a good example of a mathematical model as it deals with simple calculus but gives valid results. Two research groups have produced several models of the cell cycle simulating several organisms. They have recently produced a generic eukaryotic cell cycle model which can represent a particular eukaryote depending on the values of the parameters, demonstrating that the idiosyncrasies of the individual cell cycles are due to different protein concentrations and affinities, while the underlying mechanisms are conserved (Csikasz-Nagy et al., 2006). By means of a system of ordinary differential equations these models show the change in time (dynamical system) of the protein inside a single typical cell; this type of model is called a deterministic process (whereas a model describing a statistical distribution of protein concentrations in a population of cells is called a stochastic process). To obtain these equations an iterative series of steps must be done: first the several models and observations are combined to form a consensus diagram and the appropriate kinetic laws are chosen to write the differential equations, such as rate kinetics for stoichiometric reactions, Michaelis-Menten kinetics for enzyme substrate reactions and Goldbeter\u2013Koshland kinetics for ultrasensitive transcription factors, afterwards the parameters of the equations (rate constants, enzyme efficiency coefficients and Michaelis constants) must be fitted to match observations; when they cannot be fitted the kinetic equation is revised and when that is not possible the wiring diagram is modified. The parameters are fitted and validated using observations of both wild type and mutants, such as protein half-life and cell size. In order to fit the parameters the differential equations need to be studied. This can be done either by simulation or by analysis.  In a simulation, given a starting vector (list of the values of the variables), the progression of the system is calculated by solving the equations at each time-frame in small increments. In analysis, the properties of the equations are used to investigate the behavior of the system depending of the values of the parameters and variables. A system of differential equations can be represented as a vector field, where each vector described the change (in concentration of two or more protein) determining where and how fast the trajectory (simulation) is heading. Vector fields can have several special points: a stable point, called a sink, that attracts in all directions (forcing the concentrations to be at a certain value), an unstable point, either a source or a saddle point which repels (forcing the concentrations to change away from a certain value), and a limit cycle, a closed trajectory towards which several trajectories spiral towards (making the concentrations oscillate). A better representation which can handle the large number of variables and parameters is called a bifurcation diagram (bifurcation theory): the presence of these special steady-state points at certain values of a parameter (e.g. mass) is represented by a point and once the parameter passes a certain value, a qualitative change occurs, called a bifurcation, in which the nature of the space changes, with profound consequences for the protein concentrations: the cell cycle has phases (partially corresponding to G1 and G2) in which mass, via a stable point, controls cyclin levels, and phases (S and M phases) in which the concentrations change independently, but once the phase has changed at a bifurcation event (cell cycle checkpoint), the system cannot go back to the previous levels since at the current mass the vector field is profoundly different and the mass cannot be reversed back through the bifurcation event, making a checkpoint irreversible. In particular the S and M checkpoints are regulated by means of special bifurcations called a Hopf bifurcation and an infinite period bifurcation. Cell Collective is a modeling software that enables one to house dynamical biological data, build computational models, stimulate, break and recreate models. The development is led by Tomas Helikar, a researcher within the field of computational biology. It is designed for biologists, students learning about computational biology, teachers focused on teaching life sciences, and researchers within the field of life science. The complexities of math and computer science are built into the backend and one can learn about the methods used for modeling biological species, but complex math equations, algorithms, programming are not required and hence won't impede model building.",
            "score": 71.96351623535156
        },
        {
            "docid": "11210523_2",
            "document": "Gaussian network model . The Gaussian network model (GNM) is a representation of a biological macromolecule as an elastic mass-and-spring network to study, understand, and characterize the mechanical aspects of its long-time large-scale dynamics. The model has a wide range of applications from small proteins such as enzymes composed of a single domain, to large macromolecular assemblies such as a ribosome or a viral capsid. Protein domain dynamics plays key roles in a multitude of molecular recognition and cell signalling processes. Protein domains, connected by intrinsically disordered flexible linker domains, induce long-range allostery via . The resultant dynamic modes cannot be generally predicted from static structures of either the entire protein or individual domains.",
            "score": 71.90872955322266
        },
        {
            "docid": "43966823_37",
            "document": "Multi-state modeling of biomolecules . This method reduces the complexity of the model at the simulation stage, and thereby saves time and computational power. A detailed discussion of the computational cost of population-based versus particle-based methods is summarised in a recent study by Hogg et al. The simulation follows each particle, and at each simulation step, a particle only \"sees\" the reactions (or rules) that apply to it. This depends on the state of the particle and, in some implementation, on the states of its neighbours in a holoenzyme or complex. As the simulation proceeds, the states of particles are updated according to the rules that are fired. Figure 2 illustrates the process of particle-based modeling using a simple system with three molecules of type A and one molecular tetramer of type B, which goes through three simulation steps following a simple set of rules.",
            "score": 71.57035827636719
        },
        {
            "docid": "56717352_11",
            "document": "Time-resolved RNA sequencing . The weaknesses of this method are mainly centered around efficiency. One major difficulty is uptake of 4-sU into cultured cells. If 4-sU is given too early, then it will be incorporated into RNA that was not synthesized before the cell began responding to the experimental conditions. If it is given too late, then early stages of the cellular response are not captured by the experiment. The rate of uptake of 4-sU can be measured, but this requires additional experiments to determine optimal dosage and time. Furthermore, these parameters need to be measured in the specific cell lines of interest, as different cell lines may take up 4-sU more slowly than others. RNA is known to be prone to degradation \"in vitro\". It is common for experimental protocols involving RNA to include a number of steps to reduce chances of Ribonuclease contamination or spontaneous degradation of samples, as RNA quality affects RNA-seq results. Metabolic labeling involves a number of additional steps that must be performed in the laboratory on RNA that is in solution. Since metabolic labeling requires that the RNA be kept unfrozen in liquid solution, some level of spontaneous degradation is unavoidable, although it is usually not to such an extent that results are affected. Of greater risk is the chances of ribonuclease contamination, which would render a sample useless, wasting time and resources. It is important for researchers working with RNA in any capacity to minimize unnecessary handling of RNA due to these risks. One additional drawback of using this method is, given equivalent sample size, more sequencing runs are required compared to a time-series experiment. This is because multiple RNA samples corresponding to the initial time point must be sequenced.",
            "score": 71.53121185302734
        },
        {
            "docid": "5074413_9",
            "document": "Lattice Boltzmann methods . LBM originated from the lattice gas automata (LGA) method, which can be considered as a simplified fictitious molecular dynamics model in which space, time, and particle velocities are all discrete. For example, in the 2-dimensional FHP Model each lattice node is connected to its neighbors by 6 lattice velocities on a triangular lattice; there can be either 0 or 1 particles at a lattice node moving with a given lattice velocity. After a time interval, each particle will move to the neighboring node in its direction; this process is called the propagation or streaming step. When more than one particle arrives at the same node from different directions, they collide and change their velocities according to a set of collision rules. Streaming steps and collision steps alternate. Suitable collision rules should conserve the particle number (mass), momentum, and energy before and after the collision. LGA suffer from several innate defects for use in hydrodynamic simulations: lack of Galilean invariance for fast flows, statistical noise and poor Reynolds number scaling with lattice size. LGA are, however, well suited to simplify and extend the reach of reaction diffusion and molecular dynamics models.",
            "score": 71.49813842773438
        },
        {
            "docid": "52897329_2",
            "document": "Multi-time-step integration . In numerical analysis, multi-time-step integration, also referred to as multiple-step or asynchronous time integration, is a numerical time-integration method that uses different time-steps or time-integrators for different parts of the problem. There are different approaches to multi-time-step integration. They are based on domain decompostition and can be classified into strong (monolithic) or weak (staggered) schemes. Using different time-steps or time-integrators in the context of a weak algorithm is rather straightforward, because the numerical solvers operate independently. However, this is not the case in a strong algorithm. In the past few years a number of research articles have addressed the development of strong multi-time-step algorithms. In either case, strong or weak, the numerical accuracy and stability needs to be carefully studied. Other approaches to multi-time-step integration in the context of operator splitting methods have also been developed; i.e., multi-rate GARK method and multi-step methods for molecular dynamics simulations.",
            "score": 71.41703033447266
        },
        {
            "docid": "43677277_55",
            "document": "Mean field particle methods . We consider a standard Brownian motion formula_94 (a.k.a. Wiener Process) evaluated on a time mesh sequence formula_95 with a given time step formula_96. We choose formula_97 in equation (), we replace formula_98 and \"\u03c3\" by formula_99 and formula_100, and we write formula_101 instead of formula_77 the values of the random states evaluated at the time step formula_103 Recalling that formula_104 are independent centered Gaussian random variables with variance formula_105 the resulting equation can be rewritten in the following form",
            "score": 71.3742904663086
        },
        {
            "docid": "52311684_5",
            "document": "Weighted planar stochastic lattice . The construction process of the WPSL can be described as follows. It starts with a square of unit area which we regard as an initiator. The generator then divides the initiator, in the first step, randomly with uniform probability into four smaller blocks. In the second step and thereafter, the generator is applied to only one of the blocks. The question is: How do we pick that block when there is more than one block? The most generic choice would be to pick preferentially according to their areas so that the higher the area the higher the probability to be picked. For instance, in step one, the generator divides the initiator randomly into four smaller blocks. Let us label their areas starting from the top left corner and moving clockwise as formula_1 and formula_2. But of course the way we label is totally arbitrary and will bear no consequence to the final results of any observable quantities. Note that formula_3 is the area of the formula_4th block which can be well regarded as the probability of picking the formula_4th block. Interestingly, these probabilities are naturally normalized formula_6 since we choose the area of the initiator equal to one. In step two, we pick one of the four blocks preferentially with respect to their areas. Consider that we pick the block formula_7 and apply the generator onto it to divide it randomly into four smaller blocks. Thus the label formula_7 is now redundant and hence we recycle it to label the top left corner while the rest of three new blocks are labelled formula_9 and formula_10 in a clockwise fashion. In general, in the formula_11th step, we pick one out of formula_12 blocks preferentially with respect to area and divide randomly into four blocks. The detailed algorithm can be found in Dayeen and Hassan and Hassan, Hassan, and Pavel.",
            "score": 71.00923919677734
        },
        {
            "docid": "45455383_8",
            "document": "Social media analytics . Developing a data model is a process or method that we use to organize data elements and standardize how the individual data elements relate to each other. This step is important because we want to run a computer program over the data; we need a way to tell the computer which words or themes are important and if certain words relate to the topic we are exploring.",
            "score": 70.54788208007812
        }
    ]
}