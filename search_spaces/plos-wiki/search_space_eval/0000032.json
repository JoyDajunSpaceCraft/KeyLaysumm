{
    "q": [
        {
            "docid": "23634_42",
            "document": "Protein . The total complement of proteins present at a time in a cell or cell type is known as its proteome, and the study of such large-scale data sets defines the field of proteomics, named by analogy to the related field of genomics. Key experimental techniques in proteomics include 2D electrophoresis, which allows the separation of a large number of proteins, mass spectrometry, which allows rapid high-throughput identification of proteins and sequencing of peptides (most often after in-gel digestion), protein microarrays, which allow the detection of the relative levels of a large number of proteins present in a cell, and two-hybrid screening, which allows the systematic exploration of protein\u2013protein interactions. The total complement of biologically possible such interactions is known as the interactome. A systematic attempt to determine the structures of proteins representing every possible fold is known as structural genomics.",
            "score": 71.05144095420837
        },
        {
            "docid": "28722065_2",
            "document": "Proteogenomics . Proteogenomics is a field of biological research that utilizes a combination of proteomics, genomics, and transcriptomics to aid in the discovery and identification of peptides. Protegenomics is used to identify new peptides by comparing MS/MS spectra against a protein database that has been derived from genomic and trancriptomic information. Proteogenomics often refers to studies that use proteomic information, often derived from mass spectrometry, to improve gene annotations. Genomics deals with the genetic code of entire organisms, while transcriptomics deals with the study of RNA sequencing and transcripts. proteomics utilizes tandem mass spectrometry and liquid chromatography to identify and study the functions of proteins. Proteomics is being utilized to discover all the proteins expressed within an organism, known as its proteome. The issue with proteomics is that it relies on the assumption that current gene models are correct and that the correct protein sequences can be found using a reference protein sequence database; however, this is not always the case as some peptides cannot be located in the database. In addition, novel protein sequences can occur through mutations. these issues can be fixed with the use of proteomic, genomic, and trancriptomic data. The utilization of both proteomics and genomics led to proteogeonmics which became its own field in 2004.",
            "score": 73.38236081600189
        },
        {
            "docid": "1686272_6",
            "document": "Chemical biology . Protein levels, modifications, locations, and interactions are complex and dynamic properties. With this complexity in mind, experiments need to be carefully designed to answer specific questions especially in the face of the massive amounts of data that are generated by these analyses. The most valuable information comes from proteins that are expressed differently in a system being studied. These proteins can be compared relative to each other using quantitative proteomics, which allows a protein to be labeled with a mass tag. Proteomic technologies must be sensitive and robust, it is for these reasons, the mass spectrometer has been the workhorse of protein analysis. The high precision of mass spectrometry can distinguish between closely related species and species of interest can be isolated and fragmented within the instrument. Its applications to protein analysis was only possible in the late 1980s with the development of protein and peptide ionization with minimal fragmentation. These breakthroughs were ESI and MALDI. Mass spectrometry technologies are modular and can be chosen or optimized to the system of interest.",
            "score": 75.92266237735748
        },
        {
            "docid": "13250438_21",
            "document": "Protein mass spectrometry . Several recent methods allow for the quantitation of proteins by mass spectrometry (quantitative proteomics). Typically, stable (e.g. non-radioactive) heavier isotopes of carbon (C) or nitrogen (N) are incorporated into one sample while the other one is labeled with corresponding light isotopes (e.g. C and N). The two samples are mixed before the analysis. Peptides derived from the different samples can be distinguished due to their mass difference. The ratio of their peak intensities corresponds to the relative abundance ratio of the peptides (and proteins). The most popular methods for isotope labeling are SILAC (stable isotope labeling by amino acids in cell culture), trypsin-catalyzed O labeling, ICAT (isotope coded affinity tagging), iTRAQ (isobaric tags for relative and absolute quantitation).  \u201cSemi-quantitative\u201d mass spectrometry can be performed without labeling of samples. Typically, this is done with MALDI analysis (in linear mode). The peak intensity, or the peak area, from individual molecules (typically proteins) is here correlated to the amount of protein in the sample. However, the individual signal depends on the primary structure of the protein, on the complexity of the sample, and on the settings of the instrument. Other types of \"label-free\" quantitative mass spectrometry, uses the spectral counts (or peptide counts) of digested proteins as a means for determining relative protein amounts.",
            "score": 68.16069889068604
        },
        {
            "docid": "27360417_7",
            "document": "Edward Marcotte . In the field of proteomics, Marcotte's contributions include developing early versions of the human protein interaction network and mapping of >7,000 human protein interactions. Marcotte and colleagues developed the spotted cell microarray technique for high-throughput measurement of protein expression, subcellular location, and function, developed algorithms for analyzing mass spectrometry data, started an open access database for mass spectrometry proteomics data, and developed the APEX method for absolute protein quantification on a proteome-wide scale. Using APEX, Marcotte and colleagues demonstrated that protein abundance in a lower eukaryote is predominantly determined by mRNA levels, while human protein abundances are determined roughly equally by transcriptional and post-transcriptional regulation.",
            "score": 69.70505952835083
        },
        {
            "docid": "38991948_14",
            "document": "Single-cell analysis . In mass spectroscopy based proteomics there are three major steps needed for peptide identification: sample preparation, separation of peptides, and identification of peptides. Several groups have focused on oocytes or very early cleavage-stage cells since these cells are unusually large and provide enough material for analysis. Another approach, single cell proteomics by mass spectrometry (SCoPE-MS) has quantified thousands of proteins in mammalian cells with typical cell sizes (diameter of 10-15 \u03bcm) by combining carrier-cells and labeling. Multiple methods exist to isolate the peptides for analysis. These include using filter aided sample preparation, the use of magnetic beads, or using a series of reagents and centrifuging steps. \u00a0The separation of differently sized proteins can be accomplished by using capillary electrophoresis (CE) or liquid chromatograph (LC) (using liquid chromatography with mass spectroscopy is also known as LC-MS). This step gives order to the peptides before quantification using tandem mass-spectroscopy (MS/MS). The major difference between quantification methods is some use labels on the peptides such as tandem mass tags (TMT) or dimethyl labels which are used to identify which cell a certain protein came from (proteins coming from each cell have a different label) while others use not labels (quantify cells individually). The mass spectroscopy data is then analyzed by running data through databases that convert the information about peptides identified to quantification of protein levels. These methods are very similar to those used to quantify the proteome of bulk cells, with modifications to accommodate the very small sample volume. Improvements in sample preparation, mass-spec methods and data analysis can increase the sensitivity and throughput by orders of magnitude.",
            "score": 66.09198105335236
        },
        {
            "docid": "13706553_25",
            "document": "Quantitative proteomics . Spectral counting involves counting the spectra of an identified protein and then standardizing using some form of normalization. Typically this is done with an abundant peptide mass selection (MS) that is then fragmented and then MS/MS spectra are counted. Multiple samplings of the protein peak is required for accurate estimation of the protein abundance because of the complex physiochemical nature of peptides. Thus, optimization for MS/MS experiments is a constant concern. One alternative to get around this problems is use a data independent technique that cycles between high and low collision energies. Thus a large survey of all possible precursor and product ions is collected. This is limited, however, by the mass spectrometry software's ability to recognize and match peptide patterns of associations between the precursor and product ions.",
            "score": 49.05384039878845
        },
        {
            "docid": "1659679_3",
            "document": "Protein microarray . Protein microarrays were developed due to the limitations of using DNA microarrays for determining gene expression levels in proteomics. The quantity of mRNA in the cell often doesn't reflect the expression levels of the proteins they correspond to. Since it is usually the protein, rather than the mRNA, that has the functional role in cell response, a novel approach was needed. Additionally post-translational modifications, which are often critical for determining protein function, are not visible on DNA microarrays. Protein microarrays replace traditional proteomics techniques such as 2D gel electrophoresis or chromatography, which were time consuming, labor-intensive and ill-suited for the analysis of low abundant proteins.",
            "score": 58.21772813796997
        },
        {
            "docid": "4214_31",
            "document": "Bioinformatics . Protein microarrays and high throughput (HT) mass spectrometry (MS) can provide a snapshot of the proteins present in a biological sample. Bioinformatics is very much involved in making sense of protein microarray and HT MS data; the former approach faces similar problems as with microarrays targeted at mRNA, the latter involves the problem of matching large amounts of mass data against predicted masses from protein sequence databases, and the complicated statistical analysis of samples where multiple, but incomplete peptides from each protein are detected. Cellular protein localization in a tissue context can be achieved through affinity proteomics displayed as spatial data based on immunohistochemistry and tissue microarrays.",
            "score": 81.73355460166931
        },
        {
            "docid": "478128_8",
            "document": "Protein isoform . Because splicing is a process that occurs between transcription and translation, its primary effects have mainly been studied through genomics techniques\u2014for example, microarray analyses and RNA sequencing have been used to identify alternatively spliced transcripts and measure their abundances. Transcript abundance is often used as a proxy for the abundance of protein isoforms, though proteomics experiments using gel electrophoresis and mass spectrometry have demonstrated that the correlation between transcript and protein counts is often low, and that one protein isoform is usually dominant. One 2015 study states that the cause of this discrepancy likely occurs after translation, though the mechanism is essentially unknown. Consequently, although alternative splicing has been implicated as an important link between variation and disease, there is no conclusive evidence that it acts primarily by producing novel protein isoforms.",
            "score": 60.94281542301178
        },
        {
            "docid": "55172_52",
            "document": "Proteomics . One example of the use of bioinformatics and the use of computational methods is the study of protein biomarkers. Computational predictive models have shown that extensive and diverse feto-maternal protein trafficking occurs during pregnancy and can be readily detected non-invasively in maternal whole blood. This computational approach circumvented a major limitation, the abundance of maternal proteins interfering with the detection of fetal proteins, to fetal proteomic analysis of maternal blood. Computational models can use fetal gene transcripts previously identified in maternal whole blood to create a comprehensive proteomic network of the term neonate. Such work shows that the fetal proteins detected in pregnant woman\u2019s blood originate from a diverse group of tissues and organs from the developing fetus. The proteomic networks contain many biomarkers that are proxies for development and illustrate the potential clinical application of this technology as a way to monitor normal and abnormal fetal development.",
            "score": 88.73115634918213
        },
        {
            "docid": "13672545_11",
            "document": "Shotgun proteomics . One example of this is a study by Washburn, Wolters, & Yates in which they used shotgun proteomics on the proteome of a Saccharomyces cerevisiae strain grown to mid-log phase. They were able to detect and identify 1,484 proteins as well as identify proteins rarely seen in proteome analysis, including low-abundance proteins like transcription factors and protein kinases. They were also able to identify 131 proteins with three or more predicted transmembrane domains.",
            "score": 75.47026562690735
        },
        {
            "docid": "1659679_25",
            "document": "Protein microarray . Challenges include: 1) finding a surface and a method of attachment that allows the proteins to maintain their secondary or tertiary structure and thus their biological activity and their interactions with other molecules, 2) producing an array with a long shelf life so that the proteins on the chip do not denature over a short time, 3) identifying and isolating antibodies or other capture molecules against every protein in the human genome, 4) quantifying the levels of bound protein while assuring sensitivity and avoiding background noise, 5) extracting the detected protein from the chip in order to further analyze it, 6) reducing non-specific binding by the capture agents, 7) the capacity of the chip must be sufficient to allow as complete a representation of the proteome to be visualized as possible; abundant proteins overwhelm the detection of less abundant proteins such as signaling molecules and receptors, which are generally of more therapeutic interest.",
            "score": 69.38370656967163
        },
        {
            "docid": "30999216_20",
            "document": "Protein moonlighting . Mass spectrometry may be used to detect proteins based on their mass-to-charge ratio. Because of alternative splicing and posttranslational modification, identification of proteins based on the mass of the parent ion alone is very difficult. However tandem mass spectrometry in which each of the parent peaks is in turn fragmented can be used to unambiguously identify proteins. Hence tandem mass spectrometry is one of the tools used in proteomics to identify the presence of proteins in different cell types or subcellular locations. While the presence of a moonlighting protein in an unexpected location may complicate routine analyses, at the same time, the detection of a protein in unexpected multiprotein complexes or locations suggests that protein may have a moonlighting function. Furthermore, mass spectrometry may be used to determine if a protein has high expression levels that do not correlate to the enzyme's measured metabolic activity. These expression levels may signify that the protein is performing a different function than previously known.",
            "score": 84.9368509054184
        },
        {
            "docid": "13688008_3",
            "document": "Bottom-up proteomics . In bottom-up proteomics, the proteins may first be purified by a method such as gel electrophoresis resulting in one or a few proteins in each proteolytic digest. Alternatively, the crude protein extract is digested directly, followed by one or more dimensions of separation of the peptides by liquid chromatography coupled to mass spectrometry, a technique known as shotgun proteomics. By comparing the masses of the proteolytic peptides or their tandem mass spectra with those predicted from a sequence database or annotated peptide spectral in a peptide spectral library, peptides can be identified and multiple peptide identifications assembled into a protein identification.",
            "score": 67.40539765357971
        },
        {
            "docid": "45663583_4",
            "document": "Human interactome . Protein\u2013protein interactions are only the raw material for networks. To form useful interactome databases and create integrated networks, other types of data that can be combined with protein\u2013protein interactions include information on gene expression and co-expression, cellular co-localization of proteins (based on microscopy), genetic information, metabolic and signalling pathways, and more. The end goal of unravelling human protein interactomes is ultimately to understand mechanisms of disease and uncover previously unknown disease genes. It has been found that proteins with a high number of interactions (outward edges) are significantly more likely to be hubs in modules that correlate with disease, probably because proteins with more interactions are involved in more biological functions. By mapping disease alterations to the human interactome, we can gain a much better understanding of the pathways and biological processes of disease.",
            "score": 73.90832030773163
        },
        {
            "docid": "49503_20",
            "document": "Inductively coupled plasma mass spectrometry . In 2007, a new type of protein tagging reagents called metal-coded affinity tags (MeCAT) were introduced to label proteins quantitatively with metals, especially lanthanides. The MeCAT labelling allows relative and absolute quantification of all kind of proteins or other biomolecules like peptides. MeCAT comprises a site-specific biomolecule tagging group with at least a strong chelate group which binds metals. The MeCAT labelled proteins can be accurately quantified by ICP-MS down to low attomol amount of analyte which is at least 2\u20133 orders of magnitude more sensitive than other mass spectrometry based quantification methods. By introducing several MeCAT labels to a biomolecule and further optimization of LC-ICP-MS detection limits in the zeptomol range are within the realm of possibility. By using different lanthanides MeCAT multiplexing can be used for pharmacokinetics of proteins and peptides or the analysis of the differential expression of proteins (proteomics) e.g. in biological fluids. Breakable PAGE SDS-PAGE (DPAGE, dissolvable PAGE), two-dimensional gel electrophoresis or chromatography is used for separation of MeCAT labelled proteins. Flow-injection ICP-MS analysis of protein bands or spots from DPAGE SDS-PAGE gels can be easily performed by dissolving the DPAGE gel after electrophoresis and staining of the gel. MeCAT labelled proteins are identified and relatively quantified on peptide level by MALDI-MS or ESI-MS.",
            "score": 69.39078521728516
        },
        {
            "docid": "7324297_4",
            "document": "Genome-based peptide fingerprint scanning . In 2012 research was published where genes and proteins were found in a model organism that could not have been found without GFS because they had not been previously annotated. The planarian Schmidtea mediterranea has been used in research for over 100 years. This planarian is capable of regenerating missing body parts and is therefore emerging as potential model organism for stem cell research. Planarians are covered in mucus which aids in locomotion, in protecting them from predation, and in helping their immune system. The genome of \"Schmidtea mediterranea\" is sequenced but mostly un-annotated making it a prime candidate for genome-based peptide fingerprint scanning. When the proteins were analyzed with GFS 1,604 proteins were identified. These proteins had mostly not been annotated before they were found with GFS They were also able to find the mucous subproteome (all the genes associated with mucus production). They found that this proteome was conserved in the sister species \"Schmidtea mansoni\". The mucous subproteome is so conserved that 119 orthologs of planarians are found in humans. Due to the similarity in these genes the planarian can now be used as a model to study mucous protein function in humans. This is relevant for infections and diseases related to mucous aberrancies such as cystic fibrosis, asthma, and other lung diseases. These genes could not have been found without GFS because they had not been previously annotated.",
            "score": 63.8309131860733
        },
        {
            "docid": "7324297_3",
            "document": "Genome-based peptide fingerprint scanning . GFS was designed by Michael C. Giddings (University of North Carolina, Chapel Hill) et al., and released in 2003. Giddings expanded the algorithms for GFS from earlier ideas. Two papers were published in 1993 explaining the techniques used to identify proteins in sequence databases. These methods determined the mass of peptides using mass spectrometry, and then used the mass to search protein databases to identify the proteins In 1999 a more complex program was released called Mascot that integrated three types of protein/database searches: peptide molecular weights, tandem mass spectrometry from one or more peptide, and combination mass data with amino acid sequence. The fallback with this widely used program is that it is unable to detect alternative splice sites that are not currently annotated, and it not usually able to find proteins that have not been annotated. Giddings built upon these sources to create GFS which would compare peptide mass data to entire genomes to identify the proteins. Giddings system is able to find new annotations of genes that have not been found, such as undocumented genes and undocumented alternative splice sites.",
            "score": 64.50608682632446
        },
        {
            "docid": "55172_57",
            "document": "Proteomics . The depth of the plasma proteome encompassing a dynamic range of more than 10 between the highest abundant protein (albumin) and the lowest (some cytokines) and is thought to be one of the main challenges for proteomics. Temporal and spatial dynamics further complicate the study of human plasma proteome. The turnover of some proteins is quite faster than others and the protein content of an artery may substantially vary from that of a vein. All these differences make even the simplest proteomic task of cataloging the proteome seem out of reach. To tackle this problem, priorities need to be established. Capturing the most meaningful subset of proteins among the entire proteome to generate a diagnostic tool is one such priority. Secondly, since cancer is associated with enhanced glycosylation of proteins, methods that focus on this part of proteins will also be useful. Again: multiparameter analysis best reveals a pathological state. As these technologies improve, the disease profiles should be continually related to respective gene expression changes. Due to the above-mentioned problems plasma proteomics remained challenging. However, technological advancements and continuous developments seem to result in a revival of plasma proteomics as it was shown recently by a technology called plasma proteome profiling. Due to such technologies researchers were able to investigate inflammation processes in mice, the heretability of plasma proteomes as well as to show the effect of such a common life style change like weight loss on the plasma proteome.",
            "score": 67.0877114534378
        },
        {
            "docid": "55172_55",
            "document": "Proteomics . Advances in quantitative proteomics would clearly enable more in-depth analysis of cellular systems. Biological systems are subject to a variety of perturbations (cell cycle, cellular differentiation, carcinogenesis, environment (biophysical), etc.). Transcriptional and translational responses to these perturbations results in functional changes to the proteome implicated in response to the stimulus. Therefore, describing and quantifying proteome-wide changes in protein abundance is crucial towards understanding biological phenomenon more holistically, on the level of the entire system. In this way, proteomics can be seen as complementary to genomics, transcriptomics, epigenomics, metabolomics, and other -omics approaches in integrative analyses attempting to define biological phenotypes more comprehensively. As an example, \"The Cancer Proteome Atlas\" provides quantitative protein expression data for ~200 proteins in over 4,000 tumor samples with matched transcriptomic and genomic data from The Cancer Genome Atlas. Similar datasets in other cell types, tissue types, and species, particularly using deep shotgun mass spectrometry, will be an immensely important resource for research in fields like cancer biology, developmental and stem cell biology, medicine, and evolutionary biology.",
            "score": 66.96573162078857
        },
        {
            "docid": "55172_42",
            "document": "Proteomics . Expression proteomics includes the analysis of protein expression at larger scale. It helps identify main proteins in a particular sample, and those proteins differentially expressed in related samples\u2014such as diseased vs. healthy tissue. If a protein is found only in a diseased sample then it can be a useful drug target or diagnostic marker. Proteins with same or similar expression profiles may also be functionally related. There are technologies such as 2D-PAGE and mass spectrometry that are used in expression proteomics.",
            "score": 68.14859414100647
        },
        {
            "docid": "21723923_15",
            "document": "Reverse phase protein lysate microarray . The greatest strength of RPMAs is that they allow for high throughput, multiplexed, ultra-sensitive detection of proteins from extremely small numbers of input material, a feat which cannot be done by conventional western blotting or ELISA. The small spot size on the microarray, ranging in diameter from 85 to 200 micrometres, enables the analysis of thousands of samples with the same antibody in one experiment. RPMAs have increased sensitivity and are capable of detecting proteins in the picogram range. Some researchers have even reported detection of proteins in the attogram range. This is a significant improvement over protein detection by ELISA, which requires microgram amounts of protein (6). The increase in sensitivity of RPMAs is due to the miniature format of the array, which leads to an increase in the signal density (signal intensity/area) coupled with tyramide deposition-enabled enhancement. The high sensitivity of RPMAs allows for the detection of low abundance proteins or biomarkers such as phosphorylated signaling proteins from very small amounts of starting material such as biopsy samples, which are often contaminated with normal tissue. Using laser capture microdissection lysates can be analyzed from as few as 10 cells, with each spot containing less than a hundredth of a cell equivalent of protein.  A great improvement of RPMAs over traditional forward phase protein arrays is a reduction in the number of antibodies needed to detect a protein. Forward phase protein arrays typically use a sandwich method to capture and detect the desired protein. This implies that there must be two epitopes on the protein (one to capture the protein and one to detect the protein) for which specific antibodies are available. Other forward phase protein microarrays directly label the samples, however there is often variability in the labeling efficiency for different protein, and often the labeling destroys the epitope to which the antibody binds. This problem is overcome by RPMAs as sample need not be labeled directly.",
            "score": 72.8177797794342
        },
        {
            "docid": "4007073_6",
            "document": "Gene expression profiling . The human genome contains on the order of 25,000 genes which work in concert to produce on the order of 1,000,000 distinct proteins. This is due to alternative splicing, and also because cells make important changes to proteins through posttranslational modification after they first construct them, so a given gene serves as the basis for many possible versions of a particular protein. In any case, a single mass spectrometry experiment can identify about 2,000 proteins or 0.2% of the total. While knowledge of the precise proteins a cell makes (proteomics) is more relevant than knowing how much messenger RNA is made from each gene, gene expression profiling provides the most global picture possible in a single experiment. However, proteomics methodology is improving. In other species, such as yeast, it is possible to identify over 4,000 proteins in just over one hour.",
            "score": 67.91240286827087
        },
        {
            "docid": "8715575_16",
            "document": "Phosphoproteomics . While phosphoproteomics has greatly expanded knowledge about the numbers and types of phosphoproteins, along with their role in signaling networks, there are still several limitations to these techniques. To begin with, isolation methods such as anti-phosphotyrosine antibodies do not distinguish between isolating tyrosine-phosphorylated proteins and proteins associated with tyrosine-phosphorylated proteins. Therefore, even though phosphorylation dependent protein-protein interactions are very important, it is important to remember that a protein detected by this method is not necessarily a direct substrate of any tyrosine kinase. Only by digesting the samples before immunoprecipitation can isolation of only phosphoproteins and temporal profiles of individual phosphorylation sites be produced. Another limitation is that some relevant proteins will likely be missed since no extraction condition is all encompassing. It is possible that proteins with low stoichiometry of phosphorylation, in very low abundance, or phosphorylated as a target for rapid degradation will be lost. Bioinformatics analyses of low-throughput phosphorylation data together with high-throughput phosphoproteomics data (based mostly on MS/MS) estimate that current high-throughput protocols, after several repetitions are capable of capturing 70% to 95% of total phosphoproteins, but only 40% to 60% of total phosphorylation sites.",
            "score": 64.22396743297577
        },
        {
            "docid": "13250438_11",
            "document": "Protein mass spectrometry . In the second approach, referred to as the \"bottom-up\" MS, proteins are enzymatically digested into smaller peptides using a protease such as trypsin. Subsequently, these peptides are introduced into the mass spectrometer and identified by peptide mass fingerprinting or tandem mass spectrometry. Hence, this approach uses identification at the peptide level to infer the existence of proteins pieced back together with \"de novo\" repeat detection. The smaller and more uniform fragments are easier to analyze than intact proteins and can be also determined with high accuracy, this \"bottom-up\" approach is therefore the preferred method of studies in proteomics. A further approach that is beginning to be useful is the intermediate \"middle-down\" approach in which proteolytic peptides larger than the typical tryptic peptides are analyzed.",
            "score": 76.43743562698364
        },
        {
            "docid": "47714477_16",
            "document": "Deinococcus deserti . A total of 341 protein N termini were confidently identified in the \"D. deserti\" TMPP-labeled proteome. Among these, 63 were not correctly annotated in the first \"D. deserti\" genome annotation and should be modified accordingly. There has been a comparison between the gene sequences of the three sequenced \"Deinococcus\" genomes. It is proposed that the N termini of 37 and 100 additional proteins from \"D. geothermalis\" and \"D. radiodurans\" genomes, respectively, should be reannotated. When considering the manually validated TMPP-modified peptides, 664 unique signatures for N termini were identified with 398 tryptic and 266 chymotryptic sequences. These two digestions were thus found to be complementary. The N termini data set corresponds to 10% of the theoretical proteome. A significant number of erroneous annotations have probably still to be corrected.",
            "score": 46.59784460067749
        },
        {
            "docid": "38991948_15",
            "document": "Single-cell analysis . The purpose of studying the proteome is to better understand the activity of cells at the single cells level. Since proteins are responsible for determining how the cell acts, understanding the proteome of single cell gives the best understanding of how a cell operates, and how gene expression changes in a cell due to different environmental stimuli. Although transcriptomics has the same purpose as proteomics it is not as accurate at determining gene expression in cells as it does not take into account post-transcriptional regulation. Transcriptomics is still important as studying the difference between RNA levels and protein levels could give insight on which genes are post-transcriptionally regulated.",
            "score": 52.28546667098999
        },
        {
            "docid": "19444228_5",
            "document": "Label-free quantification . Intact protein expression spectrometry (IPEx) is a label-free quantification approach in mass spectrometry under development by the analytical chemistry group at the United States Food and Drug Administration Center for Food Safety and Applied Nutrition and elsewhere. Intact proteins are analyzed by an LCMS instrument, usually a quadrupole time-of-flight in profile mode, and the full protein profile is determined and quantified using data reduction software. Early results are very encouraging. In one study, two groups of treatment replicates from mammalian samples (different organisms with similar treatment histories, but not technical replicates) show dozens of low CV protein biomarkers, suggesting that IPEx is a viable technology for studying protein expression.",
            "score": 59.99550151824951
        },
        {
            "docid": "21953783_9",
            "document": "Selected reaction monitoring . SRM has been used to identify the proteins encoded by wild-type and mutant genes (mutant proteins) and quantify their absolute copy numbers in tumors and biological fluids, thus answering the basic questions about the absolute copy number of proteins in a single cell, which will be essential in digital modelling of mammalian cells and human body, and the relative levels of genetically abnormal proteins in tumors, and proving useful for diagnostic applications. SRM has also been used as a method of triggering full product ion scans of peptides to either a) confirm the specificity of the SRM transition, or b) detect specific post-translational modifications which are below the limit of detection of standard MS analyses. In 2017, SRM has been developed to be a highly sensitive and reproducible mass spectrometry-based protein targeted detection platform (entitled \"SAFE-SRM\"), and it has been demonstrated that the SRM-based new pipeline has major advantages in clinical proteomics applications over traditional SRM pipelines, and it has demonstrated a dramatically improved diagnostic performance over that from antibody-based protein biomarker diagnostic methods, such as ELISA.",
            "score": 73.11451077461243
        },
        {
            "docid": "55172_44",
            "document": "Proteomics . Understanding the proteome, the structure and function of each protein and the complexities of protein\u2013protein interactions is critical for developing the most effective diagnostic techniques and disease treatments in the future. For example, proteomics is highly useful in identification of candidate biomarkers (proteins in body fluids that are of value for diagnosis), identification of the bacterial antigens that are targeted by the immune response, and identification of possible immunohistochemistry markers of infectious or neoplastic diseases.",
            "score": 55.86268734931946
        },
        {
            "docid": "5655436_8",
            "document": "Antibody microarray . In the last ten years, the sensitivity of the method was improved by an optimization of the surface chemistry as well as dedicated protocols for their chemical labeling. Currently, the sensitivity of antibody arrays is comparable to that of ELISA and antibody arrays are regularly used for profiling experiments on tissue samples, plasma or serum samples and many other sample types. One main focus in antibody array based profiling studies is biomarker discovery, specifically for cancer. For cancer-related research, the development and application of an antibody array comprising 810 different cancer-related antibodies was reported in 2010. Also in 2010, an antibody array comprising 507 cytokines, chemokines, adipokines, growth factors, angiogenic factors, proteases, soluble receptors, soluble adhesion molecules, and other proteins was used to screen the serum of ovarian cancer patients and healthy individuals and found a significant difference in protein expression between normal and cancer samples. More recently, antibody arrays have helped determine specific allergy-related serum proteins whose levels are associated with glioma and can reduce the risk years before diagnosis. Protein profiling with antibody arrays have also proven successful in areas other than cancer research, specifically in neurological diseases such as Alzheimer\u2019s. A number of studies have attempted to identify biomarker panels that can distinguish Alzheimer\u2019s patients, and many have used antibody arrays in this process. Jaeger and colleagues measured nearly 600 circulatory proteins to discover biological pathways and networks affected in Alzheimer\u2019s and explored the positive and negative relationships of the levels of those individual proteins and networks with the cognitive performance of Alzheimer\u2019s patients. Currently the largest commercially available sandwich-based antibody array detects 1000 different proteins. In addition, antibody microarray based protein profiling services are available analyzing protein abundance and protein phosphorylation or ubiquitinylation status of 1030 proteins in parallel.",
            "score": 66.00406789779663
        }
    ],
    "r": [
        {
            "docid": "61836_9",
            "document": "Treponema pallidum . No vaccine for syphilis is available as of 2017. The outer membrane of \"T. pallidum\" has too few surface proteins for an antibody to be effective. Efforts to develop a safe and effective syphilis vaccine have been hindered by uncertainty about the relative importance of humoral and cellular mechanisms to protective immunity, and because \"T. pallidum\" outer membrane proteins have not been unambiguously identified. In contrast, some of the known antigens are intracellular and antibody against them are ineffective to clear the infection.",
            "score": 101.3451919555664
        },
        {
            "docid": "31312165_3",
            "document": "LocDB . Proteins are the fundamental functional components of cells. They are responsible for transforming genetic information into physical reality. These macromolecules mediate gene regulation, enzymatic catalysis, cellular metabolism, DNA replication, and transport of nutrients, recognition, and transmission of signals. The interpretation of this wealth of data to elucidate protein function in post-genomic era is a fundamental challenge. To date, even for the most well-studied organisms such as yeast, about one-fourth of the proteins remain uncharacterized. A major obstacle in experimentally determining protein function is that the studies require enormous resources. Hence, the gap between the amount of sequences deposited in databases and the experimental characterization of the corresponding proteins is ever-growing. Bioinformatics plays a central role in bridging this sequence-function gap through the development of tools for faster and more effective prediction of protein function. This repository effectively fills the gap between experimental annotations and predictions and provides a bigger and more reliable dataset for the testing of new prediction methods.",
            "score": 90.15440368652344
        },
        {
            "docid": "55172_52",
            "document": "Proteomics . One example of the use of bioinformatics and the use of computational methods is the study of protein biomarkers. Computational predictive models have shown that extensive and diverse feto-maternal protein trafficking occurs during pregnancy and can be readily detected non-invasively in maternal whole blood. This computational approach circumvented a major limitation, the abundance of maternal proteins interfering with the detection of fetal proteins, to fetal proteomic analysis of maternal blood. Computational models can use fetal gene transcripts previously identified in maternal whole blood to create a comprehensive proteomic network of the term neonate. Such work shows that the fetal proteins detected in pregnant woman\u2019s blood originate from a diverse group of tissues and organs from the developing fetus. The proteomic networks contain many biomarkers that are proxies for development and illustrate the potential clinical application of this technology as a way to monitor normal and abnormal fetal development.",
            "score": 88.73116302490234
        },
        {
            "docid": "647286_22",
            "document": "Prenatal testing . Measurement of fetal proteins in maternal serum is a part of standard prenatal screening for fetal aneuploidy and neural tube defects. Computational predictive model shows that extensive and diverse feto-maternal protein trafficking occurs during pregnancy and can be readily detected non-invasively in maternal whole blood. This computational approach circumvented a major limitation, the abundance of maternal proteins interfering with the detection of fetal proteins, to fetal proteomic analysis of maternal blood. Entering fetal gene transcripts previously identified in maternal whole blood into a computational predictive model helped develop a comprehensive proteomic network of the term neonate. It also shows that the fetal proteins detected in pregnant woman\u2019s blood originate from a diverse group of tissues and organs from the developing fetus. Development proteomic networks dominate the functional characterization of the predicted proteins, illustrating the potential clinical application of this technology as a way to monitor normal and abnormal fetal development.",
            "score": 87.3902816772461
        },
        {
            "docid": "35881441_3",
            "document": "HH-suite . Proteins are central players in all of life's processes. To understand how life in cells is organised, we have to understand what each of the proteins involved in these molecular processes does. This is particularly important in order to understand the origin of diseases. But for a large fraction of the approximately 20 000 human proteins the structures and functions remain unknown. Many proteins have been investigated in model organisms such as many bacteria, baker's yeast, fruit flies, zebra fish or mice, for which experiments can be often done more easily than with human cells. To predict the function, structure, or other properties of a protein for which only its sequence of amino acids is known, the protein sequence is compared to the sequences of other proteins in public databases. If a protein with sufficiently similar sequence is found, the two proteins are likely to be evolutionarily related (\"homologous\"). In that case, they are likely to share similar structures and functions. Therefore, if a protein with a sufficiently similar sequence and with known functions and/or structure can be found by the sequence search, the unknown protein's functions, structure, and domain composition can be predicted. Such predictions greatly facilitate the determination of the function or structure by targeted validation experiments.",
            "score": 87.07096862792969
        },
        {
            "docid": "55172_35",
            "document": "Proteomics . Balancing the use of mass spectrometers in proteomics and in medicine is the use of protein micro arrays. The aim behind protein micro arrays is to print thousands of protein detecting features for the interrogation of biological samples. Antibody arrays are an example in which a host of different antibodies are arrayed to detect their respective antigens from a sample of human blood. Another approach is the arraying of multiple protein types for the study of properties like protein-DNA, protein-protein and protein-ligand interactions. Ideally, the functional proteomic arrays would contain the entire complement of the proteins of a given organism. The first version of such arrays consisted of 5000 purified proteins from yeast deposited onto glass microscopic slides. Despite the success of first chip, it was a greater challenge for protein arrays to be implemented. Proteins are inherently much more difficult to work with than DNA. They have a broad dynamic range, are less stable than DNA and their structure is difficult to preserve on glass slides, though they are essential for most assays. The global ICAT technology has striking advantages over protein chip technologies.",
            "score": 86.68477630615234
        },
        {
            "docid": "28852_11",
            "document": "Syphilis . \"Treponema pallidum\" subspecies\" pallidum\" is a spiral-shaped, Gram-negative, highly mobile bacterium. Three other human diseases are caused by related \"Treponema pallidum\" subspecies, including yaws (subspecies \"pertenue\"), pinta (subspecies \"carateum\") and bejel (subspecies \"endemicum\"). Unlike subtype \"pallidum\", they do not cause neurological disease. Humans are the only known natural reservoir for subspecies \"pallidum\". It is unable to survive more than a few days without a host. This is due to its small genome (1.14\u00a0 Mbp) failing to encode the metabolic pathways necessary to make most of its macronutrients. It has a slow doubling time of greater than 30\u00a0hours.",
            "score": 86.25557708740234
        },
        {
            "docid": "2634439_2",
            "document": "Protein methods . Protein methods are the techniques used to study proteins. There are experimental methods for studying proteins (e.g., for detecting proteins, for isolating and purifying proteins, and for characterizing the structure and function of proteins, often requiring that the protein first be purified). Computational methods typically use computer programs to analyze proteins. However, many experimental methods (e.g., mass spectrometry) require computational analysis of the raw data.",
            "score": 85.3497543334961
        },
        {
            "docid": "30999216_20",
            "document": "Protein moonlighting . Mass spectrometry may be used to detect proteins based on their mass-to-charge ratio. Because of alternative splicing and posttranslational modification, identification of proteins based on the mass of the parent ion alone is very difficult. However tandem mass spectrometry in which each of the parent peaks is in turn fragmented can be used to unambiguously identify proteins. Hence tandem mass spectrometry is one of the tools used in proteomics to identify the presence of proteins in different cell types or subcellular locations. While the presence of a moonlighting protein in an unexpected location may complicate routine analyses, at the same time, the detection of a protein in unexpected multiprotein complexes or locations suggests that protein may have a moonlighting function. Furthermore, mass spectrometry may be used to determine if a protein has high expression levels that do not correlate to the enzyme's measured metabolic activity. These expression levels may signify that the protein is performing a different function than previously known.",
            "score": 84.93685150146484
        },
        {
            "docid": "4656324_3",
            "document": "PROSITE . PROSITE's uses include identifying possible functions of newly discovered proteins and analysis of known proteins for previously undetermined activity. Properties from well-studied genes can be propagated to biologically related organisms, and for different or poorly known genes biochemical functions can be predicted from similarities. PROSITE offers tools for protein sequence analysis and motif detection (see sequence motif, PROSITE patterns). It is part of the ExPASy proteomics analysis servers.",
            "score": 83.21961212158203
        },
        {
            "docid": "13967547_5",
            "document": "Dry lab . As a means of surpassing the limitations of these techniques, projects such as Folding@home and Rosetta@home are aimed at resolving this problem using computational analysis, this means of resolving protein structure is referred to as protein structure prediction. Although many labs have a slightly different approach, the main concept is to find, from a myriad of protein conformations, which conformation has the lowest energy or, in the case of Folding@Home, to find relatively low energies of proteins that could cause the protein to misfold and aggregate other proteins to itself\u2014like in the case of sickle cell anemia. The general scheme in these projects is that a small number of computations are parsed to, or sent to be calculated on, a computer, generally a home computer, and then that computer analyzes the likelihood that a specific protein will take a certain shape or conformation based on the amount of energy required for that protein to stay in that shape, this way of processing data is what is generally referred to as distributed computing. This analysis is done on an extraordinarily large number of different conformations, owing to the support of hundreds of thousands of home-based computers, in hopes to find the conformation of lowest possible energy or set of conformations of lowest possible energy relative to any conformations that are just slightly different. Although doing so is quite difficult, one can, by observing the energy distribution of a large number of conformations, despite the almost infinite number of different protein conformations possible for any given protein (see Levinthal Paradox), with a reasonably large number of protein energy samplings, predict relatively closely what conformation, within a range of conformations, has the expected lowest energy using methods in statistical inference. There are other factors such as salt concentration, pH, ambient temperature or chaperonins, which are proteins that assist in the folding process of other proteins, that can greatly affect how a protein folds. However, if the given protein is shown to fold on its own, especially in vitro, these findings can be further supported. Once we can see how a protein folds then we can see how it works as a catalyst, or in intracellular communication, e.g. neuroreceptor-neurotransmitter interaction. How certain compounds may be used to enhance or prevent the function of these proteins and how an elucidated protein overall plays a role in diseases such as Alzheimer's Disease or Huntington's Disease can also be much better understood.",
            "score": 83.20360565185547
        },
        {
            "docid": "29467449_4",
            "document": "Protein function prediction . While techniques such as microarray analysis, RNA interference, and the yeast two-hybrid system can be used to experimentally demonstrate the function of a protein, advances in sequencing technologies have made the rate at which proteins can be experimentally characterized much slower than the rate at which new sequences become available. Thus, the annotation of new sequences is mostly by \"prediction\" through computational methods, as these types of annotation can often be done quickly and for many genes or proteins at once. The first such methods inferred function based on homologous proteins with known functions (homology-based function prediction). The development of context-based and structure based methods have expanded what information can be predicted, and a combination of methods can now be used to get a picture of complete cellular pathways based on sequence data. The importance and prevalence of computational prediction of gene function is underlined by an analysis of 'evidence codes' used by the GO database: as of 2010, 98% of annotations were listed under the code IEA (inferred from electronic annotation) while only 0.6% were based on experimental evidence.",
            "score": 82.64630889892578
        },
        {
            "docid": "23634_44",
            "document": "Protein . The development of such tools has been driven by the large amount of genomic and proteomic data available for a variety of organisms, including the human genome. It is simply impossible to study all proteins experimentally, hence only a few are subjected to laboratory experiments while computational tools are used to extrapolate to similar proteins. Such homologous proteins can be efficiently identified in distantly related organisms by sequence alignment. Genome and gene sequences can be searched by a variety of tools for certain properties. Sequence profiling tools can find restriction enzyme sites, open reading frames in nucleotide sequences, and predict secondary structures. Phylogenetic trees can be constructed and evolutionary hypotheses developed using special software like ClustalW regarding the ancestry of modern organisms and the genes they express. The field of bioinformatics is now indispensable for the analysis of genes and proteins.",
            "score": 81.9297866821289
        },
        {
            "docid": "4635854_2",
            "document": "Hypothetical protein . In biochemistry, a hypothetical protein is a protein whose existence has been predicted, but for which there is a lack of experimental evidence that it is expressed in vivo. Sequencing of several genomes has resulted in numerous predicted open reading frames to which functions cannot be readily assigned. These proteins, either orphan or conserved hypothetical proteins, make up ~ 20% to 40% of proteins encoded in each newly sequenced genome. Even when there is enough evidence that the product of the gene is expressed, by techniques such as microarray and mass-spectrometry, it is difficult to assign a function to it given its lack of identity to protein sequences with annotated biochemical function. Nowadays, most protein sequences are inferred from computational analysis of genomic DNA sequence. Hypothetical proteins are created by gene prediction software during genome analysis. When the bioinformatic tool used for the gene identification finds a large open reading frame without a characterised homologue in the protein database, it returns \"hypothetical protein\" as an annotation remark.",
            "score": 81.817626953125
        },
        {
            "docid": "4214_31",
            "document": "Bioinformatics . Protein microarrays and high throughput (HT) mass spectrometry (MS) can provide a snapshot of the proteins present in a biological sample. Bioinformatics is very much involved in making sense of protein microarray and HT MS data; the former approach faces similar problems as with microarrays targeted at mRNA, the latter involves the problem of matching large amounts of mass data against predicted masses from protein sequence databases, and the complicated statistical analysis of samples where multiple, but incomplete peptides from each protein are detected. Cellular protein localization in a tissue context can be achieved through affinity proteomics displayed as spatial data based on immunohistochemistry and tissue microarrays.",
            "score": 81.73355102539062
        },
        {
            "docid": "61836_4",
            "document": "Treponema pallidum . This bacterium can be detected with special stains, such as the Dieterle stain. \"T. pallidum\" is also detected by serology, including nontreponemal VDRL, rapid plasma reagin, treponemal antibody tests (FTA-ABS), \"T. pallidum\" immobilization reaction, and syphilis TPHA test. It was first microscopically identified in syphilitic chancres by Fritz Schaudinn and Erich Hoffmann at the Charit\u00e9 in Berlin in 1905.",
            "score": 81.2350082397461
        },
        {
            "docid": "34567821_4",
            "document": "Critical Assessment of Function Annotation . The genome of an organism may consist of hundreds to tens of thousands of genes, which encode for hundreds of thousands of different protein sequences. Due to the relatively low cost of genome sequencing, determining gene and protein sequences is fast and inexpensive. Thousands of species have been sequenced so far, yet many of the proteins are not well characterized. The process of experimentally determining the role of a protein in the cell, is an expensive and time consuming task. Further, even when functional assays are performed they are unlikely to provide complete insight into protein function. Therefore it has become important to use computational tools in order to functionally annotate proteins. There are several computational methods of protein function prediction that can infer protein function using a variety of biological and evolutionary data, but there is significant room for improvement. Accurate prediction of protein function can have longstanding implications on biomedical and pharmaceutical research.",
            "score": 79.92414855957031
        },
        {
            "docid": "1686272_18",
            "document": "Chemical biology . Protein misfolding has previously been studied using both computational approaches as well as \"in vivo\" biological assays in model organisms such as \"Drosophila melanogaster\" and \"C. elegans\". Computational models use a \"de novo\" process to calculate possible protein structures based on input parameters such as amino acid sequence, solvent effects, and mutations. This method has the shortcoming that the cell environment has been drastically simplified, which limits the factors that influence folding and stability. On the other hand, biological assays can be quite complicated to perform \"in vivo\" with high-throughput like efficiency and there always remains the question of how well lower organism systems approximate human systems. Dobson et al. propose combining these two approaches such that computational models based on the organism studies can begin to predict what factors will lead to protein misfolding. Several experiments have already been performed based on this strategy. In experiments on \"Drosophila\", different mutations of beta amyloid peptides were evaluated based on the survival rates of the flies as well as their motile ability. The findings from the study show that the more a protein aggregates, the more detrimental the neurological dysfunction. Further studies using transthyretin, a component of cerebrospinal fluid that binds to beta amyloid peptide deterring aggregation but can itself aggregate especially when mutated, indicate that aggregation prone proteins may not aggregate where they are secreted and rather are deposited in specific organs or tissues based on each mutation. Kelly et al. have shown that the more stable, both kinetically and thermodynamically, a misfolded protein is the more likely the cell is to secrete it from the endoplasmic reticulum rather than targeting the protein for degradation. In addition, the more stress that a cell feels from misfolded proteins the more probable new proteins will misfold. These experiments as well as others having begun to elucidate both the intrinsic and extrinsic causes of misfolding as well as how the cell recognizes if proteins have folded correctly.",
            "score": 79.92184448242188
        },
        {
            "docid": "27947144_5",
            "document": "Minimotif Miner . Typical results of MnM predict more than 50 new minimotifs for a protein query. A major limitation in this type of analysis is that the low sequence complexity of short minimotifs produces false positive predictions where the sequence occurs in a protein by random chance and not because it contains the predicted function. MnM 3.0 introduces a library of advanced heuristics and filters, which enable vast reduction of false positive predictions. These filters use minimotif complexity, protein surface location, molecular processes, cellular processes, protein-protein interactions, and genetic interactions. We recently combined all of these heuristics into a single, compound filter which makes significant progress toward solving this problem with high accuracy of minimotif prediction as measured by a performance benchmarking study which evaluated both sensitivity and specificity.",
            "score": 79.76344299316406
        },
        {
            "docid": "14106058_2",
            "document": "Treponema denticola . Treponema denticola is a Gram-negative, obligate anaerobic, motile and highly proteolytic spirochete bacterium. It dwells in a complex and diverse microbial community within the oral cavity and is highly specialized to survive in this environment. \"T. denticola\" is associated with the incidence and severity of human periodontal disease. Having elevated \"T. denticola\" levels in the mouth is considered one of the main etiological agents of periodontitis. \"T. denticola\" is related to the syphilis-causing obligate human pathogen, \"Treponema pallidum\" subsp. \"pallidum\". It has also been isolated from women with bacterial vaginosis.",
            "score": 79.40299224853516
        },
        {
            "docid": "32090767_4",
            "document": "Treponema pallidum particle agglutination assay . A similar specific treponemal test for syphilis is the \"Treponema pallidum\" hemagglutination assay or TPHA. TPHA is an indirect hemagglutination assay used for the detection and titration of antibodies against the causative agent of syphilis, \"Treponema pallidum\" subspecies \"pallidum\".",
            "score": 79.3406753540039
        },
        {
            "docid": "13250438_2",
            "document": "Protein mass spectrometry . Protein mass spectrometry refers to the application of mass spectrometry to the study of proteins. Mass spectrometry is an important method for the accurate mass determination and characterization of proteins, and a variety of methods and instrumentations have been developed for its many uses. Its applications include the identification of proteins and their post-translational modifications, the elucidation of protein complexes, their subunits and functional interactions, as well as the global measurement of proteins in proteomics. It can also be used to localize proteins to the various organelles, and determine the interactions between different proteins as well as with membrane lipids.",
            "score": 78.87234497070312
        },
        {
            "docid": "40762191_7",
            "document": "Protein superfamily . Structure is much more evolutionarily conserved than sequence, such that proteins with highly similar structures can have entirely different sequences. Over very long evolutionary timescales, very few residues show detectable amino acid sequence conservation, however secondary structural elements and tertiary structural motifs are highly conserved. Conformational changes of the protein structure may also be conserved, as is seen in the serpin superfamily. Consequently, protein tertiary structure can be used to detect homology between proteins even when no evidence of relatedness remains in their sequences. Structural alignment programs, such as DALI, use the 3D structure of a protein of interest to find proteins with similar folds. However, on rare occasions, related proteins may evolve to be structurally dissimilar and relatedness can only be inferred by other methods.",
            "score": 78.59025573730469
        },
        {
            "docid": "2592262_8",
            "document": "Protein subcellular localization prediction . In 1999 PSORT was the first published program to predict subcellular localization. Subsequent tools and websites have been released using techniques such as artificial neural networks, support vector machine and protein motifs. Predictors can be specialized for proteins in different organisms. Some are specialized for eukaryotic proteins, some for human proteins, and some for plant proteins. Methods for the prediction of bacterial localization predictors, and their accuracy, have been reviewed.",
            "score": 77.67878723144531
        },
        {
            "docid": "29467449_8",
            "document": "Protein function prediction . The development of protein domain databases such as Pfam (Protein Families Database) allow us to find known domains within a query sequence, providing evidence for likely functions. The dcGO website contains annotations to both the individual domains and supra-domains (i.e., combinations of two or more successive domains), thus via dcGO Predictor allowing for the function predictions in a more realistic manner. Within protein domains, shorter signatures known as \"motifs\" are associated with particular functions, and motif databases such as PROSITE ('database of protein domains, families and functional sites') can be searched using a query sequence. Motifs can, for example, be used to predict subcellular localization of a protein (where in the cell the protein is sent after synthesis). Short signal peptides direct certain proteins to a particular location such as the mitochondria, and various tools exist for the prediction of these signals in a protein sequence. For example, SignalP, which has been updated several times as methods are improved. Thus, aspects of a protein's function can be predicted without comparison to other full-length homologous protein sequences.",
            "score": 77.37831115722656
        },
        {
            "docid": "4350008_2",
            "document": "Protein\u2013protein interaction prediction . Protein\u2013protein interaction prediction is a field combining bioinformatics and structural biology in an attempt to identify and catalog physical interactions between pairs or groups of proteins. Understanding protein\u2013protein interactions is important for the investigation of intracellular signaling pathways, modelling of protein complex structures and for gaining insights into various biochemical processes. Experimentally, physical interactions between pairs of proteins can be inferred from a variety of experimental techniques, including yeast two-hybrid systems, protein-fragment complementation assays (PCA), affinity purification/mass spectrometry, protein microarrays, fluorescence resonance energy transfer (FRET), and Microscale Thermophoresis (MST). Efforts to experimentally determine the interactome of numerous species are ongoing, and a number of computational methods for interaction prediction have been developed in recent years.",
            "score": 76.99427795410156
        },
        {
            "docid": "969126_29",
            "document": "Protein structure . The generation of a protein sequence is much easier than the determination of a protein structure. However, the structure of a protein gives much more insight in the function of the protein than its sequence. Therefore, a number of methods for the computational prediction of protein structure from its sequence have been developed. \"Ab initio\" prediction methods use just the sequence of the protein. Threading and homology modeling methods can build a 3-D model for a protein of unknown structure from experimental structures of evolutionarily-related proteins, called a protein family.",
            "score": 76.89381408691406
        },
        {
            "docid": "39368804_12",
            "document": "FAM214A . The only protein predicted according STRING to interact with the FAM214A protein is called MFSD6L. This protein belongs to the major facilitator superfamily is predicted to be a transmembrane protein. Like FAM214A, the function of this protein has not yet been characterized through experimentation or research. Because this MFSD6L protein is the only FAM214A protein interaction predicted with any certainty, the sequence for it was run through the PSORT II program. The data from the NLS subprogram predicted the presence of a single pat4 and two pat7 NLS sequences, thus indicating possible nuclear localization. The NCNN score, on the other hand, predicted cytoplasmic localiztion with 94.1% certainty, thus leaving the overall PSORT II score at 39.1% plasma membrane, 39.1% endoplasmic reticulum, 4.3% vacuolar, 4.3% vesicles of secretory system, 4.3% Golgi, 4.3% mitochondrial, and 4.3% nuclear. This is contradictory as there are three total nuclear localization signals, but this may be due to the fact that the significant transmembrane nature of the MFSD6L protein may be causing issues with these predictions.",
            "score": 76.80949401855469
        },
        {
            "docid": "1076110_2",
            "document": "Protein sequencing . Protein sequencing is the practical process of determining the amino acid sequence of all or part of a protein or peptide. This may serve to identify the protein or characterize its post-translational modifications. Typically, partial sequencing of a protein provides sufficient information (one or more sequence tags) to identify it with reference to databases of protein sequences derived from the conceptual translation of genes. The two major direct methods of protein sequencing are mass spectrometry and Edman degradation using a protein sequenator (sequencer). Mass spectrometry methods are now the most widely used for protein sequencing and identification but Edman degradation remains a valuable tool for characterizing a protein's \"N\"-terminus.",
            "score": 76.80011749267578
        },
        {
            "docid": "10085290_25",
            "document": "Lyme disease microbiology . The genome of \"B.\u00a0burgdorferi\" (B31 strain) was the third microbial genome ever to be sequenced, following the sequencing of both \"H. influenzae\" and \"M. genitalium\" in 1995, and its chromosome contains 910,725 base pairs and 853 genes. One of the most striking features of \"B.\u00a0burgdorferi\" as compared with other bacteria is its unusual genome, which is far more complex than that of its spirochetal cousin \"Treponema pallidum\", the agent of syphilis. In addition to a linear chromosome, the genome of \"B.\u00a0burgdorferi\" strain B31 includes 21 plasmids (12 linear and 9 circular) \u2013 by far the largest number of plasmids found in any known bacterium. Genetic exchange, including plasmid transfers, contributes to the pathogenicity of the organism. Long-term culture of \"B.\u00a0burgdorferi\" results in a loss of some plasmids and changes in expressed protein profiles. Associated with the loss of plasmids is a loss in the ability of the organism to infect laboratory animals, suggesting the plasmids encode key genes involved in virulence.",
            "score": 76.73070526123047
        },
        {
            "docid": "32090767_2",
            "document": "Treponema pallidum particle agglutination assay . The \"Treponema pallidum\" particle agglutination assay (also called TPPA test) is an indirect agglutination assay used for detection and titration of antibodies against the causative agent of syphilis, \"Treponema pallidum\" subspecies \"pallidum\".",
            "score": 76.6917495727539
        },
        {
            "docid": "13250438_11",
            "document": "Protein mass spectrometry . In the second approach, referred to as the \"bottom-up\" MS, proteins are enzymatically digested into smaller peptides using a protease such as trypsin. Subsequently, these peptides are introduced into the mass spectrometer and identified by peptide mass fingerprinting or tandem mass spectrometry. Hence, this approach uses identification at the peptide level to infer the existence of proteins pieced back together with \"de novo\" repeat detection. The smaller and more uniform fragments are easier to analyze than intact proteins and can be also determined with high accuracy, this \"bottom-up\" approach is therefore the preferred method of studies in proteomics. A further approach that is beginning to be useful is the intermediate \"middle-down\" approach in which proteolytic peptides larger than the typical tryptic peptides are analyzed.",
            "score": 76.43743133544922
        }
    ]
}