{
    "q": [
        {
            "docid": "56439577_34",
            "document": "Temporal envelope and fine structure . The effects of hearing loss and age on neural coding are generally believed to be smaller for slowly varying envelope responses (i.e., ENV) than for rapidly varying temporal fine structure (i.e., TFS). Enhanced ENV coding following noise-induced hearing loss has been observed in peripheral auditory responses from single neurons and in central evoked responses from the auditory midbrain. The enhancement in ENV coding of narrowband sounds occurs across the full range of modulation frequencies encoded by single neurons. For broadband sounds, the range of modulation frequencies encoded in impaired responses is broader than normal (extending to higher frequencies), as expected from reduced frequency selectivity associated with outer-hair-cell dysfunction. The enhancement observed in neural envelope responses is consistent with enhanced auditory perception of modulations following cochlear damage, which is commonly believed to result from loss of cochlear compression that occurs with outer-hair-cell dysfunction due to age or noise overexposure. \u00a0However, the influence of inner-hair-cell dysfunction (e.g., shallower response growth for mild-moderate damage and steeper growth for severe damage) can confound the effects of outer-hair-cell dysfunction on overall response growth and thus ENV coding. Thus, not surprisingly the relative effects of outer-hair-cell and inner-hair-cell dysfunction have been predicted with modeling to create individual differences in speech intelligibility based on the strength of envelope coding of speech relative to noise.",
            "score": 109.82453536987305
        },
        {
            "docid": "25260196_25",
            "document": "Neuronal encoding of sound . Primary auditory neurons carry action potentials from the cochlea into the transmission pathway shown in the adjacent image. Multiple relay stations act as integration and processing centers. The signals reach the first level of cortical processing at the primary auditory cortex (A1), in the superior temporal gyrus of the temporal lobe. Most areas up to and including A1 are tonotopically mapped (that is, frequencies are kept in an ordered arrangement). However, A1 participates in coding more complex and abstract aspects of auditory stimuli without coding well the frequency content, including the presence of a distinct sound or its echoes.  Like lower regions, this region of the brain has combination-sensitive neurons that have nonlinear responses to stimuli.",
            "score": 115.05579566955566
        },
        {
            "docid": "56439577_40",
            "document": "Temporal envelope and fine structure . Psychophysical studies have suggested that degraded TFS processing due to age and hearing loss may underlie some suprathreshold deficits, such as speech perception; however, debate remains about the underlying neural correlates. The strength of phase locking to the temporal fine structure of signals (TFS) in quiet listening conditions remains normal in peripheral single-neuron responses following cochlear hearing loss. Although these data suggest that the fundamental ability of auditory-nerve fibers to follow the rapid fluctuations of sound remains intact following cochlear hearing loss, deficits in phase locking strength do emerge in background noise. This finding, which is consistent with the common observation that listeners with cochlear hearing loss have more difficulty in noisy conditions, results from reduced cochlear frequency selectivity associated with outer-hair-cell dysfunction. \u00a0Although only limited effects of age and hearing loss have been observed in terms of TFS coding strength of narrowband sounds, more dramatic deficits have been observed in TFS coding quality in response to broadband sounds, which are more relevant for everyday listening. \u00a0A dramatic loss of tonotopicity can occur following noise induced hearing loss, where auditory-nerve fibers that should be responding to mid frequencies (e.g., 2\u20134\u00a0kHz) have dominant TFS responses to lower frequencies (e.g., 700\u00a0Hz). \u00a0Notably, the loss of tonotopicity generally occurs only for TFS coding but not for ENV coding, which is consistent with greater perceptual deficits in TFS processing. This tonotopic degradation is likely to have important implications for speech perception, and can account for degraded coding of vowels following noise-induced hearing loss in which most of the cochlea responds to only the first formant, eliminating the normal tonotopic representation of the second and third formants.",
            "score": 113.64163613319397
        },
        {
            "docid": "4301708_10",
            "document": "Cochlear nucleus . The cochlear nuclear complex is the first integrative, or processing, stage in the auditory system. Information is brought to the nuclei from the ipsilateral cochlea via the cochlear nerve. Several tasks are performed in the cochlear nuclei. By distributing acoustic input to multiple types of principal cells, the auditory pathway is subdivided into parallel ascending pathways, which can simultaneously extract different types of information. The cells of the ventral cochlear nucleus extract information that is carried by the auditory nerve in the timing of firing and in the pattern of activation of the population of auditory nerve fibers. The cells of the dorsal cochlear nucleus perform a non-linear spectral analysis and place that spectral analysis into the context of the location of the head, ears and shoulders and that separate expected, self-generated spectral cues from more interesting, unexpected spectral cues using input from the auditory cortex, pontine nuclei, trigeminal ganglion and nucleus, dorsal column nuclei and the second dorsal root ganglion. It is likely that these neurons help mammals to use spectral cues for orienting toward those sounds. The information is used by higher brainstem regions to achieve further computational objectives (such as sound source location or improvement in signal to noise ratio). The inputs from these other areas of the brain probably play a role in sound localization.",
            "score": 123.80797004699707
        },
        {
            "docid": "5198024_4",
            "document": "Efficient coding hypothesis . A key prediction of the efficient coding hypothesis is that sensory processing in the brain should be adapted to natural stimuli. Neurons in the visual (or auditory) system should be optimized for coding images (or sounds) representative of those found in nature. Researchers have shown that filters optimized for coding natural images lead to filters which resemble the receptive fields of simple-cells in V1. In the auditory domain, optimizing a network for coding natural sounds leads to filters which resemble the impulse response of cochlear filters found in the inner ear.",
            "score": 87.26785683631897
        },
        {
            "docid": "3474296_9",
            "document": "Neuronal noise . The external noise paradigm assumes \"neural noise\" and speculates that external noise should multiplicatively increase the amount of internal noise in the central nervous system. It is not clear how \"neural noise\" is theoretically distinguished from \"neural signal.\" Proponents of this paradigm believe that adding visual or auditory external noise to a stimuli, and measure how it affects reaction time or the subject's performance. If performance is more inconsistent than without the noise, the subject is said to have \"internal noise.\" As in the case of \"internal noise,\" it is not clear on what theoretical grounds researchers distinguish \"external noise\" from \"external signal\" in terms of the perceptual response of the viewer, which is a response to the stimulus as a whole.",
            "score": 83.51247835159302
        },
        {
            "docid": "17523336_19",
            "document": "Olivocochlear system . One interpretation of these findings is that MOC stimulation selectively reduces the auditory nerve\u2019s response to constant background noise, allowing a greater response to a transient sound. In this way, MOC stimulation would reduce the effect of both suppressive and adaptive masking, and for this reason, the process has been referred to as \u201cunmasking\u201d or \u201cantimasking\u201d (Kawase et al., 1993, Kawase and Liberman, 1993). Antimasking has been suggested to occur in a similar fashion in humans (Kawase and Takasaka, 1995), and has implications for selective listening since the rapid unmasking of a sound resulting from MOC activation would increase the overall signal-to-noise ratio (SNR), thus facilitating better detection of a target sound.",
            "score": 99.83102679252625
        },
        {
            "docid": "56439577_8",
            "document": "Temporal envelope and fine structure . Responses to the temporal-envelope cues of speech or other complex sounds persist up the auditory pathway, eventually to the various fields of the auditory cortex in many animals. In the Primary Auditory Cortex, responses can encode AM rates by phase-locking up to about 20\u201330\u00a0Hz, while faster rates induce sustained and often tuned responses. A topographical representation of AM rate has been demonstrated in the primary auditory cortex of awake macaques. This representation is approximately perpendicular to the axis of the tonotopic gradient, consistent with an orthogonal organization of spectral and temporal features in the auditory cortex. Combining these temporal responses with the spectral selectivity of A1 neurons gives rise to the spectro-temporal receptive fields that often capture well cortical responses to complex modulated sounds. In secondary auditory cortical fields, responses become temporally more sluggish and spectrally broader, but are still able to phase-lock to the salient features of speech and musical sounds. Tuning to AM rates below about 64\u00a0Hz is also found in the human auditory cortex as revealed by brain-imaging techniques (fMRI) and cortical recordings in epileptic patients (electrocorticography). This is consistent with neuropsychological studies of brain-damaged patients and with the notion that the central auditory system performs some form of spectral decomposition of the ENVp of incoming sounds. Interestingly, the ranges over which cortical responses encode well the temporal-envelope cues of speech have been shown to be predictive of the human ability to understand speech. In the human superior temporal gyrus (STG), an anterior-posterior spatial organization of spectro-temporal modulation tuning has been found in response to speech sounds, the posterior STG being tuned for temporally fast varying speech sounds with low spectral modulations and the anterior STG being tuned for temporally slow varying speech sounds with high spectral modulations.",
            "score": 103.07714486122131
        },
        {
            "docid": "19415143_2",
            "document": "Auditory agnosia . Auditory agnosia is a form of agnosia that manifests itself primarily in the inability to recognize or differentiate between sounds. It is not a defect of the ear or \"hearing\", but a neurological inability of the brain to process sound meaning. It is a disruption of the \"what\" pathway in the brain. Persons with auditory agnosia can physically hear the sounds and describe them using unrelated terms, but are unable to recognize them. They might describe the sound of some environmental sounds, such as a motor starting, as resembling a lion roaring, but would not be able to associate the sound with \"car\" or \"engine\", nor would they say that it \"was\" a lion creating the noise. Auditory agnosia is caused by damage to the secondary and tertiary auditory cortex of the temporal lobe of the brain.",
            "score": 114.78052425384521
        },
        {
            "docid": "1222458_3",
            "document": "Tonotopy . Tonotopy in the auditory system begins at the cochlea, the small snail-like structure in the inner ear that sends information about sound to the brain. Different regions of the basilar membrane in the organ of Corti, the sound-sensitive portion of the cochlea, vibrate at different sinusoidal frequencies due to variations in thickness and width along the length of the membrane. Nerves that transmit information from different regions of the basilar membrane therefore encode frequency tonotopically. This tonotopy then projects through the vestibulocochlear nerve and associated midbrain structures to the primary auditory cortex via the auditory radiation pathway. Throughout this radiation, organization is linear with relation to placement on the organ of Corti, in accordance to the best frequency response (that is, the frequency at which that neuron is most sensitive) of each neuron. However, binaural fusion in the superior olivary complex onward adds significant amounts of information encoded in the signal strength of each ganglion. Thus, the number of tonotopic maps varies between species and the degree of binaural synthesis and separation of sound intensities; in humans, six tonotopic maps have been identified in the primary auditory cortex. their anatomical locations along the auditory cortex.",
            "score": 130.00411319732666
        },
        {
            "docid": "8953842_15",
            "document": "Computational auditory scene analysis . While many models consider the audio signal as a complex combination of different frequencies, modeling the auditory system can also require consideration for the neural components. By taking a holistic process, where a stream (of feature-based sounds) correspond to neuronal activity distributed in many brain areas, the perception of the sound could be mapped and modeled. Two different solutions have been proposed to the binding of the audio perception and the area in the brain. Hierarchical coding models many cells to encode all possible combinations of features and objects in the auditory scene. Temporal or oscillatory correlation addressing the binding problem by focusing on the synchrony and desynchrony between neural oscillations to encode the state of binding among the auditory features. These two solutions are very similar to the debacle between place coding and temporal coding. While drawing from modeling neural components, another phenomenon of ASA comes into play with CASA systems: the extent of modeling neural mechanisms. The studies of CASA systems have involved modeling some known mechanisms, such as the bandpass nature of cochlear filtering and random auditory nerve firing patterns, however, these models may not lead to finding new mechanisms, but rather give an understanding of purpose to the known mechanisms.",
            "score": 103.130362033844
        },
        {
            "docid": "6147487_47",
            "document": "Neural coding . A typical population code involves neurons with a Gaussian tuning curve whose means vary linearly with the stimulus intensity, meaning that the neuron responds most strongly (in terms of spikes per second) to a stimulus near the mean. The actual intensity could be recovered as the stimulus level corresponding to the mean of the neuron with the greatest response. However, the noise inherent in neural responses means that a maximum likelihood estimation function is more accurate. This type of code is used to encode continuous variables such as joint position, eye position, color, or sound frequency. Any individual neuron is too noisy to faithfully encode the variable using rate coding, but an entire population ensures greater fidelity and precision. For a population of unimodal tuning curves, i.e. with a single peak, the precision typically scales linearly with the number of neurons. Hence, for half the precision, half as many neurons are required. In contrast, when the tuning curves have multiple peaks, as in grid cells that represent space, the precision of the population can scale exponentially with the number of neurons. This greatly reduces the number of neurons required for the same precision.",
            "score": 64.50560235977173
        },
        {
            "docid": "56439577_9",
            "document": "Temporal envelope and fine structure . One unexpected aspect of phase locking in the auditory cortex has been observed in the responses elicited by complex acoustic stimuli with spectrograms that exhibit relatively slow envelopes (< 20\u00a0Hz), but that are carried by fast modulations that are as high as hundreds of Hertz. Speech and music, as well as various modulated noise stimuli have such temporal structure. For these stimuli, cortical responses phase-lock to \"both\" the envelope and fine-structure induced by interactions between unresolved harmonics of the sound, thus reflecting the pitch of the sound, and exceeding the typical lower limits of cortical phase-locking to the envelopes of a few 10\u2019s of Hertz. This paradoxical relation between the slow and fast cortical phase-locking to the carrier \u201cfine structure\u201d has been demonstrated both in the auditory and visual cortices. It has also been shown to be amply manifested in measurements of the spectro-temporal receptive fields of the primary auditory cortex giving them unexpectedly fine temporal accuracy and selectivity bordering on a 5-10 ms resolution. The underlying causes of this phenomenon have been attributed to several possible origins, including nonlinear synaptic depression and facilitation, and/or a cortical network of thalamic excitation and cortical inhibition. There are many functionally significant and perceptually relevant reasons for the coexistence of these two complementary dynamic response modes. They include the ability to accurately encode onsets and other rapid \u2018events\u2019 in the ENVp of complex acoustic and other sensory signals, features that are critical for the perception of consonants (speech) and percussive sounds (music), as well as the texture of complex sounds.",
            "score": 77.58670616149902
        },
        {
            "docid": "22822496_19",
            "document": "Tinnitus masker . These masker devices use soothing natural sounds such as ocean surf, rainfall or synthetic sounds such as white noise, pink noise, or brown noise to help the auditory system become less sensitive to tinnitus and promote relaxation by reducing the contrast between tinnitus sounds and background sound.",
            "score": 100.14014387130737
        },
        {
            "docid": "994097_20",
            "document": "Auditory cortex . The auditory cortex has distinct responses to sounds in the gamma band. When subjects are exposed to three or four cycles of a 40 hertz click, an abnormal spike appears in the EEG data, which is not present for other stimuli. The spike in neuronal activity correlating to this frequency is not restrained to the tonotopic organization of the auditory cortex. It has been theorized that gamma frequencies are resonant frequencies of certain areas of the brain, and appear to affect the visual cortex as well. Gamma band activation (25 to 100\u00a0Hz) has been shown to be present during the perception of sensory events and the process of recognition. In a 2000 study by Kneif and colleagues, subjects were presented with eight musical notes to well-known tunes, such as \"Yankee Doodle\" and \"Fr\u00e8re Jacques\". Randomly, the sixth and seventh notes were omitted and an electroencephalogram, as well as a magnetoencephalogram were each employed to measure the neural results. Specifically, the presence of gamma waves, induced by the auditory task at hand, were measured from the temples of the subjects. The OSP response, or omitted stimulus response, was located in a slightly different position; 7\u00a0mm more anterior, 13\u00a0mm more medial and 13\u00a0mm more superior in respect to the complete sets. The OSP recordings were also characteristically lower in gamma waves, as compared to the complete musical set. The evoked responses during the sixth and seventh omitted notes are assumed to be imagined, and were characteristically different, especially in the right hemisphere. The right auditory cortex has long been shown to be more sensitive to tonality, while the left auditory cortex has been shown to be more sensitive to minute sequential differences in sound, such as in speech.",
            "score": 98.07986271381378
        },
        {
            "docid": "32116125_4",
            "document": "Amblyaudia . Amblyaudia is a deficit in binaural integration of environmental information entering the auditory system. It is a disorder related to brain organization and function rather than what is typically considered a \u201chearing loss\u201d (damage to the cochlea). It may be genetic or developmentally acquired or both. When animals are temporarily deprived of hearing from an early age, profound changes occur in the brain. Specifically, cell sizes in brainstem nuclei are reduced, the configuration of brainstem dendrites are altered and neurons respond in different ways to sounds presented to both the deprived and non-deprived ears (in cases of asymmetric deprivation). This last point is particularly important for listening tasks that require inputs from two ears to perform well. There are multiple auditory functions that rely on the computation of well calibrated inputs from the two ears. Chief among these is the ability to localize sound sources and separate what we want to hear from a background of noise. In the brainstem, the auditory system compares the timing and levels of sounds between the two ears to encode the location of sound sources (sounds that originate from our right as opposed to left side are louder and arrive earlier in our right ear). This ability to separate sound sources not only helps us locate the trajectories of moving objects, but also to separate different sound sources in noisy environments.",
            "score": 117.89516079425812
        },
        {
            "docid": "5051081_4",
            "document": "Eric Knudsen . In 1978, Knudsen and Konishi presented the discovery of an auditory map of space in the midbrain of the barn owl. This discovery was groundbreaking because it unearthed the first non-somatotopic space map in the brain. The map was found in the owl\u2019s midbrain, in the lateral and anterior mesencephalicus lateralis dorsalis (MLD), a structure now referred to as the inferior colliculus. Unlike most sound-localization maps, this map was found to be two-dimensional, with units arranged spatially to represent both the vertical and horizontal location of sound. Knudsen and Konishi discovered that units in this structure respond preferentially to sounds originating in a particular region in space. In the 1978 paper, elevation and azimuth (location in the horizontal plane) were shown to be the two coordinates of the map. Using a speaker set on a rotatable hemispherical track, Knudsen and Konishi presented owls with auditory stimulus from various locations in space and recorded the resulting neuronal activity. They found that neurons in this part of the MLD were organized according to the location of their receptive field, with azimuth varying along the horizontal plane of the space map and elevation varying vertically.  Knudsen followed this discovery with research into specific sound localization mechanisms. Two main auditory cues used by the barn owl to localize sound are interaural time difference (ITD) and interaural intensity difference (IID). The owl\u2019s ears are asymmetric, with the right ear\u2019s opening being directed higher than that of the left. This asymmetry allows the barn owl to determine the elevation of a sound by comparing sound levels between its two ears. Interaural time differences provide the owl with information regarding a sound\u2019s azimuth; sound will reach the ear closer to the sound source before reaching the farther ear, and this time difference can be detected and interpreted as an azimuthal direction. At low frequencies, the wavelength of a sound is wider than the owl's facial ruff, and the ruff does not affect detection of azimuth. At high frequencies, the ruff plays a role in reflecting sound for heightened sensitivity to vertical elevation. Therefore, with wide-band noise, containing both high and low frequencies, the owl could use interaural spectrum difference to obtain information about both azimuth and elevation. In 1979, Knudsen and Konishi showed that the barn owl uses interaural spectrum information in sound localization. They presented owls with both wide-bandwidth noise and pure tones. The birds were able to successfully locate pure tones (since they could still gather information from IID and ITD), but their error rate was much lower when localizing wide-bandwidth noise. This indicates that the birds utilize interaural spectrum differences to improve their accuracy.",
            "score": 115.2886757850647
        },
        {
            "docid": "18994087_28",
            "document": "Sound . Pitch is perceived as how \"low\" or \"high\" a sound is and represents the cyclic, repetitive nature of the vibrations that make up sound. For simple sounds, pitch relates to the frequency of the slowest vibration in the sound (called the fundamental harmonic). In the case of complex sounds, pitch perception can vary. Sometimes individuals identify different pitches for the same sound, based on their personal experience of particular sound patterns. Selection of a particular pitch is determined by pre-conscious examination of vibrations, including their frequencies and the balance between them. Specific attention is given to recognising potential harmonics. Every sound is placed on a pitch continuum from low to high. For example: white noise (random noise spread evenly across all frequencies) sounds higher in pitch than pink noise (random noise spread evenly across octaves) as white noise has more high frequency content. Figure 1 shows an example of pitch recognition. During the listening process, each sound is analysed for a repeating pattern (See Figure 1: orange arrows) and the results forwarded to the auditory cortex as a single pitch of a certain height (octave) and chroma (note name).",
            "score": 94.28902649879456
        },
        {
            "docid": "56439577_48",
            "document": "Temporal envelope and fine structure . The plasticity of ENV processing has been demonstrated in several ways. For instance, the ability of auditory-cortex neurons to discriminate voice-onset time cues for phonemes is degraded following moderate hearing loss (20-40\u00a0dB HL) induced by acoustic trauma. Interestingly, developmental hearing loss reduces cortical responses to slow, but not fast (100\u00a0Hz) AM stimuli, in parallel with behavioral performance. As a matter of fact, a transient hearing loss (15 days) occurring during the \"critical period\" is sufficient to elevate AM thresholds in adult gerbils. Even non-traumatic noise exposure reduces the phase-locking ability of cortical neurons as well as the animals' behavioral capacity to discriminate between different AM sounds. Behavioral training or pairing protocols involving neuromodulators also alter the ability of cortical neurons to phase lock to AM sounds. In humans, hearing loss may result in an unbalanced representation of speech cues: ENV cues are enhanced at the cost of TFS cues (see: Effects of age and hearing loss on temporal envelope processing). Auditory training may reduce the representation of speech ENV cues for elderly listeners with hearing loss, who may then reach levels comparable to those observed for normal-hearing elderly listeners. Last, intensive musical training induces both behavioral effects such as higher sensitivity to pitch variations (for Mandarin linguistic pitch) and a better synchronization of brainstem responses to the f0-contour of lexical tones for musicians compared with non-musicians.",
            "score": 98.26928997039795
        },
        {
            "docid": "56439577_43",
            "document": "Temporal envelope and fine structure . The coding of temporal information in the auditory nerve can be disrupted by two main mechanisms: reduced synchrony and loss of synapses and/or auditory nerve fibers. The impact of disrupted temporal coding on human auditory perception has been explored using physiologically inspired signal-processing tools. The reduction in neural synchrony has been simulated by jittering the phases of the multiple frequency components in speech, although this has undesired effects in the spectral domain. The loss of auditory nerve fibers or synapses has been simulated by assuming (i) that each afferent fiber operates as a stochastic sampler of the sound waveform, with greater probability of firing for higher-intensity and sustained sound features than for lower-intensity or transient features, and (ii) that deafferentation can be modeled by reducing the number of samplers. However, this also has undesired effects in the spectral domain. Both jittering and stochastic undersampling degrade the representation of the TFS more than the representation of the ENV. Both jittering and stochastic undersampling impair the recognition of speech in noisy backgrounds without degrading recognition in silence, support the argument that TFS is important for recognizing speech in noise. Both jittering and stochastic undersampling mimic the effects of aging on speech perception.",
            "score": 104.63265585899353
        },
        {
            "docid": "5198024_22",
            "document": "Efficient coding hypothesis . Observed redundancy: A comparison of the number of retinal ganglion cells to the number of neurons in the primary visual cortex shows an increase in the number of sensory neurons in the cortex as compared to the retina. Simoncelli notes that one major argument of critics in that higher up in the sensory pathway there are greater numbers of neurons that handle the processing of sensory information so this should seem to produce redundancy. However, this observation may not be fully relevant because neurons have different neural coding. In his review, Simoncelli notes \"cortical neurons tend to have lower firing rates and may use a different form of code as compared to retinal neurons\". Cortical Neurons may also have the ability to encode information over longer periods of time than their retinal counterparts. Experiments done in the auditory system have confirmed that redundancy is decreased.",
            "score": 74.78413033485413
        },
        {
            "docid": "1947410_28",
            "document": "Critical period . Recent studies have examined the possibility of a critical period for thalamocortical connectivity in the auditory system. For example, Zhou and Merzenich (2008) studied the effects of noise on development in the primary auditory cortex in rats. In their study, rats were exposed to pulsed noise during the critical period and the effect on cortical processing was measured. Rats that were exposed to pulsed noise during the critical period had cortical neurons that were less able to respond to repeated stimuli; the early auditory environment interrupted normal structural organization during development.",
            "score": 96.39588713645935
        },
        {
            "docid": "6147487_45",
            "document": "Neural coding . The correlation coding model of neuronal firing claims that correlations between action potentials, or \"spikes\", within a spike train may carry additional information above and beyond the simple timing of the spikes. Early work suggested that correlation between spike trains can only reduce, and never increase, the total mutual information present in the two spike trains about a stimulus feature. However, this was later demonstrated to be incorrect. Correlation structure can increase information content if noise and signal correlations are of opposite sign. Correlations can also carry information not present in the average firing rate of two pairs of neurons. A good example of this exists in the pentobarbital-anesthetized marmoset auditory cortex, in which a pure tone causes an increase in the number of correlated spikes, but not an increase in the mean firing rate, of pairs of neurons.",
            "score": 77.04100370407104
        },
        {
            "docid": "994097_9",
            "document": "Auditory cortex . Neurons in the auditory cortex are organized according to the frequency of sound to which they respond best. Neurons at one end of the auditory cortex respond best to low frequencies; neurons at the other respond best to high frequencies. There are multiple auditory areas (much like the multiple areas in the visual cortex), which can be distinguished anatomically and on the basis that they contain a complete \"frequency map.\" The purpose of this frequency map (known as a tonotopic map) is unknown, and is likely to reflect the fact that the cochlea is arranged according to sound frequency. The auditory cortex is involved in tasks such as identifying and segregating \"auditory objects\" and identifying the location of a sound in space. For example, it has been shown that A1 encodes complex and abstract aspects of auditory stimuli without encoding their \"raw\" aspects like frequency content, presence of a distinct sound or its echoes.",
            "score": 108.90580987930298
        },
        {
            "docid": "8953842_3",
            "document": "Computational auditory scene analysis . Since CASA serves to model functionality parts of the auditory system, it is necessary to view parts of the biological auditory system in terms of known physical models. Consisting of three areas, the outer, middle and inner ear, the auditory periphery acts as a complex transducer that converts sound vibrations into action potentials in the auditory nerve. The outer ear consists of the external ear, ear canal and the ear drum. The outer ear, like an acoustic funnel, helps locating the sound source. The ear canal acts as a resonant tube (like an organ pipe) to amplify frequencies between 2\u20135.5\u00a0kHz with a maximum amplification of about 11\u00a0dB occurring around 4\u00a0kHz. As the organ of hearing, the cochlea consists of two membranes, Reissner\u2019s and the basilar membrane. The basilar membrane moves to audio stimuli through the specific stimulus frequency matches the resonant frequency of a particular region of the basilar membrane. The movement the basilar membrane displaces the inner hair cells in one direction, which encodes a half-wave rectified signal of action potentials in the spiral ganglion cells. The axons of these cells make up the auditory nerve, encoding the rectified stimulus. The auditory nerve responses select certain frequencies, similar to the basilar membrane. For lower frequencies, the fibers exhibit \"phase locking\". Neurons in higher auditory pathway centers are tuned to specific stimuli features, such as periodicity, sound intensity, amplitude and frequency modulation.  There are also neuroanatomical associations of ASA through the posterior cortical areas, including the posterior superior temporal lobes and the posterior cingulate. Studies have found that impairments in ASA and segregation and grouping operations are affected in patients with Alzheimer's disease.",
            "score": 97.08753418922424
        },
        {
            "docid": "26166115_3",
            "document": "Audio-visual entrainment . All of our senses (except smell) access the brain's cerebral cortex via the thalamus, and because the thalamus is highly innervated with the cortex, sensory stimulation can easily influence cortical activity. In order to affect brain (neuronal) activity, sensory stimulation must be within the frequency range of roughly 0.5 to 25 hertz (Hz) . Touch, photic and auditory stimulation are capable of affecting brain wave activity. A large area of skin must be stimulated to affect brainwaves, which leaves both auditory and photic stimulation as the most effective and easiest means of affecting brain activity. Therefore, mind machines are typically in the form of light and sound devices.",
            "score": 84.41190910339355
        },
        {
            "docid": "35988494_2",
            "document": "Selective auditory attention . Selective auditory attention or selective hearing is a type of selective attention and involves the auditory system of the nervous system. Selective hearing is characterized as the action in which people focus their attention on a specific source of a sound or spoken words. The sounds and noise in the surrounding environment is heard by the auditory system but only certain parts of the auditory information are processed in the brain. Most often, auditory attention is directed at things people are most interested in hearing. In an article by Krans, Isbell, Giuliano, and Neville (2013), selective auditory attention is defined as the ability to acknowledge some stimuli while ignoring other stimuli that is occurring at the same time. An example of this is a student focusing on a teacher giving a lesson and ignoring the sounds of classmates in a rowdy classroom (p.\u00a053). This is an example of bottlenecking which means that information cannot be processed simultaneously so only some sensory information gets through the \"bottleneck\" and is processed. A brain simply cannot process all sensory information that is occurring in an environment so only that which is most important is thoroughly processed. Selective hearing is not a physiological disorder but rather it is the capability of humans to block out sounds and noise. It is the notion of ignoring certain things in the surrounding environment. Over the years, there has been increased research in the selectivity of auditory attention, namely selective hearing.",
            "score": 101.35418844223022
        },
        {
            "docid": "29354346_7",
            "document": "Change deafness . One study used fMRI data to distinguish neural correlates of physical changes in auditory input (independent of conscious change detection), from those of conscious perception of change (independent of an actual physical change). The study made use of a change deafness paradigm in which participants were exposed to complex auditory scenes consisting of six individual auditory streams differing in pitch, rhythm, and sound source location, and received a cue indicating which stream to attend to. Each participant listened to two consecutively presented auditory scenes after which they were prompted to indicate whether both scenes were identical or not. Functional MRI results revealed that physical change in stimulus was correlated with increased BOLD responses in the right auditory cortex, near the lateral portion of Heschl's gyrus, the first cortical structure to process incoming auditory information, but not in hierarchically higher brain regions. Conscious change detection was correlated with increased coupled responses in the ACC and the right insula, consistent with additional evidence that the anterior insula functions to mediate dynamic interactions between other brain networks involved in attention to external stimuli, forming a salience network with the ACC that identifies salient stimulus events and initiates additional processing. In absence of change detection, this salience network was not activated; however increased activity in other cortical areas suggests that undetected changes are still perceived on some level, but fail to trigger conscious change detection, thus producing the change deafness phenomenon.",
            "score": 78.29612922668457
        },
        {
            "docid": "25259743_16",
            "document": "Cortical cooling . To determine what parts of the auditory cortex contribute to sound localization, investigators implanted cryoloops to deactivate the 13 known regions of acoustically responsive cortex of the cat. Cats learned to make an orienting response by moving their heads and approaching a 100-ms broad-band noise stimulus emitted from a central speaker or one of 12 peripheral speakers located at 15\u00b0 intervals from left 90\u00b0 to right 90\u00b0along the horizontal plane after attending to a central visual stimulus generated by a red LED. After the cats had reached at least 80% accuracy in identifying the location of the sound stimulus, each was implanted with one or two bilateral pairs of cryoloops over the different sections of the auditory cortex; 10 sections were defined. Cryoloops were turned on so that the loops reached a temperature of 3\u00b0C (plus or minus 1\u00b0C), first unilaterally, then bilaterally, next unilaterally on the other side, and finally baseline task performance was recorded after recovering from cooling. This cycle was repeated several times for each cat.  Of the 10 sections that were deactivated, only deactivation of 3 sections, the AI (primary auditory cortex)/DZ (dorsal zone), PAF (posterior auditory field), and AES (anterior ectosylvian sulcus) sections, were found to have an effect on sound localization. At baseline, cats were able to locate 90% of the sound stimuli. Unilateral deactivation of any one of these sections resulted in a contralateral impairment in sound localization, or 10% accuracy. Bilateral deactivation of any combination of these three sections resulted in a 180\u00b0 deficit to 10% of sound locations identified, although this accuracy implied that cats were still able to orient to the hemifield where the sound occurred above chance (7.7%).  Since the primary auditory cortex and dorsal zone were concurrently cooled, the investigators performed another study in which the AI and DZ were examined as separate entities to further establish the sections of auditory cortex contributing to sound localization. The experimental design was the same as the above-mentioned design with the exception that only the AI and DZ sections were implanted with separate cryoloops. Again, it was found that unilateral simultaneous cooling deactivation of the AI and DZ generated contralateral sound localization deficits while bilateral deactivation created a deficit in both hemifields (10% sound location identification). Bilateral deactivation of AI alone resulted in only 45% accuracy within 30\u00b0 of the target. Bilateral deactivation of DZ resulted in 60% accuracy but with larger errors, often into the hemifield opposite the target. Therefore, AZ deactivation produces a higher number of small errors while deactivation of DZ leads to larger but fewer errors. This finding that AI and DZ deactivation produce partial deficits in sound localization implies that the previous finding that PAF and AES deactivation have more considerable contributions to sound localization than either the AI or DZ.",
            "score": 101.68863916397095
        },
        {
            "docid": "1021754_36",
            "document": "Sound localization . The auditory system can extract the sound of a desired sound source out of interfering noise. This allows the listener to concentrate on only one speaker if other speakers are also talking (the cocktail party effect). With the help of the cocktail party effect sound from interfering directions is perceived attenuated compared to the sound from the desired direction. The auditory system can increase the signal-to-noise ratio by up to 15\u00a0dB, which means that interfering sound is perceived to be attenuated to half (or less) of its actual loudness.",
            "score": 95.77116894721985
        },
        {
            "docid": "21312279_25",
            "document": "Music-related memory . Neural structures form and become more sophisticated as a result of experience. For example, the preference for consonance, the harmony or agreement of components, over dissonance, an unstable tone combination, is found early in development. Research suggests that this is due to both the experiencing of structured sounds and the fact they stem from development of the basilar membrane and auditory nerve, two early developing structures in the brain. An incoming auditory stimulus evokes responses measured in the form of an Event-related potential (ERP), measured brain responses resulting directly from a thought or perception. There is a difference in ERP measures for normally developing infants ranging from 2\u20136 months in age. Measures in infants 4 months and older demonstrate faster, more negative ERPs. In contrast, newborns and infants up to 4 months of age show slow, unsynchronized, positive ERPs. Trainor, et al. (2003) hypothesized that these results indicated that responses from infants less than four months of age are produced by subcortical auditory structures, whereas with older infants responses tend to originate in the higher cortical structures.",
            "score": 76.74416494369507
        },
        {
            "docid": "33246145_11",
            "document": "Neural decoding . Timescales and frequencies of stimuli being presented to the observer are also of importance to decoding the neural code. Quicker timescales and higher frequencies demand faster and more precise responses in neural spike data. In humans, millisecond precision has been observed throughout the visual cortex, the retina, and the lateral geniculate nucleus, so one would suspect this to be the appropriate measuring frequency. This has been confirmed in studies that quantify the responses of neurons in the lateral geniculate nucleus to white-noise and naturalistic movie stimuli. At the cellular level, spike-timing-dependent plasticity operates at millisecond timescales; therefore, models seeking biological relevance should be able to perform at these temporal scales.",
            "score": 46.34168601036072
        }
    ],
    "r": [
        {
            "docid": "1222458_3",
            "document": "Tonotopy . Tonotopy in the auditory system begins at the cochlea, the small snail-like structure in the inner ear that sends information about sound to the brain. Different regions of the basilar membrane in the organ of Corti, the sound-sensitive portion of the cochlea, vibrate at different sinusoidal frequencies due to variations in thickness and width along the length of the membrane. Nerves that transmit information from different regions of the basilar membrane therefore encode frequency tonotopically. This tonotopy then projects through the vestibulocochlear nerve and associated midbrain structures to the primary auditory cortex via the auditory radiation pathway. Throughout this radiation, organization is linear with relation to placement on the organ of Corti, in accordance to the best frequency response (that is, the frequency at which that neuron is most sensitive) of each neuron. However, binaural fusion in the superior olivary complex onward adds significant amounts of information encoded in the signal strength of each ganglion. Thus, the number of tonotopic maps varies between species and the degree of binaural synthesis and separation of sound intensities; in humans, six tonotopic maps have been identified in the primary auditory cortex. their anatomical locations along the auditory cortex.",
            "score": 130.00411987304688
        },
        {
            "docid": "4301708_10",
            "document": "Cochlear nucleus . The cochlear nuclear complex is the first integrative, or processing, stage in the auditory system. Information is brought to the nuclei from the ipsilateral cochlea via the cochlear nerve. Several tasks are performed in the cochlear nuclei. By distributing acoustic input to multiple types of principal cells, the auditory pathway is subdivided into parallel ascending pathways, which can simultaneously extract different types of information. The cells of the ventral cochlear nucleus extract information that is carried by the auditory nerve in the timing of firing and in the pattern of activation of the population of auditory nerve fibers. The cells of the dorsal cochlear nucleus perform a non-linear spectral analysis and place that spectral analysis into the context of the location of the head, ears and shoulders and that separate expected, self-generated spectral cues from more interesting, unexpected spectral cues using input from the auditory cortex, pontine nuclei, trigeminal ganglion and nucleus, dorsal column nuclei and the second dorsal root ganglion. It is likely that these neurons help mammals to use spectral cues for orienting toward those sounds. The information is used by higher brainstem regions to achieve further computational objectives (such as sound source location or improvement in signal to noise ratio). The inputs from these other areas of the brain probably play a role in sound localization.",
            "score": 123.80796813964844
        },
        {
            "docid": "32116125_4",
            "document": "Amblyaudia . Amblyaudia is a deficit in binaural integration of environmental information entering the auditory system. It is a disorder related to brain organization and function rather than what is typically considered a \u201chearing loss\u201d (damage to the cochlea). It may be genetic or developmentally acquired or both. When animals are temporarily deprived of hearing from an early age, profound changes occur in the brain. Specifically, cell sizes in brainstem nuclei are reduced, the configuration of brainstem dendrites are altered and neurons respond in different ways to sounds presented to both the deprived and non-deprived ears (in cases of asymmetric deprivation). This last point is particularly important for listening tasks that require inputs from two ears to perform well. There are multiple auditory functions that rely on the computation of well calibrated inputs from the two ears. Chief among these is the ability to localize sound sources and separate what we want to hear from a background of noise. In the brainstem, the auditory system compares the timing and levels of sounds between the two ears to encode the location of sound sources (sounds that originate from our right as opposed to left side are louder and arrive earlier in our right ear). This ability to separate sound sources not only helps us locate the trajectories of moving objects, but also to separate different sound sources in noisy environments.",
            "score": 117.8951644897461
        },
        {
            "docid": "1085361_2",
            "document": "Inferior colliculus . The inferior colliculus (IC) (Latin for \"lower hill\") is the principal midbrain nucleus of the auditory pathway and receives input from several peripheral brainstem nuclei in the auditory pathway, as well as inputs from the auditory cortex. The inferior colliculus has three subdivisions: the central nucleus, a dorsal cortex by which it is surrounded, and an external cortex which is located laterally. Its bimodal neurons are implicated in auditory-somatosensory interaction, receiving projections from somatosensory nuclei. This multisensory integration may underlie a filtering of self-effected sounds from vocalization, chewing, or respiration activities.",
            "score": 117.36563110351562
        },
        {
            "docid": "47862835_3",
            "document": "Virtual hammock . A network of nerves in the MSO detect the place that a threshold is overcome by sending electrochemical impulses along the auditory nerve pathway to the cortex for higher-level processing. These nerves permit the subconscious localization of a sound source; people can identify the direction from which a sound emanates before they can identify the type of source, which is done when the signals reach the cortex. A sound source produces a propagating sound wave that strikes the ear closer to it before traveling an extra distance to strike the opposite ear. The impulses produced in the auditory nerves which conduct signals to the brain for sound processing pass one another in the MSO at a point on the opposite side relative to the location sound source. If a sound impulse originates from a point equidistant to each ear (e.g. directly behind or in front of the head), the neuronal impulses from each ear will pass at a point in the center of the MSO, allowing us to unconsciously identify where a sound source is located. The Virtual Hammock effect is achieved by intentionally manipulating the passing point by shifting the maximum amplitudes of sound waveforms that are directed into each ear.",
            "score": 116.82405853271484
        },
        {
            "docid": "5051081_4",
            "document": "Eric Knudsen . In 1978, Knudsen and Konishi presented the discovery of an auditory map of space in the midbrain of the barn owl. This discovery was groundbreaking because it unearthed the first non-somatotopic space map in the brain. The map was found in the owl\u2019s midbrain, in the lateral and anterior mesencephalicus lateralis dorsalis (MLD), a structure now referred to as the inferior colliculus. Unlike most sound-localization maps, this map was found to be two-dimensional, with units arranged spatially to represent both the vertical and horizontal location of sound. Knudsen and Konishi discovered that units in this structure respond preferentially to sounds originating in a particular region in space. In the 1978 paper, elevation and azimuth (location in the horizontal plane) were shown to be the two coordinates of the map. Using a speaker set on a rotatable hemispherical track, Knudsen and Konishi presented owls with auditory stimulus from various locations in space and recorded the resulting neuronal activity. They found that neurons in this part of the MLD were organized according to the location of their receptive field, with azimuth varying along the horizontal plane of the space map and elevation varying vertically.  Knudsen followed this discovery with research into specific sound localization mechanisms. Two main auditory cues used by the barn owl to localize sound are interaural time difference (ITD) and interaural intensity difference (IID). The owl\u2019s ears are asymmetric, with the right ear\u2019s opening being directed higher than that of the left. This asymmetry allows the barn owl to determine the elevation of a sound by comparing sound levels between its two ears. Interaural time differences provide the owl with information regarding a sound\u2019s azimuth; sound will reach the ear closer to the sound source before reaching the farther ear, and this time difference can be detected and interpreted as an azimuthal direction. At low frequencies, the wavelength of a sound is wider than the owl's facial ruff, and the ruff does not affect detection of azimuth. At high frequencies, the ruff plays a role in reflecting sound for heightened sensitivity to vertical elevation. Therefore, with wide-band noise, containing both high and low frequencies, the owl could use interaural spectrum difference to obtain information about both azimuth and elevation. In 1979, Knudsen and Konishi showed that the barn owl uses interaural spectrum information in sound localization. They presented owls with both wide-bandwidth noise and pure tones. The birds were able to successfully locate pure tones (since they could still gather information from IID and ITD), but their error rate was much lower when localizing wide-bandwidth noise. This indicates that the birds utilize interaural spectrum differences to improve their accuracy.",
            "score": 115.28868103027344
        },
        {
            "docid": "25260196_25",
            "document": "Neuronal encoding of sound . Primary auditory neurons carry action potentials from the cochlea into the transmission pathway shown in the adjacent image. Multiple relay stations act as integration and processing centers. The signals reach the first level of cortical processing at the primary auditory cortex (A1), in the superior temporal gyrus of the temporal lobe. Most areas up to and including A1 are tonotopically mapped (that is, frequencies are kept in an ordered arrangement). However, A1 participates in coding more complex and abstract aspects of auditory stimuli without coding well the frequency content, including the presence of a distinct sound or its echoes.  Like lower regions, this region of the brain has combination-sensitive neurons that have nonlinear responses to stimuli.",
            "score": 115.05579376220703
        },
        {
            "docid": "19415143_2",
            "document": "Auditory agnosia . Auditory agnosia is a form of agnosia that manifests itself primarily in the inability to recognize or differentiate between sounds. It is not a defect of the ear or \"hearing\", but a neurological inability of the brain to process sound meaning. It is a disruption of the \"what\" pathway in the brain. Persons with auditory agnosia can physically hear the sounds and describe them using unrelated terms, but are unable to recognize them. They might describe the sound of some environmental sounds, such as a motor starting, as resembling a lion roaring, but would not be able to associate the sound with \"car\" or \"engine\", nor would they say that it \"was\" a lion creating the noise. Auditory agnosia is caused by damage to the secondary and tertiary auditory cortex of the temporal lobe of the brain.",
            "score": 114.780517578125
        },
        {
            "docid": "14532984_7",
            "document": "Coincidence detection in neurobiology . Coincidence detection has been shown to be a major factor in sound localization along the azimuth plane in several organisms. In 1948, Lloyd A. Jeffress proposed that some organisms may have a collection of neurons that receive auditory input from each ear. The neural pathways to these neurons are called delay lines. Jeffress claimed that the neurons that the delay lines link act as coincidence detectors by firing maximally when receiving simultaneous inputs from both ears. When a sound is heard, sound waves may reach the ears at different times. This is referred to as the interaural time difference (ITD). Due to differing lengths and a finite conduction speed within the axons of the delay lines, different coincidence detector neurons will fire when sound comes from different positions along the azimuth. Jeffress' model proposes that two signals even from an asynchronous arrival of sound in the cochlea of each ear will converge synchronously on a coincidence detector in the auditory cortex based on the magnitude of the ITD (Fig. 2). Therefore, the ITD should correspond to an anatomical map that can be found within the brain. Masakazu Konishi's study on barn owls shows that this is true. Sensory information from the hair cells of the ears travels to the ipsilateral nucleus magnocellularis. From here, the signals project ipsilaterally and contralaterally to two nucleus laminari. Each nucleus laminaris contains coincidence detectors that receive auditory input from the left and the right ear. Since the ipsilateral axons enter the nucleus laminaris dorsally while the contralateral axons enter ventrally, sounds from various positions along the azimuth correspond directly to stimulation of different depths of the nucleus laminaris. From this information, a neural map of auditory space was formed. The function of the nucleus laminaris parallels that of the medial superior olive in mammals.",
            "score": 114.45912170410156
        },
        {
            "docid": "56439577_40",
            "document": "Temporal envelope and fine structure . Psychophysical studies have suggested that degraded TFS processing due to age and hearing loss may underlie some suprathreshold deficits, such as speech perception; however, debate remains about the underlying neural correlates. The strength of phase locking to the temporal fine structure of signals (TFS) in quiet listening conditions remains normal in peripheral single-neuron responses following cochlear hearing loss. Although these data suggest that the fundamental ability of auditory-nerve fibers to follow the rapid fluctuations of sound remains intact following cochlear hearing loss, deficits in phase locking strength do emerge in background noise. This finding, which is consistent with the common observation that listeners with cochlear hearing loss have more difficulty in noisy conditions, results from reduced cochlear frequency selectivity associated with outer-hair-cell dysfunction. \u00a0Although only limited effects of age and hearing loss have been observed in terms of TFS coding strength of narrowband sounds, more dramatic deficits have been observed in TFS coding quality in response to broadband sounds, which are more relevant for everyday listening. \u00a0A dramatic loss of tonotopicity can occur following noise induced hearing loss, where auditory-nerve fibers that should be responding to mid frequencies (e.g., 2\u20134\u00a0kHz) have dominant TFS responses to lower frequencies (e.g., 700\u00a0Hz). \u00a0Notably, the loss of tonotopicity generally occurs only for TFS coding but not for ENV coding, which is consistent with greater perceptual deficits in TFS processing. This tonotopic degradation is likely to have important implications for speech perception, and can account for degraded coding of vowels following noise-induced hearing loss in which most of the cochlea responds to only the first formant, eliminating the normal tonotopic representation of the second and third formants.",
            "score": 113.64163208007812
        },
        {
            "docid": "6894544_29",
            "document": "Noise-induced hearing loss . NIHL occurs when too much sound intensity is transmitted into and through the auditory system. An acoustic signal from a sound source, such as a radio, enters into the external auditory canal (ear canal), and is funneled through to the tympanic membrane (eardrum), causing it to vibrate. The vibration of the tympanic membrane drives the middle ear ossicles, the malleus, incus, and stapes into motion. The middle ear ossicles transfer mechanical energy to the cochlea by way of the stapes footplate hammering against the oval window of the cochlea. This hammering causes the fluid within the cochlea (perilymph and endolymph) to be displaced. Displacement of the fluid causes movement of the hair cells (sensory cells in the cochlea) and an electrical signal to be sent from the auditory nerve (CN VIII) to the central auditory system within the brain. This is where sound is perceived. Different groups of hair cells are responsive to different frequencies. Hair cells at or near the base of the cochlea are most sensitive to higher frequency sounds while those at the apex are most sensitive to lower frequency sounds. There are two known biological mechanisms of NIHL from excessive sound intensity: damage to the hair cells and damage to the myelination or synaptic regions of auditory nerves.",
            "score": 112.43768310546875
        },
        {
            "docid": "25140_41",
            "document": "Perception . Hearing (or \"audition\") is the ability to perceive sound by detecting vibrations. Frequencies capable of being heard by humans are called audio or \"sonic\". The range is typically considered to be between 20\u00a0Hz and 20,000\u00a0Hz. Frequencies higher than audio are referred to as ultrasonic, while frequencies below audio are referred to as infrasonic. The auditory system includes the outer ears which collect and filter sound waves, the middle ear for transforming the sound pressure (impedance matching), and the inner ear which produces neural signals in response to the sound. By the ascending auditory pathway these are led to the primary auditory cortex within the temporal lobe of the human brain, which is where the auditory information arrives in the cerebral cortex and is further processed there.",
            "score": 111.73767852783203
        },
        {
            "docid": "32105732_6",
            "document": "Spatial hearing loss . By the time sound stream representations reach the end of the auditory pathways brainstem inhibition processing ensures that the right pathway is solely responsible for the left ear sounds and the left pathway is solely responsible for the right ear sounds. It is then the responsibility of the auditory cortex (AC) of the right hemisphere (on its own) to map the whole auditory scene. Information about the right auditory hemifield joins with the information about the left hemifield once it has passed through the corpus callosum (CC) - the brain white matter that connects homologous regions of the left and right hemispheres. Some of those with spatial hearing loss are unable to integrate the auditory representations of the left and right hemifields, and consequently are unable to maintain any representation of auditory space.",
            "score": 111.3084716796875
        },
        {
            "docid": "49604_30",
            "document": "Hearing loss . Sound travels to the brain: as follows: sound waves reach the outer ear and are conducted down the ear canal to the eardrum. The sound waves cause the eardrum to vibrate. These vibrations are passed through the 3 tiny ear bones in the middle ear, which transfer the vibrations to the fluid in the inner ear. The fluid moves hair cells, and the movement of the hair cells converts the vibrations into nerve impulses, which are then taken to the brain by the auditory nerve. The auditory nerve takes the impulses to the brainstem, which sends the impulses to the midbrain. Finally, the signal goes to the auditory cortex of the temporal lobe to be interpreted as sound.",
            "score": 110.6373519897461
        },
        {
            "docid": "56439577_34",
            "document": "Temporal envelope and fine structure . The effects of hearing loss and age on neural coding are generally believed to be smaller for slowly varying envelope responses (i.e., ENV) than for rapidly varying temporal fine structure (i.e., TFS). Enhanced ENV coding following noise-induced hearing loss has been observed in peripheral auditory responses from single neurons and in central evoked responses from the auditory midbrain. The enhancement in ENV coding of narrowband sounds occurs across the full range of modulation frequencies encoded by single neurons. For broadband sounds, the range of modulation frequencies encoded in impaired responses is broader than normal (extending to higher frequencies), as expected from reduced frequency selectivity associated with outer-hair-cell dysfunction. The enhancement observed in neural envelope responses is consistent with enhanced auditory perception of modulations following cochlear damage, which is commonly believed to result from loss of cochlear compression that occurs with outer-hair-cell dysfunction due to age or noise overexposure. \u00a0However, the influence of inner-hair-cell dysfunction (e.g., shallower response growth for mild-moderate damage and steeper growth for severe damage) can confound the effects of outer-hair-cell dysfunction on overall response growth and thus ENV coding. Thus, not surprisingly the relative effects of outer-hair-cell and inner-hair-cell dysfunction have been predicted with modeling to create individual differences in speech intelligibility based on the strength of envelope coding of speech relative to noise.",
            "score": 109.82453155517578
        },
        {
            "docid": "994097_9",
            "document": "Auditory cortex . Neurons in the auditory cortex are organized according to the frequency of sound to which they respond best. Neurons at one end of the auditory cortex respond best to low frequencies; neurons at the other respond best to high frequencies. There are multiple auditory areas (much like the multiple areas in the visual cortex), which can be distinguished anatomically and on the basis that they contain a complete \"frequency map.\" The purpose of this frequency map (known as a tonotopic map) is unknown, and is likely to reflect the fact that the cochlea is arranged according to sound frequency. The auditory cortex is involved in tasks such as identifying and segregating \"auditory objects\" and identifying the location of a sound in space. For example, it has been shown that A1 encodes complex and abstract aspects of auditory stimuli without encoding their \"raw\" aspects like frequency content, presence of a distinct sound or its echoes.",
            "score": 108.90580749511719
        },
        {
            "docid": "14942624_38",
            "document": "Vocal learning . An auditory pathway that is used for auditory learning brings auditory information into the vocal pathway, but the auditory pathway is not unique to vocal learners. Ear hair cells project to cochlear ganglia neurons to auditory pontine nuclei to midbrain and thalamic nuclei and to primary and secondary pallial areas. A descending auditory feedback pathway exists projecting from the dorsal nidopallium to the intermediate arcopallium to shell regions around the thalamic and midbrain auditory nuclei. Remaining unclear is the source of auditory input into the vocal pathways described above. It is hypothesized that songs are processed in these areas in a hierarchical manner, with the primary pallial area responsible for acoustic features (field L2), the secondary pallial area (fields L1 and L3 as well as the caudal medial nidopallium or NCM) determining sequencing and discrimination, and the highest station, the caudal mesopallium (CM), modulating fine discrimination of sounds. Secondary pallial areas including the NCM and CM are also thought to be involved in auditory memory formation of songs used for vocal learning, but more evidence is needed to substantiate this hypothesis.",
            "score": 106.95027160644531
        },
        {
            "docid": "35075711_26",
            "document": "Spontaneous recovery . The pathway of recall associated with the retrieval of sound memories is the auditory system. Within the auditory system is the auditory cortex, which can be broken down into the primary auditory cortex and the belt areas. The primary auditory cortex is the main region of the brain that processes sound and is located on the superior temporal gyrus in the temporal lobe where it receives point-to-point input from the medial geniculate nucleus. From this, the primary auditory complex had a topographic map of the cochlea. The belt areas of the auditory complex receive more diffuse input from peripheral areas of the medial geniculate nucleus and therefore are less precise in tonotopic organization compared to the primary visual cortex. A 2001 study by Trama examined how different kinds of brain damage interfere with normal perception of music. One of his studied patients lost most of his auditory cortex to strokes, allowing him to still hear but making it difficult to understand music since he could not recognize harmonic patterns. Detecting a similarity between speech perception and sound perception, spontaneous recovery of lost auditory information is possible in those patients who have experienced a stroke or other major head trauma. Amusia is a disorder manifesting itself as a defect in processing pitch but also affects one's memory and recognition for music.",
            "score": 106.71978759765625
        },
        {
            "docid": "25260196_3",
            "document": "Neuronal encoding of sound . This article explores the basic physiological principles of sound perception, and traces hearing mechanisms from sound as pressure waves in air to the transduction of these waves into electrical impulses (action potentials) along auditory nerve fibers, and further processing in the brain.",
            "score": 105.36412811279297
        },
        {
            "docid": "56439577_43",
            "document": "Temporal envelope and fine structure . The coding of temporal information in the auditory nerve can be disrupted by two main mechanisms: reduced synchrony and loss of synapses and/or auditory nerve fibers. The impact of disrupted temporal coding on human auditory perception has been explored using physiologically inspired signal-processing tools. The reduction in neural synchrony has been simulated by jittering the phases of the multiple frequency components in speech, although this has undesired effects in the spectral domain. The loss of auditory nerve fibers or synapses has been simulated by assuming (i) that each afferent fiber operates as a stochastic sampler of the sound waveform, with greater probability of firing for higher-intensity and sustained sound features than for lower-intensity or transient features, and (ii) that deafferentation can be modeled by reducing the number of samplers. However, this also has undesired effects in the spectral domain. Both jittering and stochastic undersampling degrade the representation of the TFS more than the representation of the ENV. Both jittering and stochastic undersampling impair the recognition of speech in noisy backgrounds without degrading recognition in silence, support the argument that TFS is important for recognizing speech in noise. Both jittering and stochastic undersampling mimic the effects of aging on speech perception.",
            "score": 104.63265228271484
        },
        {
            "docid": "8953842_15",
            "document": "Computational auditory scene analysis . While many models consider the audio signal as a complex combination of different frequencies, modeling the auditory system can also require consideration for the neural components. By taking a holistic process, where a stream (of feature-based sounds) correspond to neuronal activity distributed in many brain areas, the perception of the sound could be mapped and modeled. Two different solutions have been proposed to the binding of the audio perception and the area in the brain. Hierarchical coding models many cells to encode all possible combinations of features and objects in the auditory scene. Temporal or oscillatory correlation addressing the binding problem by focusing on the synchrony and desynchrony between neural oscillations to encode the state of binding among the auditory features. These two solutions are very similar to the debacle between place coding and temporal coding. While drawing from modeling neural components, another phenomenon of ASA comes into play with CASA systems: the extent of modeling neural mechanisms. The studies of CASA systems have involved modeling some known mechanisms, such as the bandpass nature of cochlear filtering and random auditory nerve firing patterns, however, these models may not lead to finding new mechanisms, but rather give an understanding of purpose to the known mechanisms.",
            "score": 103.13035583496094
        },
        {
            "docid": "56439577_8",
            "document": "Temporal envelope and fine structure . Responses to the temporal-envelope cues of speech or other complex sounds persist up the auditory pathway, eventually to the various fields of the auditory cortex in many animals. In the Primary Auditory Cortex, responses can encode AM rates by phase-locking up to about 20\u201330\u00a0Hz, while faster rates induce sustained and often tuned responses. A topographical representation of AM rate has been demonstrated in the primary auditory cortex of awake macaques. This representation is approximately perpendicular to the axis of the tonotopic gradient, consistent with an orthogonal organization of spectral and temporal features in the auditory cortex. Combining these temporal responses with the spectral selectivity of A1 neurons gives rise to the spectro-temporal receptive fields that often capture well cortical responses to complex modulated sounds. In secondary auditory cortical fields, responses become temporally more sluggish and spectrally broader, but are still able to phase-lock to the salient features of speech and musical sounds. Tuning to AM rates below about 64\u00a0Hz is also found in the human auditory cortex as revealed by brain-imaging techniques (fMRI) and cortical recordings in epileptic patients (electrocorticography). This is consistent with neuropsychological studies of brain-damaged patients and with the notion that the central auditory system performs some form of spectral decomposition of the ENVp of incoming sounds. Interestingly, the ranges over which cortical responses encode well the temporal-envelope cues of speech have been shown to be predictive of the human ability to understand speech. In the human superior temporal gyrus (STG), an anterior-posterior spatial organization of spectro-temporal modulation tuning has been found in response to speech sounds, the posterior STG being tuned for temporally fast varying speech sounds with low spectral modulations and the anterior STG being tuned for temporally slow varying speech sounds with high spectral modulations.",
            "score": 103.0771484375
        },
        {
            "docid": "994097_8",
            "document": "Auditory cortex . As with other primary sensory cortical areas, auditory sensations reach perception only if received and processed by a cortical area. Evidence for this comes from lesion studies in human patients who have sustained damage to cortical areas through tumors or strokes, or from animal experiments in which cortical areas were deactivated by surgical lesions or other methods. Damage to the Primary Auditory Cortex in humans leads to a loss of any awareness of sound, but an ability to react reflexively to sounds remains as there is a great deal of subcortical processing in the auditory brainstem and midbrain.",
            "score": 102.01715087890625
        },
        {
            "docid": "25259743_16",
            "document": "Cortical cooling . To determine what parts of the auditory cortex contribute to sound localization, investigators implanted cryoloops to deactivate the 13 known regions of acoustically responsive cortex of the cat. Cats learned to make an orienting response by moving their heads and approaching a 100-ms broad-band noise stimulus emitted from a central speaker or one of 12 peripheral speakers located at 15\u00b0 intervals from left 90\u00b0 to right 90\u00b0along the horizontal plane after attending to a central visual stimulus generated by a red LED. After the cats had reached at least 80% accuracy in identifying the location of the sound stimulus, each was implanted with one or two bilateral pairs of cryoloops over the different sections of the auditory cortex; 10 sections were defined. Cryoloops were turned on so that the loops reached a temperature of 3\u00b0C (plus or minus 1\u00b0C), first unilaterally, then bilaterally, next unilaterally on the other side, and finally baseline task performance was recorded after recovering from cooling. This cycle was repeated several times for each cat.  Of the 10 sections that were deactivated, only deactivation of 3 sections, the AI (primary auditory cortex)/DZ (dorsal zone), PAF (posterior auditory field), and AES (anterior ectosylvian sulcus) sections, were found to have an effect on sound localization. At baseline, cats were able to locate 90% of the sound stimuli. Unilateral deactivation of any one of these sections resulted in a contralateral impairment in sound localization, or 10% accuracy. Bilateral deactivation of any combination of these three sections resulted in a 180\u00b0 deficit to 10% of sound locations identified, although this accuracy implied that cats were still able to orient to the hemifield where the sound occurred above chance (7.7%).  Since the primary auditory cortex and dorsal zone were concurrently cooled, the investigators performed another study in which the AI and DZ were examined as separate entities to further establish the sections of auditory cortex contributing to sound localization. The experimental design was the same as the above-mentioned design with the exception that only the AI and DZ sections were implanted with separate cryoloops. Again, it was found that unilateral simultaneous cooling deactivation of the AI and DZ generated contralateral sound localization deficits while bilateral deactivation created a deficit in both hemifields (10% sound location identification). Bilateral deactivation of AI alone resulted in only 45% accuracy within 30\u00b0 of the target. Bilateral deactivation of DZ resulted in 60% accuracy but with larger errors, often into the hemifield opposite the target. Therefore, AZ deactivation produces a higher number of small errors while deactivation of DZ leads to larger but fewer errors. This finding that AI and DZ deactivation produce partial deficits in sound localization implies that the previous finding that PAF and AES deactivation have more considerable contributions to sound localization than either the AI or DZ.",
            "score": 101.68864440917969
        },
        {
            "docid": "35988494_2",
            "document": "Selective auditory attention . Selective auditory attention or selective hearing is a type of selective attention and involves the auditory system of the nervous system. Selective hearing is characterized as the action in which people focus their attention on a specific source of a sound or spoken words. The sounds and noise in the surrounding environment is heard by the auditory system but only certain parts of the auditory information are processed in the brain. Most often, auditory attention is directed at things people are most interested in hearing. In an article by Krans, Isbell, Giuliano, and Neville (2013), selective auditory attention is defined as the ability to acknowledge some stimuli while ignoring other stimuli that is occurring at the same time. An example of this is a student focusing on a teacher giving a lesson and ignoring the sounds of classmates in a rowdy classroom (p.\u00a053). This is an example of bottlenecking which means that information cannot be processed simultaneously so only some sensory information gets through the \"bottleneck\" and is processed. A brain simply cannot process all sensory information that is occurring in an environment so only that which is most important is thoroughly processed. Selective hearing is not a physiological disorder but rather it is the capability of humans to block out sounds and noise. It is the notion of ignoring certain things in the surrounding environment. Over the years, there has been increased research in the selectivity of auditory attention, namely selective hearing.",
            "score": 101.35418701171875
        },
        {
            "docid": "1719638_4",
            "document": "Law of specific nerve energies . As the above quotation shows, M\u00fcller's law seems to differ from the modern statement of the law in one key way. M\u00fcller attributed the quality of an experience to some specific quality of the energy in the nerves. For example, the visual experience from light shining into the eye, or from a poke in the eye, arises from some special quality of the energy carried by optic nerve, and the auditory experience from sound coming into the ear, or from electrical stimulation of the cochlea, arises from some different, special quality of the energy carried by the auditory nerve. In 1912, Lord Edgar Douglas Adrian showed that all neurons carry the same energy, electrical energy in the form of action potentials. That means that the quality of an experience depends on the part of the brain to which nerves deliver their action potentials (e.g., light from nerves arriving at the visual cortex and sound from nerves arriving at the auditory cortex).",
            "score": 101.05361938476562
        },
        {
            "docid": "14405771_9",
            "document": "Speech science . Speech perception refers to the understanding of speech. The beginning of the process towards understanding speech is first hearing the message that is spoken. The auditory system receives sound signals starting at the outer ear. They enter the pinna and continue into the external auditory canal (ear canal) and then to the eardrum. Once in the middle ear, which consists of the malleus, the incus, and the stapes; the sounds are changed into mechanical energy. After being converted into mechanical energy, the message reaches the oval window, which is the beginning of the inner ear. Once inside the inner ear, the message is transferred into hydraulic energy by going through the cochlea, which is filled with fluid, and on to the Organ of Corti. This organ again helps the sound to be transferred into a neural impulse that stimulates the auditory pathway and reaches the brain. Sound is then processed in Heschl's gyrus and associated with meaning in Wernicke's area. As for theories of speech perception, there are a motor and an auditory theory. The motor theory is based upon the premise that speech sounds are encoded in the acoustic signal rather than enciphered in it. The auditory theory puts greater emphasis on the sensory and filtering mechanisms of the listener and suggests that speech knowledge is a minor role that\u2019s only used in hard perceptual conditions.",
            "score": 100.84881591796875
        },
        {
            "docid": "5442380_15",
            "document": "Sensory cue . The auditory system of humans and animals allows individuals to assimilate information from the surroundings, represented as sound waves. Sound waves first pass through the pinnae and the auditory canal, the parts of the ear that comprise the outer ear. Sound then reaches the tympanic membrane in the middle ear (also known as the eardrum). The tympanic membrane sets the malleus, incus, and stapes into vibration. The stapes transmits these vibrations to the inner ear by pushing on the membrane covering the oval window, which separates the middle and inner ear. The inner ear contains the cochlea, the liquid-filled structure containing the hair cells. These cells serve to transform the incoming vibration to electrical signals, which can then be transmitted to the brain. The auditory nerve carries the signal generated by the hair cells away from the inner ear and towards the auditory receiving area in the cortex. The signal then travels through fibers to several subcortical structures and then to the primary auditory receiving area in the temporal lobe",
            "score": 100.70626831054688
        },
        {
            "docid": "6766895_8",
            "document": "Topographic map (neuroanatomy) . The auditory system is the sensory system for hearing in which the brain interprets information from the frequency of sound waves, yielding the perception of tones. Sound waves enter the ear through the auditory canal. These waves arrive at the eardrum where the properties of the waves are transduced into vibrations. The vibrations travel through the bones of the inner ear to the cochlea. In the cochlea, the vibrations are transduced into electrical information through the firing of hair cells in the organ of Corti. The organ of Corti projects in an orderly fashion to structures in the brainstem (namely, the cochlear nuclei and the inferior colliculus), and from there to the medial geniculate nucleus of the thalamus and the primary auditory cortex. Adjacent sites on the organ of Corti, which are themselves selective for the sound frequency, are represented by adjacent neurons in the aforementioned CNS structures. This projection pattern has been termed tonotopy.",
            "score": 100.22651672363281
        },
        {
            "docid": "22822496_19",
            "document": "Tinnitus masker . These masker devices use soothing natural sounds such as ocean surf, rainfall or synthetic sounds such as white noise, pink noise, or brown noise to help the auditory system become less sensitive to tinnitus and promote relaxation by reducing the contrast between tinnitus sounds and background sound.",
            "score": 100.14014434814453
        },
        {
            "docid": "8953842_8",
            "document": "Computational auditory scene analysis . Because the ears receive audio signals at different times, the sound source can be determined by using the delays retrieved from the two ears. By cross-correlating the delays from the left and right channels (of the model), the coincided peaks can be categorized as the same localized sound, despite their temporal location in the input signal.  The use of interaural cross-correlation mechanism has been supported through physiological studies, paralleling the arrangement of neurons in the auditory midbrain.",
            "score": 100.06954193115234
        },
        {
            "docid": "1947410_29",
            "document": "Critical period . In a related study, Barkat, Polley and Hensch (2011) looked at how exposure to different sound frequencies influences the development of the tonotopic map in the primary auditory cortex and the ventral medical geniculate body. In this experiment, mice were reared either in normal environments or in the presence of 7\u00a0kHz tones during early postnatal days. They found that mice that were exposed to an abnormal auditory environment during a critical period P11- P15 had an atypical tonotopic map in the primary auditory cortex. These studies support the notion that exposure to certain sounds within the critical period can influence the development of tonotopic maps and the response properties of neurons. Critical periods are important for the development of the brain for the function from a pattern of connectivity. In general, the early auditory environment influences the structural development and response specificity of the primary auditory cortex.",
            "score": 99.89726257324219
        }
    ]
}