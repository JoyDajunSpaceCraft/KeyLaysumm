{
    "q": [
        {
            "docid": "40841348_9",
            "document": "Computational and Statistical Genetics . Over the past few years, genome-wide association studies (GWAS) have become a powerful tool for investigating the genetic basis of common diseases and has improved our understanding of the genetic basis of many complex traits. Traditional single SNP (single-nucleotide polymorphism) GWAS is the most commonly used method to find trait associated DNA sequence variants - associations between variants and one or more phenotypes of interest are investigated by studying individuals with different phenotypes and examining their genotypes at the position of each SNP individually. The SNPs for which one variant is statistically more common in individuals belonging to one phenotypic group are then reported as being associated with the phenotype. However, most complex common diseases involve small population-level contributions from multiple genomic loci. To detect such small effects as genome-wide significant, traditional GWAS rely on increased sample size e.g. to detect an effect which accounts for 0.1% of total variance, traditional GWAS needs to sample almost 30,000 individuals. Although the development of high throughput SNP genotyping technologies has lowered the cost and improved the efficiency of genotyping. Performing such a large scale study still costs considerable money and time. Recently, association analysis methods utilizing gene-based tests have been proposed that are based on the fact that variations in protein-coding and adjacent regulatory regions are more likely to have functional relevance. These methods have the advantage that they can account for multiple independent functional variants within a gene, with the potential to greatly increase the power to identify disease/trait associated genes. Also, imputation of ungenotyped markers using known reference panels(e.g. HapMap and the 1000 Genomes Project) predicts genotypes at the missing or untyped markers thereby allowing one to accurately evaluate the evidence for association at genetic markers that are not directly genotyped (in addition to the typed markers) and has been shown to improve the power of GWAS to detect disease associated loci.",
            "score": 92.41939723491669
        },
        {
            "docid": "22921_61",
            "document": "Psychology . All researched psychological traits are influenced by both genes and environment, to varying degrees. These two sources of influence are often confounded in observational research of individuals or families. An example is the transmission of depression from a depressed mother to her offspring. Theory may hold that the offspring, by virtue of having a depressed mother in his or her (the offspring's) environment, is at risk for developing depression. However, risk for depression is also influenced to some extent by genes. The mother may both carry genes that contribute to her depression but will also have passed those genes on to her offspring thus increasing the offspring's risk for depression. Genes and environment in this simple transmission model are completely confounded. Experimental and quasi-experimental behavioral genetic research uses genetic methodologies to disentangle this confound and understand the nature and origins of individual differences in behavior. Traditionally this research has been conducted using twin studies and adoption studies, two designs where genetic and environmental influences can be partially un-confounded. More recently, the availability of microarray molecular genetic or genome sequencing technologies allows researchers to measure participant DNA variation directly, and test whether individual genetic variants within genes are associated with psychological traits and psychopathology through methods including genome-wide association studies. One goal of such research is similar to that in positional cloning and its success in Huntington's: once a causal gene is discovered biological research can be conducted to understand how that gene influences the phenotype. One major result of genetic association studies is the general finding that psychological traits and psychopathology, as well as complex medical diseases, are highly polygenic, where a large number (on the order of hundreds to thousands) of genetic variants, each of small effect, contribute to individual differences in the behavioral trait or propensity to the disorder. Active research continues to understand the genetic and environmental bases of behavior and their interaction.",
            "score": 55.818464279174805
        },
        {
            "docid": "26418006_19",
            "document": "Exome sequencing . Rare recessive disorders would not have single nucleotide polymorphisms (SNPs) in public databases such as dbSNP. More common recessive phenotypes may have disease-causing variants reported in dbSNP. For example, the most common cystic fibrosis variant has an allele frequency of about 3% in most populations. Screening out such variants might erroneously exclude such genes from consideration. Genes for recessive disorders are usually easier to identify than dominant disorders because the genes are less likely to have more than one rare nonsynonymous variant. The system that screens common genetic variants relies on dbSNP which may not have accurate information about the variation of alleles. Using lists of common variation from a study exome or genome-wide sequenced individual would be more reliable. A challenge in this approach is that as the number of exomes sequenced increases, dbSNP will also increase in the number of uncommon variants. It will be necessary to develop thresholds to define the common variants that are unlikely to be associated with a disease phenotype.",
            "score": 46.10192954540253
        },
        {
            "docid": "26418006_4",
            "document": "Exome sequencing . Exome sequencing is especially effective in the study of rare Mendelian diseases, because it is an efficient way to identify the genetic variants in all of an individual's genes. These diseases are most often caused by very rare genetic variants that are only present in a tiny number of individuals; by contrast, techniques such as SNP arrays can only detect shared genetic variants that are common to many individuals in the wider population. Furthermore, because severe disease-causing variants are much more likely (but by no means exclusively) to be in the protein coding sequence, focusing on this 1% costs far less than whole genome sequencing but still detects a high yield of relevant variants.",
            "score": 36.35735297203064
        },
        {
            "docid": "21147_42",
            "document": "Natural selection . A portion of all genetic variation is functionally neutral, producing no phenotypic effect or significant difference in fitness. Motoo Kimura's neutral theory of molecular evolution by genetic drift proposes that this variation accounts for a large fraction of observed genetic diversity. Neutral events can radically reduce genetic variation through population bottlenecks. which among other things can cause the founder effect in initially small new populations. When genetic variation does not result in differences in fitness, selection cannot directly affect the frequency of such variation. As a result, the genetic variation at those sites is higher than at sites where variation does influence fitness. However, after a period with no new mutations, the genetic variation at these sites is eliminated due to genetic drift. Natural selection reduces genetic variation by eliminating maladapted individuals, and consequently the mutations that caused the maladaptation. At the same time, new mutations occur, resulting in a mutation\u2013selection balance. The exact outcome of the two processes depends both on the rate at which new mutations occur and on the strength of the natural selection, which is a function of how unfavourable the mutation proves to be.",
            "score": 31.03291666507721
        },
        {
            "docid": "2652348_3",
            "document": "Imaging genetics . Imaging genetics uses research approaches in which genetic information and fMRI data in the same subjects are combined to define neuro-mechanisms linked to genetic variation. With the images and genetic information, it can be determined how individual differences in single nucleotide polymorphisms, or SNPs, lead to differences in brain wiring structure, and intellectual function. Imaging genetics allows the direct observation of the link between genes and brain activity in which the overall idea is that common variants in SNPs lead to common diseases. A neuroimaging phenotype is attractive because it is closer to the biology of genetic function than illnesses or cognitive phenotypes.",
            "score": 17.412326335906982
        },
        {
            "docid": "45497515_8",
            "document": "Genotype-first approach . Several methods can be used with a genotype-first approach, however, the following steps are usually included: The genotyping is generated using next-generation sequencing technologies (including whole-genome sequencing and exome sequencing) and microarray analyses. The raw data is then statistically analyzed for population-based frequency of the variants. Common variants are filtered out, and pathogenicity is determined though predicted genetic implications. These steps allow for the identification of presumed highly penetrant variants and their specific locus. The selected variants are usually resequenced for validation (by targeted Sanger sequencing). Validated genomic variants can then be analyzed for recurrences among affected individuals within the cohort. Pathogenicity of a genomic variant is statistically based on its significantly abundant presence in the affected compared to the unaffected individuals, not exclusively on the deleteriousness of the variant. A candidate variant can then be associated with a shared phenotype with the aspiration that as more patients baring the same variant with the same phenotype will be identified, a stronger association can be made.  Finally, delineation is made between a specific variant to associated clinical phenotypes [Figure 1].",
            "score": 62.27457880973816
        },
        {
            "docid": "6003871_11",
            "document": "Tag SNP . Genome-wide studies are predicated on the common disease-common variant (CD/CV) hypothesis which states that common disorders are influenced by common genetic variation. Effect size (penetrance) of the common variants needs to be smaller relative to those found in rare disorders. That means that the common SNP can explain only a small portion of the variance due to genetic factors and that common diseases are influenced by multiple common alleles of small effect size. Another hypothesis is that common diseases are caused by rare variants that are synthetically linked to common variants. In that case the signal produced from GWAS is an indirect (synthetic) association between one or more rare causal variants in linkage disequilibrium. It is important to recognize that this phenomenon is possible when selecting a group for tag SNPs. When a disease is found to be associated with a haplotype, some SNPs in that haplotype will have synthetic association with the disease. To pinpoint the causal SNPs we need a greater resolution in the selection of haplotype blocks. Since whole genome sequencing technologies are rapidly changing and becoming less expensive it is likely that they will replace the current genotyping technologies providing the resolution needed to pinpoint causal variants.",
            "score": 45.39921033382416
        },
        {
            "docid": "43765116_2",
            "document": "Rare functional variant . A rare functional variant is a genetic variant which alters gene function, and which occurs at low frequency in a population. Rare variants may play a significant role in complex disease, as well as some Mendelian conditions. Rare variants may be responsible for a portion of the missing heritability of complex diseases. The theoretical case for a significant role of rare variants is that alleles that strongly predispose an individual to disease will be kept at low frequencies in populations by purifying selection. Rare variants are increasingly being studied, as a consequence of exome and whole genome sequencing efforts. While these variants are individually infrequent in populations, there are many in human populations, and they can be unique to specific populations. They more likely to be deleterious than common variants, as a result of rapid population growth and weak purifying selection. They have been suspected of acting independently or along with common variants to cause disease states.",
            "score": 36.32294988632202
        },
        {
            "docid": "50619245_31",
            "document": "Human disease modifier gene . Zuk et al. argue that biomedical research should focus on interacting molecular mechanisms of genetic variants already discovered. They argue that genetic interactions are common. Widely used estimates of narrow-sense heritability, the ratio of additive genetic variance (calculated from measured effects of known variants) to total phenotypic variance (inferred from population data) assume additive effects in the heritability effects of variants. For most human traits, this fraction remains below 0.2. They argue that the assumption that heritability effects are additive overestimates heritability attributable to all genetic variation underlying disease, and thus underestimates the fraction of heritability attributable to already-discovered ones. Thus, they argue, research should focus on the molecular bases of already-discovered variants, as estimates of discovered sources of heritability are flawed, and there is insufficient evidence that undiscovered genetic modifiers exist. Furthermore, \"the proportion of phenotypic variance explained by a variant in the human population is a notoriously poor predictor of the importance of the gene for biology or medicine,\" and effective therapeutics may target products of genes that explain very little of the clinically-observed variation in phenotype.",
            "score": 27.819380044937134
        },
        {
            "docid": "50518079_4",
            "document": "Human Genome Structural Variation . Human genetic variation is responsible for the phenotypic differences between individuals in the human population. There are different types of genetic variation and it is studied extensively in order to better understand its significance. These studies lead to discoveries associating genetic variants to certain phenotypes as well as their implications in disease. At first, before DNA sequencing technologies, variation was studied and observed exclusively at a microscopic scale. At this scale, the only observations made were differences in chromosome number and chromosome structure. These variants that are about 3 Mb or larger in size are considered microscopic structural variants. This scale is large enough to be visualized using a microscope and include aneuploidies, heteromorphisms, and chromosomal rearrangements. When DNA sequencing was introduced, it opened the door to finding smaller and incredibly more sequence variations including SNPs and minisatellites. This also includes small inversions, duplications, insertions, and deletions that are under 1 kb in size. In the human genome project the human genome was successfully sequenced, which provided a reference human genome for comparison of genetic variation. With improving sequencing technologies and the reference genome, more and more variations were found of several different sizes that were larger than 1 kb but smaller than microscopic variants. These variants ranging from about 1 kb to 3 Mb in size are considered submicroscopic structural variants. These recently discovered structural variants are thought to play a very significant role in phenotypic diversity and disease susceptibility.",
            "score": 54.107425928115845
        },
        {
            "docid": "149306_10",
            "document": "National Center for Biotechnology Information . The Entrez Global Query Cross-Database Search System is used at NCBI for all the major databases such as Nucleotide and Protein Sequences, Protein Structures, PubMed, Taxonomy, Complete Genomes, OMIM, and several others. Entrez is both indexing and retrieval system having data from various sources for biomedical research. NCBI distributed the first version of Entrez in 1991, composed of nucleotide sequences from PDB and GenBank, protein sequences from SWISS-PROT, translated GenBank, PIR, PRF , PDB and associated abstracts and citations from PubMed. Entrez is specially designed to integrate the data from several different sources, databases and formats into a uniform information model and retrieval system which can efficiently retrieve that relevant references, sequences and structures.",
            "score": 45.98583972454071
        },
        {
            "docid": "24127822_2",
            "document": "Leiden Open Variation Database . The Leiden Open Variation Database (LOVD) is a free, flexible web-based open source database developed in the Leiden University Medical Center in the Netherlands, designed to collect and display variants in the DNA sequence. The focus of an LOVD is usually the combination between a gene and a genetic (heritable) disease. All sequence variants found in individuals are collected in the database, together with information about whether they could be causally connected to the disease (i.e. a disease-causing variant or mutation) or not (i.e. a non-disease causing variant). Specialized doctors (clinical geneticists) use LOVDs to diagnose and advise patients carrying a genetic disease. Ideally, if a patient has been screened for mutations and one has been found, information in LOVD can predict the progress of the disease.",
            "score": 39.12692904472351
        },
        {
            "docid": "52142704_6",
            "document": "Polygenic score . The standard GWAS regression can be improved on using penalized regression methods like the LASSO/ridge regression. (Penalized regression can be interpreted as placing informative priors on how many genetic variants are expected to affect a trait, and the distribution of their effect sizes; Bayesian counterparts exist for LASSO/ridge, and other priors have been suggested & used. They can perform better in some circumstances.) A multi-dataset, multi-method study found that of 15 different methods compared across four datasets, minimum redundancy maximum relevance was the best performing method. Furthermore, variable selection methods tended to outperform other methods. Variable selection methods do not use all the available genomic variants present in a dataset, but attempt to select an optimal subset of variants to use. This leads to less overfitting but more bias (see bias-variance tradeoff).",
            "score": 75.43415069580078
        },
        {
            "docid": "1795200_2",
            "document": "Candidate gene . The candidate gene approach to conducting genetic association studies focuses on associations between genetic variation within pre-specified genes of interest and phenotypes or disease states. This is in contrast to genome-wide association studies (GWAS), which scan the entire genome for common genetic variation. Candidate genes are most often selected for study based on \"a priori\" knowledge of the gene's biological functional impact on the trait or disease in question. The rationale behind focusing on allelic variation in specific, biologically relevant regions of the genome is that certain mutations will directly impact the function of the gene in question, and lead to the phenotype or disease state being investigated. This approach usually uses the case-control study design to try to answer the question, \"Is one allele of a candidate gene more frequently seen in subjects with the disease than in subjects without the disease?\"",
            "score": 46.54667365550995
        },
        {
            "docid": "21147_40",
            "document": "Natural selection . When some component of a trait is heritable, selection alters the frequencies of the different alleles, or variants of the gene that produces the variants of the trait. Selection can be divided into three classes, on the basis of its effect on allele frequencies: directional, stabilizing, and purifying selection. Directional selection occurs when an allele has a greater fitness than others, so that it increases in frequency, gaining an increasing share in the population. This process can continue until the allele is fixed and the entire population shares the fitter phenotype. Far more common is stabilizing selection, which lowers the frequency of alleles that have a deleterious effect on the phenotype \u2013 that is, produce organisms of lower fitness. This process can continue until the allele is eliminated from the population. Purifying selection conserves functional genetic features, such as protein-coding genes or regulatory sequences, over time by selective pressure against deleterious variants.",
            "score": 30.133723378181458
        },
        {
            "docid": "11808249_6",
            "document": "Genome-wide association study . Any two human genomes differ in millions of different ways. There are small variations in the individual nucleotides of the genomes (SNPs) as well as many larger variations, such as deletions, insertions and copy number variations. Any of these may cause alterations in an individual's traits, or phenotype, which can be anything from disease risk to physical properties such as height. Around the year 2000, prior to the introduction of GWA studies, the primary method of investigation was through inheritance studies of genetic linkage in families. This approach had proven highly useful towards single gene disorders. However, for common and complex diseases the results of genetic linkage studies proved hard to reproduce. A suggested alternative to linkage studies was the genetic association study. This study type asks if the allele of a genetic variant is found more often than expected in individuals with the phenotype of interest (e.g. with the disease being studied). Early calculations on statistical power indicated that this approach could be better than linkage studies at detecting weak genetic effects.",
            "score": 68.12551391124725
        },
        {
            "docid": "52142704_3",
            "document": "Polygenic score . Polygenic scores are widely employed in animal, plant, and behavioral genetics for predicting and understanding genetic architectures. In a genome-wide association study (GWAS), polygenic scores having substantially higher predictive performance than the genome-wide statistically-significant hits indicates that the trait in question is affected by a larger number of variants than just the hits and larger sample sizes will yield more hits; a conjunction of low variance explained and high heritability as measured by GCTA, twin studies or other methods, indicates that a trait may be massively polygenic and affected by thousands of variants. Once a polygenic score has been created, which explains at least a few percent of a phenotype's variance and can therefore be assumed to effectively incorporate a significant fraction of the genetic variants affecting that phenotype, it can be used in several different ways: as a lower bound to test whether heritability estimates may be biased; as a measure of genetic overlap of traits (genetic correlation), which might indicate e.g. shared genetic bases for groups of mental disorders; as a means to assess group differences in a trait such as height, or to examine changes in a trait over time due to natural selection indicative of a soft selective sweep (as e.g. for intelligence where the changes in frequency would be too small to detect on each individual hit but not on the overall polygenic score); in Mendelian randomization (assuming no pleiotropy with relevant traits); to detect & control for the presence of genetic confounds in outcomes (e.g. the correlation of schizophrenia with poverty); or to investigate gene\u2013environment interactions.",
            "score": 54.33123552799225
        },
        {
            "docid": "40152800_3",
            "document": "Imputation (genetics) . In genetic epidemiology and quantitative genetics, researchers aim at identifying genomic locations where variation between individuals is associated with variation in traits of interest between the same individuals. Such studies hence require access to the genetic make-up of a set of individuals. Sequencing the whole genome of each individual in the study is often too costly, only a subset of the genome can therefore be measured. This often means, first, only considering single-nucleotide polymorphisms (SNPs) and neglecting copy number variants, and second, only measuring SNPs known to be variable enough in the population so that they are likely to be also variable in the set of individuals under consideration. The most informative subset of SNPs is chosen based on the distribution of common genetic variation along the genome, for instance as produced by the HapMap or the 1000 Genomes Project in humans. These SNPs are then used to build a micro-array, thereby allowing each individual in the study to be genotyped at all these SNPs simultaneously.",
            "score": 47.92666172981262
        },
        {
            "docid": "44260712_12",
            "document": "SNP annotation . Comparative genomics approaches were used to predict the function-relevant variants under the assumption that the functional genetic locus should be conserved across different species at an extensive phylogenetic distance. On the other hand, some adaptive traits and the population differences are driven by positive selections of advantageous variants, and these genetic mutations are functionally relevant to population specific phenotypes. Functional prediction of variants\u2019 effect in different biological processes is pivotal to pinpoint the molecular mechanism of diseases/traits and direct the experimental validation.",
            "score": 25.479252099990845
        },
        {
            "docid": "19031099_2",
            "document": "Allelic heterogeneity . Allelic heterogeneity is the phenomenon in which different mutations at the same locus cause the same phenotype. These allelic variations can arise as a result of natural selection processes, as a result of exogenous mutagens, genetic drift, or genetic migration. Many of these mutations take the form of single nucleotide polymorphisms in which a single nucleotide base is altered compared to a consensus sequence. They can also exist as copy number variants (CNV) in which the copies of a gene or DNA sequence is different from the population.",
            "score": 29.352441549301147
        },
        {
            "docid": "47644103_19",
            "document": "David Hunter (Harvard) . The completion of the Human Genome Project and databases of population genetic variation presents the major challenge of determining how such genetic variants contribute to both human and nonhuman health conditions. Epidemiologists are responsible for both assessing the proportion of specific diseases associated with particular genotypes and how these genotypes interact with environmental and lifestyle factors in contributing causally to disease conditions. Large, population-based cohort studies can be particularly helpful in studying these issues, and Hunter and colleagues are working with colleagues in the Nurses\u2019 Health Studies, Health Professionals Follow-up Study, and the Physicians\u2019 Health Study, to conduct nested case-control studies of cancers, cardiovascular disease, diabetes, and other phenotypes. With collaborators at the National Cancer Institute, they conducted genome-wide association studies (GWAS) of breast cancer and other cancers. These studies are discovering novel genetic variants associated with risk of these cancers, and multiple other phenotypes that have been collected in these studies. For instance, in 2007 they co-discovered the most common genetic variant associated with risk of breast cancer, and in subsequent years other variants associated with breast, prostate, and skin cancer, as well as other phenotypes such as age at menarche, hair color, and other biomarkers.",
            "score": 54.35665822029114
        },
        {
            "docid": "2436363_8",
            "document": "Neontology . Neontology's fundamental theories relies on biological models of natural selection and speciation that connects genes, the unit of heredity with the mechanism of evolution, natural selection. For example, researchers utilized neontological and paleontological datasets to study mouse dentitions compared with human dentitions. In order to understand the underlying genetic mechanisms that influences this variation between nonhuman primates and humans, neontological methods are applied to the research method. By incorporating neontology with different biological research methods, it can become clear how genetic mechanisms underlie major events in things such as primate evolution.",
            "score": 51.5343132019043
        },
        {
            "docid": "37760976_3",
            "document": "Infologs . Typical protein engineering methods rely on screening a high number (10-10 or more) of gene variants to identify individuals with improved activity using a surrogate high throughput screen (HTP) to identify initial hits. Unfortunately, results are defined by what is screened for, thus the \u201chit\u201d from the HTP screen often has very little real activity in a lower throughput assay more indicative of the improved functionality for which the protein is being developed. By adapting the standard algorithms for engineering complex systems to work with biological systems, the resulting process enables researchers to deconvolute how substitutions within a protein sequence modify its function. Combining these algorithms with an integrated query and ranking mechanism allows the identification of appropriate sequence substitutions. Infologs refers to the set of designed genes, singular use Infolog describes an individual variant.",
            "score": 47.44429838657379
        },
        {
            "docid": "5818361_12",
            "document": "Relevance feedback . This automatic technique mostly works. Evidence suggests that it tends to work better than global analysis. Through a query expansion, some relevant documents missed in the initial round can then be retrieved to improve the overall performance. Clearly, the effect of this method strongly relies on the quality of selected expansion terms. It has been found to improve performance in the TREC ad hoc task . But it is not without the dangers of an automatic process. For example, if the query is about copper mines and the top several documents are all about mines in Chile, then there may be query drift in the direction of documents on Chile. In addition, if the words added to the original query are unrelated to the query topic, the quality of the retrieval is likely to be degraded, especially in Web search, where web documents often cover multiple different topics. To improve the quality of expansion words in pseudo-relevance feedback, a positional relevance feedback for pseudo-relevance feedback has been proposed to select from feedback documents those words that are focused on the query topic based on positions of words in feedback documents.  Specifically, the positional relevance model assigns more weights to words occurring closer to query words based on the intuition that words closer to query words are more likely to be related to the query topic.",
            "score": 61.32638144493103
        },
        {
            "docid": "45663583_2",
            "document": "Human interactome . The human interactome is the set of protein\u2013protein interactions (the interactome) that occur in human cells. The sequencing of reference genomes, in particular the Human Genome Project, has revolutionized human genetics, molecular biology, and clinical medicine. Genome-wide association study results have led to the association of genes with most Mendelian disorders, and over 140 000 germline mutations have been associated with at least one genetic disease. However, it became apparent that inherent to these studies is an emphasis on clinical outcome rather than a comprehensive understanding of human disease; indeed to date the most significant contributions of GWAS have been restricted to the \u201clow-hanging fruit\u201d of direct single mutation disorders, prompting a systems biology approach to genomic analysis. The connection between genotype and phenotype (how variation in genotype affects the disease or normal functioning of the cell and the human body) remain elusive, especially in the context of multigenic complex traits and cancer. To assign functional context to genotypic changes, much of recent research efforts have been devoted to the mapping of the networks formed by interactions of cellular and genetic components in humans, as well as how these networks are altered by genetic and somatic disease.",
            "score": 31.201372623443604
        },
        {
            "docid": "41254911_4",
            "document": "Joseph Buxbaum . Buxbaum's research focuses on using genetic and functional methods to identify and characterize genes and pathways involved in autism, schizophrenia, and Alzheimer disease. His research focuses on common and rare genetic variation in neuropsychiatric disorders and has been an early leader in rare genetic variation in psychiatry. With the advent of massively parallel sequencing, this focus on rare variation is now providing profound insights into many neuropsychiatric disorders, including autism, intellectual disability, schizophrenia, bipolar disorder, and Alzheimer disease.",
            "score": 34.196547746658325
        },
        {
            "docid": "34142465_10",
            "document": "Linkage based QTL mapping . Linkage and association analysis are primary tool for gene discovery, localization and functional analysis. While conceptual underpinning of these approaches have been long known, advances in recent decades in molecular genetics, development in efficient algorithms, and computing power have enabled the large scale application of these methods. While linkage studies seek to identify loci that cosegregate with the trait within families, association studies seek to identify particular variants that are associated with the phenotype at the population level. These are complementary methods that, together, provide means to probe the genome and describe etiology of complex human traits. In linkage studies, we seek to identify the loci that cosegregate with a specific genomic region, tagged by polymorphic markers, within families. In contrast, in association studies, we seek a correlation between a specific genetic variation and trait variation in sample of individuals, implicating a causal role of the variant. Linkage tests are powerful and specific for gene discovery, the localization of locus can be achieved only to a certain level of precision \u2013 on order of megabases \u2013 that potentially represents a region that potentially include hundreds of genes.",
            "score": 77.17800569534302
        },
        {
            "docid": "34132734_14",
            "document": "Family-based QTL mapping . Linkage and association analysis are primary tools for gene discovery, localization and functional analysis. While conceptual underpinning of these approaches have been long known, advances in recent decades in molecular genetics, development in efficient algorithms, and computing power have enabled the large scale application of these methods. While linkage studies seek to identify loci cosegregate with the trait within families, association studies seek to identify particular variants that are associated with the phenotype at the population level. These are complementary methods that, together, provide means to probe the genome and describe etiology of complex traits. In linkage studies, we seek to identify the loci that cosegregate with a specific genomic region, tagged by polymorphic markers, within families. In contrast, in association studies, we seek a correlation between a specific genetic variation and trait variation in sample of individuals, implicating a causal role of the variant.",
            "score": 70.57623291015625
        },
        {
            "docid": "50518079_9",
            "document": "Human Genome Structural Variation . The copy number variants continued to be studied as several studies continued to reveal the depth of their presence and their significance. A study was conducted that questioned the role of the organization of copy number variants and wondered what type of duplications they are. It was known that that copy number variation plays a big role in many human diseases but at the time large scale studies of these duplications had not been done. They decided to sequence 130 breakpoints from 112 individuals that contained 119 known CNVs by doing whole genome sequencing as well as next generation sequencing. They found that tandem duplications comprised 83% of the CNVs while 8.4% were triplications, 4.2% were adjacent duplications, 2.5% were insertional translocations, and 1.7% were other complex rearrangements. The copy number variants were predominantly tandem duplications which made it the most common type of copy number variant in the human genome according to the results of the study on this population. More was needed on the mechanistic side of the formation of structural variants. There was a study that focused on the mechanisms of very interesting and rare pathogenic copy number variants. The researchers knew that copy number variation is important in genome structural variation and contributes to human genetic disease but the actual mechanisms of most of the new and few pathogenic copy number variants had not been known. They used sequencing technologies to sequence breakpoint areas of many rare pathogenic copy number variants which was the biggest and most in depth analysis of copy number variants. They saw that the genomic architectural features were very important in the human genome and they were associated with about eighty-one percent of breakpoints. They concluded that tandem duplications and microdeletions that are rare and pathogenic do not happen in the human genome by chance. Instead, they arise from many different genomic architectural features. It was a very interesting result in that the certain architectural features of the genome physically made it possible and probable to develop certain rare and pathogenic structural variants.",
            "score": 51.00007975101471
        },
        {
            "docid": "24460323_4",
            "document": "Mark D. Shriver . Shriver's work is focused on admixture mapping, signatures of natural selection, and phenotypic variability in common trait variation. A major goal of his work is to apply these methods and understanding of genomic variation to studies of common diseases (e.g. obesity, type 2 diabetes, adaptation to altitude, hypertension and prostate cancer), and to normal variation, in particular skin pigmentation and response to UVR. More recently, his research has focused on the genetics of facial features.",
            "score": 52.987229347229004
        },
        {
            "docid": "796412_2",
            "document": "Experimental evolution . Experimental evolution is the use of laboratory experiments or controlled field manipulations to explore evolutionary dynamics. Evolution may be observed in the laboratory as individuals/populations adapt to new environmental conditions by natural selection. There are two different ways in which adaptation can arise in experimental evolution. One is via an individual organism gaining a novel beneficial mutation. The other is from allele frequency change in standing genetic variation already present in a population of organisms. Other evolutionary forces outside of mutation and natural selection can also play a role or be incorporated into experimental evolution studies, such as genetic drift and gene flow. The organism used is decided by the experimenter, based off whether the hypothesis to be tested involves adaptation through mutation or allele frequency change. A large number of generations are required for adaptive mutation to occur, and experimental evolution via mutation is carried out in viruses or unicellular organisms with rapid generation times, such as bacteria and asexual clonal yeast. Polymorphic populations of asexual or sexual yeast, and multicellular eukaryotes like Drosophila, can adapt to new environments through allele frequency change in standing genetic variation. Organisms with longer generations times, although costly, can be used in experimental evolution. Laboratory studies with foxes and with rodents (see below) have shown that notable adaptations can occur within as few as 10\u201320 generations and experiments with wild guppies have observed adaptations within comparable numbers of generations. More recently, experimentally evolved individuals or populations are often analyzed using whole genome sequencing, an approach known as Evolve and Resequence (E&R). E&R can identify mutations that lead to adaptation in clonal individuals or identify alleles that changed in frequency in polymorphic populations, by comparing the sequences of individuals/populations before and after adaptation. The sequence data makes it possible to pinpoint the site in a DNA sequence that a mutation/allele frequency change occurred to bring about adaptation. The nature of the adaptation and functional follow up studies can shed insight into what effect the mutation/allele has on phenotype.",
            "score": 33.69516122341156
        }
    ],
    "r": [
        {
            "docid": "11127802_8",
            "document": "Biocurator . There has been also recent interest in exploring the use of natural-language processing and text mining technologies to enable a more systematic extraction of candidate information for manual literature curation. Therefore, the definition of the main literature curation stages of a 'canonical' biocuration workflow has been examined. The use of text mining techniques for these various stages, from the initial detection of curation-relevant articles (triage) to the extraction of annotations and entity relationships has been attempted by various specialized systems.",
            "score": 95.2833480834961
        },
        {
            "docid": "17742682_2",
            "document": "Vertebrate and Genome Annotation Project . The Vertebrate Genome Annotation (VEGA) database is a biological database dedicated to assisting researchers in locating specific areas of the genome and annotating genes or regions of vertebrate genomes. The VEGA browser is based on Ensembl web code and infrastructure and provides a public curation of known vertebrate genes for the scientific community. The VEGA website is updated frequently to maintain the most current information about vertebrate genomes and attempts to present consistently high-quality annotation of all its published vertebrate genomes or genome regions. VEGA was developed by the Wellcome Trust Sanger Institute and is in close association with other annotation databases, such as ZFIN (The Zebrafish Information Network), the Havana Group and GenBank. Manual annotation is currently more accurate at identifying splice variants, pseudogenes, polyadenylation features, non-coding regions and complex gene arrangements than automated methods.",
            "score": 93.52479553222656
        },
        {
            "docid": "40841348_9",
            "document": "Computational and Statistical Genetics . Over the past few years, genome-wide association studies (GWAS) have become a powerful tool for investigating the genetic basis of common diseases and has improved our understanding of the genetic basis of many complex traits. Traditional single SNP (single-nucleotide polymorphism) GWAS is the most commonly used method to find trait associated DNA sequence variants - associations between variants and one or more phenotypes of interest are investigated by studying individuals with different phenotypes and examining their genotypes at the position of each SNP individually. The SNPs for which one variant is statistically more common in individuals belonging to one phenotypic group are then reported as being associated with the phenotype. However, most complex common diseases involve small population-level contributions from multiple genomic loci. To detect such small effects as genome-wide significant, traditional GWAS rely on increased sample size e.g. to detect an effect which accounts for 0.1% of total variance, traditional GWAS needs to sample almost 30,000 individuals. Although the development of high throughput SNP genotyping technologies has lowered the cost and improved the efficiency of genotyping. Performing such a large scale study still costs considerable money and time. Recently, association analysis methods utilizing gene-based tests have been proposed that are based on the fact that variations in protein-coding and adjacent regulatory regions are more likely to have functional relevance. These methods have the advantage that they can account for multiple independent functional variants within a gene, with the potential to greatly increase the power to identify disease/trait associated genes. Also, imputation of ungenotyped markers using known reference panels(e.g. HapMap and the 1000 Genomes Project) predicts genotypes at the missing or untyped markers thereby allowing one to accurately evaluate the evidence for association at genetic markers that are not directly genotyped (in addition to the typed markers) and has been shown to improve the power of GWAS to detect disease associated loci.",
            "score": 92.41939544677734
        },
        {
            "docid": "33190537_20",
            "document": "Literature Circles in EFL . On the one hand, it is generally difficult to make a distinction between cooperative and collaborative learning methods at the beginning. When we consider the advantages of small group structure and active student participation in collaborative and cooperative tasks over passive, lecture based teaching, the two terms seem quite close in meaning. In both ways learning is supported by a discovery based approach. Both methods require group skills and come with a framework upon which the group\u2019s activity resides, but cooperative learning is usually more structurally defined than collaborative learning.",
            "score": 91.0388412475586
        },
        {
            "docid": "14006459_7",
            "document": "Continuous integrated triage . Ambulatory patients are directed to self decon showers and then progress to the Green Triage holding area for re-triage using a Hospital Based Triage method and treatment.",
            "score": 90.90957641601562
        },
        {
            "docid": "14006459_12",
            "document": "Continuous integrated triage . Following assisted decon, patients found to be Alert, Oriented, Responsive and able to Follow Commands but non-ambulatory are triaged to the Yellow Triage holding area for re-triage using a Hospital Based Triage method and treatment.",
            "score": 90.69353485107422
        },
        {
            "docid": "5255433_4",
            "document": "Fiona Brinkman . Brinkman's current research interests center around improving understanding of how microbes evolve and improving computational methods that aid the analysis of microbes and the development of new vaccines, drugs and diagnostics for infectious diseases. Increasingly her methods have been applied for more environmental applications. She is noted for developing PSORTb, the most precise method available for computational protein subcellular localization prediction and the first computational method that exceeded the accuracy of some common high-throughput laboratory methods for such subcellular localization analysis. This method aids the prediction of cell surface and secreted proteins in a bacterial cell that may be suitable drug targets, vaccine components or diagnostics. She has also developed bioinformatics methods that aid the more accurate identification of genomic islands (i.e. IslandViewer) and orthologs (i.e. OrtholugeDB) . Her research has provided new insights into the evolution of pathogens and the role that horizontal gene transfer and genomic islands play. She confirmed the anecdotal assumption that virulence factors (disease-causing genes in pathogens) are disproportionately associated with genomic islands. She was among the first researchers to use whole genome sequencing to aid infectious disease outbreak investigations (\"genomic epidemiology\"), integrating genome sequence data with social network analysis. She was involved in the Pseudomonas Genome Project and is the coordinator of the Pseudomonas Genome Database, a database of Pseudomonas species genomic data and associated annotations that is continually updated. She has also developed databases (i.e. InnateDB and the Allergy and Asthma Portal) to aid more systems-based analysis of immune disorders and the immune response to infections in humans and other animals - databases that have aided the identification of new immune-modulating therapeutics. She has a long-standing interest in bioinformatics training, improving the curation of biological/bioinformatics data, and developing effective bioinformatics data standards and databases. She is a Thomson Reuter's Highly Cited Researcher, a member of national committees and Boards such as the Genome Canada Board of Directors, and has been Research Director for several Genomics projects. She has a growing interest in applying her methods to environmental applications as part of a broader interest in developing approaches for more holistic, sustainable infectious disease control and microbiome conservation - developing approaches that may select less for antimicrobial resistance, improve the tracking of pathogens and their origins, and better factor in the important role of societal changes and the environment in shaping microbiomes",
            "score": 90.25282287597656
        },
        {
            "docid": "34525276_12",
            "document": "Death Domain database . To curate data in the literature, the authors chose to focus on the analytical methods, experimental results, resources, and nomenclature. If there was insufficient data in the papers, users will see \"Not specified\" in these sections.",
            "score": 89.958251953125
        },
        {
            "docid": "3407633_2",
            "document": "Simple triage and rapid treatment . Simple triage and rapid treatment (START) is a triage method used by first responders to quickly classify victims during a mass casualty incident (MCI) based on the severity of their injury. The method was developed in 1983 by the staff members of Hoag Hospital and Newport Beach Fire Department located in California, and is currently widely used in the United States.",
            "score": 89.09320831298828
        },
        {
            "docid": "14004003_4",
            "document": "Business triage . First described in the late 1990s, business triage grew out of the need for a reproducible method for the allocation of limited resources (especially money) by start-up and expanding businesses. The concepts utilized in business triage were drawn from the real-world experience of managing limited resources in the high-stakes environments of battlefields and disaster scenes (medical triage). Process analysis methods similar to those used by information technology professionals and later business continuity professionals were added to better identify the processes that support the desired outcomes and goals.",
            "score": 89.06748962402344
        },
        {
            "docid": "49539168_35",
            "document": "No-SCAR (Scarless Cas9 Assisted Recombineering) Genome Editing . The no-SCAR method is more efficient than standard cloning techniques including ssDNA recombineering as well as other scar-free genome editing techniques. The method allows for unlimited numbers of single step genomic alterations without the use of selectable markers. The great advantage of the method, specifically in terms research applications, is the speed of the protocol. In the seminal paper by Reisch and Prather, the no-SCAR method required 5 days of work including the cloning step necessary for sgRNA targeting. Once the cells contain the pCas9cr4 plasmid, subsequent experiments can be completed in as little as 3 days allowing for rapid genome editing. Currently, the no-SCAR method is faster than any other method published and is an attractive option for researchers interested in studying the effects of numerous modifications.",
            "score": 88.13685607910156
        },
        {
            "docid": "30919_22",
            "document": "Triage . \"This section is for general concepts in triage-based treatment options and outcomes\". For specific triage systems and methods see the sections dedicated to that topic.\"",
            "score": 87.66934967041016
        },
        {
            "docid": "29206447_4",
            "document": "Diagnostic Enterprise Method . Taylor established four principles to increase efficiency at the workplace. Although these principles were established a long time ago they are still used to resolve efficiency problems. The First principle is to analyze the way in which every worker does the task given. With all the information united, build a new improved plan of activities. This principle is based in the study of time-cost and time of delivery. The second principle consists in making a new method and give it as a written rule to be a standard for working. If the method is written down and given to the workers they can learn it and apply it in their work area. The third principle is that according to needs, chose the best workers that have the abilities and that can be trained to follow the standard rules. When a worker has the knowledge he can do his tasks better and achieve the goals asked. Fourth principle is to create a common goal between the workers and make a pay system to reward the ones above the common goal. With this principle the employees would work with more efficiency and do it with auto satisfaction. From these principles we deduce that every place in the enterprise costs money, so it is necessary to complete the labor analysis technique. Although the Frederick Taylor's original technique is still used, other methods' variations have been developed from it. These methods are also effective. Each one has a different process, but the main goal is to get the objectives. By knowing the different types of methods, people can use it as a strategic plan that will give an extra value against the competition.",
            "score": 84.69911193847656
        },
        {
            "docid": "46968364_46",
            "document": "Inferring horizontal gene transfer . Parametric and phylogenetic methods draw on different sources of information; it is therefore difficult to make general statements about their relative performance. Conceptual arguments can however be invoked. While parametric methods are limited to the analysis of single or pairs of genomes, phylogenetic methods provide a natural framework to take advantage of the information contained in multiple genomes. In many cases, segments of genomes inferred as HGT based on their anomalous composition can also be recognised as such on the basis of phylogenetic analyses or through their mere absence in genomes of related organisms. In addition, phylogenetic methods rely on explicit models of sequence evolution, which provide a well-understood framework for parameter inference, hypothesis testing, and model selection. This is reflected in the literature, which tends to favour phylogenetic methods as the standard of proof for HGT. The use of phylogenetic methods thus appears to be the preferred standard, especially given that the increase in computational power coupled with algorithmic improvements has made them more tractable, and that the ever denser sampling of genomes lends more power to these tests.",
            "score": 84.54827117919922
        },
        {
            "docid": "28811429_5",
            "document": "PATRIC . The CyberInfrastructure Division at VBI develops methods, infrastructure, and resources to help enable scientific discoveries in infectious disease research and other research fields. The group applies the principles of cyberinfrastructure to integrate data, computational infrastructure, and people. CyberInfrastructure Division has developed many public resources for curated, diverse molecular and literature data from various infectious disease systems, and implemented the processes, systems, and databases required to support them. It also conducts research by applying its methods and data to make new discoveries of its own.",
            "score": 84.34061431884766
        },
        {
            "docid": "14006459_6",
            "document": "Continuous integrated triage . Using a Group (Global) Triage method (i.e. M.A.S.S. Triage), patients are divided into ambulatory (green) and non-ambulatory (red, yellow & black) triage categories.",
            "score": 83.05463409423828
        },
        {
            "docid": "1751139_4",
            "document": "Catholic Boy Scouts of Ireland . In 1925 and 1926, Father Ernest Farrell, a curate in Greystones, County Wicklow began working with a youth programme loosely modelled on the Scout method. Under the pen-name \"Sagart\", he wrote a series of articles in Our Boys, a magazine published by the Christian Brothers, advocating the formation of an official Catholic Scout organisation. This initial group, while more in line with the methods of the Boys' Brigade was viewed as an effective means of imprinting a Catholic ethos on the young men of Ireland. Father Farrell's brother, Father Tom Farrell, a curate in the Pro-Cathedral gave this fledgeling association the backing of the church and its resources. In 1927 the Catholic Boy Scouts of Ireland was officially founded, with a constitution drawn up and a headquarters from which the association could be organised, clothed and supplied.",
            "score": 82.82624053955078
        },
        {
            "docid": "30919_6",
            "document": "Triage . As medical technology has advanced, so have modern approaches to triage, which are increasingly based on scientific models. The categorizations of the victims are frequently the result of triage scores based on specific physiological assessment findings. Some models, such as the START model may be algorithm-based. As triage concepts become more sophisticated, triage guidance is also evolving into both software and hardware decision support products for use by caregivers in both hospitals and the field.",
            "score": 82.36524963378906
        },
        {
            "docid": "30919_28",
            "document": "Triage . \"This section is for examples of specific triage systems and methods. For general triage concepts, see the sections for types of triage, treatment options, and outcomes.\"",
            "score": 82.20326232910156
        },
        {
            "docid": "33190537_45",
            "document": "Literature Circles in EFL . Although this study is based on the classroom interaction as the criteria to be observed in literature circles, future studies may focus on some other variables like the development of reading and writing skills, or even grammar and vocabulary improvement through literature circles. Moreover, by using some other research methods, the results of this study may be compared with the findings of those other studies which will use different research methods. It would increase the validity and reliability issues in the findings. In addition to these points, the research could further explore the development of materials and procedures appropriate for different purposes or levels of competency in foreign languages. This study has gathered supporting ideas related to the similar research projects. This impact will provide the language teachers to be interested in the subject more and try similar applications which will provide further evidence for future studies. As a result, it is clear that the teachers and learners have problems regarding the usage of literary texts in EFL classes and the solution requires a new point of view on the teaching of literature both by teachers and by textbooks. The results of such a study can motivate the teachers to use the literature resources more effectively encouraging real life interaction in the classroom. For instance, in accordance with the findings of the study, it can be suggested that teaching with the help of reading texts should not be limited to only fiction literature. As well as the novels and short stories, some other texts like fact files should also be adapted for the discussion groups. Originating from the results of this study further research can be done by collecting more quantitative data on the subject. Maybe, some experimental quantitative research designs would be suitable for this purpose.  As teachers and educators, there are many questions waiting to be dealt with in front of us like, \u201cAre there more opportunities we can provide our students with for a better learning environment?\u201d or \u201cWhat are the contemporary modals of professional teacher development?\u201d I strongly believe that this study has put another brick on the literature circle studies in the field, contributing to the growth of this collaborative work.",
            "score": 82.1316909790039
        },
        {
            "docid": "898503_44",
            "document": "Steam (software) . Without more direct interaction on the curation process, allowing hundreds more games on the service, Valve had looked to find methods to allow players to find games they would be more likely to buy based on previous purchase patterns. The September 2014 \"Discovery Update\" added tools that would allow existing Steam users to be curators for game recommendations, and sorting functions that presented more popular titles and recommended titles specific to the user, as to allow more games to be introduced on Steam without the need of Steam Greenlight, while providing some means to highlight user-recommended games. This Discovery update was considered successful by Valve, as they reported in March 2015 in seeing increased use of the Steam Storefront and an increase in 18% of sales by revenue from just prior to the update. A second Discovery update was released November 2016, giving users more control over what titles they want to see or ignore within the Steam Store, alongside tools for developers and publishers to better customize and present their game within these new users preferences. By February 2017, Valve reported that with the second Discovery update, the number of games shown to users via the store's front page increased by 42%, with more conversions into sales from that viewership. In 2016, more games are meeting a rough metric of success defined by Valve as selling more than $200,000 in revenues in its first 90 days of release. Valve added a \"Curator Connect\" program in December 2017. Curators can set up descriptors for the type of games they are interested in, preferred languages, and other tags along with social media profiles, while developers can find and reach out to specific curators from this information, and, after review, provide them directly with access to their game. This step, which eliminates the use of a Steam redemption key, is aimed to reduce the reselling of keys, as well as dissuade users that may be trying to game the curator system to obtain free game keys.",
            "score": 82.01408386230469
        },
        {
            "docid": "233488_12",
            "document": "Machine learning . Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on \"known\" properties learned from the training data, data mining focuses on the discovery of (previously) \"unknown\" properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as \"unsupervised learning\" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to \"reproduce known\" knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously \"unknown\" knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.",
            "score": 81.77819061279297
        },
        {
            "docid": "14006459_11",
            "document": "Continuous integrated triage . Finally, non-ambulatory responsive patients undergo assisted decon followed by re-triage using a Physiological (Individual) Triage method (i.e. S.T.A.R.T. or JumpS.T.A.R.T.) modified to include behavioral triage considerations.",
            "score": 81.74952697753906
        },
        {
            "docid": "45663583_3",
            "document": "Human interactome . With the sequencing of the genomes of a diverse array or model organisms, it became clear that the number of genes does not correlate with the human perception of relative organism complexity \u2013 the human proteome contains some 20 000 genes, which is smaller than some species such as corn. A statistical approach to calculating the number of interactions in humans gives an estimate of around 650 000, one order of magnitude bigger than Drosophila and 3 times larger than C. Elegans. As of 2008, only about <0.3% of all estimated interactions among human proteins has been identified, although in recent years there has been exponential growth in discovery \u2013 as of 2015, over 210 000 unique human positive protein\u2013protein interactions are currently catalogued, and bioGRID database contains almost 750 000 literature-curated PPI's for 30 model organisms, 300 000 of which are verified or predicted human physical or genetic protein\u2013protein interactions, a 50% increase from 2013. The currently available information on the human interactome network originates from either literature-curated interactions, high-throughput experiments, or from potential interactions predicted from interactome data, whether through phylogenetic profiling (evolutionary similarity), statistical network inference, or text/literature mining methods.",
            "score": 81.17034149169922
        },
        {
            "docid": "30919_7",
            "document": "Triage . \"This section is for concepts in triage. See other sections for specific triage tools, methods, and systems\"",
            "score": 81.067138671875
        },
        {
            "docid": "590161_37",
            "document": "Queen's College, Hong Kong . Most of the surviving materials and records were collected by institutions and University Archives in the United Kingdom and Hong Kong, as well as private collectors and Old Boys. The help of internet presented both opportunities and challenges to the curators. Apart from being overwhelmed quantitatively by various materials previously not available for researchers, useful information is often fragmented and hidden deep inside these materials, which require specific historical knowledge and skills to detach them from their sources and connect them with the wider picture. As a result, materials and information previously unknown to the world were found and displayed, in the hope that visitors and researchers will benefit from a more holistic picture of the history of Queen's College, and its relation to the development of Hong Kong and Greater China. In terms of curating the exhibition, the curators have adopted both chronological and typological display methods, tailoring to the specific nature of our collection. Organising the main exhibition in a chronological manner can effectively communicate in a simple and straightforward manner, allowing visitors from different steps of life to observe and fully appreciate the changes and continuities of school policies, curriculum, student experience and activities, and their relation to the broader socio-political development in Hong Kong and the World. The 8 typologically organised cases are aimed at showing visitors the 'evolution' and development of school-related items formerly used by students and staff from different eras, reflecting the response to changing demands across history. It is hoped that both displaying patterns will complement each other and inform visitors of a more holistic picture, thus reflecting changes and continuities of the Queen's College.",
            "score": 80.95341491699219
        },
        {
            "docid": "52866970_8",
            "document": "Histone variants . \"HistoneDB 2.0 - with variants\", a database of histones and their variants maintained by National Center for Biotechnology Information, currently serves as the most comprehensive manually curated resource on histones and their variants that follows the new unified phylogeny-based nomenclature of histone variants. \"Histome: The Histone Infobase\" is manually curated database of histone variants in humans and associated post-translational modifications as well as modifying enzymes. MS_HistoneDB is a proteomics-oriented manually curated databases for mouse and human histone variants.",
            "score": 80.90011596679688
        },
        {
            "docid": "26418006_23",
            "document": "Exome sequencing . Current association studies have focused on common variation across the genome, as these are the easiest to identify with our current assays. However, disease-causing variants of large effect have been found to lie within exomes in candidate gene studies, and because of negative selection, are found in much lower allele frequencies and may remain untyped in current standard genotyping assays. Whole genome sequencing is a potential method to assay novel variant across the genome. However, in complex disorders (such as autism), a large number of genes are thought to be associated with disease risk. This heterogeneity of underlying risk means that very large sample sizes are required for gene discovery, and thus whole genome sequencing is not particularly cost-effective. This sample size issue is alleviated by the development of novel advanced analytic methods, which effectively map disease genes despite the genetic mutations are rare at variant level. In addition, variants in coding regions have been much more extensively studied and their functional implications are much easier to derive, making the practical applications of variants within the targeted exome region more immediately accessible.",
            "score": 80.86361694335938
        },
        {
            "docid": "1795879_27",
            "document": "F. C. S. Schiller . Schiller contends that in light of the other methods' failure to provide humans with a role and place in the universe, we ought avoid the adoption of these methods. By the end of \"Riddles\", Schiller offers his method of humanism as the only possible method that results in a world where we can navigate our lower existence to the achievement of our higher purpose. He asserts that it is the method we ought to adopt regardless of the evidence against it (\"even though it were but a bare possibility\").",
            "score": 80.80148315429688
        },
        {
            "docid": "14006459_10",
            "document": "Continuous integrated triage . Next, unresponsive patients with stable vital signs undergo assisted decon. After decon these patients too are re-triaged using a Physiological (Individual) Triage method (i.e. S.T.A.R.T. or JumpS.T.A.R.T.) modified to include behavioral triage considerations.",
            "score": 80.74178314208984
        },
        {
            "docid": "33259131_2",
            "document": "DisProt . In molecular biology, DisProt is a curated biological database collection of intrinsically unstructured proteins. It is a community resource annotating protein sequences for intrinsically disorder regions from the literature. DisProt classifies intrinsic disorder based on experimental methods and three ontologies for molecular function, transition and binding partners.",
            "score": 80.64073944091797
        },
        {
            "docid": "505717_70",
            "document": "Image segmentation . Most segmentation methods are based only on color information of pixels in the image. Humans use much more knowledge than this when doing image segmentation, but implementing this knowledge would cost considerable computation time and would require a huge domain knowledge database, which is currently not available. In addition to traditional segmentation methods, there are trainable segmentation methods which can model some of this knowledge.",
            "score": 80.33839416503906
        }
    ]
}