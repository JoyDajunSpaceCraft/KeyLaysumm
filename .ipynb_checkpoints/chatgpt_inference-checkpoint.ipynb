{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96365e6f-f7ac-4851-9a14-4ae75070e4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference with Chatgpt\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "client = OpenAI(api_key =\"sk-\" )\n",
    "\n",
    "text = \"Cell-fate decisions are controlled by a process called reprograming, in which the cell’s chromatin is switched on or off. This process is controlled, on the one hand, by signaling from a cell niche to the recipient cell. However, it is not clear how this process works, and how it can be controlled in intact tissues. To investigate this question, Tursun, Patel et al. studied the germline of a worm called C. elegans. In this worm, the gene for a protein called GLP-1/Notch is turned on, but this protein is silenced by another gene called LIN-53, which is required for cells to become pluripotent. The gene that encodes this gene is called the histone demethylase ( or PRC2 for short ). It is also known to regulate the activity of other genes, such as a transcription factor called LAG-5. These genes are also involved in the process of converting germ cells into neurons. But it was not known whether the genes that activate the Notch signaling pathway were also activated by the same gene, or if they were simply switched off by an enzyme called UTX-2. Now, Riddle and Patel have investigated whether this is the case. First, they used a combination of genetics, tissue-specific transcriptome analysis and functional studies to find out if this enzyme is a key target of the notch pathway. Further experiments showed that this pathway is best known for maintaining undifferentiated germ-line stem cells and that it also promotes the conversion of stem-cell into neuron-like cells. Finally, using a technique called functional analysis of human cells, these experiments revealed that the human immune system is better able to control the expression of certain genes than previously thought. By inhibiting the production of histones, this new knowledge could lead to new treatments for diseases associated with abnormal gene expression.\"\n",
    "knowledge = text\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a layman rephrase helper, your goal is to genarate the layman can understand rephrase based on user's input\"},\n",
    "    {\"role\": \"user\", \"content\": f'Here is the text I want you to help me understand: {text} Here is the external knowledge you need to know {knowledge} can you generate the rephrased result based on the description I give you '},\n",
    "  ],\n",
    "    \n",
    ")\n",
    "res = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afcbf145-c761-461c-bdd4-9b302fdc7f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two files in the val\n",
    "plos_val = \"data/GT_firstGen_plos_val.json\"\n",
    "elife_val = \"data/GT_firstGen_elife_val.json\"\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "with open(plos_val, \"r\") as plos, open(elife_val, \"r\") as elife:\n",
    "    plos_data = json.load(plos)\n",
    "    elife_data = json.load(elife)\n",
    "ground_truth = []\n",
    "first_gen = []\n",
    "for item in plos_data:\n",
    "    ground_truth.append(item[\"ground_truth\"])\n",
    "    first_gen.append(item[\"first_gen\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a3c674-3831-4e6b-a9e6-0879b767e592",
   "metadata": {},
   "source": [
    "## ChatGPT used without knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87a9cd72-2e46-4427-9650-2093f909649c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████| 10/10 [02:11<00:00, 13.17s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "\n",
    "import openai\n",
    "client = OpenAI(api_key =\"sk-\" )\n",
    "\n",
    "# file_path = \"data/PLOS_val.jsonl\"\n",
    "# output_file_path = \"plos.txt\"\n",
    "\n",
    "\n",
    "# file_path = \"plos.txt\"\n",
    "# output_file_path = \"plos_second.txt\"\n",
    "# file_path = \"elife.txt\"\n",
    "# output_file_path = \"elife_second_chatgpt.txt\"\n",
    "\n",
    "\n",
    "\n",
    "# with open(file_path, 'r') as f, open(output_file_path, \"a+\") as output_file:\n",
    "predictions = []\n",
    "for line in tqdm(first_gen[:10]):\n",
    "    # item = json.loads(line)\n",
    "    # article = item[\"article\"]\n",
    "    search_q = line.split(\"\\n\")[0]\n",
    "    searcher = LuceneSearcher.from_prebuilt_index('enwiki-paragraphs')\n",
    "    # searcher = LuceneSearcher.from_prebuilt_index('beir-v1.0.0-bioasq-flat')\n",
    "    docs = []\n",
    "    for hit in searcher.search(search_q):\n",
    "        res = searcher.doc(hit.docid).raw()\n",
    "        docs.append(res)\n",
    "    # documents = dense_retriever.retrieve(search_q, docs, n=args.n_docs, n_cands=args.n_cands)\n",
    "    # knowledge = documents[:args.n_docs]\n",
    "    # knowledge = \" \".join(knowledge)\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a layman rephrase helper, your goal is to rephrase the input and make it easier to read\"},\n",
    "        {\"role\": \"user\", \"content\": f'Here is the text I want you to help me understand: {search_q}  Make it easier for high school students to read.'},\n",
    "      ],\n",
    "        \n",
    "    )\n",
    "    res = response.choices[0].message.content\n",
    "    predictions.append(res)\n",
    "    # print(\"Generated text:\", res)\n",
    "    # res = \" \".join(res.split(\"\\n\"))\n",
    "    # output_file.write(res + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36c503bc-b913-4042-83c3-7f18cbb381ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0dd335-a1be-43f1-8f43-228ef5323211",
   "metadata": {},
   "source": [
    "## Evaluate the generation result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09cc494d-506b-4786-80aa-7fc59ee4e891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores: {'rouge1': 0.04469273743016759, 'rouge2': 0.0, 'rougeL': 0.0335195530726257}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.7.7 to v1.9.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../BioLaySumm2024-evaluation_scripts/models/AlignScore/AlignScore-base.ckpt`\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Premise must has the same length with Hypothesis!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m rouge_scores \u001b[38;5;241m=\u001b[39m compute_rouge_scores(predictions, ground_truth[:\u001b[38;5;241m10\u001b[39m])\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mROUGE Scores:\u001b[39m\u001b[38;5;124m\"\u001b[39m, rouge_scores)\n\u001b[0;32m---> 44\u001b[0m align_scores \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_align_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_truth\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# For here the generated expected as the list\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Align Scores:\u001b[39m\u001b[38;5;124m\"\u001b[39m, align_scores)\n\u001b[1;32m     46\u001b[0m score_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFKGL\u001b[39m\u001b[38;5;124m\"\u001b[39m:[], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDCRS\u001b[39m\u001b[38;5;124m\"\u001b[39m:[], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLI\u001b[39m\u001b[38;5;124m\"\u001b[39m:[]}\n",
      "Cell \u001b[0;32mIn[10], line 29\u001b[0m, in \u001b[0;36mcompute_align_scores\u001b[0;34m(predictions, references)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_align_scores\u001b[39m(predictions, references):\n\u001b[1;32m     28\u001b[0m     scorer \u001b[38;5;241m=\u001b[39m AlignScore(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroberta-base\u001b[39m\u001b[38;5;124m'\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m, ckpt_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/yuj49/BioLaySumm2024-evaluation_scripts/models/AlignScore/AlignScore-base.ckpt\u001b[39m\u001b[38;5;124m'\u001b[39m, evaluation_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnli_sp\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreferences\u001b[49m\u001b[43m,\u001b[49m\u001b[43mclaims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(score)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(score)\n",
      "File \u001b[0;32m~/anaconda3/envs/llama_factory/lib/python3.10/site-packages/alignscore/alignscore.py:16\u001b[0m, in \u001b[0;36mAlignScore.score\u001b[0;34m(self, contexts, claims)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscore\u001b[39m(\u001b[38;5;28mself\u001b[39m, contexts: List[\u001b[38;5;28mstr\u001b[39m], claims: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnlg_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclaims\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/anaconda3/envs/llama_factory/lib/python3.10/site-packages/alignscore/inference.py:290\u001b[0m, in \u001b[0;36mInferencer.nlg_eval\u001b[0;34m(self, premise, hypo)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference(premise, hypo)\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnlg_eval_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbin_sp\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnlg_eval_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnli_sp\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnlg_eval_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreg_sp\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 290\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference_example_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpremise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhypo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized NLG Eval mode!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/llama_factory/lib/python3.10/site-packages/alignscore/inference.py:46\u001b[0m, in \u001b[0;36mInferencer.inference_example_batch\u001b[0;34m(self, premise, hypo)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03minference a example,\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03mpremise: list\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03mSummaC Style aggregation\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisable_progress_bar_in_inference \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(premise) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(hypo), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPremise must has the same length with Hypothesis!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     48\u001b[0m out_score \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m one_pre, one_hypo \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mzip\u001b[39m(premise, hypo), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating\u001b[39m\u001b[38;5;124m\"\u001b[39m, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(premise), disable\u001b[38;5;241m=\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)):\n",
      "\u001b[0;31mAssertionError\u001b[0m: Premise must has the same length with Hypothesis!"
     ]
    }
   ],
   "source": [
    "# evaluate the readability \n",
    "\n",
    "# Note: The file GT_firstGen_elife_test.json and GT_firstGen_plos_test.json is the ground truth is the keywords\n",
    "from rouge_score import rouge_scorer\n",
    "from datasets import load_dataset\n",
    "import textstat\n",
    "from alignscore import AlignScore\n",
    "# Function to compute ROUGE scores\n",
    "def compute_rouge_scores(predictions, references):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = [scorer.score(ref, pred) for ref, pred in zip(references, predictions)]\n",
    "\n",
    "    # Aggregate scores\n",
    "    aggregated_scores = {}\n",
    "    for score in scores:\n",
    "        for key in score:\n",
    "            if key in aggregated_scores:\n",
    "                aggregated_scores[key].append(score[key].fmeasure)\n",
    "            else:\n",
    "                aggregated_scores[key] = [score[key].fmeasure]\n",
    "\n",
    "    # Calculate average scores\n",
    "    for key in aggregated_scores:\n",
    "        aggregated_scores[key] = sum(aggregated_scores[key]) / len(aggregated_scores[key])\n",
    "\n",
    "    return aggregated_scores\n",
    "def compute_align_scores(predictions, references):\n",
    "    scorer = AlignScore(model='roberta-base', batch_size=32, device='cuda:0', ckpt_path='/home/yuj49/BioLaySumm2024-evaluation_scripts/models/AlignScore/AlignScore-base.ckpt', evaluation_mode='nli_sp')\n",
    "    score = scorer.score(contexts=references,claims=predictions)\n",
    "    return sum(score)/len(score)\n",
    "def calc_readability(preds):\n",
    "  fkgl_scores = []\n",
    "  cli_scores = []\n",
    "  dcrs_scores = []\n",
    "  for pred in preds:\n",
    "    fkgl_scores.append(textstat.flesch_kincaid_grade(pred))\n",
    "    cli_scores.append(textstat.coleman_liau_index(pred))\n",
    "    dcrs_scores.append(textstat.dale_chall_readability_score(pred))\n",
    "  return np.mean(fkgl_scores), np.mean(cli_scores), np.mean(dcrs_scores)\n",
    "\n",
    "\n",
    "rouge_scores = compute_rouge_scores(predictions, ground_truth[:10])\n",
    "print(\"ROUGE Scores:\", rouge_scores)\n",
    "align_scores = compute_align_scores(predictions, ground_truth[:10]) # For here the generated expected as the list\n",
    "print(\"Validation Align Scores:\", align_scores)\n",
    "score_dict = {\"FKGL\":[], \"DCRS\":[], \"CLI\":[]}\n",
    "for pred in predictions:\n",
    "    fkgl_score, cli_score, dcrs_score = calc_readability(preds)\n",
    "    score_dict['FKGL'].append(fkgl_score)\n",
    "    score_dict['DCRS'].append(dcrs_score)\n",
    "    score_dict['CLI'].append(cli_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d6029f-e509-4235-b4ba-e181e3138109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First option with the directly input and try different score for evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c307b935-c86e-4865-be88-fe4e2ddebf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
