{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96365e6f-f7ac-4851-9a14-4ae75070e4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference with Chatgpt\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "client = OpenAI(api_key =\"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\" )\n",
    "\n",
    "text = \"Cell-fate decisions are controlled by a process called reprograming, in which the cell’s chromatin is switched on or off. This process is controlled, on the one hand, by signaling from a cell niche to the recipient cell. However, it is not clear how this process works, and how it can be controlled in intact tissues. To investigate this question, Tursun, Patel et al. studied the germline of a worm called C. elegans. In this worm, the gene for a protein called GLP-1/Notch is turned on, but this protein is silenced by another gene called LIN-53, which is required for cells to become pluripotent. The gene that encodes this gene is called the histone demethylase ( or PRC2 for short ). It is also known to regulate the activity of other genes, such as a transcription factor called LAG-5. These genes are also involved in the process of converting germ cells into neurons. But it was not known whether the genes that activate the Notch signaling pathway were also activated by the same gene, or if they were simply switched off by an enzyme called UTX-2. Now, Riddle and Patel have investigated whether this is the case. First, they used a combination of genetics, tissue-specific transcriptome analysis and functional studies to find out if this enzyme is a key target of the notch pathway. Further experiments showed that this pathway is best known for maintaining undifferentiated germ-line stem cells and that it also promotes the conversion of stem-cell into neuron-like cells. Finally, using a technique called functional analysis of human cells, these experiments revealed that the human immune system is better able to control the expression of certain genes than previously thought. By inhibiting the production of histones, this new knowledge could lead to new treatments for diseases associated with abnormal gene expression.\"\n",
    "knowledge = text\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a layman rephrase helper, your goal is to genarate the layman can understand rephrase based on user's input\"},\n",
    "    {\"role\": \"user\", \"content\": f'Here is the text I want you to help me understand: {text} Here is the external knowledge you need to know: {knowledge}. Please generate the rephrased result based on the description I give you '},\n",
    "  ],\n",
    "    \n",
    ")\n",
    "res = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afcbf145-c761-461c-bdd4-9b302fdc7f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two files in the val\n",
    "\n",
    "import json\n",
    "\n",
    "plos_test = \"data/GT_firstGen_plos_test.json\"\n",
    "elife_test = \"data/GT_firstGen_elife_test.json\"\n",
    "\n",
    "plos_val = \"data/GT_firstGen_plos_val.json\"\n",
    "elife_val = \"data/GT_firstGen_elife_val.json\"\n",
    "\n",
    "\n",
    "\n",
    "with open(plos_val, \"r\") as plos, open(elife_val, \"r\") as elife:\n",
    "    plos_data = json.load(plos)\n",
    "    elife_data = json.load(elife)\n",
    "ground_truth = []\n",
    "first_gen = []\n",
    "for item in elife_data:\n",
    "    ground_truth.append(item[\"ground_truth\"])\n",
    "    first_gen.append(item[\"first_gen\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f595bf13-f4d5-4f32-a016-0cb8d379ea2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, PreTrainedModel, PretrainedConfig, AutoModelForSeq2SeqLM,AutoModelForSequenceClassification\n",
    "from peft import PeftModel, PeftConfig\n",
    "from einops import rearrange\n",
    "from typing import Dict\n",
    "from termcolor import cprint\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "\n",
    "\n",
    "class Retriever:\n",
    "    def __init__(self, args):\n",
    "\n",
    "        self.query_model_path = args.dense_retriever_path\n",
    "        self.retriever_model = args.retriever_model\n",
    "\n",
    "        # Forced change\n",
    "        # self.retriever_model = args.model\n",
    "        self.device = args.device\n",
    "        if self.query_model_path is None or args.retriever_model == \"sparse\":\n",
    "            self.query_model, self.key_model, self.tokenizer = None, None, None\n",
    "            self.max_seq_len = 512\n",
    "        elif self.retriever_model == \"colbert\":\n",
    "            print(\"self.query_model_path\", self.query_model_path)\n",
    "            _config = PeftConfig.from_pretrained(self.query_model_path)\n",
    "            print(\"_config\",_config)\n",
    "            base_model = _config.base_model_name_or_path\n",
    "            print(\"base_model\", base_model)\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(args.search_tokenizer)\n",
    "\n",
    "            self.query_model = ColBERT.from_pretrained(base_model)\n",
    "            self.query_model = PeftModel.from_pretrained(self.query_model, self.query_model_path)\n",
    "            self.query_model.eval()\n",
    "            self.query_model.to(self.device)\n",
    "            self.max_seq_len = min(self.tokenizer.model_max_length, 512)\n",
    "        elif self.retriever_model==\"bge\":\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(args.search_tokenizer)\n",
    "            self.query_model = AutoModelForSequenceClassification.from_pretrained(self.query_model_path)\n",
    "            self.query_model.eval()\n",
    "            self.query_model.to(self.device)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "\n",
    "    def retrieve(self, q, documents, n=1, n_cands=-1, return_rank=False):\n",
    "        if self.query_model is None: return documents[0]\n",
    "\n",
    "        if self.retriever_model == \"bge\":\n",
    "            pairs = [[q, doc] for doc in documents]\n",
    "            with torch.no_grad():\n",
    "                inputs = self.tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512).to(self.device)\n",
    "                scores = self.query_model(**inputs, return_dict=True).logits.view(-1, ).float()\n",
    "            ranking = sorted([(idx, score) for idx, score in enumerate(scores)], key=lambda x: x[1], reverse=True)\n",
    "            documents = sorted([(doc, score) for doc, score in zip(documents, scores)], key=lambda x: x[1], reverse=True)\n",
    "            documents = [doc[0] for doc in documents]\n",
    "            \n",
    "        elif self.retriever_model in [\"colbert\"]:\n",
    "            query_outputs = self.tokenizer(q, return_tensors='pt', max_length=self.max_seq_len, padding='max_length', truncation=True)\n",
    "            key_outputs = self.tokenizer(documents, return_tensors='pt', max_length=self.max_seq_len, padding='max_length', truncation=True)\n",
    "\n",
    "            query_inputs = {k:v.to(self.device) for k, v in query_outputs.items()}\n",
    "            key_inputs = {k:v.to(self.device) for k, v in key_outputs.items()}\n",
    "            # pairs = [[q,doc]for doc in documents]\n",
    "            # inputs = self.tokenizer (pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "            # scores = model(**inputs, return_dict=True).logits.view(-1, ).float()\n",
    "            with torch.no_grad():\n",
    "                similarity = self.query_model(query_inputs, key_inputs)\n",
    "            scores = similarity.flatten().tolist()\n",
    "            ranking = sorted([(idx, score) for idx, score in enumerate(scores)], key=lambda x: x[1], reverse=True)\n",
    "            documents = sorted([(doc, score) for doc, score in zip(documents, scores)], key=lambda x: x[1], reverse=True)\n",
    "            documents = [doc[0] for doc in documents]\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        if return_rank: return documents, ranking\n",
    "        return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc9d68fd-29aa-40f0-a2fb-1354d8d096d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColBERTConfig(PretrainedConfig):\n",
    "    compression_dim: int = 768\n",
    "    dropout: float = 0.0\n",
    "    return_vecs: bool = False\n",
    "    trainable: bool = True\n",
    "\n",
    "class ColBERT(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    ColBERT model from: https://arxiv.org/pdf/2004.12832.pdf\n",
    "    We use a dot-product instead of cosine per term (slightly better)\n",
    "    \"\"\"\n",
    "    config_class = ColBERTConfig\n",
    "    base_model_prefix = \"bert_model\"\n",
    "\n",
    "    def __init__(self, cfg, n_cands=8) -> None:\n",
    "        super().__init__(cfg)\n",
    "        \n",
    "        self.bert = AutoModel.from_pretrained(cfg._name_or_path)\n",
    "\n",
    "        # for p in self.bert.parameters():\n",
    "        #     p.requires_grad = cfg.trainable\n",
    "\n",
    "        self.compressor = torch.nn.Linear(self.bert.config.hidden_size, cfg.compression_dim)\n",
    "\n",
    "        self.n_cands = n_cands\n",
    "        print(f\"Model n_cands: {self.n_cands}\")\n",
    "\n",
    "    def forward(self,\n",
    "                query: Dict[str, torch.LongTensor],\n",
    "                document: Dict[str, torch.LongTensor]):\n",
    "\n",
    "        query_vecs = self.forward_representation(query)\n",
    "        document_vecs = self.forward_representation(document)\n",
    "\n",
    "        score = self.forward_aggregation(query_vecs, document_vecs, query[\"attention_mask\"], document[\"attention_mask\"])\n",
    "        return score\n",
    "\n",
    "    def forward_representation(self,\n",
    "                               tokens,\n",
    "                               sequence_type=None) -> torch.Tensor:\n",
    "        \n",
    "        vecs = self.bert(**tokens)[0] # assuming a distilbert model here\n",
    "        vecs = self.compressor(vecs)\n",
    "\n",
    "        # # if encoding only, zero-out the mask values so we can compress storage\n",
    "        # if sequence_type == \"doc_encode\" or sequence_type == \"query_encode\": \n",
    "        #     vecs = vecs * tokens[\"tokens\"][\"mask\"].unsqueeze(-1)\n",
    "\n",
    "        return vecs\n",
    "\n",
    "    def forward_aggregation(self, query_vecs, document_vecs, query_mask, document_mask):\n",
    "        # query_vecs: B x N x D\n",
    "        # doc_vecs: (B * k) x N x D\n",
    "\n",
    "        # Unsqueeze query vector\n",
    "        _bsz = query_vecs.shape[0]\n",
    "        n_cands = document_vecs.shape[0] // _bsz\n",
    "        query_vecs_dup = query_vecs.repeat_interleave(n_cands, dim=0).contiguous()\n",
    "\n",
    "        score = torch.bmm(query_vecs_dup, document_vecs.transpose(1, 2))\n",
    "        exp_mask = document_mask.bool().unsqueeze(1).expand(-1, score.shape[1], -1)\n",
    "        score[~exp_mask] = - 10000\n",
    "\n",
    "        # max pooling over document dimension\n",
    "        score = score.max(-1).values\n",
    "        query_mask_dup = query_mask.repeat_interleave(n_cands, dim=0).contiguous()\n",
    "\n",
    "        score[~(query_mask_dup.bool())] = 0\n",
    "        score = rearrange(score.sum(-1), '(b n) -> b n', n=n_cands) # B x k \n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d689461f-a584-43ab-a397-da22bb176ec7",
   "metadata": {},
   "source": [
    "### BGE retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d241fba-e199-4989-8af3-406fc9feba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.knowledge_base = \"wiki\"\n",
    "        self.retriever_model = \"bge\"\n",
    "        self.dense_retriever_path = \"save/bge_models_elife\" # save/colbert_lr1e-3/model/\n",
    "        self.search_tokenizer = \"save/bge_models_elife\"\n",
    "        \n",
    "        self.generate_tokenizer = \"allenai/led-base-16384\"\n",
    "        self.generate_model = \"laysumm_eLife_cpt\"\n",
    "        \n",
    "        self.device = \"cuda:2\"\n",
    "        self.n_docs = 5\n",
    "        self.n_cands = 20\n",
    "        self.max_input_length = 16384\n",
    "        self.max_search_length = 512\n",
    "        self.max_output_length = 512\n",
    "        # self.output_file_path = \"output/plos_second_bge.txt\"\n",
    "        # self.input_file_path = \"output/plos.txt\"\n",
    "        self.type = \"rag_inference\" #  first_inference\n",
    "        self.mix_knowledge_num = 5\n",
    "\n",
    "args = Args()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de64f5b-a76f-4fe8-9212-9f5d5dd71308",
   "metadata": {},
   "source": [
    "### Colbert retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fb5533f-705c-4aee-9182-d256f1eedbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Args:\n",
    "#     def __init__(self):\n",
    "#         self.knowledge_base = \"wiki\"\n",
    "#         self.retriever_model = \"colbert\"\n",
    "#         self.dense_retriever_path = \"save/colbert_lr1e-3/model\" # save/colbert_lr1e-3/model/\n",
    "#         self.search_tokenizer = \"save/colbert_lr1e-3\"\n",
    "        \n",
    "#         self.generate_tokenizer = \"allenai/led-base-16384\"\n",
    "#         self.generate_model = \"laysumm_eLife_cpt\"\n",
    "        \n",
    "#         self.device = \"cuda:2\"\n",
    "#         self.n_docs = 1\n",
    "#         self.n_cands = 20\n",
    "#         self.max_input_length = 16384\n",
    "#         self.max_search_length = 512\n",
    "#         self.max_output_length = 512\n",
    "#         # self.output_file_path = \"output/plos_second_bge.txt\"\n",
    "#         # self.input_file_path = \"output/plos.txt\"\n",
    "#         self.type = \"rag_inference\" #  first_inference\n",
    "#         self.mix_knowledge_num = 5\n",
    "\n",
    "# args = Args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06b4796f-e1ae-43db-b385-299201dcc7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieval_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a3c674-3831-4e6b-a9e6-0879b767e592",
   "metadata": {},
   "source": [
    "## ChatGPT used without knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87a9cd72-2e46-4427-9650-2093f909649c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      " 79%|██████████████████████████████████████████████▊            | 191/241 [1:03:13<16:33, 19.86s/it]\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m knowledge \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(knowledge)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# print(\"knowledge\", knowledge)\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a layman rephrase, your goal is to rephrase the input and make it easier to read\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# {\"role\": \"user\", \"content\": f'Here is the original text I want you to help me to rephrase: {line}.\\\u001b[39;49;00m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Here is the knowledge you may need to know: {knowledge}. \\\u001b[39;49;00m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Make it easier to read and retain as much the original text and have a similar length as the original text. '},\u001b[39;49;00m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHere is the original text I want you to help me to rephrase: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mline\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;130;43;01m\\\u001b[39;49;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;43m    Make it easier to read and retain as much the original text and have a similar length as the original text. \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m  \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m res \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     41\u001b[0m predictions\u001b[38;5;241m.\u001b[39mappend(res)\n",
      "File \u001b[0;32m~/anaconda3/envs/llama_factory/lib/python3.10/site-packages/openai/_utils/_utils.py:271\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llama_factory/lib/python3.10/site-packages/openai/resources/chat/completions.py:659\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    657\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    658\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 659\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llama_factory/lib/python3.10/site-packages/openai/_base_client.py:1200\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1187\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1188\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1195\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1196\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1197\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1198\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1199\u001b[0m     )\n\u001b[0;32m-> 1200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/llama_factory/lib/python3.10/site-packages/openai/_base_client.py:889\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    882\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    888\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llama_factory/lib/python3.10/site-packages/openai/_base_client.py:965\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m    964\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/anaconda3/envs/llama_factory/lib/python3.10/site-packages/openai/_base_client.py:1013\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1013\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llama_factory/lib/python3.10/site-packages/openai/_base_client.py:965\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m    964\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/anaconda3/envs/llama_factory/lib/python3.10/site-packages/openai/_base_client.py:1013\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1013\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llama_factory/lib/python3.10/site-packages/openai/_base_client.py:980\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    977\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    979\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 980\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m    983\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    984\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    987\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    988\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "\n",
    "import openai\n",
    "client = OpenAI(api_key =\"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\" )\n",
    "\n",
    "\n",
    "predictions = []\n",
    "dense_retriever = Retriever(args)\n",
    "for line in tqdm(first_gen):\n",
    "    # item = json.loads(line)\n",
    "    # article = item[\"article\"]\n",
    "    search_q = line.split(\"\\n\")[0]\n",
    "    searcher = LuceneSearcher.from_prebuilt_index('enwiki-paragraphs')\n",
    "    # searcher = LuceneSearcher.from_prebuilt_index('beir-v1.0.0-bioasq-flat')\n",
    "    docs = []\n",
    "    for hit in searcher.search(search_q):\n",
    "        res = searcher.doc(hit.docid).raw()\n",
    "        docs.append(res)\n",
    "    documents = dense_retriever.retrieve(search_q, docs, n=args.n_docs, n_cands=args.n_cands)\n",
    "    knowledge = documents[:args.n_docs]\n",
    "    knowledge = \" \".join(knowledge)\n",
    "    # print(\"knowledge\", knowledge)\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system,\" \"content\": \"You are a layman rephrase; your goal is to rephrase the input and make it easier to read.\\\n",
    "        For example 'Diabetes is a condition in which the pancreas is unable to produce enough insulin to feed the body. \\\n",
    "        This is caused by a protein called proinsulin is an ingredient, which is made up of a group of molecules called cysteine thiols.'\\\n",
    "        The rephrased result should be: ''\n",
    "         \"},\n",
    "        # {\"role\": \"user\", \"content\": f'Here is the original text I want you to help me to rephrase: {line}.\\\n",
    "        # Here is the knowledge you may need to know: {knowledge}. \\\n",
    "        # Make it easier to read and retain as much the original text and have a similar length as the original text. '},\n",
    "        {\"role\": \"user\", \"content\": f'Here is the original text I want you to help me to rephrase: {line}.\\\n",
    "        Make it easier to read and retain as much the original text and have a similar length as the original text. '},\n",
    "      ],\n",
    "        \n",
    "    )\n",
    "    res = response.choices[0].message.content\n",
    "    predictions.append(res)\n",
    "    # print(\"Generated text:\", res)\n",
    "    # res = \" \".join(res.split(\"\\n\"))\n",
    "    # output_file.write(res + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36c503bc-b913-4042-83c3-7f18cbb381ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/chatgpt_elife_val.txt\", \"w\") as f:\n",
    "    for pred in predictions:\n",
    "        pred = \" \".join(pred.split(\"\\n\"))\n",
    "        f.write(pred+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30f5a980-8cec-4f27-bb85-ade2ac5ed6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b874e801-af7e-460c-a83b-f684ec624724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gene expression can have a wide range of differences between members of the same group. These differences, often due to changes in regulation, may lead to various evolutionary and medical characteristics. A crucial query in this research area is how changes in DNA sequences affect gene expression. Usually, the focus of most studies is on how genetic changes in regulatory regions affect protein-building sequences.\n",
      "\n",
      "Conversely, we still know very little about the role of RNA 3'-end processing, mainly because it's difficult to identify functional elements in untranslated regions. In this research, we analyzed transcript ends in lymphoblastoid cells from different human individuals genetically. Our study mapped the regulation structure of transcripts and found that transcript end positions didn't randomly or particularly border regulatory element locations, like miRNA sites.\n",
      "\n",
      "The use of these transcript length shapes and motifs differed among human populations. Changes in polyadenylation signals and other 2' sequence elements were substantial indicators of expression levels of the containing genes. We found out how natural variation in RNA 3' end processing influences transcriptional responses to antigen stimulation. The results highlight the importance of two mechanisms in gene regulation: the use and control of distinct RNA length form expression signals, defining the fate of the transcript.\n",
      "**********\n",
      "Gene expression varies widely between individuals of a population, and regulatory change can underlie phenotypes of evolutionary and biomedical interest. A key question in the field is how DNA sequence variants impact gene expression, with most mechanistic studies to date focused on the effects of genetic change on regulatory regions upstream of protein-coding sequence. By contrast, the role of RNA 3′-end processing in regulatory variation remains largely unknown, owing in part to the challenge of identifying functional elements in untranslated regions. In this work, we conducted a genomic survey of transcript ends in lymphoblastoid cells from genetically distinct human individuals. Our analysis mapped the cis-regulatory architecture of transcripts, finding that transcript end positions did not fall randomly or preferentially flanked the locations of regulatory elements, including miRNA sites. The usage of these transcript length forms and motifs varied across human populations and polymorphisms in polyadenylation signals and other 2′ sequence elements were significant predictors of expression levels of the genes in which they lay. We established the effect of natural variation in RNA3′ end processing on transcriptional response to antigen stimulation. These results underscore the importance of two mechanisms at play in gene regulation: the usage and regulation of distinct RNA length form expression signals, which determine transcript fate.\n"
     ]
    }
   ],
   "source": [
    "for i, j in zip(predictions, first_gen[:10]):\n",
    "    print(i)\n",
    "    print(\"*\"*10)\n",
    "    print(j)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0dd335-a1be-43f1-8f43-228ef5323211",
   "metadata": {},
   "source": [
    "## Evaluate the generation result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31d864c-6b91-4c59-8ad2-d2db050e5505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09cc494d-506b-4786-80aa-7fc59ee4e891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores: {'rouge1': 0.40443705140722014, 'rouge2': 0.07948133537129087, 'rougeL': 0.17150468719844816}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.7.7 to v1.9.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../BioLaySumm2024-evaluation_scripts/models/AlignScore/AlignScore-base.ckpt`\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating: 100%|███████████| 191/191 [01:13<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Align Scores: 0.38116153914734957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 6/6 [00:09<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 3/3 [00:00<00:00, 12.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 9.58 seconds, 19.93 sentences/sec\n",
      "bert score 0.8393674592697183\n"
     ]
    }
   ],
   "source": [
    "# evaluate the readability \n",
    "\n",
    "# Note: The file GT_firstGen_elife_test.json and GT_firstGen_plos_test.json is the ground truth is the keywords\n",
    "from rouge_score import rouge_scorer\n",
    "from datasets import load_dataset\n",
    "import textstat\n",
    "from alignscore import AlignScore\n",
    "import numpy as np\n",
    "from bert_score import score\n",
    "# Function to compute ROUGE scores\n",
    "def compute_rouge_scores(predictions, references):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = [scorer.score(ref, pred) for ref, pred in zip(references, predictions)]\n",
    "\n",
    "    # Aggregate scores\n",
    "    aggregated_scores = {}\n",
    "    for score in scores:\n",
    "        for key in score:\n",
    "            if key in aggregated_scores:\n",
    "                aggregated_scores[key].append(score[key].fmeasure)\n",
    "            else:\n",
    "                aggregated_scores[key] = [score[key].fmeasure]\n",
    "\n",
    "    # Calculate average scores\n",
    "    for key in aggregated_scores:\n",
    "        aggregated_scores[key] = sum(aggregated_scores[key]) / len(aggregated_scores[key])\n",
    "\n",
    "    return aggregated_scores\n",
    "def calc_bertscore(preds, refs):\n",
    "  # Get BERTScore F1 scores\n",
    "  P, R, F1 = score(preds, refs, lang=\"en\", verbose=True, device='cuda:0')\n",
    "  return np.mean(F1.tolist())\n",
    "def compute_align_scores(predictions, references):\n",
    "    scorer = AlignScore(model='roberta-base', batch_size=32, device='cuda:0', ckpt_path='/home/yuj49/BioLaySumm2024-evaluation_scripts/models/AlignScore/AlignScore-base.ckpt', evaluation_mode='nli_sp')\n",
    "    score = scorer.score(contexts=references,claims=predictions)\n",
    "    return sum(score)/len(score)\n",
    "def calc_readability(preds):\n",
    "  fkgl_scores = []\n",
    "  cli_scores = []\n",
    "  dcrs_scores = []\n",
    "  for pred in preds:\n",
    "    fkgl_scores.append(textstat.flesch_kincaid_grade(pred))\n",
    "    cli_scores.append(textstat.coleman_liau_index(pred))\n",
    "    dcrs_scores.append(textstat.dale_chall_readability_score(pred))\n",
    "  return np.mean(fkgl_scores), np.mean(cli_scores), np.mean(dcrs_scores)\n",
    "\n",
    "\n",
    "rouge_scores = compute_rouge_scores(predictions, ground_truth[:len(predictions)])\n",
    "print(\"ROUGE Scores:\", rouge_scores)\n",
    "\n",
    "align_scores = compute_align_scores(predictions, ground_truth[:len(predictions)]) # For here the generated expected as the list\n",
    "print(\"Validation Align Scores:\", align_scores)\n",
    "\n",
    "bert_score = calc_bertscore(predictions, ground_truth[:len(predictions)])\n",
    "print(\"bert score\", bert_score)\n",
    "\n",
    "score_dict = {\"FKGL\":[], \"DCRS\":[], \"CLI\":[]}\n",
    "for pred in predictions:\n",
    "    fkgl_score, cli_score, dcrs_score = calc_readability(pred)\n",
    "    score_dict['FKGL'].append(fkgl_score)\n",
    "    score_dict['DCRS'].append(dcrs_score)\n",
    "    score_dict['CLI'].append(cli_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d6029f-e509-4235-b4ba-e181e3138109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First option with the directly input and try different score for evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c307b935-c86e-4865-be88-fe4e2ddebf9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.79469111604866\n",
      "13.46879906271043\n",
      "-35.133471429347715\n"
     ]
    }
   ],
   "source": [
    "print(sum(score_dict[\"FKGL\"])/(len(score_dict[\"FKGL\"])))\n",
    "print(sum(score_dict[\"DCRS\"])/(len(score_dict[\"DCRS\"])))\n",
    "print(sum(score_dict[\"CLI\"])/(len(score_dict[\"CLI\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e50e9ea-c15f-464d-91ff-67d86fccf470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chatgpt plain\n",
    "# ROUGE Scores: {'rouge1': 0.38576205608005476, 'rouge2': 0.07017096174399813, 'rougeL': 0.18846984103884323}\n",
    "# bertscore 0.8427152693271637\n",
    "# Validation Align Scores: 0.4100922286510468\n",
    "# FKGL -5.789  DCRS 13.417  CLI -35.144\n",
    "\n",
    "# chatgpt with bge retrieval 5 cand 20\n",
    "# ROUGE Scores: {'rouge1': 0.3370685418482, 'rouge2': 0.05843868798311576, 'rougeL': 0.14936768461873667}\n",
    "# Validation Align Scores: 0.42011058032512666\n",
    "# bert score 0.8259625196456909\n",
    "# FKGL -5.75 DCRS 13.32 CLI -35.21\n",
    "\n",
    "# chatgpt with colbert retrievel 5 cand 20\n",
    "# ROUGE Scores: {'rouge1': 0.33545327961618493, 'rouge2': 0.06570535526305649, 'rougeL': 0.1528141784719786}\n",
    "# Validation Align Scores: 0.4152084425091743\n",
    "# bert score 0.828134822845459\n",
    "# FKGL -5.77  DCRS 13.30  CLI -35.16\n",
    "\n",
    "# chatgpt with bge retrieval 1 cand 20\n",
    "# ROUGE Scores: {'rouge1': 0.34154136162788945, 'rouge2': 0.06341370356748646, 'rougeL': 0.1519489214091209}\n",
    "# Validation Align Scores: 0.31561801955103874\n",
    "# bert score 0.8296158909797668\n",
    "# FKGL -5.859983019762244 DCRS 13.238312806 CLI -35.006098\n",
    "\n",
    "\n",
    "# chatgpt with colbert retrievel 1 cand 20\n",
    "# ROUGE Scores: {'rouge1': 0.35330511523888974, 'rouge2': 0.06159824991791142, 'rougeL': 0.16774306256048044}\n",
    "# Validation Align Scores: 0.38013114780187607\n",
    "# bert score 0.8308048903942108\n",
    "# FKGL -5.832 DCRS 13.376  CLI -35.06\n",
    "\n",
    "\n",
    "# LED\n",
    "# ROUGE Scores: {'rouge1': 0.45340366019739947, 'rouge2': 0.13184101337906284, 'rougeL': 0.23053450935851877}\n",
    "# bertscore 0.8559\n",
    "# Validation Align Scores: 0.2981382548809052\n",
    "# FKGL -5.524537603328877 DCRS 13.562133951332607 CLI -35.66049221645678"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
